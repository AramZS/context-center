{"initialLink":"https://darobin.github.io/pup/","sanitizedLink":"https://darobin.github.io/pup/","finalLink":"https://darobin.github.io/pup/","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://darobin.github.io/pup/\" itemprop=\"url\">Principles of User Privacy (PUP)</a></contexter-box-head></contexter-box-head><contexter-byline class=\"p-author author\" slot=\"author\"><span class=\"p-name byline\" rel=\"author\" itemprop=\"author\">@robinberjon</span></contexter-byline><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2022-04-05T17:38:42.153Z\">3/5/2022</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>Setting the standard for a robust, policy-ready understanding of privacy.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a is=\"contexter-link\" href=\"https://darobin.github.io/pup/\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"bb19dd3248eb6608170e294e3c890943f1b3ebe7","data":{"originalLink":"https://darobin.github.io/pup/","sanitizedLink":"https://darobin.github.io/pup/","canonical":"https://darobin.github.io/pup/","htmlText":"<!DOCTYPE html>\n<html>\n  <head>\n    <meta charset=\"utf-8\">\n    <script src=\"https://www.w3.org/Tools/respec/respec-w3c\" async class=\"remove\"></script>\n    <link rel=\"icon\" href=\"data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%220.9em%22 font-size=%22105%22>üê∂</text></svg>\">\n    <title>Principles of User Privacy (PUP)</title>\n    <meta name=\"twitter:card\" content=\"summary_large_image\">\n    <meta name=\"twitter:site\" content=\"@robinberjon\">\n    <meta name=\"twitter:creator\" content=\"@robinberjon\">\n    <meta name=\"twitter:title\" property=\"og:title\" content=\"Principles of User Privacy (PUP)\">\n    <meta name=\"twitter:description\" property=\"og:description\" content=\"Setting the standard for a robust, policy-ready understanding of privacy.\">\n    <meta name=\"twitter:image\" property=\"og:image\" content=\"https://darobin.github.io/pup/pup.png\">\n    <meta name=\"twitter:image:alt\" content=\"A cute puppy face drawing\">\n    <meta name=\"twitter:url\" property=\"og:url\" content=\"https://darobin.github.io/pup/\">\n    <meta property=\"og:locale\" content=\"en\">\n    <style>\n      body {\n        background: url(proposal.svg) no-repeat fixed !important;\n        background-size: 25px 380px !important;\n      }\n    </style>\n    <script class=\"remove\">\n      var respecConfig = {\n        specStatus: 'unofficial',\n        postProcess: [(config, doc) => {\n          let time = doc.querySelector('#w3c-state time')\n            , h2 = doc.querySelector('#w3c-state')\n          ;\n          h2.innerHTML = 'Proposal ';\n          h2.appendChild(time);\n        }],\n        editors: [{\n          name: 'Robin Berjon',\n          company: 'The New York Times',\n          companyURL: 'https://nytimes.com/',\n          url: 'https://berjon.com/',\n        }],\n        github: {\n          repoURL: 'https://github.com/darobin/pup',\n          branch: 'main',\n        },\n        edDraftURI: 'https://darobin.github.io/pup/',\n        shortName: 'pup',\n        localBiblio: {\n          'ANTI-TRACKING-POLICY': {\n            title: 'Anti-Tracking Policy',\n            href: 'https://wiki.mozilla.org/Security/Anti_tracking_policy#Tracking_Definition',\n            publisher: 'Mozilla',\n          },\n          'BIG-DATA-COMPETITION': {\n            title: 'Big Data and Competition Policy',\n            href: 'https://global.oup.com/academic/product/big-data-and-competition-policy-9780198788140?lang=en&cc=us',\n            authors: ['Maurice E. Stucke', 'Allen P. Grunes'],\n            publisher: 'Oxford University Press',\n          },\n          'BIT-BY-BIT': {\n            title: 'Bit By Bit: Social Research in the Digital Age',\n            href: 'https://www.bitbybitbook.com/',\n            authors: ['Matt Salganik'],\n            publisher: 'Princeton University Press',\n            status: 'You can read this book free of charge, but Matt is an outstanding author and I encourage you to support him by buying his book!',\n          },\n          'CAT': {\n            title: 'Content Aggregation Technology (CAT)',\n            authors: ['Robin Berjon', 'Justin Heideman'],\n            href: 'https://nytimes.github.io/std-cat/',\n          },\n          'CONFIDING': {\n            title: 'Confiding in Con Men: U.S. Privacy Law, the GDPR, and Information Fiduciaries',\n            authors: ['Lindsey Barrett'],\n            href: 'https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3354129',\n          },\n          'CONSENT-LACKEYS': {\n            title: 'Publishers tell Google: We\\'re not your consent lackeys',\n            authors: ['Rebecca Hill'],\n            href: 'https://www.theregister.com/2018/05/01/publishers_slam_google_ad_policy_gdpr_consent/',\n            publisher: 'The Register',\n          },\n          'DARK-PATTERNS': {\n            title: 'Dark patterns: past, present, and future',\n            authors: ['Arvind Narayanan', 'Arunesh Mathur', 'Marshini Chetty', 'Mihir Kshirsagar'],\n            href: 'https://dl.acm.org/doi/10.1145/3397884',\n            publisher: 'ACM',\n          },\n          'DARK-PATTERN-DARK': {\n            title: 'What Makes a Dark Pattern‚Ä¶ Dark? Design Attributes, Normative Considerations, and Measurement Methods',\n            authors: ['Arunesh Mathur', 'Jonathan Mayer', 'Mihir Kshirsagar'],\n            href: 'https://arxiv.org/abs/2101.04843v1',\n          },\n          'DATA-FUTURES-GLOSSARY': {\n            title: 'Data Futures Lab Glossary',\n            authors: ['Mozilla Insights'],\n            href: 'https://foundation.mozilla.org/en/data-futures-lab/data-for-empowerment/data-futures-lab-glossary/',\n            publisher: 'Mozilla Foundation',\n          },\n          'DEMOCRATIC-DATA': {\n            title: 'Democratic Data: A Relational Theory For Data Governance',\n            authors: ['Salom√© Viljoen'],\n            href: 'https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3727562',\n            publisher: 'Yale Law Journal',\n          },\n          'DIGITAL-MARKET-MANIPULATION': {\n            title: 'Digital Market Manipulation',\n            authors: ['Ryan Calo'],\n            href: 'https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2309703',\n            publisher: 'George Washington Law Review',\n          },\n          'EUROBAROMETER-443': {\n            title: 'Eurobarometer 443: e-Privacy',\n            authors: ['European Commission'],\n            href: 'https://ec.europa.eu/COMMFrontOffice/publicopinion/index.cfm/Survey/getSurveyDetail/instruments/FLASH/surveyKy/2124',\n          },\n          'FIDUCIARY-UA': {\n            title: 'The Fiduciary Duties of User Agents',\n            href: 'https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3827421',\n            authors: ['Robin Berjon'],\n          },\n          'FIP': {\n            title: 'Fair Information Practices: A Basic History',\n            href: 'http://bobgellman.com/rg-docs/rg-FIPShistory.pdf',\n            authors: ['Bob Gellman'],\n            status: '(PDF)',\n          },\n          'GDPR': {\n            title: 'General Data Protection Regulations (GDPR) / Regulation (EU) 2016/679',\n            href: 'https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32016R0679&from=EN',\n            authors: ['European Parliament and Council of European Union'],\n          },\n          'GPC': {\n            title: 'Global Privacy Control (GPC)',\n            authors: ['Robin Berjon', 'Sebastian Zimmeck', 'Ashkan Soltani', 'David Harbage', 'Peter Snyder'],\n            href: 'https://globalprivacycontrol.github.io/gpc-spec/',\n            publisher: 'W3C',\n          },\n          'NYT-PRIVACY': {\n            title: 'How The New York Times Thinks About Your Privacy',\n            author: ['Robin Berjon'],\n            href: 'https://open.nytimes.com/how-the-new-york-times-thinks-about-your-privacy-bc07d2171531',\n            publisher: 'NYT Open',\n          },\n          'PBD': {\n            title: 'Privacy by Design',\n            href: 'https://conversationalist.org/2019/09/13/feminism-explains-our-toxic-relationships-with-our-smartphones/',\n            publisher: 'Office of the Information and Privacy Commissioner, Ontario',\n          },\n          'PHONE-ON-FEMINISM': {\n            title: 'This is your phone on feminism',\n            href: 'https://conversationalist.org/2019/09/13/feminism-explains-our-toxic-relationships-with-our-smartphones/',\n            authors: ['Maria Farrell'],\n            publisher: 'The Conversationalist',\n            rawDate: '2019-09-13',\n          },\n          'PRIVACY-BEHAVIOR': {\n            title: 'Privacy and Human Behavior in the Age of Information',\n            authors: ['Alessandro Acquisti', 'Laura Brandimarte', 'George Loewenstein'],\n            href: 'https://www.heinz.cmu.edu/~acquisti/papers/AcquistiBrandimarteLoewenstein-S-2015.pdf',\n            publisher: 'Science',\n          },\n          'PRIVACY-CONTESTED': {\n            title: 'Privacy is an essentially contested concept: a multi-dimensional analytic for mapping privacy',\n            authors: ['Deirdre K. Mulligan', 'Colin Koopman', 'Nick Doty'],\n            href: 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5124066/',\n            publisher: 'Philosophical Transacions A',\n          },\n          'PRIVACY-HARMS': {\n            title: 'Privacy Harms',\n            authors: ['Danielle Keats Citron', 'Daniel J. Solove'],\n            href: 'https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3782222',\n          },\n          'PRIVACY-AS-CI': {\n            title: 'Privacy As Contextual Integrity',\n            authors: ['Helen Nissenbaum'],\n            href: 'https://digitalcommons.law.uw.edu/wlr/vol79/iss1/10/',\n            publisher: 'Washington Law Review',\n          },\n          'PRIVACY-IN-CONTEXT': {\n            title: 'Privacy in Context',\n            authors: ['Helen Nissenbaum'],\n            href: 'https://www.sup.org/books/title/?id=8862',\n            publisher: 'SUP',\n          },\n          'PRIVACY-IS-POWER': {\n            title: 'Privacy Is Power',\n            authors: ['Carissa V√©liz'],\n            href: 'https://www.penguin.com.au/books/privacy-is-power-9781787634046',\n            publisher: 'Bantam Press',\n          },\n          'PRIVACY-PROJECT': {\n            title: 'The Privacy Project',\n            href: 'https://www.nytimes.com/interactive/2019/opinion/internet-privacy-project.html',\n            publisher: 'The New York Times',\n          },\n          'PRIVACY-THREAT': {\n            title: 'Target Privacy Threat Model',\n            href: 'https://w3cping.github.io/privacy-threat-model/',\n            authors: ['Jeffrey Yasskin', 'Tom Lowenthal'],\n            publisher: 'W3C PING',\n          },\n          'RELATIONAL-TURN': {\n            title: 'A Relational Turn for Data Protection?',\n            href: 'https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3745973&s=09',\n            authors: ['Neil Richards', 'Woodrow Hartzog'],\n          },\n          'SEEING-LIKE-A-STATE': {\n            title: 'Seeing Like a State: How Certain Schemes to Improve the Human Condition Have Failed',\n            href: 'https://bookshop.org/books/seeing-like-a-state-how-certain-schemes-to-improve-the-human-condition-have-failed/9780300246759',\n            authors: ['James C. Scott'],\n          },\n          'SURVEILLANCE-CAPITALISM': {\n            title: 'The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power',\n            authors: ['Shoshana Zuboff'],\n            href: 'https://www.publicaffairsbooks.com/titles/shoshana-zuboff/the-age-of-surveillance-capitalism/9781610395694/',\n            publisher: 'Hachette Public Affairs',\n          },\n          'TAKING-TRUST-SERIOUSLY': {\n            title: 'Taking Trust Seriously in Privacy Law',\n            href: 'https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2655719',\n            authors: ['Neil Richards', 'Woodrow Hartzog'],\n          },\n          'TRACKING-PREVENTION-POLICY': {\n            title: 'Tracking Prevention Policy',\n            href: 'https://webkit.org/tracking-prevention-policy/',\n            publisher: 'Apple',\n          },\n          'W3C-PROCESS': {\n            title: 'W3C Process Document',\n            authors: ['Elika J. Etemad / fantasai', 'Florian Rivoal'],\n            href: 'https://www.w3.org/Consortium/Process/',\n            publisher: 'W3C',\n          },\n        },\n      };\n    </script>\n  </head>\n  <body>\n    <section id=\"abstract\">\n      <p>\n        Privacy has been an essentially contested concept [[?PRIVACY-CONTESTED]]. Its debated meaning\n        render its support problematic in the context of a standards-setting process grounded in\n        consensus [[?W3C-PROCESS]], in seeking out technical solutions grounded on shared\n        requirements, and in addressing the needs of a worldwide constituency. This document\n        provides definitions for privacy and related concepts that are suitable for a global\n        audience, that can provide building blocks for privacy threat modelling, and can guide the\n        development of the Web as a trustworthy platform. In the spirit of building a much-needed\n        bridge between technology and policy, this document is written under the expectation that\n        it can apply to both.\n      </p>\n    </section>\n    <section id=\"sotd\"></section>\n    <!--\n      XXX:\n      For a future iteration\n      - definitions around inferred and derived data that can be useful in some contexts\n        https://twitter.com/robinberjon/status/1378732256353779712?s=09\n      - take content from Privately Yours\n      - Add the difference between granularity of the means vs purpose (from Fid)\n      - economics considerations?\n      - Garuda\n      - include Mittelstadt\n      - Hartzog on FIPs\n      - Richards/Hartzog on Pathologies of Digital Consent\n      - sovereignty (mentioned, needs definition ‚Äî could go in part with sole controllership).\n        Should define \"invasive\" (or theft) practices. Cite Drummond?\n    -->\n    <section>\n      <h2>Introduction</h2>\n      <p>\n        Privacy is essential to trust, and trust is a cornerstone value of the Web [[?RFC8890]].\n        In much of everyday life, people have little difficulty assessing whether a given flow of\n        information constitutes a violation of privacy or not [[?NYT-PRIVACY]]. However, in the digital\n        space, users struggle to understand how their data may flow between contexts and how such\n        flows may affect them, not just immediately but at a much later time and in completely\n        different situations. Some actors then seize upon this confusion in order to extract and\n        exploit [=personal data=] at unprecedented scale.\n      </p>\n      <p>\n        The goal of this document is to define all the terms that may prove useful in developing\n        technology and policy that relate to privacy and [=personal data=]. It additionally provides\n        a toolbox to support the common need that is privacy threat modelling, the frequent debate\n        over consent, and the under-developed set of issues in privacy that are of a\n        collective, relational nature.\n      </p>\n      <p>\n        [=Personal data=] is a regulated object, and this document naturally recognises the\n        jurisdictional primacy of existing data protection regimes. However, the global nature of\n        the Web means that, as we develop technology, we benefit from shared concepts that guide the\n        evolution of the Web as a system built for its users [[?RFC8890]]. A clear and well-defined\n        view of privacy on the Web, grounded in an up-to-date understanding of the state of the art,\n        can hopefully help the Web's constituencies thrive across jurisdictional disparity, with the\n        shared understanding that the law is a floor, not a ceiling.\n      </p>\n    </section>\n    <section>\n      <h2>Definitions</h2>\n      <p>\n        This section provides a number of elementary building blocks from which to establish a\n        shared understanding of privacy. Some of the definitions below build atop the work in\n        <em>Tracking Preference Expression (DNT)</em> [[tracking-dnt]].\n      </p>\n      <section>\n        <h2>People &amp; Data</h2>\n        <p>\n          A <dfn>user</dfn> (also <dfn data-lt=\"individual\">person</dfn> or <dfn>data subject</dfn>) is any natural\n          person.\n        </p>\n        <p>\n          We define <dfn data-lt=\"data\">personal data</dfn> as any information relating to a [=person=] such\n          that:\n        </p>\n        <ul>\n          <li>\n            this [=person=] is identified, directly or indirectly, by reference to an\n            identifier such as a name, email address, an arbitrary identifier or identification\n            number, an online identifier such as an IP address or any identifier attached to a\n            device this [=person=] may be using, phone number, location data, or factors specific\n            to the physical, physiological, genetic, mental, economic, cultural, or social identity\n            of that [=person=], as well as identifiers derived from such data, for instance\n            through hashing; or\n          </li>\n          <li>\n            this [=person=] could reasonably be reidentified from a conjunction of this data with\n            other data; or\n          </li>\n          <li>\n            the data pertains to a group of people such that a [=person=] may find themselves to\n            be the subject of a treatment related to this group, even if the entity carrying out the\n            treatment has no way to identify that [=person=].\n          </li>\n        </ul>\n        <p>\n          Data is <dfn>permanently de-identified</dfn> when there exists a high level of confidence\n          that no human subject of the data can be identified, directly or indirectly (e.g., via\n          association with an identifier, user agent, or device), by that data alone or in\n          combination with other retained or available information, including as being part of a\n          group. Note that further considerations relating to groups are covered in the\n          <a href=\"#collective\">Collective Issues in Privacy</a> section.\n        </p>\n        <p>\n          Data is <dfn data-lt=\"pseudonymous data|pseudonymity\">pseudonymous</dfn> when:\n        </p>\n        <ul>\n          <li>\n            the identifiers used in the data are under the direct and exclusive control of the\n            [=first party=]; and\n          </li>\n          <li>\n            when these identifiers are shared with a [=third party=], they are made unique to that\n            [=third party=] such that if they are shared with more than one [=third party=] these\n            cannot then match them up with one another; and\n          </li>\n          <li>\n            there is a strong level of confidence that no [=third party=] can match them to any data\n            other than that obtained through interactions with the [=first party=]; and\n          </li>\n          <li>\n            any [=third party=] receiving such identifiers is barred (eg. based on legal terms) from\n            sharing them or the related data further; and\n          </li>\n          <li>\n            technical measures exist to prevent re-identification or the joining of different data\n            sets involving these identifiers, notably against timing or k-anonymity attacks; and\n          </li>\n          <li>\n            there exist contractual terms between the [=first party=] and [=third party=] describing\n            the limited [=purpose=] for which the data is being shared.\n          </li>\n        </ul>\n        <p>\n          This can ensure that [=pseudonymous data=] is used in a manner that provides a minimum\n          degree of governance such that technical and procedural means to guarantee the\n          maintenance of pseudonymity are preserved. Note that [=pseudonymity=], on its own, is not\n          sufficient to render [=data processing=] [=appropriate=].\n        </p>\n        <p>\n          A <dfn data-lt=\"vulnerable\">vulnerable person</dfn> is a [=person=] who, at least in the [=context=] of\n          the [=processing=] being discussed, are unable to exercise sufficient\n          self-determination for any consent they may provide to be receivable. This includes for\n          example children, employees with respect to their employers, people in some situations of\n          intellectual or psychological impairment, or refugees.\n        </p>\n      </section>\n      <section>\n        <h2>The Parties</h2>\n        <p>\n          A <dfn>party</dfn> is a [=person=], a legal entity, or a set of legal entities that\n          share common owners, common controllers, and a group identity that is readily evident to\n          the [=user=] without them needing to consult additional material, typically through\n          common branding.\n        </p>\n        <p>\n          The <dfn data-lt=\"first parties\">first party</dfn> is a [=party=] with which the [=user=] intends to\n          interact. Merely hovering over, muting, pausing, or closing a given piece of content does\n          not constitute a [=user=]'s intent to interact with another party, nor does the simple\n          fact of loading a [=party=] embedded in the one with which the user intends to\n          interact. In cases of clear and conspicuous joint branding, there can be multiple [=first\n          parties=]. The [=first party=] is necessarily a [=data controller=] of the data processing\n          that takes places as a consequence of a [=user=] interacting with it.\n        </p>\n        <p>\n          A <dfn data-lt=\"third parties\">third party</dfn> is any [=party=] other than the [=user=],\n          the [=first party=], or a [=service provider=] acting on behalf of either the [=user=] or\n          the [=first party=].\n        </p>\n        <p>\n          A <dfn>service provider</dfn> or <dfn>data processor</dfn> is considered to be the same\n          [=party=] as the entity contracting it to perform the relevant [=processing=] if it:\n        </p>\n        <ul>\n          <li>\n            is processing the data on behalf of that [=party=];\n          </li>\n          <li>\n            ensures that the data is only retained, accessed, and used as directed by that [=party=]\n            and solely for the list of explicitly-specified [=purposes=] detailed by the directing\n            [=party=] or [=data controller=];\n          </li>\n          <li>\n            may determine implementation details of the data processing in question but does not\n            determine the [=purpose=] for which the data is being [=processed=] nor the overarching\n            [=means=] through which the [=purpose=] is carried out;\n          </li>\n          <li>\n            has no independent right to use the data other than in a [=permanently de-identified=]\n            form (e.g., for monitoring service integrity, load balancing, capacity planning, or\n            billing); and,\n          </li>\n          <li>\n            has a contract in place with the [=party=] which is consistent with the above limitations.\n          </li>\n        </ul>\n        <p>\n          A <dfn>data controller</dfn> is a [=party=] that determines the [=means=] and [=purposes=]\n          of data processing. Any [=party=] that is not a [=service provider=] is a [=data controller=].\n        </p>\n        <p>\n          The <dfn>Vegas Rule</dfn> is a simple implementation of privacy in which \"<em>what happens\n          with the [=first party=] stays with the [=first party=]</em>.\" Put differently, it\n          describes a situation in which the [=first party=] is the only [=data controller=]. Note\n          that, while enforcing the [=Vegas Rule=] provides a rule of thumb describing a necessary\n          baseline for [=appropriate=] [=data processing=], it is not always sufficient to guarantee\n          [=appropriate=] [=processing=] since the [=first party=] can [=process=] data\n          [=inappropriately=].\n        </p>\n      </section>\n      <section>\n        <h2>Acting on Data</h2>\n        <p>\n          A [=party=] <dfn data-lt=\"process|processing|processed|data processing\">processes</dfn> data if it\n          carries out operations on [=personal data=], whether or not by automated means, such as\n          collection, recording, organisation, structuring, storage, adaptation or alteration,\n          retrieval, consultation, use, disclosure by transmission, [=sharing=], dissemination or\n          otherwise making available, [=selling=], alignment or combination, restriction, erasure or\n          destruction.\n        </p>\n        <p>\n          A [=party=] <dfn data-lt=\"share|sharing\">shares</dfn> data if it provides it to any other\n          [=party=]. Note that, under this definition, a [=party=] that provides data to its own\n          [=service providers=] is not [=sharing=] it.\n        </p>\n        <p>\n          A [=party=] <dfn data-lt=\"sell|selling\">sells</dfn> data when it [=shares=] it in exchange\n          for consideration, monetary or otherwise.\n        </p>\n      </section>\n      <section>\n        <h2>Contexts and Privacy</h2>\n        <p>\n          The <dfn>purpose</dfn> of a given [=processing=] of data is an anticipated, intended, or\n          planned outcome of this [=processing=] which is achieved or aimed for within a given\n          [=context=]. A [=purpose=], when described, should be specific enough to be actionable by\n          someone familiar with the relevant [=context=] (ie. they could independently determine\n          [=means=] that reasonably correspond to an implementation of the [=purpose=]).\n        </p>\n        <!--\n          XXX given that contexts are defined from *user* purpose we might wish to have purpose in\n          general and processing purpose for the above.\n        -->\n        <p>\n          The <dfn>means</dfn> are the general method of [=data processing=] through which a given\n          [=purpose=] is implemented, in a given [=context=], considered at a relatively abstract\n          level and not necessarily all the way down to implementation details. Example:\n          <em>the user will have their preferences restored (purpose) by looking up their identifier\n          in a preferences store (means)</em>.\n        </p>\n        <p>\n          A <dfn>context</dfn> is a physical or digital environment that a [=person=] interacts with\n          for a purpose of their own (that they typically share with other [=person=] who interact\n          with the same environment).\n        </p>\n        <p>\n          A [=context=] can be further described through:\n        </p>\n        <ul>\n          <li>\n            Its <dfn>actors</dfn>, which comprise the <dfn>subject</dfn> (a [=person=]) as well as\n            the <dfn>sender</dfn> and <dfn>recipient</dfn> of the data (which are [=parties=]).\n          </li>\n          <li>\n            Its <dfn>attributes</dfn>, which are the types of [=personal data=] being [=processed=]\n            in the [=context=].\n          </li>\n          <li>\n            Its <dfn>transmission principles</dfn>, which are the constraints (typically technical\n            or legal) being placed upon the [=data processing=].\n          </li>\n        </ul>\n        <p>\n          A [=context=] carries <dfn data-lt=\"norm\">context-relative informational norms</dfn> that determine\n          whether a given [=data processing=] is <dfn data-lt=\"appropriately\">appropriate</dfn>\n          (if the norms are adhered to) or <dfn data-lt=\"inappropriately\">inappropriate</dfn>\n          (when the norms are violated). A norm violation can be for instance the exfiltration of\n          [=personal data=] from a context or the lack of respect for [=transmission principles=].\n          When [=norms=] are respected in a given [=context=], we can say that <dfn>contextual\n          integrity</dfn> is maintained; otherwise that it is violated ([[?PRIVACY-IN-CONTEXT]],\n          [[?PRIVACY-AS-CI]]).\n        </p>\n        <p>\n          We define <dfn>privacy</dfn> as a right to [=appropriate=] [=data processing=]. A\n          <dfn>privacy violation</dfn> is, correspondingly, [=inappropriate=] [=data processing=]\n          [[?PRIVACY-IN-CONTEXT]].\n        </p>\n        <p>\n          Note that a [=first party=] can be comprised of multiple [=contexts=] if it is large\n          enough that [=people=] would interact with it for more than one [=purpose=]. [=Sharing=]\n          [=personal data=] across [=contexts=] is, in the overwhelming majority of cases,\n          [=inappropriate=].\n        </p>\n        <div class=\"example\">\n          <p>\n            Your cute little pup uses <em>Poodle Naps</em> to find comfortable places to snooze,\n            and <em>Poodle Fetch</em> to locate the best sticks. Napping and fetching are different\n            [=contexts=] with different norms, and sharing data between these contexts is a\n            [=privacy violation=] despite the shared ownership of <em>Naps</em> and <em>Fetch</em>\n            by the <em>Poodle</em> conglomerate.\n          </p>\n        </div>\n        <p>\n          Colloquially, <dfn>tracking</dfn> is understood to be any kind of [=inappropriate=] data\n          collection.\n        </p>\n        <p>\n          Additionally, <dfn data-lt=\"privacy-labor\">privacy labour</dfn> is the practice of having\n          a [=person=] carry out the work of ensuring [=data processing=] of which they are the\n          subject is [=appropriate=], instead of having the [=parties=] be responsible for that\n          work as is more respectable.\n        </p>\n      </section>\n      <section>\n        <h2>User Agents</h2>\n        <p>\n          The <dfn>user agent</dfn> acts as an intermediary between a [=user=] and the web. The\n          [=user agent=] is <em>not</em> a [=context=] in that it is expected to coincide with the\n          [=subject=] and operate exclusively in the [=subject=]'s interest. It is <em>not</em> the\n          [=first party=]. The [=user agent=] serves the [=user=] in a\n          <dfn data-lt=\"fiduciary relationship\">relationship of fiduciary agency</dfn>: it always puts\n          the [=user=]'s interest first, up to and including, on occasion, protecting the [=user=]\n          from themselves by preventing them from carrying out a harmful decision, or at the very\n          least by speed-bumping it [[?FIDUCIARY-UA]]. For example, the [=user agent=] will make it\n          difficult for the [=user=] to connect to a site the authenticity of which is hard to\n          ascertain, will double-check that the user really intends to expose a sensitive device\n          capability, or will prevent the [=user=] from consenting to permanent monitoring of their\n          behaviour. Its <dfn>fiduciary duties</dfn> include [[?TAKING-TRUST-SERIOUSLY]]:\n        </p>\n        <dl>\n          <dt><dfn>Duty of Protection</dfn></dt>\n          <dd>\n            Protection requires [=user agents=] to affirmatively protect a [=user=]'s data, beyond\n            simple security measures. It is insufficient simply to encrypt at rest and in transit,\n            but one must further limit retention, ensure that the strictly necessary data is\n            collected, or require guarantees from those it is shared to.\n          </dd>\n          <dt><dfn>Duty of Discretion</dfn></dt>\n          <dd>\n            Discretion requires the [=user agent=] to make best efforts to enforce\n            [=context-relative informational norms=] by placing contextual limits on the flow and\n            [=processing=] of [=personal data=]. Discretion is not confidentiality and may place\n            limits on nondisclosure: trust can be preserved even when the [=user agent=] shares the\n            [=personal data=], so long as it is done in an [=appropriately=] discreet manner.\n          </dd>\n          <dt><dfn>Duty of Honesty</dfn></dt>\n          <dd>\n            Honesty requires that the [=user agent=] make sure that the [=user=] is proactively\n            provided with information that is relevant to them and that will enhance the [=user=]'s\n            autonomy, to the extent possible in a manner that they will comprehend and at the right\n            moment, which is almost never when the [=user=] is trying to do something else such as\n            read a page or activate a feature. The duty of honesty goes well beyond that of\n            transparency that dominates legacy privacy regimes. Unlike with transparency, honesty\n            cannot get away with hiding relevant information in complex out-of-band legal notices\n            no more than it can rely on overly cursory information provided in a consent dialog.\n          </dd>\n          <dt><dfn>Duty of Loyalty</dfn></dt>\n          <dd>\n            Because of the special [=fiduciary relationship=] that obtains between [=user=] and\n            [=user agent=], the latter is held to be loyal to the former in all situations, up to and\n            including in preference to the [=user agent=]'s implementer. When a [=user agent=] carries\n            out [=processing=] that is not directly in the [=user=]'s interest but rather benefits\n            another entity such as its implementer, including by piggybacking on [=processing=] that\n            may be in the user's interest, that behaviour is known as <dfn>self-dealing</dfn>.\n            [=Self-dealing=] is always [=inappropriate=]. Loyalty is the avoidance of [=self-dealing=].\n          </dd>\n        </dl>\n        <p>\n          These duties ensure the [=user agent=] will <em>care</em> for the user. It is important to\n          note that there is a subtle difference between care and <dfn>data paternalism</dfn> which\n          is that the latter purports to help in part by removing agency (\"<em>don't worry about it,\n          so long as your data is with us it's safe, you don't need to know what we do with it, it's\n          all good because we're good people</em>\") whereas care aims to support people by enhancing\n          their agency and sovereignty.\n        </p>\n      </section>\n    </section>\n    <section>\n      <h2>Privacy Threat Model Building Blocks</h2>\n      <section>\n        <h2>Threats Are Context-Relative</h2>\n        <p>\n          Privacy threat models for the Web exist ([[?TRACKING-PREVENTION-POLICY]], [[?ANTI-TRACKING-POLICY]],\n          [[?PRIVACY-THREAT]]). While good within the scope of what they intend to do, they tend to be\n          very specific to cross-context tracking as seen by the browser, which is but one problem.\n          Privacy threat modelling could benefit from having more of a toolbox to reuse in different\n          situations.\n        </p>\n        <p>\n          This document provides building blocks for the creation of <dfn>privacy threat models</dfn> on\n          the Web. Note that privacy threat models have an important difference with security threat\n          models in that <em>all</em> [=parties=] are potential threats, even when they are not rogue.\n          In fact the [=first party=] is typically considered to be the primary threat, the one\n          against which the brunt of mitigating techniques are to be leveraged.\n        </p>\n        <p>\n          The most important building blocks for a privacy threat model are those that define a\n          [=context=]: [=actors=], [=attributes=], and [=transmission principles=], as well as the\n          [=context-relative informational norms=]. Collectively, these define the expectations that\n          obtain in a given [=context=]. Based on these expectations, it becomes possible to ask\n          questions about the ways in which they could fail and how to prevent. What happens if the\n          [=subject=] is a [=vulnerable person=]? How well does the context fare if expectations of\n          consent are undermined by manipulative techniques ([[?DIGITAL-MARKET-MANIPULATION]],\n          [[?PRIVACY-BEHAVIOR]])? If the [=transmission principles=] are based on sharing data with\n          parties while telling them that they cannot use it, what confidence do we have that they\n          are following the rules?\n        </p>\n        <p>\n          When assessing privacy threats, it is not necessary to establish harms since breaking the\n          [=context=]'s [=norms=] is [=inappropriate=] in all cases. However, consideration of harms can\n          inform which issues to prioritise. Where individual <dfn>privacy harms</dfn> are to be\n          defined, the definitive source is <em>Privacy Harms</em> [[?PRIVACY-HARMS]].\n        </p>\n      </section>\n      <section>\n        <h2>Identity on the Web</h2>\n        <p>\n          A [=person=]'s <dfn>identity</dfn> is the set of characteristics that define them. In\n          computer systems, [=identity=] is typically attached to a means of denotation that makes\n          it easier for an automated system to recognise the user, an <dfn>identifier</dfn> of\n          some type.\n        </p>\n        <p>\n          A [=person=]'s characteristics, and therefore [=identity=], is entirely\n          [=context=]-dependent. Recognising a [=person=] in distinct [=contexts=] can at times be\n          [=appropriate=] but this relies on an understanding of applicable [=norms=] and will\n          likely require compartmentalisation. (For example, if you meet your therapist at a\n          cocktail party, you expect them to have rather different discussion topics with you than\n          they usually would, and possibly even to pretend they do not know you.) As a result,\n          automating the recognition of a [=person=]'s identity across different [=contexts=] is\n          [=inappropriate=]. This is particularly true for [=vulnerable=] people as recognising them\n          in different [=contexts=] may force their vulnerability into the open.\n        </p>\n        <p>\n          [=Identity=] being by nature fragmented, [=user agents=] must work in support of [=users=]\n          having different identities in different [=context=] with respect to <em>all</em> [=parties=]\n          (including the user agent vendor) and to prevent their recognition through other means,\n          where possible.\n        </p>\n        <p>\n          A keystone principle of the Web is trust [[RFC8890]]. An important part of trust is to\n          ensure that [=data=] collected for a [=purpose=] that matches a clearly delineated [=user=]\n          feature should not then be used for additional secondary [=purposes=]. Email is often used\n          for login and communication [=purposes=], including essential transactional interactions.\n          Using emails for cross-context recognition [=purposes=] is therefore not only\n          [=inappropriate=] but also threatens key messaging infrastructure. Future iterations of\n          the Web platform should ensure that users can log into sites and receive communication\n          without relying on email at all.\n        </p>\n      </section>\n    </section>\n    <section>\n      <h2>User Control and Autonomy</h2>\n      <p>\n        A [=person=]'s <dfn data-lt=\"autonomous\">autonomy</dfn> is their ability to make decisions of their own volition,\n        without undue influence from other parties. People have limited intellectual resources and\n        time with which to weigh decisions, and by necessity rely on shortcuts when making\n        decisions. This makes their privacy preferences malleable [[?PRIVACY-BEHAVIOR]] and susceptible to\n        manipulation [[?DIGITAL-MARKET-MANIPULATION]]. A [=person=]'s [=autonomy=] is enhanced by a\n        system or device when that system offers a shortcut that aligns more with what that [=person=] would\n        have decided given arbitrary amounts of time and relatively unfettered intellectual ability;\n        and [=autonomy=] is decreased when a similar shortcut goes against decisions made under\n        ideal conditions.\n      </p>\n      <p>\n        Affordances and interactions that decrease [=autonomy=] are known as <dfn>dark patterns</dfn>.\n        A [=dark pattern=] does not have to be intentional, the deceptive effect is sufficient to\n        define them [[?DARK-PATTERNS]], [[?DARK-PATTERN-DARK]].\n      </p>\n      <p>\n        Because we are all subject to motivated reasoning, the design of defaults and affordances\n        that may impact [=user=] [=autonomy=] should be the subject of independent scrutiny.\n        Implementers are enjoined to be particularly cautious to avoid slipping into\n        [=data paternalism=].\n      </p>\n      <p>\n        Given the sheer volume of potential [=data=]-related decisions in today's data economy,\n        complete informational self-determination is impossible. This fact, however, should not be\n        confused with the contention that privacy is dead. Careful design of our technological\n        infrastructure can ensure that [=users=]' [=autonomy=] as pertaining to their own [=data=]\n        is enhanced through [=appropriate=] defaults and choice architectures.\n      </p>\n      <p>\n        In the 1970s, the <dfn>Fair Information Practices</dfn> or <dfn>FIPs</dfn> were elaborated\n        in support of individual [=autonomy=] in the face of growing concerns with databases. The\n        [=FIPs=] assume that there is sufficiently little [=data processing=] taking place that any\n        [=person=] will be able to carry out sufficient diligence to enable [=autonomy=] in their\n        decision-making. Since they entirely offload the [=privacy labour=]\n        to [=users=] and assume perfect, unfettered [=autonomy=], the [=FIPs=] do not forbid specific\n        types of [=data processing=] but only place them under different procedural requirements.\n        Such an approach is [=appropriate=] for [=parties=] that are processing data in the 1970s.\n      </p>\n      <p>\n        One notable issue with procedural approaches to privacy is that they tend to have the same\n        requirements in situations where the [=user=] finds themselves in a significant asymmetry of\n        power with a [=party=] ‚Äî for instance the [=user=] of an essential service provided by a\n        monopolistic platform ‚Äî and those where [=user=] and [=parties=] are very much on equal\n        footing, or even where the [=user=] may have greater power, as is the case with small\n        businesses operating in a competitive environment. It further does not consider cases in\n        which one [=party=] may coerce other [=parties=] into facilitating its [=inappropriate=]\n        practices, as is often the case with dominant players in advertising [[?CONSENT-LACKEYS]] or\n        in content aggregation [[?CAT]].\n      </p>\n      <p>\n        Reference to the [=FIPs=] survives to this day. They are often referenced as <em>transparency\n        and choice</em>, which, in today's digital environment, is often a strong indication that\n        [=inappropriate=] [=processing=] is being described.\n      </p>\n      <figure>\n        <img src=\"agnes-tc.jpg\" alt=\"Agnes from Wandavision winking 'Transparency and choice'\">\n        <figcaption>\n          A method of privacy regulation which promises honesty and autonomy but delivers neither.\n          [[?CONFIDING]].\n        </figcaption>\n      </figure>\n      <section>\n        <h2>Opt-in, Consent, Opt-out, Global Controls</h2>\n        <p>\n          Different procedural mechanisms exist to enable [=people=] to control the [=processing=]\n          done to their [=data=]. Mechanisms that increase the number of [=purposes=] for which\n          their [=data=] is being [=processed=] are referred to as <dfn data-lt=\"opt in\">opt-in</dfn> or\n          <dfn>consent</dfn>; mechanisms that decrease this number of [=purposes=] are known as\n          <dfn data-lt=\"opt out\">opt-out</dfn>.\n        </p>\n        <p>\n          When deployed thoughtfully, these mechanisms can enhance [=people=]'s [=autonomy=]. Often,\n          however, they are used as a way to avoid putting in the difficult work of deciding which\n          types of [=processing=] are [=appropriate=] and which are not, offloading [=privacy labour=]\n          to the [=user=].\n        </p>\n        <p>\n          Privacy regulatory regimes are often anchored at extremes: either they default to allowing\n          only very few strictly essential [=purposes=] such that many [=parties=] will have to\n          resort to [=consent=], habituating [=people=] to ignore legal prompts and incentivising\n          [=dark patterns=], or, conversely, they default to forbidding only very few, particularly\n          egregious [=purposes=], such that [=people=] will have to perform the [=privacy labour=] to\n          [=opt out=] in every [=context=] in order to produce [=appropriate=] [=processing=].\n        </p>\n        <p>\n          An approach that is more aligned with the expectation that the Web should provide a\n          trustworthy, [=person=]-centric environment is to establish a regime consisting of\n          three privacy tiers:\n        </p>\n        <dl>\n          <dt><dfn>Default Privacy Tier</dfn></dt>\n          <dd>\n            This is the set of [=purposes=] that are deemed [=appropriate=] in a given [=context=]\n            and the [=processing=] that a [=person=] can expect without having triggered any\n            mechanisms to change their preferences. The exact details for this tier require clear\n            definition, including aspects such as data retention, but [=data=] would have to be\n            systematically siloed by [=context=] (<em>not</em> by [=party=], making it stricter than\n            the [=Vegas Rule=] and more in line with respecting [=privacy=]). This default tier\n            should also be defined differently for certain kinds of [=contexts=]. The legitimate\n            [=processing=] that can take place in this tier derives its legitimacy from matching the\n            expectations and interests of both the [=user=] and the [=first party=] in their\n            relationship, as guided by the applicable [=norms=]. This tier is more [=appropriate=] the more the\n            [=first party=] acts in accordance with [=fiduciary duties=].\n          </dd>\n          <dt><dfn>Opt-out Privacy Tier</dfn></dt>\n          <dd>\n            [=People=] who, either from personal preference or because they are [=vulnerable=],\n            require greater obscurity in some or even most [=contexts=], would be able to transition\n            to this tier through an [=opt-out=] mechanism. This tier would only permit strictly\n            necessary [=purposes=]. This tier should be [=appropriate=] for [=vulnerable=]\n            [=people=].\n          </dd>\n          <dt><dfn>Opt-in Privacy Tier</dfn></dt>\n          <dd>\n            In rare and highly specific cases, [=people=] should be able to [=consent=] to more\n            sensitive [=purposes=], such as having their [=identity=] recognised across contexts or\n            their reading history shared with a company. The burden of proof on ensuring that informed [=consent=] has\n            been obtained needs in this case to be very high (much higher than what prevails for\n            instance in [[?GDPR]] jurisdictions as currently practices). [=Consent=] is comparable to the general problem\n            of permissions on the Web platform. In the same way that it should be clear when a given\n            device capability is in use (eg. you are providing geolocation or camera access), sharing\n            data under this tier should be set up in such a way that it requires deliberate, specific\n            action from the [=user=] (eg. triggering a form control) and if that [=consent=] is\n            persistent, there should be an indicator that data is being transmitted shown at all\n            times, in such a way that the user can easily switch it off. In general, providing\n            [=consent=] should be rare, difficult, highly intentional, and temporary.\n          </dd>\n        </dl>\n        <p>\n          When an [=opt-out=] mechanism exists, it should preferably be complemented by a\n          <dfn>global opt-out</dfn> mechanism. The function of a [=global opt-out=] mechanism is to\n          rectify the <dfn>automation asymmetry</dfn> whereby service providers can automate\n          [=data processing=] but [=people=] have to take manual action. A good example of a\n          [=global opt-out=] mechanism is the <em>Global Privacy Control</em> [[?GPC]].\n        </p>\n        <p>\n          Conceptually, a [=global opt-out=] mechanism is an automaton operating as part of the\n          [=user agent=], which is to say that it is equivalent to a robot that would carry out the\n          [=user=]'s bidding by pressing an [=opt-out=] button with every interaction that the\n          [=user=] has with a site, or more generally conveys an expression of the [=user=]'s\n          rights in a relevant jurisdiction. (For instance, under [[?GDPR]], the [=user=] may be\n          conveying objections to [=processing=] based on legitimate interest or the withdrawal of\n          [=consent=] to specific [=purposes=].) It should be noted that, since a [=global opt-out=]\n          signal is reaffirmed automatically with every [=user=] interaction, it will take precedence\n          in terms of specificity over any manner of blanket [=consent=] that a site may obtain,\n          unless that [=consent=] is directly attached to an interaction (eg. terms specified on a\n          form upon submission).\n        </p>\n      </section>\n    </section>\n    <section id=\"collective\">\n      <h2>Collective Issues in Privacy</h2>\n      <p>\n        When designing Web technology, we naturally pay attention to potential impacts on the [=person=]\n        using the Web through their [=user agent=]. In addition to potential individual harms we also\n        pay heed to collective effects that emerge from the accumulation of individual actions as\n        influenced by entities and the structure of technology.\n      </p>\n      <p>\n        Note that in evaluating impact, we deliberately ignore what implementers or specifiers may\n        have <em>intended</em> and only focus on outcomes. This framing is known as <dfn>POSIWID</dfn>, or\n        \"<em>the Purpose Of a System Is What It Does</em>\".\n      </p>\n      <p>\n        The collective problem of privacy is known as <dfn>legibility</dfn>. [=Legibility=] concerns\n        population-level [=data processing=] that may impact populations or individuals, including in\n        ways that [=people=] could not control even under the optimistic assumptions of the [=FIPs=].\n        For example, based on population-level analysis, a company may know that <var>site.example</var>\n        is predominantly visited by [=people=] of a given race or gender, and decide not to run its\n        job ads there. Visitors to that page are implicitly having their [=data=] processed in\n        [=inappropriate=] ways, with no way to discover the discrimination or seek relief\n        [[?DEMOCRATIC-DATA]].\n      </p>\n      <p>\n        What we consider is therefore not just the relation between the people who expose themselves\n        and the entities that invite that disclosure [[?RELATIONAL-TURN]], but also between the people\n        who expose themselves and those who do not but may find themselves recognised as such indirectly\n        anyway. One key understanding here is that such relations may persists even when data is\n        [=permanently de-identified=].\n      </p>\n      <p>\n        [=Legibility=] practices can be <dfn data-lt=\"legitimacy\">legitimate</dfn> or\n        <dfn data-lt=\"illegitimacy\">illegitimate</dfn> depending\n        on the [=context=] and on the [=norms=] that apply in that [=context=]. Typically, a\n        [=legibility=] practice may be [=legitimate=] if it is managed through an acceptable process\n        of collective [=governance=]. For example, it is often considered [=legitimate=] for a\n        government, under the control of its citizens, to maintain a database of license plates for\n        the [=purpose=] of enforcing the rules of the road. It would be [=illegitimate=] to observe\n        the same license plates near places of worship to build a database of religious identity.\n      </p>\n      <p>\n        [=Legibility=] is often used to order information about the world. This can notably create\n        problems of [=reflexivity=] and of [=autonomy=].\n      </p>\n      <p>\n        Problems of <dfn>reflexivity</dfn> occur when the ordering of information about the world\n        used to produce [=legibility=] finds itself changing the way in which the world operates.\n        This can produce self-reinforcing loops that can have deleterious effects both individual and\n        collective [[?SEEING-LIKE-A-STATE]].\n      </p>\n      <p>\n        Issues of [=autonomy=] occur depending on the manner in which [=legibility=] is implemented.\n        When [=legibility=] is used to order the world following rules set by the [=user=] or\n        following methods subject to public scrutiny and [=governance=] models with strong checks and\n        balances (such as a newspaper's editorial decisions), then it will enhance [=user=] [=autonomy=]\n        and tend to be [=legitimate=]. When it is done in the [=user=]'s stead and without [=governance=],\n        it decreases [=user=] [=autonomy=] and tends to be [=illegitimate=].\n      </p>\n      <!--\n      <p>\n        From an economics standpoint, it is important to note that the broad sharing of data can lead\n        to anticompetitive outcomes, notably due to network effects stemming from processing data across\n        multiple [=contexts=] [[?BIG-DATA-COMPETITION]]. Restricting flows of [=data=] between\n        different [=contexts=] (and not just [=parties=]) is therefore likely to improve competition.\n      </p>\n      -->\n      <p>\n        <dfn data-lt=\"governance\">Data governance</dfn> refers to the rules and processes for how [=data=] is\n        [=processed=] in any given [=context=]. How data is <em>governed</em> describes who has\n        power to make decisions over [=data=] and how [[?DATA-FUTURES-GLOSSARY]].\n      </p>\n      <p>\n        In general, collective issues in [=data=] require collective solutions. The proper goal of\n        [=data governance=] at the standards-setting level is the development of structural controls\n        in [=user agents=] and the provision of institutions that can handle population-level problems\n        in [=data=]. [=Governance=] will often struggle to achieve its goals if it works primarily by\n        increasing <em>individual</em> control over [=data=]. A collective approach reduces the cost of\n        control.\n      </p>\n      <p>\n        Collecting data at large scales can have significant pro-social outcomes. Problems tend to\n        emerge when entities take part in dual-use collection in which [=data=] is [=processed=]\n        for collective benefit but also for [=self-dealing=] [=purposes=] that may degrade welfare.\n        The [=self-dealing=] [=purposes=] will be justified as bankrolling the pro-social outcomes,\n        which, absent collective oversight, cannot be considered to support claims to [=legitimacy=]\n        for such [=legibility=]. It is vital for standards-setting organisations to establish not\n        just purely technical devices but techno-social systems that can govern data at scale.\n      </p>\n      <!--\n        Imagine a case in which collecting browser data can be leveraged to improve both search\n        services (which have a business model) and user security (which generally doesn't). This\n        data could be collected through a data trust under stakeholder governance, search\n        companies paying for access with strong checks and balances, and privacy guarantees, and\n        the funds would be used to support enhanced security for all.\n      -->\n    </section>\n  </body>\n</html>\n","oembed":false,"readabilityObject":{"title":"Principles of User Privacy (PUP)","content":"<div id=\"readability-page-1\" class=\"page\">\n    <section id=\"abstract\">\n      <p>\n        Privacy has been an essentially contested concept [[?PRIVACY-CONTESTED]]. Its debated meaning\n        render its support problematic in the context of a standards-setting process grounded in\n        consensus [[?W3C-PROCESS]], in seeking out technical solutions grounded on shared\n        requirements, and in addressing the needs of a worldwide constituency. This document\n        provides definitions for privacy and related concepts that are suitable for a global\n        audience, that can provide building blocks for privacy threat modelling, and can guide the\n        development of the Web as a trustworthy platform. In the spirit of building a much-needed\n        bridge between technology and policy, this document is written under the expectation that\n        it can apply to both.\n      </p>\n    </section>\n    \n    <!--\n      XXX:\n      For a future iteration\n      - definitions around inferred and derived data that can be useful in some contexts\n        https://twitter.com/robinberjon/status/1378732256353779712?s=09\n      - take content from Privately Yours\n      - Add the difference between granularity of the means vs purpose (from Fid)\n      - economics considerations?\n      - Garuda\n      - include Mittelstadt\n      - Hartzog on FIPs\n      - Richards/Hartzog on Pathologies of Digital Consent\n      - sovereignty (mentioned, needs definition ‚Äî could go in part with sole controllership).\n        Should define \"invasive\" (or theft) practices. Cite Drummond?\n    -->\n    <section>\n      <h2>Introduction</h2>\n      <p>\n        Privacy is essential to trust, and trust is a cornerstone value of the Web [[?RFC8890]].\n        In much of everyday life, people have little difficulty assessing whether a given flow of\n        information constitutes a violation of privacy or not [[?NYT-PRIVACY]]. However, in the digital\n        space, users struggle to understand how their data may flow between contexts and how such\n        flows may affect them, not just immediately but at a much later time and in completely\n        different situations. Some actors then seize upon this confusion in order to extract and\n        exploit [=personal data=] at unprecedented scale.\n      </p>\n      <p>\n        The goal of this document is to define all the terms that may prove useful in developing\n        technology and policy that relate to privacy and [=personal data=]. It additionally provides\n        a toolbox to support the common need that is privacy threat modelling, the frequent debate\n        over consent, and the under-developed set of issues in privacy that are of a\n        collective, relational nature.\n      </p>\n      <p>\n        [=Personal data=] is a regulated object, and this document naturally recognises the\n        jurisdictional primacy of existing data protection regimes. However, the global nature of\n        the Web means that, as we develop technology, we benefit from shared concepts that guide the\n        evolution of the Web as a system built for its users [[?RFC8890]]. A clear and well-defined\n        view of privacy on the Web, grounded in an up-to-date understanding of the state of the art,\n        can hopefully help the Web's constituencies thrive across jurisdictional disparity, with the\n        shared understanding that the law is a floor, not a ceiling.\n      </p>\n    </section>\n    <section>\n      <h2>Definitions</h2>\n      <p>\n        This section provides a number of elementary building blocks from which to establish a\n        shared understanding of privacy. Some of the definitions below build atop the work in\n        <em>Tracking Preference Expression (DNT)</em> [[tracking-dnt]].\n      </p>\n      <section>\n        <h2>People &amp; Data</h2>\n        <p>\n          A <dfn>user</dfn> (also <dfn data-lt=\"individual\">person</dfn> or <dfn>data subject</dfn>) is any natural\n          person.\n        </p>\n        <p>\n          We define <dfn data-lt=\"data\">personal data</dfn> as any information relating to a [=person=] such\n          that:\n        </p>\n        <ul>\n          <li>\n            this [=person=] is identified, directly or indirectly, by reference to an\n            identifier such as a name, email address, an arbitrary identifier or identification\n            number, an online identifier such as an IP address or any identifier attached to a\n            device this [=person=] may be using, phone number, location data, or factors specific\n            to the physical, physiological, genetic, mental, economic, cultural, or social identity\n            of that [=person=], as well as identifiers derived from such data, for instance\n            through hashing; or\n          </li>\n          <li>\n            this [=person=] could reasonably be reidentified from a conjunction of this data with\n            other data; or\n          </li>\n          <li>\n            the data pertains to a group of people such that a [=person=] may find themselves to\n            be the subject of a treatment related to this group, even if the entity carrying out the\n            treatment has no way to identify that [=person=].\n          </li>\n        </ul>\n        <p>\n          Data is <dfn>permanently de-identified</dfn> when there exists a high level of confidence\n          that no human subject of the data can be identified, directly or indirectly (e.g., via\n          association with an identifier, user agent, or device), by that data alone or in\n          combination with other retained or available information, including as being part of a\n          group. Note that further considerations relating to groups are covered in the\n          <a href=\"#collective\">Collective Issues in Privacy</a> section.\n        </p>\n        <p>\n          Data is <dfn data-lt=\"pseudonymous data|pseudonymity\">pseudonymous</dfn> when:\n        </p>\n        <ul>\n          <li>\n            the identifiers used in the data are under the direct and exclusive control of the\n            [=first party=]; and\n          </li>\n          <li>\n            when these identifiers are shared with a [=third party=], they are made unique to that\n            [=third party=] such that if they are shared with more than one [=third party=] these\n            cannot then match them up with one another; and\n          </li>\n          <li>\n            there is a strong level of confidence that no [=third party=] can match them to any data\n            other than that obtained through interactions with the [=first party=]; and\n          </li>\n          <li>\n            any [=third party=] receiving such identifiers is barred (eg. based on legal terms) from\n            sharing them or the related data further; and\n          </li>\n          <li>\n            technical measures exist to prevent re-identification or the joining of different data\n            sets involving these identifiers, notably against timing or k-anonymity attacks; and\n          </li>\n          <li>\n            there exist contractual terms between the [=first party=] and [=third party=] describing\n            the limited [=purpose=] for which the data is being shared.\n          </li>\n        </ul>\n        <p>\n          This can ensure that [=pseudonymous data=] is used in a manner that provides a minimum\n          degree of governance such that technical and procedural means to guarantee the\n          maintenance of pseudonymity are preserved. Note that [=pseudonymity=], on its own, is not\n          sufficient to render [=data processing=] [=appropriate=].\n        </p>\n        <p>\n          A <dfn data-lt=\"vulnerable\">vulnerable person</dfn> is a [=person=] who, at least in the [=context=] of\n          the [=processing=] being discussed, are unable to exercise sufficient\n          self-determination for any consent they may provide to be receivable. This includes for\n          example children, employees with respect to their employers, people in some situations of\n          intellectual or psychological impairment, or refugees.\n        </p>\n      </section>\n      <section>\n        <h2>The Parties</h2>\n        <p>\n          A <dfn>party</dfn> is a [=person=], a legal entity, or a set of legal entities that\n          share common owners, common controllers, and a group identity that is readily evident to\n          the [=user=] without them needing to consult additional material, typically through\n          common branding.\n        </p>\n        <p>\n          The <dfn data-lt=\"first parties\">first party</dfn> is a [=party=] with which the [=user=] intends to\n          interact. Merely hovering over, muting, pausing, or closing a given piece of content does\n          not constitute a [=user=]'s intent to interact with another party, nor does the simple\n          fact of loading a [=party=] embedded in the one with which the user intends to\n          interact. In cases of clear and conspicuous joint branding, there can be multiple [=first\n          parties=]. The [=first party=] is necessarily a [=data controller=] of the data processing\n          that takes places as a consequence of a [=user=] interacting with it.\n        </p>\n        <p>\n          A <dfn data-lt=\"third parties\">third party</dfn> is any [=party=] other than the [=user=],\n          the [=first party=], or a [=service provider=] acting on behalf of either the [=user=] or\n          the [=first party=].\n        </p>\n        <p>\n          A <dfn>service provider</dfn> or <dfn>data processor</dfn> is considered to be the same\n          [=party=] as the entity contracting it to perform the relevant [=processing=] if it:\n        </p>\n        <ul>\n          <li>\n            is processing the data on behalf of that [=party=];\n          </li>\n          <li>\n            ensures that the data is only retained, accessed, and used as directed by that [=party=]\n            and solely for the list of explicitly-specified [=purposes=] detailed by the directing\n            [=party=] or [=data controller=];\n          </li>\n          <li>\n            may determine implementation details of the data processing in question but does not\n            determine the [=purpose=] for which the data is being [=processed=] nor the overarching\n            [=means=] through which the [=purpose=] is carried out;\n          </li>\n          <li>\n            has no independent right to use the data other than in a [=permanently de-identified=]\n            form (e.g., for monitoring service integrity, load balancing, capacity planning, or\n            billing); and,\n          </li>\n          <li>\n            has a contract in place with the [=party=] which is consistent with the above limitations.\n          </li>\n        </ul>\n        <p>\n          A <dfn>data controller</dfn> is a [=party=] that determines the [=means=] and [=purposes=]\n          of data processing. Any [=party=] that is not a [=service provider=] is a [=data controller=].\n        </p>\n        <p>\n          The <dfn>Vegas Rule</dfn> is a simple implementation of privacy in which \"<em>what happens\n          with the [=first party=] stays with the [=first party=]</em>.\" Put differently, it\n          describes a situation in which the [=first party=] is the only [=data controller=]. Note\n          that, while enforcing the [=Vegas Rule=] provides a rule of thumb describing a necessary\n          baseline for [=appropriate=] [=data processing=], it is not always sufficient to guarantee\n          [=appropriate=] [=processing=] since the [=first party=] can [=process=] data\n          [=inappropriately=].\n        </p>\n      </section>\n      <section>\n        <h2>Acting on Data</h2>\n        <p>\n          A [=party=] <dfn data-lt=\"process|processing|processed|data processing\">processes</dfn> data if it\n          carries out operations on [=personal data=], whether or not by automated means, such as\n          collection, recording, organisation, structuring, storage, adaptation or alteration,\n          retrieval, consultation, use, disclosure by transmission, [=sharing=], dissemination or\n          otherwise making available, [=selling=], alignment or combination, restriction, erasure or\n          destruction.\n        </p>\n        <p>\n          A [=party=] <dfn data-lt=\"share|sharing\">shares</dfn> data if it provides it to any other\n          [=party=]. Note that, under this definition, a [=party=] that provides data to its own\n          [=service providers=] is not [=sharing=] it.\n        </p>\n        <p>\n          A [=party=] <dfn data-lt=\"sell|selling\">sells</dfn> data when it [=shares=] it in exchange\n          for consideration, monetary or otherwise.\n        </p>\n      </section>\n      <section>\n        <h2>Contexts and Privacy</h2>\n        <p>\n          The <dfn>purpose</dfn> of a given [=processing=] of data is an anticipated, intended, or\n          planned outcome of this [=processing=] which is achieved or aimed for within a given\n          [=context=]. A [=purpose=], when described, should be specific enough to be actionable by\n          someone familiar with the relevant [=context=] (ie. they could independently determine\n          [=means=] that reasonably correspond to an implementation of the [=purpose=]).\n        </p>\n        <!--\n          XXX given that contexts are defined from *user* purpose we might wish to have purpose in\n          general and processing purpose for the above.\n        -->\n        <p>\n          The <dfn>means</dfn> are the general method of [=data processing=] through which a given\n          [=purpose=] is implemented, in a given [=context=], considered at a relatively abstract\n          level and not necessarily all the way down to implementation details. Example:\n          <em>the user will have their preferences restored (purpose) by looking up their identifier\n          in a preferences store (means)</em>.\n        </p>\n        <p>\n          A <dfn>context</dfn> is a physical or digital environment that a [=person=] interacts with\n          for a purpose of their own (that they typically share with other [=person=] who interact\n          with the same environment).\n        </p>\n        <p>\n          A [=context=] can be further described through:\n        </p>\n        <ul>\n          <li>\n            Its <dfn>actors</dfn>, which comprise the <dfn>subject</dfn> (a [=person=]) as well as\n            the <dfn>sender</dfn> and <dfn>recipient</dfn> of the data (which are [=parties=]).\n          </li>\n          <li>\n            Its <dfn>attributes</dfn>, which are the types of [=personal data=] being [=processed=]\n            in the [=context=].\n          </li>\n          <li>\n            Its <dfn>transmission principles</dfn>, which are the constraints (typically technical\n            or legal) being placed upon the [=data processing=].\n          </li>\n        </ul>\n        <p>\n          A [=context=] carries <dfn data-lt=\"norm\">context-relative informational norms</dfn> that determine\n          whether a given [=data processing=] is <dfn data-lt=\"appropriately\">appropriate</dfn>\n          (if the norms are adhered to) or <dfn data-lt=\"inappropriately\">inappropriate</dfn>\n          (when the norms are violated). A norm violation can be for instance the exfiltration of\n          [=personal data=] from a context or the lack of respect for [=transmission principles=].\n          When [=norms=] are respected in a given [=context=], we can say that <dfn>contextual\n          integrity</dfn> is maintained; otherwise that it is violated ([[?PRIVACY-IN-CONTEXT]],\n          [[?PRIVACY-AS-CI]]).\n        </p>\n        <p>\n          We define <dfn>privacy</dfn> as a right to [=appropriate=] [=data processing=]. A\n          <dfn>privacy violation</dfn> is, correspondingly, [=inappropriate=] [=data processing=]\n          [[?PRIVACY-IN-CONTEXT]].\n        </p>\n        <p>\n          Note that a [=first party=] can be comprised of multiple [=contexts=] if it is large\n          enough that [=people=] would interact with it for more than one [=purpose=]. [=Sharing=]\n          [=personal data=] across [=contexts=] is, in the overwhelming majority of cases,\n          [=inappropriate=].\n        </p>\n        <p>\n            Your cute little pup uses <em>Poodle Naps</em> to find comfortable places to snooze,\n            and <em>Poodle Fetch</em> to locate the best sticks. Napping and fetching are different\n            [=contexts=] with different norms, and sharing data between these contexts is a\n            [=privacy violation=] despite the shared ownership of <em>Naps</em> and <em>Fetch</em>\n            by the <em>Poodle</em> conglomerate.\n          </p>\n        <p>\n          Colloquially, <dfn>tracking</dfn> is understood to be any kind of [=inappropriate=] data\n          collection.\n        </p>\n        <p>\n          Additionally, <dfn data-lt=\"privacy-labor\">privacy labour</dfn> is the practice of having\n          a [=person=] carry out the work of ensuring [=data processing=] of which they are the\n          subject is [=appropriate=], instead of having the [=parties=] be responsible for that\n          work as is more respectable.\n        </p>\n      </section>\n      <section>\n        <h2>User Agents</h2>\n        <p>\n          The <dfn>user agent</dfn> acts as an intermediary between a [=user=] and the web. The\n          [=user agent=] is <em>not</em> a [=context=] in that it is expected to coincide with the\n          [=subject=] and operate exclusively in the [=subject=]'s interest. It is <em>not</em> the\n          [=first party=]. The [=user agent=] serves the [=user=] in a\n          <dfn data-lt=\"fiduciary relationship\">relationship of fiduciary agency</dfn>: it always puts\n          the [=user=]'s interest first, up to and including, on occasion, protecting the [=user=]\n          from themselves by preventing them from carrying out a harmful decision, or at the very\n          least by speed-bumping it [[?FIDUCIARY-UA]]. For example, the [=user agent=] will make it\n          difficult for the [=user=] to connect to a site the authenticity of which is hard to\n          ascertain, will double-check that the user really intends to expose a sensitive device\n          capability, or will prevent the [=user=] from consenting to permanent monitoring of their\n          behaviour. Its <dfn>fiduciary duties</dfn> include [[?TAKING-TRUST-SERIOUSLY]]:\n        </p>\n        <dl>\n          <dt><dfn>Duty of Protection</dfn></dt>\n          <dd>\n            Protection requires [=user agents=] to affirmatively protect a [=user=]'s data, beyond\n            simple security measures. It is insufficient simply to encrypt at rest and in transit,\n            but one must further limit retention, ensure that the strictly necessary data is\n            collected, or require guarantees from those it is shared to.\n          </dd>\n          <dt><dfn>Duty of Discretion</dfn></dt>\n          <dd>\n            Discretion requires the [=user agent=] to make best efforts to enforce\n            [=context-relative informational norms=] by placing contextual limits on the flow and\n            [=processing=] of [=personal data=]. Discretion is not confidentiality and may place\n            limits on nondisclosure: trust can be preserved even when the [=user agent=] shares the\n            [=personal data=], so long as it is done in an [=appropriately=] discreet manner.\n          </dd>\n          <dt><dfn>Duty of Honesty</dfn></dt>\n          <dd>\n            Honesty requires that the [=user agent=] make sure that the [=user=] is proactively\n            provided with information that is relevant to them and that will enhance the [=user=]'s\n            autonomy, to the extent possible in a manner that they will comprehend and at the right\n            moment, which is almost never when the [=user=] is trying to do something else such as\n            read a page or activate a feature. The duty of honesty goes well beyond that of\n            transparency that dominates legacy privacy regimes. Unlike with transparency, honesty\n            cannot get away with hiding relevant information in complex out-of-band legal notices\n            no more than it can rely on overly cursory information provided in a consent dialog.\n          </dd>\n          <dt><dfn>Duty of Loyalty</dfn></dt>\n          <dd>\n            Because of the special [=fiduciary relationship=] that obtains between [=user=] and\n            [=user agent=], the latter is held to be loyal to the former in all situations, up to and\n            including in preference to the [=user agent=]'s implementer. When a [=user agent=] carries\n            out [=processing=] that is not directly in the [=user=]'s interest but rather benefits\n            another entity such as its implementer, including by piggybacking on [=processing=] that\n            may be in the user's interest, that behaviour is known as <dfn>self-dealing</dfn>.\n            [=Self-dealing=] is always [=inappropriate=]. Loyalty is the avoidance of [=self-dealing=].\n          </dd>\n        </dl>\n        <p>\n          These duties ensure the [=user agent=] will <em>care</em> for the user. It is important to\n          note that there is a subtle difference between care and <dfn>data paternalism</dfn> which\n          is that the latter purports to help in part by removing agency (\"<em>don't worry about it,\n          so long as your data is with us it's safe, you don't need to know what we do with it, it's\n          all good because we're good people</em>\") whereas care aims to support people by enhancing\n          their agency and sovereignty.\n        </p>\n      </section>\n    </section>\n    <section>\n      <h2>Privacy Threat Model Building Blocks</h2>\n      <section>\n        <h2>Threats Are Context-Relative</h2>\n        <p>\n          Privacy threat models for the Web exist ([[?TRACKING-PREVENTION-POLICY]], [[?ANTI-TRACKING-POLICY]],\n          [[?PRIVACY-THREAT]]). While good within the scope of what they intend to do, they tend to be\n          very specific to cross-context tracking as seen by the browser, which is but one problem.\n          Privacy threat modelling could benefit from having more of a toolbox to reuse in different\n          situations.\n        </p>\n        <p>\n          This document provides building blocks for the creation of <dfn>privacy threat models</dfn> on\n          the Web. Note that privacy threat models have an important difference with security threat\n          models in that <em>all</em> [=parties=] are potential threats, even when they are not rogue.\n          In fact the [=first party=] is typically considered to be the primary threat, the one\n          against which the brunt of mitigating techniques are to be leveraged.\n        </p>\n        <p>\n          The most important building blocks for a privacy threat model are those that define a\n          [=context=]: [=actors=], [=attributes=], and [=transmission principles=], as well as the\n          [=context-relative informational norms=]. Collectively, these define the expectations that\n          obtain in a given [=context=]. Based on these expectations, it becomes possible to ask\n          questions about the ways in which they could fail and how to prevent. What happens if the\n          [=subject=] is a [=vulnerable person=]? How well does the context fare if expectations of\n          consent are undermined by manipulative techniques ([[?DIGITAL-MARKET-MANIPULATION]],\n          [[?PRIVACY-BEHAVIOR]])? If the [=transmission principles=] are based on sharing data with\n          parties while telling them that they cannot use it, what confidence do we have that they\n          are following the rules?\n        </p>\n        <p>\n          When assessing privacy threats, it is not necessary to establish harms since breaking the\n          [=context=]'s [=norms=] is [=inappropriate=] in all cases. However, consideration of harms can\n          inform which issues to prioritise. Where individual <dfn>privacy harms</dfn> are to be\n          defined, the definitive source is <em>Privacy Harms</em> [[?PRIVACY-HARMS]].\n        </p>\n      </section>\n      <section>\n        <h2>Identity on the Web</h2>\n        <p>\n          A [=person=]'s <dfn>identity</dfn> is the set of characteristics that define them. In\n          computer systems, [=identity=] is typically attached to a means of denotation that makes\n          it easier for an automated system to recognise the user, an <dfn>identifier</dfn> of\n          some type.\n        </p>\n        <p>\n          A [=person=]'s characteristics, and therefore [=identity=], is entirely\n          [=context=]-dependent. Recognising a [=person=] in distinct [=contexts=] can at times be\n          [=appropriate=] but this relies on an understanding of applicable [=norms=] and will\n          likely require compartmentalisation. (For example, if you meet your therapist at a\n          cocktail party, you expect them to have rather different discussion topics with you than\n          they usually would, and possibly even to pretend they do not know you.) As a result,\n          automating the recognition of a [=person=]'s identity across different [=contexts=] is\n          [=inappropriate=]. This is particularly true for [=vulnerable=] people as recognising them\n          in different [=contexts=] may force their vulnerability into the open.\n        </p>\n        <p>\n          [=Identity=] being by nature fragmented, [=user agents=] must work in support of [=users=]\n          having different identities in different [=context=] with respect to <em>all</em> [=parties=]\n          (including the user agent vendor) and to prevent their recognition through other means,\n          where possible.\n        </p>\n        <p>\n          A keystone principle of the Web is trust [[RFC8890]]. An important part of trust is to\n          ensure that [=data=] collected for a [=purpose=] that matches a clearly delineated [=user=]\n          feature should not then be used for additional secondary [=purposes=]. Email is often used\n          for login and communication [=purposes=], including essential transactional interactions.\n          Using emails for cross-context recognition [=purposes=] is therefore not only\n          [=inappropriate=] but also threatens key messaging infrastructure. Future iterations of\n          the Web platform should ensure that users can log into sites and receive communication\n          without relying on email at all.\n        </p>\n      </section>\n    </section>\n    <section>\n      <h2>User Control and Autonomy</h2>\n      <p>\n        A [=person=]'s <dfn data-lt=\"autonomous\">autonomy</dfn> is their ability to make decisions of their own volition,\n        without undue influence from other parties. People have limited intellectual resources and\n        time with which to weigh decisions, and by necessity rely on shortcuts when making\n        decisions. This makes their privacy preferences malleable [[?PRIVACY-BEHAVIOR]] and susceptible to\n        manipulation [[?DIGITAL-MARKET-MANIPULATION]]. A [=person=]'s [=autonomy=] is enhanced by a\n        system or device when that system offers a shortcut that aligns more with what that [=person=] would\n        have decided given arbitrary amounts of time and relatively unfettered intellectual ability;\n        and [=autonomy=] is decreased when a similar shortcut goes against decisions made under\n        ideal conditions.\n      </p>\n      <p>\n        Affordances and interactions that decrease [=autonomy=] are known as <dfn>dark patterns</dfn>.\n        A [=dark pattern=] does not have to be intentional, the deceptive effect is sufficient to\n        define them [[?DARK-PATTERNS]], [[?DARK-PATTERN-DARK]].\n      </p>\n      <p>\n        Because we are all subject to motivated reasoning, the design of defaults and affordances\n        that may impact [=user=] [=autonomy=] should be the subject of independent scrutiny.\n        Implementers are enjoined to be particularly cautious to avoid slipping into\n        [=data paternalism=].\n      </p>\n      <p>\n        Given the sheer volume of potential [=data=]-related decisions in today's data economy,\n        complete informational self-determination is impossible. This fact, however, should not be\n        confused with the contention that privacy is dead. Careful design of our technological\n        infrastructure can ensure that [=users=]' [=autonomy=] as pertaining to their own [=data=]\n        is enhanced through [=appropriate=] defaults and choice architectures.\n      </p>\n      <p>\n        In the 1970s, the <dfn>Fair Information Practices</dfn> or <dfn>FIPs</dfn> were elaborated\n        in support of individual [=autonomy=] in the face of growing concerns with databases. The\n        [=FIPs=] assume that there is sufficiently little [=data processing=] taking place that any\n        [=person=] will be able to carry out sufficient diligence to enable [=autonomy=] in their\n        decision-making. Since they entirely offload the [=privacy labour=]\n        to [=users=] and assume perfect, unfettered [=autonomy=], the [=FIPs=] do not forbid specific\n        types of [=data processing=] but only place them under different procedural requirements.\n        Such an approach is [=appropriate=] for [=parties=] that are processing data in the 1970s.\n      </p>\n      <p>\n        One notable issue with procedural approaches to privacy is that they tend to have the same\n        requirements in situations where the [=user=] finds themselves in a significant asymmetry of\n        power with a [=party=] ‚Äî for instance the [=user=] of an essential service provided by a\n        monopolistic platform ‚Äî and those where [=user=] and [=parties=] are very much on equal\n        footing, or even where the [=user=] may have greater power, as is the case with small\n        businesses operating in a competitive environment. It further does not consider cases in\n        which one [=party=] may coerce other [=parties=] into facilitating its [=inappropriate=]\n        practices, as is often the case with dominant players in advertising [[?CONSENT-LACKEYS]] or\n        in content aggregation [[?CAT]].\n      </p>\n      <p>\n        Reference to the [=FIPs=] survives to this day. They are often referenced as <em>transparency\n        and choice</em>, which, in today's digital environment, is often a strong indication that\n        [=inappropriate=] [=processing=] is being described.\n      </p>\n      <figure>\n        <img src=\"agnes-tc.jpg\" alt=\"Agnes from Wandavision winking 'Transparency and choice'\">\n        <figcaption>\n          A method of privacy regulation which promises honesty and autonomy but delivers neither.\n          [[?CONFIDING]].\n        </figcaption>\n      </figure>\n      <section>\n        <h2>Opt-in, Consent, Opt-out, Global Controls</h2>\n        <p>\n          Different procedural mechanisms exist to enable [=people=] to control the [=processing=]\n          done to their [=data=]. Mechanisms that increase the number of [=purposes=] for which\n          their [=data=] is being [=processed=] are referred to as <dfn data-lt=\"opt in\">opt-in</dfn> or\n          <dfn>consent</dfn>; mechanisms that decrease this number of [=purposes=] are known as\n          <dfn data-lt=\"opt out\">opt-out</dfn>.\n        </p>\n        <p>\n          When deployed thoughtfully, these mechanisms can enhance [=people=]'s [=autonomy=]. Often,\n          however, they are used as a way to avoid putting in the difficult work of deciding which\n          types of [=processing=] are [=appropriate=] and which are not, offloading [=privacy labour=]\n          to the [=user=].\n        </p>\n        <p>\n          Privacy regulatory regimes are often anchored at extremes: either they default to allowing\n          only very few strictly essential [=purposes=] such that many [=parties=] will have to\n          resort to [=consent=], habituating [=people=] to ignore legal prompts and incentivising\n          [=dark patterns=], or, conversely, they default to forbidding only very few, particularly\n          egregious [=purposes=], such that [=people=] will have to perform the [=privacy labour=] to\n          [=opt out=] in every [=context=] in order to produce [=appropriate=] [=processing=].\n        </p>\n        <p>\n          An approach that is more aligned with the expectation that the Web should provide a\n          trustworthy, [=person=]-centric environment is to establish a regime consisting of\n          three privacy tiers:\n        </p>\n        <dl>\n          <dt><dfn>Default Privacy Tier</dfn></dt>\n          <dd>\n            This is the set of [=purposes=] that are deemed [=appropriate=] in a given [=context=]\n            and the [=processing=] that a [=person=] can expect without having triggered any\n            mechanisms to change their preferences. The exact details for this tier require clear\n            definition, including aspects such as data retention, but [=data=] would have to be\n            systematically siloed by [=context=] (<em>not</em> by [=party=], making it stricter than\n            the [=Vegas Rule=] and more in line with respecting [=privacy=]). This default tier\n            should also be defined differently for certain kinds of [=contexts=]. The legitimate\n            [=processing=] that can take place in this tier derives its legitimacy from matching the\n            expectations and interests of both the [=user=] and the [=first party=] in their\n            relationship, as guided by the applicable [=norms=]. This tier is more [=appropriate=] the more the\n            [=first party=] acts in accordance with [=fiduciary duties=].\n          </dd>\n          <dt><dfn>Opt-out Privacy Tier</dfn></dt>\n          <dd>\n            [=People=] who, either from personal preference or because they are [=vulnerable=],\n            require greater obscurity in some or even most [=contexts=], would be able to transition\n            to this tier through an [=opt-out=] mechanism. This tier would only permit strictly\n            necessary [=purposes=]. This tier should be [=appropriate=] for [=vulnerable=]\n            [=people=].\n          </dd>\n          <dt><dfn>Opt-in Privacy Tier</dfn></dt>\n          <dd>\n            In rare and highly specific cases, [=people=] should be able to [=consent=] to more\n            sensitive [=purposes=], such as having their [=identity=] recognised across contexts or\n            their reading history shared with a company. The burden of proof on ensuring that informed [=consent=] has\n            been obtained needs in this case to be very high (much higher than what prevails for\n            instance in [[?GDPR]] jurisdictions as currently practices). [=Consent=] is comparable to the general problem\n            of permissions on the Web platform. In the same way that it should be clear when a given\n            device capability is in use (eg. you are providing geolocation or camera access), sharing\n            data under this tier should be set up in such a way that it requires deliberate, specific\n            action from the [=user=] (eg. triggering a form control) and if that [=consent=] is\n            persistent, there should be an indicator that data is being transmitted shown at all\n            times, in such a way that the user can easily switch it off. In general, providing\n            [=consent=] should be rare, difficult, highly intentional, and temporary.\n          </dd>\n        </dl>\n        <p>\n          When an [=opt-out=] mechanism exists, it should preferably be complemented by a\n          <dfn>global opt-out</dfn> mechanism. The function of a [=global opt-out=] mechanism is to\n          rectify the <dfn>automation asymmetry</dfn> whereby service providers can automate\n          [=data processing=] but [=people=] have to take manual action. A good example of a\n          [=global opt-out=] mechanism is the <em>Global Privacy Control</em> [[?GPC]].\n        </p>\n        <p>\n          Conceptually, a [=global opt-out=] mechanism is an automaton operating as part of the\n          [=user agent=], which is to say that it is equivalent to a robot that would carry out the\n          [=user=]'s bidding by pressing an [=opt-out=] button with every interaction that the\n          [=user=] has with a site, or more generally conveys an expression of the [=user=]'s\n          rights in a relevant jurisdiction. (For instance, under [[?GDPR]], the [=user=] may be\n          conveying objections to [=processing=] based on legitimate interest or the withdrawal of\n          [=consent=] to specific [=purposes=].) It should be noted that, since a [=global opt-out=]\n          signal is reaffirmed automatically with every [=user=] interaction, it will take precedence\n          in terms of specificity over any manner of blanket [=consent=] that a site may obtain,\n          unless that [=consent=] is directly attached to an interaction (eg. terms specified on a\n          form upon submission).\n        </p>\n      </section>\n    </section>\n    <section id=\"collective\">\n      <h2>Collective Issues in Privacy</h2>\n      <p>\n        When designing Web technology, we naturally pay attention to potential impacts on the [=person=]\n        using the Web through their [=user agent=]. In addition to potential individual harms we also\n        pay heed to collective effects that emerge from the accumulation of individual actions as\n        influenced by entities and the structure of technology.\n      </p>\n      <p>\n        Note that in evaluating impact, we deliberately ignore what implementers or specifiers may\n        have <em>intended</em> and only focus on outcomes. This framing is known as <dfn>POSIWID</dfn>, or\n        \"<em>the Purpose Of a System Is What It Does</em>\".\n      </p>\n      <p>\n        The collective problem of privacy is known as <dfn>legibility</dfn>. [=Legibility=] concerns\n        population-level [=data processing=] that may impact populations or individuals, including in\n        ways that [=people=] could not control even under the optimistic assumptions of the [=FIPs=].\n        For example, based on population-level analysis, a company may know that <var>site.example</var>\n        is predominantly visited by [=people=] of a given race or gender, and decide not to run its\n        job ads there. Visitors to that page are implicitly having their [=data=] processed in\n        [=inappropriate=] ways, with no way to discover the discrimination or seek relief\n        [[?DEMOCRATIC-DATA]].\n      </p>\n      <p>\n        What we consider is therefore not just the relation between the people who expose themselves\n        and the entities that invite that disclosure [[?RELATIONAL-TURN]], but also between the people\n        who expose themselves and those who do not but may find themselves recognised as such indirectly\n        anyway. One key understanding here is that such relations may persists even when data is\n        [=permanently de-identified=].\n      </p>\n      <p>\n        [=Legibility=] practices can be <dfn data-lt=\"legitimacy\">legitimate</dfn> or\n        <dfn data-lt=\"illegitimacy\">illegitimate</dfn> depending\n        on the [=context=] and on the [=norms=] that apply in that [=context=]. Typically, a\n        [=legibility=] practice may be [=legitimate=] if it is managed through an acceptable process\n        of collective [=governance=]. For example, it is often considered [=legitimate=] for a\n        government, under the control of its citizens, to maintain a database of license plates for\n        the [=purpose=] of enforcing the rules of the road. It would be [=illegitimate=] to observe\n        the same license plates near places of worship to build a database of religious identity.\n      </p>\n      <p>\n        [=Legibility=] is often used to order information about the world. This can notably create\n        problems of [=reflexivity=] and of [=autonomy=].\n      </p>\n      <p>\n        Problems of <dfn>reflexivity</dfn> occur when the ordering of information about the world\n        used to produce [=legibility=] finds itself changing the way in which the world operates.\n        This can produce self-reinforcing loops that can have deleterious effects both individual and\n        collective [[?SEEING-LIKE-A-STATE]].\n      </p>\n      <p>\n        Issues of [=autonomy=] occur depending on the manner in which [=legibility=] is implemented.\n        When [=legibility=] is used to order the world following rules set by the [=user=] or\n        following methods subject to public scrutiny and [=governance=] models with strong checks and\n        balances (such as a newspaper's editorial decisions), then it will enhance [=user=] [=autonomy=]\n        and tend to be [=legitimate=]. When it is done in the [=user=]'s stead and without [=governance=],\n        it decreases [=user=] [=autonomy=] and tends to be [=illegitimate=].\n      </p>\n      <!--\n      <p>\n        From an economics standpoint, it is important to note that the broad sharing of data can lead\n        to anticompetitive outcomes, notably due to network effects stemming from processing data across\n        multiple [=contexts=] [[?BIG-DATA-COMPETITION]]. Restricting flows of [=data=] between\n        different [=contexts=] (and not just [=parties=]) is therefore likely to improve competition.\n      </p>\n      -->\n      <p>\n        <dfn data-lt=\"governance\">Data governance</dfn> refers to the rules and processes for how [=data=] is\n        [=processed=] in any given [=context=]. How data is <em>governed</em> describes who has\n        power to make decisions over [=data=] and how [[?DATA-FUTURES-GLOSSARY]].\n      </p>\n      <p>\n        In general, collective issues in [=data=] require collective solutions. The proper goal of\n        [=data governance=] at the standards-setting level is the development of structural controls\n        in [=user agents=] and the provision of institutions that can handle population-level problems\n        in [=data=]. [=Governance=] will often struggle to achieve its goals if it works primarily by\n        increasing <em>individual</em> control over [=data=]. A collective approach reduces the cost of\n        control.\n      </p>\n      <p>\n        Collecting data at large scales can have significant pro-social outcomes. Problems tend to\n        emerge when entities take part in dual-use collection in which [=data=] is [=processed=]\n        for collective benefit but also for [=self-dealing=] [=purposes=] that may degrade welfare.\n        The [=self-dealing=] [=purposes=] will be justified as bankrolling the pro-social outcomes,\n        which, absent collective oversight, cannot be considered to support claims to [=legitimacy=]\n        for such [=legibility=]. It is vital for standards-setting organisations to establish not\n        just purely technical devices but techno-social systems that can govern data at scale.\n      </p>\n      <!--\n        Imagine a case in which collecting browser data can be leveraged to improve both search\n        services (which have a business model) and user security (which generally doesn't). This\n        data could be collected through a data trust under stakeholder governance, search\n        companies paying for access with strong checks and balances, and privacy guarantees, and\n        the funds would be used to support enhanced security for all.\n      -->\n    </section>\n  \n\n</div>","textContent":"\n    \n      \n        Privacy has been an essentially contested concept [[?PRIVACY-CONTESTED]]. Its debated meaning\n        render its support problematic in the context of a standards-setting process grounded in\n        consensus [[?W3C-PROCESS]], in seeking out technical solutions grounded on shared\n        requirements, and in addressing the needs of a worldwide constituency. This document\n        provides definitions for privacy and related concepts that are suitable for a global\n        audience, that can provide building blocks for privacy threat modelling, and can guide the\n        development of the Web as a trustworthy platform. In the spirit of building a much-needed\n        bridge between technology and policy, this document is written under the expectation that\n        it can apply to both.\n      \n    \n    \n    \n    \n      Introduction\n      \n        Privacy is essential to trust, and trust is a cornerstone value of the Web [[?RFC8890]].\n        In much of everyday life, people have little difficulty assessing whether a given flow of\n        information constitutes a violation of privacy or not [[?NYT-PRIVACY]]. However, in the digital\n        space, users struggle to understand how their data may flow between contexts and how such\n        flows may affect them, not just immediately but at a much later time and in completely\n        different situations. Some actors then seize upon this confusion in order to extract and\n        exploit [=personal data=] at unprecedented scale.\n      \n      \n        The goal of this document is to define all the terms that may prove useful in developing\n        technology and policy that relate to privacy and [=personal data=]. It additionally provides\n        a toolbox to support the common need that is privacy threat modelling, the frequent debate\n        over consent, and the under-developed set of issues in privacy that are of a\n        collective, relational nature.\n      \n      \n        [=Personal data=] is a regulated object, and this document naturally recognises the\n        jurisdictional primacy of existing data protection regimes. However, the global nature of\n        the Web means that, as we develop technology, we benefit from shared concepts that guide the\n        evolution of the Web as a system built for its users [[?RFC8890]]. A clear and well-defined\n        view of privacy on the Web, grounded in an up-to-date understanding of the state of the art,\n        can hopefully help the Web's constituencies thrive across jurisdictional disparity, with the\n        shared understanding that the law is a floor, not a ceiling.\n      \n    \n    \n      Definitions\n      \n        This section provides a number of elementary building blocks from which to establish a\n        shared understanding of privacy. Some of the definitions below build atop the work in\n        Tracking Preference Expression (DNT) [[tracking-dnt]].\n      \n      \n        People & Data\n        \n          A user (also person or data subject) is any natural\n          person.\n        \n        \n          We define personal data as any information relating to a [=person=] such\n          that:\n        \n        \n          \n            this [=person=] is identified, directly or indirectly, by reference to an\n            identifier such as a name, email address, an arbitrary identifier or identification\n            number, an online identifier such as an IP address or any identifier attached to a\n            device this [=person=] may be using, phone number, location data, or factors specific\n            to the physical, physiological, genetic, mental, economic, cultural, or social identity\n            of that [=person=], as well as identifiers derived from such data, for instance\n            through hashing; or\n          \n          \n            this [=person=] could reasonably be reidentified from a conjunction of this data with\n            other data; or\n          \n          \n            the data pertains to a group of people such that a [=person=] may find themselves to\n            be the subject of a treatment related to this group, even if the entity carrying out the\n            treatment has no way to identify that [=person=].\n          \n        \n        \n          Data is permanently de-identified when there exists a high level of confidence\n          that no human subject of the data can be identified, directly or indirectly (e.g., via\n          association with an identifier, user agent, or device), by that data alone or in\n          combination with other retained or available information, including as being part of a\n          group. Note that further considerations relating to groups are covered in the\n          Collective Issues in Privacy section.\n        \n        \n          Data is pseudonymous when:\n        \n        \n          \n            the identifiers used in the data are under the direct and exclusive control of the\n            [=first party=]; and\n          \n          \n            when these identifiers are shared with a [=third party=], they are made unique to that\n            [=third party=] such that if they are shared with more than one [=third party=] these\n            cannot then match them up with one another; and\n          \n          \n            there is a strong level of confidence that no [=third party=] can match them to any data\n            other than that obtained through interactions with the [=first party=]; and\n          \n          \n            any [=third party=] receiving such identifiers is barred (eg. based on legal terms) from\n            sharing them or the related data further; and\n          \n          \n            technical measures exist to prevent re-identification or the joining of different data\n            sets involving these identifiers, notably against timing or k-anonymity attacks; and\n          \n          \n            there exist contractual terms between the [=first party=] and [=third party=] describing\n            the limited [=purpose=] for which the data is being shared.\n          \n        \n        \n          This can ensure that [=pseudonymous data=] is used in a manner that provides a minimum\n          degree of governance such that technical and procedural means to guarantee the\n          maintenance of pseudonymity are preserved. Note that [=pseudonymity=], on its own, is not\n          sufficient to render [=data processing=] [=appropriate=].\n        \n        \n          A vulnerable person is a [=person=] who, at least in the [=context=] of\n          the [=processing=] being discussed, are unable to exercise sufficient\n          self-determination for any consent they may provide to be receivable. This includes for\n          example children, employees with respect to their employers, people in some situations of\n          intellectual or psychological impairment, or refugees.\n        \n      \n      \n        The Parties\n        \n          A party is a [=person=], a legal entity, or a set of legal entities that\n          share common owners, common controllers, and a group identity that is readily evident to\n          the [=user=] without them needing to consult additional material, typically through\n          common branding.\n        \n        \n          The first party is a [=party=] with which the [=user=] intends to\n          interact. Merely hovering over, muting, pausing, or closing a given piece of content does\n          not constitute a [=user=]'s intent to interact with another party, nor does the simple\n          fact of loading a [=party=] embedded in the one with which the user intends to\n          interact. In cases of clear and conspicuous joint branding, there can be multiple [=first\n          parties=]. The [=first party=] is necessarily a [=data controller=] of the data processing\n          that takes places as a consequence of a [=user=] interacting with it.\n        \n        \n          A third party is any [=party=] other than the [=user=],\n          the [=first party=], or a [=service provider=] acting on behalf of either the [=user=] or\n          the [=first party=].\n        \n        \n          A service provider or data processor is considered to be the same\n          [=party=] as the entity contracting it to perform the relevant [=processing=] if it:\n        \n        \n          \n            is processing the data on behalf of that [=party=];\n          \n          \n            ensures that the data is only retained, accessed, and used as directed by that [=party=]\n            and solely for the list of explicitly-specified [=purposes=] detailed by the directing\n            [=party=] or [=data controller=];\n          \n          \n            may determine implementation details of the data processing in question but does not\n            determine the [=purpose=] for which the data is being [=processed=] nor the overarching\n            [=means=] through which the [=purpose=] is carried out;\n          \n          \n            has no independent right to use the data other than in a [=permanently de-identified=]\n            form (e.g., for monitoring service integrity, load balancing, capacity planning, or\n            billing); and,\n          \n          \n            has a contract in place with the [=party=] which is consistent with the above limitations.\n          \n        \n        \n          A data controller is a [=party=] that determines the [=means=] and [=purposes=]\n          of data processing. Any [=party=] that is not a [=service provider=] is a [=data controller=].\n        \n        \n          The Vegas Rule is a simple implementation of privacy in which \"what happens\n          with the [=first party=] stays with the [=first party=].\" Put differently, it\n          describes a situation in which the [=first party=] is the only [=data controller=]. Note\n          that, while enforcing the [=Vegas Rule=] provides a rule of thumb describing a necessary\n          baseline for [=appropriate=] [=data processing=], it is not always sufficient to guarantee\n          [=appropriate=] [=processing=] since the [=first party=] can [=process=] data\n          [=inappropriately=].\n        \n      \n      \n        Acting on Data\n        \n          A [=party=] processes data if it\n          carries out operations on [=personal data=], whether or not by automated means, such as\n          collection, recording, organisation, structuring, storage, adaptation or alteration,\n          retrieval, consultation, use, disclosure by transmission, [=sharing=], dissemination or\n          otherwise making available, [=selling=], alignment or combination, restriction, erasure or\n          destruction.\n        \n        \n          A [=party=] shares data if it provides it to any other\n          [=party=]. Note that, under this definition, a [=party=] that provides data to its own\n          [=service providers=] is not [=sharing=] it.\n        \n        \n          A [=party=] sells data when it [=shares=] it in exchange\n          for consideration, monetary or otherwise.\n        \n      \n      \n        Contexts and Privacy\n        \n          The purpose of a given [=processing=] of data is an anticipated, intended, or\n          planned outcome of this [=processing=] which is achieved or aimed for within a given\n          [=context=]. A [=purpose=], when described, should be specific enough to be actionable by\n          someone familiar with the relevant [=context=] (ie. they could independently determine\n          [=means=] that reasonably correspond to an implementation of the [=purpose=]).\n        \n        \n        \n          The means are the general method of [=data processing=] through which a given\n          [=purpose=] is implemented, in a given [=context=], considered at a relatively abstract\n          level and not necessarily all the way down to implementation details. Example:\n          the user will have their preferences restored (purpose) by looking up their identifier\n          in a preferences store (means).\n        \n        \n          A context is a physical or digital environment that a [=person=] interacts with\n          for a purpose of their own (that they typically share with other [=person=] who interact\n          with the same environment).\n        \n        \n          A [=context=] can be further described through:\n        \n        \n          \n            Its actors, which comprise the subject (a [=person=]) as well as\n            the sender and recipient of the data (which are [=parties=]).\n          \n          \n            Its attributes, which are the types of [=personal data=] being [=processed=]\n            in the [=context=].\n          \n          \n            Its transmission principles, which are the constraints (typically technical\n            or legal) being placed upon the [=data processing=].\n          \n        \n        \n          A [=context=] carries context-relative informational norms that determine\n          whether a given [=data processing=] is appropriate\n          (if the norms are adhered to) or inappropriate\n          (when the norms are violated). A norm violation can be for instance the exfiltration of\n          [=personal data=] from a context or the lack of respect for [=transmission principles=].\n          When [=norms=] are respected in a given [=context=], we can say that contextual\n          integrity is maintained; otherwise that it is violated ([[?PRIVACY-IN-CONTEXT]],\n          [[?PRIVACY-AS-CI]]).\n        \n        \n          We define privacy as a right to [=appropriate=] [=data processing=]. A\n          privacy violation is, correspondingly, [=inappropriate=] [=data processing=]\n          [[?PRIVACY-IN-CONTEXT]].\n        \n        \n          Note that a [=first party=] can be comprised of multiple [=contexts=] if it is large\n          enough that [=people=] would interact with it for more than one [=purpose=]. [=Sharing=]\n          [=personal data=] across [=contexts=] is, in the overwhelming majority of cases,\n          [=inappropriate=].\n        \n        \n            Your cute little pup uses Poodle Naps to find comfortable places to snooze,\n            and Poodle Fetch to locate the best sticks. Napping and fetching are different\n            [=contexts=] with different norms, and sharing data between these contexts is a\n            [=privacy violation=] despite the shared ownership of Naps and Fetch\n            by the Poodle conglomerate.\n          \n        \n          Colloquially, tracking is understood to be any kind of [=inappropriate=] data\n          collection.\n        \n        \n          Additionally, privacy labour is the practice of having\n          a [=person=] carry out the work of ensuring [=data processing=] of which they are the\n          subject is [=appropriate=], instead of having the [=parties=] be responsible for that\n          work as is more respectable.\n        \n      \n      \n        User Agents\n        \n          The user agent acts as an intermediary between a [=user=] and the web. The\n          [=user agent=] is not a [=context=] in that it is expected to coincide with the\n          [=subject=] and operate exclusively in the [=subject=]'s interest. It is not the\n          [=first party=]. The [=user agent=] serves the [=user=] in a\n          relationship of fiduciary agency: it always puts\n          the [=user=]'s interest first, up to and including, on occasion, protecting the [=user=]\n          from themselves by preventing them from carrying out a harmful decision, or at the very\n          least by speed-bumping it [[?FIDUCIARY-UA]]. For example, the [=user agent=] will make it\n          difficult for the [=user=] to connect to a site the authenticity of which is hard to\n          ascertain, will double-check that the user really intends to expose a sensitive device\n          capability, or will prevent the [=user=] from consenting to permanent monitoring of their\n          behaviour. Its fiduciary duties include [[?TAKING-TRUST-SERIOUSLY]]:\n        \n        \n          Duty of Protection\n          \n            Protection requires [=user agents=] to affirmatively protect a [=user=]'s data, beyond\n            simple security measures. It is insufficient simply to encrypt at rest and in transit,\n            but one must further limit retention, ensure that the strictly necessary data is\n            collected, or require guarantees from those it is shared to.\n          \n          Duty of Discretion\n          \n            Discretion requires the [=user agent=] to make best efforts to enforce\n            [=context-relative informational norms=] by placing contextual limits on the flow and\n            [=processing=] of [=personal data=]. Discretion is not confidentiality and may place\n            limits on nondisclosure: trust can be preserved even when the [=user agent=] shares the\n            [=personal data=], so long as it is done in an [=appropriately=] discreet manner.\n          \n          Duty of Honesty\n          \n            Honesty requires that the [=user agent=] make sure that the [=user=] is proactively\n            provided with information that is relevant to them and that will enhance the [=user=]'s\n            autonomy, to the extent possible in a manner that they will comprehend and at the right\n            moment, which is almost never when the [=user=] is trying to do something else such as\n            read a page or activate a feature. The duty of honesty goes well beyond that of\n            transparency that dominates legacy privacy regimes. Unlike with transparency, honesty\n            cannot get away with hiding relevant information in complex out-of-band legal notices\n            no more than it can rely on overly cursory information provided in a consent dialog.\n          \n          Duty of Loyalty\n          \n            Because of the special [=fiduciary relationship=] that obtains between [=user=] and\n            [=user agent=], the latter is held to be loyal to the former in all situations, up to and\n            including in preference to the [=user agent=]'s implementer. When a [=user agent=] carries\n            out [=processing=] that is not directly in the [=user=]'s interest but rather benefits\n            another entity such as its implementer, including by piggybacking on [=processing=] that\n            may be in the user's interest, that behaviour is known as self-dealing.\n            [=Self-dealing=] is always [=inappropriate=]. Loyalty is the avoidance of [=self-dealing=].\n          \n        \n        \n          These duties ensure the [=user agent=] will care for the user. It is important to\n          note that there is a subtle difference between care and data paternalism which\n          is that the latter purports to help in part by removing agency (\"don't worry about it,\n          so long as your data is with us it's safe, you don't need to know what we do with it, it's\n          all good because we're good people\") whereas care aims to support people by enhancing\n          their agency and sovereignty.\n        \n      \n    \n    \n      Privacy Threat Model Building Blocks\n      \n        Threats Are Context-Relative\n        \n          Privacy threat models for the Web exist ([[?TRACKING-PREVENTION-POLICY]], [[?ANTI-TRACKING-POLICY]],\n          [[?PRIVACY-THREAT]]). While good within the scope of what they intend to do, they tend to be\n          very specific to cross-context tracking as seen by the browser, which is but one problem.\n          Privacy threat modelling could benefit from having more of a toolbox to reuse in different\n          situations.\n        \n        \n          This document provides building blocks for the creation of privacy threat models on\n          the Web. Note that privacy threat models have an important difference with security threat\n          models in that all [=parties=] are potential threats, even when they are not rogue.\n          In fact the [=first party=] is typically considered to be the primary threat, the one\n          against which the brunt of mitigating techniques are to be leveraged.\n        \n        \n          The most important building blocks for a privacy threat model are those that define a\n          [=context=]: [=actors=], [=attributes=], and [=transmission principles=], as well as the\n          [=context-relative informational norms=]. Collectively, these define the expectations that\n          obtain in a given [=context=]. Based on these expectations, it becomes possible to ask\n          questions about the ways in which they could fail and how to prevent. What happens if the\n          [=subject=] is a [=vulnerable person=]? How well does the context fare if expectations of\n          consent are undermined by manipulative techniques ([[?DIGITAL-MARKET-MANIPULATION]],\n          [[?PRIVACY-BEHAVIOR]])? If the [=transmission principles=] are based on sharing data with\n          parties while telling them that they cannot use it, what confidence do we have that they\n          are following the rules?\n        \n        \n          When assessing privacy threats, it is not necessary to establish harms since breaking the\n          [=context=]'s [=norms=] is [=inappropriate=] in all cases. However, consideration of harms can\n          inform which issues to prioritise. Where individual privacy harms are to be\n          defined, the definitive source is Privacy Harms [[?PRIVACY-HARMS]].\n        \n      \n      \n        Identity on the Web\n        \n          A [=person=]'s identity is the set of characteristics that define them. In\n          computer systems, [=identity=] is typically attached to a means of denotation that makes\n          it easier for an automated system to recognise the user, an identifier of\n          some type.\n        \n        \n          A [=person=]'s characteristics, and therefore [=identity=], is entirely\n          [=context=]-dependent. Recognising a [=person=] in distinct [=contexts=] can at times be\n          [=appropriate=] but this relies on an understanding of applicable [=norms=] and will\n          likely require compartmentalisation. (For example, if you meet your therapist at a\n          cocktail party, you expect them to have rather different discussion topics with you than\n          they usually would, and possibly even to pretend they do not know you.) As a result,\n          automating the recognition of a [=person=]'s identity across different [=contexts=] is\n          [=inappropriate=]. This is particularly true for [=vulnerable=] people as recognising them\n          in different [=contexts=] may force their vulnerability into the open.\n        \n        \n          [=Identity=] being by nature fragmented, [=user agents=] must work in support of [=users=]\n          having different identities in different [=context=] with respect to all [=parties=]\n          (including the user agent vendor) and to prevent their recognition through other means,\n          where possible.\n        \n        \n          A keystone principle of the Web is trust [[RFC8890]]. An important part of trust is to\n          ensure that [=data=] collected for a [=purpose=] that matches a clearly delineated [=user=]\n          feature should not then be used for additional secondary [=purposes=]. Email is often used\n          for login and communication [=purposes=], including essential transactional interactions.\n          Using emails for cross-context recognition [=purposes=] is therefore not only\n          [=inappropriate=] but also threatens key messaging infrastructure. Future iterations of\n          the Web platform should ensure that users can log into sites and receive communication\n          without relying on email at all.\n        \n      \n    \n    \n      User Control and Autonomy\n      \n        A [=person=]'s autonomy is their ability to make decisions of their own volition,\n        without undue influence from other parties. People have limited intellectual resources and\n        time with which to weigh decisions, and by necessity rely on shortcuts when making\n        decisions. This makes their privacy preferences malleable [[?PRIVACY-BEHAVIOR]] and susceptible to\n        manipulation [[?DIGITAL-MARKET-MANIPULATION]]. A [=person=]'s [=autonomy=] is enhanced by a\n        system or device when that system offers a shortcut that aligns more with what that [=person=] would\n        have decided given arbitrary amounts of time and relatively unfettered intellectual ability;\n        and [=autonomy=] is decreased when a similar shortcut goes against decisions made under\n        ideal conditions.\n      \n      \n        Affordances and interactions that decrease [=autonomy=] are known as dark patterns.\n        A [=dark pattern=] does not have to be intentional, the deceptive effect is sufficient to\n        define them [[?DARK-PATTERNS]], [[?DARK-PATTERN-DARK]].\n      \n      \n        Because we are all subject to motivated reasoning, the design of defaults and affordances\n        that may impact [=user=] [=autonomy=] should be the subject of independent scrutiny.\n        Implementers are enjoined to be particularly cautious to avoid slipping into\n        [=data paternalism=].\n      \n      \n        Given the sheer volume of potential [=data=]-related decisions in today's data economy,\n        complete informational self-determination is impossible. This fact, however, should not be\n        confused with the contention that privacy is dead. Careful design of our technological\n        infrastructure can ensure that [=users=]' [=autonomy=] as pertaining to their own [=data=]\n        is enhanced through [=appropriate=] defaults and choice architectures.\n      \n      \n        In the 1970s, the Fair Information Practices or FIPs were elaborated\n        in support of individual [=autonomy=] in the face of growing concerns with databases. The\n        [=FIPs=] assume that there is sufficiently little [=data processing=] taking place that any\n        [=person=] will be able to carry out sufficient diligence to enable [=autonomy=] in their\n        decision-making. Since they entirely offload the [=privacy labour=]\n        to [=users=] and assume perfect, unfettered [=autonomy=], the [=FIPs=] do not forbid specific\n        types of [=data processing=] but only place them under different procedural requirements.\n        Such an approach is [=appropriate=] for [=parties=] that are processing data in the 1970s.\n      \n      \n        One notable issue with procedural approaches to privacy is that they tend to have the same\n        requirements in situations where the [=user=] finds themselves in a significant asymmetry of\n        power with a [=party=] ‚Äî for instance the [=user=] of an essential service provided by a\n        monopolistic platform ‚Äî and those where [=user=] and [=parties=] are very much on equal\n        footing, or even where the [=user=] may have greater power, as is the case with small\n        businesses operating in a competitive environment. It further does not consider cases in\n        which one [=party=] may coerce other [=parties=] into facilitating its [=inappropriate=]\n        practices, as is often the case with dominant players in advertising [[?CONSENT-LACKEYS]] or\n        in content aggregation [[?CAT]].\n      \n      \n        Reference to the [=FIPs=] survives to this day. They are often referenced as transparency\n        and choice, which, in today's digital environment, is often a strong indication that\n        [=inappropriate=] [=processing=] is being described.\n      \n      \n        \n        \n          A method of privacy regulation which promises honesty and autonomy but delivers neither.\n          [[?CONFIDING]].\n        \n      \n      \n        Opt-in, Consent, Opt-out, Global Controls\n        \n          Different procedural mechanisms exist to enable [=people=] to control the [=processing=]\n          done to their [=data=]. Mechanisms that increase the number of [=purposes=] for which\n          their [=data=] is being [=processed=] are referred to as opt-in or\n          consent; mechanisms that decrease this number of [=purposes=] are known as\n          opt-out.\n        \n        \n          When deployed thoughtfully, these mechanisms can enhance [=people=]'s [=autonomy=]. Often,\n          however, they are used as a way to avoid putting in the difficult work of deciding which\n          types of [=processing=] are [=appropriate=] and which are not, offloading [=privacy labour=]\n          to the [=user=].\n        \n        \n          Privacy regulatory regimes are often anchored at extremes: either they default to allowing\n          only very few strictly essential [=purposes=] such that many [=parties=] will have to\n          resort to [=consent=], habituating [=people=] to ignore legal prompts and incentivising\n          [=dark patterns=], or, conversely, they default to forbidding only very few, particularly\n          egregious [=purposes=], such that [=people=] will have to perform the [=privacy labour=] to\n          [=opt out=] in every [=context=] in order to produce [=appropriate=] [=processing=].\n        \n        \n          An approach that is more aligned with the expectation that the Web should provide a\n          trustworthy, [=person=]-centric environment is to establish a regime consisting of\n          three privacy tiers:\n        \n        \n          Default Privacy Tier\n          \n            This is the set of [=purposes=] that are deemed [=appropriate=] in a given [=context=]\n            and the [=processing=] that a [=person=] can expect without having triggered any\n            mechanisms to change their preferences. The exact details for this tier require clear\n            definition, including aspects such as data retention, but [=data=] would have to be\n            systematically siloed by [=context=] (not by [=party=], making it stricter than\n            the [=Vegas Rule=] and more in line with respecting [=privacy=]). This default tier\n            should also be defined differently for certain kinds of [=contexts=]. The legitimate\n            [=processing=] that can take place in this tier derives its legitimacy from matching the\n            expectations and interests of both the [=user=] and the [=first party=] in their\n            relationship, as guided by the applicable [=norms=]. This tier is more [=appropriate=] the more the\n            [=first party=] acts in accordance with [=fiduciary duties=].\n          \n          Opt-out Privacy Tier\n          \n            [=People=] who, either from personal preference or because they are [=vulnerable=],\n            require greater obscurity in some or even most [=contexts=], would be able to transition\n            to this tier through an [=opt-out=] mechanism. This tier would only permit strictly\n            necessary [=purposes=]. This tier should be [=appropriate=] for [=vulnerable=]\n            [=people=].\n          \n          Opt-in Privacy Tier\n          \n            In rare and highly specific cases, [=people=] should be able to [=consent=] to more\n            sensitive [=purposes=], such as having their [=identity=] recognised across contexts or\n            their reading history shared with a company. The burden of proof on ensuring that informed [=consent=] has\n            been obtained needs in this case to be very high (much higher than what prevails for\n            instance in [[?GDPR]] jurisdictions as currently practices). [=Consent=] is comparable to the general problem\n            of permissions on the Web platform. In the same way that it should be clear when a given\n            device capability is in use (eg. you are providing geolocation or camera access), sharing\n            data under this tier should be set up in such a way that it requires deliberate, specific\n            action from the [=user=] (eg. triggering a form control) and if that [=consent=] is\n            persistent, there should be an indicator that data is being transmitted shown at all\n            times, in such a way that the user can easily switch it off. In general, providing\n            [=consent=] should be rare, difficult, highly intentional, and temporary.\n          \n        \n        \n          When an [=opt-out=] mechanism exists, it should preferably be complemented by a\n          global opt-out mechanism. The function of a [=global opt-out=] mechanism is to\n          rectify the automation asymmetry whereby service providers can automate\n          [=data processing=] but [=people=] have to take manual action. A good example of a\n          [=global opt-out=] mechanism is the Global Privacy Control [[?GPC]].\n        \n        \n          Conceptually, a [=global opt-out=] mechanism is an automaton operating as part of the\n          [=user agent=], which is to say that it is equivalent to a robot that would carry out the\n          [=user=]'s bidding by pressing an [=opt-out=] button with every interaction that the\n          [=user=] has with a site, or more generally conveys an expression of the [=user=]'s\n          rights in a relevant jurisdiction. (For instance, under [[?GDPR]], the [=user=] may be\n          conveying objections to [=processing=] based on legitimate interest or the withdrawal of\n          [=consent=] to specific [=purposes=].) It should be noted that, since a [=global opt-out=]\n          signal is reaffirmed automatically with every [=user=] interaction, it will take precedence\n          in terms of specificity over any manner of blanket [=consent=] that a site may obtain,\n          unless that [=consent=] is directly attached to an interaction (eg. terms specified on a\n          form upon submission).\n        \n      \n    \n    \n      Collective Issues in Privacy\n      \n        When designing Web technology, we naturally pay attention to potential impacts on the [=person=]\n        using the Web through their [=user agent=]. In addition to potential individual harms we also\n        pay heed to collective effects that emerge from the accumulation of individual actions as\n        influenced by entities and the structure of technology.\n      \n      \n        Note that in evaluating impact, we deliberately ignore what implementers or specifiers may\n        have intended and only focus on outcomes. This framing is known as POSIWID, or\n        \"the Purpose Of a System Is What It Does\".\n      \n      \n        The collective problem of privacy is known as legibility. [=Legibility=] concerns\n        population-level [=data processing=] that may impact populations or individuals, including in\n        ways that [=people=] could not control even under the optimistic assumptions of the [=FIPs=].\n        For example, based on population-level analysis, a company may know that site.example\n        is predominantly visited by [=people=] of a given race or gender, and decide not to run its\n        job ads there. Visitors to that page are implicitly having their [=data=] processed in\n        [=inappropriate=] ways, with no way to discover the discrimination or seek relief\n        [[?DEMOCRATIC-DATA]].\n      \n      \n        What we consider is therefore not just the relation between the people who expose themselves\n        and the entities that invite that disclosure [[?RELATIONAL-TURN]], but also between the people\n        who expose themselves and those who do not but may find themselves recognised as such indirectly\n        anyway. One key understanding here is that such relations may persists even when data is\n        [=permanently de-identified=].\n      \n      \n        [=Legibility=] practices can be legitimate or\n        illegitimate depending\n        on the [=context=] and on the [=norms=] that apply in that [=context=]. Typically, a\n        [=legibility=] practice may be [=legitimate=] if it is managed through an acceptable process\n        of collective [=governance=]. For example, it is often considered [=legitimate=] for a\n        government, under the control of its citizens, to maintain a database of license plates for\n        the [=purpose=] of enforcing the rules of the road. It would be [=illegitimate=] to observe\n        the same license plates near places of worship to build a database of religious identity.\n      \n      \n        [=Legibility=] is often used to order information about the world. This can notably create\n        problems of [=reflexivity=] and of [=autonomy=].\n      \n      \n        Problems of reflexivity occur when the ordering of information about the world\n        used to produce [=legibility=] finds itself changing the way in which the world operates.\n        This can produce self-reinforcing loops that can have deleterious effects both individual and\n        collective [[?SEEING-LIKE-A-STATE]].\n      \n      \n        Issues of [=autonomy=] occur depending on the manner in which [=legibility=] is implemented.\n        When [=legibility=] is used to order the world following rules set by the [=user=] or\n        following methods subject to public scrutiny and [=governance=] models with strong checks and\n        balances (such as a newspaper's editorial decisions), then it will enhance [=user=] [=autonomy=]\n        and tend to be [=legitimate=]. When it is done in the [=user=]'s stead and without [=governance=],\n        it decreases [=user=] [=autonomy=] and tends to be [=illegitimate=].\n      \n      \n      \n        Data governance refers to the rules and processes for how [=data=] is\n        [=processed=] in any given [=context=]. How data is governed describes who has\n        power to make decisions over [=data=] and how [[?DATA-FUTURES-GLOSSARY]].\n      \n      \n        In general, collective issues in [=data=] require collective solutions. The proper goal of\n        [=data governance=] at the standards-setting level is the development of structural controls\n        in [=user agents=] and the provision of institutions that can handle population-level problems\n        in [=data=]. [=Governance=] will often struggle to achieve its goals if it works primarily by\n        increasing individual control over [=data=]. A collective approach reduces the cost of\n        control.\n      \n      \n        Collecting data at large scales can have significant pro-social outcomes. Problems tend to\n        emerge when entities take part in dual-use collection in which [=data=] is [=processed=]\n        for collective benefit but also for [=self-dealing=] [=purposes=] that may degrade welfare.\n        The [=self-dealing=] [=purposes=] will be justified as bankrolling the pro-social outcomes,\n        which, absent collective oversight, cannot be considered to support claims to [=legitimacy=]\n        for such [=legibility=]. It is vital for standards-setting organisations to establish not\n        just purely technical devices but techno-social systems that can govern data at scale.\n      \n      \n    \n  \n\n","length":38851,"excerpt":"Setting the standard for a robust, policy-ready understanding of privacy.","byline":null,"dir":null,"siteName":null,"lang":null},"finalizedMeta":{"title":"Principles of User Privacy (PUP)","description":"Setting the standard for a robust, policy-ready understanding of privacy.","author":false,"creator":"@robinberjon","publisher":false,"date":"2022-04-05T17:38:42.153Z","topics":[]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"Principles of User Privacy (PUP)","description":false,"canonical":"https://darobin.github.io/pup/","keywords":[],"image":"agnes-tc.jpg","firstParagraph":"\n        Privacy has been an essentially contested concept [[?PRIVACY-CONTESTED]]. Its debated meaning\n        render its support problematic in the context of a standards-setting process grounded in\n        consensus [[?W3C-PROCESS]], in seeking out technical solutions grounded on shared\n        requirements, and in addressing the needs of a worldwide constituency. This document\n        provides definitions for privacy and related concepts that are suitable for a global\n        audience, that can provide building blocks for privacy threat modelling, and can guide the\n        development of the Web as a trustworthy platform. In the spirit of building a much-needed\n        bridge between technology and policy, this document is written under the expectation that\n        it can apply to both.\n      "},"dublinCore":{},"opengraph":{"title":"Principles of User Privacy (PUP)","description":"Setting the standard for a robust, policy-ready understanding of privacy.","url":"https://darobin.github.io/pup/","site_name":false,"locale":"en","type":false,"typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":"https://darobin.github.io/pup/pup.png"},"twitter":{"site":"@robinberjon","description":"Setting the standard for a robust, policy-ready understanding of privacy.","card":"summary_large_image","creator":"@robinberjon","title":"Principles of User Privacy (PUP)","image":"https://darobin.github.io/pup/pup.png","image:alt":"A cute puppy face drawing","url":"https://darobin.github.io/pup/"},"archivedData":{"link":false,"wayback":false}}}