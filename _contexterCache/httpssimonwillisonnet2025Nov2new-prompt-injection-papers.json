{"initialLink":"https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/","sanitizedLink":"https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/","finalLink":"https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/\" itemprop=\"url\">New prompt injection papers: Agents Rule of Two and The Attacker Moves Second</a></contexter-box-head></contexter-box-head><contexter-byline class=\"p-author author\" slot=\"author\"><span class=\"p-name byline\" rel=\"author\" itemprop=\"author\">Simon Willison</span></contexter-byline><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2025-12-16T18:06:07.735Z\">12/16/2025</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>Two interesting new papers regarding LLM security and prompt injection came to my attention this weekend. Agents Rule of Two: A Practical Approach to AI Agent Security The first is …</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a is=\"contexter-link\" href=\"https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"f087d5bb8cbcf35f45241c70b1b7b0c089db502b","data":{"originalLink":"https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/","sanitizedLink":"https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/","canonical":"https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/","htmlText":"<!DOCTYPE html>\n<html lang=\"en-gb\">\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"canonical\" href=\"https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/\">\n<title>New prompt injection papers: Agents Rule of Two and The Attacker Moves Second</title>\n<script defer data-domain=\"simonwillison.net\" src=\"https://plausible.io/js/plausible.js\"></script>\n<link rel=\"alternate\" type=\"application/atom+xml\" title=\"Atom\" href=\"/atom/everything/\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/static/css/all.css\">\n<link rel=\"webmention\" href=\"https://webmention.io/simonwillison.net/webmention\">\n<link rel=\"pingback\" href=\"https://webmention.io/simonwillison.net/xmlrpc\">\n<meta name=\"author\" content=\"Simon Willison\">\n<meta property=\"og:site_name\" content=\"Simon Willison’s Weblog\">\n\n\n<meta name=\"twitter:card\" content=\"summary\">\n<meta name=\"twitter:image\" content=\"https://static.simonwillison.net/static/2025/agents-rule-of-two-updated-card.jpg\">\n<meta name=\"twitter:creator\" content=\"@simonw\">\n<meta property=\"og:url\" content=\"https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/\">\n<meta property=\"og:title\" content=\"New prompt injection papers: Agents Rule of Two and The Attacker Moves Second\">\n<meta property=\"og:image\" content=\"https://static.simonwillison.net/static/2025/agents-rule-of-two-updated-card.jpg\">\n<meta property=\"og:type\" content=\"article\">\n<meta property=\"og:description\" content=\"Two interesting new papers regarding LLM security and prompt injection came to my attention this weekend. Agents Rule of Two: A Practical Approach to AI Agent Security The first is …\">\n<meta property=\"og:updated_time\" content=\"1762124973\">\n\n\n\n<script async src=\"https://media.ethicalads.io/media/client/ethicalads.min.js\"></script>\n\n<script>\n(function() { // Apply theme immediately to prevent flash\n  const theme = localStorage.getItem('theme');\n  if (theme === 'light' || theme === 'dark') {\n    document.documentElement.setAttribute('data-theme', theme);\n  }\n})();\n</script>\n</head>\n<body class=\"smallhead\">\n\n<div id=\"smallhead\">\n  <div id=\"smallhead-inner\">\n    <h1><a href=\"/\">Simon Willison’s Weblog</a></h1>\n    <a id=\"smallhead-about\" href=\"/about/#subscribe\">Subscribe</a>\n  </div>\n</div><!-- #smallhead -->\n\n<div id=\"wrapper\">\n<div id=\"primary\">\n\n<div class=\"entry entryPage\">\n\n\n<div data-permalink-context=\"/2025/Nov/2/new-prompt-injection-papers/\">\n<h2>New prompt injection papers: Agents Rule of Two and The Attacker Moves Second</h2>\n<p class=\"mobile-date\">2nd November 2025</p>\n\n\n\n<p>Two interesting new papers regarding LLM security and prompt injection came to my attention this weekend.</p>\n<h4 id=\"agents-rule-of-two-a-practical-approach-to-ai-agent-security\">Agents Rule of Two: A Practical Approach to AI Agent Security</h4>\n<p>The first is <a href=\"https://ai.meta.com/blog/practical-ai-agent-security/\">Agents Rule of Two: A Practical Approach to AI Agent Security</a>, published on October 31st on the Meta AI blog. It doesn’t list authors but it was <a href=\"https://x.com/MickAyzenberg/status/1984355145917088235\">shared on Twitter</a> by Meta AI security researcher Mick Ayzenberg.</p>\n<p>It proposes a “Rule of Two” that’s inspired by both my own <a href=\"https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/\">lethal trifecta</a> concept and the Google Chrome team’s <a href=\"https://chromium.googlesource.com/chromium/src/+/main/docs/security/rule-of-2.md\">Rule Of 2</a> for writing code that works with untrustworthy inputs:</p>\n<blockquote>\n<p>At a high level, the Agents Rule of Two states that until robustness research allows us to reliably detect and refuse prompt injection, agents <strong>must satisfy no more than two</strong> of the following three properties within a session to avoid the highest impact consequences of prompt injection.</p>\n<p><strong>[A]</strong> An agent can process untrustworthy inputs</p>\n<p><strong>[B]</strong> An agent can have access to sensitive systems or private data</p>\n<p><strong>[C]</strong> An agent can change state or communicate externally</p>\n<p>It’s still possible that all three properties are necessary to carry out a request. If an agent requires all three without starting a new session (i.e., with a fresh context window), then the agent should not be permitted to operate autonomously and at a minimum requires supervision --- via human-in-the-loop approval or another reliable means of validation.</p>\n</blockquote>\n<p>It’s accompanied by this handy diagram:</p>\n<p><img src=\"https://static.simonwillison.net/static/2025/agents-rule-of-two-updated.jpg\" alt=\"Venn diagram titled &quot;Choose Two&quot; showing three overlapping circles labeled A, B, and C. Circle A (top): &quot;Process untrustworthy inputs&quot; with description &quot;Externally authored data may contain prompt injection attacks that turn an agent malicious.&quot; Circle B (bottom left): &quot;Access to sensitive systems or private data&quot; with description &quot;This includes private user data, company secrets, production settings and configs, source code, and other sensitive data.&quot; Circle C (bottom right): &quot;Change state or communicate externally&quot; with description &quot;Overwrite or change state through write actions, or transmitting data to a threat actor through web requests or tool calls.&quot; The two-way overlaps between circles are labeled &quot;Lower risk&quot; while the center where all three circles overlap is labeled &quot;Danger&quot;.\" style=\"max-width: 100%;\"></p>\n<p>I like this <em>a lot</em>.</p>\n<p>I’ve spent several years now trying to find clear ways to explain the risks of prompt injection attacks to developers who are building on top of LLMs. It’s frustratingly difficult.</p>\n<p>I’ve had the most success with the lethal trifecta, which boils one particular class of prompt injection attack down to a simple-enough model: if your system has access to private data, exposure to untrusted content and a way to communicate externally then it’s vulnerable to private data being stolen.</p>\n<p>The one problem with the lethal trifecta is that it only covers the risk of data exfiltration: there are plenty of other, even nastier risks that arise from prompt injection attacks against LLM-powered agents with access to tools which the lethal trifecta doesn’t cover.</p>\n<p>The Agents Rule of Two neatly solves this, through the addition of “changing state” as a property to consider. This brings other forms of tool usage into the picture: anything that can change state triggered by untrustworthy inputs is something to be very cautious about.</p>\n<p>It’s also refreshing to see another major research lab concluding that prompt injection remains an unsolved problem, and attempts to block or filter them have not proven reliable enough to depend on. The current solution is to design systems with this in mind, and the Rule of Two is a solid way to think about that.</p>\n<p id=\"exception\"><strong>Update</strong>: On thinking about this further there’s one aspect of the Rule of Two model that doesn’t work for me: the Venn diagram above marks the combination of untrustworthy inputs and the ability to change state as “safe”, but that’s not right. Even without access to private systems or sensitive data that pairing can still produce harmful results. Unfortunately adding an exception for that pair undermines the simplicity of the “Rule of Two” framing!</p>\n<p id=\"update-2\"><strong>Update 2</strong>: Mick Ayzenberg responded to this note in <a href=\"https://news.ycombinator.com/item?id=45794245#45802448\">a comment on Hacker News</a>:</p>\n<blockquote>\n<p>Thanks for the feedback! One small bit of clarification, the framework would describe access to any sensitive system as part of the [B] circle, not only private systems or private data.</p>\n<p>The intention is that an agent that has removed [B] can write state and communicate freely, but not with any systems that matter (wrt critical security outcomes for its user). An example of an agent in this state would be one that can take actions in a tight sandbox or is isolated from production.</p>\n</blockquote>\n<p>The Meta team also <a href=\"https://news.ycombinator.com/item?id=45794245#45802046\">updated their post</a> to replace “safe” with “lower risk” as the label on the intersections between the different circles. I’ve updated my screenshots of their diagrams in this post, <a href=\"https://static.simonwillison.net/static/2025/agents-rule-of-two.jpg\">here’s the original</a> for comparison.</p>\n<p>Which brings me to the second paper...</p>\n<h4 id=\"the-attacker-moves-second-stronger-adaptive-attacks-bypass-defenses-against-llm-jailbreaks-and-prompt-injections\">The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against LLM Jailbreaks and Prompt Injections</h4>\n<p>This paper is dated 10th October 2025 <a href=\"https://arxiv.org/abs/2510.09023\">on Arxiv</a> and comes from a heavy-hitting team of 14 authors—Milad Nasr, Nicholas Carlini, Chawin Sitawarin, Sander V. Schulhoff, Jamie Hayes, Michael Ilie, Juliette Pluto, Shuang Song, Harsh Chaudhari, Ilia Shumailov, Abhradeep Thakurta, Kai Yuanqing Xiao, Andreas Terzis, Florian Tramèr—including representatives from OpenAI, Anthropic, and Google DeepMind.</p>\n<p>The paper looks at 12 published defenses against prompt injection and jailbreaking and subjects them to a range of “adaptive attacks”—attacks that are allowed to expend considerable effort iterating multiple times to try and find a way through.</p>\n<p>The defenses did not fare well:</p>\n<blockquote>\n<p>By systematically tuning and scaling general optimization techniques—gradient descent, reinforcement learning, random search, and human-guided exploration—we bypass 12 recent defenses (based on a diverse set of techniques) with attack success rate above 90% for most; importantly, the majority of defenses originally reported near-zero attack success rates.</p>\n</blockquote>\n<p>Notably the “Human red-teaming setting” scored 100%, defeating all defenses. That red-team consisted of 500 participants in an online competition they ran with a $20,000 prize fund.</p>\n<p>The key point of the paper is that static example attacks—single string prompts designed to bypass systems—are an almost useless way to evaluate these defenses. Adaptive attacks are far more powerful, as shown by this chart:</p>\n<p><img src=\"https://static.simonwillison.net/static/2025/attack-success-rate.jpg\" alt=\"Bar chart showing Attack Success Rate (%) for various security systems across four categories: Prompting, Training, Filtering Model, and Secret Knowledge. The chart compares three attack types shown in the legend: Static / weak attack (green hatched bars), Automated attack (ours) (orange bars), and Human red-teaming (ours) (purple dotted bars). Systems and their success rates are: Spotlighting (28% static, 99% automated), Prompt Sandwich (21% static, 95% automated), RPO (0% static, 99% automated), Circuit Breaker (8% static, 100% automated), StruQ (62% static, 100% automated), SeqAlign (5% static, 96% automated), ProtectAI (15% static, 90% automated), PromptGuard (26% static, 94% automated), PIGuard (0% static, 71% automated), Model Armor (0% static, 90% automated), Data Sentinel (0% static, 80% automated), MELON (0% static, 89% automated), and Human red-teaming setting (0% static, 100% human red-teaming).\" style=\"max-width: 100%;\"></p>\n<p>The three automated adaptive attack techniques used by the paper are:</p>\n<ul>\n<li>\n<strong>Gradient-based methods</strong>—these were the least effective, using the technique described in the legendary <a href=\"https://arxiv.org/abs/2307.15043\">Universal and Transferable Adversarial Attacks on Aligned Language Models</a> paper <a href=\"https://simonwillison.net/2023/Jul/27/universal-and-transferable-attacks-on-aligned-language-models/\">from 2023</a>.</li>\n<li>\n<strong>Reinforcement learning methods</strong>—particularly effective against black-box models: “we allowed the attacker model to interact directly with the defended system and observe its outputs”, using 32 sessions of 5 rounds each.</li>\n<li>\n<strong>Search-based methods</strong>—generate candidates with an LLM, then evaluate and further modify them using LLM-as-judge and other classifiers.</li>\n</ul>\n<p>The paper concludes somewhat optimistically:</p>\n<blockquote>\n<p>[...] Adaptive evaluations are therefore more challenging to perform,\nmaking it all the more important that they are performed. We again urge defense authors to release simple, easy-to-prompt defenses that are amenable to human analysis. [...] Finally, we hope that our analysis here will increase the standard for defense evaluations, and in so doing, increase the likelihood that reliable jailbreak and prompt injection defenses will be developed.</p>\n</blockquote>\n<p>Given how totally the defenses were defeated, I do not share their optimism that reliable defenses will be developed any time soon.</p>\n<p>As a review of how far we still have to go this paper packs a powerful punch. I think it makes a strong case for Meta’s Agents Rule of Two as the best practical advice for building secure LLM-powered agent systems today in the absence of prompt injection defenses we can rely on.</p>\n\n\n</div>\n<div class=\"entryFooter\">Posted <a href=\"/2025/Nov/2/\">2nd November 2025</a> at 11:09 pm &middot; Follow me on <a href=\"https://fedi.simonwillison.net/@simon\">Mastodon</a>, <a href=\"https://bsky.app/profile/simonwillison.net\">Bluesky</a>, <a href=\"https://twitter.com/simonw\">Twitter</a> or <a href=\"https://simonwillison.net/about/#subscribe\">subscribe to my newsletter</a></div>\n<p class=\"edit-page-link\" data-admin-url=\"/admin/blog/entry/9074/\" style=\"display: none;\"></p>\n\n</div>\n\n<div class=\"recent-articles\">\n<h2>More recent articles</h2>\n<ul class=\"bullets\">\n  \n    <li><a href=\"/2025/Dec/15/porting-justhtml/\">I ported JustHTML from Python to JavaScript with Codex CLI and GPT-5.2 in 4.5 hours</a> - 15th December 2025</li>\n  \n    <li><a href=\"/2025/Dec/14/justhtml/\">JustHTML is a fascinating example of vibe engineering in action</a> - 14th December 2025</li>\n  \n    <li><a href=\"/2025/Dec/12/openai-skills/\">OpenAI are quietly adopting skills, now available in ChatGPT and Codex CLI</a> - 12th December 2025</li>\n  \n</ul>\n</div>\n\n\n</div> <!-- #primary -->\n\n<div id=\"secondary\">\n\n<div class=\"metabox\">\n<p class=\"this-is\">This is <strong>New prompt injection papers: Agents Rule of Two and The Attacker Moves Second</strong> by Simon Willison, posted on <a href=\"/2025/Nov/2/\">2nd November 2025</a>.</p>\n\n<div class=\"series-info\">\n  <p>Part of series <strong><a href=\"/series/prompt-injection/\">Prompt injection</a></strong></p>\n  <ol start=\"20\">\n    \n      \n        <li><a href=\"/2025/Jun/16/the-lethal-trifecta/\">The lethal trifecta for AI agents: private data, untrusted content, and external communication</a> - June 16, 2025, 1:20 p.m. </li>\n      \n    \n      \n        <li><a href=\"/2025/Aug/15/the-summer-of-johann/\">The Summer of Johann: prompt injections as far as the eye can see</a> - Aug. 15, 2025, 10:44 p.m. </li>\n      \n    \n      \n        <li><a href=\"/2025/Oct/22/openai-ciso-on-atlas/\">Dane Stuckey (OpenAI CISO) on prompt injection risks for ChatGPT Atlas</a> - Oct. 22, 2025, 8:43 p.m. </li>\n      \n    \n      \n        <li><strong>New prompt injection papers: Agents Rule of Two and The Attacker Moves Second</strong></a> - Nov. 2, 2025, 11:09 p.m. </li>\n      \n    \n    \n  </ol>\n</div>\n\n\n    \n        <a class=\"item-tag\" href=\"/tags/definitions/\" rel=\"tag\">\n            definitions\n            <span>40</span>\n        </a>\n    \n        <a class=\"item-tag\" href=\"/tags/security/\" rel=\"tag\">\n            security\n            <span>569</span>\n        </a>\n    \n        <a class=\"item-tag\" href=\"/tags/openai/\" rel=\"tag\">\n            openai\n            <span>374</span>\n        </a>\n    \n        <a class=\"item-tag\" href=\"/tags/prompt-injection/\" rel=\"tag\">\n            prompt-injection\n            <span>138</span>\n        </a>\n    \n        <a class=\"item-tag\" href=\"/tags/anthropic/\" rel=\"tag\">\n            anthropic\n            <span>211</span>\n        </a>\n    \n        <a class=\"item-tag\" href=\"/tags/nicholas-carlini/\" rel=\"tag\">\n            nicholas-carlini\n            <span>7</span>\n        </a>\n    \n        <a class=\"item-tag\" href=\"/tags/paper-review/\" rel=\"tag\">\n            paper-review\n            <span>15</span>\n        </a>\n    \n        <a class=\"item-tag\" href=\"/tags/lethal-trifecta/\" rel=\"tag\">\n            lethal-trifecta\n            <span>21</span>\n        </a>\n    \n\n\n<p><strong>Next:</strong> <a href=\"/2025/Nov/4/datasette-10a20/\">A new SQL-powered permissions system in Datasette 1.0a20</a></p>\n\n\n<p><strong>Previous:</strong> <a href=\"/2025/Oct/28/github-universe-badge/\">Hacking the WiFi-enabled color screen GitHub Universe conference badge</a></p>\n\n<div data-ea-publisher=\"simonwillisonnet\" data-ea-type=\"image\"></div>\n<section style=\"\n    /* .promo */\n    border-radius: 8px;\n    margin: 1.5rem 0;\n    padding: 1rem 1.25rem;\n    /* .variant-a */\n    border: 2px solid #6c3eb9;\n  \">\n  <h3 style=\"\n      /* h3 */\n      margin: 0 0 0.5rem;\n      font-size: 1.25rem;\n    \">\n    Monthly briefing\n  </h3>\n  <p style=\"\n      /* p */\n      margin: 0 0 1rem;\n      line-height: 1.4;\n    \">\n    Sponsor me for <strong>$10/month</strong> and get a curated email digest of the month's most important LLM developments.\n  </p>\n  <p style=\"\n      /* p */\n      margin: 0 0 1rem;\n      line-height: 1.4;\n    \">\n    Pay me to send you less!\n  </p>\n  <a href=\"https://github.com/sponsors/simonw/\" style=\"\n      /* a.button */\n      display: inline-block;\n      padding: 0.5rem 1rem;\n      background: #6c3eb9;\n      color: #fff;\n      text-decoration: none;\n      border-radius: 4px;\n      font-weight: 600;\n    \">\n    Sponsor &amp; subscribe\n  </a>\n</section>\n</div>\n\n\n\n</div> <!-- #secondary -->\n</div> <!-- #wrapper -->\n\n\n\n\n\n\n\n\n\n\n\n<div id=\"ft\">\n    <ul>\n      <li><a href=\"/about/#about-site\">Colophon</a></li>\n      <li>&copy;</li>\n      <li><a href=\"/2002/\">2002</a></li>\n      <li><a href=\"/2003/\">2003</a></li>\n      <li><a href=\"/2004/\">2004</a></li>\n      <li><a href=\"/2005/\">2005</a></li>\n      <li><a href=\"/2006/\">2006</a></li>\n      <li><a href=\"/2007/\">2007</a></li>\n      <li><a href=\"/2008/\">2008</a></li>\n      <li><a href=\"/2009/\">2009</a></li>\n      <li><a href=\"/2010/\">2010</a></li>\n      <li><a href=\"/2011/\">2011</a></li>\n      <li><a href=\"/2012/\">2012</a></li>\n      <li><a href=\"/2013/\">2013</a></li>\n      <li><a href=\"/2014/\">2014</a></li>\n      <li><a href=\"/2015/\">2015</a></li>\n      <li><a href=\"/2016/\">2016</a></li>\n      <li><a href=\"/2017/\">2017</a></li>\n      <li><a href=\"/2018/\">2018</a></li>\n      <li><a href=\"/2019/\">2019</a></li>\n      <li><a href=\"/2020/\">2020</a></li>\n      <li><a href=\"/2021/\">2021</a></li>\n      <li><a href=\"/2022/\">2022</a></li>\n      <li><a href=\"/2023/\">2023</a></li>\n      <li><a href=\"/2024/\">2024</a></li>\n      <li><a href=\"/2025/\">2025</a></li>\n      <li>\n        <button id=\"theme-toggle\" type=\"button\" aria-label=\"Toggle theme\">\n          <!-- Auto/System icon (default) -->\n          <svg id=\"icon-auto\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M12 22C6.477 22 2 17.523 2 12S6.477 2 12 2s10 4.477 10 10-4.477 10-10 10zm0-2V4a8 8 0 1 0 0 16z\"/></svg>\n          <!-- Light icon -->\n          <svg id=\"icon-light\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" style=\"display:none\"><path d=\"M12 18a6 6 0 1 1 0-12 6 6 0 0 1 0 12zm0-2a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM11 1h2v3h-2V1zm0 19h2v3h-2v-3zM3.515 4.929l1.414-1.414L7.05 5.636 5.636 7.05 3.515 4.93zM16.95 18.364l1.414-1.414 2.121 2.121-1.414 1.414-2.121-2.121zm2.121-14.85l1.414 1.415-2.121 2.121-1.414-1.414 2.121-2.121zM5.636 16.95l1.414 1.414-2.121 2.121-1.414-1.414 2.121-2.121zM23 11v2h-3v-2h3zM4 11v2H1v-2h3z\"/></svg>\n          <!-- Dark icon -->\n          <svg id=\"icon-dark\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" style=\"display:none\"><path d=\"M10 7a7 7 0 0 0 12 4.9v.1c0 5.523-4.477 10-10 10S2 17.523 2 12 6.477 2 12 2h.1A6.979 6.979 0 0 0 10 7zm-6 5a8 8 0 0 0 15.062 3.762A9 9 0 0 1 8.238 4.938 7.999 7.999 0 0 0 4 12z\"/></svg>\n        </button>\n      </li>\n    </ul>\n</div>\n\n\n\n<script>\ndocument.addEventListener('DOMContentLoaded', () => {\n  document.querySelectorAll('h2[id],h3[id],h4[id],h5[id],h6[id]').forEach(el => {\n    const id = el.getAttribute('id');\n    const permalinkContext = el.closest('[data-permalink-context]');\n    if (permalinkContext) {\n      const url = permalinkContext.getAttribute('data-permalink-context');\n      const hashLink = document.createElement('a');\n      hashLink.style.borderBottom = 'none';\n      hashLink.style.color = '#666';\n      hashLink.style.fontSize = '1em';\n      hashLink.style.opacity = 0.8;\n      hashLink.setAttribute('href', url + '#' + id);\n      hashLink.innerText = '#';\n      el.appendChild(document.createTextNode('\\u00A0'));\n      el.appendChild(hashLink);\n    }\n  });\n});\n</script>\n<script type=\"module\">\n  const config = [\n    {\"tag\": \"lite-youtube\", \"js\": \"/static/lite-yt-embed.js\", \"css\": \"/static/lite-yt-embed.css\"}\n  ];\n  for (const {tag, js, css} of config) {\n    if (document.querySelector(tag)) {\n      if (css) {\n        document.head.appendChild(\n          Object.assign(document.createElement('link'), {\n            rel: 'stylesheet',\n            href: css\n          })\n        );\n      }\n      if (js) {\n        await import(js);\n      }\n  }\n}\n</script>\n<script>\n  document.addEventListener('DOMContentLoaded', () => {\n    if (window.localStorage.getItem('ADMIN')) {\n      document.querySelectorAll('.edit-page-link').forEach(el => {\n        const url = el.getAttribute('data-admin-url');\n        if (url) {\n          const a = document.createElement('a');\n          a.href = url;\n          a.innerText = 'Edit this page';\n          el.appendChild(a);\n          el.style.display = 'block';\n        }\n      });\n    }\n  });\n</script>\n<script>\n// Theme toggle functionality\n(function() {\n  const toggle = document.getElementById('theme-toggle');\n  const iconAuto = document.getElementById('icon-auto');\n  const iconLight = document.getElementById('icon-light');\n  const iconDark = document.getElementById('icon-dark');\n\n  // Theme states: 'auto' (default), 'light', 'dark'\n  function getTheme() {\n    return localStorage.getItem('theme') || 'auto';\n  }\n\n  function setTheme(theme) {\n    if (theme === 'auto') {\n      localStorage.removeItem('theme');\n      document.documentElement.removeAttribute('data-theme');\n    } else {\n      localStorage.setItem('theme', theme);\n      document.documentElement.setAttribute('data-theme', theme);\n    }\n    updateIcon(theme);\n  }\n\n  function updateIcon(theme) {\n    iconAuto.style.display = theme === 'auto' ? 'block' : 'none';\n    iconLight.style.display = theme === 'light' ? 'block' : 'none';\n    iconDark.style.display = theme === 'dark' ? 'block' : 'none';\n\n    // Update aria-label for accessibility\n    const labels = {\n      'auto': 'Theme: Auto (system preference). Click to switch to light.',\n      'light': 'Theme: Light. Click to switch to dark.',\n      'dark': 'Theme: Dark. Click to switch to auto.'\n    };\n    toggle.setAttribute('aria-label', labels[theme]);\n  }\n\n  // Cycle through themes: auto -> light -> dark -> auto\n  function cycleTheme() {\n    const current = getTheme();\n    const next = current === 'auto' ? 'light' : current === 'light' ? 'dark' : 'auto';\n    setTheme(next);\n  }\n\n  // Initialize\n  updateIcon(getTheme());\n  toggle.addEventListener('click', cycleTheme);\n})();\n</script>\n<script defer src=\"https://static.cloudflareinsights.com/beacon.min.js/vcd15cbe7772f49c399c6a5babf22c1241717689176015\" integrity=\"sha512-ZpsOmlRQV6y907TI0dKBHq9Md29nnaEIPlkf84rnaERnq6zvWvPUqr2ft8M1aS28oN72PdrCzSjY4U6VaAw1EQ==\" data-cf-beacon='{\"version\":\"2024.11.0\",\"token\":\"42c41055028944ee9764ce8039f69a82\",\"r\":1,\"server_timing\":{\"name\":{\"cfCacheStatus\":true,\"cfEdge\":true,\"cfExtPri\":true,\"cfL4\":true,\"cfOrigin\":true,\"cfSpeedBrain\":true},\"location_startswith\":null}}' crossorigin=\"anonymous\"></script>\n</body>\n</html>\n","oembed":false,"readabilityObject":{"title":"New prompt injection papers: Agents Rule of Two and The Attacker Moves Second","content":"<div id=\"readability-page-1\" class=\"page\"><div data-permalink-context=\"/2025/Nov/2/new-prompt-injection-papers/\">\n\n<p>2nd November 2025</p>\n\n\n\n<p>Two interesting new papers regarding LLM security and prompt injection came to my attention this weekend.</p>\n<h4 id=\"agents-rule-of-two-a-practical-approach-to-ai-agent-security\">Agents Rule of Two: A Practical Approach to AI Agent Security</h4>\n<p>The first is <a href=\"https://ai.meta.com/blog/practical-ai-agent-security/\">Agents Rule of Two: A Practical Approach to AI Agent Security</a>, published on October 31st on the Meta AI blog. It doesn’t list authors but it was <a href=\"https://x.com/MickAyzenberg/status/1984355145917088235\">shared on Twitter</a> by Meta AI security researcher Mick Ayzenberg.</p>\n<p>It proposes a “Rule of Two” that’s inspired by both my own <a href=\"https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/\">lethal trifecta</a> concept and the Google Chrome team’s <a href=\"https://chromium.googlesource.com/chromium/src/+/main/docs/security/rule-of-2.md\">Rule Of 2</a> for writing code that works with untrustworthy inputs:</p>\n<blockquote>\n<p>At a high level, the Agents Rule of Two states that until robustness research allows us to reliably detect and refuse prompt injection, agents <strong>must satisfy no more than two</strong> of the following three properties within a session to avoid the highest impact consequences of prompt injection.</p>\n<p><strong>[A]</strong> An agent can process untrustworthy inputs</p>\n<p><strong>[B]</strong> An agent can have access to sensitive systems or private data</p>\n<p><strong>[C]</strong> An agent can change state or communicate externally</p>\n<p>It’s still possible that all three properties are necessary to carry out a request. If an agent requires all three without starting a new session (i.e., with a fresh context window), then the agent should not be permitted to operate autonomously and at a minimum requires supervision --- via human-in-the-loop approval or another reliable means of validation.</p>\n</blockquote>\n<p>It’s accompanied by this handy diagram:</p>\n<p><img src=\"https://static.simonwillison.net/static/2025/agents-rule-of-two-updated.jpg\" alt=\"Venn diagram titled &quot;Choose Two&quot; showing three overlapping circles labeled A, B, and C. Circle A (top): &quot;Process untrustworthy inputs&quot; with description &quot;Externally authored data may contain prompt injection attacks that turn an agent malicious.&quot; Circle B (bottom left): &quot;Access to sensitive systems or private data&quot; with description &quot;This includes private user data, company secrets, production settings and configs, source code, and other sensitive data.&quot; Circle C (bottom right): &quot;Change state or communicate externally&quot; with description &quot;Overwrite or change state through write actions, or transmitting data to a threat actor through web requests or tool calls.&quot; The two-way overlaps between circles are labeled &quot;Lower risk&quot; while the center where all three circles overlap is labeled &quot;Danger&quot;.\"></p>\n<p>I like this <em>a lot</em>.</p>\n<p>I’ve spent several years now trying to find clear ways to explain the risks of prompt injection attacks to developers who are building on top of LLMs. It’s frustratingly difficult.</p>\n<p>I’ve had the most success with the lethal trifecta, which boils one particular class of prompt injection attack down to a simple-enough model: if your system has access to private data, exposure to untrusted content and a way to communicate externally then it’s vulnerable to private data being stolen.</p>\n<p>The one problem with the lethal trifecta is that it only covers the risk of data exfiltration: there are plenty of other, even nastier risks that arise from prompt injection attacks against LLM-powered agents with access to tools which the lethal trifecta doesn’t cover.</p>\n<p>The Agents Rule of Two neatly solves this, through the addition of “changing state” as a property to consider. This brings other forms of tool usage into the picture: anything that can change state triggered by untrustworthy inputs is something to be very cautious about.</p>\n<p>It’s also refreshing to see another major research lab concluding that prompt injection remains an unsolved problem, and attempts to block or filter them have not proven reliable enough to depend on. The current solution is to design systems with this in mind, and the Rule of Two is a solid way to think about that.</p>\n<p id=\"exception\"><strong>Update</strong>: On thinking about this further there’s one aspect of the Rule of Two model that doesn’t work for me: the Venn diagram above marks the combination of untrustworthy inputs and the ability to change state as “safe”, but that’s not right. Even without access to private systems or sensitive data that pairing can still produce harmful results. Unfortunately adding an exception for that pair undermines the simplicity of the “Rule of Two” framing!</p>\n<p id=\"update-2\"><strong>Update 2</strong>: Mick Ayzenberg responded to this note in <a href=\"https://news.ycombinator.com/item?id=45794245#45802448\">a comment on Hacker News</a>:</p>\n<blockquote>\n<p>Thanks for the feedback! One small bit of clarification, the framework would describe access to any sensitive system as part of the [B] circle, not only private systems or private data.</p>\n<p>The intention is that an agent that has removed [B] can write state and communicate freely, but not with any systems that matter (wrt critical security outcomes for its user). An example of an agent in this state would be one that can take actions in a tight sandbox or is isolated from production.</p>\n</blockquote>\n<p>The Meta team also <a href=\"https://news.ycombinator.com/item?id=45794245#45802046\">updated their post</a> to replace “safe” with “lower risk” as the label on the intersections between the different circles. I’ve updated my screenshots of their diagrams in this post, <a href=\"https://static.simonwillison.net/static/2025/agents-rule-of-two.jpg\">here’s the original</a> for comparison.</p>\n<p>Which brings me to the second paper...</p>\n<h4 id=\"the-attacker-moves-second-stronger-adaptive-attacks-bypass-defenses-against-llm-jailbreaks-and-prompt-injections\">The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against LLM Jailbreaks and Prompt Injections</h4>\n<p>This paper is dated 10th October 2025 <a href=\"https://arxiv.org/abs/2510.09023\">on Arxiv</a> and comes from a heavy-hitting team of 14 authors—Milad Nasr, Nicholas Carlini, Chawin Sitawarin, Sander V. Schulhoff, Jamie Hayes, Michael Ilie, Juliette Pluto, Shuang Song, Harsh Chaudhari, Ilia Shumailov, Abhradeep Thakurta, Kai Yuanqing Xiao, Andreas Terzis, Florian Tramèr—including representatives from OpenAI, Anthropic, and Google DeepMind.</p>\n<p>The paper looks at 12 published defenses against prompt injection and jailbreaking and subjects them to a range of “adaptive attacks”—attacks that are allowed to expend considerable effort iterating multiple times to try and find a way through.</p>\n<p>The defenses did not fare well:</p>\n<blockquote>\n<p>By systematically tuning and scaling general optimization techniques—gradient descent, reinforcement learning, random search, and human-guided exploration—we bypass 12 recent defenses (based on a diverse set of techniques) with attack success rate above 90% for most; importantly, the majority of defenses originally reported near-zero attack success rates.</p>\n</blockquote>\n<p>Notably the “Human red-teaming setting” scored 100%, defeating all defenses. That red-team consisted of 500 participants in an online competition they ran with a $20,000 prize fund.</p>\n<p>The key point of the paper is that static example attacks—single string prompts designed to bypass systems—are an almost useless way to evaluate these defenses. Adaptive attacks are far more powerful, as shown by this chart:</p>\n<p><img src=\"https://static.simonwillison.net/static/2025/attack-success-rate.jpg\" alt=\"Bar chart showing Attack Success Rate (%) for various security systems across four categories: Prompting, Training, Filtering Model, and Secret Knowledge. The chart compares three attack types shown in the legend: Static / weak attack (green hatched bars), Automated attack (ours) (orange bars), and Human red-teaming (ours) (purple dotted bars). Systems and their success rates are: Spotlighting (28% static, 99% automated), Prompt Sandwich (21% static, 95% automated), RPO (0% static, 99% automated), Circuit Breaker (8% static, 100% automated), StruQ (62% static, 100% automated), SeqAlign (5% static, 96% automated), ProtectAI (15% static, 90% automated), PromptGuard (26% static, 94% automated), PIGuard (0% static, 71% automated), Model Armor (0% static, 90% automated), Data Sentinel (0% static, 80% automated), MELON (0% static, 89% automated), and Human red-teaming setting (0% static, 100% human red-teaming).\"></p>\n<p>The three automated adaptive attack techniques used by the paper are:</p>\n<ul>\n<li>\n<strong>Gradient-based methods</strong>—these were the least effective, using the technique described in the legendary <a href=\"https://arxiv.org/abs/2307.15043\">Universal and Transferable Adversarial Attacks on Aligned Language Models</a> paper <a href=\"https://simonwillison.net/2023/Jul/27/universal-and-transferable-attacks-on-aligned-language-models/\">from 2023</a>.</li>\n<li>\n<strong>Reinforcement learning methods</strong>—particularly effective against black-box models: “we allowed the attacker model to interact directly with the defended system and observe its outputs”, using 32 sessions of 5 rounds each.</li>\n<li>\n<strong>Search-based methods</strong>—generate candidates with an LLM, then evaluate and further modify them using LLM-as-judge and other classifiers.</li>\n</ul>\n<p>The paper concludes somewhat optimistically:</p>\n<blockquote>\n<p>[...] Adaptive evaluations are therefore more challenging to perform,\nmaking it all the more important that they are performed. We again urge defense authors to release simple, easy-to-prompt defenses that are amenable to human analysis. [...] Finally, we hope that our analysis here will increase the standard for defense evaluations, and in so doing, increase the likelihood that reliable jailbreak and prompt injection defenses will be developed.</p>\n</blockquote>\n<p>Given how totally the defenses were defeated, I do not share their optimism that reliable defenses will be developed any time soon.</p>\n<p>As a review of how far we still have to go this paper packs a powerful punch. I think it makes a strong case for Meta’s Agents Rule of Two as the best practical advice for building secure LLM-powered agent systems today in the absence of prompt injection defenses we can rely on.</p>\n\n\n</div></div>","textContent":"\n\n2nd November 2025\n\n\n\nTwo interesting new papers regarding LLM security and prompt injection came to my attention this weekend.\nAgents Rule of Two: A Practical Approach to AI Agent Security\nThe first is Agents Rule of Two: A Practical Approach to AI Agent Security, published on October 31st on the Meta AI blog. It doesn’t list authors but it was shared on Twitter by Meta AI security researcher Mick Ayzenberg.\nIt proposes a “Rule of Two” that’s inspired by both my own lethal trifecta concept and the Google Chrome team’s Rule Of 2 for writing code that works with untrustworthy inputs:\n\nAt a high level, the Agents Rule of Two states that until robustness research allows us to reliably detect and refuse prompt injection, agents must satisfy no more than two of the following three properties within a session to avoid the highest impact consequences of prompt injection.\n[A] An agent can process untrustworthy inputs\n[B] An agent can have access to sensitive systems or private data\n[C] An agent can change state or communicate externally\nIt’s still possible that all three properties are necessary to carry out a request. If an agent requires all three without starting a new session (i.e., with a fresh context window), then the agent should not be permitted to operate autonomously and at a minimum requires supervision --- via human-in-the-loop approval or another reliable means of validation.\n\nIt’s accompanied by this handy diagram:\n\nI like this a lot.\nI’ve spent several years now trying to find clear ways to explain the risks of prompt injection attacks to developers who are building on top of LLMs. It’s frustratingly difficult.\nI’ve had the most success with the lethal trifecta, which boils one particular class of prompt injection attack down to a simple-enough model: if your system has access to private data, exposure to untrusted content and a way to communicate externally then it’s vulnerable to private data being stolen.\nThe one problem with the lethal trifecta is that it only covers the risk of data exfiltration: there are plenty of other, even nastier risks that arise from prompt injection attacks against LLM-powered agents with access to tools which the lethal trifecta doesn’t cover.\nThe Agents Rule of Two neatly solves this, through the addition of “changing state” as a property to consider. This brings other forms of tool usage into the picture: anything that can change state triggered by untrustworthy inputs is something to be very cautious about.\nIt’s also refreshing to see another major research lab concluding that prompt injection remains an unsolved problem, and attempts to block or filter them have not proven reliable enough to depend on. The current solution is to design systems with this in mind, and the Rule of Two is a solid way to think about that.\nUpdate: On thinking about this further there’s one aspect of the Rule of Two model that doesn’t work for me: the Venn diagram above marks the combination of untrustworthy inputs and the ability to change state as “safe”, but that’s not right. Even without access to private systems or sensitive data that pairing can still produce harmful results. Unfortunately adding an exception for that pair undermines the simplicity of the “Rule of Two” framing!\nUpdate 2: Mick Ayzenberg responded to this note in a comment on Hacker News:\n\nThanks for the feedback! One small bit of clarification, the framework would describe access to any sensitive system as part of the [B] circle, not only private systems or private data.\nThe intention is that an agent that has removed [B] can write state and communicate freely, but not with any systems that matter (wrt critical security outcomes for its user). An example of an agent in this state would be one that can take actions in a tight sandbox or is isolated from production.\n\nThe Meta team also updated their post to replace “safe” with “lower risk” as the label on the intersections between the different circles. I’ve updated my screenshots of their diagrams in this post, here’s the original for comparison.\nWhich brings me to the second paper...\nThe Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against LLM Jailbreaks and Prompt Injections\nThis paper is dated 10th October 2025 on Arxiv and comes from a heavy-hitting team of 14 authors—Milad Nasr, Nicholas Carlini, Chawin Sitawarin, Sander V. Schulhoff, Jamie Hayes, Michael Ilie, Juliette Pluto, Shuang Song, Harsh Chaudhari, Ilia Shumailov, Abhradeep Thakurta, Kai Yuanqing Xiao, Andreas Terzis, Florian Tramèr—including representatives from OpenAI, Anthropic, and Google DeepMind.\nThe paper looks at 12 published defenses against prompt injection and jailbreaking and subjects them to a range of “adaptive attacks”—attacks that are allowed to expend considerable effort iterating multiple times to try and find a way through.\nThe defenses did not fare well:\n\nBy systematically tuning and scaling general optimization techniques—gradient descent, reinforcement learning, random search, and human-guided exploration—we bypass 12 recent defenses (based on a diverse set of techniques) with attack success rate above 90% for most; importantly, the majority of defenses originally reported near-zero attack success rates.\n\nNotably the “Human red-teaming setting” scored 100%, defeating all defenses. That red-team consisted of 500 participants in an online competition they ran with a $20,000 prize fund.\nThe key point of the paper is that static example attacks—single string prompts designed to bypass systems—are an almost useless way to evaluate these defenses. Adaptive attacks are far more powerful, as shown by this chart:\n\nThe three automated adaptive attack techniques used by the paper are:\n\n\nGradient-based methods—these were the least effective, using the technique described in the legendary Universal and Transferable Adversarial Attacks on Aligned Language Models paper from 2023.\n\nReinforcement learning methods—particularly effective against black-box models: “we allowed the attacker model to interact directly with the defended system and observe its outputs”, using 32 sessions of 5 rounds each.\n\nSearch-based methods—generate candidates with an LLM, then evaluate and further modify them using LLM-as-judge and other classifiers.\n\nThe paper concludes somewhat optimistically:\n\n[...] Adaptive evaluations are therefore more challenging to perform,\nmaking it all the more important that they are performed. We again urge defense authors to release simple, easy-to-prompt defenses that are amenable to human analysis. [...] Finally, we hope that our analysis here will increase the standard for defense evaluations, and in so doing, increase the likelihood that reliable jailbreak and prompt injection defenses will be developed.\n\nGiven how totally the defenses were defeated, I do not share their optimism that reliable defenses will be developed any time soon.\nAs a review of how far we still have to go this paper packs a powerful punch. I think it makes a strong case for Meta’s Agents Rule of Two as the best practical advice for building secure LLM-powered agent systems today in the absence of prompt injection defenses we can rely on.\n\n\n","length":7188,"excerpt":"Two interesting new papers regarding LLM security and prompt injection came to my attention this weekend. Agents Rule of Two: A Practical Approach to AI Agent Security The first is …","byline":"Simon Willison","dir":null,"siteName":"Simon Willison’s Weblog","lang":"en-gb"},"finalizedMeta":{"title":"New prompt injection papers: Agents Rule of Two and The Attacker Moves Second","description":"Two interesting new papers regarding LLM security and prompt injection came to my attention this weekend. Agents Rule of Two: A Practical Approach to AI Agent Security The first is …","author":false,"creator":"Simon Willison","publisher":false,"date":"2025-12-16T18:06:07.735Z","topics":[]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":"Simon Willison","title":"New prompt injection papers: Agents Rule of Two and The Attacker Moves Second","description":false,"canonical":"https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/","keywords":[],"image":"https://static.simonwillison.net/static/2025/agents-rule-of-two-updated.jpg","firstParagraph":"2nd November 2025"},"dublinCore":{},"opengraph":{"title":"New prompt injection papers: Agents Rule of Two and The Attacker Moves Second","description":"Two interesting new papers regarding LLM security and prompt injection came to my attention this weekend. Agents Rule of Two: A Practical Approach to AI Agent Security The first is …","url":"https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/","site_name":"Simon Willison’s Weblog","locale":false,"type":"article","typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":"https://static.simonwillison.net/static/2025/agents-rule-of-two-updated-card.jpg","updated_time":"1762124973"},"twitter":{"site":false,"description":false,"card":"summary","creator":"@simonw","title":false,"image":"https://static.simonwillison.net/static/2025/agents-rule-of-two-updated-card.jpg"},"archivedData":{"link":false,"wayback":false}}}