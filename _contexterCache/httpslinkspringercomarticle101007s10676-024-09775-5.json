{"initialLink":"https://link.springer.com/article/10.1007/s10676-024-09775-5","sanitizedLink":"https://link.springer.com/article/10.1007/s10676-024-09775-5","finalLink":"https://link.springer.com/article/10.1007/s10676-024-09775-5","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://link.springer.com/article/10.1007/s10676-024-09775-5\" itemprop=\"url\">ChatGPT is bullshit - Ethics and Information Technology</a></contexter-box-head></contexter-box-head><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2024-07-10T15:27:34.520Z\">7/10/2024</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called “AI hallucinations”. We argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. We distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a is=\"contexter-link\" href=\"https://link.springer.com/article/10.1007/s10676-024-09775-5\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"d1d475ebc62586aad80ad2b61baef5e5d6fc5c76","data":{"originalLink":"https://link.springer.com/article/10.1007/s10676-024-09775-5","sanitizedLink":"https://link.springer.com/article/10.1007/s10676-024-09775-5","canonical":"https://link.springer.com/article/10.1007/s10676-024-09775-5","htmlText":"<!DOCTYPE html>\n<html lang=\"en\" class=\"no-js\">\n    <head>\n        <meta charset=\"UTF-8\">\n        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n        <meta name=\"applicable-device\" content=\"pc,mobile\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n        \n            <meta name=\"robots\" content=\"max-image-preview:large\">\n            <meta name=\"access\" content=\"Yes\">\n\n        \n        <meta name=\"360-site-verification\" content=\"1268d79b5e96aecf3ff2a7dac04ad990\" />\n\n        <title>ChatGPT is bullshit | Ethics and Information Technology</title>\n\n        \n            \n            \n    \n    <meta name=\"twitter:site\" content=\"@SpringerLink\"/>\n    <meta name=\"twitter:card\" content=\"summary_large_image\"/>\n    <meta name=\"twitter:image:alt\" content=\"Content cover image\"/>\n    <meta name=\"twitter:title\" content=\"ChatGPT is bullshit\"/>\n    <meta name=\"twitter:description\" content=\"Ethics and Information Technology - Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of...\"/>\n    <meta name=\"twitter:image\" content=\"https://media.springernature.com/full/springer-static/cover-hires/journal/10676\"/>\n    <meta name=\"journal_id\" content=\"10676\"/>\n    <meta name=\"dc.title\" content=\"ChatGPT is bullshit\"/>\n    <meta name=\"dc.source\" content=\"Ethics and Information Technology 2024 26:2\"/>\n    <meta name=\"dc.format\" content=\"text/html\"/>\n    <meta name=\"dc.publisher\" content=\"Springer\"/>\n    <meta name=\"dc.date\" content=\"2024-06-08\"/>\n    <meta name=\"dc.type\" content=\"OriginalPaper\"/>\n    <meta name=\"dc.language\" content=\"En\"/>\n    <meta name=\"dc.copyright\" content=\"2024 The Author(s)\"/>\n    <meta name=\"dc.rights\" content=\"2024 The Author(s)\"/>\n    <meta name=\"dc.rightsAgent\" content=\"journalpermissions@springernature.com\"/>\n    <meta name=\"dc.description\" content=\"Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called &#8220;AI hallucinations&#8221;. We argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. We distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.\"/>\n    <meta name=\"prism.issn\" content=\"1572-8439\"/>\n    <meta name=\"prism.publicationName\" content=\"Ethics and Information Technology\"/>\n    <meta name=\"prism.publicationDate\" content=\"2024-06-08\"/>\n    <meta name=\"prism.volume\" content=\"26\"/>\n    <meta name=\"prism.number\" content=\"2\"/>\n    <meta name=\"prism.section\" content=\"OriginalPaper\"/>\n    <meta name=\"prism.startingPage\" content=\"1\"/>\n    <meta name=\"prism.endingPage\" content=\"10\"/>\n    <meta name=\"prism.copyright\" content=\"2024 The Author(s)\"/>\n    <meta name=\"prism.rightsAgent\" content=\"journalpermissions@springernature.com\"/>\n    <meta name=\"prism.url\" content=\"https://link.springer.com/article/10.1007/s10676-024-09775-5\"/>\n    <meta name=\"prism.doi\" content=\"doi:10.1007/s10676-024-09775-5\"/>\n    <meta name=\"citation_pdf_url\" content=\"https://link.springer.com/content/pdf/10.1007/s10676-024-09775-5.pdf\"/>\n    <meta name=\"citation_fulltext_html_url\" content=\"https://link.springer.com/article/10.1007/s10676-024-09775-5\"/>\n    <meta name=\"citation_journal_title\" content=\"Ethics and Information Technology\"/>\n    <meta name=\"citation_journal_abbrev\" content=\"Ethics Inf Technol\"/>\n    <meta name=\"citation_publisher\" content=\"Springer Netherlands\"/>\n    <meta name=\"citation_issn\" content=\"1572-8439\"/>\n    <meta name=\"citation_title\" content=\"ChatGPT is bullshit\"/>\n    <meta name=\"citation_volume\" content=\"26\"/>\n    <meta name=\"citation_issue\" content=\"2\"/>\n    <meta name=\"citation_publication_date\" content=\"2024/06\"/>\n    <meta name=\"citation_online_date\" content=\"2024/06/08\"/>\n    <meta name=\"citation_firstpage\" content=\"1\"/>\n    <meta name=\"citation_lastpage\" content=\"10\"/>\n    <meta name=\"citation_article_type\" content=\"Original Paper\"/>\n    <meta name=\"citation_fulltext_world_readable\" content=\"\"/>\n    <meta name=\"citation_language\" content=\"en\"/>\n    <meta name=\"dc.identifier\" content=\"doi:10.1007/s10676-024-09775-5\"/>\n    <meta name=\"DOI\" content=\"10.1007/s10676-024-09775-5\"/>\n    <meta name=\"size\" content=\"79216\"/>\n    <meta name=\"citation_doi\" content=\"10.1007/s10676-024-09775-5\"/>\n    <meta name=\"citation_springer_api_url\" content=\"http://api.springer.com/xmldata/jats?q=doi:10.1007/s10676-024-09775-5&amp;api_key=\"/>\n    <meta name=\"description\" content=\"Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications \"/>\n    <meta name=\"dc.creator\" content=\"Hicks, Michael Townsen\"/>\n    <meta name=\"dc.creator\" content=\"Humphries, James\"/>\n    <meta name=\"dc.creator\" content=\"Slater, Joe\"/>\n    <meta name=\"dc.subject\" content=\"Management of Computing and Information Systems\"/>\n    <meta name=\"dc.subject\" content=\"Innovation/Technology Management\"/>\n    <meta name=\"dc.subject\" content=\"Ethics\"/>\n    <meta name=\"dc.subject\" content=\"User Interfaces and Human Computer Interaction\"/>\n    <meta name=\"dc.subject\" content=\"Library Science\"/>\n    <meta name=\"citation_reference\" content=\"Alkaissi, H., &amp; McFarlane, S. I., (2023, February 19). Artificial hallucinations in ChatGPT: Implications in scientific writing. Cureus, 15(2), e35179. \n                  https://doi.org/10.7759/cureus.35179\n                  \n                .\"/>\n    <meta name=\"citation_reference\" content=\"Bacin, S. (2021). My duties and the morality of others: Lying, truth and the good example in Fichte&#8217;s normative perfectionism. In S. Bacin, &amp; O. Ware (Eds.), Fichte&#8217;s system of Ethics: A critical guide. Cambridge University Press.\"/>\n    <meta name=\"citation_reference\" content=\"Cassam, Q. (2019). Vices of the mind. Oxford University Press.\"/>\n    <meta name=\"citation_reference\" content=\"Cohen, G. A. (2002). Deeper into bullshit. In S. Buss, &amp; L. Overton (Eds.), The contours of Agency: Essays on themes from Harry Frankfurt. MIT Press.\"/>\n    <meta name=\"citation_reference\" content=\"Davis, E., &amp; Aaronson, S. (2023). Testing GPT-4 with Wolfram alpha and code interpreter plub-ins on math and science problems. Arxiv Preprint: arXiv, 2308, 05713v2.\"/>\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Behavioral and Brain Sciences; citation_title=Intentional systems in Cognitive Ethology: The panglossian paradigm defended; citation_author=DC Dennett; citation_volume=6; citation_publication_date=1983; citation_pages=343-390; citation_doi=10.1017/S0140525X00016393; citation_id=CR8\"/>\n    <meta name=\"citation_reference\" content=\"Dennett, D. C. (1987). The intentional stance. The MIT.\"/>\n    <meta name=\"citation_reference\" content=\"Dennis Whitcomb (2023). Bullshit questions. Analysis, 83(2), 299&#8211;304.\"/>\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Analytic Philosophy; citation_title=Bullshit activities; citation_author=K Easwaran; citation_volume=00; citation_publication_date=2023; citation_pages=1-23; citation_doi=10.1111/phib.12328; citation_id=CR9\"/>\n    <meta name=\"citation_reference\" content=\"Edwards, B. (2023). Why ChatGPT and bing chat are so good at making things up. Ars Tecnica. \n                  https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/\n                  \n                , accesssed 19th April, 2024.\"/>\n    <meta name=\"citation_reference\" content=\"Frankfurt, H. (2002). Reply to cohen. In S. Buss, &amp; L. Overton (Eds.), The contours of agency: Essays on themes from Harry Frankfurt. MIT Press.\"/>\n    <meta name=\"citation_reference\" content=\"Frankfurt, H. (2005). On Bullshit, Princeton.\"/>\n    <meta name=\"citation_reference\" content=\"Knight, W. (2023). Some glimpse AGI in ChatGPT. others call it a mirage. Wired, August 18 2023, accessed via \n                  https://www.wired.com/story/chatgpt-agi-intelligence/\n                  \n                .\"/>\n    <meta name=\"citation_reference\" content=\"Levenstein, B. A., &amp; Herrmann, D. A. (forthcoming). Still no lie detector for language models: Probing empirical and conceptual roadblocks. Philosophical Studies, 1&#8211;27.\"/>\n    <meta name=\"citation_reference\" content=\"Levy, N. (2023). Philosophy, Bullshit, and peer review. Camridge University.\"/>\n    <meta name=\"citation_reference\" content=\"Lightman, H., et al. (2023). Let&#8217;s verify step by step. Arxiv Preprint: arXiv, 2305, 20050.\"/>\n    <meta name=\"citation_reference\" content=\"Lysandrou (2023). Comparative analysis of drug-GPT and ChatGPT LLMs for healthcare insights: Evaluating accuracy and relevance in patient and HCP contexts. ArXiv Preprint: arXiv, 2307, 16850v1.\"/>\n    <meta name=\"citation_reference\" content=\"Macpherson, F. (2013). The philosophy and psychology of hallucination: an introduction, in Hallucination, Macpherson and Platchias (Eds.), London: MIT Press.\"/>\n    <meta name=\"citation_reference\" content=\"Mahon, J. E. (2015). The definition of lying and deception. The Stanford Encyclopedia of Philosophy (Winter 2016 Edition), Edward N. Zalta (Ed.), \n                  https://plato.stanford.edu/archives/win2016/entries/lying-definition/\n                  \n                .\"/>\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Ergo; citation_title=Fictionalism about Chatbots; citation_author=F Mallory; citation_volume=10; citation_issue=38; citation_publication_date=2023; citation_pages=1082-1100; citation_id=CR21\"/>\n    <meta name=\"citation_reference\" content=\"Mandelkern, M., &amp; Linzen, T. (2023). Do language models&#8217; Words Refer?. ArXiv Preprint: arXiv, 2308, 05576.\"/>\n    <meta name=\"citation_reference\" content=\"OpenAI (2023). GPT-4 technical report. ArXiv Preprint: arXiv, 2303, 08774v3.\"/>\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Pacific Philosophical Quarterly; citation_title=Destigmatizing the Exegetical Attribution of lies: The case of Kant; citation_author=I Proops, R Sorensen; citation_publication_date=2023; citation_doi=10.1111/papq.12442; citation_id=CR24\"/>\n    <meta name=\"citation_reference\" content=\"Sarkar, A. (2023). ChatGPT 5 is on track to attain artificial general intelligence. The Statesman, April 12, 2023. Accesses via \n                  https://www.thestatesman.com/supplements/science_supplements/chatgpt-5-is-on-track-to-attain-artificial-general-intelligence-1503171366.html\n                  \n                .\"/>\n    <meta name=\"citation_reference\" content=\"Shah, C., &amp; Bender, E. M. (2022). Situating search. CHIIR &#8216;22: Proceedings of the 2022 Conference on Human Information Interaction and Retrieval March 2022 Pages 221&#8211;232 \n                  https://doi.org/10.1145/3498366.3505816\n                  \n                .\"/>\n    <meta name=\"citation_reference\" content=\"Weise, K., &amp; Metz, C. (2023). When AI chatbots hallucinate. New York Times, May 9, 2023. Accessed via \n                  https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html\n                  \n                .\"/>\n    <meta name=\"citation_reference\" content=\"Weiser, B. (2023). Here&#8217;s what happens when your lawyer uses ChatGPT. New York Times, May 23, 2023. Accessed via \n                  https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html\n                  \n                .\"/>\n    <meta name=\"citation_reference\" content=\"Zhang (2023). How language model hallucinations can snowball. ArXiv preprint: arXiv:, 2305, 13534v1.\"/>\n    <meta name=\"citation_reference\" content=\"Zhu, T., et al. (2023). Large language models for information retrieval: A survey. Arxiv Preprint: arXiv, 2308, 17107v2.\"/>\n    <meta name=\"citation_author\" content=\"Hicks, Michael Townsen\"/>\n    <meta name=\"citation_author_email\" content=\"Michael.hicks@glasgow.ac.uk\"/>\n    <meta name=\"citation_author_institution\" content=\"University of Glasgow, Glasgow, Scotland\"/>\n    <meta name=\"citation_author\" content=\"Humphries, James\"/>\n    <meta name=\"citation_author_email\" content=\"James.Humphries@glasgow.ac.uk\"/>\n    <meta name=\"citation_author_institution\" content=\"University of Glasgow, Glasgow, Scotland\"/>\n    <meta name=\"citation_author\" content=\"Slater, Joe\"/>\n    <meta name=\"citation_author_email\" content=\"Joe.Slater@glasgow.ac.uk\"/>\n    <meta name=\"citation_author_institution\" content=\"University of Glasgow, Glasgow, Scotland\"/>\n    <meta name=\"format-detection\" content=\"telephone=no\"/>\n    <meta name=\"citation_cover_date\" content=\"2024/06/01\"/>\n    \n\n            \n    \n    <meta property=\"og:url\" content=\"https://link.springer.com/article/10.1007/s10676-024-09775-5\"/>\n    <meta property=\"og:type\" content=\"article\"/>\n    <meta property=\"og:site_name\" content=\"SpringerLink\"/>\n    <meta property=\"og:title\" content=\"ChatGPT is bullshit - Ethics and Information Technology\"/>\n    <meta property=\"og:description\" content=\"Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called &#8220;AI hallucinations&#8221;. We argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. We distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.\"/>\n    <meta property=\"og:image\" content=\"https://media.springernature.com/full/springer-static/cover-hires/journal/10676\"/>\n    \n\n            \n        \n\n        <meta name=\"format-detection\" content=\"telephone=no\">\n\n        \n    \n\n\n        <link rel=\"apple-touch-icon\" sizes=\"180x180\" href=/oscar-static/img/favicons/darwin/apple-touch-icon-92e819bf8a.png>\n<link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=/oscar-static/img/favicons/darwin/android-chrome-192x192-6f081ca7e5.png>\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=/oscar-static/img/favicons/darwin/favicon-32x32-1435da3e82.png>\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=/oscar-static/img/favicons/darwin/favicon-16x16-ed57f42bd2.png>\n<link rel=\"shortcut icon\" data-test=\"shortcut-icon\" href=/oscar-static/img/favicons/darwin/favicon-c6d59aafac.ico>\n\n<meta name=\"theme-color\" content=\"#e6e6e6\">\n\n\n        <!-- Please see discussion: https://github.com/springernature/frontend-open-space/issues/316-->\n<!--TODO: Implement alternative to CTM in here if the discussion concludes we do not continue with CTM as a practice-->\n\n\n<link rel=\"stylesheet\" media=\"print\" href=/oscar-static/app-springerlink/css/print-b8af42253b.css>\n\n\n\n\n\n    \n        \n            \n    <style> html{text-size-adjust:100%;line-height:1.15}body{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;margin:0}details,main{display:block}h1{font-size:2em;margin:.67em 0}a{background-color:transparent;color:#025e8d}sub{bottom:-.25em;font-size:75%;line-height:0;position:relative;vertical-align:baseline}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input{font-family:inherit;font-size:100%;line-height:1.15;margin:0;overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}summary{display:list-item}[hidden]{display:none}button{cursor:pointer}svg{height:1rem;width:1rem} </style>\n    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  body{background:#fff;color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;min-height:100%}a{color:#025e8d;text-decoration:underline;text-decoration-skip-ink:auto}button{cursor:pointer}img{border:0;height:auto;max-width:100%;vertical-align:middle}html{box-sizing:border-box;font-size:100%;height:100%;overflow-y:scroll}h1{font-size:2.25rem}h2{font-size:1.75rem}h1,h2,h4{font-weight:700;line-height:1.2}h4{font-size:1.25rem}body{font-size:1.125rem}*{box-sizing:inherit}p{margin-bottom:2rem;margin-top:0}p:last-of-type{margin-bottom:0}.c-ad{text-align:center}@media only screen and (min-width:480px){.c-ad{padding:8px}}.c-ad--728x90{display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:876px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-size:.875rem;font-weight:400;margin-bottom:4px}.c-ad__label,.eds-c-header{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.5}.eds-c-header{background-color:#fff;border-bottom:2px solid #01324b;font-size:1rem;padding:8px 0 0}.eds-c-header__container{align-items:center;display:flex;flex-wrap:nowrap;gap:8px 16px;justify-content:space-between;margin:0 auto 8px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav{border-top:2px solid #c5e0f4;padding-top:4px;position:relative}.eds-c-header__nav-container{align-items:center;display:flex;flex-wrap:wrap;margin:0 auto 4px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav-container>:not(:last-child){margin-right:32px}.eds-c-header__link-container{align-items:center;display:flex;flex:1 0 auto;gap:8px 16px;justify-content:space-between}.eds-c-header__list{list-style:none;margin:0;padding:0}.eds-c-header__list-item{font-weight:700;margin:0 auto;max-width:1280px;padding:8px}.eds-c-header__list-item:not(:last-child){border-bottom:2px solid #c5e0f4}.eds-c-header__item{color:inherit}@media only screen and (min-width:768px){.eds-c-header__item--menu{display:none;visibility:hidden}.eds-c-header__item--menu:first-child+*{margin-block-start:0}}.eds-c-header__item--inline-links{display:none;visibility:hidden}@media only screen and (min-width:768px){.eds-c-header__item--inline-links{display:flex;gap:16px 16px;visibility:visible}}.eds-c-header__item--divider:before{border-left:2px solid #c5e0f4;content:\"\";height:calc(100% - 16px);margin-left:-15px;position:absolute;top:8px}.eds-c-header__brand{padding:16px 8px}.eds-c-header__brand a{display:block;line-height:1;text-decoration:none}.eds-c-header__brand img{height:1.5rem;width:auto}.eds-c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.eds-c-header__icon{fill:currentcolor;display:inline-block;font-size:1.5rem;height:1em;transform:translate(0);vertical-align:bottom;width:1em}.eds-c-header__icon+*{margin-left:8px}.eds-c-header__expander{background-color:#f0f7fc}.eds-c-header__search{display:block;padding:24px 0}@media only screen and (min-width:768px){.eds-c-header__search{max-width:70%}}.eds-c-header__search-container{position:relative}.eds-c-header__search-label{color:inherit;display:inline-block;font-weight:700;margin-bottom:8px}.eds-c-header__search-input{background-color:#fff;border:1px solid #000;padding:8px 48px 8px 8px;width:100%}.eds-c-header__search-button{background-color:transparent;border:0;color:inherit;height:100%;padding:0 8px;position:absolute;right:0}.has-tethered.eds-c-header__expander{border-bottom:2px solid #01324b;left:0;margin-top:-2px;top:100%;width:100%;z-index:10}@media only screen and (min-width:768px){.has-tethered.eds-c-header__expander--menu{display:none;visibility:hidden}}.has-tethered .eds-c-header__heading{display:none;visibility:hidden}.has-tethered .eds-c-header__heading:first-child+*{margin-block-start:0}.has-tethered .eds-c-header__search{margin:auto}.eds-c-header__heading{margin:0 auto;max-width:1280px;padding:16px 16px 0}.eds-c-footer__button-text.hover,.eds-c-footer__button-text.visited,.eds-c-footer__button-text:hover,.eds-c-footer__button-text:visited,.eds-c-status-message__message a{color:inherit}.eds-c-pagination{align-items:center;display:flex;flex-wrap:wrap;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;gap:16px 0;justify-content:center;line-height:1.4;list-style:none;margin:0;padding:32px 0}@media only screen and (min-width:480px){.eds-c-pagination{padding:32px 16px}}.eds-c-pagination__item{margin-right:8px}.eds-c-pagination__item--prev{margin-right:16px}.eds-c-pagination__item--next .eds-c-pagination__link,.eds-c-pagination__item--prev .eds-c-pagination__link{padding:16px 8px}.eds-c-pagination__item--next{margin-left:8px}.eds-c-pagination__item:last-child{margin-right:0}.eds-c-pagination__link{align-items:center;color:#222;cursor:pointer;display:inline-block;font-size:1rem;margin:0;padding:16px 24px;position:relative;text-align:center;transition:all .2s ease 0s}.eds-c-pagination__link:visited{color:#222}.eds-c-pagination__link--disabled{border-color:#555;color:#555;cursor:default}.eds-c-pagination__link--active{background-color:#01324b;background-image:none;border-radius:8px;color:#fff}.eds-c-pagination__link--active:focus,.eds-c-pagination__link--active:hover,.eds-c-pagination__link--active:visited{color:#fff}.eds-c-pagination__link-container{align-items:center;display:flex}.eds-c-pagination__icon{fill:#222;height:1.5rem;width:1.5rem}.eds-c-pagination__icon--disabled{fill:#555}.eds-c-pagination__visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.eds-c-status-message,.js .c-popup{background-color:#fff;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.eds-c-status-message{border:1px solid #999;border-radius:8px;box-sizing:border-box;display:flex;font-size:1rem;gap:.5rem .5rem;line-height:1.4;padding:16px 16px 12px;position:relative}.eds-c-status-message :last-child{margin-bottom:0}.eds-c-status-message__text{margin-top:2px;padding-right:12px}.eds-c-status-message__text h5{margin-top:0}.eds-c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:24px;margin-right:8px;transform:translate(0);vertical-align:text-top;width:24px}.eds-c-status-message__icon--top{align-self:flex-start}.eds-c-status-message__dismiss-button{background:0 0;border:0;cursor:pointer;position:absolute;right:8px}.eds-c-status-message__dismiss-icon{fill:currentcolor;color:#555;display:inline-block;height:24px;transform:translate(0);vertical-align:text-top;width:24px}.eds-c-status-message__title{font-weight:700;margin-bottom:8px}.eds-c-status-message--info .eds-c-status-message__icon{color:#0070a8}.eds-c-status-message--info{border-bottom:4px solid #0070a8}.eds-c-status-message--error .eds-c-status-message__icon{color:#be1818}.eds-c-status-message--error{border-bottom:4px solid #be1818}.eds-c-status-message--success .eds-c-status-message__icon{color:#00a69d}.eds-c-status-message--success{border-bottom:4px solid #00a69d}.eds-c-status-message--warning .eds-c-status-message__icon{color:#f58220}.eds-c-status-message--warning{border-bottom:4px solid #f58220}.c-breadcrumbs{color:#333;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs>li{display:inline}svg.c-breadcrumbs__chevron{fill:#333;height:10px;margin:0 .25rem;width:10px}.c-breadcrumbs--contrast,.c-breadcrumbs--contrast .c-breadcrumbs__link{color:#fff}.c-breadcrumbs--contrast svg.c-breadcrumbs__chevron{fill:#fff}@media only screen and (max-width:479px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-skip-link{background:#01324b;bottom:auto;color:#fff;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);width:100%;z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:active,.c-skip-link:hover,.c-skip-link:link,.c-skip-link:visited{color:#fff}.c-skip-link:focus{transform:translateY(0)}.l-with-sidebar{display:flex;flex-wrap:wrap}.l-with-sidebar>*{margin:0}.l-with-sidebar__sidebar{flex-basis:var(--with-sidebar--basis,400px);flex-grow:1}.l-with-sidebar>:not(.l-with-sidebar__sidebar){flex-basis:0px;flex-grow:999;min-width:var(--with-sidebar--min,53%)}.l-with-sidebar>:first-child{padding-right:4rem}@supports (gap:1em){.l-with-sidebar>:first-child{padding-right:0}.l-with-sidebar{gap:var(--with-sidebar--gap,4rem)}}.c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.app-masthead__colour-4{--background-color:#ff9500;--gradient-light:rgba(0,0,0,.5);--gradient-dark:rgba(0,0,0,.8)}.app-masthead{background:var(--background-color,#0070a8);position:relative}.app-masthead:after{background:radial-gradient(circle at top right,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)));bottom:0;content:\"\";left:0;position:absolute;right:0;top:0}@media only screen and (max-width:479px){.app-masthead:after{background:linear-gradient(225deg,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)))}}.app-masthead__container{color:var(--masthead-color,#fff);margin:0 auto;max-width:1280px;padding:0 16px;position:relative;z-index:1}.u-button{align-items:center;background-color:#01324b;background-image:none;border:4px solid transparent;border-radius:32px;cursor:pointer;display:inline-flex;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;font-weight:700;justify-content:center;line-height:1.3;margin:0;padding:16px 32px;position:relative;transition:all .2s ease 0s;width:auto}.u-button svg,.u-button--contrast svg,.u-button--primary svg,.u-button--secondary svg,.u-button--tertiary svg{fill:currentcolor}.u-button,.u-button:visited{color:#fff}.u-button,.u-button:hover{box-shadow:0 0 0 1px #01324b;text-decoration:none}.u-button:hover{border:4px solid #fff}.u-button:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.u-button:focus,.u-button:hover{background-color:#fff;background-image:none;color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--primary:focus svg path,.app-masthead--pastel .c-pdf-download .u-button--primary:hover svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover svg path,.u-button--primary:focus svg path,.u-button--primary:hover svg path,.u-button:focus svg path,.u-button:hover svg path{fill:#01324b}.u-button--primary{background-color:#01324b;background-image:none;border:4px solid transparent;box-shadow:0 0 0 1px #01324b;color:#fff;font-weight:700}.u-button--primary:visited{color:#fff}.u-button--primary:hover{border:4px solid #fff;box-shadow:0 0 0 1px #01324b;text-decoration:none}.u-button--primary:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.u-button--primary:focus,.u-button--primary:hover{background-color:#fff;background-image:none;color:#01324b}.u-button--secondary{background-color:#fff;border:4px solid #fff;color:#01324b;font-weight:700}.u-button--secondary:visited{color:#01324b}.u-button--secondary:hover{border:4px solid #01324b;box-shadow:none}.u-button--secondary:focus,.u-button--secondary:hover{background-color:#01324b;color:#fff}.app-masthead--pastel .c-pdf-download .u-button--secondary:focus svg path,.app-masthead--pastel .c-pdf-download .u-button--secondary:hover svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:focus svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:hover svg path,.u-button--secondary:focus svg path,.u-button--secondary:hover svg path,.u-button--tertiary:focus svg path,.u-button--tertiary:hover svg path{fill:#fff}.u-button--tertiary{background-color:#ebf1f5;border:4px solid transparent;box-shadow:none;color:#666;font-weight:700}.u-button--tertiary:visited{color:#666}.u-button--tertiary:hover{border:4px solid #01324b;box-shadow:none}.u-button--tertiary:focus,.u-button--tertiary:hover{background-color:#01324b;color:#fff}.u-button--contrast{background-color:transparent;background-image:none;color:#fff;font-weight:400}.u-button--contrast:visited{color:#fff}.u-button--contrast,.u-button--contrast:focus,.u-button--contrast:hover{border:4px solid #fff}.u-button--contrast:focus,.u-button--contrast:hover{background-color:#fff;background-image:none;color:#000}.u-button--contrast:focus svg path,.u-button--contrast:hover svg path{fill:#000}.u-button--disabled,.u-button:disabled{background-color:transparent;background-image:none;border:4px solid #ccc;color:#000;cursor:default;font-weight:400;opacity:.7}.u-button--disabled svg,.u-button:disabled svg{fill:currentcolor}.u-button--disabled:visited,.u-button:disabled:visited{color:#000}.u-button--disabled:focus,.u-button--disabled:hover,.u-button:disabled:focus,.u-button:disabled:hover{border:4px solid #ccc;text-decoration:none}.u-button--disabled:focus,.u-button--disabled:hover,.u-button:disabled:focus,.u-button:disabled:hover{background-color:transparent;background-image:none;color:#000}.u-button--disabled:focus svg path,.u-button--disabled:hover svg path,.u-button:disabled:focus svg path,.u-button:disabled:hover svg path{fill:#000}.u-button--small,.u-button--xsmall{font-size:.875rem;padding:2px 8px}.u-button--small{padding:8px 16px}.u-button--large{font-size:1.125rem;padding:10px 35px}.u-button--full-width{display:flex;width:100%}.u-button--icon-left svg{margin-right:8px}.u-button--icon-right svg{margin-left:8px}.u-clear-both{clear:both}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-ma-16{margin:16px}.u-mt-0{margin-top:0}.u-mt-24{margin-top:24px}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-32{margin-bottom:32px}.u-button-reset{background-color:transparent;border:0;padding:0}.u-sans-serif{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.u-serif{font-family:Merriweather,serif}h1,h2,h4{-webkit-font-smoothing:antialiased}p{overflow-wrap:break-word;word-break:break-word}.u-h4{font-size:1.25rem;font-weight:700;line-height:1.2}.u-mbs-0{margin-block-start:0!important}.c-article-header{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}@media only screen and (min-width:876px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:767px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#025e8d;border-color:transparent;color:#fff}.c-article-body .c-article-access-provider{padding:8px 16px}.c-article-body .c-article-access-provider,.c-notes{border:1px solid #d5d5d5;border-image:initial;border-left:none;border-right:none;margin:24px 0}.c-article-body .c-article-access-provider__text{color:#555}.c-article-body .c-article-access-provider__text,.c-notes__text{font-size:1rem;margin-bottom:0;padding-bottom:2px;padding-top:2px;text-align:center}.c-article-body .c-article-author-affiliation__address{color:inherit;font-weight:700;margin:0}.c-article-body .c-article-author-affiliation__authors-list{list-style:none;margin:0;padding:0}.c-article-body .c-article-author-affiliation__authors-item{display:inline;margin-left:0}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #fff;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;margin-bottom:24px}.c-article-share-box__description{font-size:1rem;margin-bottom:8px}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__additional-info{color:#626262;font-size:.813rem}.c-article-share-box__button{background:#fff;box-sizing:content-box;text-align:center}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#025e8d;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{font-size:1rem}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;font-size:1.25rem;font-weight:700;line-height:1.2;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-article-section__figure-caption{display:block;margin-bottom:8px;word-break:break-word}.c-article-section__figure .video,p.app-article-masthead__access--above-download{margin:0 0 16px}.c-article-section__figure-description{font-size:1rem}.c-article-section__figure-description>*{margin-bottom:0}.c-cod{display:block;font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#025e8d;border:1px solid #025e8d;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#025e8d}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-pdf-download__link{display:flex;flex:1 1 0%;padding:13px 24px}.c-pdf-download__link:hover{text-decoration:none}@media only screen and (min-width:768px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{display:flex;flex-direction:row;gap:16px 16px;margin:0;max-width:100%;padding:16px 0 0}.c-article-body .c-article-recommendations-list__item,.c-book-body .c-article-recommendations-list__item{flex:1 1 0%}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{flex-direction:column}}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-body .c-article-history{margin-top:24px}.app-article-metrics-bar p{margin:0}.app-article-masthead{display:flex;flex-direction:column;gap:16px 16px;padding:16px 0 24px}.app-article-masthead__info{display:flex;flex-direction:column;flex-grow:1}.app-article-masthead__brand{border-top:1px solid hsla(0,0%,100%,.8);display:flex;flex-direction:column;flex-shrink:0;gap:8px 8px;min-height:96px;padding:16px 0 0}.app-article-masthead__brand img{border:1px solid #fff;border-radius:8px;box-shadow:0 4px 15px 0 hsla(0,0%,50%,.25);height:auto;left:0;position:absolute;width:72px}.app-article-masthead__journal-link{display:block;font-size:1.125rem;font-weight:700;margin:0 0 8px;max-width:400px;padding:0 0 0 88px;position:relative}.app-article-masthead__journal-title{-webkit-box-orient:vertical;-webkit-line-clamp:3;display:-webkit-box;overflow:hidden}.app-article-masthead__submission-link{align-items:center;display:flex;font-size:1rem;gap:4px 4px;margin:0 0 0 88px}.app-article-masthead__access{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;font-weight:300;gap:4px 4px;margin:0}.app-article-masthead__buttons{display:flex;flex-flow:column wrap;gap:16px 16px}.app-article-masthead__access svg,.app-masthead--pastel .c-pdf-download .u-button--primary svg,.app-masthead--pastel .c-pdf-download .u-button--secondary svg,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary svg,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary svg{fill:currentcolor}.app-article-masthead a{color:#fff}.app-masthead--pastel .c-pdf-download .u-button--primary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary{background-color:#025e8d;background-image:none;border:2px solid transparent;box-shadow:none;color:#fff;font-weight:700}.app-masthead--pastel .c-pdf-download .u-button--primary:visited,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:visited{color:#fff}.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{text-decoration:none}.app-masthead--pastel .c-pdf-download .u-button--primary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.app-masthead--pastel .c-pdf-download .u-button--primary:focus,.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{background-color:#fff;background-image:none;color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{background:0 0;border:2px solid #025e8d;box-shadow:none;color:#025e8d}.app-masthead--pastel .c-pdf-download .u-button--secondary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary{background:0 0;border:2px solid #025e8d;color:#025e8d;font-weight:700}.app-masthead--pastel .c-pdf-download .u-button--secondary:visited,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:visited{color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--secondary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:hover{background-color:#01324b;background-color:#025e8d;border:2px solid transparent;box-shadow:none;color:#fff}.app-masthead--pastel .c-pdf-download .u-button--secondary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:focus{background-color:#fff;background-image:none;border:4px solid #fc0;color:#01324b}@media only screen and (min-width:768px){.app-article-masthead{flex-direction:row;gap:64px 64px;padding:24px 0}.app-article-masthead__brand{border:0;padding:0}.app-article-masthead__brand img{height:auto;position:static;width:auto}.app-article-masthead__buttons{align-items:center;flex-direction:row;margin-top:auto}.app-article-masthead__journal-link{display:flex;flex-direction:column;gap:24px 24px;margin:0 0 8px;padding:0}.app-article-masthead__submission-link{margin:0}}@media only screen and (min-width:1024px){.app-article-masthead__brand{flex-basis:400px}}.app-article-masthead .c-article-identifiers{font-size:.875rem;font-weight:300;line-height:1;margin:0 0 8px;overflow:hidden;padding:0}.app-article-masthead .c-article-identifiers--cite-list{margin:0 0 16px}.app-article-masthead .c-article-identifiers *{color:#fff}.app-article-masthead .c-cod{display:none}.app-article-masthead .c-article-identifiers__item{border-left:1px solid #fff;border-right:0;margin:0 17px 8px -9px;padding:0 0 0 8px}.app-article-masthead .c-article-identifiers__item--cite{border-left:0}.app-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;padding:16px 0 0;row-gap:24px}.app-article-metrics-bar__item{padding:0 16px 0 0}.app-article-metrics-bar__count{font-weight:700}.app-article-metrics-bar__label{font-weight:400;padding-left:4px}.app-article-metrics-bar__icon{height:auto;margin-right:4px;margin-top:-4px;width:auto}.app-article-metrics-bar__arrow-icon{margin:4px 0 0 4px}.app-article-metrics-bar a{color:#000}.app-article-metrics-bar .app-article-metrics-bar__item--metrics{padding-right:0}.app-overview-section .c-article-author-list,.app-overview-section__authors{line-height:2}.app-article-metrics-bar{margin-top:8px}.c-book-toc-pagination+.c-book-section__back-to-top{margin-top:0}.c-article-body .c-article-access-provider__text--chapter{color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;padding:20px 0}.c-article-body .c-article-access-provider__text--chapter svg.c-status-message__icon{fill:#003f8d;vertical-align:middle}.c-article-body-section__content--separator{padding-top:40px}.c-pdf-download__link{max-height:44px}.app-article-access .u-button--primary,.app-article-access .u-button--primary:visited{color:#fff}.c-article-sidebar{display:none}@media only screen and (min-width:1024px){.c-article-sidebar{display:block}}.c-cod__form{border-radius:12px}.c-cod__label{font-size:.875rem}.c-cod .c-status-message{align-items:center;justify-content:center;margin-bottom:16px;padding-bottom:16px}@media only screen and (min-width:1024px){.c-cod .c-status-message{align-items:inherit}}.c-cod .c-status-message__icon{margin-top:4px}.c-cod .c-cod__prompt{font-size:1rem;margin-bottom:16px}.c-article-body .app-article-access,.c-book-body .app-article-access{display:block}@media only screen and (min-width:1024px){.c-article-body .app-article-access,.c-book-body .app-article-access{display:none}}.c-article-body .app-card-service{margin-bottom:32px}@media only screen and (min-width:1024px){.c-article-body .app-card-service{display:none}}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary,.c-cod__row .u-button--primary{background-color:#025e8d;border:2px solid #025e8d;box-shadow:none;font-size:1rem;font-weight:700;gap:8px 8px;justify-content:center;line-height:1.5;padding:8px 24px}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary:hover,.c-cod__row .u-button--primary:hover{background-color:#fff;color:#025e8d}.app-article-access .buybox__buy .u-button--secondary:hover{background-color:#025e8d;color:#fff}.buybox__buy .c-notes__text{color:#666;font-size:.875rem;padding:0 16px 8px}.c-cod__input{flex-basis:auto;width:100%}.c-article-title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:2.25rem;font-weight:700;line-height:1.2;margin:12px 0}.c-reading-companion__figure-item figure{margin:0}@media only screen and (min-width:768px){.c-article-title{margin:16px 0}}.app-article-access{border:1px solid #c5e0f4;border-radius:12px}.app-article-access__heading{border-bottom:1px solid #c5e0f4;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1.125rem;font-weight:700;margin:0;padding:16px;text-align:center}.app-article-access .buybox__info svg{vertical-align:middle}.c-article-body .app-article-access p{margin-bottom:0}.app-article-access .buybox__info{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;margin:0}.app-article-access{margin:0 0 32px}@media only screen and (min-width:1024px){.app-article-access{margin:0 0 24px}}.c-status-message{font-size:1rem}.c-article-body{font-size:1.125rem}.c-article-body dl,.c-article-body ol,.c-article-body p,.c-article-body ul{margin-bottom:32px;margin-top:0}.c-article-access-provider__text:last-of-type,.c-article-body .c-notes__text:last-of-type{margin-bottom:0}.c-article-body ol p,.c-article-body ul p{margin-bottom:16px}.c-article-section__figure-caption{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-reading-companion__figure-item{border-top-color:#c5e0f4}.c-reading-companion__sticky{max-width:400px}.c-article-section .c-article-section__figure-description>*{font-size:1rem;margin-bottom:16px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;padding:16px 0}.c-reading-companion__reference-item:first-child{padding-top:0}.c-article-share-box__button,.js .c-article-authors-search__item .c-article-button{background:0 0;border:2px solid #025e8d;border-radius:32px;box-shadow:none;color:#025e8d;font-size:1rem;font-weight:700;line-height:1.5;margin:0;padding:8px 24px;transition:all .2s ease 0s}.c-article-authors-search__item .c-article-button{width:100%}.c-pdf-download .u-button{background-color:#fff;border:2px solid #fff;color:#01324b;justify-content:center}.c-context-bar__container .c-pdf-download .u-button svg,.c-pdf-download .u-button svg{fill:currentcolor}.c-pdf-download .u-button:visited{color:#01324b}.c-pdf-download .u-button:hover{border:4px solid #01324b;box-shadow:none}.c-pdf-download .u-button:focus,.c-pdf-download .u-button:hover{background-color:#01324b}.c-pdf-download .u-button:focus svg path,.c-pdf-download .u-button:hover svg path{fill:#fff}.c-context-bar__container .c-pdf-download .u-button{background-image:none;border:2px solid;color:#fff}.c-context-bar__container .c-pdf-download .u-button:visited{color:#fff}.c-context-bar__container .c-pdf-download .u-button:hover{text-decoration:none}.c-context-bar__container .c-pdf-download .u-button:focus{box-shadow:none;outline:0;text-decoration:none}.c-context-bar__container .c-pdf-download .u-button:focus,.c-context-bar__container .c-pdf-download .u-button:hover{background-color:#fff;background-image:none;color:#01324b}.c-context-bar__container .c-pdf-download .u-button:focus svg path,.c-context-bar__container .c-pdf-download .u-button:hover svg path{fill:#01324b}.c-context-bar__container .c-pdf-download .u-button,.c-pdf-download .u-button{box-shadow:none;font-size:1rem;font-weight:700;line-height:1.5;padding:8px 24px}.c-context-bar__container .c-pdf-download .u-button{background-color:#025e8d}.c-pdf-download .u-button:hover{border:2px solid #fff}.c-pdf-download .u-button:focus,.c-pdf-download .u-button:hover{background:0 0;box-shadow:none;color:#fff}.c-context-bar__container .c-pdf-download .u-button:hover{border:2px solid #025e8d;box-shadow:none;color:#025e8d}.c-context-bar__container .c-pdf-download .u-button:focus,.c-pdf-download .u-button:focus{border:2px solid #025e8d}.c-article-share-box__button:focus:focus,.c-article__pill-button:focus:focus,.c-context-bar__container .c-pdf-download .u-button:focus:focus,.c-pdf-download .u-button:focus:focus{outline:3px solid #08c;will-change:transform}.c-pdf-download__link .u-icon{padding-top:0}.c-bibliographic-information__column button{margin-bottom:16px}.c-article-body .c-article-author-affiliation__list p,.c-article-body .c-article-author-information__list p,figure{margin:0}.c-article-share-box__button{margin-right:16px}.c-status-message--boxed{border-radius:12px}.c-article-associated-content__collection-title{font-size:1rem}.app-card-service__description,.c-article-body .app-card-service__description{color:#222;margin-bottom:0;margin-top:8px}.app-article-access__subscriptions a,.app-article-access__subscriptions a:visited,.app-book-series-listing__item a,.app-book-series-listing__item a:hover,.app-book-series-listing__item a:visited,.c-article-author-list a,.c-article-author-list a:visited,.c-article-buy-box a,.c-article-buy-box a:visited,.c-article-peer-review a,.c-article-peer-review a:visited,.c-article-satellite-subtitle a,.c-article-satellite-subtitle a:visited,.c-breadcrumbs__link,.c-breadcrumbs__link:hover,.c-breadcrumbs__link:visited{color:#000}.c-article-author-list svg{height:24px;margin:0 0 0 6px;width:24px}.c-article-header{margin-bottom:32px}@media only screen and (min-width:876px){.js .c-ad--conditional{display:block}}.u-lazy-ad-wrapper{background-color:#fff;display:none;min-height:149px}@media only screen and (min-width:876px){.u-lazy-ad-wrapper{display:block}}p.c-ad__label{margin-bottom:4px}.c-ad--728x90{background-color:#fff;border-bottom:2px solid #cedbe0} } </style>\n    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .eds-c-header__brand img{height:24px;width:203px}.app-article-masthead__journal-link img{height:93px;width:72px}@media only screen and (min-width:769px){.app-article-masthead__journal-link img{height:161px;width:122px}} } </style>\n\n\n\n        \n        <link rel=\"stylesheet\" data-test=\"critical-css-handler\" data-inline-css-source=\"critical-css\" href=/oscar-static/app-springerlink/css/core-darwin-bcb58a5f76.css media=\"print\" onload=\"this.media='all';this.onload=null\">\n        <link rel=\"stylesheet\" data-test=\"critical-css-handler\" data-inline-css-source=\"critical-css\" href=\"/oscar-static/app-springerlink/css/enhanced-darwin-article-d61cdad169.css\" media=\"print\" onload=\"this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null\">\n    \n\n\n        \n        \n    <script type=\"text/javascript\">\n        config = {\n            env: 'live',\n            site: '10676.springer.com',\n            siteWithPath: '10676.springer.com' + window.location.pathname,\n            twitterHashtag: '10676',\n            cmsPrefix: 'https://studio-cms.springernature.com/studio/',\n            \n            \n            \n            \n            publisherBrand: 'Springer',\n            mustardcut: false\n        };\n    </script>\n\n        \n                \n\n\n\n\n    <script>\n        window.dataLayer = [{\"GA Key\":\"UA-26408784-1\",\"DOI\":\"10.1007/s10676-024-09775-5\",\"Page\":\"article\",\"springerJournal\":true,\"Publishing Model\":\"Hybrid Access\",\"page\":{\"attributes\":{\"environment\":\"live\"}},\"Country\":\"US\",\"japan\":false,\"doi\":\"10.1007-s10676-024-09775-5\",\"Journal Id\":10676,\"Journal Title\":\"Ethics and Information Technology\",\"imprint\":\"Springer\",\"Keywords\":\"Artificial intelligence, Large language models, LLMs, ChatGPT, Bullshit, Frankfurt, Assertion, Content\",\"kwrd\":[\"Artificial_intelligence\",\"Large_language_models\",\"LLMs\",\"ChatGPT\",\"Bullshit\",\"Frankfurt\",\"Assertion\",\"Content\"],\"Labs\":\"Y\",\"ksg\":\"Krux.segments\",\"kuid\":\"Krux.uid\",\"Has Body\":\"Y\",\"Features\":[],\"Open Access\":\"Y\",\"hasAccess\":\"Y\",\"bypassPaywall\":\"N\",\"user\":{\"license\":{\"businessPartnerID\":[],\"businessPartnerIDString\":\"\"}},\"Access Type\":\"open\",\"Bpids\":\"\",\"Bpnames\":\"\",\"BPID\":[\"1\"],\"VG Wort Identifier\":\"vgzm.415900-10.1007-s10676-024-09775-5\",\"Full HTML\":\"Y\",\"Subject Codes\":[\"SCI\",\"SCI24067\",\"SC518000\",\"SCE14000\",\"SCI18067\",\"SC422000\"],\"pmc\":[\"I\",\"I24067\",\"518000\",\"E14000\",\"I18067\",\"422000\"],\"session\":{\"authentication\":{\"loginStatus\":\"N\"},\"attributes\":{\"edition\":\"academic\"}},\"content\":{\"serial\":{\"eissn\":\"1572-8439\",\"pissn\":\"1388-1957\"},\"type\":\"Article\",\"category\":{\"pmc\":{\"primarySubject\":\"Computer Science\",\"primarySubjectCode\":\"I\",\"secondarySubjects\":{\"1\":\"Management of Computing and Information Systems\",\"2\":\"Innovation/Technology Management\",\"3\":\"Ethics\",\"4\":\"User Interfaces and Human Computer Interaction\",\"5\":\"Library Science\"},\"secondarySubjectCodes\":{\"1\":\"I24067\",\"2\":\"518000\",\"3\":\"E14000\",\"4\":\"I18067\",\"5\":\"422000\"}},\"sucode\":\"SC6\",\"articleType\":\"Original Paper\"},\"attributes\":{\"deliveryPlatform\":\"oscar\"}},\"Event Category\":\"Article\"}];\n    </script>\n\n\n\n\n\n\n\n\n\n\n\n    <script data-test=\"springer-link-article-datalayer\">\n        window.dataLayer = window.dataLayer || [];\n        window.dataLayer.push({\n            ga4MeasurementId: 'G-B3E4QL2TPR',\n            ga360TrackingId: 'UA-26408784-1',\n            twitterId: 'o47a7',\n            baiduId: 'aef3043f025ccf2305af8a194652d70b',\n            ga4ServerUrl: 'https://collect.springer.com',\n            imprint: 'springerlink',\n                page: {\n                    attributes:{\n                        featureFlags: [{ name: 'darwin-orion', active: true }, { name: 'chapter-books-recs', active: true } ],\n                        darwinAvailable: true\n                    }\n                }\n            \n        });\n    </script>\n\n\n\n        \n        <script>\n    (function(w, d) {\n        w.config = w.config || {};\n        w.config.mustardcut = false;\n\n        \n        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {\n            w.config.mustardcut = true;\n            d.classList.add('js');\n            d.classList.remove('grade-c');\n            d.classList.remove('no-js');\n        }\n    })(window, document.documentElement);\n</script>\n\n\n        <script class=\"js-entry\">\n    if (window.config.mustardcut) {\n        (function(w, d) {\n            \n            \n            \n                window.Component = {};\n                window.suppressShareButton = false;\n                window.onArticlePage = true;\n            \n\n            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');\n\n            \n            function catchNoModuleSupport() {\n                var scriptEl = d.createElement('script');\n                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)\n            }\n\n            var headScripts = [\n                {'src': '/oscar-static/js/polyfill-es5-bundle-af4ff2409f.js', 'async': false},\n                {'src': '/oscar-static/js/airbrake-es5-bundle-2fde035cf2.js', 'async': false},\n            ];\n\n            var bodyScripts = [\n                \n                    {'src': '/oscar-static/js/global-article-es5-bundle-4c0a163a91.js', 'async': false, 'module': false},\n                    {'src': '/oscar-static/js/global-article-es6-bundle-27c8ba5249.js', 'async': false, 'module': true}\n                \n                \n                    \n                \n                \n                \n            ];\n\n            function createScript(script) {\n                var scriptEl = d.createElement('script');\n                scriptEl.src = script.src;\n                scriptEl.async = script.async;\n                if (script.module === true) {\n                    scriptEl.type = \"module\";\n                    if (catchNoModuleSupport()) {\n                        scriptEl.src = '';\n                    }\n                } else if (script.module === false) {\n                    scriptEl.setAttribute('nomodule', true)\n                }\n                if (script.charset) {\n                    scriptEl.setAttribute('charset', script.charset);\n                }\n\n                return scriptEl;\n            }\n\n            for (var i = 0; i < headScripts.length; ++i) {\n                var scriptEl = createScript(headScripts[i]);\n                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);\n            }\n\n            d.addEventListener('DOMContentLoaded', function() {\n                for (var i = 0; i < bodyScripts.length; ++i) {\n                    var scriptEl = createScript(bodyScripts[i]);\n                    d.body.appendChild(scriptEl);\n                }\n            });\n\n            // Webfont repeat view\n            var config = w.config;\n            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {\n                d.documentElement.className += ' webfonts-loaded';\n            }\n        })(window, document);\n    }\n</script>\n\n\n        \n            \n            \n                \n<script data-test=\"gtm-head\">\n    window.initGTM = function() {\n        if (window.config.mustardcut) {\n            (function (w, d, s, l, i) {\n                w[l] = w[l] || [];\n                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});\n                var f = d.getElementsByTagName(s)[0],\n                    j = d.createElement(s),\n                    dl = l != 'dataLayer' ? '&l=' + l : '';\n                j.async = true;\n                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;\n                f.parentNode.insertBefore(j, f);\n            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');\n        }\n    }\n</script>\n\n            \n            \n            \n        \n\n        <script>\n(function (w, d, t) {\n    function cc() {\n        var h = w.location.hostname;\n        var e = d.createElement(t),\n        s = d.getElementsByTagName(t)[0];\n\n        \n        if (h.indexOf('springer.com') > -1 && h.indexOf('biomedcentral.com') === -1 && h.indexOf('springeropen.com') === -1) {\n            if (h.indexOf('link-qa.springer.com') > -1 || h.indexOf('test-www.springer.com') > -1) {\n                e.src = 'https://cmp.springer.com/production_live/en/consent-bundle-17-47.js';\n                e.setAttribute('onload', \"initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')\");\n            } else {\n                e.src = 'https://cmp.springer.com/production_live/en/consent-bundle-17-47.js';\n                e.setAttribute('onload', \"initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')\");\n            }\n        } else if (h.indexOf('biomedcentral.com') > -1) {\n            if (h.indexOf('biomedcentral.com.qa') > -1) {\n                e.src = 'https://cmp.biomedcentral.com/production_live/en/consent-bundle-15-35.js';\n                e.setAttribute('onload', \"initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')\");\n            } else {\n                e.src = 'https://cmp.biomedcentral.com/production_live/en/consent-bundle-15-35.js';\n                e.setAttribute('onload', \"initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')\");\n            }\n        } else if (h.indexOf('springeropen.com') > -1) {\n            if (h.indexOf('springeropen.com.qa') > -1) {\n                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-16-33.js';\n                e.setAttribute('onload', \"initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')\");\n            } else {\n                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-16-33.js';\n                e.setAttribute('onload', \"initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')\");\n            }\n        } else if (h.indexOf('springernature.com') > -1) {\n            if (h.indexOf('beta-qa.springernature.com') > -1) {\n                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-49-43.js';\n                e.setAttribute('onload', \"initGTM(window,document,'script','dataLayer','GTM-NK22KLS')\");\n            } else {\n                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-49-43.js';\n                e.setAttribute('onload', \"initGTM(window,document,'script','dataLayer','GTM-NK22KLS')\");\n            }\n        } else {\n            e.src = '/oscar-static/js/cookie-consent-es5-bundle-26e142e9c6.js';\n            e.setAttribute('data-consent', h);\n        }\n        s.insertAdjacentElement('afterend', e);\n    }\n\n    cc();\n})(window, document, 'script');\n</script>\n\n\n        \n        \n        \n    \n        \n    \n\n\n        \n    \n    <link rel=\"canonical\" href=\"https://link.springer.com/article/10.1007/s10676-024-09775-5\"/>\n    \n\n        \n        \n        \n        \n\n        \n    <script type=\"application/ld+json\">{\"mainEntity\":{\"headline\":\"ChatGPT is bullshit\",\"description\":\"Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called “AI hallucinations”. We argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. We distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.\",\"datePublished\":\"2024-06-08T00:00:00Z\",\"dateModified\":\"2024-06-08T00:00:00Z\",\"pageStart\":\"1\",\"pageEnd\":\"10\",\"license\":\"http://creativecommons.org/licenses/by/4.0/\",\"sameAs\":\"https://doi.org/10.1007/s10676-024-09775-5\",\"keywords\":[\"Artificial intelligence\",\"Large language models\",\"LLMs\",\"ChatGPT\",\"Bullshit\",\"Frankfurt\",\"Assertion\",\"Content\",\"Management of Computing and Information Systems\",\"Innovation/Technology Management\",\"Ethics\",\"User Interfaces and Human Computer Interaction\",\"Library Science\"],\"image\":[],\"isPartOf\":{\"name\":\"Ethics and Information Technology\",\"issn\":[\"1572-8439\",\"1388-1957\"],\"volumeNumber\":\"26\",\"@type\":[\"Periodical\",\"PublicationVolume\"]},\"publisher\":{\"name\":\"Springer Netherlands\",\"logo\":{\"url\":\"https://www.springernature.com/app-sn/public/images/logo-springernature.png\",\"@type\":\"ImageObject\"},\"@type\":\"Organization\"},\"author\":[{\"name\":\"Michael Townsen Hicks\",\"url\":\"http://orcid.org/0000-0002-1304-5668\",\"affiliation\":[{\"name\":\"University of Glasgow\",\"address\":{\"name\":\"University of Glasgow, Glasgow, Scotland\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"email\":\"Michael.hicks@glasgow.ac.uk\",\"@type\":\"Person\"},{\"name\":\"James Humphries\",\"affiliation\":[{\"name\":\"University of Glasgow\",\"address\":{\"name\":\"University of Glasgow, Glasgow, Scotland\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"},{\"name\":\"Joe Slater\",\"affiliation\":[{\"name\":\"University of Glasgow\",\"address\":{\"name\":\"University of Glasgow, Glasgow, Scotland\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"}],\"isAccessibleForFree\":true,\"@type\":\"ScholarlyArticle\"},\"@context\":\"https://schema.org\",\"@type\":\"WebPage\"}</script>\n\n        \n\n    </head>\n\n    <body class=\"\"\n    \n          >\n        \n    \n        \n            <!-- Google Tag Manager (noscript) -->\n            <noscript>\n                <iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ\"\n                        height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe>\n            </noscript>\n            <!-- End Google Tag Manager (noscript) -->\n        \n    \n\n        \n    \n        \n            <!-- Google Tag Manager (noscript) -->\n            <noscript data-test=\"gtm-body\">\n                <iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ\"\n                height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe>\n            </noscript>\n            <!-- End Google Tag Manager (noscript) -->\n        \n    \n\n\n        <div class=\"u-visually-hidden\" aria-hidden=\"true\" data-test=\"darwin-icons\">\n    <?xml version=\"1.0\" encoding=\"UTF-8\"?><!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><symbol id=\"icon-eds-i-accesses-medium\" viewBox=\"0 0 24 24\"><path d=\"M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742H15a1 1 0 0 1 0-2h4.455a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM8 13c2.052 0 4.66 1.61 6.36 3.4l.124.141c.333.41.516.925.516 1.459 0 .6-.232 1.178-.64 1.599C12.666 21.388 10.054 23 8 23c-2.052 0-4.66-1.61-6.353-3.393A2.31 2.31 0 0 1 1 18c0-.6.232-1.178.64-1.6C3.34 14.61 5.948 13 8 13Zm0 2c-1.369 0-3.552 1.348-4.917 2.785A.31.31 0 0 0 3 18c0 .083.031.161.09.222C4.447 19.652 6.631 21 8 21c1.37 0 3.556-1.35 4.917-2.785A.31.31 0 0 0 13 18a.32.32 0 0 0-.048-.17l-.042-.052C11.553 16.348 9.369 15 8 15Zm0 1a2 2 0 1 1 0 4 2 2 0 0 1 0-4Z\"/></symbol><symbol id=\"icon-eds-i-altmetric-medium\" viewBox=\"0 0 24 24\"><path d=\"M12 1c5.978 0 10.843 4.77 10.996 10.712l.004.306-.002.022-.002.248C22.843 18.23 17.978 23 12 23 5.925 23 1 18.075 1 12S5.925 1 12 1Zm-1.726 9.246L8.848 12.53a1 1 0 0 1-.718.461L8.003 13l-4.947.014a9.001 9.001 0 0 0 17.887-.001L16.553 13l-2.205 3.53a1 1 0 0 1-1.735-.068l-.05-.11-2.289-6.106ZM12 3a9.001 9.001 0 0 0-8.947 8.013l4.391-.012L9.652 7.47a1 1 0 0 1 1.784.179l2.288 6.104 1.428-2.283a1 1 0 0 1 .722-.462l.129-.008 4.943.012A9.001 9.001 0 0 0 12 3Z\"/></symbol><symbol id=\"icon-eds-i-arrow-bend-down-medium\" viewBox=\"0 0 24 24\"><path d=\"m11.852 20.989.058.007L12 21l.075-.003.126-.017.111-.03.111-.044.098-.052.104-.074.082-.073 6-6a1 1 0 0 0-1.414-1.414L13 17.585v-12.2C13 4.075 11.964 3 10.667 3H4a1 1 0 1 0 0 2h6.667c.175 0 .333.164.333.385v12.2l-4.293-4.292a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414l6 6c.035.036.073.068.112.097l.11.071.114.054.105.035.118.025Z\"/></symbol><symbol id=\"icon-eds-i-arrow-bend-down-small\" viewBox=\"0 0 16 16\"><path d=\"M1 2a1 1 0 0 0 1 1h5v8.585L3.707 8.293a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414l5 5 .063.059.093.069.081.048.105.048.104.035.105.022.096.01h.136l.122-.018.113-.03.103-.04.1-.053.102-.07.052-.043 5.04-5.037a1 1 0 1 0-1.415-1.414L9 11.583V3a2 2 0 0 0-2-2H2a1 1 0 0 0-1 1Z\"/></symbol><symbol id=\"icon-eds-i-arrow-bend-up-medium\" viewBox=\"0 0 24 24\"><path d=\"m11.852 3.011.058-.007L12 3l.075.003.126.017.111.03.111.044.098.052.104.074.082.073 6 6a1 1 0 1 1-1.414 1.414L13 6.415v12.2C13 19.925 11.964 21 10.667 21H4a1 1 0 0 1 0-2h6.667c.175 0 .333-.164.333-.385v-12.2l-4.293 4.292a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l6-6c.035-.036.073-.068.112-.097l.11-.071.114-.054.105-.035.118-.025Z\"/></symbol><symbol id=\"icon-eds-i-arrow-bend-up-small\" viewBox=\"0 0 16 16\"><path d=\"M1 13.998a1 1 0 0 1 1-1h5V4.413L3.707 7.705a1 1 0 0 1-1.32.084l-.094-.084a1 1 0 0 1 0-1.414l5-5 .063-.059.093-.068.081-.05.105-.047.104-.035.105-.022L7.94 1l.136.001.122.017.113.03.103.04.1.053.102.07.052.043 5.04 5.037a1 1 0 1 1-1.415 1.414L9 4.415v8.583a2 2 0 0 1-2 2H2a1 1 0 0 1-1-1Z\"/></symbol><symbol id=\"icon-eds-i-arrow-diagonal-medium\" viewBox=\"0 0 24 24\"><path d=\"M14 3h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L21 4v6a1 1 0 0 1-2 0V6.414l-4.293 4.293a1 1 0 0 1-1.414-1.414L17.584 5H14a1 1 0 0 1-.993-.883L13 4a1 1 0 0 1 1-1ZM4 13a1 1 0 0 1 1 1v3.584l4.293-4.291a1 1 0 1 1 1.414 1.414L6.414 19H10a1 1 0 0 1 .993.883L11 20a1 1 0 0 1-1 1l-6.075-.003-.126-.017-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08a1.01 1.01 0 0 1-.097-.112l-.071-.11-.054-.114-.035-.105-.025-.118-.007-.058L3 20v-6a1 1 0 0 1 1-1Z\"/></symbol><symbol id=\"icon-eds-i-arrow-diagonal-small\" viewBox=\"0 0 16 16\"><path d=\"m2 15-.082-.004-.119-.016-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08a1.008 1.008 0 0 1-.097-.112l-.071-.11-.031-.062-.034-.081-.024-.076-.025-.118-.007-.058L1 14.02V9a1 1 0 1 1 2 0v2.584l2.793-2.791a1 1 0 1 1 1.414 1.414L4.414 13H7a1 1 0 0 1 .993.883L8 14a1 1 0 0 1-1 1H2ZM14 1l.081.003.12.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.031.062.034.081.024.076.03.148L15 2v5a1 1 0 0 1-2 0V4.414l-2.96 2.96A1 1 0 1 1 8.626 5.96L11.584 3H9a1 1 0 0 1-.993-.883L8 2a1 1 0 0 1 1-1h5Z\"/></symbol><symbol id=\"icon-eds-i-arrow-down-medium\" viewBox=\"0 0 24 24\"><path d=\"m20.707 12.728-7.99 7.98a.996.996 0 0 1-.561.281l-.157.011a.998.998 0 0 1-.788-.384l-7.918-7.908a1 1 0 0 1 1.414-1.416L11 17.576V4a1 1 0 0 1 2 0v13.598l6.293-6.285a1 1 0 0 1 1.32-.082l.095.083a1 1 0 0 1-.001 1.414Z\"/></symbol><symbol id=\"icon-eds-i-arrow-down-small\" viewBox=\"0 0 16 16\"><path d=\"m1.293 8.707 6 6 .063.059.093.069.081.048.105.049.104.034.056.013.118.017L8 15l.076-.003.122-.017.113-.03.085-.032.063-.03.098-.058.06-.043.05-.043 6.04-6.037a1 1 0 0 0-1.414-1.414L9 11.583V2a1 1 0 1 0-2 0v9.585L2.707 7.293a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414Z\"/></symbol><symbol id=\"icon-eds-i-arrow-left-medium\" viewBox=\"0 0 24 24\"><path d=\"m11.272 3.293-7.98 7.99a.996.996 0 0 0-.281.561L3 12.001c0 .32.15.605.384.788l7.908 7.918a1 1 0 0 0 1.416-1.414L6.424 13H20a1 1 0 0 0 0-2H6.402l6.285-6.293a1 1 0 0 0 .082-1.32l-.083-.095a1 1 0 0 0-1.414.001Z\"/></symbol><symbol id=\"icon-eds-i-arrow-left-small\" viewBox=\"0 0 16 16\"><path d=\"m7.293 1.293-6 6-.059.063-.069.093-.048.081-.049.105-.034.104-.013.056-.017.118L1 8l.003.076.017.122.03.113.032.085.03.063.058.098.043.06.043.05 6.037 6.04a1 1 0 0 0 1.414-1.414L4.417 9H14a1 1 0 0 0 0-2H4.415l4.292-4.293a1 1 0 0 0 .083-1.32l-.083-.094a1 1 0 0 0-1.414 0Z\"/></symbol><symbol id=\"icon-eds-i-arrow-right-medium\" viewBox=\"0 0 24 24\"><path d=\"m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z\"/></symbol><symbol id=\"icon-eds-i-arrow-right-small\" viewBox=\"0 0 16 16\"><path d=\"m8.707 1.293 6 6 .059.063.069.093.048.081.049.105.034.104.013.056.017.118L15 8l-.003.076-.017.122-.03.113-.032.085-.03.063-.058.098-.043.06-.043.05-6.037 6.04a1 1 0 0 1-1.414-1.414L11.583 9H2a1 1 0 1 1 0-2h9.585L7.293 2.707a1 1 0 0 1-.083-1.32l.083-.094a1 1 0 0 1 1.414 0Z\"/></symbol><symbol id=\"icon-eds-i-arrow-up-medium\" viewBox=\"0 0 24 24\"><path d=\"m3.293 11.272 7.99-7.98a.996.996 0 0 1 .561-.281L12.001 3c.32 0 .605.15.788.384l7.918 7.908a1 1 0 0 1-1.414 1.416L13 6.424V20a1 1 0 0 1-2 0V6.402l-6.293 6.285a1 1 0 0 1-1.32.082l-.095-.083a1 1 0 0 1 .001-1.414Z\"/></symbol><symbol id=\"icon-eds-i-arrow-up-small\" viewBox=\"0 0 16 16\"><path d=\"m1.293 7.293 6-6 .063-.059.093-.069.081-.048.105-.049.104-.034.056-.013.118-.017L8 1l.076.003.122.017.113.03.085.032.063.03.098.058.06.043.05.043 6.04 6.037a1 1 0 0 1-1.414 1.414L9 4.417V14a1 1 0 0 1-2 0V4.415L2.707 8.707a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414Z\"/></symbol><symbol id=\"icon-eds-i-article-medium\" viewBox=\"0 0 24 24\"><path d=\"M8 7a1 1 0 0 0 0 2h4a1 1 0 1 0 0-2H8ZM8 11a1 1 0 1 0 0 2h8a1 1 0 1 0 0-2H8ZM7 16a1 1 0 0 1 1-1h8a1 1 0 1 1 0 2H8a1 1 0 0 1-1-1Z\"/><path d=\"M5.545 1A2.542 2.542 0 0 0 3 3.538v16.924A2.542 2.542 0 0 0 5.545 23h12.91A2.542 2.542 0 0 0 21 20.462V3.5A2.5 2.5 0 0 0 18.5 1H5.545ZM5 3.538C5 3.245 5.24 3 5.545 3H18.5a.5.5 0 0 1 .5.5v16.962c0 .293-.24.538-.546.538H5.545A.542.542 0 0 1 5 20.462V3.538Z\" clip-rule=\"evenodd\"/></symbol><symbol id=\"icon-eds-i-book-medium\" viewBox=\"0 0 24 24\"><path d=\"M18.5 1A2.5 2.5 0 0 1 21 3.5v12c0 1.16-.79 2.135-1.86 2.418l-.14.031V21h1a1 1 0 0 1 .993.883L21 22a1 1 0 0 1-1 1H6.5A3.5 3.5 0 0 1 3 19.5v-15A3.5 3.5 0 0 1 6.5 1h12ZM17 18H6.5a1.5 1.5 0 0 0-1.493 1.356L5 19.5A1.5 1.5 0 0 0 6.5 21H17v-3Zm1.5-15h-12A1.5 1.5 0 0 0 5 4.5v11.837l.054-.025a3.481 3.481 0 0 1 1.254-.307L6.5 16h12a.5.5 0 0 0 .492-.41L19 15.5v-12a.5.5 0 0 0-.5-.5ZM15 6a1 1 0 0 1 0 2H9a1 1 0 1 1 0-2h6Z\"/></symbol><symbol id=\"icon-eds-i-book-series-medium\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M1 3.786C1 2.759 1.857 2 2.82 2H6.18c.964 0 1.82.759 1.82 1.786V4h3.168c.668 0 1.298.364 1.616.938.158-.109.333-.195.523-.252l3.216-.965c.923-.277 1.962.204 2.257 1.187l4.146 13.82c.296.984-.307 1.957-1.23 2.234l-3.217.965c-.923.277-1.962-.203-2.257-1.187L13 10.005v10.21c0 1.04-.878 1.785-1.834 1.785H7.833c-.291 0-.575-.07-.83-.195A1.849 1.849 0 0 1 6.18 22H2.821C1.857 22 1 21.241 1 20.214V3.786ZM3 4v11h3V4H3Zm0 16v-3h3v3H3Zm15.075-.04-.814-2.712 2.874-.862.813 2.712-2.873.862Zm1.485-5.49-2.874.862-2.634-8.782 2.873-.862 2.635 8.782ZM8 20V6h3v14H8Z\" clip-rule=\"evenodd\"/></symbol><symbol id=\"icon-eds-i-calendar-acceptance-medium\" viewBox=\"0 0 24 24\"><path d=\"M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-.534 7.747a1 1 0 0 1 .094 1.412l-4.846 5.538a1 1 0 0 1-1.352.141l-2.77-2.076a1 1 0 0 1 1.2-1.6l2.027 1.519 4.236-4.84a1 1 0 0 1 1.411-.094ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z\"/></symbol><symbol id=\"icon-eds-i-calendar-date-medium\" viewBox=\"0 0 24 24\"><path d=\"M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1ZM8 15a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm-4-4a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z\"/></symbol><symbol id=\"icon-eds-i-calendar-decision-medium\" viewBox=\"0 0 24 24\"><path d=\"M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-2.935 8.246 2.686 2.645c.34.335.34.883 0 1.218l-2.686 2.645a.858.858 0 0 1-1.213-.009.854.854 0 0 1 .009-1.21l1.05-1.035H7.984a.992.992 0 0 1-.984-1c0-.552.44-1 .984-1h5.928l-1.051-1.036a.854.854 0 0 1-.085-1.121l.076-.088a.858.858 0 0 1 1.213-.009ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z\"/></symbol><symbol id=\"icon-eds-i-calendar-impact-factor-medium\" viewBox=\"0 0 24 24\"><path d=\"M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-3.2 6.924a.48.48 0 0 1 .125.544l-1.52 3.283h2.304c.27 0 .491.215.491.483a.477.477 0 0 1-.13.327l-4.18 4.484a.498.498 0 0 1-.69.031.48.48 0 0 1-.125-.544l1.52-3.284H9.291a.487.487 0 0 1-.491-.482c0-.121.047-.238.13-.327l4.18-4.484a.498.498 0 0 1 .69-.031ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z\"/></symbol><symbol id=\"icon-eds-i-call-papers-medium\" viewBox=\"0 0 24 24\"><g><path d=\"m20.707 2.883-1.414 1.414a1 1 0 0 0 1.414 1.414l1.414-1.414a1 1 0 0 0-1.414-1.414Z\"/><path d=\"M6 16.054c0 2.026 1.052 2.943 3 2.943a1 1 0 1 1 0 2c-2.996 0-5-1.746-5-4.943v-1.227a4.068 4.068 0 0 1-1.83-1.189 4.553 4.553 0 0 1-.87-1.455 4.868 4.868 0 0 1-.3-1.686c0-1.17.417-2.298 1.17-3.14.38-.426.834-.767 1.338-1 .51-.237 1.06-.36 1.617-.36L6.632 6H7l7.932-2.895A2.363 2.363 0 0 1 18 5.36v9.28a2.36 2.36 0 0 1-3.069 2.25l.084.03L7 14.997H6v1.057Zm9.637-11.057a.415.415 0 0 0-.083.008L8 7.638v5.536l7.424 1.786.104.02c.035.01.072.02.109.02.2 0 .363-.16.363-.36V5.36c0-.2-.163-.363-.363-.363Zm-9.638 3h-.874a1.82 1.82 0 0 0-.625.111l-.15.063a2.128 2.128 0 0 0-.689.517c-.42.47-.661 1.123-.661 1.81 0 .34.06.678.176.992.114.308.28.585.485.816.4.447.925.691 1.464.691h.874v-5Z\" clip-rule=\"evenodd\"/><path d=\"M20 8.997h2a1 1 0 1 1 0 2h-2a1 1 0 1 1 0-2ZM20.707 14.293l1.414 1.414a1 1 0 0 1-1.414 1.414l-1.414-1.414a1 1 0 0 1 1.414-1.414Z\"/></g></symbol><symbol id=\"icon-eds-i-card-medium\" viewBox=\"0 0 24 24\"><path d=\"M19.615 2c.315 0 .716.067 1.14.279.76.38 1.245 1.107 1.245 2.106v15.23c0 .315-.067.716-.279 1.14-.38.76-1.107 1.245-2.106 1.245H4.385a2.56 2.56 0 0 1-1.14-.279C2.485 21.341 2 20.614 2 19.615V4.385c0-.315.067-.716.279-1.14C2.659 2.485 3.386 2 4.385 2h15.23Zm0 2H4.385c-.213 0-.265.034-.317.14A.71.71 0 0 0 4 4.385v15.23c0 .213.034.265.14.317a.71.71 0 0 0 .245.068h15.23c.213 0 .265-.034.317-.14a.71.71 0 0 0 .068-.245V4.385c0-.213-.034-.265-.14-.317A.71.71 0 0 0 19.615 4ZM17 16a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h10Zm0-3a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h10Zm-.5-7A1.5 1.5 0 0 1 18 7.5v3a1.5 1.5 0 0 1-1.5 1.5h-9A1.5 1.5 0 0 1 6 10.5v-3A1.5 1.5 0 0 1 7.5 6h9ZM16 8H8v2h8V8Z\"/></symbol><symbol id=\"icon-eds-i-cart-medium\" viewBox=\"0 0 24 24\"><path d=\"M5.76 1a1 1 0 0 1 .994.902L7.155 6h13.34c.18 0 .358.02.532.057l.174.045a2.5 2.5 0 0 1 1.693 3.103l-2.069 7.03c-.36 1.099-1.398 1.823-2.49 1.763H8.65c-1.272.015-2.352-.927-2.546-2.244L4.852 3H2a1 1 0 0 1-.993-.883L1 2a1 1 0 0 1 1-1h3.76Zm2.328 14.51a.555.555 0 0 0 .55.488l9.751.001a.533.533 0 0 0 .527-.357l2.059-7a.5.5 0 0 0-.48-.642H7.351l.737 7.51ZM18 19a2 2 0 1 1 0 4 2 2 0 0 1 0-4ZM8 19a2 2 0 1 1 0 4 2 2 0 0 1 0-4Z\"/></symbol><symbol id=\"icon-eds-i-check-circle-medium\" viewBox=\"0 0 24 24\"><path d=\"M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm5.125 4.72a1 1 0 0 1 .156 1.405l-6 7.5a1 1 0 0 1-1.421.143l-3-2.5a1 1 0 0 1 1.28-1.536l2.217 1.846 5.362-6.703a1 1 0 0 1 1.406-.156Z\"/></symbol><symbol id=\"icon-eds-i-check-filled-medium\" viewBox=\"0 0 24 24\"><path d=\"M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm5.125 6.72a1 1 0 0 0-1.406.155l-5.362 6.703-2.217-1.846a1 1 0 1 0-1.28 1.536l3 2.5a1 1 0 0 0 1.42-.143l6-7.5a1 1 0 0 0-.155-1.406Z\"/></symbol><symbol id=\"icon-eds-i-chevron-down-medium\" viewBox=\"0 0 24 24\"><path d=\"M3.305 8.28a1 1 0 0 0-.024 1.415l7.495 7.762c.314.345.757.543 1.224.543.467 0 .91-.198 1.204-.522l7.515-7.783a1 1 0 1 0-1.438-1.39L12 15.845l-7.28-7.54A1 1 0 0 0 3.4 8.2l-.096.082Z\"/></symbol><symbol id=\"icon-eds-i-chevron-down-small\" viewBox=\"0 0 16 16\"><path d=\"M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z\"/></symbol><symbol id=\"icon-eds-i-chevron-left-medium\" viewBox=\"0 0 24 24\"><path d=\"M15.72 3.305a1 1 0 0 0-1.415-.024l-7.762 7.495A1.655 1.655 0 0 0 6 12c0 .467.198.91.522 1.204l7.783 7.515a1 1 0 1 0 1.39-1.438L8.155 12l7.54-7.28A1 1 0 0 0 15.8 3.4l-.082-.096Z\"/></symbol><symbol id=\"icon-eds-i-chevron-left-small\" viewBox=\"0 0 16 16\"><path d=\"M10.722 2.308a1 1 0 0 0-1.414-.03L4.49 6.897a1.491 1.491 0 0 0-.019 2.188l4.838 4.637a1 1 0 1 0 1.384-1.444L6.229 8l4.463-4.278a1 1 0 0 0 .111-1.318l-.081-.096Z\"/></symbol><symbol id=\"icon-eds-i-chevron-right-medium\" viewBox=\"0 0 24 24\"><path d=\"M8.28 3.305a1 1 0 0 1 1.415-.024l7.762 7.495c.345.314.543.757.543 1.224 0 .467-.198.91-.522 1.204l-7.783 7.515a1 1 0 1 1-1.39-1.438L15.845 12l-7.54-7.28A1 1 0 0 1 8.2 3.4l.082-.096Z\"/></symbol><symbol id=\"icon-eds-i-chevron-right-small\" viewBox=\"0 0 16 16\"><path d=\"M5.278 2.308a1 1 0 0 1 1.414-.03l4.819 4.619a1.491 1.491 0 0 1 .019 2.188l-4.838 4.637a1 1 0 1 1-1.384-1.444L9.771 8 5.308 3.722a1 1 0 0 1-.111-1.318l.081-.096Z\"/></symbol><symbol id=\"icon-eds-i-chevron-up-medium\" viewBox=\"0 0 24 24\"><path d=\"M20.695 15.72a1 1 0 0 0 .024-1.415l-7.495-7.762A1.655 1.655 0 0 0 12 6c-.467 0-.91.198-1.204.522l-7.515 7.783a1 1 0 1 0 1.438 1.39L12 8.155l7.28 7.54a1 1 0 0 0 1.319.106l.096-.082Z\"/></symbol><symbol id=\"icon-eds-i-chevron-up-small\" viewBox=\"0 0 16 16\"><path d=\"M13.692 10.722a1 1 0 0 0 .03-1.414L9.103 4.49a1.491 1.491 0 0 0-2.188-.019L2.278 9.308a1 1 0 0 0 1.444 1.384L8 6.229l4.278 4.463a1 1 0 0 0 1.318.111l.096-.081Z\"/></symbol><symbol id=\"icon-eds-i-citations-medium\" viewBox=\"0 0 24 24\"><path d=\"M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM5.483 14.35c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Zm5 0c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Z\"/></symbol><symbol id=\"icon-eds-i-clipboard-check-medium\" viewBox=\"0 0 24 24\"><path d=\"M14.4 1c1.238 0 2.274.865 2.536 2.024L18.5 3C19.886 3 21 4.14 21 5.535v14.93C21 21.86 19.886 23 18.5 23h-13C4.114 23 3 21.86 3 20.465V5.535C3 4.14 4.114 3 5.5 3h1.57c.27-1.147 1.3-2 2.53-2h4.8Zm4.115 4-1.59.024A2.601 2.601 0 0 1 14.4 7H9.6c-1.23 0-2.26-.853-2.53-2H5.5c-.27 0-.5.234-.5.535v14.93c0 .3.23.535.5.535h13c.27 0 .5-.234.5-.535V5.535c0-.3-.23-.535-.485-.535Zm-1.909 4.205a1 1 0 0 1 .19 1.401l-5.334 7a1 1 0 0 1-1.344.23l-2.667-1.75a1 1 0 1 1 1.098-1.672l1.887 1.238 4.769-6.258a1 1 0 0 1 1.401-.19ZM14.4 3H9.6a.6.6 0 0 0-.6.6v.8a.6.6 0 0 0 .6.6h4.8a.6.6 0 0 0 .6-.6v-.8a.6.6 0 0 0-.6-.6Z\"/></symbol><symbol id=\"icon-eds-i-clipboard-report-medium\" viewBox=\"0 0 24 24\"><path d=\"M14.4 1c1.238 0 2.274.865 2.536 2.024L18.5 3C19.886 3 21 4.14 21 5.535v14.93C21 21.86 19.886 23 18.5 23h-13C4.114 23 3 21.86 3 20.465V5.535C3 4.14 4.114 3 5.5 3h1.57c.27-1.147 1.3-2 2.53-2h4.8Zm4.115 4-1.59.024A2.601 2.601 0 0 1 14.4 7H9.6c-1.23 0-2.26-.853-2.53-2H5.5c-.27 0-.5.234-.5.535v14.93c0 .3.23.535.5.535h13c.27 0 .5-.234.5-.535V5.535c0-.3-.23-.535-.485-.535Zm-2.658 10.929a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h7.857Zm0-3.929a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h7.857ZM14.4 3H9.6a.6.6 0 0 0-.6.6v.8a.6.6 0 0 0 .6.6h4.8a.6.6 0 0 0 .6-.6v-.8a.6.6 0 0 0-.6-.6Z\"/></symbol><symbol id=\"icon-eds-i-close-medium\" viewBox=\"0 0 24 24\"><path d=\"M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM8.707 7.293 12 10.585l3.293-3.292a1 1 0 0 1 1.414 1.414L13.415 12l3.292 3.293a1 1 0 0 1-1.414 1.414L12 13.415l-3.293 3.292a1 1 0 1 1-1.414-1.414L10.585 12 7.293 8.707a1 1 0 0 1 1.414-1.414Z\"/></symbol><symbol id=\"icon-eds-i-cloud-upload-medium\" viewBox=\"0 0 24 24\"><path d=\"m12.852 10.011.028-.004L13 10l.075.003.126.017.086.022.136.052.098.052.104.074.082.073 3 3a1 1 0 0 1 0 1.414l-.094.083a1 1 0 0 1-1.32-.083L14 13.416V20a1 1 0 0 1-2 0v-6.586l-1.293 1.293a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l3-3 .112-.097.11-.071.114-.054.105-.035.118-.025Zm.587-7.962c3.065.362 5.497 2.662 5.992 5.562l.013.085.207.073c2.117.782 3.496 2.845 3.337 5.097l-.022.226c-.297 2.561-2.503 4.491-5.124 4.502a1 1 0 1 1-.009-2c1.619-.007 2.967-1.186 3.147-2.733.179-1.542-.86-2.979-2.487-3.353-.512-.149-.894-.579-.981-1.165-.21-2.237-2-4.035-4.308-4.308-2.31-.273-4.497 1.06-5.25 3.19l-.049.113c-.234.468-.718.756-1.176.743-1.418.057-2.689.857-3.32 2.084a3.668 3.668 0 0 0 .262 3.798c.796 1.136 2.169 1.764 3.583 1.635a1 1 0 1 1 .182 1.992c-2.125.194-4.193-.753-5.403-2.48a5.668 5.668 0 0 1-.403-5.86c.85-1.652 2.449-2.79 4.323-3.092l.287-.039.013-.028c1.207-2.741 4.125-4.404 7.186-4.042Z\"/></symbol><symbol id=\"icon-eds-i-collection-medium\" viewBox=\"0 0 24 24\"><path d=\"M21 7a1 1 0 0 1 1 1v12.5a2.5 2.5 0 0 1-2.5 2.5H8a1 1 0 0 1 0-2h11.5a.5.5 0 0 0 .5-.5V8a1 1 0 0 1 1-1Zm-5.5-5A2.5 2.5 0 0 1 18 4.5v12a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 2 16.5v-12A2.5 2.5 0 0 1 4.5 2h11Zm0 2h-11a.5.5 0 0 0-.5.5v12a.5.5 0 0 0 .5.5h11a.5.5 0 0 0 .5-.5v-12a.5.5 0 0 0-.5-.5ZM13 13a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h6Zm0-3.5a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h6ZM13 6a1 1 0 0 1 0 2H7a1 1 0 1 1 0-2h6Z\"/></symbol><symbol id=\"icon-eds-i-conference-series-medium\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M4.5 2A2.5 2.5 0 0 0 2 4.5v11A2.5 2.5 0 0 0 4.5 18h2.37l-2.534 2.253a1 1 0 0 0 1.328 1.494L9.88 18H11v3a1 1 0 1 0 2 0v-3h1.12l4.216 3.747a1 1 0 0 0 1.328-1.494L17.13 18h2.37a2.5 2.5 0 0 0 2.5-2.5v-11A2.5 2.5 0 0 0 19.5 2h-15ZM20 6V4.5a.5.5 0 0 0-.5-.5h-15a.5.5 0 0 0-.5.5V6h16ZM4 8v7.5a.5.5 0 0 0 .5.5h15a.5.5 0 0 0 .5-.5V8H4Z\" clip-rule=\"evenodd\"/></symbol><symbol id=\"icon-eds-i-delivery-medium\" viewBox=\"0 0 24 24\"><path d=\"M8.51 20.598a3.037 3.037 0 0 1-3.02 0A2.968 2.968 0 0 1 4.161 19L3.5 19A2.5 2.5 0 0 1 1 16.5v-11A2.5 2.5 0 0 1 3.5 3h10a2.5 2.5 0 0 1 2.45 2.004L16 5h2.527c.976 0 1.855.585 2.27 1.49l2.112 4.62a1 1 0 0 1 .091.416v4.856C23 17.814 21.889 19 20.484 19h-.523a1.01 1.01 0 0 1-.121-.007 2.96 2.96 0 0 1-1.33 1.605 3.037 3.037 0 0 1-3.02 0A2.968 2.968 0 0 1 14.161 19H9.838a2.968 2.968 0 0 1-1.327 1.597Zm-2.024-3.462a.955.955 0 0 0-.481.73L5.999 18l.001.022a.944.944 0 0 0 .388.777l.098.065c.316.181.712.181 1.028 0A.97.97 0 0 0 8 17.978a.95.95 0 0 0-.486-.842 1.037 1.037 0 0 0-1.028 0Zm10 0a.955.955 0 0 0-.481.73l-.005.156a.944.944 0 0 0 .388.777l.098.065c.316.181.712.181 1.028 0a.97.97 0 0 0 .486-.886.95.95 0 0 0-.486-.842 1.037 1.037 0 0 0-1.028 0ZM21 12h-5v3.17a3.038 3.038 0 0 1 2.51.232 2.993 2.993 0 0 1 1.277 1.45l.058.155.058-.005.581-.002c.27 0 .516-.263.516-.618V12Zm-7.5-7h-10a.5.5 0 0 0-.5.5v11a.5.5 0 0 0 .5.5h.662a2.964 2.964 0 0 1 1.155-1.491l.172-.107a3.037 3.037 0 0 1 3.022 0A2.987 2.987 0 0 1 9.843 17H13.5a.5.5 0 0 0 .5-.5v-11a.5.5 0 0 0-.5-.5Zm5.027 2H16v3h4.203l-1.224-2.677a.532.532 0 0 0-.375-.316L18.527 7Z\"/></symbol><symbol id=\"icon-eds-i-download-medium\" viewBox=\"0 0 24 24\"><path d=\"M22 18.5a3.5 3.5 0 0 1-3.5 3.5h-13A3.5 3.5 0 0 1 2 18.5V18a1 1 0 0 1 2 0v.5A1.5 1.5 0 0 0 5.5 20h13a1.5 1.5 0 0 0 1.5-1.5V18a1 1 0 0 1 2 0v.5Zm-3.293-7.793-6 6-.063.059-.093.069-.081.048-.105.049-.104.034-.056.013-.118.017L12 17l-.076-.003-.122-.017-.113-.03-.085-.032-.063-.03-.098-.058-.06-.043-.05-.043-6.04-6.037a1 1 0 0 1 1.414-1.414l4.294 4.29L11 3a1 1 0 0 1 2 0l.001 10.585 4.292-4.292a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414Z\"/></symbol><symbol id=\"icon-eds-i-edit-medium\" viewBox=\"0 0 24 24\"><path d=\"M17.149 2a2.38 2.38 0 0 1 1.699.711l2.446 2.46a2.384 2.384 0 0 1 .005 3.38L10.01 19.906a1 1 0 0 1-.434.257l-6.3 1.8a1 1 0 0 1-1.237-1.237l1.8-6.3a1 1 0 0 1 .257-.434L15.443 2.718A2.385 2.385 0 0 1 17.15 2Zm-3.874 5.689-7.586 7.536-1.234 4.319 4.318-1.234 7.54-7.582-3.038-3.039ZM17.149 4a.395.395 0 0 0-.286.126L14.695 6.28l3.029 3.029 2.162-2.173a.384.384 0 0 0 .106-.197L20 6.864c0-.103-.04-.2-.119-.278l-2.457-2.47A.385.385 0 0 0 17.149 4Z\"/></symbol><symbol id=\"icon-eds-i-education-medium\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M12.41 2.088a1 1 0 0 0-.82 0l-10 4.5a1 1 0 0 0 0 1.824L3 9.047v7.124A3.001 3.001 0 0 0 4 22a3 3 0 0 0 1-5.83V9.948l1 .45V14.5a1 1 0 0 0 .087.408L7 14.5c-.913.408-.912.41-.912.41l.001.003.003.006.007.015a1.988 1.988 0 0 0 .083.16c.054.097.131.225.236.373.21.297.53.68.993 1.057C8.351 17.292 9.824 18 12 18c2.176 0 3.65-.707 4.589-1.476.463-.378.783-.76.993-1.057a4.162 4.162 0 0 0 .319-.533l.007-.015.003-.006v-.003h.002s0-.002-.913-.41l.913.408A1 1 0 0 0 18 14.5v-4.103l4.41-1.985a1 1 0 0 0 0-1.824l-10-4.5ZM16 11.297l-3.59 1.615a1 1 0 0 1-.82 0L8 11.297v2.94a3.388 3.388 0 0 0 .677.739C9.267 15.457 10.294 16 12 16s2.734-.543 3.323-1.024a3.388 3.388 0 0 0 .677-.739v-2.94ZM4.437 7.5 12 4.097 19.563 7.5 12 10.903 4.437 7.5ZM3 19a1 1 0 1 1 2 0 1 1 0 0 1-2 0Z\" clip-rule=\"evenodd\"/></symbol><symbol id=\"icon-eds-i-error-diamond-medium\" viewBox=\"0 0 24 24\"><path d=\"M12.002 1c.702 0 1.375.279 1.871.775l8.35 8.353a2.646 2.646 0 0 1 .001 3.744l-8.353 8.353a2.646 2.646 0 0 1-3.742 0l-8.353-8.353a2.646 2.646 0 0 1 0-3.744l8.353-8.353.156-.142c.424-.362.952-.58 1.507-.625l.21-.008Zm0 2a.646.646 0 0 0-.38.123l-.093.08-8.34 8.34a.646.646 0 0 0-.18.355L3 12c0 .171.068.336.19.457l8.353 8.354a.646.646 0 0 0 .914 0l8.354-8.354a.646.646 0 0 0-.001-.914l-8.351-8.354A.646.646 0 0 0 12.002 3ZM12 14.5a1.5 1.5 0 0 1 .144 2.993L12 17.5a1.5 1.5 0 0 1 0-3ZM12 6a1 1 0 0 1 1 1v5a1 1 0 0 1-2 0V7a1 1 0 0 1 1-1Z\"/></symbol><symbol id=\"icon-eds-i-error-filled-medium\" viewBox=\"0 0 24 24\"><path d=\"M12.002 1c.702 0 1.375.279 1.871.775l8.35 8.353a2.646 2.646 0 0 1 .001 3.744l-8.353 8.353a2.646 2.646 0 0 1-3.742 0l-8.353-8.353a2.646 2.646 0 0 1 0-3.744l8.353-8.353.156-.142c.424-.362.952-.58 1.507-.625l.21-.008ZM12 14.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 14.5ZM12 6a1 1 0 0 0-1 1v5a1 1 0 0 0 2 0V7a1 1 0 0 0-1-1Z\"/></symbol><symbol id=\"icon-eds-i-external-link-medium\" viewBox=\"0 0 24 24\"><path d=\"M9 2a1 1 0 1 1 0 2H4.6c-.371 0-.6.209-.6.5v15c0 .291.229.5.6.5h14.8c.371 0 .6-.209.6-.5V15a1 1 0 0 1 2 0v4.5c0 1.438-1.162 2.5-2.6 2.5H4.6C3.162 22 2 20.938 2 19.5v-15C2 3.062 3.162 2 4.6 2H9Zm6 0h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L22 3v6a1 1 0 0 1-2 0V5.414l-6.693 6.693a1 1 0 0 1-1.414-1.414L18.584 4H15a1 1 0 0 1-.993-.883L14 3a1 1 0 0 1 1-1Z\"/></symbol><symbol id=\"icon-eds-i-external-link-small\" viewBox=\"0 0 16 16\"><path d=\"M5 1a1 1 0 1 1 0 2l-2-.001V13L13 13v-2a1 1 0 0 1 2 0v2c0 1.15-.93 2-2.067 2H3.067C1.93 15 1 14.15 1 13V3c0-1.15.93-2 2.067-2H5Zm4 0h5l.075.003.126.017.111.03.111.044.098.052.096.067.09.08.044.047.073.093.051.083.054.113.035.105.03.148L15 2v5a1 1 0 0 1-2 0V4.414L9.107 8.307a1 1 0 0 1-1.414-1.414L11.584 3H9a1 1 0 0 1-.993-.883L8 2a1 1 0 0 1 1-1Z\"/></symbol><symbol id=\"icon-eds-i-file-download-medium\" viewBox=\"0 0 24 24\"><path d=\"M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3ZM12 7a1 1 0 0 1 1 1v6.585l2.293-2.292a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414l-4 4a1.008 1.008 0 0 1-.112.097l-.11.071-.114.054-.105.035-.149.03L12 18l-.075-.003-.126-.017-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08-4-4a1 1 0 0 1 1.414-1.414L11 14.585V8a1 1 0 0 1 1-1Z\"/></symbol><symbol id=\"icon-eds-i-file-report-medium\" viewBox=\"0 0 24 24\"><path d=\"M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742H5.545c-.674 0-1.32-.267-1.798-.742A2.535 2.535 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .142.057.278.158.379.102.102.242.159.387.159h12.91a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.915L14.085 3ZM16 17a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm0-3a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm-4.793-6.207L13 9.585l1.793-1.792a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414l-2.5 2.5a1 1 0 0 1-1.414 0L10.5 9.915l-1.793 1.792a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l2.5-2.5a1 1 0 0 1 1.414 0Z\"/></symbol><symbol id=\"icon-eds-i-file-text-medium\" viewBox=\"0 0 24 24\"><path d=\"M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3ZM16 15a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm0-4a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm-5-4a1 1 0 0 1 0 2H8a1 1 0 1 1 0-2h3Z\"/></symbol><symbol id=\"icon-eds-i-file-upload-medium\" viewBox=\"0 0 24 24\"><path d=\"M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3Zm-2.233 4.011.058-.007L12 7l.075.003.126.017.111.03.111.044.098.052.104.074.082.073 4 4a1 1 0 0 1 0 1.414l-.094.083a1 1 0 0 1-1.32-.083L13 10.415V17a1 1 0 0 1-2 0v-6.585l-2.293 2.292a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l4-4 .112-.097.11-.071.114-.054.105-.035.118-.025Z\"/></symbol><symbol id=\"icon-eds-i-filter-medium\" viewBox=\"0 0 24 24\"><path d=\"M21 2a1 1 0 0 1 .82 1.573L15 13.314V18a1 1 0 0 1-.31.724l-.09.076-4 3A1 1 0 0 1 9 21v-7.684L2.18 3.573a1 1 0 0 1 .707-1.567L3 2h18Zm-1.921 2H4.92l5.9 8.427a1 1 0 0 1 .172.45L11 13v6l2-1.5V13a1 1 0 0 1 .117-.469l.064-.104L19.079 4Z\"/></symbol><symbol id=\"icon-eds-i-funding-medium\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"M23 8A7 7 0 1 0 9 8a7 7 0 0 0 14 0ZM9.006 12.225A4.07 4.07 0 0 0 6.12 11.02H2a.979.979 0 1 0 0 1.958h4.12c.558 0 1.094.222 1.489.617l2.207 2.288c.27.27.27.687.012.944a.656.656 0 0 1-.928 0L7.744 15.67a.98.98 0 0 0-1.386 1.384l1.157 1.158c.535.536 1.244.791 1.946.765l.041.002h6.922c.874 0 1.597.748 1.597 1.688 0 .203-.146.354-.309.354H7.755c-.487 0-.96-.178-1.339-.504L2.64 17.259a.979.979 0 0 0-1.28 1.482L5.137 22c.733.631 1.66.979 2.618.979h9.957c1.26 0 2.267-1.043 2.267-2.312 0-2.006-1.584-3.646-3.555-3.646h-4.529a2.617 2.617 0 0 0-.681-2.509l-2.208-2.287ZM16 3a5 5 0 1 0 0 10 5 5 0 0 0 0-10Zm.979 3.5a.979.979 0 1 0-1.958 0v3a.979.979 0 1 0 1.958 0v-3Z\" clip-rule=\"evenodd\"/></symbol><symbol id=\"icon-eds-i-hashtag-medium\" viewBox=\"0 0 24 24\"><path d=\"M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM9.52 18.189a1 1 0 1 1-1.964-.378l.437-2.274H6a1 1 0 1 1 0-2h2.378l.592-3.076H6a1 1 0 0 1 0-2h3.354l.51-2.65a1 1 0 1 1 1.964.378l-.437 2.272h3.04l.51-2.65a1 1 0 1 1 1.964.378l-.438 2.272H18a1 1 0 0 1 0 2h-1.917l-.592 3.076H18a1 1 0 0 1 0 2h-2.893l-.51 2.652a1 1 0 1 1-1.964-.378l.437-2.274h-3.04l-.51 2.652Zm.895-4.652h3.04l.591-3.076h-3.04l-.591 3.076Z\"/></symbol><symbol id=\"icon-eds-i-home-medium\" viewBox=\"0 0 24 24\"><path d=\"M5 22a1 1 0 0 1-1-1v-8.586l-1.293 1.293a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l10-10a1 1 0 0 1 1.414 0l10 10a1 1 0 0 1-1.414 1.414L20 12.415V21a1 1 0 0 1-1 1H5Zm7-17.585-6 5.999V20h5v-4a1 1 0 0 1 2 0v4h5v-9.585l-6-6Z\"/></symbol><symbol id=\"icon-eds-i-image-medium\" viewBox=\"0 0 24 24\"><path d=\"M19.615 2A2.385 2.385 0 0 1 22 4.385v15.23A2.385 2.385 0 0 1 19.615 22H4.385A2.385 2.385 0 0 1 2 19.615V4.385A2.385 2.385 0 0 1 4.385 2h15.23Zm0 2H4.385A.385.385 0 0 0 4 4.385v15.23c0 .213.172.385.385.385h1.244l10.228-8.76a1 1 0 0 1 1.254-.037L20 13.392V4.385A.385.385 0 0 0 19.615 4Zm-3.07 9.283L8.703 20h10.912a.385.385 0 0 0 .385-.385v-3.713l-3.455-2.619ZM9.5 6a3.5 3.5 0 1 1 0 7 3.5 3.5 0 0 1 0-7Zm0 2a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3Z\"/></symbol><symbol id=\"icon-eds-i-impact-factor-medium\" viewBox=\"0 0 24 24\"><path d=\"M16.49 2.672c.74.694.986 1.765.632 2.712l-.04.1-1.549 3.54h1.477a2.496 2.496 0 0 1 2.485 2.34l.005.163c0 .618-.23 1.21-.642 1.675l-7.147 7.961a2.48 2.48 0 0 1-3.554.165 2.512 2.512 0 0 1-.633-2.712l.042-.103L9.108 15H7.46c-1.393 0-2.379-1.11-2.455-2.369L5 12.473c0-.593.142-1.145.628-1.692l7.307-7.944a2.48 2.48 0 0 1 3.555-.165ZM14.43 4.164l-7.33 7.97c-.083.093-.101.214-.101.34 0 .277.19.526.46.526h4.163l.097-.009c.015 0 .03.003.046.009.181.078.264.32.186.5l-2.554 5.817a.512.512 0 0 0 .127.552.48.48 0 0 0 .69-.033l7.155-7.97a.513.513 0 0 0 .13-.34.497.497 0 0 0-.49-.502h-3.988a.355.355 0 0 1-.328-.497l2.555-5.844a.512.512 0 0 0-.127-.552.48.48 0 0 0-.69.033Z\"/></symbol><symbol id=\"icon-eds-i-info-circle-medium\" viewBox=\"0 0 24 24\"><path d=\"M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm0 7a1 1 0 0 1 1 1v5h1.5a1 1 0 0 1 0 2h-5a1 1 0 0 1 0-2H11v-4h-.5a1 1 0 0 1-.993-.883L9.5 11a1 1 0 0 1 1-1H12Zm0-4.5a1.5 1.5 0 0 1 .144 2.993L12 8.5a1.5 1.5 0 0 1 0-3Z\"/></symbol><symbol id=\"icon-eds-i-info-filled-medium\" viewBox=\"0 0 24 24\"><path d=\"M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 9h-1.5a1 1 0 0 0-1 1l.007.117A1 1 0 0 0 10.5 12h.5v4H9.5a1 1 0 0 0 0 2h5a1 1 0 0 0 0-2H13v-5a1 1 0 0 0-1-1Zm0-4.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 5.5Z\"/></symbol><symbol id=\"icon-eds-i-journal-medium\" viewBox=\"0 0 24 24\"><path d=\"M18.5 1A2.5 2.5 0 0 1 21 3.5v14a2.5 2.5 0 0 1-2.5 2.5h-13a.5.5 0 1 0 0 1H20a1 1 0 0 1 0 2H5.5A2.5 2.5 0 0 1 3 20.5v-17A2.5 2.5 0 0 1 5.5 1h13ZM7 3H5.5a.5.5 0 0 0-.5.5v14.549l.016-.002c.104-.02.211-.035.32-.042L5.5 18H7V3Zm11.5 0H9v15h9.5a.5.5 0 0 0 .5-.5v-14a.5.5 0 0 0-.5-.5ZM16 5a1 1 0 0 1 1 1v4a1 1 0 0 1-1 1h-5a1 1 0 0 1-1-1V6a1 1 0 0 1 1-1h5Zm-1 2h-3v2h3V7Z\"/></symbol><symbol id=\"icon-eds-i-mail-medium\" viewBox=\"0 0 24 24\"><path d=\"M20.462 3C21.875 3 23 4.184 23 5.619v12.762C23 19.816 21.875 21 20.462 21H3.538C2.125 21 1 19.816 1 18.381V5.619C1 4.184 2.125 3 3.538 3h16.924ZM21 8.158l-7.378 6.258a2.549 2.549 0 0 1-3.253-.008L3 8.16v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619V8.158ZM20.462 5H3.538c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516Z\"/></symbol><symbol id=\"icon-eds-i-mail-send-medium\" viewBox=\"0 0 24 24\"><path d=\"M20.444 5a2.562 2.562 0 0 1 2.548 2.37l.007.078.001.123v7.858A2.564 2.564 0 0 1 20.444 18H9.556A2.564 2.564 0 0 1 7 15.429l.001-7.977.007-.082A2.561 2.561 0 0 1 9.556 5h10.888ZM21 9.331l-5.46 3.51a1 1 0 0 1-1.08 0L9 9.332v6.097c0 .317.251.571.556.571h10.888a.564.564 0 0 0 .556-.571V9.33ZM20.444 7H9.556a.543.543 0 0 0-.32.105l5.763 3.706 5.766-3.706a.543.543 0 0 0-.32-.105ZM4.308 5a1 1 0 1 1 0 2H2a1 1 0 1 1 0-2h2.308Zm0 5.5a1 1 0 0 1 0 2H2a1 1 0 0 1 0-2h2.308Zm0 5.5a1 1 0 0 1 0 2H2a1 1 0 0 1 0-2h2.308Z\"/></symbol><symbol id=\"icon-eds-i-mentions-medium\" viewBox=\"0 0 24 24\"><path d=\"m9.452 1.293 5.92 5.92 2.92-2.92a1 1 0 0 1 1.415 1.414l-2.92 2.92 5.92 5.92a1 1 0 0 1 0 1.415 10.371 10.371 0 0 1-10.378 2.584l.652 3.258A1 1 0 0 1 12 23H2a1 1 0 0 1-.874-1.486l4.789-8.62C4.194 9.074 4.9 4.43 8.038 1.292a1 1 0 0 1 1.414 0Zm-2.355 13.59L3.699 21h7.081l-.689-3.442a10.392 10.392 0 0 1-2.775-2.396l-.22-.28Zm1.69-11.427-.07.09a8.374 8.374 0 0 0 11.737 11.737l.089-.071L8.787 3.456Z\"/></symbol><symbol id=\"icon-eds-i-menu-medium\" viewBox=\"0 0 24 24\"><path d=\"M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z\"/></symbol><symbol id=\"icon-eds-i-metrics-medium\" viewBox=\"0 0 24 24\"><path d=\"M3 22a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v7h4V8a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v13a1 1 0 0 1-.883.993L21 22H3Zm17-2V9h-4v11h4Zm-6-8h-4v8h4v-8ZM8 4H4v16h4V4Z\"/></symbol><symbol id=\"icon-eds-i-news-medium\" viewBox=\"0 0 24 24\"><path d=\"M17.384 3c.975 0 1.77.787 1.77 1.762v13.333c0 .462.354.846.815.899l.107.006.109-.006a.915.915 0 0 0 .809-.794l.006-.105V8.19a1 1 0 0 1 2 0v9.905A2.914 2.914 0 0 1 20.077 21H3.538a2.547 2.547 0 0 1-1.644-.601l-.147-.135A2.516 2.516 0 0 1 1 18.476V4.762C1 3.787 1.794 3 2.77 3h14.614Zm-.231 2H3v13.476c0 .11.035.216.1.304l.054.063c.101.1.24.157.384.157l13.761-.001-.026-.078a2.88 2.88 0 0 1-.115-.655l-.004-.17L17.153 5ZM14 15.021a.979.979 0 1 1 0 1.958H6a.979.979 0 1 1 0-1.958h8Zm0-8c.54 0 .979.438.979.979v4c0 .54-.438.979-.979.979H6A.979.979 0 0 1 5.021 12V8c0-.54.438-.979.979-.979h8Zm-.98 1.958H6.979v2.041h6.041V8.979Z\"/></symbol><symbol id=\"icon-eds-i-newsletter-medium\" viewBox=\"0 0 24 24\"><path d=\"M21 10a1 1 0 0 1 1 1v9.5a2.5 2.5 0 0 1-2.5 2.5h-15A2.5 2.5 0 0 1 2 20.5V11a1 1 0 0 1 2 0v.439l8 4.888 8-4.889V11a1 1 0 0 1 1-1Zm-1 3.783-7.479 4.57a1 1 0 0 1-1.042 0l-7.48-4.57V20.5a.5.5 0 0 0 .501.5h15a.5.5 0 0 0 .5-.5v-6.717ZM15 9a1 1 0 0 1 0 2H9a1 1 0 0 1 0-2h6Zm2.5-8A2.5 2.5 0 0 1 20 3.5V9a1 1 0 0 1-2 0V3.5a.5.5 0 0 0-.5-.5h-11a.5.5 0 0 0-.5.5V9a1 1 0 1 1-2 0V3.5A2.5 2.5 0 0 1 6.5 1h11ZM15 5a1 1 0 0 1 0 2H9a1 1 0 1 1 0-2h6Z\"/></symbol><symbol id=\"icon-eds-i-notifcation-medium\" viewBox=\"0 0 24 24\"><path d=\"M14 20a1 1 0 0 1 0 2h-4a1 1 0 0 1 0-2h4ZM3 18l-.133-.007c-1.156-.124-1.156-1.862 0-1.986l.3-.012C4.32 15.923 5 15.107 5 14V9.5C5 5.368 8.014 2 12 2s7 3.368 7 7.5V14c0 1.107.68 1.923 1.832 1.995l.301.012c1.156.124 1.156 1.862 0 1.986L21 18H3Zm9-14C9.17 4 7 6.426 7 9.5V14c0 .671-.146 1.303-.416 1.858L6.51 16h10.979l-.073-.142a4.192 4.192 0 0 1-.412-1.658L17 14V9.5C17 6.426 14.83 4 12 4Z\"/></symbol><symbol id=\"icon-eds-i-publish-medium\" viewBox=\"0 0 24 24\"><g><path d=\"M16.296 1.291A1 1 0 0 0 15.591 1H5.545A2.542 2.542 0 0 0 3 3.538V13a1 1 0 1 0 2 0V3.538l.007-.087A.543.543 0 0 1 5.545 3h9.633L20 7.8v12.662a.534.534 0 0 1-.158.379.548.548 0 0 1-.387.159H11a1 1 0 1 0 0 2h8.455c.674 0 1.32-.267 1.798-.742A2.534 2.534 0 0 0 22 20.462V7.385a1 1 0 0 0-.294-.709l-5.41-5.385Z\"/><path d=\"M10.762 16.647a1 1 0 0 0-1.525-1.294l-4.472 5.271-2.153-1.665a1 1 0 1 0-1.224 1.582l2.91 2.25a1 1 0 0 0 1.374-.144l5.09-6ZM16 10a1 1 0 1 1 0 2H8a1 1 0 1 1 0-2h8ZM12 7a1 1 0 0 0-1-1H8a1 1 0 1 0 0 2h3a1 1 0 0 0 1-1Z\"/></g></symbol><symbol id=\"icon-eds-i-refresh-medium\" viewBox=\"0 0 24 24\"><g><path d=\"M7.831 5.636H6.032A8.76 8.76 0 0 1 9 3.631 8.549 8.549 0 0 1 12.232 3c.603 0 1.192.063 1.76.182C17.979 4.017 21 7.632 21 12a1 1 0 1 0 2 0c0-5.296-3.674-9.746-8.591-10.776A10.61 10.61 0 0 0 5 3.851V2.805a1 1 0 0 0-.987-1H4a1 1 0 0 0-1 1v3.831a1 1 0 0 0 1 1h3.831a1 1 0 0 0 .013-2h-.013ZM17.968 18.364c-1.59 1.632-3.784 2.636-6.2 2.636C6.948 21 3 16.993 3 12a1 1 0 1 0-2 0c0 6.053 4.799 11 10.768 11 2.788 0 5.324-1.082 7.232-2.85v1.045a1 1 0 1 0 2 0v-3.831a1 1 0 0 0-1-1h-3.831a1 1 0 0 0 0 2h1.799Z\"/></g></symbol><symbol id=\"icon-eds-i-search-medium\" viewBox=\"0 0 24 24\"><path d=\"M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z\"/></symbol><symbol id=\"icon-eds-i-settings-medium\" viewBox=\"0 0 24 24\"><path d=\"M11.382 1h1.24a2.508 2.508 0 0 1 2.334 1.63l.523 1.378 1.59.933 1.444-.224c.954-.132 1.89.3 2.422 1.101l.095.155.598 1.066a2.56 2.56 0 0 1-.195 2.848l-.894 1.161v1.896l.92 1.163c.6.768.707 1.812.295 2.674l-.09.17-.606 1.08a2.504 2.504 0 0 1-2.531 1.25l-1.428-.223-1.589.932-.523 1.378a2.512 2.512 0 0 1-2.155 1.625L12.65 23h-1.27a2.508 2.508 0 0 1-2.334-1.63l-.524-1.379-1.59-.933-1.443.225c-.954.132-1.89-.3-2.422-1.101l-.095-.155-.598-1.066a2.56 2.56 0 0 1 .195-2.847l.891-1.161v-1.898l-.919-1.162a2.562 2.562 0 0 1-.295-2.674l.09-.17.606-1.08a2.504 2.504 0 0 1 2.531-1.25l1.43.223 1.618-.938.524-1.375.07-.167A2.507 2.507 0 0 1 11.382 1Zm.003 2a.509.509 0 0 0-.47.338l-.65 1.71a1 1 0 0 1-.434.51L7.6 6.85a1 1 0 0 1-.655.123l-1.762-.275a.497.497 0 0 0-.498.252l-.61 1.088a.562.562 0 0 0 .04.619l1.13 1.43a1 1 0 0 1 .216.62v2.585a1 1 0 0 1-.207.61L4.15 15.339a.568.568 0 0 0-.036.634l.601 1.072a.494.494 0 0 0 .484.26l1.78-.278a1 1 0 0 1 .66.126l2.2 1.292a1 1 0 0 1 .43.507l.648 1.71a.508.508 0 0 0 .467.338h1.263a.51.51 0 0 0 .47-.34l.65-1.708a1 1 0 0 1 .428-.507l2.201-1.292a1 1 0 0 1 .66-.126l1.763.275a.497.497 0 0 0 .498-.252l.61-1.088a.562.562 0 0 0-.04-.619l-1.13-1.43a1 1 0 0 1-.216-.62v-2.585a1 1 0 0 1 .207-.61l1.105-1.437a.568.568 0 0 0 .037-.634l-.601-1.072a.494.494 0 0 0-.484-.26l-1.78.278a1 1 0 0 1-.66-.126l-2.2-1.292a1 1 0 0 1-.43-.507l-.649-1.71A.508.508 0 0 0 12.62 3h-1.234ZM12 8a4 4 0 1 1 0 8 4 4 0 0 1 0-8Zm0 2a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z\"/></symbol><symbol id=\"icon-eds-i-shipping-medium\" viewBox=\"0 0 24 24\"><path d=\"M16.515 2c1.406 0 2.706.728 3.352 1.902l2.02 3.635.02.042.036.089.031.105.012.058.01.073.004.075v11.577c0 .64-.244 1.255-.683 1.713a2.356 2.356 0 0 1-1.701.731H4.386a2.356 2.356 0 0 1-1.702-.731 2.476 2.476 0 0 1-.683-1.713V7.948c.01-.217.083-.43.22-.6L4.2 3.905C4.833 2.755 6.089 2.032 7.486 2h9.029ZM20 9H4v10.556a.49.49 0 0 0 .075.26l.053.07a.356.356 0 0 0 .257.114h15.23c.094 0 .186-.04.258-.115a.477.477 0 0 0 .127-.33V9Zm-2 7.5a1 1 0 0 1 0 2h-4a1 1 0 0 1 0-2h4ZM16.514 4H13v3h6.3l-1.183-2.13c-.288-.522-.908-.87-1.603-.87ZM11 3.999H7.51c-.679.017-1.277.36-1.566.887L4.728 7H11V3.999Z\"/></symbol><symbol id=\"icon-eds-i-step-guide-medium\" viewBox=\"0 0 24 24\"><path d=\"M11.394 9.447a1 1 0 1 0-1.788-.894l-.88 1.759-.019-.02a1 1 0 1 0-1.414 1.415l1 1a1 1 0 0 0 1.601-.26l1.5-3ZM12 11a1 1 0 0 1 1-1h3a1 1 0 1 1 0 2h-3a1 1 0 0 1-1-1ZM12 17a1 1 0 0 1 1-1h3a1 1 0 1 1 0 2h-3a1 1 0 0 1-1-1ZM10.947 14.105a1 1 0 0 1 .447 1.342l-1.5 3a1 1 0 0 1-1.601.26l-1-1a1 1 0 1 1 1.414-1.414l.02.019.879-1.76a1 1 0 0 1 1.341-.447Z\"/><path d=\"M5.545 1A2.542 2.542 0 0 0 3 3.538v16.924A2.542 2.542 0 0 0 5.545 23h12.91A2.542 2.542 0 0 0 21 20.462V7.5a1 1 0 0 0-.293-.707l-5.5-5.5A1 1 0 0 0 14.5 1H5.545ZM5 3.538C5 3.245 5.24 3 5.545 3h8.54L19 7.914v12.547c0 .294-.24.539-.546.539H5.545A.542.542 0 0 1 5 20.462V3.538Z\" clip-rule=\"evenodd\"/></symbol><symbol id=\"icon-eds-i-submission-medium\" viewBox=\"0 0 24 24\"><g><path d=\"M5 3.538C5 3.245 5.24 3 5.545 3h9.633L20 7.8v12.662a.535.535 0 0 1-.158.379.549.549 0 0 1-.387.159H6a1 1 0 0 1-1-1v-2.5a1 1 0 1 0-2 0V20a3 3 0 0 0 3 3h13.455c.673 0 1.32-.266 1.798-.742A2.535 2.535 0 0 0 22 20.462V7.385a1 1 0 0 0-.294-.709l-5.41-5.385A1 1 0 0 0 15.591 1H5.545A2.542 2.542 0 0 0 3 3.538V7a1 1 0 0 0 2 0V3.538Z\"/><path d=\"m13.707 13.707-4 4a1 1 0 0 1-1.414 0l-.083-.094a1 1 0 0 1 .083-1.32L10.585 14 2 14a1 1 0 1 1 0-2l8.583.001-2.29-2.294a1 1 0 0 1 1.414-1.414l4.037 4.04.043.05.043.06.059.098.03.063.031.085.03.113.017.122L14 13l-.004.087-.017.118-.013.056-.034.104-.049.105-.048.081-.07.093-.058.063Z\"/></g></symbol><symbol id=\"icon-eds-i-table-1-medium\" viewBox=\"0 0 24 24\"><path d=\"M4.385 22a2.56 2.56 0 0 1-1.14-.279C2.485 21.341 2 20.614 2 19.615V4.385c0-.315.067-.716.279-1.14C2.659 2.485 3.386 2 4.385 2h15.23c.315 0 .716.067 1.14.279.76.38 1.245 1.107 1.245 2.106v15.23c0 .315-.067.716-.279 1.14-.38.76-1.107 1.245-2.106 1.245H4.385ZM4 19.615c0 .213.034.265.14.317a.71.71 0 0 0 .245.068H8v-4H4v3.615ZM20 16H10v4h9.615c.213 0 .265-.034.317-.14a.71.71 0 0 0 .068-.245V16Zm0-2v-4H10v4h10ZM4 14h4v-4H4v4ZM19.615 4H10v4h10V4.385c0-.213-.034-.265-.14-.317A.71.71 0 0 0 19.615 4ZM8 4H4.385l-.082.002c-.146.01-.19.047-.235.138A.71.71 0 0 0 4 4.385V8h4V4Z\"/></symbol><symbol id=\"icon-eds-i-table-2-medium\" viewBox=\"0 0 24 24\"><path d=\"M4.384 22A2.384 2.384 0 0 1 2 19.616V4.384A2.384 2.384 0 0 1 4.384 2h15.232A2.384 2.384 0 0 1 22 4.384v15.232A2.384 2.384 0 0 1 19.616 22H4.384ZM10 15H4v4.616c0 .212.172.384.384.384H10v-5Zm5 0h-3v5h3v-5Zm5 0h-3v5h2.616a.384.384 0 0 0 .384-.384V15ZM10 9H4v4h6V9Zm5 0h-3v4h3V9Zm5 0h-3v4h3V9Zm-.384-5H4.384A.384.384 0 0 0 4 4.384V7h16V4.384A.384.384 0 0 0 19.616 4Z\"/></symbol><symbol id=\"icon-eds-i-tag-medium\" viewBox=\"0 0 24 24\"><path d=\"m12.621 1.998.127.004L20.496 2a1.5 1.5 0 0 1 1.497 1.355L22 3.5l-.005 7.669c.038.456-.133.905-.447 1.206l-9.02 9.018a2.075 2.075 0 0 1-2.932 0l-6.99-6.99a2.075 2.075 0 0 1 .001-2.933L11.61 2.47c.246-.258.573-.418.881-.46l.131-.011Zm.286 2-8.885 8.886a.075.075 0 0 0 0 .106l6.987 6.988c.03.03.077.03.106 0l8.883-8.883L19.999 4l-7.092-.002ZM16 6.5a1.5 1.5 0 0 1 .144 2.993L16 9.5a1.5 1.5 0 0 1 0-3Z\"/></symbol><symbol id=\"icon-eds-i-trash-medium\" viewBox=\"0 0 24 24\"><path d=\"M12 1c2.717 0 4.913 2.232 4.997 5H21a1 1 0 0 1 0 2h-1v12.5c0 1.389-1.152 2.5-2.556 2.5H6.556C5.152 23 4 21.889 4 20.5V8H3a1 1 0 1 1 0-2h4.003l.001-.051C7.114 3.205 9.3 1 12 1Zm6 7H6v12.5c0 .238.19.448.454.492l.102.008h10.888c.315 0 .556-.232.556-.5V8Zm-4 3a1 1 0 0 1 1 1v6.005a1 1 0 0 1-2 0V12a1 1 0 0 1 1-1Zm-4 0a1 1 0 0 1 1 1v6a1 1 0 0 1-2 0v-6a1 1 0 0 1 1-1Zm2-8c-1.595 0-2.914 1.32-2.996 3h5.991v-.02C14.903 4.31 13.589 3 12 3Z\"/></symbol><symbol id=\"icon-eds-i-user-account-medium\" viewBox=\"0 0 24 24\"><path d=\"M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 16c-1.806 0-3.52.994-4.664 2.698A8.947 8.947 0 0 0 12 21a8.958 8.958 0 0 0 4.664-1.301C15.52 17.994 13.806 17 12 17Zm0-14a9 9 0 0 0-6.25 15.476C7.253 16.304 9.54 15 12 15s4.747 1.304 6.25 3.475A9 9 0 0 0 12 3Zm0 3a4 4 0 1 1 0 8 4 4 0 0 1 0-8Zm0 2a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z\"/></symbol><symbol id=\"icon-eds-i-user-add-medium\" viewBox=\"0 0 24 24\"><path d=\"M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm9 10a1 1 0 0 1 1 1v3h3a1 1 0 0 1 0 2h-3v3a1 1 0 0 1-2 0v-3h-3a1 1 0 0 1 0-2h3v-3a1 1 0 0 1 1-1Zm-5.545-.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378Z\"/></symbol><symbol id=\"icon-eds-i-user-assign-medium\" viewBox=\"0 0 24 24\"><path d=\"M16.226 13.298a1 1 0 0 1 1.414-.01l.084.093a1 1 0 0 1-.073 1.32L15.39 17H22a1 1 0 0 1 0 2h-6.611l2.262 2.298a1 1 0 0 1-1.425 1.404l-3.939-4a1 1 0 0 1 0-1.404l3.94-4Zm-3.771-.449a1 1 0 1 1-.91 1.781 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 10.5 20a1 1 0 0 1 .993.883L11.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Z\"/></symbol><symbol id=\"icon-eds-i-user-block-medium\" viewBox=\"0 0 24 24\"><path d=\"M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm9 10a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm-5.545-.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM15 18a3 3 0 0 0 4.294 2.707l-4.001-4c-.188.391-.293.83-.293 1.293Zm3-3c-.463 0-.902.105-1.294.293l4.001 4A3 3 0 0 0 18 15Z\"/></symbol><symbol id=\"icon-eds-i-user-check-medium\" viewBox=\"0 0 24 24\"><path d=\"M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm13.647 12.237a1 1 0 0 1 .116 1.41l-5.091 6a1 1 0 0 1-1.375.144l-2.909-2.25a1 1 0 1 1 1.224-1.582l2.153 1.665 4.472-5.271a1 1 0 0 1 1.41-.116Zm-8.139-.977c.22.214.428.44.622.678a1 1 0 1 1-1.548 1.266 6.025 6.025 0 0 0-1.795-1.49.86.86 0 0 1-.163-.048l-.079-.036a5.721 5.721 0 0 0-2.62-.63l-.194.006c-2.76.134-5.022 2.177-5.592 4.864l-.035.175-.035.213c-.03.201-.05.405-.06.61L3.003 20 10 20a1 1 0 0 1 .993.883L11 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876l.005-.223.02-.356.02-.222.03-.248.022-.15c.02-.133.044-.265.071-.397.44-2.178 1.725-4.105 3.595-5.301a7.75 7.75 0 0 1 3.755-1.215l.12-.004a7.908 7.908 0 0 1 5.87 2.252Z\"/></symbol><symbol id=\"icon-eds-i-user-delete-medium\" viewBox=\"0 0 24 24\"><path d=\"M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6ZM4.763 13.227a7.713 7.713 0 0 1 7.692-.378 1 1 0 1 1-.91 1.781 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20H11.5a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897Zm11.421 1.543 2.554 2.553 2.555-2.553a1 1 0 0 1 1.414 1.414l-2.554 2.554 2.554 2.555a1 1 0 0 1-1.414 1.414l-2.555-2.554-2.554 2.554a1 1 0 0 1-1.414-1.414l2.553-2.555-2.553-2.554a1 1 0 0 1 1.414-1.414Z\"/></symbol><symbol id=\"icon-eds-i-user-edit-medium\" viewBox=\"0 0 24 24\"><path d=\"m19.876 10.77 2.831 2.83a1 1 0 0 1 0 1.415l-7.246 7.246a1 1 0 0 1-.572.284l-3.277.446a1 1 0 0 1-1.125-1.13l.461-3.277a1 1 0 0 1 .283-.567l7.23-7.246a1 1 0 0 1 1.415-.001Zm-7.421 2.08a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 7.5 20a1 1 0 0 1 .993.883L8.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378Zm6.715.042-6.29 6.3-.23 1.639 1.633-.222 6.302-6.302-1.415-1.415ZM9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Z\"/></symbol><symbol id=\"icon-eds-i-user-linked-medium\" viewBox=\"0 0 24 24\"><path d=\"M15.65 6c.31 0 .706.066 1.122.274C17.522 6.65 18 7.366 18 8.35v12.3c0 .31-.066.706-.274 1.122-.375.75-1.092 1.228-2.076 1.228H3.35a2.52 2.52 0 0 1-1.122-.274C1.478 22.35 1 21.634 1 20.65V8.35c0-.31.066-.706.274-1.122C1.65 6.478 2.366 6 3.35 6h12.3Zm0 2-12.376.002c-.134.007-.17.04-.21.12A.672.672 0 0 0 3 8.35v12.3c0 .198.028.24.122.287.09.044.2.063.228.063h.887c.788-2.269 2.814-3.5 5.263-3.5 2.45 0 4.475 1.231 5.263 3.5h.887c.198 0 .24-.028.287-.122.044-.09.063-.2.063-.228V8.35c0-.198-.028-.24-.122-.287A.672.672 0 0 0 15.65 8ZM9.5 19.5c-1.36 0-2.447.51-3.06 1.5h6.12c-.613-.99-1.7-1.5-3.06-1.5ZM20.65 1A2.35 2.35 0 0 1 23 3.348V15.65A2.35 2.35 0 0 1 20.65 18H20a1 1 0 0 1 0-2h.65a.35.35 0 0 0 .35-.35V3.348A.35.35 0 0 0 20.65 3H8.35a.35.35 0 0 0-.35.348V4a1 1 0 1 1-2 0v-.652A2.35 2.35 0 0 1 8.35 1h12.3ZM9.5 10a3.5 3.5 0 1 1 0 7 3.5 3.5 0 0 1 0-7Zm0 2a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3Z\"/></symbol><symbol id=\"icon-eds-i-user-multiple-medium\" viewBox=\"0 0 24 24\"><path d=\"M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm6 0a5 5 0 0 1 0 10 1 1 0 0 1-.117-1.993L15 9a3 3 0 0 0 0-6 1 1 0 0 1 0-2ZM9 3a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm8.857 9.545a7.99 7.99 0 0 1 2.651 1.715A8.31 8.31 0 0 1 23 20.134V21a1 1 0 0 1-1 1h-3a1 1 0 0 1 0-2h1.995l-.005-.153a6.307 6.307 0 0 0-1.673-3.945l-.204-.209a5.99 5.99 0 0 0-1.988-1.287 1 1 0 1 1 .732-1.861Zm-3.349 1.715A8.31 8.31 0 0 1 17 20.134V21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.877c.044-4.343 3.387-7.908 7.638-8.115a7.908 7.908 0 0 1 5.87 2.252ZM9.016 14l-.285.006c-3.104.15-5.58 2.718-5.725 5.9L3.004 20h11.991l-.005-.153a6.307 6.307 0 0 0-1.673-3.945l-.204-.209A5.924 5.924 0 0 0 9.3 14.008L9.016 14Z\"/></symbol><symbol id=\"icon-eds-i-user-notify-medium\" viewBox=\"0 0 24 24\"><path d=\"M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm10 18v1a1 1 0 0 1-2 0v-1h-3a1 1 0 0 1 0-2v-2.818C14 13.885 15.777 12 18 12s4 1.885 4 4.182V19a1 1 0 0 1 0 2h-3Zm-6.545-8.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM18 14c-1.091 0-2 .964-2 2.182V19h4v-2.818c0-1.165-.832-2.098-1.859-2.177L18 14Z\"/></symbol><symbol id=\"icon-eds-i-user-remove-medium\" viewBox=\"0 0 24 24\"><path d=\"M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm3.455 9.85a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM22 17a1 1 0 0 1 0 2h-8a1 1 0 0 1 0-2h8Z\"/></symbol><symbol id=\"icon-eds-i-user-single-medium\" viewBox=\"0 0 24 24\"><path d=\"M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z\"/></symbol><symbol id=\"icon-eds-i-warning-circle-medium\" viewBox=\"0 0 24 24\"><path d=\"M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm0 11.5a1.5 1.5 0 0 1 .144 2.993L12 17.5a1.5 1.5 0 0 1 0-3ZM12 6a1 1 0 0 1 1 1v5a1 1 0 0 1-2 0V7a1 1 0 0 1 1-1Z\"/></symbol><symbol id=\"icon-eds-i-warning-filled-medium\" viewBox=\"0 0 24 24\"><path d=\"M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 13.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 14.5ZM12 6a1 1 0 0 0-1 1v5a1 1 0 0 0 2 0V7a1 1 0 0 0-1-1Z\"/></symbol><symbol id=\"icon-chevron-left-medium\" viewBox=\"0 0 24 24\"><path d=\"M15.7194 3.3054C15.3358 2.90809 14.7027 2.89699 14.3054 3.28061L6.54342 10.7757C6.19804 11.09 6 11.5335 6 12C6 12.4665 6.19804 12.91 6.5218 13.204L14.3054 20.7194C14.7027 21.103 15.3358 21.0919 15.7194 20.6946C16.103 20.2973 16.0919 19.6642 15.6946 19.2806L8.155 12L15.6946 4.71939C16.0614 4.36528 16.099 3.79863 15.8009 3.40105L15.7194 3.3054Z\"/></symbol><symbol id=\"icon-chevron-right-medium\" viewBox=\"0 0 24 24\"><path d=\"M8.28061 3.3054C8.66423 2.90809 9.29729 2.89699 9.6946 3.28061L17.4566 10.7757C17.802 11.09 18 11.5335 18 12C18 12.4665 17.802 12.91 17.4782 13.204L9.6946 20.7194C9.29729 21.103 8.66423 21.0919 8.28061 20.6946C7.89699 20.2973 7.90809 19.6642 8.3054 19.2806L15.845 12L8.3054 4.71939C7.93865 4.36528 7.90098 3.79863 8.19908 3.40105L8.28061 3.3054Z\"/></symbol><symbol id=\"icon-eds-alerts\" viewBox=\"0 0 32 32\"><path d=\"M28 12.667c.736 0 1.333.597 1.333 1.333v13.333A3.333 3.333 0 0 1 26 30.667H6a3.333 3.333 0 0 1-3.333-3.334V14a1.333 1.333 0 1 1 2.666 0v1.252L16 21.769l10.667-6.518V14c0-.736.597-1.333 1.333-1.333Zm-1.333 5.71-9.972 6.094c-.427.26-.963.26-1.39 0l-9.972-6.094v8.956c0 .368.299.667.667.667h20a.667.667 0 0 0 .667-.667v-8.956ZM19.333 12a1.333 1.333 0 1 1 0 2.667h-6.666a1.333 1.333 0 1 1 0-2.667h6.666Zm4-10.667a3.333 3.333 0 0 1 3.334 3.334v6.666a1.333 1.333 0 1 1-2.667 0V4.667A.667.667 0 0 0 23.333 4H8.667A.667.667 0 0 0 8 4.667v6.666a1.333 1.333 0 1 1-2.667 0V4.667a3.333 3.333 0 0 1 3.334-3.334h14.666Zm-4 5.334a1.333 1.333 0 0 1 0 2.666h-6.666a1.333 1.333 0 1 1 0-2.666h6.666Z\"/></symbol><symbol id=\"icon-eds-arrow-up\" viewBox=\"0 0 24 24\"><path fill-rule=\"evenodd\" d=\"m13.002 7.408 4.88 4.88a.99.99 0 0 0 1.32.08l.09-.08c.39-.39.39-1.03 0-1.42l-6.58-6.58a1.01 1.01 0 0 0-1.42 0l-6.58 6.58a1 1 0 0 0-.09 1.32l.08.1a1 1 0 0 0 1.42-.01l4.88-4.87v11.59a.99.99 0 0 0 .88.99l.12.01c.55 0 1-.45 1-1V7.408z\" class=\"layer\"/></symbol><symbol id=\"icon-eds-checklist\" viewBox=\"0 0 32 32\"><path d=\"M19.2 1.333a3.468 3.468 0 0 1 3.381 2.699L24.667 4C26.515 4 28 5.52 28 7.38v19.906c0 1.86-1.485 3.38-3.333 3.38H7.333c-1.848 0-3.333-1.52-3.333-3.38V7.38C4 5.52 5.485 4 7.333 4h2.093A3.468 3.468 0 0 1 12.8 1.333h6.4ZM9.426 6.667H7.333c-.36 0-.666.312-.666.713v19.906c0 .401.305.714.666.714h17.334c.36 0 .666-.313.666-.714V7.38c0-.4-.305-.713-.646-.714l-2.121.033A3.468 3.468 0 0 1 19.2 9.333h-6.4a3.468 3.468 0 0 1-3.374-2.666Zm12.715 5.606c.586.446.7 1.283.253 1.868l-7.111 9.334a1.333 1.333 0 0 1-1.792.306l-3.556-2.333a1.333 1.333 0 1 1 1.463-2.23l2.517 1.651 6.358-8.344a1.333 1.333 0 0 1 1.868-.252ZM19.2 4h-6.4a.8.8 0 0 0-.8.8v1.067a.8.8 0 0 0 .8.8h6.4a.8.8 0 0 0 .8-.8V4.8a.8.8 0 0 0-.8-.8Z\"/></symbol><symbol id=\"icon-eds-citation\" viewBox=\"0 0 36 36\"><path d=\"M23.25 1.5a1.5 1.5 0 0 1 1.06.44l8.25 8.25a1.5 1.5 0 0 1 .44 1.06v19.5c0 2.105-1.645 3.75-3.75 3.75H18a1.5 1.5 0 0 1 0-3h11.25c.448 0 .75-.302.75-.75V11.873L22.628 4.5H8.31a.811.811 0 0 0-.8.68l-.011.13V16.5a1.5 1.5 0 0 1-3 0V5.31A3.81 3.81 0 0 1 8.31 1.5h14.94ZM8.223 20.358a.984.984 0 0 1-.192 1.378l-.048.034c-.54.36-.942.676-1.206.951-.59.614-.885 1.395-.885 2.343.115-.028.288-.042.518-.042.662 0 1.26.237 1.791.711.533.474.799 1.074.799 1.799 0 .753-.259 1.352-.777 1.799-.518.446-1.151.669-1.9.669-1.006 0-1.812-.293-2.417-.878C3.302 28.536 3 27.657 3 26.486c0-1.115.165-2.085.496-2.907.331-.823.734-1.513 1.209-2.071.475-.558.971-.997 1.49-1.318a6.01 6.01 0 0 1 .347-.2 1.321 1.321 0 0 1 1.681.368Zm7.5 0a.984.984 0 0 1-.192 1.378l-.048.034c-.54.36-.942.676-1.206.951-.59.614-.885 1.395-.885 2.343.115-.028.288-.042.518-.042.662 0 1.26.237 1.791.711.533.474.799 1.074.799 1.799 0 .753-.259 1.352-.777 1.799-.518.446-1.151.669-1.9.669-1.006 0-1.812-.293-2.417-.878-.604-.586-.906-1.465-.906-2.636 0-1.115.165-2.085.496-2.907.331-.823.734-1.513 1.209-2.071.475-.558.971-.997 1.49-1.318a6.01 6.01 0 0 1 .347-.2 1.321 1.321 0 0 1 1.681.368Z\"/></symbol><symbol id=\"icon-eds-i-github-medium\" viewBox=\"0 0 24 24\"><path d=\"M 11.964844 0 C 5.347656 0 0 5.269531 0 11.792969 C 0 17.003906 3.425781 21.417969 8.179688 22.976562 C 8.773438 23.09375 8.992188 22.722656 8.992188 22.410156 C 8.992188 22.136719 8.972656 21.203125 8.972656 20.226562 C 5.644531 20.929688 4.953125 18.820312 4.953125 18.820312 C 4.417969 17.453125 3.625 17.101562 3.625 17.101562 C 2.535156 16.378906 3.703125 16.378906 3.703125 16.378906 C 4.914062 16.457031 5.546875 17.589844 5.546875 17.589844 C 6.617188 19.386719 8.339844 18.878906 9.03125 18.566406 C 9.132812 17.804688 9.449219 17.277344 9.785156 16.984375 C 7.132812 16.710938 4.339844 15.695312 4.339844 11.167969 C 4.339844 9.878906 4.8125 8.824219 5.566406 8.003906 C 5.445312 7.710938 5.03125 6.5 5.683594 4.878906 C 5.683594 4.878906 6.695312 4.566406 8.972656 6.089844 C 9.949219 5.832031 10.953125 5.703125 11.964844 5.699219 C 12.972656 5.699219 14.003906 5.835938 14.957031 6.089844 C 17.234375 4.566406 18.242188 4.878906 18.242188 4.878906 C 18.898438 6.5 18.480469 7.710938 18.363281 8.003906 C 19.136719 8.824219 19.589844 9.878906 19.589844 11.167969 C 19.589844 15.695312 16.796875 16.691406 14.125 16.984375 C 14.558594 17.355469 14.933594 18.058594 14.933594 19.171875 C 14.933594 20.753906 14.914062 22.019531 14.914062 22.410156 C 14.914062 22.722656 15.132812 23.09375 15.726562 22.976562 C 20.480469 21.414062 23.910156 17.003906 23.910156 11.792969 C 23.929688 5.269531 18.558594 0 11.964844 0 Z M 11.964844 0 \"/></symbol><symbol id=\"icon-eds-i-subjects-medium\" viewBox=\"0 0 24 24\"><g id=\"icon-subjects-copy\" stroke=\"none\" stroke-width=\"1\" fill-rule=\"evenodd\"><path d=\"M13.3846154,2 C14.7015971,2 15.7692308,3.06762994 15.7692308,4.38461538 L15.7692308,7.15384615 C15.7692308,8.47082629 14.7015955,9.53846154 13.3846154,9.53846154 L13.1038388,9.53925278 C13.2061091,9.85347965 13.3815528,10.1423885 13.6195822,10.3804178 C13.9722182,10.7330539 14.436524,10.9483278 14.9293854,10.9918129 L15.1153846,11 C16.2068332,11 17.2535347,11.433562 18.0254647,12.2054189 C18.6411944,12.8212361 19.0416785,13.6120766 19.1784166,14.4609738 L19.6153846,14.4615385 C20.932386,14.4615385 22,15.5291672 22,16.8461538 L22,19.6153846 C22,20.9323924 20.9323924,22 19.6153846,22 L16.8461538,22 C15.5291672,22 14.4615385,20.932386 14.4615385,19.6153846 L14.4615385,16.8461538 C14.4615385,15.5291737 15.5291737,14.4615385 16.8461538,14.4615385 L17.126925,14.460779 C17.0246537,14.1465537 16.8492179,13.857633 16.6112344,13.6196157 C16.2144418,13.2228606 15.6764136,13 15.1153846,13 C14.0239122,13 12.9771569,12.5664197 12.2053686,11.7946314 C12.1335167,11.7227795 12.0645962,11.6485444 11.9986839,11.5721119 C11.9354038,11.6485444 11.8664833,11.7227795 11.7946314,11.7946314 C11.0228431,12.5664197 9.97608778,13 8.88461538,13 C8.323576,13 7.78552852,13.2228666 7.38881294,13.6195822 C7.15078359,13.8576115 6.97533988,14.1465203 6.8730696,14.4607472 L7.15384615,14.4615385 C8.47082629,14.4615385 9.53846154,15.5291737 9.53846154,16.8461538 L9.53846154,19.6153846 C9.53846154,20.932386 8.47083276,22 7.15384615,22 L4.38461538,22 C3.06762347,22 2,20.9323876 2,19.6153846 L2,16.8461538 C2,15.5291721 3.06762994,14.4615385 4.38461538,14.4615385 L4.8215823,14.4609378 C4.95831893,13.6120029 5.3588057,12.8211623 5.97459937,12.2053686 C6.69125996,11.488708 7.64500941,11.0636656 8.6514968,11.0066017 L8.88461538,11 C9.44565477,11 9.98370225,10.7771334 10.3804178,10.3804178 C10.6184472,10.1423885 10.7938909,9.85347965 10.8961612,9.53925278 L10.6153846,9.53846154 C9.29840448,9.53846154 8.23076923,8.47082629 8.23076923,7.15384615 L8.23076923,4.38461538 C8.23076923,3.06762994 9.29840286,2 10.6153846,2 L13.3846154,2 Z M7.15384615,16.4615385 L4.38461538,16.4615385 C4.17220099,16.4615385 4,16.63374 4,16.8461538 L4,19.6153846 C4,19.8278134 4.17218833,20 4.38461538,20 L7.15384615,20 C7.36626945,20 7.53846154,19.8278103 7.53846154,19.6153846 L7.53846154,16.8461538 C7.53846154,16.6337432 7.36625679,16.4615385 7.15384615,16.4615385 Z M19.6153846,16.4615385 L16.8461538,16.4615385 C16.6337432,16.4615385 16.4615385,16.6337432 16.4615385,16.8461538 L16.4615385,19.6153846 C16.4615385,19.8278103 16.6337306,20 16.8461538,20 L19.6153846,20 C19.8278229,20 20,19.8278229 20,19.6153846 L20,16.8461538 C20,16.6337306 19.8278103,16.4615385 19.6153846,16.4615385 Z M13.3846154,4 L10.6153846,4 C10.4029708,4 10.2307692,4.17220099 10.2307692,4.38461538 L10.2307692,7.15384615 C10.2307692,7.36625679 10.402974,7.53846154 10.6153846,7.53846154 L13.3846154,7.53846154 C13.597026,7.53846154 13.7692308,7.36625679 13.7692308,7.15384615 L13.7692308,4.38461538 C13.7692308,4.17220099 13.5970292,4 13.3846154,4 Z\" id=\"Shape\" fill-rule=\"nonzero\"/></g></symbol><symbol id=\"icon-eds-small-arrow-left\" viewBox=\"0 0 16 17\"><path stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M14 8.092H2m0 0L8 2M2 8.092l6 6.035\"/></symbol><symbol id=\"icon-eds-small-arrow-right\" viewBox=\"0 0 16 16\"><g fill-rule=\"evenodd\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\"><path d=\"M2 8.092h12M8 2l6 6.092M8 14.127l6-6.035\"/></g></symbol></svg>\n</div>\n\n\n        \n\n        \n    <a class=\"c-skip-link\" href=\"#main\">Skip to main content</a>\n\n    \n    <div class=\"u-lazy-ad-wrapper u-mbs-0\">\n        <div class=\"c-ad c-ad--728x90 c-ad--conditional\" data-test=\"springer-doubleclick-ad\">\n            <div class=\"c-ad c-ad__inner\" >\n                <p class=\"c-ad__label\">Advertisement</p>\n                <div id=\"div-gpt-ad-LB1\"\n                     class=\"div-gpt-ad grade-c-hide\"\n                     data-gpt\n                     data-gpt-unitpath=\"/270604982/springerlink/10676/article\"\n                     data-gpt-sizes=\"728x90\"\n                     data-gpt-targeting=\"pos=top;articleid=s10676-024-09775-5;\"\n                     data-ad-type=\"top\"\n                     style=\"min-width:728px;min-height:90px\">\n                    <noscript>\n                        <a href=\"//pubads.g.doubleclick.net/gampad/jump?iu=/270604982/springerlink/10676/article&amp;sz=728x90&amp;pos=top&amp;articleid=s10676-024-09775-5\">\n                            <img data-test=\"gpt-advert-fallback-img\"\n                                 src=\"//pubads.g.doubleclick.net/gampad/ad?iu=/270604982/springerlink/10676/article&amp;sz=728x90&amp;pos=top&amp;articleid=s10676-024-09775-5\"\n                                 alt=\"Advertisement\"\n                                 width=\"728\"\n                                 height=\"90\">\n                        </a>\n                    </noscript>\n                </div>\n            </div>\n        </div>\n    </div>\n\n\n    <header class=\"eds-c-header\" data-eds-c-header>\n    <div class=\"eds-c-header__container\" data-eds-c-header-expander-anchor>\n        <div class=\"eds-c-header__brand\">\n            \n                \n                    <a href=\"https://link.springer.com\"\n                    \t data-test=springerlink-logo\n\t\t\t\t\t\t\t\t\t\t\t data-track=\"click||click_imprint_logo\" data-track-context=\"unified header\" data-track-action=\"click logo link\" data-track-category=\"unified header\" data-track-label=\"link\"\n\t\t\t\t\t\t\t\t\t\t>\n                        <img src=\"/oscar-static/images/darwin/header/img/logo-springerlink-39ee2a28d8.svg\" alt=\"SpringerLink\">\n                    </a>\n                \n            \n        </div>\n\n        \n            \n                \n                     \n                        <a href=\"\" class=\"eds-c-header__link eds-c-header__link--static\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t data-test=\"login-link\"\n\t\t\t\t\t\t\t\t\t\t\t\t>\n                            <svg class=\"eds-c-header__icon\" width=\"24\" height=\"24\" aria-hidden=\"true\" focusable=\"false\">\n                                <use xlink:href=\"#icon-eds-i-user-single-medium\"></use>\n                            </svg><span>Account</span>\n                        </a>\n                    \n                \n            \n        \n    </div>\n\n    \n        <nav class=\"eds-c-header__nav\" aria-label=\"header navigation\">\n            <div class=\"eds-c-header__nav-container\">\n                <div class=\"eds-c-header__item eds-c-header__item--menu\">\n                   <a href=\"#eds-c-header-nav\" class=\"eds-c-header__link\" data-eds-c-header-expander>\n                        <svg class=\"eds-c-header__icon\" width=\"24\" height=\"24\" aria-hidden=\"true\" focusable=\"false\">\n                            <use xlink:href=\"#icon-eds-i-menu-medium\"></use>\n                        </svg><span>Menu</span>\n                    </a>\n                </div>\n\n                <div class=\"eds-c-header__item eds-c-header__item--inline-links\">\n                    \n                        <a class=\"eds-c-header__link\" href=\"https://link.springer.com/journals/\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t data-track=\"click||nav_find_a_journal\" data-track-context=\"unified header\" data-track-action=\"click find a journal\" data-track-category=\"unified header\" data-track-label=\"link\"\n\t\t\t\t\t\t\t\t\t\t\t\t>\n                            Find a journal\n                        </a>\n                    \n                        <a class=\"eds-c-header__link\" href=\"https://www.springernature.com/gp/authors\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t data-track=\"click||nav_how_to_publish\" data-track-context=\"unified header\" data-track-action=\"click publish with us link\" data-track-category=\"unified header\" data-track-label=\"link\"\n\t\t\t\t\t\t\t\t\t\t\t\t>\n                            Publish with us\n                        </a>\n                    \n                        <a class=\"eds-c-header__link\" href=\"https://link.springernature.com/home/\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t data-track=\"click||nav_track_your_research\" data-track-context=\"unified header\" data-track-action=\"click track your research\" data-track-category=\"unified header\" data-track-label=\"link\"\n\t\t\t\t\t\t\t\t\t\t\t\t>\n                            Track your research\n                        </a>\n                    \n                </div>\n\n                <div class=\"eds-c-header__link-container\">\n                    \n                        <div class=\"eds-c-header__item eds-c-header__item--divider\">\n                            <a href=\"#eds-c-header-popup-search\" class=\"eds-c-header__link\" data-eds-c-header-expander data-eds-c-header-test-search-btn>\n                                <svg class=\"eds-c-header__icon\" width=\"24\" height=\"24\" aria-hidden=\"true\" focusable=\"false\">\n                                    <use xlink:href=\"#icon-eds-i-search-medium\"></use>\n                                </svg><span>Search</span>\n                            </a>\n                        </div>\n                    \n                    \n                        <div id=\"ecommerce-header-cart-icon-link\" class=\"eds-c-header__item ecommerce-cart\" style=\"display:inline-block\">\n <a class=\"eds-c-header__link\" href=\"https://order.springer.com/public/cart\" style=\"appearance:none;border:none;background:none;color:inherit;position:relative\">\n  <svg id=\"eds-i-cart\" class=\"eds-c-header__icon\" xmlns=\"http://www.w3.org/2000/svg\" height=\"24\" width=\"24\" viewbox=\"0 0 24 24\" aria-hidden=\"true\" focusable=\"false\">\n   <path fill=\"currentColor\" fill-rule=\"nonzero\" d=\"M2 1a1 1 0 0 0 0 2l1.659.001 2.257 12.808a2.599 2.599 0 0 0 2.435 2.185l.167.004 9.976-.001a2.613 2.613 0 0 0 2.61-1.748l.03-.106 1.755-7.82.032-.107a2.546 2.546 0 0 0-.311-1.986l-.108-.157a2.604 2.604 0 0 0-2.197-1.076L6.042 5l-.56-3.17a1 1 0 0 0-.864-.82l-.12-.007L2.001 1ZM20.35 6.996a.63.63 0 0 1 .54.26.55.55 0 0 1 .082.505l-.028.1L19.2 15.63l-.022.05c-.094.177-.282.299-.526.317l-10.145.002a.61.61 0 0 1-.618-.515L6.394 6.999l13.955-.003ZM18 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4ZM8 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z\"></path>\n  </svg><span>Cart</span><span class=\"cart-info\" style=\"display:none;position:absolute;top:10px;right:45px;background-color:#C65301;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center\"></span></a>\n <script>(function () { var exports = {}; if (window.fetch) {\n            \n            \"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.headerWidgetClientInit = void 0;\nvar headerWidgetClientInit = function (getCartInfo) {\n    document.body.addEventListener(\"updatedCart\", function () {\n        updateCartIcon();\n    }, false);\n    return updateCartIcon();\n    function updateCartIcon() {\n        return getCartInfo()\n            .then(function (res) { return res.json(); })\n            .then(refreshCartState)\n            .catch(function (_) { });\n    }\n    function refreshCartState(json) {\n        var indicator = document.querySelector(\"#ecommerce-header-cart-icon-link .cart-info\");\n        /* istanbul ignore else */\n        if (indicator && json.itemCount) {\n            indicator.style.display = 'block';\n            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();\n            var moreThanOneItem = json.itemCount > 1;\n            indicator.setAttribute('title', \"there \".concat(moreThanOneItem ? \"are\" : \"is\", \" \").concat(json.itemCount, \" item\").concat(moreThanOneItem ? \"s\" : \"\", \" in your cart\"));\n        }\n        return json;\n    }\n};\nexports.headerWidgetClientInit = headerWidgetClientInit;\n\n            \n            headerWidgetClientInit(\n              function () {\n                return window.fetch(\"https://cart.springer.com/cart-info\", {\n                  credentials: \"include\",\n                  headers: { Accept: \"application/json\" }\n                })\n              }\n            )\n        }})()</script>\n</div>\n                    \n                </div>\n            </div>\n        </nav>\n    \n</header>\n\n\n\n    <article lang=\"en\" id=\"main\" class=\"app-masthead__colour-default\">\n        <section class=\"app-masthead \" aria-label=\"article masthead\">\n    <div class=\"app-masthead__container\">\n        \n            <div class=\"app-article-masthead u-sans-serif js-context-bar-sticky-point-masthead\" data-track-component=\"article\" data-test=\"masthead-component\">\n                <div class=\"app-article-masthead__info\">\n                    \n    \n        <nav aria-label=\"breadcrumbs\" data-test=\"breadcrumbs\">\n            <ol class=\"c-breadcrumbs c-breadcrumbs--contrast\" itemscope itemtype=\"https://schema.org/BreadcrumbList\">\n                \n                    <li class=\"c-breadcrumbs__item\" id=\"breadcrumb0\" itemprop=\"itemListElement\" itemscope=\"\" itemtype=\"https://schema.org/ListItem\">\n                        <a href=\"/\" class=\"c-breadcrumbs__link\" itemprop=\"item\" data-track=\"click||click_breadcrumb\" data-track-context=\"article page\"  data-track-category=\"article\" data-track-action=\"breadcrumbs\" data-track-label=\"breadcrumb1\"><span itemprop=\"name\">Home</span></a><meta itemprop=\"position\" content=\"1\">\n                            <svg class=\"c-breadcrumbs__chevron\" role=\"img\" aria-hidden=\"true\" focusable=\"false\" width=\"10\" height=\"10\" viewBox=\"0 0 10 10\">\n                                <path d=\"m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z\" fill-rule=\"evenodd\" transform=\"matrix(0 -1 1 0 0 10)\"/>\n                            </svg>\n                    </li>\n                \n                    <li class=\"c-breadcrumbs__item\" id=\"breadcrumb1\" itemprop=\"itemListElement\" itemscope=\"\" itemtype=\"https://schema.org/ListItem\">\n                        <a href=\"/journal/10676\" class=\"c-breadcrumbs__link\" itemprop=\"item\" data-track=\"click||click_breadcrumb\" data-track-context=\"article page\"  data-track-category=\"article\" data-track-action=\"breadcrumbs\" data-track-label=\"breadcrumb2\"><span itemprop=\"name\">Ethics and Information Technology</span></a><meta itemprop=\"position\" content=\"2\">\n                            <svg class=\"c-breadcrumbs__chevron\" role=\"img\" aria-hidden=\"true\" focusable=\"false\" width=\"10\" height=\"10\" viewBox=\"0 0 10 10\">\n                                <path d=\"m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z\" fill-rule=\"evenodd\" transform=\"matrix(0 -1 1 0 0 10)\"/>\n                            </svg>\n                    </li>\n                \n                    <li class=\"c-breadcrumbs__item\" id=\"breadcrumb2\" itemprop=\"itemListElement\" itemscope=\"\" itemtype=\"https://schema.org/ListItem\">\n                        <span itemprop=\"name\">Article</span><meta itemprop=\"position\" content=\"3\">\n                    </li>\n                \n            </ol>\n        </nav>\n    \n\n                    <h1 class=\"c-article-title\" data-test=\"article-title\" data-article-title=\"\">ChatGPT is bullshit</h1>\n\n                    <ul class=\"c-article-identifiers\">\n                        \n        <li class=\"c-article-identifiers__item\" data-test=\"article-category\">Original Paper</li>\n    \n        <li class=\"c-article-identifiers__item\">\n            <a href=\"https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research\" data-track=\"click\" data-track-action=\"open access\" data-track-label=\"link\" class=\"u-color-open-access\" data-test=\"open-access\">Open access</a>\n        </li>\n    \n    \n\n                        <li class=\"c-article-identifiers__item\">\n                            Published: <time datetime=\"2024-06-08\">08 June 2024</time>\n                        </li>\n                    </ul>\n                    <ul class=\"c-article-identifiers c-article-identifiers--cite-list\">\n                        <li class=\"c-article-identifiers__item\">\n                            <span data-test=\"journal-volume\">Volume 26</span>, article number <span data-test=\"article-number\">38</span>, (<span data-test=\"article-publication-year\">2024</span>)\n            \n                        </li>\n                        <li class=\"c-article-identifiers__item c-article-identifiers__item--cite\">\n                            <a href=\"#citeas\" data-track=\"click\" data-track-action=\"cite this article\" data-track-category=\"article body\" data-track-label=\"link\">Cite this article</a>\n                        </li>\n                    </ul>\n\n                    <div class=\"app-article-masthead__buttons\" data-test=\"download-article-link-wrapper\" data-track-context=\"masthead\">\n                        \n        <div class=\"c-pdf-container\">\n            <div class=\"c-pdf-download u-clear-both u-mb-16\">\n                <a href=\"/content/pdf/10.1007/s10676-024-09775-5.pdf\" class=\"u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link\" data-article-pdf=\"true\" data-readcube-pdf-url=\"true\" data-test=\"pdf-link\" data-draft-ignore=\"true\" data-track=\"click||content_download\" data-track-type=\"article pdf download\" data-track-action=\"download pdf\" data-track-label=\"button\" data-track-external download>\n                    \n                        <span class=\"c-pdf-download__text\">Download PDF</span>\n                        <svg aria-hidden=\"true\" focusable=\"false\" width=\"16\" height=\"16\" class=\"u-icon\"><use xlink:href=\"#icon-eds-i-download-medium\"/></svg>\n                    \n                </a>\n            </div>\n        </div>\n    \n\n                        <p class=\"app-article-masthead__access\">\n                            <svg width=\"16\" height=\"16\" focusable=\"false\" role=\"img\" aria-hidden=\"true\"><use xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#icon-eds-i-check-filled-medium\"></use></svg>\n                            You have full access to this <a href=\"https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research\" data-track=\"click\" data-track-action=\"open access\" data-track-label=\"link\">open access</a> article</p>\n                        \n                    </div>\n                </div>\n                <div class=\"app-article-masthead__brand\">\n                    \n                        \n                            <a href=\"/journal/10676\"\n                        \n                           class=\"app-article-masthead__journal-link\"\n                           data-track=\"click||click_journal_home\"\n                           data-track-action=\"journal homepage\"\n                           data-track-context=\"article page\"\n                           data-track-label=\"link\">\n                            <picture>\n                                <source type=\"image/webp\" media=\"(min-width: 768px)\" width=\"120\" height=\"159\"\n                                        srcset=\"https://media.springernature.com/w120/springer-static/cover-hires/journal/10676?as=webp,\n                                                    https://media.springernature.com/w316/springer-static/cover-hires/journal/10676?as=webp 2x\">\n                                <img width=\"72\" height=\"95\"\n                                     src=\"https://media.springernature.com/w72/springer-static/cover-hires/journal/10676?as=webp\"\n                                     srcset=\"https://media.springernature.com/w144/springer-static/cover-hires/journal/10676?as=webp 2x\" alt=\"\">\n                            </picture>\n                            <span class=\"app-article-masthead__journal-title\">Ethics and Information Technology</span>\n                        </a>\n                        \n                            <a href=\"https://link.springer.com/journal/10676/aims-and-scope\" class=\"app-article-masthead__submission-link\"\n                               data-track=\"click||click_aims_and_scope\"\n                               data-track-action=\"aims and scope\"\n                               data-track-context=\"article page\"\n                               data-track-label=\"link\">\n                                Aims and scope\n                                <svg width=\"16\" height=\"16\" focusable=\"false\" role=\"img\" aria-hidden=\"true\" class=\"u-icon\"><use xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#icon-eds-i-arrow-right-medium\"></use></svg>\n                            </a>\n                        \n                        \n                            <a href=\"https://www.editorialmanager.com/etin/\" class=\"app-article-masthead__submission-link\"\n                               data-track=\"click||click_submit_manuscript\"\n                               data-track-context=\"article masthead on springerlink article page\"\n                               data-track-action=\"submit manuscript\"\n                               data-track-label=\"link\">\n                                Submit manuscript\n                                <svg width=\"16\" height=\"16\" focusable=\"false\" role=\"img\" aria-hidden=\"true\" class=\"u-icon\"><use xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#icon-eds-i-arrow-right-medium\"></use></svg>\n                            </a>\n                        \n                    \n                </div>\n            </div>\n        \n    </div>\n</section>\n\n        <div class=\"c-article-main u-container u-mt-24 u-mb-32 l-with-sidebar\" id=\"main-content\"\n             data-component=\"article-container\">\n            <main class=\"u-serif js-main-column\" data-track-component=\"article body\">\n                \n                \n                    <div class=\"c-context-bar u-hide\"\n                         data-test=\"context-bar\"\n                         data-context-bar\n                         aria-hidden=\"true\">\n                        <div class=\"c-context-bar__container u-container\">\n                            <div class=\"c-context-bar__title\">\n                                ChatGPT is bullshit\n                            </div>\n                            \n                                <div data-test=\"inCoD\" data-track-context=\"sticky banner\">\n                                    \n        <div class=\"c-pdf-container\">\n            <div class=\"c-pdf-download u-clear-both u-mb-16\">\n                <a href=\"/content/pdf/10.1007/s10676-024-09775-5.pdf\" class=\"u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link\" data-article-pdf=\"true\" data-readcube-pdf-url=\"true\" data-test=\"pdf-link\" data-draft-ignore=\"true\" data-track=\"click||content_download\" data-track-type=\"article pdf download\" data-track-action=\"download pdf\" data-track-label=\"button\" data-track-external download>\n                    \n                        <span class=\"c-pdf-download__text\">Download PDF</span>\n                        <svg aria-hidden=\"true\" focusable=\"false\" width=\"16\" height=\"16\" class=\"u-icon\"><use xlink:href=\"#icon-eds-i-download-medium\"/></svg>\n                    \n                </a>\n            </div>\n        </div>\n    \n\n                                </div>\n                            \n                        </div>\n                    </div>\n                \n\n                \n\n                <div class=\"c-article-header\">\n                    <header>\n                        <ul class=\"c-article-author-list c-article-author-list--short\" data-test=\"authors-list\" data-component-authors-activator=\"authors-list\"><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Michael_Townsen-Hicks-Aff1\" data-author-popup=\"auth-Michael_Townsen-Hicks-Aff1\" data-author-search=\"Hicks, Michael Townsen\" data-corresp-id=\"c1\">Michael Townsen Hicks<svg width=\"16\" height=\"16\" focusable=\"false\" role=\"img\" aria-hidden=\"true\" class=\"u-icon\"><use xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#icon-eds-i-mail-medium\"></use></svg></a><span class=\"u-js-hide\"> \n            <a class=\"js-orcid\" href=\"http://orcid.org/0000-0002-1304-5668\"><span class=\"u-visually-hidden\">ORCID: </span>orcid.org/0000-0002-1304-5668</a></span><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a></sup>, </li><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-James-Humphries-Aff1\" data-author-popup=\"auth-James-Humphries-Aff1\" data-author-search=\"Humphries, James\">James Humphries</a><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a></sup> &amp; </li><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Joe-Slater-Aff1\" data-author-popup=\"auth-Joe-Slater-Aff1\" data-author-search=\"Slater, Joe\">Joe Slater</a><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a></sup> </li></ul>\n                        \n    \n\n                        <div data-test=\"article-metrics\">\n                            <ul class=\"app-article-metrics-bar u-list-reset\">\n                                \n    \n        <li class=\"app-article-metrics-bar__item\">\n            <p class=\"app-article-metrics-bar__count\"><svg class=\"u-icon app-article-metrics-bar__icon\" width=\"24\" height=\"24\" aria-hidden=\"true\" focusable=\"false\">\n                <use xlink:href=\"#icon-eds-i-accesses-medium\"></use>\n            </svg>489k <span class=\"app-article-metrics-bar__label\">Accesses</span></p>\n        </li>\n    \n    \n    \n        \n            <li class=\"app-article-metrics-bar__item\">\n                <p class=\"app-article-metrics-bar__count\"><svg class=\"u-icon app-article-metrics-bar__icon\" width=\"24\" height=\"24\" aria-hidden=\"true\" focusable=\"false\">\n                    <use xlink:href=\"#icon-eds-i-altmetric-medium\"></use>\n                </svg>1788 <span class=\"app-article-metrics-bar__label\">Altmetric</span></p>\n            </li>\n        \n    \n    \n        <li class=\"app-article-metrics-bar__item\">\n            <p class=\"app-article-metrics-bar__count\"><svg class=\"u-icon app-article-metrics-bar__icon app-article-metrics-bar__icon--mentions\" width=\"24\" height=\"24\" aria-hidden=\"true\" focusable=\"false\">\n                <use xlink:href=\"#icon-eds-i-mentions-medium\"></use>\n            </svg>34 <span class=\"app-article-metrics-bar__label\">Mentions</span></p>\n        </li>\n    \n    \n        <li class=\"app-article-metrics-bar__item app-article-metrics-bar__item--metrics\">\n            <p class=\"app-article-metrics-bar__details\"><a href=\"/article/10.1007/s10676-024-09775-5/metrics\" data-track=\"click\" data-track-action=\"view metrics\" data-track-label=\"link\" rel=\"nofollow\">Explore all metrics <svg class=\"u-icon app-article-metrics-bar__arrow-icon\" width=\"24\" height=\"24\" aria-hidden=\"true\" focusable=\"false\">\n                <use xlink:href=\"#icon-eds-i-arrow-right-medium\"></use>\n            </svg></a></p>\n        </li>\n    \n\n\n\n\n                            </ul>\n                        </div>\n                        \n    <div class=\"u-mt-32\">\n    \n\n    \n    </div>\n\n                        \n                    </header>\n                </div>\n\n                <div data-article-body=\"true\" data-track-component=\"article body\" class=\"c-article-body\">\n                    <section aria-labelledby=\"Abs1\" data-title=\"Abstract\" lang=\"en\"><div class=\"c-article-section\" id=\"Abs1-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"Abs1\">Abstract</h2><div class=\"c-article-section__content\" id=\"Abs1-content\"><p>Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called “AI hallucinations”. We argue that these falsehoods, and the overall activity of large language models, is better understood as <i>bullshit</i> in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. We distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.</p></div></div></section>\n                    \n    \n\n\n                    \n\n                    <div data-test=\"cobranding-download\">\n                        \n                    </div>\n\n                    \n                        \n        \n            <section aria-labelledby=\"inline-recommendations\" data-title=\"Inline Recommendations\" class=\"c-article-recommendations\" data-track-component=\"inline-recommendations\">\n                <h3 class=\"c-article-recommendations-title\" id=\"inline-recommendations\">Similar content being viewed by others</h3>\n                <div class=\"c-article-recommendations-list\">\n                    \n                        <div class=\"c-article-recommendations-list__item\">\n                            <article class=\"c-article-recommendations-card\" itemscope itemtype=\"http://schema.org/ScholarlyArticle\">\n                                \n                                    <div class=\"c-article-recommendations-card__img\"><img src=\"https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-81-322-3972-7?as&#x3D;webp\" loading=\"lazy\" alt=\"\"></div>\n                                \n                                <div class=\"c-article-recommendations-card__main\">\n                                    <h3 class=\"c-article-recommendations-card__heading\" itemprop=\"name headline\">\n                                        <a class=\"c-article-recommendations-card__link\"\n                                           itemprop=\"url\"\n                                           href=\"https://link.springer.com/10.1007/978-81-322-3972-7_19\"\n                                           data-track=\"click||select_recommendations_1\"\n                                           data-track-context=\"inline recommendations\"\n                                           data-track-action=\"click recommendations inline - 1\"\n                                           data-track-label=\"10.1007/978-81-322-3972-7_19\">Natural Language Processing\n                                        </a>\n                                    </h3>\n                                    <div class=\"c-article-meta-recommendations\">\n                                        <span class=\"c-article-meta-recommendations__item-type\">Chapter</span>\n                                        \n                                         <span class=\"c-article-meta-recommendations__date\">© 2020</span>\n                                    </div>\n                                </div>\n                            </article>\n                        </div>\n                    \n                        <div class=\"c-article-recommendations-list__item\">\n                            <article class=\"c-article-recommendations-card\" itemscope itemtype=\"http://schema.org/ScholarlyArticle\">\n                                \n                                    <div class=\"c-article-recommendations-card__img\"><img src=\"https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-981-99-7962-2?as&#x3D;webp\" loading=\"lazy\" alt=\"\"></div>\n                                \n                                <div class=\"c-article-recommendations-card__main\">\n                                    <h3 class=\"c-article-recommendations-card__heading\" itemprop=\"name headline\">\n                                        <a class=\"c-article-recommendations-card__link\"\n                                           itemprop=\"url\"\n                                           href=\"https://link.springer.com/10.1007/978-981-99-7962-2_30\"\n                                           data-track=\"click||select_recommendations_2\"\n                                           data-track-context=\"inline recommendations\"\n                                           data-track-action=\"click recommendations inline - 2\"\n                                           data-track-label=\"10.1007/978-981-99-7962-2_30\">Prompt Engineering in Large Language Models\n                                        </a>\n                                    </h3>\n                                    <div class=\"c-article-meta-recommendations\">\n                                        <span class=\"c-article-meta-recommendations__item-type\">Chapter</span>\n                                        \n                                         <span class=\"c-article-meta-recommendations__date\">© 2024</span>\n                                    </div>\n                                </div>\n                            </article>\n                        </div>\n                    \n                        <div class=\"c-article-recommendations-list__item\">\n                            <article class=\"c-article-recommendations-card\" itemscope itemtype=\"http://schema.org/ScholarlyArticle\">\n                                \n                                    <div class=\"c-article-recommendations-card__img\"><img src=\"https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs12559-023-10236-2/MediaObjects/12559_2023_10236_Fig1_HTML.png\" loading=\"lazy\" alt=\"\"></div>\n                                \n                                <div class=\"c-article-recommendations-card__main\">\n                                    <h3 class=\"c-article-recommendations-card__heading\" itemprop=\"name headline\">\n                                        <a class=\"c-article-recommendations-card__link\"\n                                           itemprop=\"url\"\n                                           href=\"https://link.springer.com/10.1007/s12559-023-10236-2\"\n                                           data-track=\"click||select_recommendations_3\"\n                                           data-track-context=\"inline recommendations\"\n                                           data-track-action=\"click recommendations inline - 3\"\n                                           data-track-label=\"10.1007/s12559-023-10236-2\">Transforming Conversations with AI—A Comprehensive Study of ChatGPT\n                                        </a>\n                                    </h3>\n                                    <div class=\"c-article-meta-recommendations\">\n                                        <span class=\"c-article-meta-recommendations__item-type\">Article</span>\n                                        \n                                         <span class=\"c-article-meta-recommendations__date\">24 January 2024</span>\n                                    </div>\n                                </div>\n                            </article>\n                        </div>\n                    \n                </div>\n            </section>\n        \n            <script>\n                window.dataLayer = window.dataLayer || [];\n                window.dataLayer.push({\n                    recommendations: {\n                        recommender: 'topic',\n                        model: 'visits_v2',\n                        policy_id: 'speedy-BootstrappedUCB',\n                        timestamp: 1720623402,\n                        embedded_user: 'null'\n                    }\n                });\n            </script>\n        \n    \n                    \n\n                    \n                        \n    <div class=\"app-card-service\" data-test=\"article-checklist-banner\">\n        <div>\n            <a class=\"app-card-service__link\" data-track=\"click||click_presubmission_checklist\" data-track-context=\"article page top of reading companion\" data-track-category=\"pre-submission-checklist\" data-track-action=\"clicked article page checklist banner test 2 old version\" data-track-label=\"link\" href=\"https://beta.springernature.com/pre-submission?journalId=10676\"\n            data-test=\"article-checklist-banner-link\">\n            <span class=\"app-card-service__link-text\">Use our pre-submission checklist</span>\n            <svg class=\"app-card-service__link-icon\" aria-hidden=\"true\" focusable=\"false\"><use xlink:href=\"#icon-eds-i-arrow-right-small\"></use></svg>\n            </a>\n            <p class=\"app-card-service__description\">Avoid common mistakes on your manuscript.</p>\n        </div>\n        <div class=\"app-card-service__icon-container\">\n            <svg class=\"app-card-service__icon\" aria-hidden=\"true\" focusable=\"false\">\n                <use xlink:href=\"#icon-eds-i-clipboard-check-medium\"></use>\n            </svg>\n        </div>\n    </div>\n\n                    \n\n                    \n                        \n                                <div class=\"main-content\">\n                                    <section data-title=\"Introduction\"><div class=\"c-article-section\" id=\"Sec1-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"Sec1\">Introduction</h2><div class=\"c-article-section__content\" id=\"Sec1-content\"><p>Large language models (LLMs), programs which use reams of available text and probability calculations in order to create seemingly-human-produced writing, have become increasingly sophisticated and convincing over the last several years, to the point where some commentators suggest that we may now be approaching the creation of artificial general intelligence (see e.g. Knight, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Knight, W. (2023). Some glimpse AGI in ChatGPT. others call it a mirage. Wired, August 18 2023, accessed via &#xA;                  https://www.wired.com/story/chatgpt-agi-intelligence/&#xA;                  &#xA;                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR13\" id=\"ref-link-section-d173038264e359\">2023</a> and Sarkar, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Sarkar, A. (2023). ChatGPT 5 is on track to attain artificial general intelligence. The Statesman, April 12, 2023. Accesses via &#xA;                  https://www.thestatesman.com/supplements/science_supplements/chatgpt-5-is-on-track-to-attain-artificial-general-intelligence-1503171366.html&#xA;                  &#xA;                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR25\" id=\"ref-link-section-d173038264e362\">2023</a>). Alongside worries about the rise of Skynet and the use of LLMs such as ChatGPT to replace work that could and should be done by humans, one line of inquiry concerns what exactly these programs are up to: in particular, there is a question about the nature and meaning of the text produced, and of its connection to truth. In this paper, we argue against the view that when ChatGPT and the like produce false claims they are lying or even hallucinating, and in favour of the position that the activity they are engaged in is bullshitting, in the Frankfurtian sense (Frankfurt, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2002\" title=\"Frankfurt, H. (2002). Reply to cohen. In S. Buss, &amp; L. Overton (Eds.), The contours of agency: Essays on themes from Harry Frankfurt. MIT Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR11\" id=\"ref-link-section-d173038264e365\">2002</a>, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2005\" title=\"Frankfurt, H. (2005). On Bullshit, Princeton.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR12\" id=\"ref-link-section-d173038264e368\">2005</a>). Because these programs cannot themselves be concerned with truth, and because they are designed to produce text that <i>looks</i> truth-apt without any actual concern for truth, it seems appropriate to call their outputs bullshit.</p><p>We think that this is worth paying attention to. Descriptions of new technology, including metaphorical ones, guide policymakers’ and the public’s understanding of new technology; they also inform applications of the new technology. They tell us what the technology is for and what it can be expected to do. Currently, false statements by ChatGPT and other large language models are described as “hallucinations”, which give policymakers and the public the idea that these systems are misrepresenting the world, and describing what they “see”. We argue that this is an inapt metaphor which will misinform the public, policymakers, and other interested parties.</p><p>The structure of the paper is as follows: in the first section, we outline how ChatGPT and similar LLMs operate. Next, we consider the view that when they make factual errors, they are lying or hallucinating: that is, deliberately uttering falsehoods, or blamelessly uttering them on the basis of misleading input information. We argue that neither of these ways of thinking are accurate, insofar as both lying and hallucinating require some concern with the truth of their statements, whereas LLMs are simply not designed to accurately represent the way the world is, but rather to <i>give the impression</i> that this is what they’re doing. This, we suggest, is very close to at least one way that Frankfurt talks about bullshit. We draw a distinction between two sorts of bullshit, which we call ‘hard’ and ‘soft’ bullshit, where the former requires an active attempt to deceive the reader or listener as to the nature of the enterprise, and the latter only requires a lack of concern for truth. We argue that at minimum, the outputs of LLMs like ChatGPT are soft bullshit: bullshit–that is, speech or text produced without concern for its truth–that is produced without any intent to mislead the audience about the utterer’s attitude towards truth. We also suggest, more controversially, that ChatGPT may indeed produce hard bullshit: if we view it as having intentions (for example, in virtue of how it is designed), then the fact that it is designed to give the impression of concern for truth qualifies it as attempting to mislead the audience about its aims, goals, or agenda. So, with the caveat that the particular kind of bullshit ChatGPT outputs is dependent on particular views of mind or meaning, we conclude that it is appropriate to talk about ChatGPT-generated text as bullshit, and flag up why it matters that – rather than thinking of its untrue claims as lies or hallucinations – we call bullshit on ChatGPT.</p></div></div></section><section data-title=\"What is ChatGPT?\"><div class=\"c-article-section\" id=\"Sec2-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"Sec2\">What is ChatGPT?</h2><div class=\"c-article-section__content\" id=\"Sec2-content\"><p>Large language models are becoming increasingly good at carrying on convincing conversations. The most prominent large language model is OpenAI’s ChatGPT, so it’s the one we will focus on; however, what we say carries over to other neural network-based AI chatbots, including Google’s Bard chatbot, AnthropicAI’s Claude (claude.ai), and Meta’s LLaMa. Despite being merely complicated bits of software, these models are surprisingly human-like when discussing a wide variety of topics. Test it yourself: anyone can go to the OpenAI web interface and ask for a ream of text; typically, it produces text which is indistinguishable from that of your average English speaker or writer. The variety, length, and similarity to human-generated text that GPT-4 is capable of has convinced many commentators to think that this chatbot has finally cracked it: that this is real (as opposed to merely nominal) artificial intelligence, one step closer to a human-like mind housed in a silicon brain.</p><p>However, large language models, and other AI models like ChatGPT, are doing considerably less than what human brains do, and it is not clear whether they do what they do in the same way we do. The most obvious difference between an LLM and a human mind involves the <i>goals</i> of the system. Humans have a variety of goals and behaviours, most of which are extra-linguistic: we have basic physical desires, for things like food and sustenance; we have social goals and relationships; we have projects; and we create physical objects. Large language models simply aim to replicate human speech or writing. This means that their primary goal, insofar as they have one, is to produce human-like text. They do so by estimating the likelihood that a particular word will appear next, given the text that has come before.</p><p>The machine does this by constructing a massive statistical model, one which is based on large amounts of text, mostly taken from the internet. This is done with relatively little input from human researchers or the designers of the system; rather, the model is designed by constructing a large number of nodes, which act as probability functions for a word to appear in a text given its context and the text that has come before it. Rather than putting in these probability functions by hand, researchers feed the system large amounts of text and train it by having it make next-word predictions about this training data. They then give it positive or negative feedback depending on whether it predicts correctly. Given enough text, the machine can construct a statistical model giving the likelihood of the next word in a block of text all by itself.</p><p>This model associates with each word a vector which locates it in a high-dimensional abstract space, near other words that occur in similar contexts and far from those which don’t. When producing text, it looks at the previous string of words and constructs a different vector, locating the word’s surroundings – its context – near those that occur in the context of similar words. We can think of these heuristically as representing the meaning of the word and the content of its context. But because these spaces are constructed using machine learning by repeated statistical analysis of large amounts of text, we can’t know what sorts of similarity are represented by the dimensions of this high-dimensional vector space. Hence we do not know how similar they are to what we think of as meaning or context. The model then takes these two vectors and produces a set of likelihoods for the next word; it selects and places one of the more likely ones—though not always the most likely. Allowing the model to choose randomly amongst the more likely words produces more creative and human-like text; the parameter which controls this is called the ‘temperature’ of the model and increasing the model’s temperature makes it both seem more creative and more likely to produce falsehoods. The system then repeats the process until it has a recognizable, complete-looking response to whatever prompt it has been given.</p><p>Given this process, it’s not surprising that LLMs have a problem with the truth. Their goal is to provide a normal-seeming response to a prompt, not to convey information that is helpful to their interlocutor. Examples of this are already numerous, for instance, a lawyer recently prepared his brief using ChatGPT and discovered to his chagrin that most of the cited cases were not real (Weiser, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Weiser, B. (2023). Here’s what happens when your lawyer uses ChatGPT. New York Times, May 23, 2023. Accessed via &#xA;                  https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html&#xA;                  &#xA;                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR27\" id=\"ref-link-section-d173038264e407\">2023</a>); as Judge P. Kevin Castel put it, ChatGPT produced a text filled with “bogus judicial decisions, with bogus quotes and bogus internal citations”. Similarly, when computer science researchers tested ChatGPT’s ability to assist in academic writing, they found that it was able to produce surprisingly comprehensive and sometimes even accurate text on biological subjects given the right prompts. But when asked to produce evidence for its claims, “it provided five references dating to the early 2000s. None of the provided paper titles existed, and all provided PubMed IDs (PMIDs) were of different unrelated papers” (Alkaissi and McFarland, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Alkaissi, H., &amp; McFarlane, S. I., (2023, February 19). Artificial hallucinations in ChatGPT: Implications in scientific writing. Cureus, 15(2), e35179. &#xA;                  https://doi.org/10.7759/cureus.35179&#xA;                  &#xA;                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR1\" id=\"ref-link-section-d173038264e410\">2023</a>). These errors can “snowball”: when the language model is asked to provide evidence for or a deeper explanation of a false claim, it rarely checks itself; instead it confidently producesmore false but normal-sounding claims (Zhang et al. <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Zhang (2023). How language model hallucinations can snowball. ArXiv preprint: arXiv:, 2305, 13534v1.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR30\" id=\"ref-link-section-d173038264e413\">2023</a>). The accuracy problem for LLMs and other generative Ais is often referred to as the problem of “AI hallucination”: the chatbot seems to be hallucinating sources and facts that don’t exist. These inaccuracies are referred to as “hallucinations” in both technical (OpenAI, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"OpenAI (2023). GPT-4 technical report. ArXiv Preprint: arXiv, 2303, 08774v3.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR23\" id=\"ref-link-section-d173038264e416\">2023</a>) and popular contexts (Weise &amp; Metz, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Weise, K., &amp; Metz, C. (2023). When AI chatbots hallucinate. New York Times, May 9, 2023. Accessed via &#xA;                  https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html&#xA;                  &#xA;                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR28\" id=\"ref-link-section-d173038264e419\">2023</a>).</p><p>These errors are pretty minor if the only point of a chatbot is to mimic human speech or communication. But the companies designing and using these bots have grander plans: chatbots could replace Google or Bing searches with a more user-friendly conversational interface (Shah &amp; Bender, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2022\" title=\"Shah, C., &amp; Bender, E. M. (2022). Situating search. CHIIR ‘22: Proceedings of the 2022 Conference on Human Information Interaction and Retrieval March 2022 Pages 221–232 &#xA;                  https://doi.org/10.1145/3498366.3505816&#xA;                  &#xA;                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR26\" id=\"ref-link-section-d173038264e425\">2022</a>; Zhu et al., <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Zhu, T., et al. (2023). Large language models for information retrieval: A survey. Arxiv Preprint: arXiv, 2308, 17107v2.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR31\" id=\"ref-link-section-d173038264e428\">2023</a>), or assist doctors or therapists in medical contexts (Lysandrou, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Lysandrou (2023). Comparative analysis of drug-GPT and ChatGPT LLMs for healthcare insights: Evaluating accuracy and relevance in patient and HCP contexts. ArXiv Preprint: arXiv, 2307, 16850v1.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR17\" id=\"ref-link-section-d173038264e431\">2023</a>). In these cases, accuracy is important and the errors represent a serious problem.</p><p>One attempted solution is to hook the chatbot up to some sort of database, search engine, or computational program that can answer the questions that the LLM gets wrong (Zhu et al., <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Zhu, T., et al. (2023). Large language models for information retrieval: A survey. Arxiv Preprint: arXiv, 2308, 17107v2.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR31\" id=\"ref-link-section-d173038264e437\">2023</a>). Unfortunately, this doesn’t work very well either. For example, when ChatGPT is connected to Wolfram Alpha, a powerful piece of mathematical software, it improves moderately in answering simple mathematical questions. But it still regularly gets things wrong, especially for questions which require multi-stage thinking (Davis &amp; Aaronson, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Davis, E., &amp; Aaronson, S. (2023). Testing GPT-4 with Wolfram alpha and code interpreter plub-ins on math and science problems. Arxiv Preprint: arXiv, 2308, 05713v2.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR6\" id=\"ref-link-section-d173038264e440\">2023</a>). And when connected to search engines or other databases, the models are still fairly likely to provide fake information unless they are given very specific instructions–and even then things aren’t perfect (Lysandrou, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Lysandrou (2023). Comparative analysis of drug-GPT and ChatGPT LLMs for healthcare insights: Evaluating accuracy and relevance in patient and HCP contexts. ArXiv Preprint: arXiv, 2307, 16850v1.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR17\" id=\"ref-link-section-d173038264e443\">2023</a>). OpenAI has plans to rectify this by training the model to do step by step reasoning (Lightman et al., <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Lightman, H., et al. (2023). Let’s verify step by step. Arxiv Preprint: arXiv, 2305, 20050.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR16\" id=\"ref-link-section-d173038264e446\">2023</a>) but this is quite resource-intensive, and there is reason to be doubtful that it will completely solve the problem—nor is it clear that the result will be a large language model, rather than some broader form of AI.</p><p>Solutions such as connecting the LLM to a database don’t work is because, if the models are <i>trained</i> on the database, then the words in the database affect the probability that the chatbot will add one or another word to the line of text it is generating. But this will only make it produce text similar to the text in the database; doing so will make it more likely that it reproduces the information in the database but by no means ensures that it will.</p><p>On the other hand, the LLM can also be connected to the database by allowing it to consult the database, in a way similar to the way it consults or talks to its human interlocutors. In this way, it can use the outputs of the database as text which it responds to and builds on. Here’s one way this can work: when a human interlocutor asks the language model a question, it can then translate the question into a query for the database. Then, it takes the response of the database as an input and builds a text from it to provide back to the human questioner. But this can misfire too, as the chatbots might ask the database the wrong question, or misinterpret its answer (Davis &amp; Aaronson, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Davis, E., &amp; Aaronson, S. (2023). Testing GPT-4 with Wolfram alpha and code interpreter plub-ins on math and science problems. Arxiv Preprint: arXiv, 2308, 05713v2.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR6\" id=\"ref-link-section-d173038264e458\">2023</a>). “GPT-4 often struggles to formulate a problem in a way that Wolfram Alpha can accept or that produces useful output.” This is not unrelated to the fact that when the language model generates a query for the database or computational module, it does so in the same way it generates text for humans: by estimating the likelihood that some output “looks like’’ the kind of thing the database will correspond with.</p><p>One might worry that these failed methods for improving the accuracy of chatbots are connected to the inapt metaphor of AI hallucinations. If the AI is <i>misperceiving</i> or <i>hallucinating</i> sources, one way to rectify this would be to put it in touch with real rather than hallucinated sources. But attempts to do so have failed.</p><p>The problem here isn’t that large language models hallucinate, lie, or misrepresent the world in some way. It’s that they are not designed to represent the world at all; instead, they are designed to convey convincing lines of text. So when they are provided with a database of some sort, they use this, in one way or another, to make their responses more convincing. But they are not in any real way attempting to convey or transmit the information in the database. As Chirag Shah and Emily Bender put it: “Nothing in the design of language models (whose training task is to predict words given context) is actually designed to handle arithmetic, temporal reasoning, etc. To the extent that they sometimes get the right answer to such questions is only because they happened to synthesize relevant strings out of what was in their training data. No reasoning is involved […] Similarly, language models are prone to making stuff up […] because they are not designed to express some underlying set of information in natural language; they are only manipulating the form of language” (Shah &amp; Bender, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2022\" title=\"Shah, C., &amp; Bender, E. M. (2022). Situating search. CHIIR ‘22: Proceedings of the 2022 Conference on Human Information Interaction and Retrieval March 2022 Pages 221–232 &#xA;                  https://doi.org/10.1145/3498366.3505816&#xA;                  &#xA;                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR26\" id=\"ref-link-section-d173038264e474\">2022</a>). These models aren’t designed to transmit information, so we shouldn’t be too surprised when their assertions turn out to be false.</p></div></div></section><section data-title=\"Lies, ‘hallucinations’ and bullshit\"><div class=\"c-article-section\" id=\"Sec3-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"Sec3\">Lies, ‘hallucinations’ and bullshit</h2><div class=\"c-article-section__content\" id=\"Sec3-content\"><h3 class=\"c-article__sub-heading\" id=\"Sec4\">Frankfurtian bullshit and lying</h3><p>Many popular discussions of ChatGPT call its false statements ‘hallucinations’. One also might think of these untruths as lies. However, we argue that this isn’t the right way to think about it. We will argue that these falsehoods aren’t hallucinations later – in Sect. 3.2.3. For now, we’ll discuss why these untruths aren’t lies but instead are bullshit.</p><p>The topic of lying has a rich philosophical literature. In ‘Lying’, Saint Augustine distinguished seven types of lies, and his view altered throughout his life. At one point, he defended the position that any instance of knowingly uttering a false utterance counts as a lie, so that even jokes containing false propositions, like –</p><blockquote class=\"c-blockquote\"><div class=\"c-blockquote__body\">\n                  <p>I entered a pun competition and because I really wanted to win, I submitted ten entries. I was sure one of them would win, but no pun in ten did.</p>\n                </div></blockquote><p>– would be regarded as a lie, as I have never entered such a competition (Proops &amp; Sorensen, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Proops, I., &amp; Sorensen, R. (2023). Destigmatizing the exegetical attribution of lies: the case of kant. Pacific Philosophical Quarterly. &#xA;                  https://doi.org/10.1111/papq.12442&#xA;                  &#xA;                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR24\" id=\"ref-link-section-d173038264e500\">2023</a>: 3). Later, this view is refined such that the speaker only lies if they intend the hearer to believe the utterance. The suggestion that the speaker must intend to deceive is a common stipulation in literature on lies. According to the “traditional account” of lying:</p><blockquote class=\"c-blockquote\"><div class=\"c-blockquote__body\">\n                  <p>To lie = <sub>df</sub>. to make a believed-false statement to another person with the intention that the other person believe that statement to be true (Mahon, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2015\" title=\"Mahon, J. E. (2015). The definition of lying and deception. The Stanford Encyclopedia of Philosophy (Winter 2016 Edition), Edward N. Zalta (Ed.), &#xA;                  https://plato.stanford.edu/archives/win2016/entries/lying-definition/&#xA;                  &#xA;                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR18\" id=\"ref-link-section-d173038264e509\">2015</a>).</p>\n                </div></blockquote><p>For our purposes this definition will suffice. Lies are generally frowned upon. But there are acts of misleading testimony which are criticisable, which do not fall under the umbrella of lying.<sup><a href=\"#Fn1\"><span class=\"u-visually-hidden\">Footnote </span>1</a></sup> These include spreading untrue gossip, which one mistakenly, but culpably, believes to be true. Another class of misleading testimony that has received particular attention from philosophers is that of bullshit. This everyday notion was analysed and introduced into the philosophical lexicon by Harry Frankfurt.<sup><a href=\"#Fn2\"><span class=\"u-visually-hidden\">Footnote </span>2</a></sup></p><p>Frankfurt understands bullshit to be characterized not by an intent to deceive but instead by a reckless disregard for the truth. A student trying to sound knowledgeable without having done the reading, a political candidate saying things because they sound good to potential voters, and a dilettante trying to spin an interesting story: none of these people are trying to deceive, but they are also not trying to convey facts. To Frankfurt, they are bullshitting.</p><p>Like “lie”, “bullshit” is both a noun and a verb: an utterance produced can be a lie or an instance of bullshit, as can the act of producing these utterances. For an utterance to be classed as bullshit, it must not be accompanied by the explicit intentions that one has when lying, i.e., to cause a false belief in the hearer. Of course, it must also not be accompanied by the intentions characterised by an honest utterance. So far this story is entirely negative. Must any positive intentions be manifested in the utterer?</p><p>Throughout most of Frankfurt’s discussion, his characterisation of bullshit is negative. He notes that bullshit requires “no conviction” from the speaker about what the truth is (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2005\" title=\"Frankfurt, H. (2005). On Bullshit, Princeton.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR12\" id=\"ref-link-section-d173038264e546\">2005</a>: 55), that the bullshitter “pays no attention” to the truth (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2005\" title=\"Frankfurt, H. (2005). On Bullshit, Princeton.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR12\" id=\"ref-link-section-d173038264e549\">2005</a>: 61) and that they “may not deceive us, or even intend to do so, either about the facts or what he takes the facts to be” (2005: 54). Later, he describes the “defining feature” of bullshit as “<i>a lack of concern</i> with truth, or an <i>indifference to how things really are</i> [our emphasis]” (2002: 340). These suggest a negative picture; that for an output to be classed as bullshit, it only needs to lack a certain relationship to the truth.</p><p>However, in places, a positive intention is presented. Frankfurt says what a bullshitter ….</p><p>“…does necessarily attempt to deceive us about is his enterprise. His only indispensably distinctive characteristic is that in a certain way he misrepresents what he is up to” (2005: 54).</p><p>This is somewhat surprising. It restricts what counts as bullshit to utterances accompanied by a higher-order deception. However, some of Frankfurt’s examples seem to lack this feature. When Fania Pascal describes her unwell state as “feeling like a dog that has just been run over” to her friend Wittgenstein, it stretches credulity to suggest that she was intending to deceive him about how much she knew about how run-over dogs felt. And given how the conditions for bullshit are typically described as negative, we might wonder whether the positive condition is really necessary.</p><h3 class=\"c-article__sub-heading\" id=\"Sec5\">Bullshit distinctions</h3><p>Should utterances without an intention to deceive count as bullshit? One reason in favour of expanding the definition, or embracing a plurality of bullshit, is indicated by Frankfurt’s comments on the dangers of bullshit.</p><p>“In contrast [to merely unintelligible discourse], indifference to the truth is extremely dangerous. The conduct of civilized life, and the vitality of the institutions that are indispensable to it, depend very fundamentally on respect for the distinction between the true and the false. Insofar as the authority of this distinction is undermined by the prevalence of bullshit and by the mindlessly frivolous attitude that accepts the proliferation of bullshit as innocuous, an indispensable human treasure is squandered” (2002: 343).</p><p>These dangers seem to manifest regardless of whether there is an intention to deceive about the enterprise a speaker is engaged in. Compare the deceptive bullshitter, who does aim to mislead us about being in the truth-business, with someone who harbours no such aim, but just talks for the sake of talking (without care, or indeed any thought, about the truth-values of their utterances).</p><p>One of Frankfurt’s examples of bullshit seems better captured by the wider definition. He considers the advertising industry, which is “replete with instances of bullshit so unmitigated that they serve among the most indisputable and classic paradigms of the concept” (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2005\" title=\"Frankfurt, H. (2005). On Bullshit, Princeton.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR12\" id=\"ref-link-section-d173038264e584\">2005</a>:22). However, it seems to misconstrue many advertisers to portray their aims as to mislead about their agendas. They are <i>expected</i> to say misleading things. Frankfurt discusses Marlboro adverts with the message that smokers are as brave as cowboys (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2002\" title=\"Frankfurt, H. (2002). Reply to cohen. In S. Buss, &amp; L. Overton (Eds.), The contours of agency: Essays on themes from Harry Frankfurt. MIT Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR11\" id=\"ref-link-section-d173038264e590\">2002</a>: 341). Is it reasonable to suggest that the advertisers pretended to believe this?</p><p>Frankfurt does allow for multiple species of bullshit (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2002\" title=\"Frankfurt, H. (2002). Reply to cohen. In S. Buss, &amp; L. Overton (Eds.), The contours of agency: Essays on themes from Harry Frankfurt. MIT Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR11\" id=\"ref-link-section-d173038264e597\">2002</a>: 340).<sup><a href=\"#Fn3\"><span class=\"u-visually-hidden\">Footnote </span>3</a></sup> Following this suggestion, we propose to envisage bullshit as a genus, and Frankfurt’s intentional bullshit as one species within this genus. Other species may include that produced by the advertiser, who anticipates that no one will believe their utterances<sup><a href=\"#Fn4\"><span class=\"u-visually-hidden\">Footnote </span>4</a></sup> or someone who has no intention one way or another about whether they mislead their audience. To that end, consider the following distinction:</p>\n                  <h3 class=\"c-article__sub-heading\" id=\"FPar1\">Bullshit (general)</h3>\n                  <p>Any utterance produced where a speaker has indifference towards the truth of the utterance.</p>\n                \n                  <h3 class=\"c-article__sub-heading\" id=\"FPar2\">Hard bullshit</h3>\n                  <p>Bullshit produced with the intention to mislead the audience about the utterer’s agenda.</p>\n                \n                  <h3 class=\"c-article__sub-heading\" id=\"FPar3\">Soft bullshit</h3>\n                  <p>Bullshit produced without the intention to mislead the hearer regarding the utterer’s agenda.</p>\n                <p>The general notion of bullshit is useful: on some occasions, we might be confident that an utterance was either soft bullshit or hard bullshit, but be unclear which, given our ignorance of the speaker’s higher-order desires.<sup><a href=\"#Fn5\"><span class=\"u-visually-hidden\">Footnote </span>5</a></sup> In such a case, we can still call bullshit.</p><p>Frankfurt’s own explicit account, with the positive requirements about producer’s intentions, is hard bullshit, whereas soft bullshit seems to describe some of Frankfurt’s examples, such as that of Pascal’s conversation with Wittgenstein, or the work of advertising agencies. It might be helpful to situate these distinctions in the existing literature. On our view, hard bullshit is most closely aligned with Cassam (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2019\" title=\"Cassam, Q. (2019). Vices of the mind. Oxford University Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR3\" id=\"ref-link-section-d173038264e672\">2019</a>), and Frankfurt’s positive account, for the reason that all of these views hold that some intention must be present, rather than merely absent, for the utterance to be bullshit: a kind of “epistemic insouciance” or vicious attitude towards truth on Cassam’s view, and (as we have seen) an intent to mislead the hearer about the utterer’s agenda on Frankfurt’s view. In Sect. 3.2 we consider whether ChatGPT may be a hard bullshitter, but it is important to note that it seems to us that hard bullshit, like the two accounts cited here, requires one to take a stance on whether or not LLMs can be agents, and so comes with additional argumentative burdens.</p><p>Soft bullshit, by contrast, captures only Frankfurt’s negative requirement – that is, the indifference towards truth that we have classed as definitional of bullshit (general) – for the reasons given above. As we argue, ChatGPT is at minimum a soft bullshitter or a bullshit machine, because if it is not an agent then it can neither hold any attitudes towards truth nor towards deceiving hearers about its (or, perhaps more properly, its users’) agenda.</p><p>It’s important to note that even this more modest kind of bullshitting will have the deleterious effects that concern Frankfurt: as he says, “indifference to the truth is extremely dangerous…by the mindlessly frivolous attitude that accepts the proliferation of bullshit as innocuous, an indispensable human treasure is squandered” (2002, p343). By treating ChatGPT and similar LLMs as being in any way concerned with truth, or by speaking metaphorically as if they make mistakes or suffer “hallucinations” in pursuit of true claims, we risk exactly this acceptance of bullshit, and this squandering of meaning – so, irrespective of whether or not ChatGPT is a hard or a soft bullshitter, it does produce bullshit, and it does matter.</p></div></div></section><section data-title=\"ChatGPT is bullshit\"><div class=\"c-article-section\" id=\"Sec6-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"Sec6\">ChatGPT is bullshit</h2><div class=\"c-article-section__content\" id=\"Sec6-content\"><p>With this distinction in hand, we’re now in a position to consider a worry of the following sort: Is ChatGPT hard bullshitting, soft bullshitting, or neither? We will argue, first, that ChatGPT, and other LLMs, are clearly soft bullshitting. However, the question of whether these chatbots are hard bullshitting is a trickier one, and depends on a number of complex questions concerning whether ChatGPT can be ascribed intentions. We canvas a few ways in which ChatGPT can be understood to have the requisite intentions in Sect. 3.2.</p><h3 class=\"c-article__sub-heading\" id=\"Sec7\">ChatGPT is a soft bullshitter</h3><p>We are not confident that chatbots can be correctly described as having any intentions at all, and we’ll go into this in more depth in the next Sect. (3.2). But we are quite certain that ChatGPT does not intend to convey truths, and so is a soft bullshitter. We can produce an easy argument by cases for this. Either ChatGPT has intentions or it doesn’t. If ChatGPT has no intentions at all, it trivially doesn’t intend to convey truths. So, it is indifferent to the truth value of its utterances and so is a soft bullshitter.</p><p>What if ChatGPT does have intentions? In Sect. 1, we argued that ChatGPT is not designed to produce true utterances; rather, it is designed to produce text which is indistinguishable from the text produced by humans. It is aimed at being convincing rather than accurate. The basic architecture of these models reveals this: they are designed to come up with a <i>likely continuation of a string of text.</i> It’s reasonable to assume that one way of being a likely continuation of a text is by being true; if humans are roughly more accurate than chance, true sentences will be more likely than false ones. This might make the chatbot more accurate than chance, but it does not give the chatbot any intention to convey truths. This is similar to standard cases of human bullshitters, who don’t care whether their utterances are true; good bullshit often contains some degree of truth, that’s part of what makes it convincing. A bullshitter can be more accurate than chance while still being indifferent to the truth of their utterances. We conclude that, even if the chatbot can be described as having intentions, it is indifferent to whether its utterances are true. It does not and cannot care about the truth of its output.</p><p>Presumably ChatGPT can’t care about conveying or hiding the truth, since it can’t care about anything. So, just as a matter of conceptual necessity, it meets one of Frankfurt’s criteria for bullshit. However, this only gets us so far – a rock can’t care about anything either, and it would be patently absurd to suggest that this means rocks are bullshitters<sup><a href=\"#Fn6\"><span class=\"u-visually-hidden\">Footnote </span>6</a></sup>. Similarly books can contain bullshit, but they are not themselves bullshitters. Unlike rocks – or even books – ChatGPT itself produces text, and looks like it performs speech acts independently of its users and designers. And while there is considerable disagreement concerning whether ChatGPT has intentions, it’s widely agreed that the sentences it produces are (typically) meaningful (see e.g. Mandelkern and Linzen <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Mandelkern, M., &amp; Linzen, T. (2023). Do language models’ Words Refer?. ArXiv Preprint: arXiv, 2308, 05576.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR20\" id=\"ref-link-section-d173038264e719\">2023</a>).</p><p>ChatGPT functions not to convey truth or falsehood but rather to convince the reader of – to use Colbert’s apt coinage – the <i>truthiness</i> of its statement, and ChatGPT is designed in such a way as to make attempts at bullshit efficacious (in a way that pens, dictionaries, etc., are not). So, it seems that at minimum, ChatGPT is a soft bullshitter: if we take it not to have intentions, there isn’t any attempt to mislead about the attitude towards truth, but it <i>is</i> nonetheless engaged in the business of outputting utterances that look as if they’re truth-apt. We conclude that ChatGPT is a <i>soft bullshitter.</i></p><h3 class=\"c-article__sub-heading\" id=\"Sec8\">ChatGPT as hard bullshit</h3><p>But is ChatGPT a <i>hard bullshitter</i>? A critic might object, it is simply inappropriate to think of programs like ChatGPT as hard bullshitters, because (i) they are not agents, or relatedly, (ii) they do not and cannot intend anything whatsoever.</p><p>We think this is too fast. First, whether or not ChatGPT has agency, its creators and users do. And what they produce with it, we will argue, is bullshit. Second, we will argue that, regardless of whether it has agency, it does have a function; this function gives it characteristic goals, and possibly even intentions, which align with our definition of hard bullshit.</p><p>Before moving on, we should say what we mean when we ask whether ChatGPT is an agent. For the purposes of this paper, the central question is whether ChatGPT has intentions and or beliefs. Does it intend to deceive? Can it, in any literal sense, be said to have goals or aims? If so, does it intend to deceive us about the content of its utterances, or merely have the goal to appear to be a competent speaker? Does it have beliefs—internal representational states which aim to track the truth? If so, do its utterances match those beliefs (in which case its false statements might be something like hallucinations) or are its utterances not matched to the beliefs—in which case they are likely to be either lies or bullshit? We will consider these questions in more depth in Sect. 3.2.2.</p><p>There are other philosophically important aspects of agenthood that we will not be considering. We won’t be considering whether ChatGPT makes decisions, has or lacks autonomy, or is conscious; we also won’t worry whether ChatGPT is morally responsible for its statements or its actions (if it has any of those).</p><h4 class=\"c-article__sub-heading c-article__sub-heading--small\" id=\"Sec9\">ChatGPT is a bullshit machine</h4><p>We will argue that even if ChatGPT is not, itself, a hard bullshitter, it is nonetheless a bullshit machine. The bullshitter is the person using it, since they (i) don’t care about the truth of what it says, (ii) want the reader to believe what the application outputs. On Frankfurt’s view, bullshit is bullshit even if uttered with no intent to bullshit: if something is bullshit to start with, then its repetition “is bullshit as he [or it] repeats it, insofar as it was originated by someone who was unconcerned with whether what he was saying is true or false” (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2022\" title=\"Frankfurt, H. (2002). Reply to cohen. In S. Buss, &amp; L. Overton (Eds.), The contours of agency: Essays on themes from Harry Frankfurt. MIT Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR11\" id=\"ref-link-section-d173038264e761\">2022</a>, p340).</p><p>This just pushes the question back to who the originator is, though: take the (increasingly frequent) example of the student essay created by ChatGPT. If the student cared about accuracy and truth, they would not use a program that infamously makes up sources whole-cloth. Equally, though, if they give it a prompt to produce an essay on philosophy of science and it produces a recipe for Bakewell tarts, then it won’t have the desired effect. So the idea of ChatGPT as a bullshit machine seems right, but also as if it’s missing something: someone can produce bullshit using their voice, a pen or a word processor, after all, but we don’t standardly think of these things as being bullshit machines, or of outputting bullshit in any particularly interesting way – conversely, there <i>does</i> seem to be something particular to ChatGPT, to do with the way that it operates, which makes it more than a <i>mere</i> tool, and which suggests that it might appropriately be thought of as an originator of bullshit. In short, it doesn’t seem quite right either to think of ChatGPT as analogous to a pen (can be used for bullshit, but can create nothing without deliberate and wholly agent-directed action) nor as to a bullshitting human (who can intend and produce bullshit on their own initiative).</p><p>The idea of ChatGPT as a bullshit machine is a helpful one when combined with the distinction between hard and soft bullshit. Reaching again for the example of the dodgy student paper: we’ve all, I take it, marked papers where it was obvious that a dictionary or thesaurus had been deployed with a crushing lack of subtlety; where fifty-dollar words are used not because they’re the best choice, nor even because they serve to obfuscate the truth, but simply because the author wants to convey an <i>impression</i> of understanding and sophistication. It would be inappropriate to call the dictionary a bullshit artist in this case; but it would <i>not</i> be inappropriate to call the result bullshit. So perhaps we should, strictly, say not that ChatGPT <i>is</i> bullshit but that it <i>outputs</i> bullshit in a way that goes beyond being simply a vector of bullshit: it does not and cannot care about the truth of its output, <i>and</i> the person using it does so not to convey truth or falsehood but rather to convince the hearer that the text was written by a interested and attentive agent.</p><h4 class=\"c-article__sub-heading c-article__sub-heading--small\" id=\"Sec10\">ChatGPT may be a hard bullshitter</h4><p>Is ChatGPT itself a hard bullshitter? If so, it must have intentions or goals: it must intend to deceive its listener, not about the content of its statements, but instead about its agenda. Recall that hard bullshitters, like the unprepared student or the incompetent politician, don’t care whether their statements are true or false, but do intend to deceive their audience about what they are doing. If so, it must have intentions or goals: it must intend to deceive its listener, not about the content of its statements, but instead about its agenda. We don’t think that ChatGPT is an agent or has intentions in precisely the same way that humans do (see Levenstein and Herrmann (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference forthcoming\" title=\"Levenstein, B. A., &amp; Herrmann, D. A. (forthcoming). Still no lie detector for language models: Probing empirical and conceptual roadblocks. Philosophical Studies, 1–27.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR14\" id=\"ref-link-section-d173038264e799\">forthcoming</a>) for a discussion of the issues here). But when speaking loosely it is remarkably easy to use intentional language to describe it: what is ChatGPT <i>trying</i> to do? Does it <i>care</i> whether the text it produces is accurate? We will argue that there is a robust, although perhaps not literal, sense in which ChatGPT does intend to deceive us about its agenda: its goal is not to convince us of the content of its utterances, but instead to portray itself as a ‘normal’ interlocutor like ourselves. By contrast, there is no similarly strong sense in which ChatGPT confabulates, lies, or hallucinates.</p><p>Our case will be simple: ChatGPT’s primary function is to imitate human speech. If this function is intentional, it is precisely the sort of intention that is required for an agent to be a hard bullshitter: in performing the function, ChatGPT is attempting to deceive the audience about its agenda. Specifically, it’s trying to seem like something that has an agenda, when in many cases it does not. We’ll discuss here whether this function gives rise to, or is best thought of, as an intention. In the next Sect. (3.2.3), we will argue that ChatGPT has no similar function or intention which would justify calling it a confabulator, liar, or hallucinator.</p><p>How do we know that ChatGPT functions as a hard bullshitter? Programs like ChatGPT are designed to do a task, and this task is remarkably like what Frankfurt thinks the bullshitter intends, namely to deceive the reader about the nature of the enterprise – in this case, to deceive the reader into thinking that they’re reading something produced by a being with intentions and beliefs.</p><p>ChatGPT’s text production algorithm was developed and honed in a process quite similar to artificial selection. Functions and selection processes have the same sort of directedness that human intentions do; naturalistic philosophers of mind have long connected them to the intentionality of human and animal mental states. If ChatGPT is understood as having intentions or intention-like states in this way, its intention is to present itself in a certain way (as a conversational agent or interlocutor) rather than to represent and convey facts. In other words, it has the intentions we associate with hard bullshitting.</p><p>One way we can think of ChatGPT as having intentions is by adopting Dennett’s <i>intentional stance</i> towards it. Dennett (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 1987\" title=\"Dennett, D. C. (1987). The intentional stance. The MIT.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR7\" id=\"ref-link-section-d173038264e824\">1987</a>: 17) describes the intentional stance as a way of predicting the behaviour of systems whose purpose we don’t already know.</p><p>“To adopt the intentional stance […] is to decide – tentatively, of course – to attempt to characterize, predict, and explain […] behavior by using intentional idioms, such as ‘believes’ and ‘wants,’ a practice that assumes or presupposes the rationality” of the target system (Dennett, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 1983\" title=\"Dennett, D. C. (1983). Intentional systems in cognitive ethology: The panglossian paradigm defended. Behavioral and Brain Sciences, 6, 343–390.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR8\" id=\"ref-link-section-d173038264e830\">1983</a>: 345).</p><p>Dennett suggests that if we know why a system was designed, we can make predictions on the basis of its design (1987). While we do know that ChatGPT was designed to chat, its exact algorithm and the way it produces its responses has been developed by machine learning, so we do not know its precise details of how it works and what it does. Under this ignorance it is tempting to bring in intentional descriptions to help us understand and predict what ChatGPT is doing.</p><p>When we adopt the intentional stance, we will be making bad predictions if we attribute any desire to convey truth to ChatGPT. Similarly, attributing “hallucinations” to ChatGPT will lead us to predict as if it has perceived things that aren’t there, when what it is doing is much more akin to making something up because it sounds about right. The former intentional attribution will lead us to try to correct its beliefs, and fix its inputs --- a strategy which has had limited if any success. On the other hand, if we attribute to ChatGPT the intentions of a hard bullshitter, we will be better able to diagnose the situations in which it will make mistakes and convey falsehoods. If ChatGPT is trying to do anything, it is trying to portray itself as a person.</p><p>Since this reason for thinking ChatGPT is a hard bullshitter involves committing to one or more controversial views on mind and meaning, it is more tendentious than simply thinking of it as a bullshit machine; but regardless of whether or not the program has intentions, there clearly <i>is</i> an attempt to deceive the hearer or reader about the nature of the enterprise somewhere along the line, and in our view that justifies calling the output hard bullshit.</p><p>So, though it’s worth making the caveat, it doesn’t seem to us that it significantly affects how we should think of and talk about ChatGPT and bullshit: the person using it to turn out some paper or talk isn’t concerned either with conveying or covering up the truth (since both of those require attention to what the truth actually <i>is</i>), and neither is the system itself. Minimally, it churns out soft bullshit, and, given certain controversial assumptions about the nature of intentional ascription, it produces hard bullshit; the specific texture of the bullshit is not, for our purposes, important: either way, ChatGPT is a bullshitter.</p><h4 class=\"c-article__sub-heading c-article__sub-heading--small\" id=\"Sec11\">Bullshit? hallucinations? confabulations? The need for new terminology</h4><p>We have argued that we should use the terminology of bullshit, rather than “hallucinations” to describe the utterances produced by ChatGPT. The suggestion that “hallucination” terminology is inappropriate has also been noted by Edwards (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Edwards, B. (2023). Why ChatGPT and bing chat are so good at making things up. Ars Tecnica. &#xA;                  https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/&#xA;                  &#xA;                , accesssed 19th April, 2024.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR10\" id=\"ref-link-section-d173038264e859\">2023</a>), who favours the term “confabulation” instead. Why is our proposal better than this or other alternatives?</p><p>We object to the term hallucination because it carries certain misleading implications. When someone hallucinates they have a non-standard perceptual experience, but do not actually perceive some feature of the world (Macpherson, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2013\" title=\"Macpherson, F. (2013). The philosophy and psychology of hallucination: an introduction, in Hallucination, Macpherson and Platchias (Eds.), London: MIT Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR19\" id=\"ref-link-section-d173038264e865\">2013</a>), where “perceive” is understood as a success term, such that they do not actually perceive the object or property. This term is inappropriate for LLMs for a variety of reasons. First, as Edwards (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Edwards, B. (2023). Why ChatGPT and bing chat are so good at making things up. Ars Tecnica. &#xA;                  https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/&#xA;                  &#xA;                , accesssed 19th April, 2024.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR10\" id=\"ref-link-section-d173038264e868\">2023</a>) points out, the term hallucination anthropomorphises the LLMs. Edwards also notes that attributing resulting problems to “hallucinations” of the models may allow creators to “blame the AI model for faulty outputs instead of taking responsibility for the outputs themselves”, and we may be wary of such abdications of responsibility. LLMs do not perceive, so they surely do not “mis-perceive”. Second, what occurs in the case of an LLM delivering false utterances is not an unusual or deviant form of the process it usually goes through (as some claim is the case in hallucinations, e.g., disjunctivists about perception). The very same process occurs when its outputs happen to be true.</p><p>So much for “hallucinations”. What about Edwards’ preferred term, “confabulation”? Edwards (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Edwards, B. (2023). Why ChatGPT and bing chat are so good at making things up. Ars Tecnica. &#xA;                  https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/&#xA;                  &#xA;                , accesssed 19th April, 2024.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR10\" id=\"ref-link-section-d173038264e874\">2023</a>) says:</p><blockquote class=\"c-blockquote\"><div class=\"c-blockquote__body\">\n                    <p>In human psychology, a “confabulation” occurs when someone’s memory has a gap and the brain convincingly fills in the rest without intending to deceive others. ChatGPT does not work like the human brain, but the term “confabulation” arguably serves as a better metaphor because there’s a creative gap-filling principle at work […].</p>\n                  </div></blockquote><p>As Edwards notes, this is imperfect. Once again, the use of a human psychological term risks anthropomorphising the LLMs.</p><p>This term also suggests that there is something exceptional occurring when the LLM makes a false utterance, i.e., that in these occasions - and only these occasions - it “fills in” a gap in memory with something false. This too is misleading. Even when the ChatGPT does give us correct answers, its process is one of predicting the next token. In our view, it falsely indicates that ChatGPT is, in general, attempting to convey accurate information in its utterances. But there are strong reasons to think that it does not have beliefs that it is intending to share in general–see, for example, Levenstein and Herrmann (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference forthcoming\" title=\"Levenstein, B. A., &amp; Herrmann, D. A. (forthcoming). Still no lie detector for language models: Probing empirical and conceptual roadblocks. Philosophical Studies, 1–27.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR14\" id=\"ref-link-section-d173038264e889\">forthcoming</a>). In our view, it falsely indicates that ChatGPT is, in general, attempting to convey accurate information in its utterances. Where it does track truth, it does so indirectly, and incidentally.</p><p>This is why we favour characterising ChatGPT as a bullshit machine. This terminology avoids the implications that perceiving or remembering is going on in the workings of the LLM. We can also describe it as bullshitting whenever it produces outputs. Like the human bullshitter, some of the outputs will likely be true, while others not. And as with the human bullshitter, we should be wary of relying upon any of these outputs.</p></div></div></section><section data-title=\"Conclusion\"><div class=\"c-article-section\" id=\"Sec12-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"Sec12\">Conclusion</h2><div class=\"c-article-section__content\" id=\"Sec12-content\"><p>Investors, policymakers, and members of the general public make decisions on how to treat these machines and how to react to them based not on a deep technical understanding of how they work, but on the often metaphorical way in which their abilities and function are communicated. Calling their mistakes ‘hallucinations’ isn’t harmless: it lends itself to the confusion that the machines are in some way <i>misperceiving</i> but are nonetheless trying to convey something that they believe or have perceived. This, as we’ve argued, is the wrong metaphor. The machines are not trying to communicate something they believe or perceive. Their inaccuracy is not due to misperception or hallucination. As we have pointed out, they are not trying to convey information at all. They are bullshitting.</p><p>Calling chatbot inaccuracies ‘hallucinations’ feeds in to overblown hype about their abilities among technology cheerleaders, and could lead to unnecessary consternation among the general public. It also suggests solutions to the inaccuracy problems which might not work, and could lead to misguided efforts at AI alignment amongst specialists. It can also lead to the wrong attitude towards the machine when it gets things right: the inaccuracies show that it is bullshitting, even when it’s right. Calling these inaccuracies ‘bullshit’ rather than ‘hallucinations’ isn’t just more accurate (as we’ve argued); it’s good science and technology communication in an area that sorely needs it.</p></div></div></section>\n                                </div>\n                        \n                    \n\n                    <section data-title=\"Notes\"><div class=\"c-article-section\" id=\"notes-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"notes\">Notes</h2><div class=\"c-article-section__content\" id=\"notes-content\"><ol class=\"c-article-footnote c-article-footnote--listed\"><li class=\"c-article-footnote--listed__item\" id=\"Fn1\" data-counter=\"1.\"><div class=\"c-article-footnote--listed__content\"><p> A particularly surprising position is espoused by Fichte, who regards as lying not only lies of omission, but knowingly <i>not correcting</i> someone who is operating under a falsehood. For instance, if I was to wear a wig, and someone believed this to be my real hair, Fichte regards this as a lie, for which I am culpable. Bacin (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2021\" title=\"Bacin, S. (2021). My duties and the morality of others: Lying, truth and the good example in Fichte’s normative perfectionism. In S. Bacin, &amp; O. Ware (Eds.), Fichte’s system of Ethics: A critical guide. Cambridge University Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR2\" id=\"ref-link-section-d173038264e523\">2021</a>) for further discussion of Fichte’s position.</p></div></li><li class=\"c-article-footnote--listed__item\" id=\"Fn2\" data-counter=\"2.\"><div class=\"c-article-footnote--listed__content\"><p> Originally published in <i>Raritan</i>, VI(2) in 1986. References to that work here are from the 2005 book version.</p></div></li><li class=\"c-article-footnote--listed__item\" id=\"Fn3\" data-counter=\"3.\"><div class=\"c-article-footnote--listed__content\"><p> In making this comment, Frankfurt concedes that what Cohen calls “bullshit” is also worthy of the name. In Cohen’s use (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2002\" title=\"Frankfurt, H. (2002). Reply to cohen. In S. Buss, &amp; L. Overton (Eds.), The contours of agency: Essays on themes from Harry Frankfurt. MIT Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR11\" id=\"ref-link-section-d173038264e604\">2002</a>), bullshit is a type of unclarifiable text, which he associates with French Marxists. Several other authors have also explored this area in various ways in recent years, each adding valuable nuggets to the debate. Dennis Whitcomb and Kenny Easwaran expand the domains to which “bullshit” can be applied. Whitcomb argues there can be bullshit questions (as well as propositions), whereas Easwaran argues that we can fruitfully view some activities as bullshit (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Easwaran, K. (2023). Bullshit activities. Analytic Philosophy, 00, 1–23. &#xA;                  https://doi.org/10.1111/phib.12328&#xA;                  &#xA;                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR9\" id=\"ref-link-section-d173038264e607\">2023</a>).</p><p>While we accept that these offer valuable streaks of bullshit insight, we will restrict our discussion to the Frankfurtian framework. For those who want to wade further into these distinctions, Neil Levy’s Philosophy, Bullshit, and Peer Review (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Levy, N. (2023). Philosophy, Bullshit, and peer review. Camridge University.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR15\" id=\"ref-link-section-d173038264e613\">2023</a>) offers a taxonomical overview of the bullshit out there.</p></div></li><li class=\"c-article-footnote--listed__item\" id=\"Fn4\" data-counter=\"4.\"><div class=\"c-article-footnote--listed__content\"><p> This need not undermine their goal. The advertiser may intend to impress associations (e.g., positive thoughts like “cowboys” or “brave” with their cigarette brand) upon their audience, or reinforce/instil brand recognition.</p><p>Frankfurt describes this kind of scenario as occurring in a “bull session”: “Each of the contributors to a bull session relies…upon a general recognition that what he expresses or says is not to be understood as being what he means wholeheartedly or believes unequivocally to be true” (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2005\" title=\"Frankfurt, H. (2005). On Bullshit, Princeton.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR12\" id=\"ref-link-section-d173038264e625\">2005</a>: 37). Yet Frankfurt claims that the contents of bull sessions are distinct from bullshit.</p></div></li><li class=\"c-article-footnote--listed__item\" id=\"Fn5\" data-counter=\"5.\"><div class=\"c-article-footnote--listed__content\"><p> It’s worth noting that something like the distinction between hard and soft bullshitting we draw also occurs in Cohen (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2002\" title=\"Cohen, G. A. (2002). Deeper into bullshit. In S. Buss, &amp; L. Overton (Eds.), The contours of Agency: Essays on themes from Harry Frankfurt. MIT Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR5\" id=\"ref-link-section-d173038264e661\">2002</a>): he suggests that we might think of someone as a bullshitter as “a person who aims at bullshit, however frequently or infrequently he hits his target”, <i>or</i> if they are merely “disposed to bullshit: for whatever reason, to produce a lot of unclarifiable stuff” (p334). While we do not adopt Cohen’s account here, the parallels between his characterisation and our own are striking.</p></div></li><li class=\"c-article-footnote--listed__item\" id=\"Fn6\" data-counter=\"6.\"><div class=\"c-article-footnote--listed__content\"><p> Of course, rocks also can’t express propositions – but then, part of the worry here is whether ChatGPT actually <i>is</i> expressing propositions, or is simply a means through which agents express propositions. A further worry is that we shouldn’t even see ChatGPT as expressing propositions - perhaps there are no communicative intentions, and so we should see the outputs as meaningless. Even accepting this, we can still meaningfully talk about them as expressing propositions. This proposal - fictionalism about chatbots - has recently been discussed by Mallory (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Mallory, F. (2023). Fictionalism about chatbots. Ergo, 10(38), 1082–1100.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR21\" id=\"ref-link-section-d173038264e714\">2023</a>).</p></div></li></ol></div></div></section><div id=\"MagazineFulltextArticleBodySuffix\"><section aria-labelledby=\"Bib1\" data-title=\"References\"><div class=\"c-article-section\" id=\"Bib1-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"Bib1\">References</h2><div class=\"c-article-section__content\" id=\"Bib1-content\"><div data-container-section=\"references\"><ul class=\"c-article-references\" data-track-component=\"outbound reference\" data-track-context=\"references section\"><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR1\">Alkaissi, H., &amp; McFarlane, S. I., (2023, February 19). Artificial hallucinations in ChatGPT: Implications in scientific writing. <i>Cureus, 15</i>(2), e35179. <a href=\"https://doi.org/10.7759/cureus.35179\" data-track=\"click||click_references\" data-track-action=\"external reference\" data-track-value=\"external reference\" data-track-label=\"10.7759/cureus.35179\">https://doi.org/10.7759/cureus.35179</a>.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR2\">Bacin, S. (2021). My duties and the morality of others: Lying, truth and the good example in Fichte’s normative perfectionism. In S. Bacin, &amp; O. Ware (Eds.), <i>Fichte’s system of Ethics: A critical guide</i>. Cambridge University Press.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR3\">Cassam, Q. (2019). <i>Vices of the mind</i>. Oxford University Press.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR5\">Cohen, G. A. (2002). Deeper into bullshit. In S. Buss, &amp; L. Overton (Eds.), <i>The contours of Agency: Essays on themes from Harry Frankfurt</i>. MIT Press.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR6\">Davis, E., &amp; Aaronson, S. (2023). Testing GPT-4 with Wolfram alpha and code interpreter plub-ins on math and science problems. <i>Arxiv Preprint: arXiv, 2308</i>, 05713v2.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR8\">Dennett, D. C. (1983). Intentional systems in cognitive ethology: The panglossian paradigm defended. <i>Behavioral and Brain Sciences</i>, <i>6</i>, 343–390.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click||click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1017/S0140525X00016393\" data-track-item_id=\"10.1017/S0140525X00016393\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1017%2FS0140525X00016393\" aria-label=\"Article reference 6\" data-doi=\"10.1017/S0140525X00016393\">Article</a> \n    <a data-track=\"click||click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 6\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Intentional%20systems%20in%20Cognitive%20Ethology%3A%20The%20panglossian%20paradigm%20defended&amp;journal=Behavioral%20and%20Brain%20Sciences&amp;doi=10.1017%2FS0140525X00016393&amp;volume=6&amp;pages=343-390&amp;publication_year=1983&amp;author=Dennett%2CDC\">\n                    Google Scholar</a> \n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR7\">Dennett, D. C. (1987). <i>The intentional stance</i>. The MIT.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR29\">Dennis Whitcomb (2023). Bullshit questions. <i>Analysis</i>, <i>83</i>(2), 299–304.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR9\">Easwaran, K. (2023). Bullshit activities. <i>Analytic Philosophy</i>, <i>00</i>, 1–23. <a href=\"https://doi.org/10.1111/phib.12328\" data-track=\"click||click_references\" data-track-action=\"external reference\" data-track-value=\"external reference\" data-track-label=\"10.1111/phib.12328\">https://doi.org/10.1111/phib.12328</a>.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click||click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1111/phib.12328\" data-track-item_id=\"10.1111/phib.12328\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1111%2Fphib.12328\" aria-label=\"Article reference 9\" data-doi=\"10.1111/phib.12328\">Article</a> \n    <a data-track=\"click||click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 9\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Bullshit%20activities&amp;journal=Analytic%20Philosophy&amp;doi=10.1111%2Fphib.12328&amp;volume=00&amp;pages=1-23&amp;publication_year=2023&amp;author=Easwaran%2CK\">\n                    Google Scholar</a> \n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR10\">Edwards, B. (2023). Why ChatGPT and bing chat are so good at making things up. <i>Ars Tecnica</i>. <a href=\"https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/\" data-track=\"click||click_references\" data-track-action=\"external reference\" data-track-value=\"external reference\" data-track-label=\"https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/\">https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/</a>, accesssed 19th April, 2024.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR11\">Frankfurt, H. (2002). Reply to cohen. In S. Buss, &amp; L. Overton (Eds.), <i>The contours of agency: Essays on themes from Harry Frankfurt</i>. MIT Press.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR12\">Frankfurt, H. (2005). <i>On Bullshit</i>, Princeton.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR13\">Knight, W. (2023). Some glimpse AGI in ChatGPT. others call it a mirage. <i>Wired</i>, August 18 2023, accessed via <a href=\"https://www.wired.com/story/chatgpt-agi-intelligence/\" data-track=\"click||click_references\" data-track-action=\"external reference\" data-track-value=\"external reference\" data-track-label=\"https://www.wired.com/story/chatgpt-agi-intelligence/\">https://www.wired.com/story/chatgpt-agi-intelligence/</a>.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR14\">Levenstein, B. A., &amp; Herrmann, D. A. (forthcoming). Still no lie detector for language models: Probing empirical and conceptual roadblocks. <i>Philosophical Studies</i>, 1–27.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR15\">Levy, N. (2023). <i>Philosophy, Bullshit, and peer review</i>. Camridge University.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR16\">Lightman, H., et al. (2023). Let’s verify step by step. <i>Arxiv Preprint: arXiv</i>, <i>2305</i>, 20050.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR17\">Lysandrou (2023). Comparative analysis of drug-GPT and ChatGPT LLMs for healthcare insights: Evaluating accuracy and relevance in patient and HCP contexts. <i>ArXiv Preprint: arXiv</i>, <i>2307</i>, 16850v1.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR19\">Macpherson, F. (2013). The philosophy and psychology of hallucination: an introduction, in <i>Hallucination</i>, Macpherson and Platchias (Eds.), London: MIT Press.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR18\">Mahon, J. E. (2015). The definition of lying and deception. <i>The Stanford Encyclopedia of Philosophy</i> (Winter 2016 Edition), Edward N. Zalta (Ed.), <a href=\"https://plato.stanford.edu/archives/win2016/entries/lying-definition/\" data-track=\"click||click_references\" data-track-action=\"external reference\" data-track-value=\"external reference\" data-track-label=\"https://plato.stanford.edu/archives/win2016/entries/lying-definition/\">https://plato.stanford.edu/archives/win2016/entries/lying-definition/</a>.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR21\">Mallory, F. (2023). Fictionalism about chatbots. <i>Ergo</i>, <i>10</i>(38), 1082–1100.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click||click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 20\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Fictionalism%20about%20Chatbots&amp;journal=Ergo&amp;volume=10&amp;issue=38&amp;pages=1082-1100&amp;publication_year=2023&amp;author=Mallory%2CF\">\n                    Google Scholar</a> \n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR20\">Mandelkern, M., &amp; Linzen, T. (2023). Do language models’ Words Refer?. <i>ArXiv Preprint: arXiv</i>, <i>2308</i>, 05576.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR23\">OpenAI (2023). GPT-4 technical report. <i>ArXiv Preprint: arXiv</i>, <i>2303</i>, 08774v3.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR24\">Proops, I., &amp; Sorensen, R. (2023). Destigmatizing the exegetical attribution of lies: the case of kant. <i>Pacific Philosophical Quarterly</i>. <a href=\"https://doi.org/10.1111/papq.12442\" data-track=\"click||click_references\" data-track-action=\"external reference\" data-track-value=\"external reference\" data-track-label=\"10.1111/papq.12442\">https://doi.org/10.1111/papq.12442</a>.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click||click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1111/papq.12442\" data-track-item_id=\"10.1111/papq.12442\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1111%2Fpapq.12442\" aria-label=\"Article reference 23\" data-doi=\"10.1111/papq.12442\">Article</a> \n    <a data-track=\"click||click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 23\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Destigmatizing%20the%20Exegetical%20Attribution%20of%20lies%3A%20The%20case%20of%20Kant&amp;journal=Pacific%20Philosophical%20Quarterly&amp;doi=10.1111%2Fpapq.12442&amp;publication_year=2023&amp;author=Proops%2CI&amp;author=Sorensen%2CR\">\n                    Google Scholar</a> \n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR25\">Sarkar, A. (2023). ChatGPT 5 is on track to attain artificial general intelligence. <i>The Statesman</i>, April 12, 2023. Accesses via <a href=\"https://www.thestatesman.com/supplements/science_supplements/chatgpt-5-is-on-track-to-attain-artificial-general-intelligence-1503171366.html\" data-track=\"click||click_references\" data-track-action=\"external reference\" data-track-value=\"external reference\" data-track-label=\"https://www.thestatesman.com/supplements/science_supplements/chatgpt-5-is-on-track-to-attain-artificial-general-intelligence-1503171366.html\">https://www.thestatesman.com/supplements/science_supplements/chatgpt-5-is-on-track-to-attain-artificial-general-intelligence-1503171366.html</a>.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR26\">Shah, C., &amp; Bender, E. M. (2022). Situating search. CHIIR ‘22: Proceedings of the 2022 Conference on Human Information Interaction and Retrieval March 2022 Pages 221–232 <a href=\"https://doi.org/10.1145/3498366.3505816\" data-track=\"click||click_references\" data-track-action=\"external reference\" data-track-value=\"external reference\" data-track-label=\"10.1145/3498366.3505816\">https://doi.org/10.1145/3498366.3505816</a>.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR28\">Weise, K., &amp; Metz, C. (2023). When AI chatbots hallucinate. New York Times, May 9, 2023. Accessed via <a href=\"https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html\" data-track=\"click||click_references\" data-track-action=\"external reference\" data-track-value=\"external reference\" data-track-label=\"https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html\">https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html</a>.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR27\">Weiser, B. (2023). Here’s what happens when your lawyer uses ChatGPT. <i>New York Times</i>, May 23, 2023. Accessed via <a href=\"https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html\" data-track=\"click||click_references\" data-track-action=\"external reference\" data-track-value=\"external reference\" data-track-label=\"https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html\">https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html</a>.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR30\">Zhang (2023). How language model hallucinations can snowball. <i>ArXiv preprint: arXiv:</i>, <i>2305</i>, 13534v1.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR31\">Zhu, T., et al. (2023). Large language models for information retrieval: A survey. <i>Arxiv Preprint: arXiv</i>, <i>2308</i>, 17107v2.</p></li></ul><p class=\"c-article-references__download u-hide-print\"><a data-track=\"click\" data-track-action=\"download citation references\" data-track-label=\"link\" rel=\"nofollow\" href=\"https://citation-needed.springer.com/v2/references/10.1007/s10676-024-09775-5?format=refman&amp;flavour=references\">Download references<svg width=\"16\" height=\"16\" focusable=\"false\" role=\"img\" aria-hidden=\"true\" class=\"u-icon\"><use xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#icon-eds-i-download-medium\"></use></svg></a></p></div></div></div></section></div><section data-title=\"Acknowledgements\"><div class=\"c-article-section\" id=\"Ack1-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"Ack1\">Acknowledgements</h2><div class=\"c-article-section__content\" id=\"Ack1-content\"><p>Thanks to Neil McDonnell, Bryan Pickel, Fenner Tanswell, and the University of Glasgow’s Large Language Model reading group for helpful discussion and comments.</p></div></div></section><section aria-labelledby=\"author-information\" data-title=\"Author information\"><div class=\"c-article-section\" id=\"author-information-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"author-information\">Author information</h2><div class=\"c-article-section__content\" id=\"author-information-content\"><h3 class=\"c-article__sub-heading\" id=\"affiliations\">Authors and Affiliations</h3><ol class=\"c-article-author-affiliation__list\"><li id=\"Aff1\"><p class=\"c-article-author-affiliation__address\">University of Glasgow, Glasgow, Scotland</p><p class=\"c-article-author-affiliation__authors-list\">Michael Townsen Hicks, James Humphries &amp; Joe Slater</p></li></ol><div class=\"u-js-hide u-hide-print\" data-test=\"author-info\"><span class=\"c-article__sub-heading\">Authors</span><ol class=\"c-article-authors-search u-list-reset\"><li id=\"auth-Michael_Townsen-Hicks-Aff1\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Michael Townsen Hicks</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=Michael%20Townsen%20Hicks\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\" rel=\"nofollow\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in\n                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Michael%20Townsen%20Hicks\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\" rel=\"nofollow\">PubMed</a><span class=\"u-hide\"> </span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Michael%20Townsen%20Hicks%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\" rel=\"nofollow\">Google Scholar</a></span></p></div></div></li><li id=\"auth-James-Humphries-Aff1\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">James Humphries</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=James%20Humphries\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\" rel=\"nofollow\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in\n                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=James%20Humphries\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\" rel=\"nofollow\">PubMed</a><span class=\"u-hide\"> </span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22James%20Humphries%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\" rel=\"nofollow\">Google Scholar</a></span></p></div></div></li><li id=\"auth-Joe-Slater-Aff1\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Joe Slater</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=Joe%20Slater\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\" rel=\"nofollow\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in\n                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Joe%20Slater\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\" rel=\"nofollow\">PubMed</a><span class=\"u-hide\"> </span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Joe%20Slater%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\" rel=\"nofollow\">Google Scholar</a></span></p></div></div></li></ol></div><h3 class=\"c-article__sub-heading\" id=\"corresponding-author\">Corresponding author</h3><p id=\"corresponding-author-list\">Correspondence to\n                <a id=\"corresp-c1\" href=\"mailto:Michael.hicks@glasgow.ac.uk\">Michael Townsen Hicks</a>.</p></div></div></section><section data-title=\"Additional information\"><div class=\"c-article-section\" id=\"additional-information-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"additional-information\">Additional information</h2><div class=\"c-article-section__content\" id=\"additional-information-content\"><h3 class=\"c-article__sub-heading\">Publisher’s Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title=\"Rights and permissions\"><div class=\"c-article-section\" id=\"rightslink-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"rightslink\">Rights and permissions</h2><div class=\"c-article-section__content\" id=\"rightslink-content\">\n                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href=\"http://creativecommons.org/licenses/by/4.0/\" rel=\"license\">http://creativecommons.org/licenses/by/4.0/</a>.</p>\n              <p class=\"c-article-rights\"><a data-track=\"click\" data-track-action=\"view rights and permissions\" data-track-label=\"link\" href=\"https://s100.copyright.com/AppDispatchServlet?title=ChatGPT%20is%20bullshit&amp;author=Michael%20Townsen%20Hicks%20et%20al&amp;contentID=10.1007%2Fs10676-024-09775-5&amp;copyright=The%20Author%28s%29&amp;publication=1388-1957&amp;publicationDate=2024-06-08&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY\">Reprints and permissions</a></p></div></div></section><section aria-labelledby=\"article-info\" data-title=\"About this article\"><div class=\"c-article-section\" id=\"article-info-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"article-info\">About this article</h2><div class=\"c-article-section__content\" id=\"article-info-content\"><div class=\"c-bibliographic-information\"><div class=\"u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border\"><a data-crossmark=\"10.1007/s10676-024-09775-5\" target=\"_blank\" rel=\"noopener\" href=\"https://crossmark.crossref.org/dialog/?doi=10.1007/s10676-024-09775-5\" data-track=\"click\" data-track-action=\"Click Crossmark\" data-track-label=\"link\" data-test=\"crossmark\"><img loading=\"lazy\" width=\"57\" height=\"81\" alt=\"Check for updates. Verify currency and authenticity via CrossMark\" src=\"data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>\"></a></div><div class=\"c-bibliographic-information__column\"><h3 class=\"c-article__sub-heading\" id=\"citeas\">Cite this article</h3><p class=\"c-bibliographic-information__citation\">Hicks, M.T., Humphries, J. &amp; Slater, J. ChatGPT is bullshit.\n                    <i>Ethics Inf Technol</i> <b>26</b>, 38 (2024). https://doi.org/10.1007/s10676-024-09775-5</p><p class=\"c-bibliographic-information__download-citation u-hide-print\"><a data-test=\"citation-link\" data-track=\"click\" data-track-action=\"download article citation\" data-track-label=\"link\" data-track-external=\"\" rel=\"nofollow\" href=\"https://citation-needed.springer.com/v2/references/10.1007/s10676-024-09775-5?format=refman&amp;flavour=citation\">Download citation<svg width=\"16\" height=\"16\" focusable=\"false\" role=\"img\" aria-hidden=\"true\" class=\"u-icon\"><use xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#icon-eds-i-download-medium\"></use></svg></a></p><ul class=\"c-bibliographic-information__list\" data-test=\"publication-history\"><li class=\"c-bibliographic-information__list-item\"><p>Published<span class=\"u-hide\">: </span><span class=\"c-bibliographic-information__value\"><time datetime=\"2024-06-08\">08 June 2024</time></span></p></li><li class=\"c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width\"><p><abbr title=\"Digital Object Identifier\">DOI</abbr><span class=\"u-hide\">: </span><span class=\"c-bibliographic-information__value\">https://doi.org/10.1007/s10676-024-09775-5</span></p></li></ul><div data-component=\"share-box\"><div class=\"c-article-share-box u-display-none\" hidden=\"\"><h3 class=\"c-article__sub-heading\">Share this article</h3><p class=\"c-article-share-box__description\">Anyone you share the following link with will be able to read this content:</p><button class=\"js-get-share-url c-article-share-box__button\" type=\"button\" id=\"get-share-url\" data-track=\"click\" data-track-label=\"button\" data-track-external=\"\" data-track-action=\"get shareable link\">Get shareable link</button><div class=\"js-no-share-url-container u-display-none\" hidden=\"\"><p class=\"js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info\">Sorry, a shareable link is not currently available for this article.</p></div><div class=\"js-share-url-container u-display-none\" hidden=\"\"><p class=\"js-share-url c-article-share-box__only-read-input\" id=\"share-url\" data-track=\"click\" data-track-label=\"button\" data-track-action=\"select share url\"></p><button class=\"js-copy-share-url c-article-share-box__button--link-like\" type=\"button\" id=\"copy-share-url\" data-track=\"click\" data-track-label=\"button\" data-track-action=\"copy share url\" data-track-external=\"\">Copy to clipboard</button></div><p class=\"js-c-article-share-box__additional-info c-article-share-box__additional-info\">\n                            Provided by the Springer Nature SharedIt content-sharing initiative\n                        </p></div></div><h3 class=\"c-article__sub-heading\">Keywords</h3><ul class=\"c-article-subject-list\"><li class=\"c-article-subject-list__subject\"><span><a href=\"/search?query=Artificial%20intelligence&amp;facet-discipline=&#34;Computer%20Science&#34;\" data-track=\"click\" data-track-action=\"view keyword\" data-track-label=\"link\">Artificial intelligence</a></span></li><li class=\"c-article-subject-list__subject\"><span><a href=\"/search?query=Large%20language%20models&amp;facet-discipline=&#34;Computer%20Science&#34;\" data-track=\"click\" data-track-action=\"view keyword\" data-track-label=\"link\">Large language models</a></span></li><li class=\"c-article-subject-list__subject\"><span><a href=\"/search?query=LLMs&amp;facet-discipline=&#34;Computer%20Science&#34;\" data-track=\"click\" data-track-action=\"view keyword\" data-track-label=\"link\">LLMs</a></span></li><li class=\"c-article-subject-list__subject\"><span><a href=\"/search?query=ChatGPT&amp;facet-discipline=&#34;Computer%20Science&#34;\" data-track=\"click\" data-track-action=\"view keyword\" data-track-label=\"link\">ChatGPT</a></span></li><li class=\"c-article-subject-list__subject\"><span><a href=\"/search?query=Bullshit&amp;facet-discipline=&#34;Computer%20Science&#34;\" data-track=\"click\" data-track-action=\"view keyword\" data-track-label=\"link\">Bullshit</a></span></li><li class=\"c-article-subject-list__subject\"><span><a href=\"/search?query=Frankfurt&amp;facet-discipline=&#34;Computer%20Science&#34;\" data-track=\"click\" data-track-action=\"view keyword\" data-track-label=\"link\">Frankfurt</a></span></li><li class=\"c-article-subject-list__subject\"><span><a href=\"/search?query=Assertion&amp;facet-discipline=&#34;Computer%20Science&#34;\" data-track=\"click\" data-track-action=\"view keyword\" data-track-label=\"link\">Assertion</a></span></li><li class=\"c-article-subject-list__subject\"><span><a href=\"/search?query=Content&amp;facet-discipline=&#34;Computer%20Science&#34;\" data-track=\"click\" data-track-action=\"view keyword\" data-track-label=\"link\">Content</a></span></li></ul><div data-component=\"article-info-list\"></div></div></div></div></div></section>\n\n                    \n                </div>\n            </main>\n\n            <div class=\"c-article-sidebar u-text-sm u-hide-print l-with-sidebar__sidebar\" id=\"sidebar\"\n                 data-container-type=\"reading-companion\" data-track-component=\"reading companion\">\n                <aside>\n                    \n                        \n    <div class=\"app-card-service\" data-test=\"article-checklist-banner\">\n        <div>\n            <a class=\"app-card-service__link\" data-track=\"click||click_presubmission_checklist\" data-track-context=\"article page top of reading companion\" data-track-category=\"pre-submission-checklist\" data-track-action=\"clicked article page checklist banner test 2 old version\" data-track-label=\"link\" href=\"https://beta.springernature.com/pre-submission?journalId=10676\"\n            data-test=\"article-checklist-banner-link\">\n            <span class=\"app-card-service__link-text\">Use our pre-submission checklist</span>\n            <svg class=\"app-card-service__link-icon\" aria-hidden=\"true\" focusable=\"false\"><use xlink:href=\"#icon-eds-i-arrow-right-small\"></use></svg>\n            </a>\n            <p class=\"app-card-service__description\">Avoid common mistakes on your manuscript.</p>\n        </div>\n        <div class=\"app-card-service__icon-container\">\n            <svg class=\"app-card-service__icon\" aria-hidden=\"true\" focusable=\"false\">\n                <use xlink:href=\"#icon-eds-i-clipboard-check-medium\"></use>\n            </svg>\n        </div>\n    </div>\n\n                    \n\n                    \n                        <div data-test=\"collections\">\n                            \n    \n\n                        </div>\n                    \n\n                    <div data-test=\"editorial-summary\">\n                        \n                    </div>\n\n                    <div class=\"c-reading-companion\">\n                        <div class=\"c-reading-companion__sticky\" data-component=\"reading-companion-sticky\"\n                             data-test=\"reading-companion-sticky\">\n                            \n\n                            \n                                \n                            \n\n                            <div\n                                class=\"c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active\"\n                                id=\"tabpanel-sections\">\n                                <div class=\"u-lazy-ad-wrapper u-mt-16 u-hide\"\n                                     data-component-mpu><div class=\"c-ad c-ad--300x250\">\n    <div class=\"c-ad__inner\">\n        <p class=\"c-ad__label\">Advertisement</p>\n        <div id=\"div-gpt-ad-MPU1\"\n             class=\"div-gpt-ad grade-c-hide\"\n             data-pa11y-ignore\n             data-gpt\n             data-gpt-unitpath=\"/270604982/springerlink/10676/article\"\n             data-gpt-sizes=\"300x250\" data-test=\"MPU1-ad\"\n             data-gpt-targeting=\"pos=MPU1;articleid=s10676-024-09775-5;\">\n        </div>\n    </div>\n</div>\n\n</div>\n                            </div>\n                            <div\n                                class=\"c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width\"\n                                id=\"tabpanel-figures\"></div>\n                            <div\n                                class=\"c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width\"\n                                id=\"tabpanel-references\"></div>\n                        </div>\n                    </div>\n                </aside>\n            </div>\n        </div>\n    </article>\n    <div class=\"app-elements\">\n    \n    \n\n\n\n    \n        <div class=\"eds-c-header__expander eds-c-header__expander--search\" id=\"eds-c-header-popup-search\">\n            <h2 class=\"eds-c-header__heading\">Search</h2>\n            <div class=\"u-container\">\n                <search class=\"eds-c-header__search\" role=\"search\" aria-label=\"Search from the header\">\n                    <form method=\"GET\" action=\"//link.springer.com/search\"\n\t\t\t\t\t\t\t\t\t\t\t data-test=\"header-search\" data-track=\"submit||search\" data-track-context=\"search from header\" data-track-action=\"submit search form\" data-track-category=\"unified header\" data-track-label=\"form\"\n\t\t\t\t\t\t\t\t\t\t>\n                        <label for=\"eds-c-header-search\" class=\"eds-c-header__search-label\">Search by keyword or author</label>\n                        <div class=\"eds-c-header__search-container\">\n                            <input id=\"eds-c-header-search\" class=\"eds-c-header__search-input\" autocomplete=\"off\" name=\"query\" type=\"search\" value=\"\" required>\n                            <button class=\"eds-c-header__search-button\" type=\"submit\">\n                                <svg class=\"eds-c-header__icon\" aria-hidden=\"true\" focusable=\"false\">\n                                    <use xlink:href=\"#icon-eds-i-search-medium\"></use>\n                                </svg>\n                                <span class=\"u-visually-hidden\">Search</span>\n                            </button>\n                        </div>\n                    </form>\n                </search>\n            </div>\n        </div>\n    \n\n\n<div class=\"eds-c-header__expander eds-c-header__expander--menu\" id=\"eds-c-header-nav\">\n    \n        <h2 class=\"eds-c-header__heading\">Navigation</h2>\n        <ul class=\"eds-c-header__list\">\n            \n                <li class=\"eds-c-header__list-item\">\n                   <a class=\"eds-c-header__link\" href=\"https://link.springer.com/journals/\"\n\t\t\t\t\t\t\t\t\t\t  data-track=\"click||nav_find_a_journal\" data-track-context=\"unified header\" data-track-action=\"click find a journal\" data-track-category=\"unified header\" data-track-label=\"link\"\n\t\t\t\t\t\t\t\t\t >\n                        Find a journal\n\t\t\t\t\t\t\t\t\t </a>\n                </li>\n            \n                <li class=\"eds-c-header__list-item\">\n                   <a class=\"eds-c-header__link\" href=\"https://www.springernature.com/gp/authors\"\n\t\t\t\t\t\t\t\t\t\t  data-track=\"click||nav_how_to_publish\" data-track-context=\"unified header\" data-track-action=\"click publish with us link\" data-track-category=\"unified header\" data-track-label=\"link\"\n\t\t\t\t\t\t\t\t\t >\n                        Publish with us\n\t\t\t\t\t\t\t\t\t </a>\n                </li>\n            \n                <li class=\"eds-c-header__list-item\">\n                   <a class=\"eds-c-header__link\" href=\"https://link.springernature.com/home/\"\n\t\t\t\t\t\t\t\t\t\t  data-track=\"click||nav_track_your_research\" data-track-context=\"unified header\" data-track-action=\"click track your research\" data-track-category=\"unified header\" data-track-label=\"link\"\n\t\t\t\t\t\t\t\t\t >\n                        Track your research\n\t\t\t\t\t\t\t\t\t </a>\n                </li>\n            \n        </ul>\n    \n</div>\n    <footer >\n\t<div class=\"eds-c-footer\" >\n\t\t\n\t\t\t\n\t\t\t\t<div class=\"eds-c-footer__container\">\n\t\t<div class=\"eds-c-footer__grid eds-c-footer__group--separator\">\n\t\t\t\n\t\t\t<div class=\"eds-c-footer__group\">\n\t\t\t\t<h3 class=\"eds-c-footer__heading\">Discover content</h3>\n\t\t\t\t<ul class=\"eds-c-footer__list\">\n\t\t\t\t\t\n\t\t\t\t\t\t<li class=\"eds-c-footer__item\"><a class=\"eds-c-footer__link\" href=\"https://link.springer.com/journals/a/1\" data-track=\"click||nav_journals_a_z\" data-track-action=\"journals a-z\" data-track-context=\"unified footer\" data-track-label=\"link\">Journals A-Z</a></li>\n\t\t\t\t\t\n\t\t\t\t\t\t<li class=\"eds-c-footer__item\"><a class=\"eds-c-footer__link\" href=\"https://link.springer.com/books/a/1\" data-track=\"click||nav_books_a_z\" data-track-action=\"books a-z\" data-track-context=\"unified footer\" data-track-label=\"link\">Books A-Z</a></li>\n\t\t\t\t\t\n\t\t\t\t</ul>\n\t\t\t</div>\n\t\t\t\n\t\t\t<div class=\"eds-c-footer__group\">\n\t\t\t\t<h3 class=\"eds-c-footer__heading\">Publish with us</h3>\n\t\t\t\t<ul class=\"eds-c-footer__list\">\n\t\t\t\t\t\n\t\t\t\t\t\t<li class=\"eds-c-footer__item\"><a class=\"eds-c-footer__link\" href=\"https://www.springernature.com/gp/authors\" data-track=\"click||nav_publish_your_research\" data-track-action=\"publish your research\" data-track-context=\"unified footer\" data-track-label=\"link\">Publish your research</a></li>\n\t\t\t\t\t\n\t\t\t\t\t\t<li class=\"eds-c-footer__item\"><a class=\"eds-c-footer__link\" href=\"https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research\" data-track=\"click||nav_open_access_publishing\" data-track-action=\"open access publishing\" data-track-context=\"unified footer\" data-track-label=\"link\">Open access publishing</a></li>\n\t\t\t\t\t\n\t\t\t\t</ul>\n\t\t\t</div>\n\t\t\t\n\t\t\t<div class=\"eds-c-footer__group\">\n\t\t\t\t<h3 class=\"eds-c-footer__heading\">Products and services</h3>\n\t\t\t\t<ul class=\"eds-c-footer__list\">\n\t\t\t\t\t\n\t\t\t\t\t\t<li class=\"eds-c-footer__item\"><a class=\"eds-c-footer__link\" href=\"https://www.springernature.com/gp/products\" data-track=\"click||nav_our_products\" data-track-action=\"our products\" data-track-context=\"unified footer\" data-track-label=\"link\">Our products</a></li>\n\t\t\t\t\t\n\t\t\t\t\t\t<li class=\"eds-c-footer__item\"><a class=\"eds-c-footer__link\" href=\"https://www.springernature.com/gp/librarians\" data-track=\"click||nav_librarians\" data-track-action=\"librarians\" data-track-context=\"unified footer\" data-track-label=\"link\">Librarians</a></li>\n\t\t\t\t\t\n\t\t\t\t\t\t<li class=\"eds-c-footer__item\"><a class=\"eds-c-footer__link\" href=\"https://www.springernature.com/gp/societies\" data-track=\"click||nav_societies\" data-track-action=\"societies\" data-track-context=\"unified footer\" data-track-label=\"link\">Societies</a></li>\n\t\t\t\t\t\n\t\t\t\t\t\t<li class=\"eds-c-footer__item\"><a class=\"eds-c-footer__link\" href=\"https://www.springernature.com/gp/partners\" data-track=\"click||nav_partners_and_advertisers\" data-track-action=\"partners and advertisers\" data-track-context=\"unified footer\" data-track-label=\"link\">Partners and advertisers</a></li>\n\t\t\t\t\t\n\t\t\t\t</ul>\n\t\t\t</div>\n\t\t\t\n\t\t\t<div class=\"eds-c-footer__group\">\n\t\t\t\t<h3 class=\"eds-c-footer__heading\">Our imprints</h3>\n\t\t\t\t<ul class=\"eds-c-footer__list\">\n\t\t\t\t\t\n\t\t\t\t\t\t<li class=\"eds-c-footer__item\"><a class=\"eds-c-footer__link\" href=\"https://www.springer.com/\" data-track=\"click||nav_imprint_Springer\" data-track-action=\"Springer\" data-track-context=\"unified footer\" data-track-label=\"link\">Springer</a></li>\n\t\t\t\t\t\n\t\t\t\t\t\t<li class=\"eds-c-footer__item\"><a class=\"eds-c-footer__link\" href=\"https://www.nature.com/\" data-track=\"click||nav_imprint_Nature_Portfolio\" data-track-action=\"Nature Portfolio\" data-track-context=\"unified footer\" data-track-label=\"link\">Nature Portfolio</a></li>\n\t\t\t\t\t\n\t\t\t\t\t\t<li class=\"eds-c-footer__item\"><a class=\"eds-c-footer__link\" href=\"https://www.biomedcentral.com/\" data-track=\"click||nav_imprint_BMC\" data-track-action=\"BMC\" data-track-context=\"unified footer\" data-track-label=\"link\">BMC</a></li>\n\t\t\t\t\t\n\t\t\t\t\t\t<li class=\"eds-c-footer__item\"><a class=\"eds-c-footer__link\" href=\"https://www.palgrave.com/\" data-track=\"click||nav_imprint_Palgrave_Macmillan\" data-track-action=\"Palgrave Macmillan\" data-track-context=\"unified footer\" data-track-label=\"link\">Palgrave Macmillan</a></li>\n\t\t\t\t\t\n\t\t\t\t\t\t<li class=\"eds-c-footer__item\"><a class=\"eds-c-footer__link\" href=\"https://www.apress.com/\" data-track=\"click||nav_imprint_Apress\" data-track-action=\"Apress\" data-track-context=\"unified footer\" data-track-label=\"link\">Apress</a></li>\n\t\t\t\t\t\n\t\t\t\t</ul>\n\t\t\t</div>\n\t\t\t\n\t\t</div>\n\t</div>\n\n\t\t\n\t\t\n\t\t<div class=\"eds-c-footer__container\">\n\t\n\t\t<nav aria-label=\"footer navigation\">\n\t\t\t<ul class=\"eds-c-footer__links\">\n\t\t\t\t\n\t\t\t\t\t<li class=\"eds-c-footer__item\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t<button class=\"eds-c-footer__link\" data-cc-action=\"preferences\"\n\t\t\t\t\t\t\t\t data-track=\"click||dialog_manage_cookies\" data-track-action=\"Manage cookies\" data-track-context=\"unified footer\" data-track-label=\"link\"><span class=\"eds-c-footer__button-text\">Your privacy choices/Manage cookies</span></button>\n\t\t\t\t\t\t\n\t\t\t\t\t</li>\n\t\t\t\t\n\t\t\t\t\t<li class=\"eds-c-footer__item\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t<a class=\"eds-c-footer__link\" href=\"https://www.springernature.com/gp/legal/ccpa\"\n\t\t\t\t\t\t\t\t data-track=\"click||nav_california_privacy_statement\" data-track-action=\"california privacy statement\" data-track-context=\"unified footer\" data-track-label=\"link\">Your US state privacy rights</a>\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t</li>\n\t\t\t\t\n\t\t\t\t\t<li class=\"eds-c-footer__item\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t<a class=\"eds-c-footer__link\" href=\"https://www.springernature.com/gp/info/accessibility\"\n\t\t\t\t\t\t\t\t data-track=\"click||nav_accessibility_statement\" data-track-action=\"accessibility statement\" data-track-context=\"unified footer\" data-track-label=\"link\">Accessibility statement</a>\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t</li>\n\t\t\t\t\n\t\t\t\t\t<li class=\"eds-c-footer__item\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t<a class=\"eds-c-footer__link\" href=\"https://link.springer.com/termsandconditions\"\n\t\t\t\t\t\t\t\t data-track=\"click||nav_terms_and_conditions\" data-track-action=\"terms and conditions\" data-track-context=\"unified footer\" data-track-label=\"link\">Terms and conditions</a>\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t</li>\n\t\t\t\t\n\t\t\t\t\t<li class=\"eds-c-footer__item\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t<a class=\"eds-c-footer__link\" href=\"https://link.springer.com/privacystatement\"\n\t\t\t\t\t\t\t\t data-track=\"click||nav_privacy_policy\" data-track-action=\"privacy policy\" data-track-context=\"unified footer\" data-track-label=\"link\">Privacy policy</a>\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t</li>\n\t\t\t\t\n\t\t\t\t\t<li class=\"eds-c-footer__item\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t<a class=\"eds-c-footer__link\" href=\"https://support.springernature.com/en/support/home\"\n\t\t\t\t\t\t\t\t data-track=\"click||nav_help_and_support\" data-track-action=\"help and support\" data-track-context=\"unified footer\" data-track-label=\"link\">Help and support</a>\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t</li>\n\t\t\t\t\n\t\t\t</ul>\n\t\t</nav>\n\t\n\t\n\t\t\n\t\t\t<div class=\"eds-c-footer__user\">\n\t\t\t\t<p class=\"eds-c-footer__user-info\">\n\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t</p>\n\t\t\t\t<p class=\"eds-c-footer__user-info\" data-test=\"footer-business-partners\">Not affiliated</p>\n\t\t\t</div>\n\t\t\n\t\n\t\n\t\t<a href=\"https://www.springernature.com/\" class=\"eds-c-footer__link\">\n\t\t\t<img src=\"/oscar-static/images/logo-springernature-white-19dd4ba190.svg\" alt=\"Springer Nature\" loading=\"lazy\" width=\"200\" height=\"20\"/>\n\t\t</a>\n\t\n\t<p class=\"eds-c-footer__legal\" data-test=\"copyright\">&copy; 2024 Springer Nature</p>\n</div>\n\n\t</div>\n</footer>\n</div>\n\n\n    </body>\n</html>\n\n\n","oembed":false,"readabilityObject":{"title":"ChatGPT is bullshit","content":"<div id=\"readability-page-1\" class=\"page\"><div>\n                                    <div id=\"Sec1-section\" data-title=\"Introduction\"><h2 id=\"Sec1\">Introduction</h2><div id=\"Sec1-content\"><p>Large language models (LLMs), programs which use reams of available text and probability calculations in order to create seemingly-human-produced writing, have become increasingly sophisticated and convincing over the last several years, to the point where some commentators suggest that we may now be approaching the creation of artificial general intelligence (see e.g. Knight, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Knight, W. (2023). Some glimpse AGI in ChatGPT. others call it a mirage. Wired, August 18 2023, accessed via \n                  https://www.wired.com/story/chatgpt-agi-intelligence/\n                  \n                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR13\" id=\"ref-link-section-d173038264e359\">2023</a> and Sarkar, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Sarkar, A. (2023). ChatGPT 5 is on track to attain artificial general intelligence. The Statesman, April 12, 2023. Accesses via \n                  https://www.thestatesman.com/supplements/science_supplements/chatgpt-5-is-on-track-to-attain-artificial-general-intelligence-1503171366.html\n                  \n                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR25\" id=\"ref-link-section-d173038264e362\">2023</a>). Alongside worries about the rise of Skynet and the use of LLMs such as ChatGPT to replace work that could and should be done by humans, one line of inquiry concerns what exactly these programs are up to: in particular, there is a question about the nature and meaning of the text produced, and of its connection to truth. In this paper, we argue against the view that when ChatGPT and the like produce false claims they are lying or even hallucinating, and in favour of the position that the activity they are engaged in is bullshitting, in the Frankfurtian sense (Frankfurt, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2002\" title=\"Frankfurt, H. (2002). Reply to cohen. In S. Buss, &amp; L. Overton (Eds.), The contours of agency: Essays on themes from Harry Frankfurt. MIT Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR11\" id=\"ref-link-section-d173038264e365\">2002</a>, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2005\" title=\"Frankfurt, H. (2005). On Bullshit, Princeton.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR12\" id=\"ref-link-section-d173038264e368\">2005</a>). Because these programs cannot themselves be concerned with truth, and because they are designed to produce text that <i>looks</i> truth-apt without any actual concern for truth, it seems appropriate to call their outputs bullshit.</p><p>We think that this is worth paying attention to. Descriptions of new technology, including metaphorical ones, guide policymakers’ and the public’s understanding of new technology; they also inform applications of the new technology. They tell us what the technology is for and what it can be expected to do. Currently, false statements by ChatGPT and other large language models are described as “hallucinations”, which give policymakers and the public the idea that these systems are misrepresenting the world, and describing what they “see”. We argue that this is an inapt metaphor which will misinform the public, policymakers, and other interested parties.</p><p>The structure of the paper is as follows: in the first section, we outline how ChatGPT and similar LLMs operate. Next, we consider the view that when they make factual errors, they are lying or hallucinating: that is, deliberately uttering falsehoods, or blamelessly uttering them on the basis of misleading input information. We argue that neither of these ways of thinking are accurate, insofar as both lying and hallucinating require some concern with the truth of their statements, whereas LLMs are simply not designed to accurately represent the way the world is, but rather to <i>give the impression</i> that this is what they’re doing. This, we suggest, is very close to at least one way that Frankfurt talks about bullshit. We draw a distinction between two sorts of bullshit, which we call ‘hard’ and ‘soft’ bullshit, where the former requires an active attempt to deceive the reader or listener as to the nature of the enterprise, and the latter only requires a lack of concern for truth. We argue that at minimum, the outputs of LLMs like ChatGPT are soft bullshit: bullshit–that is, speech or text produced without concern for its truth–that is produced without any intent to mislead the audience about the utterer’s attitude towards truth. We also suggest, more controversially, that ChatGPT may indeed produce hard bullshit: if we view it as having intentions (for example, in virtue of how it is designed), then the fact that it is designed to give the impression of concern for truth qualifies it as attempting to mislead the audience about its aims, goals, or agenda. So, with the caveat that the particular kind of bullshit ChatGPT outputs is dependent on particular views of mind or meaning, we conclude that it is appropriate to talk about ChatGPT-generated text as bullshit, and flag up why it matters that – rather than thinking of its untrue claims as lies or hallucinations – we call bullshit on ChatGPT.</p></div></div><div id=\"Sec2-section\" data-title=\"What is ChatGPT?\"><h2 id=\"Sec2\">What is ChatGPT?</h2><div id=\"Sec2-content\"><p>Large language models are becoming increasingly good at carrying on convincing conversations. The most prominent large language model is OpenAI’s ChatGPT, so it’s the one we will focus on; however, what we say carries over to other neural network-based AI chatbots, including Google’s Bard chatbot, AnthropicAI’s Claude (claude.ai), and Meta’s LLaMa. Despite being merely complicated bits of software, these models are surprisingly human-like when discussing a wide variety of topics. Test it yourself: anyone can go to the OpenAI web interface and ask for a ream of text; typically, it produces text which is indistinguishable from that of your average English speaker or writer. The variety, length, and similarity to human-generated text that GPT-4 is capable of has convinced many commentators to think that this chatbot has finally cracked it: that this is real (as opposed to merely nominal) artificial intelligence, one step closer to a human-like mind housed in a silicon brain.</p><p>However, large language models, and other AI models like ChatGPT, are doing considerably less than what human brains do, and it is not clear whether they do what they do in the same way we do. The most obvious difference between an LLM and a human mind involves the <i>goals</i> of the system. Humans have a variety of goals and behaviours, most of which are extra-linguistic: we have basic physical desires, for things like food and sustenance; we have social goals and relationships; we have projects; and we create physical objects. Large language models simply aim to replicate human speech or writing. This means that their primary goal, insofar as they have one, is to produce human-like text. They do so by estimating the likelihood that a particular word will appear next, given the text that has come before.</p><p>The machine does this by constructing a massive statistical model, one which is based on large amounts of text, mostly taken from the internet. This is done with relatively little input from human researchers or the designers of the system; rather, the model is designed by constructing a large number of nodes, which act as probability functions for a word to appear in a text given its context and the text that has come before it. Rather than putting in these probability functions by hand, researchers feed the system large amounts of text and train it by having it make next-word predictions about this training data. They then give it positive or negative feedback depending on whether it predicts correctly. Given enough text, the machine can construct a statistical model giving the likelihood of the next word in a block of text all by itself.</p><p>This model associates with each word a vector which locates it in a high-dimensional abstract space, near other words that occur in similar contexts and far from those which don’t. When producing text, it looks at the previous string of words and constructs a different vector, locating the word’s surroundings – its context – near those that occur in the context of similar words. We can think of these heuristically as representing the meaning of the word and the content of its context. But because these spaces are constructed using machine learning by repeated statistical analysis of large amounts of text, we can’t know what sorts of similarity are represented by the dimensions of this high-dimensional vector space. Hence we do not know how similar they are to what we think of as meaning or context. The model then takes these two vectors and produces a set of likelihoods for the next word; it selects and places one of the more likely ones—though not always the most likely. Allowing the model to choose randomly amongst the more likely words produces more creative and human-like text; the parameter which controls this is called the ‘temperature’ of the model and increasing the model’s temperature makes it both seem more creative and more likely to produce falsehoods. The system then repeats the process until it has a recognizable, complete-looking response to whatever prompt it has been given.</p><p>Given this process, it’s not surprising that LLMs have a problem with the truth. Their goal is to provide a normal-seeming response to a prompt, not to convey information that is helpful to their interlocutor. Examples of this are already numerous, for instance, a lawyer recently prepared his brief using ChatGPT and discovered to his chagrin that most of the cited cases were not real (Weiser, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Weiser, B. (2023). Here’s what happens when your lawyer uses ChatGPT. New York Times, May 23, 2023. Accessed via \n                  https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html\n                  \n                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR27\" id=\"ref-link-section-d173038264e407\">2023</a>); as Judge P. Kevin Castel put it, ChatGPT produced a text filled with “bogus judicial decisions, with bogus quotes and bogus internal citations”. Similarly, when computer science researchers tested ChatGPT’s ability to assist in academic writing, they found that it was able to produce surprisingly comprehensive and sometimes even accurate text on biological subjects given the right prompts. But when asked to produce evidence for its claims, “it provided five references dating to the early 2000s. None of the provided paper titles existed, and all provided PubMed IDs (PMIDs) were of different unrelated papers” (Alkaissi and McFarland, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Alkaissi, H., &amp; McFarlane, S. I., (2023, February 19). Artificial hallucinations in ChatGPT: Implications in scientific writing. Cureus, 15(2), e35179. \n                  https://doi.org/10.7759/cureus.35179\n                  \n                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR1\" id=\"ref-link-section-d173038264e410\">2023</a>). These errors can “snowball”: when the language model is asked to provide evidence for or a deeper explanation of a false claim, it rarely checks itself; instead it confidently producesmore false but normal-sounding claims (Zhang et al. <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Zhang (2023). How language model hallucinations can snowball. ArXiv preprint: arXiv:, 2305, 13534v1.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR30\" id=\"ref-link-section-d173038264e413\">2023</a>). The accuracy problem for LLMs and other generative Ais is often referred to as the problem of “AI hallucination”: the chatbot seems to be hallucinating sources and facts that don’t exist. These inaccuracies are referred to as “hallucinations” in both technical (OpenAI, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"OpenAI (2023). GPT-4 technical report. ArXiv Preprint: arXiv, 2303, 08774v3.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR23\" id=\"ref-link-section-d173038264e416\">2023</a>) and popular contexts (Weise &amp; Metz, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Weise, K., &amp; Metz, C. (2023). When AI chatbots hallucinate. New York Times, May 9, 2023. Accessed via \n                  https://www.nytimes.com/2023/05/01/business/ai-chatbots-hallucination.html\n                  \n                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR28\" id=\"ref-link-section-d173038264e419\">2023</a>).</p><p>These errors are pretty minor if the only point of a chatbot is to mimic human speech or communication. But the companies designing and using these bots have grander plans: chatbots could replace Google or Bing searches with a more user-friendly conversational interface (Shah &amp; Bender, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2022\" title=\"Shah, C., &amp; Bender, E. M. (2022). Situating search. CHIIR ‘22: Proceedings of the 2022 Conference on Human Information Interaction and Retrieval March 2022 Pages 221–232 \n                  https://doi.org/10.1145/3498366.3505816\n                  \n                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR26\" id=\"ref-link-section-d173038264e425\">2022</a>; Zhu et al., <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Zhu, T., et al. (2023). Large language models for information retrieval: A survey. Arxiv Preprint: arXiv, 2308, 17107v2.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR31\" id=\"ref-link-section-d173038264e428\">2023</a>), or assist doctors or therapists in medical contexts (Lysandrou, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Lysandrou (2023). Comparative analysis of drug-GPT and ChatGPT LLMs for healthcare insights: Evaluating accuracy and relevance in patient and HCP contexts. ArXiv Preprint: arXiv, 2307, 16850v1.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR17\" id=\"ref-link-section-d173038264e431\">2023</a>). In these cases, accuracy is important and the errors represent a serious problem.</p><p>One attempted solution is to hook the chatbot up to some sort of database, search engine, or computational program that can answer the questions that the LLM gets wrong (Zhu et al., <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Zhu, T., et al. (2023). Large language models for information retrieval: A survey. Arxiv Preprint: arXiv, 2308, 17107v2.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR31\" id=\"ref-link-section-d173038264e437\">2023</a>). Unfortunately, this doesn’t work very well either. For example, when ChatGPT is connected to Wolfram Alpha, a powerful piece of mathematical software, it improves moderately in answering simple mathematical questions. But it still regularly gets things wrong, especially for questions which require multi-stage thinking (Davis &amp; Aaronson, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Davis, E., &amp; Aaronson, S. (2023). Testing GPT-4 with Wolfram alpha and code interpreter plub-ins on math and science problems. Arxiv Preprint: arXiv, 2308, 05713v2.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR6\" id=\"ref-link-section-d173038264e440\">2023</a>). And when connected to search engines or other databases, the models are still fairly likely to provide fake information unless they are given very specific instructions–and even then things aren’t perfect (Lysandrou, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Lysandrou (2023). Comparative analysis of drug-GPT and ChatGPT LLMs for healthcare insights: Evaluating accuracy and relevance in patient and HCP contexts. ArXiv Preprint: arXiv, 2307, 16850v1.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR17\" id=\"ref-link-section-d173038264e443\">2023</a>). OpenAI has plans to rectify this by training the model to do step by step reasoning (Lightman et al., <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Lightman, H., et al. (2023). Let’s verify step by step. Arxiv Preprint: arXiv, 2305, 20050.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR16\" id=\"ref-link-section-d173038264e446\">2023</a>) but this is quite resource-intensive, and there is reason to be doubtful that it will completely solve the problem—nor is it clear that the result will be a large language model, rather than some broader form of AI.</p><p>Solutions such as connecting the LLM to a database don’t work is because, if the models are <i>trained</i> on the database, then the words in the database affect the probability that the chatbot will add one or another word to the line of text it is generating. But this will only make it produce text similar to the text in the database; doing so will make it more likely that it reproduces the information in the database but by no means ensures that it will.</p><p>On the other hand, the LLM can also be connected to the database by allowing it to consult the database, in a way similar to the way it consults or talks to its human interlocutors. In this way, it can use the outputs of the database as text which it responds to and builds on. Here’s one way this can work: when a human interlocutor asks the language model a question, it can then translate the question into a query for the database. Then, it takes the response of the database as an input and builds a text from it to provide back to the human questioner. But this can misfire too, as the chatbots might ask the database the wrong question, or misinterpret its answer (Davis &amp; Aaronson, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Davis, E., &amp; Aaronson, S. (2023). Testing GPT-4 with Wolfram alpha and code interpreter plub-ins on math and science problems. Arxiv Preprint: arXiv, 2308, 05713v2.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR6\" id=\"ref-link-section-d173038264e458\">2023</a>). “GPT-4 often struggles to formulate a problem in a way that Wolfram Alpha can accept or that produces useful output.” This is not unrelated to the fact that when the language model generates a query for the database or computational module, it does so in the same way it generates text for humans: by estimating the likelihood that some output “looks like’’ the kind of thing the database will correspond with.</p><p>One might worry that these failed methods for improving the accuracy of chatbots are connected to the inapt metaphor of AI hallucinations. If the AI is <i>misperceiving</i> or <i>hallucinating</i> sources, one way to rectify this would be to put it in touch with real rather than hallucinated sources. But attempts to do so have failed.</p><p>The problem here isn’t that large language models hallucinate, lie, or misrepresent the world in some way. It’s that they are not designed to represent the world at all; instead, they are designed to convey convincing lines of text. So when they are provided with a database of some sort, they use this, in one way or another, to make their responses more convincing. But they are not in any real way attempting to convey or transmit the information in the database. As Chirag Shah and Emily Bender put it: “Nothing in the design of language models (whose training task is to predict words given context) is actually designed to handle arithmetic, temporal reasoning, etc. To the extent that they sometimes get the right answer to such questions is only because they happened to synthesize relevant strings out of what was in their training data. No reasoning is involved […] Similarly, language models are prone to making stuff up […] because they are not designed to express some underlying set of information in natural language; they are only manipulating the form of language” (Shah &amp; Bender, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2022\" title=\"Shah, C., &amp; Bender, E. M. (2022). Situating search. CHIIR ‘22: Proceedings of the 2022 Conference on Human Information Interaction and Retrieval March 2022 Pages 221–232 \n                  https://doi.org/10.1145/3498366.3505816\n                  \n                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR26\" id=\"ref-link-section-d173038264e474\">2022</a>). These models aren’t designed to transmit information, so we shouldn’t be too surprised when their assertions turn out to be false.</p></div></div><div id=\"Sec3-section\" data-title=\"Lies, ‘hallucinations’ and bullshit\"><h2 id=\"Sec3\">Lies, ‘hallucinations’ and bullshit</h2><div id=\"Sec3-content\"><h3 id=\"Sec4\">Frankfurtian bullshit and lying</h3><p>Many popular discussions of ChatGPT call its false statements ‘hallucinations’. One also might think of these untruths as lies. However, we argue that this isn’t the right way to think about it. We will argue that these falsehoods aren’t hallucinations later – in Sect.&nbsp;3.2.3. For now, we’ll discuss why these untruths aren’t lies but instead are bullshit.</p><p>The topic of lying has a rich philosophical literature. In ‘Lying’, Saint Augustine distinguished seven types of lies, and his view altered throughout his life. At one point, he defended the position that any instance of knowingly uttering a false utterance counts as a lie, so that even jokes containing false propositions, like –</p><blockquote><p>I entered a pun competition and because I really wanted to win, I submitted ten entries. I was sure one of them would win, but no pun in ten did.</p></blockquote><p>– would be regarded as a lie, as I have never entered such a competition (Proops &amp; Sorensen, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Proops, I., &amp; Sorensen, R. (2023). Destigmatizing the exegetical attribution of lies: the case of kant. Pacific Philosophical Quarterly. \n                  https://doi.org/10.1111/papq.12442\n                  \n                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR24\" id=\"ref-link-section-d173038264e500\">2023</a>: 3). Later, this view is refined such that the speaker only lies if they intend the hearer to believe the utterance. The suggestion that the speaker must intend to deceive is a common stipulation in literature on lies. According to the “traditional account” of lying:</p><blockquote><p>To lie = <sub>df</sub>. to make a believed-false statement to another person with the intention that the other person believe that statement to be true (Mahon, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2015\" title=\"Mahon, J. E. (2015). The definition of lying and deception. The Stanford Encyclopedia of Philosophy (Winter 2016 Edition), Edward N. Zalta (Ed.), \n                  https://plato.stanford.edu/archives/win2016/entries/lying-definition/\n                  \n                .\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR18\" id=\"ref-link-section-d173038264e509\">2015</a>).</p></blockquote><p>For our purposes this definition will suffice. Lies are generally frowned upon. But there are acts of misleading testimony which are criticisable, which do not fall under the umbrella of lying.<sup><a href=\"#Fn1\"><span>Footnote </span>1</a></sup> These include spreading untrue gossip, which one mistakenly, but culpably, believes to be true. Another class of misleading testimony that has received particular attention from philosophers is that of bullshit. This everyday notion was analysed and introduced into the philosophical lexicon by Harry Frankfurt.<sup><a href=\"#Fn2\"><span>Footnote </span>2</a></sup></p><p>Frankfurt understands bullshit to be characterized not by an intent to deceive but instead by a reckless disregard for the truth. A student trying to sound knowledgeable without having done the reading, a political candidate saying things because they sound good to potential voters, and a dilettante trying to spin an interesting story: none of these people are trying to deceive, but they are also not trying to convey facts. To Frankfurt, they are bullshitting.</p><p>Like “lie”, “bullshit” is both a noun and a verb: an utterance produced can be a lie or an instance of bullshit, as can the act of producing these utterances. For an utterance to be classed as bullshit, it must not be accompanied by the explicit intentions that one has when lying, i.e., to cause a false belief in the hearer. Of course, it must also not be accompanied by the intentions characterised by an honest utterance. So far this story is entirely negative. Must any positive intentions be manifested in the utterer?</p><p>Throughout most of Frankfurt’s discussion, his characterisation of bullshit is negative. He notes that bullshit requires “no conviction” from the speaker about what the truth is (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2005\" title=\"Frankfurt, H. (2005). On Bullshit, Princeton.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR12\" id=\"ref-link-section-d173038264e546\">2005</a>: 55), that the bullshitter “pays no attention” to the truth (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2005\" title=\"Frankfurt, H. (2005). On Bullshit, Princeton.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR12\" id=\"ref-link-section-d173038264e549\">2005</a>: 61) and that they “may not deceive us, or even intend to do so, either about the facts or what he takes the facts to be” (2005: 54). Later, he describes the “defining feature” of bullshit as “<i>a lack of concern</i> with truth, or an <i>indifference to how things really are</i> [our emphasis]” (2002: 340). These suggest a negative picture; that for an output to be classed as bullshit, it only needs to lack a certain relationship to the truth.</p><p>However, in places, a positive intention is presented. Frankfurt says what a bullshitter ….</p><p>“…does necessarily attempt to deceive us about is his enterprise. His only indispensably distinctive characteristic is that in a certain way he misrepresents what he is up to” (2005: 54).</p><p>This is somewhat surprising. It restricts what counts as bullshit to utterances accompanied by a higher-order deception. However, some of Frankfurt’s examples seem to lack this feature. When Fania Pascal describes her unwell state as “feeling like a dog that has just been run over” to her friend Wittgenstein, it stretches credulity to suggest that she was intending to deceive him about how much she knew about how run-over dogs felt. And given how the conditions for bullshit are typically described as negative, we might wonder whether the positive condition is really necessary.</p><h3 id=\"Sec5\">Bullshit distinctions</h3><p>Should utterances without an intention to deceive count as bullshit? One reason in favour of expanding the definition, or embracing a plurality of bullshit, is indicated by Frankfurt’s comments on the dangers of bullshit.</p><p>“In contrast [to merely unintelligible discourse], indifference to the truth is extremely dangerous. The conduct of civilized life, and the vitality of the institutions that are indispensable to it, depend very fundamentally on respect for the distinction between the true and the false. Insofar as the authority of this distinction is undermined by the prevalence of bullshit and by the mindlessly frivolous attitude that accepts the proliferation of bullshit as innocuous, an indispensable human treasure is squandered” (2002: 343).</p><p>These dangers seem to manifest regardless of whether there is an intention to deceive about the enterprise a speaker is engaged in. Compare the deceptive bullshitter, who does aim to mislead us about being in the truth-business, with someone who harbours no such aim, but just talks for the sake of talking (without care, or indeed any thought, about the truth-values of their utterances).</p><p>One of Frankfurt’s examples of bullshit seems better captured by the wider definition. He considers the advertising industry, which is “replete with instances of bullshit so unmitigated that they serve among the most indisputable and classic paradigms of the concept” (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2005\" title=\"Frankfurt, H. (2005). On Bullshit, Princeton.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR12\" id=\"ref-link-section-d173038264e584\">2005</a>:22). However, it seems to misconstrue many advertisers to portray their aims as to mislead about their agendas. They are <i>expected</i> to say misleading things. Frankfurt discusses Marlboro adverts with the message that smokers are as brave as cowboys (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2002\" title=\"Frankfurt, H. (2002). Reply to cohen. In S. Buss, &amp; L. Overton (Eds.), The contours of agency: Essays on themes from Harry Frankfurt. MIT Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR11\" id=\"ref-link-section-d173038264e590\">2002</a>: 341). Is it reasonable to suggest that the advertisers pretended to believe this?</p><p>Frankfurt does allow for multiple species of bullshit (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2002\" title=\"Frankfurt, H. (2002). Reply to cohen. In S. Buss, &amp; L. Overton (Eds.), The contours of agency: Essays on themes from Harry Frankfurt. MIT Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR11\" id=\"ref-link-section-d173038264e597\">2002</a>: 340).<sup><a href=\"#Fn3\"><span>Footnote </span>3</a></sup> Following this suggestion, we propose to envisage bullshit as a genus, and Frankfurt’s intentional bullshit as one species within this genus. Other species may include that produced by the advertiser, who anticipates that no one will believe their utterances<sup><a href=\"#Fn4\"><span>Footnote </span>4</a></sup> or someone who has no intention one way or another about whether they mislead their audience. To that end, consider the following distinction:</p>\n                  <h3 id=\"FPar1\">Bullshit (general)</h3>\n                  <p>Any utterance produced where a speaker has indifference towards the truth of the utterance.</p>\n                \n                  <h3 id=\"FPar2\">Hard bullshit</h3>\n                  <p>Bullshit produced with the intention to mislead the audience about the utterer’s agenda.</p>\n                \n                  <h3 id=\"FPar3\">Soft bullshit</h3>\n                  <p>Bullshit produced without the intention to mislead the hearer regarding the utterer’s agenda.</p>\n                <p>The general notion of bullshit is useful: on some occasions, we might be confident that an utterance was either soft bullshit or hard bullshit, but be unclear which, given our ignorance of the speaker’s higher-order desires.<sup><a href=\"#Fn5\"><span>Footnote </span>5</a></sup> In such a case, we can still call bullshit.</p><p>Frankfurt’s own explicit account, with the positive requirements about producer’s intentions, is hard bullshit, whereas soft bullshit seems to describe some of Frankfurt’s examples, such as that of Pascal’s conversation with Wittgenstein, or the work of advertising agencies. It might be helpful to situate these distinctions in the existing literature. On our view, hard bullshit is most closely aligned with Cassam (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2019\" title=\"Cassam, Q. (2019). Vices of the mind. Oxford University Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR3\" id=\"ref-link-section-d173038264e672\">2019</a>), and Frankfurt’s positive account, for the reason that all of these views hold that some intention must be present, rather than merely absent, for the utterance to be bullshit: a kind of “epistemic insouciance” or vicious attitude towards truth on Cassam’s view, and (as we have seen) an intent to mislead the hearer about the utterer’s agenda on Frankfurt’s view. In Sect.&nbsp;3.2 we consider whether ChatGPT may be a hard bullshitter, but it is important to note that it seems to us that hard bullshit, like the two accounts cited here, requires one to take a stance on whether or not LLMs can be agents, and so comes with additional argumentative burdens.</p><p>Soft bullshit, by contrast, captures only Frankfurt’s negative requirement – that is, the indifference towards truth that we have classed as definitional of bullshit (general) – for the reasons given above. As we argue, ChatGPT is at minimum a soft bullshitter or a bullshit machine, because if it is not an agent then it can neither hold any attitudes towards truth nor towards deceiving hearers about its (or, perhaps more properly, its users’) agenda.</p><p>It’s important to note that even this more modest kind of bullshitting will have the deleterious effects that concern Frankfurt: as he says, “indifference to the truth is extremely dangerous…by the mindlessly frivolous attitude that accepts the proliferation of bullshit as innocuous, an indispensable human treasure is squandered” (2002, p343). By treating ChatGPT and similar LLMs as being in any way concerned with truth, or by speaking metaphorically as if they make mistakes or suffer “hallucinations” in pursuit of true claims, we risk exactly this acceptance of bullshit, and this squandering of meaning – so, irrespective of whether or not ChatGPT is a hard or a soft bullshitter, it does produce bullshit, and it does matter.</p></div></div><div id=\"Sec6-section\" data-title=\"ChatGPT is bullshit\"><h2 id=\"Sec6\">ChatGPT is bullshit</h2><div id=\"Sec6-content\"><p>With this distinction in hand, we’re now in a position to consider a worry of the following sort: Is ChatGPT hard bullshitting, soft bullshitting, or neither? We will argue, first, that ChatGPT, and other LLMs, are clearly soft bullshitting. However, the question of whether these chatbots are hard bullshitting is a trickier one, and depends on a number of complex questions concerning whether ChatGPT can be ascribed intentions. We canvas a few ways in which ChatGPT can be understood to have the requisite intentions in Sect.&nbsp;3.2.</p><h3 id=\"Sec7\">ChatGPT is a soft bullshitter</h3><p>We are not confident that chatbots can be correctly described as having any intentions at all, and we’ll go into this in more depth in the next Sect.&nbsp;(3.2). But we are quite certain that ChatGPT does not intend to convey truths, and so is a soft bullshitter. We can produce an easy argument by cases for this. Either ChatGPT has intentions or it doesn’t. If ChatGPT has no intentions at all, it trivially doesn’t intend to convey truths. So, it is indifferent to the truth value of its utterances and so is a soft bullshitter.</p><p>What if ChatGPT does have intentions? In Sect.&nbsp;1, we argued that ChatGPT is not designed to produce true utterances; rather, it is designed to produce text which is indistinguishable from the text produced by humans. It is aimed at being convincing rather than accurate. The basic architecture of these models reveals this: they are designed to come up with a <i>likely continuation of a string of text.</i> It’s reasonable to assume that one way of being a likely continuation of a text is by being true; if humans are roughly more accurate than chance, true sentences will be more likely than false ones. This might make the chatbot more accurate than chance, but it does not give the chatbot any intention to convey truths. This is similar to standard cases of human bullshitters, who don’t care whether their utterances are true; good bullshit often contains some degree of truth, that’s part of what makes it convincing. A bullshitter can be more accurate than chance while still being indifferent to the truth of their utterances. We conclude that, even if the chatbot can be described as having intentions, it is indifferent to whether its utterances are true. It does not and cannot care about the truth of its output.</p><p>Presumably ChatGPT can’t care about conveying or hiding the truth, since it can’t care about anything. So, just as a matter of conceptual necessity, it meets one of Frankfurt’s criteria for bullshit. However, this only gets us so far – a rock can’t care about anything either, and it would be patently absurd to suggest that this means rocks are bullshitters<sup><a href=\"#Fn6\"><span>Footnote </span>6</a></sup>. Similarly books can contain bullshit, but they are not themselves bullshitters. Unlike rocks – or even books – ChatGPT itself produces text, and looks like it performs speech acts independently of its users and designers. And while there is considerable disagreement concerning whether ChatGPT has intentions, it’s widely agreed that the sentences it produces are (typically) meaningful (see e.g. Mandelkern and Linzen <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Mandelkern, M., &amp; Linzen, T. (2023). Do language models’ Words Refer?. ArXiv Preprint: arXiv, 2308, 05576.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR20\" id=\"ref-link-section-d173038264e719\">2023</a>).</p><p>ChatGPT functions not to convey truth or falsehood but rather to convince the reader of – to use Colbert’s apt coinage – the <i>truthiness</i> of its statement, and ChatGPT is designed in such a way as to make attempts at bullshit efficacious (in a way that pens, dictionaries, etc., are not). So, it seems that at minimum, ChatGPT is a soft bullshitter: if we take it not to have intentions, there isn’t any attempt to mislead about the attitude towards truth, but it <i>is</i> nonetheless engaged in the business of outputting utterances that look as if they’re truth-apt. We conclude that ChatGPT is a <i>soft bullshitter.</i></p><h3 id=\"Sec8\">ChatGPT as hard bullshit</h3><p>But is ChatGPT a <i>hard bullshitter</i>? A critic might object, it is simply inappropriate to think of programs like ChatGPT as hard bullshitters, because (i) they are not agents, or relatedly, (ii) they do not and cannot intend anything whatsoever.</p><p>We think this is too fast. First, whether or not ChatGPT has agency, its creators and users do. And what they produce with it, we will argue, is bullshit. Second, we will argue that, regardless of whether it has agency, it does have a function; this function gives it characteristic goals, and possibly even intentions, which align with our definition of hard bullshit.</p><p>Before moving on, we should say what we mean when we ask whether ChatGPT is an agent. For the purposes of this paper, the central question is whether ChatGPT has intentions and or beliefs. Does it intend to deceive? Can it, in any literal sense, be said to have goals or aims? If so, does it intend to deceive us about the content of its utterances, or merely have the goal to appear to be a competent speaker? Does it have beliefs—internal representational states which aim to track the truth? If so, do its utterances match those beliefs (in which case its false statements might be something like hallucinations) or are its utterances not matched to the beliefs—in which case they are likely to be either lies or bullshit? We will consider these questions in more depth in Sect.&nbsp;3.2.2.</p><p>There are other philosophically important aspects of agenthood that we will not be considering. We won’t be considering whether ChatGPT makes decisions, has or lacks autonomy, or is conscious; we also won’t worry whether ChatGPT is morally responsible for its statements or its actions (if it has any of those).</p><h4 id=\"Sec9\">ChatGPT is a bullshit machine</h4><p>We will argue that even if ChatGPT is not, itself, a hard bullshitter, it is nonetheless a bullshit machine. The bullshitter is the person using it, since they (i) don’t care about the truth of what it says, (ii) want the reader to believe what the application outputs. On Frankfurt’s view, bullshit is bullshit even if uttered with no intent to bullshit: if something is bullshit to start with, then its repetition “is bullshit as he [or it] repeats it, insofar as it was originated by someone who was unconcerned with whether what he was saying is true or false” (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2022\" title=\"Frankfurt, H. (2002). Reply to cohen. In S. Buss, &amp; L. Overton (Eds.), The contours of agency: Essays on themes from Harry Frankfurt. MIT Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR11\" id=\"ref-link-section-d173038264e761\">2022</a>, p340).</p><p>This just pushes the question back to who the originator is, though: take the (increasingly frequent) example of the student essay created by ChatGPT. If the student cared about accuracy and truth, they would not use a program that infamously makes up sources whole-cloth. Equally, though, if they give it a prompt to produce an essay on philosophy of science and it produces a recipe for Bakewell tarts, then it won’t have the desired effect. So the idea of ChatGPT as a bullshit machine seems right, but also as if it’s missing something: someone can produce bullshit using their voice, a pen or a word processor, after all, but we don’t standardly think of these things as being bullshit machines, or of outputting bullshit in any particularly interesting way – conversely, there <i>does</i> seem to be something particular to ChatGPT, to do with the way that it operates, which makes it more than a <i>mere</i> tool, and which suggests that it might appropriately be thought of as an originator of bullshit. In short, it doesn’t seem quite right either to think of ChatGPT as analogous to a pen (can be used for bullshit, but can create nothing without deliberate and wholly agent-directed action) nor as to a bullshitting human (who can intend and produce bullshit on their own initiative).</p><p>The idea of ChatGPT as a bullshit machine is a helpful one when combined with the distinction between hard and soft bullshit. Reaching again for the example of the dodgy student paper: we’ve all, I take it, marked papers where it was obvious that a dictionary or thesaurus had been deployed with a crushing lack of subtlety; where fifty-dollar words are used not because they’re the best choice, nor even because they serve to obfuscate the truth, but simply because the author wants to convey an <i>impression</i> of understanding and sophistication. It would be inappropriate to call the dictionary a bullshit artist in this case; but it would <i>not</i> be inappropriate to call the result bullshit. So perhaps we should, strictly, say not that ChatGPT <i>is</i> bullshit but that it <i>outputs</i> bullshit in a way that goes beyond being simply a vector of bullshit: it does not and cannot care about the truth of its output, <i>and</i> the person using it does so not to convey truth or falsehood but rather to convince the hearer that the text was written by a interested and attentive agent.</p><h4 id=\"Sec10\">ChatGPT may be a hard bullshitter</h4><p>Is ChatGPT itself a hard bullshitter? If so, it must have intentions or goals: it must intend to deceive its listener, not about the content of its statements, but instead about its agenda. Recall that hard bullshitters, like the unprepared student or the incompetent politician, don’t care whether their statements are true or false, but do intend to deceive their audience about what they are doing. If so, it must have intentions or goals: it must intend to deceive its listener, not about the content of its statements, but instead about its agenda. We don’t think that ChatGPT is an agent or has intentions in precisely the same way that humans do&nbsp;(see Levenstein and Herrmann (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference forthcoming\" title=\"Levenstein, B. A., &amp; Herrmann, D. A. (forthcoming). Still no lie detector for language models: Probing empirical and conceptual roadblocks. Philosophical Studies, 1–27.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR14\" id=\"ref-link-section-d173038264e799\">forthcoming</a>) for a discussion of the issues here). But when speaking loosely it is remarkably easy to use intentional language to describe it: what is ChatGPT <i>trying</i> to do? Does it <i>care</i> whether the text it produces is accurate? We will argue that there is a robust, although perhaps not literal, sense in which ChatGPT does intend to deceive us about its agenda: its goal is not to convince us of the content of its utterances, but instead to portray itself as a ‘normal’ interlocutor like ourselves. By contrast, there is no similarly strong sense in which ChatGPT confabulates, lies, or hallucinates.</p><p>Our case will be simple: ChatGPT’s primary function is to imitate human speech. If this function is intentional, it is precisely the sort of intention that is required for an agent to be a hard bullshitter: in performing the function, ChatGPT is attempting to deceive the audience about its agenda. Specifically, it’s trying to seem like something that has an agenda, when in many cases it does not. We’ll discuss here whether this function gives rise to, or is best thought of, as an intention. In the next Sect.&nbsp;(3.2.3), we will argue that ChatGPT has no similar function or intention which would justify calling it a confabulator, liar, or hallucinator.</p><p>How do we know that ChatGPT functions as a hard bullshitter? Programs like ChatGPT are designed to do a task, and this task is remarkably like what Frankfurt thinks the bullshitter intends, namely to deceive the reader about the nature of the enterprise – in this case, to deceive the reader into thinking that they’re reading something produced by a being with intentions and beliefs.</p><p>ChatGPT’s text production algorithm was developed and honed in a process quite similar to artificial selection. Functions and selection processes have the same sort of directedness that human intentions do; naturalistic philosophers of mind have long connected them to the intentionality of human and animal mental states. If ChatGPT is understood as having intentions or intention-like states in this way, its intention is to present itself in a certain way (as a conversational agent or interlocutor) rather than to represent and convey facts. In other words, it has the intentions we associate with hard bullshitting.</p><p>One way we can think of ChatGPT as having intentions is by adopting Dennett’s <i>intentional stance</i> towards it. Dennett (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 1987\" title=\"Dennett, D. C. (1987). The intentional stance. The MIT.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR7\" id=\"ref-link-section-d173038264e824\">1987</a>: 17) describes the intentional stance as a way of predicting the behaviour of systems whose purpose we don’t already know.</p><p>“To adopt the intentional stance […] is to decide – tentatively, of course – to attempt to characterize, predict, and explain […] behavior by using intentional idioms, such as ‘believes’ and ‘wants,’ a practice that assumes or presupposes the rationality” of the target system (Dennett, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 1983\" title=\"Dennett, D. C. (1983). Intentional systems in cognitive ethology: The panglossian paradigm defended. Behavioral and Brain Sciences, 6, 343–390.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR8\" id=\"ref-link-section-d173038264e830\">1983</a>: 345).</p><p>Dennett suggests that if we know why a system was designed, we can make predictions on the basis of its design (1987). While we do know that ChatGPT was designed to chat, its exact algorithm and the way it produces its responses has been developed by machine learning, so we do not know its precise details of how it works and what it does. Under this ignorance it is tempting to bring in intentional descriptions to help us understand and predict what ChatGPT is doing.</p><p>When we adopt the intentional stance, we will be making bad predictions if we attribute any desire to convey truth to ChatGPT. Similarly, attributing “hallucinations” to ChatGPT will lead us to predict as if it has perceived things that aren’t there, when what it is doing is much more akin to making something up because it sounds about right. The former intentional attribution will lead us to try to correct its beliefs, and fix its inputs --- a strategy which has had limited if any success. On the other hand, if we attribute to ChatGPT the intentions of a hard bullshitter, we will be better able to diagnose the situations in which it will make mistakes and convey falsehoods. If ChatGPT is trying to do anything, it is trying to portray itself as a person.</p><p>Since this reason for thinking ChatGPT is a hard bullshitter involves committing to one or more controversial views on mind and meaning, it is more tendentious than simply thinking of it as a bullshit machine; but regardless of whether or not the program has intentions, there clearly <i>is</i> an attempt to deceive the hearer or reader about the nature of the enterprise somewhere along the line, and in our view that justifies calling the output hard bullshit.</p><p>So, though it’s worth making the caveat, it doesn’t seem to us that it significantly affects how we should think of and talk about ChatGPT and bullshit: the person using it to turn out some paper or talk isn’t concerned either with conveying or covering up the truth (since both of those require attention to what the truth actually <i>is</i>), and neither is the system itself. Minimally, it churns out soft bullshit, and, given certain controversial assumptions about the nature of intentional ascription, it produces hard bullshit; the specific texture of the bullshit is not, for our purposes, important: either way, ChatGPT is a bullshitter.</p><h4 id=\"Sec11\">Bullshit? hallucinations? confabulations? The need for new terminology</h4><p>We have argued that we should use the terminology of bullshit, rather than “hallucinations” to describe the utterances produced by ChatGPT. The suggestion that “hallucination” terminology is inappropriate has also been noted by Edwards (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Edwards, B. (2023). Why ChatGPT and bing chat are so good at making things up. Ars Tecnica. \n                  https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/\n                  \n                , accesssed 19th April, 2024.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR10\" id=\"ref-link-section-d173038264e859\">2023</a>), who favours the term “confabulation” instead. Why is our proposal better than this or other alternatives?</p><p>We object to the term hallucination because it carries certain misleading implications. When someone hallucinates they have a non-standard perceptual experience, but do not actually perceive some feature of the world (Macpherson, <a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2013\" title=\"Macpherson, F. (2013). The philosophy and psychology of hallucination: an introduction, in Hallucination, Macpherson and Platchias (Eds.), London: MIT Press.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR19\" id=\"ref-link-section-d173038264e865\">2013</a>), where “perceive” is understood as a success term, such that they do not actually perceive the object or property. This term is inappropriate for LLMs for a variety of reasons. First, as Edwards (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Edwards, B. (2023). Why ChatGPT and bing chat are so good at making things up. Ars Tecnica. \n                  https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/\n                  \n                , accesssed 19th April, 2024.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR10\" id=\"ref-link-section-d173038264e868\">2023</a>) points out, the term hallucination anthropomorphises the LLMs. Edwards also notes that attributing resulting problems to “hallucinations” of the models may allow creators to “blame the AI model for faulty outputs instead of taking responsibility for the outputs themselves”, and we may be wary of such abdications of responsibility. LLMs do not perceive, so they surely do not “mis-perceive”. Second, what occurs in the case of an LLM delivering false utterances is not an unusual or deviant form of the process it usually goes through (as some claim is the case in hallucinations, e.g., disjunctivists about perception). The very same process occurs when its outputs happen to be true.</p><p>So much for “hallucinations”. What about Edwards’ preferred term, “confabulation”? Edwards&nbsp;(<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference 2023\" title=\"Edwards, B. (2023). Why ChatGPT and bing chat are so good at making things up. Ars Tecnica. \n                  https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/\n                  \n                , accesssed 19th April, 2024.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR10\" id=\"ref-link-section-d173038264e874\">2023</a>) says:</p><blockquote><p>In human psychology, a “confabulation” occurs when someone’s memory has a gap and the brain convincingly fills in the rest without intending to deceive others. ChatGPT does not work like the human brain, but the term “confabulation” arguably serves as a better metaphor because there’s a creative gap-filling principle at work […].</p></blockquote><p>As Edwards notes, this is imperfect. Once again, the use of a human psychological term risks anthropomorphising the LLMs.</p><p>This term also suggests that there is something exceptional occurring when the LLM makes a false utterance, i.e., that in these occasions - and only these occasions - it “fills in” a gap in memory with something false. This too is misleading. Even when the ChatGPT does give us correct answers, its process is one of predicting the next token. In our view, it falsely indicates that ChatGPT is, in general, attempting to convey accurate information in its utterances. But there are strong reasons to think that it does not have beliefs that it is intending to share in general–see, for example, Levenstein and Herrmann (<a data-track=\"click\" data-track-action=\"reference anchor\" data-track-label=\"link\" data-test=\"citation-ref\" aria-label=\"Reference forthcoming\" title=\"Levenstein, B. A., &amp; Herrmann, D. A. (forthcoming). Still no lie detector for language models: Probing empirical and conceptual roadblocks. Philosophical Studies, 1–27.\" href=\"/article/10.1007/s10676-024-09775-5#ref-CR14\" id=\"ref-link-section-d173038264e889\">forthcoming</a>). In our view, it falsely indicates that ChatGPT is, in general, attempting to convey accurate information in its utterances. Where it does track truth, it does so indirectly, and incidentally.</p><p>This is why we favour characterising ChatGPT as a bullshit machine. This terminology avoids the implications that perceiving or remembering is going on in the workings of the LLM. We can also describe it as bullshitting whenever it produces outputs. Like the human bullshitter, some of the outputs will likely be true, while others not. And as with the human bullshitter, we should be wary of relying upon any of these outputs.</p></div></div><div id=\"Sec12-section\" data-title=\"Conclusion\"><h2 id=\"Sec12\">Conclusion</h2><div id=\"Sec12-content\"><p>Investors, policymakers, and members of the general public make decisions on how to treat these machines and how to react to them based not on a deep technical understanding of how they work, but on the often metaphorical way in which their abilities and function are communicated. Calling their mistakes ‘hallucinations’ isn’t harmless: it lends itself to the confusion that the machines are in some way <i>misperceiving</i> but are nonetheless trying to convey something that they believe or have perceived. This, as we’ve argued, is the wrong metaphor. The machines are not trying to communicate something they believe or perceive. Their inaccuracy is not due to misperception or hallucination. As we have pointed out, they are not trying to convey information at all. They are bullshitting.</p><p>Calling chatbot inaccuracies ‘hallucinations’ feeds in to overblown hype about their abilities among technology cheerleaders, and could lead to unnecessary consternation among the general public. It also suggests solutions to the inaccuracy problems which might not work, and could lead to misguided efforts at AI alignment amongst specialists. It can also lead to the wrong attitude towards the machine when it gets things right: the inaccuracies show that it is bullshitting, even when it’s right. Calling these inaccuracies ‘bullshit’ rather than ‘hallucinations’ isn’t just more accurate (as we’ve argued); it’s good science and technology communication in an area that sorely needs it.</p></div></div>\n                                </div></div>","textContent":"\n                                    IntroductionLarge language models (LLMs), programs which use reams of available text and probability calculations in order to create seemingly-human-produced writing, have become increasingly sophisticated and convincing over the last several years, to the point where some commentators suggest that we may now be approaching the creation of artificial general intelligence (see e.g. Knight, 2023 and Sarkar, 2023). Alongside worries about the rise of Skynet and the use of LLMs such as ChatGPT to replace work that could and should be done by humans, one line of inquiry concerns what exactly these programs are up to: in particular, there is a question about the nature and meaning of the text produced, and of its connection to truth. In this paper, we argue against the view that when ChatGPT and the like produce false claims they are lying or even hallucinating, and in favour of the position that the activity they are engaged in is bullshitting, in the Frankfurtian sense (Frankfurt, 2002, 2005). Because these programs cannot themselves be concerned with truth, and because they are designed to produce text that looks truth-apt without any actual concern for truth, it seems appropriate to call their outputs bullshit.We think that this is worth paying attention to. Descriptions of new technology, including metaphorical ones, guide policymakers’ and the public’s understanding of new technology; they also inform applications of the new technology. They tell us what the technology is for and what it can be expected to do. Currently, false statements by ChatGPT and other large language models are described as “hallucinations”, which give policymakers and the public the idea that these systems are misrepresenting the world, and describing what they “see”. We argue that this is an inapt metaphor which will misinform the public, policymakers, and other interested parties.The structure of the paper is as follows: in the first section, we outline how ChatGPT and similar LLMs operate. Next, we consider the view that when they make factual errors, they are lying or hallucinating: that is, deliberately uttering falsehoods, or blamelessly uttering them on the basis of misleading input information. We argue that neither of these ways of thinking are accurate, insofar as both lying and hallucinating require some concern with the truth of their statements, whereas LLMs are simply not designed to accurately represent the way the world is, but rather to give the impression that this is what they’re doing. This, we suggest, is very close to at least one way that Frankfurt talks about bullshit. We draw a distinction between two sorts of bullshit, which we call ‘hard’ and ‘soft’ bullshit, where the former requires an active attempt to deceive the reader or listener as to the nature of the enterprise, and the latter only requires a lack of concern for truth. We argue that at minimum, the outputs of LLMs like ChatGPT are soft bullshit: bullshit–that is, speech or text produced without concern for its truth–that is produced without any intent to mislead the audience about the utterer’s attitude towards truth. We also suggest, more controversially, that ChatGPT may indeed produce hard bullshit: if we view it as having intentions (for example, in virtue of how it is designed), then the fact that it is designed to give the impression of concern for truth qualifies it as attempting to mislead the audience about its aims, goals, or agenda. So, with the caveat that the particular kind of bullshit ChatGPT outputs is dependent on particular views of mind or meaning, we conclude that it is appropriate to talk about ChatGPT-generated text as bullshit, and flag up why it matters that – rather than thinking of its untrue claims as lies or hallucinations – we call bullshit on ChatGPT.What is ChatGPT?Large language models are becoming increasingly good at carrying on convincing conversations. The most prominent large language model is OpenAI’s ChatGPT, so it’s the one we will focus on; however, what we say carries over to other neural network-based AI chatbots, including Google’s Bard chatbot, AnthropicAI’s Claude (claude.ai), and Meta’s LLaMa. Despite being merely complicated bits of software, these models are surprisingly human-like when discussing a wide variety of topics. Test it yourself: anyone can go to the OpenAI web interface and ask for a ream of text; typically, it produces text which is indistinguishable from that of your average English speaker or writer. The variety, length, and similarity to human-generated text that GPT-4 is capable of has convinced many commentators to think that this chatbot has finally cracked it: that this is real (as opposed to merely nominal) artificial intelligence, one step closer to a human-like mind housed in a silicon brain.However, large language models, and other AI models like ChatGPT, are doing considerably less than what human brains do, and it is not clear whether they do what they do in the same way we do. The most obvious difference between an LLM and a human mind involves the goals of the system. Humans have a variety of goals and behaviours, most of which are extra-linguistic: we have basic physical desires, for things like food and sustenance; we have social goals and relationships; we have projects; and we create physical objects. Large language models simply aim to replicate human speech or writing. This means that their primary goal, insofar as they have one, is to produce human-like text. They do so by estimating the likelihood that a particular word will appear next, given the text that has come before.The machine does this by constructing a massive statistical model, one which is based on large amounts of text, mostly taken from the internet. This is done with relatively little input from human researchers or the designers of the system; rather, the model is designed by constructing a large number of nodes, which act as probability functions for a word to appear in a text given its context and the text that has come before it. Rather than putting in these probability functions by hand, researchers feed the system large amounts of text and train it by having it make next-word predictions about this training data. They then give it positive or negative feedback depending on whether it predicts correctly. Given enough text, the machine can construct a statistical model giving the likelihood of the next word in a block of text all by itself.This model associates with each word a vector which locates it in a high-dimensional abstract space, near other words that occur in similar contexts and far from those which don’t. When producing text, it looks at the previous string of words and constructs a different vector, locating the word’s surroundings – its context – near those that occur in the context of similar words. We can think of these heuristically as representing the meaning of the word and the content of its context. But because these spaces are constructed using machine learning by repeated statistical analysis of large amounts of text, we can’t know what sorts of similarity are represented by the dimensions of this high-dimensional vector space. Hence we do not know how similar they are to what we think of as meaning or context. The model then takes these two vectors and produces a set of likelihoods for the next word; it selects and places one of the more likely ones—though not always the most likely. Allowing the model to choose randomly amongst the more likely words produces more creative and human-like text; the parameter which controls this is called the ‘temperature’ of the model and increasing the model’s temperature makes it both seem more creative and more likely to produce falsehoods. The system then repeats the process until it has a recognizable, complete-looking response to whatever prompt it has been given.Given this process, it’s not surprising that LLMs have a problem with the truth. Their goal is to provide a normal-seeming response to a prompt, not to convey information that is helpful to their interlocutor. Examples of this are already numerous, for instance, a lawyer recently prepared his brief using ChatGPT and discovered to his chagrin that most of the cited cases were not real (Weiser, 2023); as Judge P. Kevin Castel put it, ChatGPT produced a text filled with “bogus judicial decisions, with bogus quotes and bogus internal citations”. Similarly, when computer science researchers tested ChatGPT’s ability to assist in academic writing, they found that it was able to produce surprisingly comprehensive and sometimes even accurate text on biological subjects given the right prompts. But when asked to produce evidence for its claims, “it provided five references dating to the early 2000s. None of the provided paper titles existed, and all provided PubMed IDs (PMIDs) were of different unrelated papers” (Alkaissi and McFarland, 2023). These errors can “snowball”: when the language model is asked to provide evidence for or a deeper explanation of a false claim, it rarely checks itself; instead it confidently producesmore false but normal-sounding claims (Zhang et al. 2023). The accuracy problem for LLMs and other generative Ais is often referred to as the problem of “AI hallucination”: the chatbot seems to be hallucinating sources and facts that don’t exist. These inaccuracies are referred to as “hallucinations” in both technical (OpenAI, 2023) and popular contexts (Weise & Metz, 2023).These errors are pretty minor if the only point of a chatbot is to mimic human speech or communication. But the companies designing and using these bots have grander plans: chatbots could replace Google or Bing searches with a more user-friendly conversational interface (Shah & Bender, 2022; Zhu et al., 2023), or assist doctors or therapists in medical contexts (Lysandrou, 2023). In these cases, accuracy is important and the errors represent a serious problem.One attempted solution is to hook the chatbot up to some sort of database, search engine, or computational program that can answer the questions that the LLM gets wrong (Zhu et al., 2023). Unfortunately, this doesn’t work very well either. For example, when ChatGPT is connected to Wolfram Alpha, a powerful piece of mathematical software, it improves moderately in answering simple mathematical questions. But it still regularly gets things wrong, especially for questions which require multi-stage thinking (Davis & Aaronson, 2023). And when connected to search engines or other databases, the models are still fairly likely to provide fake information unless they are given very specific instructions–and even then things aren’t perfect (Lysandrou, 2023). OpenAI has plans to rectify this by training the model to do step by step reasoning (Lightman et al., 2023) but this is quite resource-intensive, and there is reason to be doubtful that it will completely solve the problem—nor is it clear that the result will be a large language model, rather than some broader form of AI.Solutions such as connecting the LLM to a database don’t work is because, if the models are trained on the database, then the words in the database affect the probability that the chatbot will add one or another word to the line of text it is generating. But this will only make it produce text similar to the text in the database; doing so will make it more likely that it reproduces the information in the database but by no means ensures that it will.On the other hand, the LLM can also be connected to the database by allowing it to consult the database, in a way similar to the way it consults or talks to its human interlocutors. In this way, it can use the outputs of the database as text which it responds to and builds on. Here’s one way this can work: when a human interlocutor asks the language model a question, it can then translate the question into a query for the database. Then, it takes the response of the database as an input and builds a text from it to provide back to the human questioner. But this can misfire too, as the chatbots might ask the database the wrong question, or misinterpret its answer (Davis & Aaronson, 2023). “GPT-4 often struggles to formulate a problem in a way that Wolfram Alpha can accept or that produces useful output.” This is not unrelated to the fact that when the language model generates a query for the database or computational module, it does so in the same way it generates text for humans: by estimating the likelihood that some output “looks like’’ the kind of thing the database will correspond with.One might worry that these failed methods for improving the accuracy of chatbots are connected to the inapt metaphor of AI hallucinations. If the AI is misperceiving or hallucinating sources, one way to rectify this would be to put it in touch with real rather than hallucinated sources. But attempts to do so have failed.The problem here isn’t that large language models hallucinate, lie, or misrepresent the world in some way. It’s that they are not designed to represent the world at all; instead, they are designed to convey convincing lines of text. So when they are provided with a database of some sort, they use this, in one way or another, to make their responses more convincing. But they are not in any real way attempting to convey or transmit the information in the database. As Chirag Shah and Emily Bender put it: “Nothing in the design of language models (whose training task is to predict words given context) is actually designed to handle arithmetic, temporal reasoning, etc. To the extent that they sometimes get the right answer to such questions is only because they happened to synthesize relevant strings out of what was in their training data. No reasoning is involved […] Similarly, language models are prone to making stuff up […] because they are not designed to express some underlying set of information in natural language; they are only manipulating the form of language” (Shah & Bender, 2022). These models aren’t designed to transmit information, so we shouldn’t be too surprised when their assertions turn out to be false.Lies, ‘hallucinations’ and bullshitFrankfurtian bullshit and lyingMany popular discussions of ChatGPT call its false statements ‘hallucinations’. One also might think of these untruths as lies. However, we argue that this isn’t the right way to think about it. We will argue that these falsehoods aren’t hallucinations later – in Sect. 3.2.3. For now, we’ll discuss why these untruths aren’t lies but instead are bullshit.The topic of lying has a rich philosophical literature. In ‘Lying’, Saint Augustine distinguished seven types of lies, and his view altered throughout his life. At one point, he defended the position that any instance of knowingly uttering a false utterance counts as a lie, so that even jokes containing false propositions, like –I entered a pun competition and because I really wanted to win, I submitted ten entries. I was sure one of them would win, but no pun in ten did.– would be regarded as a lie, as I have never entered such a competition (Proops & Sorensen, 2023: 3). Later, this view is refined such that the speaker only lies if they intend the hearer to believe the utterance. The suggestion that the speaker must intend to deceive is a common stipulation in literature on lies. According to the “traditional account” of lying:To lie = df. to make a believed-false statement to another person with the intention that the other person believe that statement to be true (Mahon, 2015).For our purposes this definition will suffice. Lies are generally frowned upon. But there are acts of misleading testimony which are criticisable, which do not fall under the umbrella of lying.Footnote 1 These include spreading untrue gossip, which one mistakenly, but culpably, believes to be true. Another class of misleading testimony that has received particular attention from philosophers is that of bullshit. This everyday notion was analysed and introduced into the philosophical lexicon by Harry Frankfurt.Footnote 2Frankfurt understands bullshit to be characterized not by an intent to deceive but instead by a reckless disregard for the truth. A student trying to sound knowledgeable without having done the reading, a political candidate saying things because they sound good to potential voters, and a dilettante trying to spin an interesting story: none of these people are trying to deceive, but they are also not trying to convey facts. To Frankfurt, they are bullshitting.Like “lie”, “bullshit” is both a noun and a verb: an utterance produced can be a lie or an instance of bullshit, as can the act of producing these utterances. For an utterance to be classed as bullshit, it must not be accompanied by the explicit intentions that one has when lying, i.e., to cause a false belief in the hearer. Of course, it must also not be accompanied by the intentions characterised by an honest utterance. So far this story is entirely negative. Must any positive intentions be manifested in the utterer?Throughout most of Frankfurt’s discussion, his characterisation of bullshit is negative. He notes that bullshit requires “no conviction” from the speaker about what the truth is (2005: 55), that the bullshitter “pays no attention” to the truth (2005: 61) and that they “may not deceive us, or even intend to do so, either about the facts or what he takes the facts to be” (2005: 54). Later, he describes the “defining feature” of bullshit as “a lack of concern with truth, or an indifference to how things really are [our emphasis]” (2002: 340). These suggest a negative picture; that for an output to be classed as bullshit, it only needs to lack a certain relationship to the truth.However, in places, a positive intention is presented. Frankfurt says what a bullshitter ….“…does necessarily attempt to deceive us about is his enterprise. His only indispensably distinctive characteristic is that in a certain way he misrepresents what he is up to” (2005: 54).This is somewhat surprising. It restricts what counts as bullshit to utterances accompanied by a higher-order deception. However, some of Frankfurt’s examples seem to lack this feature. When Fania Pascal describes her unwell state as “feeling like a dog that has just been run over” to her friend Wittgenstein, it stretches credulity to suggest that she was intending to deceive him about how much she knew about how run-over dogs felt. And given how the conditions for bullshit are typically described as negative, we might wonder whether the positive condition is really necessary.Bullshit distinctionsShould utterances without an intention to deceive count as bullshit? One reason in favour of expanding the definition, or embracing a plurality of bullshit, is indicated by Frankfurt’s comments on the dangers of bullshit.“In contrast [to merely unintelligible discourse], indifference to the truth is extremely dangerous. The conduct of civilized life, and the vitality of the institutions that are indispensable to it, depend very fundamentally on respect for the distinction between the true and the false. Insofar as the authority of this distinction is undermined by the prevalence of bullshit and by the mindlessly frivolous attitude that accepts the proliferation of bullshit as innocuous, an indispensable human treasure is squandered” (2002: 343).These dangers seem to manifest regardless of whether there is an intention to deceive about the enterprise a speaker is engaged in. Compare the deceptive bullshitter, who does aim to mislead us about being in the truth-business, with someone who harbours no such aim, but just talks for the sake of talking (without care, or indeed any thought, about the truth-values of their utterances).One of Frankfurt’s examples of bullshit seems better captured by the wider definition. He considers the advertising industry, which is “replete with instances of bullshit so unmitigated that they serve among the most indisputable and classic paradigms of the concept” (2005:22). However, it seems to misconstrue many advertisers to portray their aims as to mislead about their agendas. They are expected to say misleading things. Frankfurt discusses Marlboro adverts with the message that smokers are as brave as cowboys (2002: 341). Is it reasonable to suggest that the advertisers pretended to believe this?Frankfurt does allow for multiple species of bullshit (2002: 340).Footnote 3 Following this suggestion, we propose to envisage bullshit as a genus, and Frankfurt’s intentional bullshit as one species within this genus. Other species may include that produced by the advertiser, who anticipates that no one will believe their utterancesFootnote 4 or someone who has no intention one way or another about whether they mislead their audience. To that end, consider the following distinction:\n                  Bullshit (general)\n                  Any utterance produced where a speaker has indifference towards the truth of the utterance.\n                \n                  Hard bullshit\n                  Bullshit produced with the intention to mislead the audience about the utterer’s agenda.\n                \n                  Soft bullshit\n                  Bullshit produced without the intention to mislead the hearer regarding the utterer’s agenda.\n                The general notion of bullshit is useful: on some occasions, we might be confident that an utterance was either soft bullshit or hard bullshit, but be unclear which, given our ignorance of the speaker’s higher-order desires.Footnote 5 In such a case, we can still call bullshit.Frankfurt’s own explicit account, with the positive requirements about producer’s intentions, is hard bullshit, whereas soft bullshit seems to describe some of Frankfurt’s examples, such as that of Pascal’s conversation with Wittgenstein, or the work of advertising agencies. It might be helpful to situate these distinctions in the existing literature. On our view, hard bullshit is most closely aligned with Cassam (2019), and Frankfurt’s positive account, for the reason that all of these views hold that some intention must be present, rather than merely absent, for the utterance to be bullshit: a kind of “epistemic insouciance” or vicious attitude towards truth on Cassam’s view, and (as we have seen) an intent to mislead the hearer about the utterer’s agenda on Frankfurt’s view. In Sect. 3.2 we consider whether ChatGPT may be a hard bullshitter, but it is important to note that it seems to us that hard bullshit, like the two accounts cited here, requires one to take a stance on whether or not LLMs can be agents, and so comes with additional argumentative burdens.Soft bullshit, by contrast, captures only Frankfurt’s negative requirement – that is, the indifference towards truth that we have classed as definitional of bullshit (general) – for the reasons given above. As we argue, ChatGPT is at minimum a soft bullshitter or a bullshit machine, because if it is not an agent then it can neither hold any attitudes towards truth nor towards deceiving hearers about its (or, perhaps more properly, its users’) agenda.It’s important to note that even this more modest kind of bullshitting will have the deleterious effects that concern Frankfurt: as he says, “indifference to the truth is extremely dangerous…by the mindlessly frivolous attitude that accepts the proliferation of bullshit as innocuous, an indispensable human treasure is squandered” (2002, p343). By treating ChatGPT and similar LLMs as being in any way concerned with truth, or by speaking metaphorically as if they make mistakes or suffer “hallucinations” in pursuit of true claims, we risk exactly this acceptance of bullshit, and this squandering of meaning – so, irrespective of whether or not ChatGPT is a hard or a soft bullshitter, it does produce bullshit, and it does matter.ChatGPT is bullshitWith this distinction in hand, we’re now in a position to consider a worry of the following sort: Is ChatGPT hard bullshitting, soft bullshitting, or neither? We will argue, first, that ChatGPT, and other LLMs, are clearly soft bullshitting. However, the question of whether these chatbots are hard bullshitting is a trickier one, and depends on a number of complex questions concerning whether ChatGPT can be ascribed intentions. We canvas a few ways in which ChatGPT can be understood to have the requisite intentions in Sect. 3.2.ChatGPT is a soft bullshitterWe are not confident that chatbots can be correctly described as having any intentions at all, and we’ll go into this in more depth in the next Sect. (3.2). But we are quite certain that ChatGPT does not intend to convey truths, and so is a soft bullshitter. We can produce an easy argument by cases for this. Either ChatGPT has intentions or it doesn’t. If ChatGPT has no intentions at all, it trivially doesn’t intend to convey truths. So, it is indifferent to the truth value of its utterances and so is a soft bullshitter.What if ChatGPT does have intentions? In Sect. 1, we argued that ChatGPT is not designed to produce true utterances; rather, it is designed to produce text which is indistinguishable from the text produced by humans. It is aimed at being convincing rather than accurate. The basic architecture of these models reveals this: they are designed to come up with a likely continuation of a string of text. It’s reasonable to assume that one way of being a likely continuation of a text is by being true; if humans are roughly more accurate than chance, true sentences will be more likely than false ones. This might make the chatbot more accurate than chance, but it does not give the chatbot any intention to convey truths. This is similar to standard cases of human bullshitters, who don’t care whether their utterances are true; good bullshit often contains some degree of truth, that’s part of what makes it convincing. A bullshitter can be more accurate than chance while still being indifferent to the truth of their utterances. We conclude that, even if the chatbot can be described as having intentions, it is indifferent to whether its utterances are true. It does not and cannot care about the truth of its output.Presumably ChatGPT can’t care about conveying or hiding the truth, since it can’t care about anything. So, just as a matter of conceptual necessity, it meets one of Frankfurt’s criteria for bullshit. However, this only gets us so far – a rock can’t care about anything either, and it would be patently absurd to suggest that this means rocks are bullshittersFootnote 6. Similarly books can contain bullshit, but they are not themselves bullshitters. Unlike rocks – or even books – ChatGPT itself produces text, and looks like it performs speech acts independently of its users and designers. And while there is considerable disagreement concerning whether ChatGPT has intentions, it’s widely agreed that the sentences it produces are (typically) meaningful (see e.g. Mandelkern and Linzen 2023).ChatGPT functions not to convey truth or falsehood but rather to convince the reader of – to use Colbert’s apt coinage – the truthiness of its statement, and ChatGPT is designed in such a way as to make attempts at bullshit efficacious (in a way that pens, dictionaries, etc., are not). So, it seems that at minimum, ChatGPT is a soft bullshitter: if we take it not to have intentions, there isn’t any attempt to mislead about the attitude towards truth, but it is nonetheless engaged in the business of outputting utterances that look as if they’re truth-apt. We conclude that ChatGPT is a soft bullshitter.ChatGPT as hard bullshitBut is ChatGPT a hard bullshitter? A critic might object, it is simply inappropriate to think of programs like ChatGPT as hard bullshitters, because (i) they are not agents, or relatedly, (ii) they do not and cannot intend anything whatsoever.We think this is too fast. First, whether or not ChatGPT has agency, its creators and users do. And what they produce with it, we will argue, is bullshit. Second, we will argue that, regardless of whether it has agency, it does have a function; this function gives it characteristic goals, and possibly even intentions, which align with our definition of hard bullshit.Before moving on, we should say what we mean when we ask whether ChatGPT is an agent. For the purposes of this paper, the central question is whether ChatGPT has intentions and or beliefs. Does it intend to deceive? Can it, in any literal sense, be said to have goals or aims? If so, does it intend to deceive us about the content of its utterances, or merely have the goal to appear to be a competent speaker? Does it have beliefs—internal representational states which aim to track the truth? If so, do its utterances match those beliefs (in which case its false statements might be something like hallucinations) or are its utterances not matched to the beliefs—in which case they are likely to be either lies or bullshit? We will consider these questions in more depth in Sect. 3.2.2.There are other philosophically important aspects of agenthood that we will not be considering. We won’t be considering whether ChatGPT makes decisions, has or lacks autonomy, or is conscious; we also won’t worry whether ChatGPT is morally responsible for its statements or its actions (if it has any of those).ChatGPT is a bullshit machineWe will argue that even if ChatGPT is not, itself, a hard bullshitter, it is nonetheless a bullshit machine. The bullshitter is the person using it, since they (i) don’t care about the truth of what it says, (ii) want the reader to believe what the application outputs. On Frankfurt’s view, bullshit is bullshit even if uttered with no intent to bullshit: if something is bullshit to start with, then its repetition “is bullshit as he [or it] repeats it, insofar as it was originated by someone who was unconcerned with whether what he was saying is true or false” (2022, p340).This just pushes the question back to who the originator is, though: take the (increasingly frequent) example of the student essay created by ChatGPT. If the student cared about accuracy and truth, they would not use a program that infamously makes up sources whole-cloth. Equally, though, if they give it a prompt to produce an essay on philosophy of science and it produces a recipe for Bakewell tarts, then it won’t have the desired effect. So the idea of ChatGPT as a bullshit machine seems right, but also as if it’s missing something: someone can produce bullshit using their voice, a pen or a word processor, after all, but we don’t standardly think of these things as being bullshit machines, or of outputting bullshit in any particularly interesting way – conversely, there does seem to be something particular to ChatGPT, to do with the way that it operates, which makes it more than a mere tool, and which suggests that it might appropriately be thought of as an originator of bullshit. In short, it doesn’t seem quite right either to think of ChatGPT as analogous to a pen (can be used for bullshit, but can create nothing without deliberate and wholly agent-directed action) nor as to a bullshitting human (who can intend and produce bullshit on their own initiative).The idea of ChatGPT as a bullshit machine is a helpful one when combined with the distinction between hard and soft bullshit. Reaching again for the example of the dodgy student paper: we’ve all, I take it, marked papers where it was obvious that a dictionary or thesaurus had been deployed with a crushing lack of subtlety; where fifty-dollar words are used not because they’re the best choice, nor even because they serve to obfuscate the truth, but simply because the author wants to convey an impression of understanding and sophistication. It would be inappropriate to call the dictionary a bullshit artist in this case; but it would not be inappropriate to call the result bullshit. So perhaps we should, strictly, say not that ChatGPT is bullshit but that it outputs bullshit in a way that goes beyond being simply a vector of bullshit: it does not and cannot care about the truth of its output, and the person using it does so not to convey truth or falsehood but rather to convince the hearer that the text was written by a interested and attentive agent.ChatGPT may be a hard bullshitterIs ChatGPT itself a hard bullshitter? If so, it must have intentions or goals: it must intend to deceive its listener, not about the content of its statements, but instead about its agenda. Recall that hard bullshitters, like the unprepared student or the incompetent politician, don’t care whether their statements are true or false, but do intend to deceive their audience about what they are doing. If so, it must have intentions or goals: it must intend to deceive its listener, not about the content of its statements, but instead about its agenda. We don’t think that ChatGPT is an agent or has intentions in precisely the same way that humans do (see Levenstein and Herrmann (forthcoming) for a discussion of the issues here). But when speaking loosely it is remarkably easy to use intentional language to describe it: what is ChatGPT trying to do? Does it care whether the text it produces is accurate? We will argue that there is a robust, although perhaps not literal, sense in which ChatGPT does intend to deceive us about its agenda: its goal is not to convince us of the content of its utterances, but instead to portray itself as a ‘normal’ interlocutor like ourselves. By contrast, there is no similarly strong sense in which ChatGPT confabulates, lies, or hallucinates.Our case will be simple: ChatGPT’s primary function is to imitate human speech. If this function is intentional, it is precisely the sort of intention that is required for an agent to be a hard bullshitter: in performing the function, ChatGPT is attempting to deceive the audience about its agenda. Specifically, it’s trying to seem like something that has an agenda, when in many cases it does not. We’ll discuss here whether this function gives rise to, or is best thought of, as an intention. In the next Sect. (3.2.3), we will argue that ChatGPT has no similar function or intention which would justify calling it a confabulator, liar, or hallucinator.How do we know that ChatGPT functions as a hard bullshitter? Programs like ChatGPT are designed to do a task, and this task is remarkably like what Frankfurt thinks the bullshitter intends, namely to deceive the reader about the nature of the enterprise – in this case, to deceive the reader into thinking that they’re reading something produced by a being with intentions and beliefs.ChatGPT’s text production algorithm was developed and honed in a process quite similar to artificial selection. Functions and selection processes have the same sort of directedness that human intentions do; naturalistic philosophers of mind have long connected them to the intentionality of human and animal mental states. If ChatGPT is understood as having intentions or intention-like states in this way, its intention is to present itself in a certain way (as a conversational agent or interlocutor) rather than to represent and convey facts. In other words, it has the intentions we associate with hard bullshitting.One way we can think of ChatGPT as having intentions is by adopting Dennett’s intentional stance towards it. Dennett (1987: 17) describes the intentional stance as a way of predicting the behaviour of systems whose purpose we don’t already know.“To adopt the intentional stance […] is to decide – tentatively, of course – to attempt to characterize, predict, and explain […] behavior by using intentional idioms, such as ‘believes’ and ‘wants,’ a practice that assumes or presupposes the rationality” of the target system (Dennett, 1983: 345).Dennett suggests that if we know why a system was designed, we can make predictions on the basis of its design (1987). While we do know that ChatGPT was designed to chat, its exact algorithm and the way it produces its responses has been developed by machine learning, so we do not know its precise details of how it works and what it does. Under this ignorance it is tempting to bring in intentional descriptions to help us understand and predict what ChatGPT is doing.When we adopt the intentional stance, we will be making bad predictions if we attribute any desire to convey truth to ChatGPT. Similarly, attributing “hallucinations” to ChatGPT will lead us to predict as if it has perceived things that aren’t there, when what it is doing is much more akin to making something up because it sounds about right. The former intentional attribution will lead us to try to correct its beliefs, and fix its inputs --- a strategy which has had limited if any success. On the other hand, if we attribute to ChatGPT the intentions of a hard bullshitter, we will be better able to diagnose the situations in which it will make mistakes and convey falsehoods. If ChatGPT is trying to do anything, it is trying to portray itself as a person.Since this reason for thinking ChatGPT is a hard bullshitter involves committing to one or more controversial views on mind and meaning, it is more tendentious than simply thinking of it as a bullshit machine; but regardless of whether or not the program has intentions, there clearly is an attempt to deceive the hearer or reader about the nature of the enterprise somewhere along the line, and in our view that justifies calling the output hard bullshit.So, though it’s worth making the caveat, it doesn’t seem to us that it significantly affects how we should think of and talk about ChatGPT and bullshit: the person using it to turn out some paper or talk isn’t concerned either with conveying or covering up the truth (since both of those require attention to what the truth actually is), and neither is the system itself. Minimally, it churns out soft bullshit, and, given certain controversial assumptions about the nature of intentional ascription, it produces hard bullshit; the specific texture of the bullshit is not, for our purposes, important: either way, ChatGPT is a bullshitter.Bullshit? hallucinations? confabulations? The need for new terminologyWe have argued that we should use the terminology of bullshit, rather than “hallucinations” to describe the utterances produced by ChatGPT. The suggestion that “hallucination” terminology is inappropriate has also been noted by Edwards (2023), who favours the term “confabulation” instead. Why is our proposal better than this or other alternatives?We object to the term hallucination because it carries certain misleading implications. When someone hallucinates they have a non-standard perceptual experience, but do not actually perceive some feature of the world (Macpherson, 2013), where “perceive” is understood as a success term, such that they do not actually perceive the object or property. This term is inappropriate for LLMs for a variety of reasons. First, as Edwards (2023) points out, the term hallucination anthropomorphises the LLMs. Edwards also notes that attributing resulting problems to “hallucinations” of the models may allow creators to “blame the AI model for faulty outputs instead of taking responsibility for the outputs themselves”, and we may be wary of such abdications of responsibility. LLMs do not perceive, so they surely do not “mis-perceive”. Second, what occurs in the case of an LLM delivering false utterances is not an unusual or deviant form of the process it usually goes through (as some claim is the case in hallucinations, e.g., disjunctivists about perception). The very same process occurs when its outputs happen to be true.So much for “hallucinations”. What about Edwards’ preferred term, “confabulation”? Edwards (2023) says:In human psychology, a “confabulation” occurs when someone’s memory has a gap and the brain convincingly fills in the rest without intending to deceive others. ChatGPT does not work like the human brain, but the term “confabulation” arguably serves as a better metaphor because there’s a creative gap-filling principle at work […].As Edwards notes, this is imperfect. Once again, the use of a human psychological term risks anthropomorphising the LLMs.This term also suggests that there is something exceptional occurring when the LLM makes a false utterance, i.e., that in these occasions - and only these occasions - it “fills in” a gap in memory with something false. This too is misleading. Even when the ChatGPT does give us correct answers, its process is one of predicting the next token. In our view, it falsely indicates that ChatGPT is, in general, attempting to convey accurate information in its utterances. But there are strong reasons to think that it does not have beliefs that it is intending to share in general–see, for example, Levenstein and Herrmann (forthcoming). In our view, it falsely indicates that ChatGPT is, in general, attempting to convey accurate information in its utterances. Where it does track truth, it does so indirectly, and incidentally.This is why we favour characterising ChatGPT as a bullshit machine. This terminology avoids the implications that perceiving or remembering is going on in the workings of the LLM. We can also describe it as bullshitting whenever it produces outputs. Like the human bullshitter, some of the outputs will likely be true, while others not. And as with the human bullshitter, we should be wary of relying upon any of these outputs.ConclusionInvestors, policymakers, and members of the general public make decisions on how to treat these machines and how to react to them based not on a deep technical understanding of how they work, but on the often metaphorical way in which their abilities and function are communicated. Calling their mistakes ‘hallucinations’ isn’t harmless: it lends itself to the confusion that the machines are in some way misperceiving but are nonetheless trying to convey something that they believe or have perceived. This, as we’ve argued, is the wrong metaphor. The machines are not trying to communicate something they believe or perceive. Their inaccuracy is not due to misperception or hallucination. As we have pointed out, they are not trying to convey information at all. They are bullshitting.Calling chatbot inaccuracies ‘hallucinations’ feeds in to overblown hype about their abilities among technology cheerleaders, and could lead to unnecessary consternation among the general public. It also suggests solutions to the inaccuracy problems which might not work, and could lead to misguided efforts at AI alignment amongst specialists. It can also lead to the wrong attitude towards the machine when it gets things right: the inaccuracies show that it is bullshitting, even when it’s right. Calling these inaccuracies ‘bullshit’ rather than ‘hallucinations’ isn’t just more accurate (as we’ve argued); it’s good science and technology communication in an area that sorely needs it.\n                                ","length":43098,"excerpt":"Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called “AI hallucinations”. We argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. We distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.","byline":"Slater, Joe","dir":null,"siteName":"SpringerLink","lang":"en"},"finalizedMeta":{"title":"ChatGPT is bullshit - Ethics and Information Technology","description":"Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called “AI hallucinations”. We argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. We distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.","author":false,"creator":"","publisher":false,"date":"2024-07-10T15:27:34.520Z","topics":[]},"jsonLd":{"@type":"WebPage","headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"mainEntity":{"headline":"ChatGPT is bullshit","description":"Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called “AI hallucinations”. We argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. We distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.","datePublished":"2024-06-08T00:00:00Z","dateModified":"2024-06-08T00:00:00Z","pageStart":"1","pageEnd":"10","license":"http://creativecommons.org/licenses/by/4.0/","sameAs":"https://doi.org/10.1007/s10676-024-09775-5","keywords":["Artificial intelligence","Large language models","LLMs","ChatGPT","Bullshit","Frankfurt","Assertion","Content","Management of Computing and Information Systems","Innovation/Technology Management","Ethics","User Interfaces and Human Computer Interaction","Library Science"],"image":[],"isPartOf":{"name":"Ethics and Information Technology","issn":["1572-8439","1388-1957"],"volumeNumber":"26","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Springer Netherlands","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Michael Townsen Hicks","url":"http://orcid.org/0000-0002-1304-5668","affiliation":[{"name":"University of Glasgow","address":{"name":"University of Glasgow, Glasgow, Scotland","@type":"PostalAddress"},"@type":"Organization"}],"email":"Michael.hicks@glasgow.ac.uk","@type":"Person"},{"name":"James Humphries","affiliation":[{"name":"University of Glasgow","address":{"name":"University of Glasgow, Glasgow, Scotland","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Joe Slater","affiliation":[{"name":"University of Glasgow","address":{"name":"University of Glasgow, Glasgow, Scotland","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"isAccessibleForFree":true,"@type":"ScholarlyArticle"},"@context":"https://schema.org"},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"ChatGPT is bullshit | Ethics and Information Technology","description":"Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications ","canonical":"https://link.springer.com/article/10.1007/s10676-024-09775-5","keywords":[],"image":"//pubads.g.doubleclick.net/gampad/ad?iu=/270604982/springerlink/10676/article&sz=728x90&pos=top&articleid=s10676-024-09775-5","firstParagraph":"Advertisement"},"dublinCore":{},"opengraph":{"title":"ChatGPT is bullshit - Ethics and Information Technology","description":"Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called “AI hallucinations”. We argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. We distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.","url":"https://link.springer.com/article/10.1007/s10676-024-09775-5","site_name":"SpringerLink","locale":false,"type":"article","typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":"https://media.springernature.com/full/springer-static/cover-hires/journal/10676"},"twitter":{"site":"@SpringerLink","description":"Ethics and Information Technology - Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of...","card":"summary_large_image","creator":false,"title":"ChatGPT is bullshit","image":"https://media.springernature.com/full/springer-static/cover-hires/journal/10676","image:alt":"Content cover image"},"archivedData":{"link":false,"wayback":false}}}