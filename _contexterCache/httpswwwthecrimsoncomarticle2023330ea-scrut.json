{"initialLink":"https://www.thecrimson.com/article/2023/3/30/ea-scrut/","sanitizedLink":"https://www.thecrimson.com/article/2023/3/30/ea-scrut/","finalLink":"https://www.thecrimson.com/article/2023/3/30/ea-scrut/","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://www.thecrimson.com/article/2023/3/30/ea-scrut/\" itemprop=\"url\">What is Going On With Effective Altruism? | Magazine | The Harvard Crimson</a></contexter-box-head></contexter-box-head><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2024-08-29T04:36:05.853Z\">8/29/2024</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>“Most of us want to improve the world. We see suffering, injustice, and death and feel moved to do something about it,” the Harvard EA website says. “But figuring out what that ‘something’ is, let alone actually doing it, can be a difficult and disheartening challenge. Effective altruism is a response to this challenge.” Can it live up to that goal?</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a is=\"contexter-link\" href=\"https://www.thecrimson.com/article/2023/3/30/ea-scrut/\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"7e70c1c27426870fbd13cf89d63757354e7c9d69","data":{"originalLink":"https://www.thecrimson.com/article/2023/3/30/ea-scrut/","sanitizedLink":"https://www.thecrimson.com/article/2023/3/30/ea-scrut/","canonical":"https://www.thecrimson.com/article/2023/3/30/ea-scrut/","htmlText":"\n      <!doctype html>\n      <html>\n        <head>\n          <!-- Google tag (gtag.js) -->\n          <script async src='https://www.googletagmanager.com/gtag/js?id=G-VVW6YCM8M9'></script>\n          <script>\n            window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n\n            gtag('config', 'G-VVW6YCM8M9');\n          </script>\n\n          <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n          <script src=\"/static/js/bundle.802094fd.js\" defer></script>\n          <meta data-react-helmet=\"true\" property=\"og:title\" content=\"What is Going On With Effective Altruism? | Magazine | The Harvard Crimson\"/><meta data-react-helmet=\"true\" property=\"og:description\" content=\"“Most of us want to improve the world. We see suffering, injustice, and death and feel moved to do something about it,” the Harvard EA website says. “But figuring out what that ‘something’ is, let alone actually doing it, can be a difficult and disheartening challenge. Effective altruism is a response to this challenge.” Can it live up to that goal?\"/><meta data-react-helmet=\"true\" property=\"og:url\" content=\"https://www.thecrimson.com/article/2023/3/30/ea-scrut/\"/><meta data-react-helmet=\"true\" name=\"twitter:card\" content=\"summary_large_image\"/><meta data-react-helmet=\"true\" name=\"twitter:site\" content=\"@thecrimson\"/><meta data-react-helmet=\"true\" name=\"twitter:title\" content=\"What is Going On With Effective Altruism? | Magazine | The Harvard Crimson\"/><meta data-react-helmet=\"true\" name=\"twitter:description\" content=\"What is Going On With Effective Altruism? | Magazine | The Harvard Crimson\"/><meta data-react-helmet=\"true\" property=\"og:image\" content=\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/143735_1362611.jpeg.2000x1294_q95_crop-smart_upscale.jpg\"/><meta data-react-helmet=\"true\" property=\"og:type\" content=\"Post\"/><meta data-react-helmet=\"true\" name=\"twitter:image\" content=\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/143735_1362611.jpeg.2000x1294_q95_crop-smart_upscale.jpg\"/>\n          \n          <title data-react-helmet=\"true\">What is Going On With Effective Altruism? | Magazine | The Harvard Crimson</title>\n\n          <script>\n            (function(i, s, o, g, r, a, m) {\n              i['GoogleAnalyticsObject'] = r;\n              (i[r] =\n                i[r] ||\n                function() {\n                  (i[r].q = i[r].q || []).push(arguments);\n                }),\n                (i[r].l = 1 * new Date());\n              (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);\n              a.async = 1;\n              a.src = g;\n              m.parentNode.insertBefore(a, m);\n            })(\n              window,\n              document,\n              'script',\n              'https://www.google-analytics.com/analytics.js',\n              'ga'\n            );\n\n            ga('create', 'UA-327124-1', 'auto');\n            ga('send', 'pageview');\n          </script>\n\n          <script type=\"text/javascript\"> (function (w, d, s, p) { let f = d.getElementsByTagName(s)[0], j = d.createElement(s); j.id = 'flytedigital'; j.async = true; j.src = 'https://digital.flytedesk.com/js/head.js#' + p; f.parentNode.insertBefore(j, f); })(window, document, 'script', '8b8310fb-e8db-444b-98f7-7895a23c3a87'); </script>\n\n          <script type=\"text/javascript\" src=\"//s7.addthis.com/js/300/addthis_widget.js#pubid=thecrimson\"></script>\n          <meta charset=\"utf-8\" />\n          <link rel=\"shortcut icon\" href=\"/favicon.ico\" />\n          <meta\n            name=\"viewport\"\n            content=\"width=device-width, initial-scale=1, shrink-to-fit=no\"\n          />\n          \n          <meta name=\"theme-color\" content=\"#000000\" />\n          <script\n            async\n            src=\"https://www.googletagservices.com/tag/js/gpt.js\"></script>\n          <script>\n            var googletag = googletag || {};\n            googletag.cmd = googletag.cmd || [];\n          </script>\n\n        </head>\n        <body >\n          <div id=\"root\"><style data-emotion-css=\"1td45uq\">@import url(https://fonts.googleapis.com/css?family=PT+Serif&display=swap);@import url(https://fonts.googleapis.com/css?family=Vollkorn:400italic,700italic,400,700&display=swap);@import url(https://fonts.googleapis.com/css?family=Roboto:300,400&display=swap);@import url(https://fonts.googleapis.com/css?family=Open+Sans:300,400,600&display=swap);@import url(https://fonts.googleapis.com/css?family=Lato:300,400,700&display=swap);@import url(https://fonts.googleapis.com/css?family=Lora:400,700,400italic&display=swap);@import url(https://fonts.googleapis.com/css?family=Montserrat:400,500,700&display=swap);@import url(https://fonts.googleapis.com/css?family=Fira+Sans:300,400,500&display=swap);@import url(https://fonts.googleapis.com/css?family=Fira+Sans+Condensed:300,400,500,600&display=swap);@import url(https://fonts.googleapis.com/css?family=Poppins:400,600&display=swap);@import url(https://fonts.googleapis.com/css?family=Encode+Sans:400,600,700&display=swap);@import url(https://fonts.googleapis.com/css?family=Source+Sans+Pro&display=swap);{/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */}html{line-height:1.15;-webkit-text-size-adjust:100%;}body{margin:0;}main{display:block;}h1{font-size:2em;margin:0.67em 0;}hr{box-sizing:content-box;height:0;overflow:visible;}pre{font-family:monospace,monospace;font-size:1em;}a{background-color:transparent;}abbr[title]{border-bottom:none;-webkit-text-decoration:underline;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted;}b,strong{font-weight:bolder;}code,kbd,samp{font-family:monospace,monospace;font-size:1em;}small{font-size:80%;}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline;}sub{bottom:-0.25em;}sup{top:-0.5em;}img{border-style:none;}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0;}button,input{overflow:visible;}button,select{text-transform:none;}button,[type='button'],[type='reset'],[type='submit']{-webkit-appearance:button;}button::-moz-focus-inner,[type='button']::-moz-focus-inner,[type='reset']::-moz-focus-inner,[type='submit']::-moz-focus-inner{border-style:none;padding:0;}button:-moz-focusring,[type='button']:-moz-focusring,[type='reset']:-moz-focusring,[type='submit']:-moz-focusring{outline:1px dotted ButtonText;}fieldset{padding:0.35em 0.75em 0.625em;}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal;}progress{vertical-align:baseline;}textarea{overflow:auto;}[type='checkbox'],[type='radio']{box-sizing:border-box;padding:0;}[type='number']::-webkit-inner-spin-button,[type='number']::-webkit-outer-spin-button{height:auto;}[type='search']{-webkit-appearance:textfield;outline-offset:-2px;}[type='search']::-webkit-search-decoration{-webkit-appearance:none;}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit;}details{display:block;}summary{display:list-item;}template{display:none;}[hidden]{display:none;}@font-face{font-family:'Colaborate';src:url(/static/media/ColabReg-webfont.efc47d70.eot);src:url(/static/media/ColabReg-webfont.efc47d70.eot?#iefix) format('embedded-opentype'), url(/static/media/ColabReg-webfont.334f282b.woff) format('woff'), url(/static/media/ColabReg-webfont.62678982.ttf) format('truetype'), url(/static/media/ColabReg-webfont.7e060816.svg#ColaborateThinRegular) format('svg');font-weight:normal;font-style:normal;}@font-face{font-family:'Colaborate';src:url(/static/media/ColabBol-webfont.f0533ef1.eot);src:url(/static/media/ColabBol-webfont.f0533ef1.eot?#iefix) format('embedded-opentype'), url(/static/media/ColabBol-webfont.82177114.woff) format('woff'), url(/static/media/ColabBol-webfont.20ecdcc7.ttf) format('truetype'), url(/static/media/ColabBol-webfont.b3bed328.svg#ColaborateThinRegular) format('svg');font-weight:bold;font-style:normal;}@font-face{font-family:'Colaborate Thin';src:url(/static/media/ColabThi-webfont.a81e90df.eot);src:url(/static/media/ColabThi-webfont.a81e90df.eot?#iefix) format('embedded-opentype'), url(/static/media/ColabThi-webfont.82c0a70c.woff) format('woff'), url(/static/media/ColabThi-webfont.34506b78.ttf) format('truetype'), url(/static/media/ColabThi-webfont.917e0e2c.svg#ColaborateThinRegular) format('svg');font-weight:normal;font-style:normal;}@font-face{font-family:'Colaborate Light';src:url(/static/media/ColabLig-webfont.7750d539.eot);src:url(/static/media/ColabLig-webfont.7750d539.eot?#iefix) format('embedded-opentype'), url(/static/media/ColabLig-webfont.333a60a4.woff) format('woff'), url(/static/media/ColabLig-webfont.cefedb99.ttf) format('truetype'), url(/static/media/ColabLig-webfont.9aaf8456.svg#ColaborateLightRegular) format('svg');font-weight:normal;font-style:normal;}@font-face{font-family:'Crimson';src:url(/static/media/Crimson-Roman-webfont.41f04f70.eot);src:url(/static/media/Crimson-Roman-webfont.41f04f70.eot?#iefix) format('embedded-opentype'), url(/static/media/Crimson-Roman-webfont.88717972.woff) format('woff'), url(/static/media/Crimson-Roman-webfont.dbb6146d.ttf) format('truetype'), url(/static/media/Crimson-Roman-webfont.11ffe883.svg#CrimsonRoman) format('svg');font-weight:normal;font-style:normal;}@font-face{font-family:'Crimson';src:url(/static/media/Crimson-Bold-webfont.4bb7e323.eot);src:url(/static/media/Crimson-Bold-webfont.4bb7e323.eot?#iefix) format('embedded-opentype'), url(/static/media/Crimson-Bold-webfont.22be8efe.woff) format('woff'), url(/static/media/Crimson-Bold-webfont.8dd77d8b.ttf) format('truetype'), url(/static/media/Crimson-Bold-webfont.f974c011.svg#CrimsonBold) format('svg');font-weight:bold;font-style:normal;}@font-face{font-family:'Crimson';src:url(/static/media/Crimson-BoldItalic-webfont.a1245e00.eot);src:url(/static/media/Crimson-BoldItalic-webfont.a1245e00.eot?#iefix) format('embedded-opentype'), url(/static/media/Crimson-BoldItalic-webfont.f949e760.woff) format('woff'), url(/static/media/Crimson-BoldItalic-webfont.fd85eb5e.ttf) format('truetype'), url(/static/media/Crimson-BoldItalic-webfont.b45607f4.svg#CrimsonBoldItalic) format('svg');font-weight:bold;font-style:italic;}@font-face{font-family:'Crimson';src:url(/static/media/Crimson-Italic-webfont.38f478ef.eot);src:url(/static/media/Crimson-Italic-webfont.38f478ef.eot?#iefix) format('embedded-opentype'), url(/static/media/Crimson-Italic-webfont.9141f13f.woff) format('woff'), url(/static/media/Crimson-Italic-webfont.41e95e6c.ttf) format('truetype'), url(/static/media/Crimson-Italic-webfont.b22dfb7f.svg#CrimsonItalic) format('svg');font-weight:normal;font-style:italic;}@font-face{font-family:'Chunk Five';src:url(/static/media/Chunkfive-webfont.ba95aac3.eot);src:url(/static/media/Chunkfive-webfont.ba95aac3.eot?#iefix) format('embedded-opentype'), url(/static/media/Chunkfive-webfont.af5dc4da.woff) format('woff'), url(/static/media/Chunkfive-webfont.0581c277.ttf) format('truetype'), url(/static/media/Chunkfive-webfont.c9ca1787.svg#ColaborateThinRegular) format('svg');font-weight:normal;font-style:normal;}@font-face{font-family:'Bebas';src:url(/static/media/BEBAS___-webfont.973cf37a.eot);src:url(/static/media/BEBAS___-webfont.973cf37a.eot?#iefix) format('embedded-opentype'), url(/static/media/BEBAS___-webfont.f3ac9c0c.woff) format('woff'), url(/static/media/BEBAS___-webfont.9254ccd9.ttf) format('truetype'), url(/static/media/BEBAS___-webfont.49b796e3.svg#bebasregular) format('svg');font-weight:normal;font-style:normal;}@font-face{font-family:'Big Moore';src:url(/static/media/BigMoore-Regular.f213cb76.woff) format('woff');}@font-face{font-family:'FranklinGothicBook';src:url(/static/media/FranklinGothicBook.6eb311e8.woff) format('woff');}@font-face{font-family:'Melbourne';src:url(/static/media/Melbourne_reg.b036c47e.woff) format('woff'), url(/static/media/Melbourne_reg.9f3eb53d.ttf) format('truetype');font-weight:normal;}@font-face{font-family:'Mercury Text G2';src:url(/static/media/MercuryTextG2-Roman.6f645676.eot);src:url(/static/media/MercuryTextG2-Roman.df98aacb.woff) format('woff'), url(/static/media/MercuryTextG2-Roman.5700f1d5.ttf) format('truetype');font-weight:normal;}@font-face{font-family:'SuecaSans-Medium';src:url(/static/media/SuecaSans-Medium.fab10fdc.otf);font-weight:normal;}@font-face{font-family:'SuecaSlab-Heavy';src:url(/static/media/SuecaSlab-Heavy.48424d82.otf);font-weight:normal;}@font-face{font-family:'SuecaSlab-Semibold';src:url(/static/media/SuecaSlab-Semibold.4cb057c2.otf);font-weight:normal;}@font-face{font-family:'SuecaSlab-Extralight';src:url(/static/media/SuecaSlab-Extralight.93d5d9ac.otf);font-weight:normal;}body{font-family:Georgia,serif;}a{color:#000000;-webkit-text-decoration:none;text-decoration:none;}p a{color:#bc413e;}a:active,a:hover{color:#7b1b18;-moz-transition:color 0.3s;-o-transition:color 0.3s;-webkit-transition:color 0.3s;-webkit-transition:color 0.3s;transition:color 0.3s;}h1,h2,h3,h4,h5,h6{font-family:Crimson,Georgia,serif;font-weight:normal;}</style><script id=\"parsely-cfg\" src=\"//cdn.parsely.com/keys/thecrimson.com/p.js\"></script><div><style data-emotion-css=\"1pkz6kq\">.css-1pkz6kq .sticky-navbar{width:100%;z-index:1001;height:48px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;overflow:hidden;-webkit-transition:top 0.3s;transition:top 0.3s;}@media (max-width:1000px){.css-1pkz6kq .sticky-navbar{top:0px;}}.css-1pkz6kq .sticky-navbar-active{top:0px;}.css-1pkz6kq .sidebar-menu{position:fixed;left:-201px;width:200px;height:100%;-webkit-transition:all 0.2s;transition:all 0.2s;z-index:1000;padding-top:66px;top:auto;background:white;border-right:1px solid #ccc;}.css-1pkz6kq .sidebar-menu form{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;padding:10px 10px 10px 0px;background-color:whitesmoke;}.css-1pkz6kq .sidebar-menu form input{height:20px;background-color:whitesmoke;}.css-1pkz6kq .sidebar-menu form .sidebar-search-box{border:0px;margin:0px 20px;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;min-width:0px;}.css-1pkz6kq .sidebar-menu form .sidebar-search-box:focus{outline-width:0;}.css-1pkz6kq .sidebar-menu .sidebar-list{padding:0px 20px;}.css-1pkz6kq .sidebar-menu .sidebar-list .sidebar-menu-title{font-size:14px;margin:14px 0px;}.css-1pkz6kq .sidebar-menu .sidebar-list ul{list-style-type:none;text-transform:uppercase;padding:0px;}.css-1pkz6kq .sidebar-menu .sidebar-list ul li{padding:4px 0px;}.css-1pkz6kq .sidebar-menu .sidebar-list ul li.links-top{font:300 16px Lato,sans-serif;}.css-1pkz6kq .sidebar-menu .sidebar-list ul li.links-bottom{font:300 12px Lato,sans-serif;}.css-1pkz6kq .sidebar-menu .sidebar-list .social-icons{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}.css-1pkz6kq .sidebar-menu .sidebar-list .social-icons img{width:20px;height:20px;margin-right:16px;}.css-1pkz6kq .sidebar-menu-active{left:0px;}.css-1pkz6kq #wrapped-burger{border:1px solid rgba(255,255,255,0.5);border-radius:2px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:5px;z-index:3;}@media (max-width:576px){.css-1pkz6kq #wrapped-burger{padding:2px;}}@media (max-width:319px){.css-1pkz6kq #wrapped-burger{display:none;}}.css-1pkz6kq #wrapped-burger-black{border:1px solid #000000;border-radius:2px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:5px;z-index:3;}@media (max-width:576px){.css-1pkz6kq #wrapped-burger-black{padding:2px;}}@media (max-width:319px){.css-1pkz6kq #wrapped-burger-black{display:none;}}.css-1pkz6kq #wrapped-burger-crimson{border:1px solid #a50a0e;border-radius:2px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:5px;z-index:3;}@media (max-width:576px){.css-1pkz6kq #wrapped-burger-crimson{padding:2px;}}@media (max-width:319px){.css-1pkz6kq #wrapped-burger-crimson{display:none;}}.css-1pkz6kq #hamburger-title{display:block;font-family:Lato;font-style:normal;font-weight:800;font-size:14px;line-height:12px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;text-align:center;color:#ffffff;margin:auto;}@media (max-width:576px){.css-1pkz6kq #hamburger-title{display:none;}}.css-1pkz6kq #hamburger-title-black{display:block;font-family:Lato;font-style:normal;font-weight:800;font-size:14px;line-height:12px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;text-align:center;color:#000000;margin:auto;}@media (max-width:576px){.css-1pkz6kq #hamburger-title-black{display:none;}}.css-1pkz6kq #hamburger-title-crimson{display:block;font-family:Lato;font-style:normal;font-weight:800;font-size:14px;line-height:12px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;text-align:center;color:#a50a0e;margin:auto;}@media (max-width:576px){.css-1pkz6kq #hamburger-title-crimson{display:none;}}</style><div class=\"css-1pkz6kq\"><style data-emotion-css=\"1frh5bl\">.css-1frh5bl{position:absolute;top:0px;left:0px;background:rgb(255,255,255,0);box-shadow:0px 2px 4px rgba(0,0,0,0.4);border-bottom:none;top:0px;}</style><div class=\"sticky-navbar css-1frh5bl\"><style data-emotion-css=\"1y6c6l1\">.css-1y6c6l1{position:absolute;left:9px;height:20px;-webkit-transition:all 0.1s;transition:all 0.1s;cursor:pointer;fill:white;-webkit-filter:drop-shadow(0px 0px 6px rgba(0,0,0,1));filter:drop-shadow(0px 0px 6px rgba(0,0,0,1));}</style><svg height=\"32px\" id=\"Layer_1\" style=\"enable-background:new 0 0 32 32\" viewBox=\"0 0 32 32\" width=\"32px\" xml:space=\"preserve\" class=\"css-1y6c6l1\"><path d=\"M4,10h24c1.104,0,2-0.896,2-2s-0.896-2-2-2H4C2.896,6,2,6.896,2,8S2.896,10,4,10z M28,14H4c-1.104,0-2,0.896-2,2  s0.896,2,2,2h24c1.104,0,2-0.896,2-2S29.104,14,28,14z M28,22H4c-1.104,0-2,0.896-2,2s0.896,2,2,2h24c1.104,0,2-0.896,2-2  S29.104,22,28,22z\"></path></svg><a href=\"/\" to=\"/\"><style data-emotion-css=\"1p0q6qr\">.css-1p0q6qr{height:18px;-webkit-filter:drop-shadow(0px 0px 6px rgba(0,0,0,1));filter:drop-shadow(0px 0px 6px rgba(0,0,0,1));width:unset;}.css-1p0q6qr path{fill:white;}</style><svg width=\"381\" height=\"31\" viewBox=\"0 0 381 31\" fill=\"none\" class=\"css-1p0q6qr\"><path d=\"M22.7326 30H7.84963V29.59C8.91563 29.3713 9.68096 29.1937 10.1456 29.057C10.6376 28.893 11.116 28.606 11.5806 28.196C12.0726 27.7587 12.4006 27.171 12.5646 26.433C12.756 25.6677 12.8653 24.629 12.8926 23.317C12.9473 21.759 12.9746 19.176 12.9746 15.568C12.9746 11.0307 12.9336 6.466 12.8516 1.874H12.0316C9.24363 1.874 6.90663 2.47533 5.02063 3.678C3.16196 4.85333 1.97296 6.61633 1.45363 8.967H0.961629L1.53563 1.382H28.7596L29.3336 8.967H28.8416C27.721 4.23833 24.195 1.874 18.2636 1.874H17.4436C17.3616 6.466 17.3206 11.0717 17.3206 15.691C17.3206 19.791 17.348 22.4287 17.4026 23.604C17.4573 24.588 17.594 25.4353 17.8126 26.146C18.0313 26.8293 18.2636 27.3623 18.5096 27.745C18.783 28.1277 19.1793 28.4557 19.6986 28.729C20.2453 29.0023 20.6963 29.18 21.0516 29.262C21.4343 29.344 21.9946 29.4533 22.7326 29.59V30ZM38.7233 29.59V30H29.3343V29.59C30.4823 29.4533 31.2749 29.098 31.7123 28.524C32.1769 27.9227 32.4093 26.6653 32.4093 24.752V4.785C32.4093 4.42967 32.3819 4.11533 32.3273 3.842C32.2726 3.54133 32.1906 3.29533 32.0813 3.104C31.9993 2.91267 31.9036 2.735 31.7943 2.571C31.6849 2.407 31.5483 2.284 31.3843 2.202C31.2203 2.09267 31.0836 2.01067 30.9743 1.956C30.8649 1.90133 30.7146 1.86033 30.5233 1.833C30.3319 1.77833 30.1953 1.751 30.1133 1.751C30.0586 1.72366 29.9493 1.71 29.7853 1.71H29.5393V1.341L35.5663 0.603H35.7303V14.625C37.0696 12.575 39.1059 11.55 41.8393 11.55C43.5339 11.55 44.9553 11.9463 46.1033 12.739C47.2786 13.5043 47.8663 14.6933 47.8663 16.306V26.023C47.8663 26.4057 47.8799 26.7473 47.9073 27.048C47.9346 27.3487 47.9893 27.6083 48.0713 27.827C48.1806 28.0457 48.2626 28.237 48.3173 28.401C48.3719 28.565 48.4813 28.7153 48.6453 28.852C48.8366 28.9613 48.9596 29.057 49.0143 29.139C49.0963 29.221 49.2466 29.2893 49.4653 29.344C49.7113 29.3987 49.8616 29.4397 49.9163 29.467C49.9709 29.4943 50.1349 29.5353 50.4083 29.59C50.6816 29.6173 50.8319 29.631 50.8593 29.631V30H41.3063V29.549C41.8256 29.4943 42.1946 29.4397 42.4133 29.385C42.6593 29.3303 42.9736 29.2073 43.3563 29.016C43.7663 28.7973 44.0533 28.4283 44.2173 27.909C44.4086 27.3897 44.5043 26.7337 44.5043 25.941C44.5043 22.6337 44.4906 19.75 44.4633 17.29C44.4359 15.65 44.1216 14.42 43.5203 13.6C42.9189 12.78 41.9349 12.37 40.5683 12.37C39.5569 12.37 38.6276 12.698 37.7803 13.354C36.9603 13.9827 36.3999 14.953 36.0993 16.265C35.7986 17.6043 35.6483 19.3673 35.6483 21.554C35.6483 23.03 35.6756 24.6973 35.7303 26.556C35.7303 27.3213 35.8806 27.9363 36.1813 28.401C36.5093 28.8383 36.8509 29.1253 37.2063 29.262C37.5616 29.3713 38.0673 29.4807 38.7233 29.59ZM68.6843 25.367L69.1763 25.49C69.067 26.7473 68.3973 27.9227 67.1673 29.016C65.9373 30.082 64.2153 30.615 62.0013 30.615C58.9673 30.615 56.603 29.754 54.9083 28.032C53.241 26.31 52.4073 24.096 52.4073 21.39C52.4073 18.7387 53.2273 16.4427 54.8673 14.502C56.5073 12.5613 58.7897 11.591 61.7143 11.591C63.9283 11.591 65.6777 12.2197 66.9623 13.477C68.2743 14.7343 68.9987 16.265 69.1353 18.069H56.4663C56.357 18.8343 56.3023 19.7773 56.3023 20.898C56.3023 22.4833 56.521 23.8637 56.9583 25.039C57.423 26.2143 58.0243 27.1027 58.7623 27.704C59.5003 28.278 60.2247 28.7017 60.9353 28.975C61.646 29.221 62.3703 29.344 63.1083 29.344C64.6937 29.344 65.951 28.975 66.8803 28.237C67.837 27.4717 68.4383 26.515 68.6843 25.367ZM56.5483 17.413H65.6503C65.5957 15.9917 65.1993 14.7617 64.4613 13.723C63.7507 12.657 62.7803 12.124 61.5503 12.124C58.899 12.124 57.2317 13.887 56.5483 17.413ZM114.604 30H101.074V29.59C101.156 29.5627 101.402 29.5217 101.812 29.467C102.249 29.4123 102.522 29.3713 102.632 29.344C102.741 29.3167 102.96 29.262 103.288 29.18C103.643 29.0707 103.862 28.975 103.944 28.893C104.053 28.7837 104.231 28.647 104.477 28.483C104.723 28.2917 104.887 28.0867 104.969 27.868C105.051 27.622 105.146 27.3487 105.256 27.048C105.392 26.72 105.488 26.351 105.543 25.941C105.597 25.5037 105.625 25.0253 105.625 24.506C105.707 20.98 105.748 18.0417 105.748 15.691H90.6597C90.6597 16.9757 90.6597 18.643 90.6597 20.693C90.687 22.743 90.7007 24.0003 90.7007 24.465C90.7007 25.4763 90.769 26.2963 90.9057 26.925C91.0697 27.5537 91.3704 28.0457 91.8077 28.401C92.245 28.729 92.614 28.9477 92.9147 29.057C93.2427 29.1663 93.7894 29.303 94.5547 29.467C94.8007 29.5217 94.9784 29.5627 95.0877 29.59V30H81.9267V29.59C82.9927 29.3713 83.7307 29.1937 84.1407 29.057C84.5507 28.9203 84.947 28.647 85.3297 28.237C85.7124 27.7997 85.9447 27.2667 86.0267 26.638C86.136 26.0093 86.2044 25.0663 86.2317 23.809C86.3137 19.053 86.3547 16.306 86.3547 15.568C86.3547 12.4793 86.341 10.1833 86.3137 8.68C86.2864 6.712 86.1497 5.30433 85.9037 4.457C85.685 3.58233 85.3024 2.981 84.7557 2.653C84.2364 2.325 83.307 2.02433 81.9677 1.751V1.382H95.0877V1.751C95.0057 1.77833 94.8007 1.81933 94.4727 1.874C94.172 1.92867 93.9534 1.98333 93.8167 2.038C93.7074 2.06533 93.5024 2.13366 93.2017 2.243C92.901 2.325 92.6687 2.42067 92.5047 2.53C92.368 2.612 92.1904 2.74866 91.9717 2.94C91.753 3.104 91.5754 3.29533 91.4387 3.514C91.3294 3.73267 91.22 3.99233 91.1107 4.293C91.0014 4.59366 90.933 4.93533 90.9057 5.318C90.7417 7.204 90.6597 10.3747 90.6597 14.83H105.748C105.748 11.304 105.666 8.16067 105.502 5.4C105.474 5.04467 105.42 4.71667 105.338 4.416C105.283 4.11533 105.174 3.85567 105.01 3.637C104.873 3.41833 104.75 3.227 104.641 3.063C104.559 2.899 104.395 2.74866 104.149 2.612C103.903 2.47533 103.725 2.37967 103.616 2.325C103.506 2.243 103.301 2.161 103.001 2.079C102.7 1.997 102.509 1.956 102.427 1.956C102.372 1.92866 102.167 1.88766 101.812 1.833L101.32 1.751V1.382H114.563V1.751C112.622 2.079 111.419 2.62566 110.955 3.391C110.517 4.15633 110.271 5.892 110.217 8.598C110.135 11.3313 110.094 13.6547 110.094 15.568C110.094 16.6613 110.107 18.151 110.135 20.037C110.162 21.8957 110.176 23.0163 110.176 23.399C110.176 24.465 110.23 25.3397 110.34 26.023C110.476 26.7063 110.613 27.253 110.75 27.663C110.914 28.073 111.214 28.4147 111.652 28.688C112.116 28.934 112.513 29.1117 112.841 29.221C113.196 29.303 113.784 29.426 114.604 29.59V30ZM126.287 19.545C125.604 20.3377 124.223 21.2397 122.146 22.251C120.123 23.2897 119.112 24.6837 119.112 26.433C119.112 27.3897 119.413 28.155 120.014 28.729C120.615 29.303 121.353 29.59 122.228 29.59C124.77 29.59 126.109 27.827 126.246 24.301C126.273 23.399 126.287 21.8137 126.287 19.545ZM133.667 28.114L133.995 28.278C133.913 28.8793 133.503 29.4123 132.765 29.877C132.027 30.3417 131.07 30.574 129.895 30.574C128.72 30.574 127.818 30.3553 127.189 29.918C126.56 29.4533 126.219 28.8657 126.164 28.155C124.661 29.795 122.87 30.615 120.793 30.615C119.126 30.615 117.827 30.205 116.898 29.385C115.996 28.565 115.545 27.4717 115.545 26.105C115.545 24.957 115.914 23.973 116.652 23.153C117.39 22.3057 118.401 21.718 119.686 21.39C120.069 21.2807 120.629 21.1577 121.367 21.021C122.105 20.8843 122.747 20.7477 123.294 20.611C123.841 20.4743 124.305 20.2967 124.688 20.078C125.289 19.75 125.699 19.34 125.918 18.848C126.164 18.356 126.287 17.618 126.287 16.634C126.287 15.3767 125.986 14.3107 125.385 13.436C124.811 12.5613 123.991 12.124 122.925 12.124C121.886 12.124 121.135 12.4247 120.67 13.026C120.397 13.3267 120.11 13.969 119.809 14.953C119.563 15.9097 119.016 16.388 118.169 16.388C117.759 16.388 117.404 16.2513 117.103 15.978C116.802 15.6773 116.652 15.281 116.652 14.789C116.652 13.9417 117.185 13.2037 118.251 12.575C119.317 11.919 120.861 11.591 122.884 11.591C127.394 11.591 129.649 13.026 129.649 15.896V25.285C129.649 26.8977 129.799 28.0047 130.1 28.606C130.428 29.18 131.002 29.467 131.822 29.467C132.724 29.467 133.339 29.016 133.667 28.114ZM143.514 30H134.084V29.59C135.259 29.4533 136.052 29.1663 136.462 28.729C136.899 28.2643 137.118 27.2393 137.118 25.654V17.577C137.118 14.297 136.312 12.657 134.699 12.657H133.92V12.247L139.783 11.345H139.906L140.439 15.117C141.669 12.7663 143.377 11.591 145.564 11.591C146.603 11.591 147.368 11.8233 147.86 12.288C148.352 12.7253 148.598 13.1763 148.598 13.641C148.598 14.1057 148.461 14.502 148.188 14.83C147.942 15.158 147.587 15.322 147.122 15.322C146.876 15.322 146.671 15.2947 146.507 15.24C146.343 15.1853 146.165 15.076 145.974 14.912C145.81 14.748 145.701 14.6387 145.646 14.584C145.619 14.5293 145.509 14.3653 145.318 14.092C144.744 13.2993 144.197 12.903 143.678 12.903C143.022 12.903 142.434 13.2173 141.915 13.846C141.396 14.4473 141.013 15.2127 140.767 16.142C140.494 17.372 140.357 19.176 140.357 21.554C140.357 22.9753 140.384 24.6153 140.439 26.474C140.466 27.6493 140.753 28.4557 141.3 28.893C141.874 29.303 142.612 29.5353 143.514 29.59V30ZM158.795 30.451H158.098C156.731 26.925 154.79 22.1827 152.276 16.224C151.62 14.666 151.046 13.682 150.554 13.272C150.062 12.8347 149.392 12.575 148.545 12.493V12.124H157.975V12.493C156.143 12.6843 155.228 13.2173 155.228 14.092C155.228 14.4473 155.542 15.4723 156.171 17.167C156.635 18.3423 157.865 21.267 159.861 25.941C161.255 22.6883 162.321 20.0643 163.059 18.069C163.687 16.4017 164.002 15.2127 164.002 14.502C164.002 13.4907 163.168 12.821 161.501 12.493V12.124H168.635V12.493C168.033 12.657 167.582 12.8073 167.282 12.944C166.981 13.0533 166.571 13.3813 166.052 13.928C165.56 14.4747 165.109 15.24 164.699 16.224C161.61 23.5767 159.642 28.319 158.795 30.451ZM179.179 19.545C178.495 20.3377 177.115 21.2397 175.038 22.251C173.015 23.2897 172.004 24.6837 172.004 26.433C172.004 27.3897 172.304 28.155 172.906 28.729C173.507 29.303 174.245 29.59 175.12 29.59C177.662 29.59 179.001 27.827 179.138 24.301C179.165 23.399 179.179 21.8137 179.179 19.545ZM186.559 28.114L186.887 28.278C186.805 28.8793 186.395 29.4123 185.657 29.877C184.919 30.3417 183.962 30.574 182.787 30.574C181.611 30.574 180.709 30.3553 180.081 29.918C179.452 29.4533 179.11 28.8657 179.056 28.155C177.552 29.795 175.762 30.615 173.685 30.615C172.017 30.615 170.719 30.205 169.79 29.385C168.888 28.565 168.437 27.4717 168.437 26.105C168.437 24.957 168.806 23.973 169.544 23.153C170.282 22.3057 171.293 21.718 172.578 21.39C172.96 21.2807 173.521 21.1577 174.259 21.021C174.997 20.8843 175.639 20.7477 176.186 20.611C176.732 20.4743 177.197 20.2967 177.58 20.078C178.181 19.75 178.591 19.34 178.81 18.848C179.056 18.356 179.179 17.618 179.179 16.634C179.179 15.3767 178.878 14.3107 178.277 13.436C177.703 12.5613 176.883 12.124 175.817 12.124C174.778 12.124 174.026 12.4247 173.562 13.026C173.288 13.3267 173.001 13.969 172.701 14.953C172.455 15.9097 171.908 16.388 171.061 16.388C170.651 16.388 170.295 16.2513 169.995 15.978C169.694 15.6773 169.544 15.281 169.544 14.789C169.544 13.9417 170.077 13.2037 171.143 12.575C172.209 11.919 173.753 11.591 175.776 11.591C180.286 11.591 182.541 13.026 182.541 15.896V25.285C182.541 26.8977 182.691 28.0047 182.992 28.606C183.32 29.18 183.894 29.467 184.714 29.467C185.616 29.467 186.231 29.016 186.559 28.114ZM196.406 30H186.976V29.59C188.151 29.4533 188.944 29.1663 189.354 28.729C189.791 28.2643 190.01 27.2393 190.01 25.654V17.577C190.01 14.297 189.203 12.657 187.591 12.657H186.812V12.247L192.675 11.345H192.798L193.331 15.117C194.561 12.7663 196.269 11.591 198.456 11.591C199.494 11.591 200.26 11.8233 200.752 12.288C201.244 12.7253 201.49 13.1763 201.49 13.641C201.49 14.1057 201.353 14.502 201.08 14.83C200.834 15.158 200.478 15.322 200.014 15.322C199.768 15.322 199.563 15.2947 199.399 15.24C199.235 15.1853 199.057 15.076 198.866 14.912C198.702 14.748 198.592 14.6387 198.538 14.584C198.51 14.5293 198.401 14.3653 198.21 14.092C197.636 13.2993 197.089 12.903 196.57 12.903C195.914 12.903 195.326 13.2173 194.807 13.846C194.287 14.4473 193.905 15.2127 193.659 16.142C193.385 17.372 193.249 19.176 193.249 21.554C193.249 22.9753 193.276 24.6153 193.331 26.474C193.358 27.6493 193.645 28.4557 194.192 28.893C194.766 29.303 195.504 29.5353 196.406 29.59V30ZM216.256 26.638C216.639 25.654 216.83 23.9183 216.83 21.431C216.83 18.561 216.584 16.4837 216.092 15.199C215.3 13.1763 213.851 12.165 211.746 12.165C210.106 12.165 208.753 13.067 207.687 14.871C206.621 16.675 206.088 18.7387 206.088 21.062C206.088 23.5493 206.608 25.6677 207.646 27.417C208.685 29.139 210.106 30 211.91 30C213.96 30 215.409 28.8793 216.256 26.638ZM223.226 29.836L217.158 30.533H217.035L216.748 28.36C215.354 29.8633 213.55 30.615 211.336 30.615C208.603 30.615 206.389 29.754 204.694 28.032C203.027 26.31 202.193 24.0277 202.193 21.185C202.193 18.3423 203.109 16.0327 204.94 14.256C206.772 12.4793 208.945 11.591 211.459 11.591C213.755 11.591 215.532 12.3153 216.789 13.764V5.605C216.789 3.91033 216.53 2.80333 216.01 2.284C215.491 1.73733 214.63 1.43667 213.427 1.382V0.972L220.028 0.603H220.192C220.138 3.33633 220.11 5.523 220.11 7.163V24.998C220.11 25.654 220.11 26.1323 220.11 26.433C220.138 26.7063 220.192 27.0753 220.274 27.54C220.384 28.0047 220.548 28.3463 220.766 28.565C220.985 28.7837 221.299 28.9887 221.709 29.18C222.119 29.344 222.625 29.4397 223.226 29.467V29.836ZM261.009 21.226L261.665 21.349C261.61 22.3057 261.392 23.2897 261.009 24.301C260.626 25.285 260.039 26.2827 259.246 27.294C258.481 28.278 257.374 29.0843 255.925 29.713C254.476 30.3417 252.795 30.656 250.882 30.656C246.071 30.656 242.354 29.344 239.73 26.72C237.133 24.096 235.835 20.6793 235.835 16.47C235.835 11.6047 237.243 7.76433 240.058 4.949C242.901 2.13367 246.591 0.725998 251.128 0.725998C252.413 0.725998 253.629 0.848998 254.777 1.095C255.925 1.341 256.841 1.587 257.524 1.833C258.207 2.079 258.631 2.202 258.795 2.202C259.369 2.202 259.834 1.81933 260.189 1.054H260.599L260.968 10.525H260.517C259.451 7.327 258.098 5.00367 256.458 3.555C254.818 2.079 253.041 1.341 251.128 1.341C248.395 1.341 246.03 2.68033 244.035 5.359C242.04 8.03767 241.042 11.468 241.042 15.65C241.042 19.75 241.971 23.0437 243.83 25.531C245.716 28.0183 248.299 29.262 251.579 29.262C254.394 29.262 256.567 28.5377 258.098 27.089C259.656 25.6403 260.626 23.686 261.009 21.226ZM272.36 30H262.93V29.59C264.105 29.4533 264.898 29.1663 265.308 28.729C265.745 28.2643 265.964 27.2393 265.964 25.654V17.577C265.964 14.297 265.158 12.657 263.545 12.657H262.766V12.247L268.629 11.345H268.752L269.285 15.117C270.515 12.7663 272.223 11.591 274.41 11.591C275.449 11.591 276.214 11.8233 276.706 12.288C277.198 12.7253 277.444 13.1763 277.444 13.641C277.444 14.1057 277.307 14.502 277.034 14.83C276.788 15.158 276.433 15.322 275.968 15.322C275.722 15.322 275.517 15.2947 275.353 15.24C275.189 15.1853 275.011 15.076 274.82 14.912C274.656 14.748 274.547 14.6387 274.492 14.584C274.465 14.5293 274.355 14.3653 274.164 14.092C273.59 13.2993 273.043 12.903 272.524 12.903C271.868 12.903 271.28 13.2173 270.761 13.846C270.242 14.4473 269.859 15.2127 269.613 16.142C269.34 17.372 269.203 19.176 269.203 21.554C269.203 22.9753 269.23 24.6153 269.285 26.474C269.312 27.6493 269.599 28.4557 270.146 28.893C270.72 29.303 271.458 29.5353 272.36 29.59V30ZM284.011 4.252C283.655 4.60733 283.218 4.785 282.699 4.785C282.179 4.785 281.728 4.59367 281.346 4.211C280.963 3.82833 280.772 3.37733 280.772 2.858C280.772 2.33866 280.963 1.90133 281.346 1.546C281.728 1.16333 282.179 0.972 282.699 0.972C283.218 0.972 283.655 1.16333 284.011 1.546C284.366 1.90133 284.544 2.33866 284.544 2.858C284.544 3.40467 284.366 3.86933 284.011 4.252ZM287.25 30H277.738V29.59C279.104 29.3987 279.993 29.0433 280.403 28.524C280.813 27.9773 281.018 26.72 281.018 24.752V17.29C281.018 17.208 281.018 16.9757 281.018 16.593C281.018 16.2103 281.018 15.9643 281.018 15.855C281.018 15.7183 281.004 15.486 280.977 15.158C280.977 14.83 280.949 14.5977 280.895 14.461C280.84 14.3243 280.772 14.133 280.69 13.887C280.608 13.641 280.485 13.4633 280.321 13.354C280.184 13.2447 280.02 13.1217 279.829 12.985C279.637 12.8483 279.405 12.7527 279.132 12.698C278.858 12.6433 278.558 12.616 278.23 12.616V12.206L284.216 11.345H284.339C284.311 16.4837 284.298 19.8867 284.298 21.554C284.298 23.9867 284.311 25.6677 284.339 26.597C284.339 27.2257 284.407 27.745 284.544 28.155C284.708 28.5377 284.954 28.8247 285.282 29.016C285.637 29.2073 285.924 29.3303 286.143 29.385C286.361 29.4397 286.73 29.508 287.25 29.59V30ZM298.02 29.59V30H288.713V29.59C289.888 29.4533 290.695 29.098 291.132 28.524C291.597 27.95 291.829 26.6927 291.829 24.752V16.388C291.829 13.9007 290.831 12.657 288.836 12.657V12.247L294.576 11.468H294.699L295.109 14.379C296.448 12.493 298.403 11.55 300.972 11.55C303.979 11.55 305.865 12.5203 306.63 14.461C307.942 12.5203 309.965 11.55 312.698 11.55C314.393 11.55 315.814 11.9327 316.962 12.698C318.137 13.436 318.725 14.5703 318.725 16.101V26.023C318.725 26.351 318.739 26.6653 318.766 26.966C318.821 27.2393 318.862 27.4853 318.889 27.704C318.944 27.8953 319.026 28.0867 319.135 28.278C319.244 28.442 319.326 28.5787 319.381 28.688C319.436 28.7973 319.545 28.9067 319.709 29.016C319.873 29.098 319.982 29.1663 320.037 29.221C320.119 29.2757 320.269 29.3303 320.488 29.385C320.707 29.4397 320.843 29.467 320.898 29.467C320.953 29.467 321.089 29.4943 321.308 29.549C321.554 29.5763 321.691 29.59 321.718 29.59V30H312.247V29.549C312.766 29.4943 313.149 29.4397 313.395 29.385C313.641 29.3303 313.942 29.1937 314.297 28.975C314.68 28.7563 314.953 28.401 315.117 27.909C315.281 27.3897 315.363 26.7337 315.363 25.941C315.363 21.923 315.349 19.0667 315.322 17.372C315.267 14.0373 313.969 12.37 311.427 12.37C310.443 12.37 309.555 12.6707 308.762 13.272C307.997 13.846 307.491 14.7753 307.245 16.06C307.026 17.1807 306.917 18.8617 306.917 21.103C306.917 23.3443 306.931 24.9843 306.958 26.023C306.958 26.351 306.972 26.6517 306.999 26.925C307.054 27.1983 307.095 27.4443 307.122 27.663C307.177 27.8543 307.259 28.0457 307.368 28.237C307.477 28.401 307.546 28.5377 307.573 28.647C307.628 28.7563 307.737 28.8657 307.901 28.975C308.092 29.057 308.202 29.1253 308.229 29.18C308.284 29.2347 308.42 29.2893 308.639 29.344C308.858 29.3987 308.981 29.4397 309.008 29.467C309.035 29.467 309.172 29.4807 309.418 29.508L309.828 29.59V30H300.439V29.549C300.931 29.4943 301.3 29.4397 301.546 29.385C301.792 29.3303 302.093 29.1937 302.448 28.975C302.803 28.7563 303.063 28.401 303.227 27.909C303.418 27.3897 303.514 26.7337 303.514 25.941C303.541 24.6837 303.555 23.2077 303.555 21.513V17.372C303.555 14.0373 302.27 12.37 299.701 12.37C297.405 12.37 295.984 13.641 295.437 16.183C295.191 17.249 295.068 19.0393 295.068 21.554C295.068 23.03 295.095 24.6973 295.15 26.556C295.15 27.3213 295.3 27.9363 295.601 28.401C295.902 28.8383 296.216 29.1253 296.544 29.262C296.899 29.3713 297.391 29.4807 298.02 29.59ZM323.928 30.615H323.641L323.272 24.26L323.682 24.219C324.01 25.7223 324.652 27.0753 325.609 28.278C326.593 29.4807 327.782 30.082 329.176 30.082C330.16 30.082 331.021 29.8087 331.759 29.262C332.497 28.7153 332.866 27.9363 332.866 26.925C332.866 26.0503 332.538 25.244 331.882 24.506C331.226 23.7407 330.091 22.9617 328.479 22.169C325.472 20.693 323.969 18.807 323.969 16.511C323.969 15.199 324.515 14.051 325.609 13.067C326.729 12.0557 328.096 11.55 329.709 11.55C330.255 11.55 330.952 11.673 331.8 11.919C332.674 12.165 333.166 12.288 333.276 12.288C333.604 12.288 333.863 12.083 334.055 11.673H334.424L334.793 16.962H334.383C333.207 13.7367 331.636 12.124 329.668 12.124C328.738 12.124 327.973 12.4247 327.372 13.026C326.798 13.6 326.511 14.2013 326.511 14.83C326.511 15.7867 326.811 16.6067 327.413 17.29C328.014 17.946 329.107 18.6293 330.693 19.34C332.497 20.16 333.809 21.0347 334.629 21.964C335.476 22.866 335.9 24.0003 335.9 25.367C335.9 26.925 335.312 28.196 334.137 29.18C332.961 30.164 331.335 30.656 329.258 30.656C328.41 30.656 327.508 30.492 326.552 30.164C325.595 29.836 325.076 29.672 324.994 29.672C324.611 29.672 324.256 29.9863 323.928 30.615ZM357.728 21.103C357.728 23.809 356.689 26.0777 354.612 27.909C352.535 29.713 350.266 30.615 347.806 30.615C345.045 30.615 342.695 29.672 340.754 27.786C338.813 25.8727 337.843 23.6313 337.843 21.062C337.843 18.602 338.8 16.4153 340.713 14.502C342.626 12.5613 344.991 11.591 347.806 11.591C350.758 11.591 353.15 12.534 354.981 14.42C356.812 16.306 357.728 18.5337 357.728 21.103ZM347.806 30C349.829 30 351.332 29.1253 352.316 27.376C353.327 25.6267 353.833 23.5357 353.833 21.103C353.833 18.5883 353.259 16.47 352.111 14.748C350.963 13.026 349.528 12.165 347.806 12.165C345.783 12.165 344.266 13.0123 343.255 14.707C342.244 16.4017 341.738 18.52 341.738 21.062C341.738 23.5493 342.312 25.6677 343.46 27.417C344.608 29.139 346.057 30 347.806 30ZM368.771 29.59V30H359.382V29.59C360.557 29.4533 361.364 29.098 361.801 28.524C362.238 27.95 362.457 26.6927 362.457 24.752C362.457 24.0687 362.457 22.7293 362.457 20.734C362.484 18.7113 362.498 17.2627 362.498 16.388C362.498 13.9007 361.5 12.657 359.505 12.657V12.247L365.245 11.345H365.368L365.819 14.707C366.366 13.7503 367.172 12.985 368.238 12.411C369.304 11.837 370.534 11.55 371.928 11.55C373.623 11.55 375.044 11.9053 376.192 12.616C377.34 13.3267 377.914 14.42 377.914 15.896V26.023C377.914 26.4057 377.928 26.7473 377.955 27.048C377.982 27.3487 378.037 27.6083 378.119 27.827C378.228 28.0457 378.31 28.237 378.365 28.401C378.447 28.565 378.57 28.7153 378.734 28.852C378.898 28.9613 379.021 29.057 379.103 29.139C379.185 29.221 379.335 29.2893 379.554 29.344C379.773 29.3987 379.909 29.4397 379.964 29.467C380.046 29.4943 380.21 29.5353 380.456 29.59C380.729 29.6173 380.893 29.631 380.948 29.631V30H371.354V29.549C371.873 29.4943 372.256 29.4397 372.502 29.385C372.748 29.3303 373.062 29.2073 373.445 29.016C373.855 28.7973 374.142 28.4283 374.306 27.909C374.497 27.3897 374.593 26.7337 374.593 25.941C374.593 22.6337 374.579 19.75 374.552 17.29C374.525 15.65 374.21 14.42 373.609 13.6C373.008 12.78 372.024 12.37 370.657 12.37C369.646 12.37 368.716 12.698 367.869 13.354C367.022 13.9827 366.448 14.953 366.147 16.265C365.874 17.4677 365.737 19.2307 365.737 21.554C365.737 23.03 365.764 24.6973 365.819 26.556C365.819 27.3213 365.969 27.9363 366.27 28.401C366.598 28.8383 366.926 29.1253 367.254 29.262C367.609 29.3713 368.115 29.4807 368.771 29.59Z\" fill=\"black\"></path></svg></a></div><style data-emotion-css=\"es4rml\">.css-es4rml{top:0px;}</style><div class=\"sidebar-menu  css-es4rml\"><form action=\"/search/\" id=\"cse-search-box\"><input type=\"hidden\" name=\"cx\" value=\"013815813102981840311:aw6l9tjs1a0\"/><input type=\"hidden\" name=\"cof\" value=\"FORID:10\"/><input type=\"hidden\" name=\"ie\" value=\"UTF-8\"/><input type=\"text\" class=\"sidebar-search-box\" name=\"q\"/><input type=\"image\" src=\"/static/media/magnifying_glass.b9bc1a3e.svg\" alt=\"search\"/></form><div class=\"sidebar-list\"><p class=\"sidebar-menu-title\">Sections</p><ul><li class=\"links-top\"><a href=\"/section/news/\" to=\"/section/news/\">News</a></li><li class=\"links-top\"><a href=\"/section/opinion/\" to=\"/section/opinion/\">Opinion</a></li><li class=\"links-top\"><a href=\"/section/arts/\" to=\"/section/arts/\">Arts</a></li><li class=\"links-top\"><a href=\"/flyby/\" to=\"/flyby/\">Blog</a></li><li class=\"links-top\"><a href=\"/section/fm/\" to=\"/section/fm/\">Magazine</a></li><li class=\"links-top\"><a href=\"/section/metro/\" to=\"/section/metro/\">Metro</a></li><li class=\"links-top\"><a href=\"/section/media/\" to=\"/section/media/\">Multimedia</a></li><li class=\"links-top\"><a href=\"/section/sports/\" to=\"/section/sports/\">Sports</a></li><li class=\"links-top\"><a href=\"/subscribe/\" to=\"/subscribe/\">Newsletter</a></li><li class=\"links-top\"><a href=\"/section/features/\" to=\"/section/features/\">Editor&#x27;s Pick</a></li><li class=\"links-top\"><a href=\"mailto:managingeditor@thecrimson.com\" to=\"mailto:managingeditor@thecrimson.com\">Tips</a></li><li class=\"links-top\"><a href=\"/150th-campaign/\" to=\"/150th-campaign/\">Donate </a></li><li class=\"links-top\"><a href=\"/store\" to=\"/store\"> Store </a></li></ul><div class=\"social-icons\"><a href=\"https://twitter.com/thecrimson\" to=\"https://twitter.com/thecrimson\"><img src=\"data:image/png;base64,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\"/></a><a href=\"https://www.facebook.com/TheHarvardCrimson/\" to=\"https://www.facebook.com/TheHarvardCrimson/\"><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAYAAADimHc4AAAACXBIWXMAABYlAAAWJQFJUiTwAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAMhSURBVHgB7d29UhNRGMbx5z2CQ+fegXgFpBTGIlyBlGCjVEwqzRUoV0BohE5oDFbmDowzoJZ4BcQ7SJnC7OtZgh8zjjMmJ+TZbJ5fAZnMJgz7349wzoYYbjzcPqrfMXsJ8xrcMsj0mffdrZu7N7+cNXrXdxVf1p8cPw+OFmRmcljzc3uvZXHLX41b/hVk5sx9MwQLBxAKj4f8YJbXISRWCzrhMnkWIFQKQKYAZApApgBkCkCmAGQKQLYEgQO9+O3SHH2Hf/t7AcvMcG90E1lcLoNZFpfN4mjmKhIscoCuu5/eHax0up3dPhJs7BxfTRpisQLE8XjLcbg0WGmlrvRpWaQA3WGO3Z8TIWWxEAHMfP/8beMVSqjyAeJxfvei3ThBSVX6ZWix5X86K+/KL1R3D3CcnLfLedj5U1X3gN4Qvo85UMkA8dBzWrZXO/9SwQDW/57jBHOiegHcO/Oy9RcqF8Dgp5gjlQtwftboYo5U7WVoF1OwvvN6yxCexpu1eELPbvPSnUoFiMPKX5FgdJlmeB+fqfb7SQ23qWIBrIcEwexDfJZVzFClzgHm+cRDzMXl+amTK5OoVIAwmtma7LFmWyDQnPAvdh8ECnDD3CkXKSsAmQKQKQCZApApAJkCkCkAmQKQKQBZiUdD7dI9PxznEcOEsaA4k7afO8afTTN7HAfxJh5HKnEA78/yoqpJZ9I2do5rSKBDUCJzrCGBAqSytOlKBUhl+SoSKECC+rOD5Al7BUgwGKwknYALCpBg2dIncRQgwdBNewCVmfYAptS/AQoKkEJ7AJvrHMBSXEeKKSjvYJwhG/eXTHljxrg/aznYWhw9TVbeAI7auP9Q9tH20eako5p3YG9i9Pr/Lj+NlV/QIYhMAcgUgEwByBSATAHIFIBMAcgUgEwByBSATAHIFIBMAcgUgEwByBSATAHIFIBMAcgUgEwByBSATAHIFIBMAcgUgEwByBSATAHIFIAsFB/vByGxfkBulxAK9/xjsDn5uKcqyoEX4fotPbk1ITNmzeI9bdcn4Yt3e62h+wMHOjon3KLRuu2a++ZFe69V3PUDMGbYPs/yVIAAAAAASUVORK5CYII=\"/></a><a href=\"https://www.instagram.com/theharvardcrimson/\" to=\"https://www.instagram.com/theharvardcrimson/\"><img src=\"/static/media/instagram.a983f1c5.png\"/></a></div></div></div></div><style data-emotion-css=\"1n8f8sc\">.css-1n8f8sc{height:45px;display:none;}</style><div class=\"css-1n8f8sc\"></div></div><style data-emotion-css=\"1sdhhn1\">.css-1sdhhn1{background-image:url(https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/143735_1362611.jpeg.2000x1294_q95_crop-smart_upscale.jpg);background-repeat:no-repeat;background-size:cover;background-position:50% center,50% center;background-attachment:fixed;height:100vh;width:100%;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.css-1sdhhn1 .quoteWrapper{max-width:500px;}.css-1sdhhn1 .quoteWrapper h1{text-shadow:1px 1px 8px black;font-size:3.5em;color:white;text-align:left;line-height:120%;}.css-1sdhhn1 .quote{font-family:'Crimson',Geogria,serif;text-align:left;}.css-1sdhhn1 .quote span{text-shadow:1px 1px 8px black;font-size:1.2em;margin:0;color:white;font-style:normal;}</style><div class=\"css-1sdhhn1\"><div class=\"quoteWrapper\"><style data-emotion-css=\"1rfyg0l\">.css-1rfyg0l{max-width:600px;margin:0px auto;padding:0px 20px;box-sizing:content-box;font-size:3.5em;line-height:1;color:#7b1b18;margin-bottom:20px;text-align:center;}@media (max-width:768px){.css-1rfyg0l{font-size:2.2em;text-align:left;}}</style><h1 class=\"css-1rfyg0l\">What is Going On With Effective Altruism?</h1><div class=\"quote\"><style data-emotion-css=\"19d27pp\">.css-19d27pp{max-width:600px;margin:0px auto;padding:0px 20px;box-sizing:content-box;display:block;font-size:1.1em;line-height:1.5em;color:#555;font-style:italic;}</style><span class=\"css-19d27pp\">“Most of us want to improve the world. We see suffering, injustice, and death and feel moved to do something about it,” the Harvard EA website says. “But figuring out what that ‘something’ is, let alone actually doing it, can be a difficult and disheartening challenge. Effective altruism is a response to this challenge.” Can it live up to that goal?</span></div></div></div><style data-emotion-css=\"y7m26u\">.css-y7m26u{margin:2px 20px;font-family:'Colaborate Light','Lucida Sans Unicode',sans-serif;color:#777;font-size:0.9em;text-align:center;}</style><div class=\"css-y7m26u\"><span>“Most of us want to improve the world. We see suffering, injustice, and death and feel moved to do something about it,” the Harvard EA website says. “But figuring out what that ‘something’ is, let alone actually doing it, can be a difficult and disheartening challenge. Effective altruism is a response to this challenge.” Can it live up to that goal?</span><style data-emotion-css=\"1ys3e0l\">.css-1ys3e0l img{padding-right:7px;}</style><span class=\"css-1ys3e0l\">By <style data-emotion-css=\"1fvj8jv\">.css-1fvj8jv{color:#ba0600;}</style><a href=\"/writer/1217921/Pema_Choedon_/\" to=\"/writer/1217921/Pema_Choedon_/\" class=\"css-1fvj8jv\">Pema Choedon</a></span></div><style data-emotion-css=\"1w8oes6\">.css-1w8oes6{max-width:600px;margin:0px auto;padding:0px 20px;box-sizing:content-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;margin-top:25px;margin-bottom:8px;}</style><div class=\"css-1w8oes6\"><style data-emotion-css=\"1irq2cm\">.css-1irq2cm{font-family:'Open Sans';color:#777;text-transform:uppercase;font-size:12px;}</style><style data-emotion-css=\"x0hxbi\">.css-x0hxbi{font-family:'Open Sans';color:#777;text-transform:uppercase;font-size:12px;}.css-x0hxbi img{padding-right:7px;}</style><span class=\"css-x0hxbi\">By <style data-emotion-css=\"1fvj8jv\">.css-1fvj8jv{color:#ba0600;}</style><a href=\"/writer/1217270/Sophia_C._Scott/\" to=\"/writer/1217270/Sophia_C._Scott/\" class=\"css-1fvj8jv\">Sophia C. Scott</a> and <a href=\"/writer/1217332/Bea__Wall-Feng/\" to=\"/writer/1217332/Bea__Wall-Feng/\" class=\"css-1fvj8jv\">Bea Wall-Feng</a></span></div><style data-emotion-css=\"1e9ktqy\">.css-1e9ktqy{max-width:600px;margin:0px auto;padding:0px 20px;box-sizing:content-box;border:0.5px solid #ccc;}</style><hr class=\"css-1e9ktqy\"/><style data-emotion-css=\"ujgn17\">.css-ujgn17{position:relative;width:100%;}.css-ujgn17 p{max-width:600px;margin:0px auto;padding:0px 20px;box-sizing:content-box;font:17px/25px Georgia,serif;margin-top:1em;margin-bottom:1em;}.css-ujgn17 ol{max-width:600px;margin:0px auto;padding:0px 20px;box-sizing:content-box;font:17px/25px Georgia,serif;padding-left:80px;}.css-ujgn17 p a,.css-ujgn17 ol a,.css-ujgn17 p a:visited,.css-ujgn17 ol a:visited{color:#ba0600;-webkit-transition:0.3s all;transition:0.3s all;-webkit-text-decoration:none;text-decoration:none;}.css-ujgn17 p a:hover,.css-ujgn17 ol a:hover{-webkit-text-decoration:underline;text-decoration:underline;}.css-ujgn17 h1,.css-ujgn17 h2,.css-ujgn17 h3,.css-ujgn17 h4,.css-ujgn17 h5,.css-ujgn17 h6{max-width:600px;margin:0px auto;padding:0px 20px;box-sizing:content-box;margin-top:1em;margin-bottom:1em;color:#7b1b18;}</style><div class=\"css-ujgn17\"><style data-emotion-css=\"sgicso\">.css-sgicso{height:100%;width:40px;position:absolute;right:calc((100% - 640px) / 2 + 640px);z-index:-1;}@media (max-width:950px){.css-sgicso{display:none;}}</style><div class=\"css-sgicso\"><style data-emotion-css=\"1kt3bzc\">.css-1kt3bzc{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:auto;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:-webkit-sticky;position:sticky;top:70px;}.css-1kt3bzc .icons{display:inline-block;padding:0px;}.css-1kt3bzc .icons > span{width:35px !important;height:35px !important;text-align:center;border-radius:50%;}.css-1kt3bzc .icons svg{width:25px !important;height:35px !important;}.css-1kt3bzc .addthis_button_email > span,.css-1kt3bzc .addthis_button_print > span,.css-1kt3bzc .addthis_button_link > span{background-color:#ebebeb !important;}.css-1kt3bzc .addthis_counter .addthis_button_expanded{width:35px;height:35px;margin:0px;padding:0px;background:none;text-align:center;border:1px solid #ebebeb;border-radius:50%;color:#ebebeb !important;}.css-1kt3bzc .addthis_button_compact{display:none !important;}</style><div class=\"addthis_toolbox addthis_default_style addthis_32x32_style social-icons css-1kt3bzc\"><a class=\"icons addthis_button_facebook\"></a><a class=\"icons addthis_button_twitter\"></a><a class=\"icons gray addthis_button_email\"></a><a class=\"icons gray addthis_button_print\"></a><a class=\"icons gray addthis_button_link\"></a><a class=\"icons gray addthis_button_compact\"></a><a class=\"icons addthis_counter\"></a></div></div><style data-emotion-css=\"8sr032\">.css-8sr032{display:none;max-width:600px;margin:0px auto;padding:0px 20px;box-sizing:content-box;padding-top:20px;padding-bottom:20px;margin-top:20px;border-top:1px solid #ebebeb;border-bottom:1px solid #ebebeb;}@media (max-width:950px){.css-8sr032{display:block;}}</style><div class=\"css-8sr032\"><style data-emotion-css=\"u1fend\">.css-u1fend{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;width:auto;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:-webkit-sticky;position:sticky;top:70px;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:unset;}.css-u1fend .icons{display:inline-block;padding:0px;}.css-u1fend .icons > span{width:35px !important;height:35px !important;text-align:center;border-radius:50%;}.css-u1fend .icons svg{width:25px !important;height:35px !important;}.css-u1fend .addthis_button_email > span,.css-u1fend .addthis_button_print > span,.css-u1fend .addthis_button_link > span{background-color:#ebebeb !important;}.css-u1fend .addthis_counter .addthis_button_expanded{width:35px;height:35px;margin:0px;padding:0px;background:none;text-align:center;border:1px solid #ebebeb;border-radius:50%;color:#ebebeb !important;}.css-u1fend .addthis_button_compact{display:none !important;}.css-u1fend .icons{margin:0px 5px;}</style><div class=\"addthis_toolbox addthis_default_style addthis_32x32_style social-icons css-u1fend\"><a class=\"icons addthis_button_facebook\"></a><a class=\"icons addthis_button_twitter\"></a><a class=\"icons gray addthis_button_email\"></a><a class=\"icons gray addthis_button_print\"></a><a class=\"icons gray addthis_button_link\"></a><a class=\"icons gray addthis_button_compact\"></a><a class=\"icons addthis_counter\"></a></div></div><div class=\"css-ujgn17\"><p><style data-emotion-css=\"bp9mjk\">.css-bp9mjk{font-size:3em;color:#ba0600;float:left;line-height:1.1;height:0.7em;padding-right:0.1em;color:AF2234;}</style><span class=\"css-bp9mjk\">W</span><span>hen we meet Nikola Jurkovic ’25 on Zoom, he’s sitting in front of a whiteboard covered with equations. We chat about his interest in folk punk music; his headphones push his hair back into a kind of emo swoop. Jurkovic comes across as friendly, but also guarded: He seems to want to make a good impression.</span></p><p><span>Jurkovic tells us he first learned about the philosophical movement called effective altruism from the comments section of a YouTube video about veganism during his gap year. When he moved from Croatia to Harvard, he joined the Harvard Undergraduate EA group and eventually became its president. “I think I’ve been looking for ways to make the world better for a really long time,” he says, “I think as far back as I can remember.”</span></p><p><span>EA, its proponents will tell you, is aimed at “doing good better.” It starts with the problem that the ways that we approach charity, aid, and other kinds of good-doing are clouded by human biases, and tries to find the best solution using statistical tools. For example, an effective altruist might argue that donating money to foundations that provide malaria treatments maximizes your lives saved per dollar spent.</span></p><p><span>The movement began in the early 2010s as the brainchild of the philosopher Will MacAskill. In the years since, it has flourished in both in-person spaces, like conferences, as well as blogs and forums. Users map out their worldviews in long, technical posts, debating what exactly different EA principles look like in practice, the merits of various criticisms of the movement, how to adapt to the more meritorious criticisms, and how to counter the less meritorious ones.</span></p><p><span>EA has also garnered support from a number of high-profile acolytes.</span></p><p><span>“This is a close match for my philosophy,” Elon Musk <a href=\"https://twitter.com/elonmusk/status/1554335028313718784\">tweeted</a> last August about MacAskill’s new book.</span></p><p><span>“Effective altruism — efforts that actually help people rather than making you feel good or helping you show off — is one of the great new ideas of the 21st century,” celebrity academic and Harvard psychology professor Steven Pinker wrote.</span></p><p><span>One of the highest-profile effective altruists was the former billionaire Sam Bankman-Fried, whose commitment to donate much of his wealth to EA causes turned out to be impossible after his cryptocurrency company, FTX, collapsed due to alleged fraud. His commitment also turned out to be disingenuous: In a postmortem interview with Vox, he described his apparent embrace of ethics as “this dumb game we woke westerners play where we say all the right shibboleths and so everyone likes us.”</span></p><p><span>Nevertheless, EA’s profile has continued to rise. Even at Harvard, some students find the pull inescapable: “Effective altruism is a <em>huge</em> trend on campus, seeping into everything,” Henry Haimo ’24 told the New Yorker in March, in an <a href=\"https://www.newyorker.com/magazine/2023/03/06/the-end-of-the-english-major\">article</a> ostensibly about the decline of the humanities.</span></p><style data-emotion-css=\"12m1k7q\">.css-12m1k7q{display:block;margin:0px auto;width:100%;max-width:100%;background:transparent;padding:10px 0px;}.css-12m1k7q .ad-tag{font-family:'Colaborate Light','Lucida Sans Unicode',sans-serif;color:#333333;text-align:center;font-size:0.7em;text-transform:uppercase;display:block;}.css-12m1k7q .ad-body{width:-webkit-fit-content;width:-moz-fit-content;width:fit-content;margin:0px auto;text-align:center;}</style><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_AllArticles_300x250_Add00\"></div></div><p><span>When Harvard’s undergraduate EA group started on campus a decade ago, though, EA was not so widespread. At first, getting students to join was a challenge. “I realized that the idea of HEA seemed crazy: ‘join us and we’ll try to figure out how to maximize the good we do in the world!’” Ben S. Kuhn ’15, the co-president at the time, wrote in a 2014 blog post. “But my mistake was <em>letting other people know</em> that I knew HEA seemed crazy. As soon as they realized that I myself felt goofy, it was game over for convincing them to get involved.” He described expanding the group’s influence through a combination of savvy advertising tactics and a slate of famous speakers such as the philosopher Peter Singer, one of the most prominent intellectuals associated with the movement.</span></p><style data-emotion-css=\"4kxa28\">.css-4kxa28{display:block;margin:0px auto;max-width:640px;z-index:20;position:relative;}.css-4kxa28 > div{box-sizing:content-box;padding:0px 20px;margin:1em auto;}.css-4kxa28 > div.shortcode-left{float:left;}.css-4kxa28 > div.shortcode-right{float:right;}.css-4kxa28 > div.shortcode-small{max-width:200px;}.css-4kxa28 > div.shortcode-medium{max-width:400px;}.css-4kxa28 > div.shortcode-large{max-width:600px;}.css-4kxa28 > div.shortcode-xlarge{max-width:1000px;}.css-4kxa28 > div.shortcode-fullscreen{padding:0px;}</style><div class=\"css-4kxa28\"><div class=\"shortcode-large shortcode-center\"><style data-emotion-css=\"8atqhb\">.css-8atqhb{width:100%;}</style><img src=\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/214432_1362617.JPG.1500x1000_q95_crop-smart_upscale.jpg\" alt=\"Nikola Jurkovic ’25 is the president of Harvard Undergraduate Effective Altruism.\" class=\"css-8atqhb\"/><style data-emotion-css=\"8pot\">.css-8pot{max-width:1000px;margin:5px 0px auto;font-family:'Colaborate Light','Lucida Sans Unicode',sans-serif;color:#777;font-size:15px;}</style><div class=\"css-8pot\"><span>Nikola Jurkovic ’25 is the president of Harvard Undergraduate Effective Altruism.<!-- --> </span><style data-emotion-css=\"1ys3e0l\">.css-1ys3e0l img{padding-right:7px;}</style><span class=\"css-1ys3e0l\">By <style data-emotion-css=\"1fvj8jv\">.css-1fvj8jv{color:#ba0600;}</style><a href=\"/writer/1217257/Marina__Qu/\" to=\"/writer/1217257/Marina__Qu/\" class=\"css-1fvj8jv\">Marina Qu</a></span></div></div></div><p><span>Today, there are several different EA groups across the University, including at the College, law school, and GSAS, as well as in the Boston area. (In this article, “Harvard EA” refers to the undergraduate group unless otherwise specified.) Jurkovic guesses there are “between 20 and 50” students involved in Harvard EA’s regular programming; there are dozens more in its introductory fellowships and hundreds more on the mailing lists.</span></p><p><span>Harvard markets itself as “developing leaders who make a difference globally,” and pop culture spins this principle into myth: Here is a university whose students will go on to save the world. EA is a movement that claims to know how to do it, from shifts in diet — a lot of the people we meet are vegan, in recognition of the statistically unmatched suffering that factory farming inflicts on chickens — to shifts in career, like the students who have devoted their futures to trying to prevent human extinction by AI.</span></p><p><span>“Most of us want to improve the world,” the Harvard EA website says. “But figuring out what that ‘something’ is, let alone actually doing it, can be a difficult and disheartening challenge.<a href=\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism/\"> Effective altruism</a> is a response to this challenge.” Can it live up to that goal?</span></p><h1><span>Counterfactual Impact</span></h1><p><span class=\"css-bp9mjk\">I</span><span>f you are not already a committed effective altruist — and if you are interested in a discussion group that zigzags from deworming methodology to Bayesian statistics to the number of animals killed for food in the U.S. in the last 30 seconds — your first involvement with EA at Harvard might come through the introductory Arete Fellowship, a seven-week program that explores different effective altruism topics every week.</span></p><p><span>Daniela R. Shuman ’24, a Computer Science and Statistics concentrator interested in urban development, is one of the Arete chairs. She did the fellowship herself a year ago and “fell in love with the whole concept,” she says. She guesses there were around 40 fellows last fall.</span></p><p><span>We ask Will A. Nickols ’24, the other chair, if we can sit in on one of the sections. After some back-and-forth, Nickols suggests we attend the second week of the fellowship, which focuses on global health.</span></p><p><span>We get there before the meeting room opens. The discussion leaders, Nick C. Gabrieli ’24 and Jorge O. Guerra Jr. ’24, a former Crimson News editor, are sitting in near-darkness on the stairs outside the Adams House Upper Common Room, chatting: “What else are you up to this semester?” “What do you see as the limits of randomized control trials?” The vaulted blue ceiling looks like the night sky.</span></p><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_AllArticles_300x250_Add01\"></div></div><p><span>Claire Guo ’24, one of the Arete fellows and a former Crimson News editor, wanders up wearing two baseball caps; she wants to know if the leaders are participating in the junior class-wide game of Assassins.</span></p><p><span>No, Gabrieli tells her, adding, “I feel like I’m in the market for developing more hobbies.”</span></p><p><span>At 6 p.m., we file into the meeting room and assign ourselves to armchairs. There are three fellows in attendance, and another will show up halfway through. Gabrieli and Guerra walk us through the vocabulary terms from the readings — two blog posts, a lecture by an Oxford Professor, a TED talk, and several charts about life expectancy in different countries — asking the group to define and react to them.</span></p><p><span>This week, the discussion involves identifying your “problem”: So you’ve already decided you want to do good. How do you figure out <em>what</em> to do?</span></p><p><span>EA takes cues from rationalism, a commitment to logic rather than feelings as a basis for decision-making. For the Arete fellows, this requires learning a lot of terminology. For example, the “counterfactual impact” of an action is the result of doing that action, relative to the result of not doing it. Say you want to volunteer in a soup kitchen — a lot of EA readings use this example — but there are plenty of other volunteers; if you don’t do it, someone else will. Unless you are extremely good at serving soup, the amount of soup served in a world where you are a server is probably not that different from the amount of soup served in a world where you are not, and you might want to focus your altruistic intentions elsewhere.</span></p><div class=\"css-4kxa28\"><div class=\"shortcode-large shortcode-center\"><img src=\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2020/03/09/004053_1343308.jpg.1500x844_q95_crop-smart_upscale.jpg\" alt=\"The Arete Fellowship discussion section takes place in the Adams House Upper Common Room. The Arete Fellows learn a lot of terminology to help them think about how they can do the most good.\" class=\"css-8atqhb\"/><div class=\"css-8pot\"><span>The Arete Fellowship discussion section takes place in the Adams House Upper Common Room. The Arete Fellows learn a lot of terminology to help them think about how they can do the most good.<!-- --> </span><span class=\"css-1ys3e0l\">By <a href=\"/writer/1215627/Zadoc_I.N._Gee/\" to=\"/writer/1215627/Zadoc_I.N._Gee/\" class=\"css-1fvj8jv\">Zadoc I.N. Gee</a></span></div></div></div><p><span>At the discussion leaders’ prompting, the fellows — Guo; Nathanael Tjandra ’26; and Kai C. Hostin ’25 — talk among themselves, trying to recall concepts from the readings. More than anything, it feels like a class section.</span></p><p><span>Gabrieli asks, “What is ‘importance?’” Guo and Hostin look at each other with uncertainty. “It should be able to be inferred, I think, from the word itself,” he continues. “What is the absolute magnitude of the thing we’re interested in?”</span></p><p><span>There are a lot of math terms, too. One of the readings constructs a complicated-looking product of derivatives to make the conceptual point that importance, tractability, and neglectedness are all vital considerations. Importance is how important a problem is; tractability is how much good can be done relative to effort; neglectedness is how much other people are already working in that area.</span></p><p><span>Still, the fellows have questions, and Gabrieli and Guerra are happy to discuss them. Should your personal interests play a role in what you decide to focus on? What about problems that require immediate responses, rather than careful calculation of impact? They talk but don’t come to any singular answer.</span></p><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_AllArticles_300x250_Add02\"></div></div><p><span>Near the end of the hour, Guerra brings up GiveWell, a well-known EA organization that maintains a ranked list of recommended charities. He asks the group: Of the top four causes on the website, which one would you donate to, and why?</span></p><p><span>First on the list is Malaria Consortium, which provides a kind of medicine called seasonal malaria chemoprevention. Malaria treatments are high-importance, and tractable because they are relatively cheap; GiveWell estimates that it costs $7 to protect a child from malaria. Second on the list is the Against Malaria Foundation, which provides malaria nets for about $5 each.</span></p><p><span>Tjandra, a Crimson Multimedia editor, ultimately chooses the fourth-ranked charity, which provides cash incentives to caregivers in Nigeria who vaccinate their babies. Unlike malaria treatments, he reasons, vaccines are “more generalizable to poor people everywhere.”</span></p><p><span>Guo asks if instead of providing Vitamin A supplements to areas in sub-Saharan Africa, as the third-ranked charity does, we could try to integrate vegetables rich in Vitamin A into those communities. “‘Eat some yams,’” she says, laughing.</span></p><p><span>Hostin concurs: “I don’t want to be like ‘Oh, here’s a fish’ rather than teach them how to fish, in a way.”</span></p><h1><span>A One in Ten Risk of Extinction</span></h1><p><span class=\"css-bp9mjk\">A</span><span>t the end of the Arete fellowship, you are given a $10 gift card to the charity donation site Every.org. If you donate $10 of your own money, you receive another gift card. But what happens next is up to you: Since Harvard EA has no centralized meetings or agenda, continued membership means “joining the various side groups that we have, depending on what you’re interested in,” Shuman says.</span></p><p><span>Donating large parts of your income to charity — or earning to give, in the style of Bankman-Fried — are largely out of reach for college students, Jurkovic tells us. Instead, we find out, undergrad EA groups tend to focus on research and recruitment.</span></p><p><span>Still, the issues Harvard EA focuses on don’t always line up with the picture of EA suggested by the Arete fellowship. Zazie Huml ’25, Harvard EA’s Global Health Programming Lead and one of four people on its board, said when they joined Harvard EA there was “no major initiative” in global health or international development — two of the five major topics covered in the Arete fellowship — and there were only “a couple people” involved with animal rights, a third focus area.</span></p><p><span>“In Harvard EA we try not to present unfairly biased opinions towards any particular world problem,” Jurkovic says. “We aim to present the facts about the world problems and also give people useful decision-making tools so that they can examine the facts themselves.”</span></p><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_AllArticles_300x250_Add03\"></div></div><p><span>“My entire experience with EA at Harvard last semester was, ‘Oh, this is not for me, this is not my community. They’re not interested in the same things as me,’” Huml says, until someone they met in the organization encouraged them to take another look. “If I was to take initiative within the system, there would be resources to support me,” they recall the person saying. Huml has since led several global health initiatives under the umbrella of Harvard EA, including a “comprehensive study on source apportionment of lead exposure” in lower-income countries in partnership with the Lead Exposure Elimination Project.</span></p><p><span>So what was Harvard EA doing? “It was only focused on longtermism and AI,” Huml remembers.</span></p><p><span>Longtermism — week five on the Arete syllabus — is the view that people who will be alive in the future warrant the same moral consideration as people alive today. “If we want to do the most good, that means we want to help the most people,” Shuman says, “and the most people is not at the specific time that we’re living in.” Since future people will significantly outnumber today's people, barring a mass extinction event, longtermists argue that we should devote more resources to preventing “existential risks” like nuclear warfare or engineered pandemics.</span></p><p><span>This is something of a departure from EA principles in other areas, which our interviewees explain are a refinement of natural human instincts: You already want to do good, and EA just teaches you how to be smart about it. For a longtermist, though, what is at stake is the future of human existence.</span></p><p><span>In recent years, longtermists have turned their attention to the field of AI safety. As AI models grow increasingly powerful, EA researchers have argued, the existential threat they pose may become insurmountable. They call this the problem of “AI alignment”: ensuring that if or when a superhuman AI comes into existence, its values align with our own, so that it does not kill everyone on Earth.</span></p><p><span>Some AI safety researchers have been sounding this alarm for decades — but this past year, thanks to the rise of shockingly powerful, publicly available AI models like <a href=\"https://www.thecrimson.com/article/2023/2/23/chatgpt-scrut/\">ChatGPT</a>, the issue has made it into the mainstream. ChatGPT is already prone to spreading misinformation. A “sufficiently powerful” AI could be much worse, researcher Elizier Yudkowsky argued in a recent <a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\">Time magazine op-ed</a> calling for an indefinite moratorium on AI development. “In today’s world you can email DNA strings to laboratories that will produce proteins on demand allowing an AI initially confined to the internet to build artificial life forms or bootstrap straight to postbiological molecular manufacturing,” he wrote.</span></p><p><span>How do you quantify incalculable destruction? “The Precipice: Existential Risk and the Future of Humanity,” a book by Toby Ord about existential risk, for which Harvard EA runs a reading group, puts the risk of extinction in the next century from unaligned AI at 1 in 10, higher than any other source of risk. Last July, Jurkovic wrote in a comment on the EA forum that “existential risks are high up on the list of most probable causes of death for college aged-people”: Assume that the probability of achieving superhuman AI by 2045 is 50 percent, and assume that the probability of death given superhuman AI is at least 10 percent. Then the probability of death by AI in the next few years might be comparable to around 1 in 6000, he wrote, explaining that this probability is similar to the two largest causes of death for “college-aged people in the US,” suicide and vehicle accidents, although he did not write out the calculations leading to this conclusion.</span></p><p><span>Jurkovic guesses that there are more people in Harvard EA working on AI than other problems, but points out the existence of organizations with other EA-related focuses, including Harvard College Animal Advocates.</span></p><p><span>Harvard EA doesn’t have an AI safety program itself, but there are several related organizations that do. One of these is the Harvard AI Safety Team, which was founded in 2022 by Xander L. Davies ’23 and Fiona E. Pollack ’25. Though HAIST is not an EA organization, many members of Harvard EA work with HAIST, and HAIST also receives funding from national EA philanthropy organizations. HAIST and Harvard EA also share members with other local AI groups, including Cambridge Boston Alignment Initiative and MIT AI Alignment.</span></p><p><span>HAIST hosts reading groups and talks by professors, with a focus on machine learning research, Davies tells us on Zoom. He has curly hair and a black hoodie; the “L” in his name stands for Laser. Although he doesn’t believe that AI right now is poised to destroy humanity, he also doesn’t believe we have the tools to stop it. “I think how I look at it is, it’s currently impossible to get our AIs to not do things,” Davies says, referring to the ease with which users have bypassed ChatGPT-like models’ built-in filters against violent speech and misinformation.</span></p><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_AllArticles_300x250_Add04\"></div></div><p><span>“Rapid progress in AI is becoming more and more economically useful, becoming more and more trusted, while at the same time this stark lack of progress on actually understanding how these systems work, on getting confidence that we actually know how to make these systems do what we want, is very startling to me,” Davies says. “And I think it should be a core priority on the global stage.”</span></p><h1><span>‘The Warm Fuzzy Feeling Just Doesn’t Matter as Much’</span></h1><p><span class=\"css-bp9mjk\">I</span><span>n some respects, EA seems fairly intuitive: Who doesn’t want to minimize suffering as much as possible? In other respects, it pushes you to rethink your intuitions.</span></p><p><span>Take, for instance, a thought experiment effective altruists often use to illustrate the unique way this philosophy navigates moral quandaries. It’s called “<a href=\"https://www.harvardea.org/blog/2022/lena-ashooh-the-child-in-the-pond\">the drowning child scenario</a>,” originally formulated by Peter Singer. Imagine you’re on your way to an important event, and you notice a child is drowning in a nearby pond. Do you jump in to save the child?</span></p><p><span>Barring circumstances like an inability to swim, most people answer yes. But then, you’re asked the question again and again. Each time, the stakes are higher: What if you will ruin your clothes and waterlog your phone by jumping in? What if you already saved a drowning child last week? What if this child was an undocumented immigrant? What if the pond was far enough away that you would have to spend gas money to get there?</span></p><p><span>As the hypotheticals escalate, generally, participants continue to decide to save the drowning child each time. But in the real world, when facing situations like determining healthcare access for immigrants or people in other countries, the increased distance leads some people to make what is in effect the opposite choice. EA wants to know: Why should you value those peoples’ lives less than those of people closer to you?</span></p><p><span>When we spoke to Marka F. X. Ellertson ’23, then the president of Harvard EA, last September, she told us that with EA efforts, “the warm fuzzy feeling just doesn’t matter as much to me as the rational thought that I know that I’ve had a bigger impact.”</span></p><p><span>“And I actually still do want that warm fuzzy feeling,” Ellertson added, explaining that she donates to local causes that are particularly meaningful to her.</span></p><p><span>Joshua D. Greene ’97, Harvard EA’s faculty advisor, disputes the idea that EA strips away the warmth of charity work.</span></p><p><span>Utilitarianism might make you think of “the things that kind of serve a function but don’t nurture our souls or to speak to our heart’s greatest desires, right? And utilitarianism is not just about cold functionality,” he says. “It’s about everything that makes life good or bad, everything that makes life worth living, everything that makes life meaningful.”</span></p><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_AllArticles_300x250_Add05\"></div></div><p><span>Harold H. Klapper ’25, who participated in a Harvard EA fellowship last year, tells us that in some EA dialogues about utilitarianism “get really wild.”</span></p><p><span>At a Boston-area EA event, for instance, “I’ve had conversations arguing about whether we should kill all wild animals, because they have negative lives,” Klapper says. “An ant colony must just have negative utility in the sense that they’re just not enjoying life, and so it’d be better if we just eliminated them.”</span></p><p><span>“When things are a movement, you kind of have to buy into the whole thing, and when you buy into the whole thing, you get really wacky and fucked up answers to problems,” Klapper adds.</span></p><p><span>Effective altruists seek to apply EA principles to personal decisions: what to study, where to work. If you are a college student interested in building EA communities, you might “consider not going to Harvard, as there are a bunch of people there doing great things,” Jurkovic wrote on the EA forum in December, suggesting that going to other colleges without strong EA movements could be better. (Was this something Jurkovic himself considered when applying to Harvard? “No,” he says, laughing.)</span></p><p><span>A lot of EA discourse revolves around career choice: You will probably work for around 80,000 hours in your lifetime — several of the people we talk to cite this estimate — and you should spend them doing things that count, even if they may not be things you enjoy.</span></p><p><span>Harvard EA, Shuman says, focuses mainly on “getting high-potential individuals into careers where they can spend their 80,000 hours of their career on solving these issues.”</span></p><div class=\"css-4kxa28\"><div class=\"shortcode-large shortcode-center\"><img src=\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/214644_1362618.JPG.1500x1000_q95_crop-smart_upscale.jpg\" alt=\"Jōsh P. Mysoré ’26 completed the Arete fellowship last fall and is considering becoming a discussion leader at some point in the future. Mysoré loves poetry, but is planning on concentrating in Computer Science and Linguistics.\" class=\"css-8atqhb\"/><div class=\"css-8pot\"><span>Jōsh P. Mysoré ’26 completed the Arete fellowship last fall and is considering becoming a discussion leader at some point in the future. Mysoré loves poetry, but is planning on concentrating in Computer Science and Linguistics.<!-- --> </span><span class=\"css-1ys3e0l\">By <a href=\"/writer/1217257/Marina__Qu/\" to=\"/writer/1217257/Marina__Qu/\" class=\"css-1fvj8jv\">Marina Qu</a></span></div></div></div><p><span>To get them to that point, EA might also influence Harvard students’ concentration decisions. One such student is Jōsh P. Mysoré ’26; when we meet him outside of Blackbird, he’s reading Giovanni Boccaccio’s “The Decameron” for class. Mysoré completed the Arete fellowship last fall and is considering becoming a discussion leader at some point in the future.</span></p><p><span>“I love poetry. I loooove poetry,” he tells us. “Will I be going into poetry? No. Because I don’t think it will actually do good for people.” At the moment, Mysoré plans to concentrate in Computer Science and Linguistics.</span></p><p><span>Computer Science, as well as Statistics and Applied Mathematics, are fairly common concentration choices among the people we meet. Klapper tells us he knows someone in Harvard EA who studies Computer Science and dislikes it, but continues in the field because they believe it’s the most effective use of their time.</span></p><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_AllArticles_300x250_Add06\"></div></div><p><span>Mysoré “was given a certain amount of privilege in my life to even get to this point,” he tells us. “I do think I owe something to the greater good of humanity to do something that impacts more people in a tangible way.”</span></p><p><span>Does he think anyone should go into poetry, we ask.</span></p><p><span>“I don't think it’s a contradiction to say that I can hold two opposing viewpoints at the same time,” Mysoré says. “Like in my heart, I’m a humanist, and I’m very romantic.” He tells us that he joined EA specifically to challenge these humanist viewpoints, but his perspective might flip again. “Honestly, I do think there should be poets,” he says.</span></p><p><span>Mysoré tells us he still believes that EA has a noble mission, even if he disagrees with some of its particular approaches. “I think at the baseline, EA is creating dialogue,” he says. “That is really what counts.”</span></p><h1><span>‘Not To Create a Club, But Rather to Create a World’</span></h1><p><span class=\"css-bp9mjk\">O</span><span>n Saturdays, Harvard EA throws socials in a house near campus where Jurkovic lives with four of his friends. Half the time, the socials are just for Harvard affiliates; every other week, they are open to students from other Boston-area schools.</span></p><p><span>Of course, the socials are designed to be fun, but they have a functional purpose as well. “One important part of having a community is that the people talk to each other and have time spent together, so that they can collaborate and talk about their projects,” Jurkovic says.</span></p><p><span>Though Jurkovic declined our request to attend a social on the record, we can try to reconstruct the vibe from a guide that he posted on the EA forum called “How to Organize a Social.” Indeed, in the post, he records every step of preparing for a social in granular detail, providing recommendations for everything from grocery lists — CLIF Bars, Diet Coke, several varieties of Vitaminwater — to music, such as the Spotify-curated playlist “my life is a movie.” Jurkovic suggests you make it easy for guests to find answers to anticipated questions: “The shoes on/off policy? Where the bathroom is? Where one can get water? What the wi-fi password is?”</span></p><p><span>Last year, Trevor J. Levin ’19, who is currently on leave as the co-president of the university-wide EA group, also created a list of recommendations for effective retreats: They should happen in the beginning of the semester, when people are less busy; include lots of time for one-on-one interactions and a “structured vulnerable/emotional thing”; and include a healthy mix of new recruits and “moderately charismatic people for whom EA is a major consideration in how they make decisions.” These suggestions were embedded in a long post, which, citing feedback from Ellertson, Davies, Jurkovic, and others, argues that college EA groups should focus more on retreats as a method of bonding.</span></p><p><span>For Levin, a former Crimson editor, this kind of immersive social situation is vital to capturing those who might be interested in EA but don’t prioritize it.</span></p><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_AllArticles_300x250_Add07\"></div></div><p><span>“While most of the important cognition that happens is social/emotional, this is not the same thing as tricking or manipulating people into being EAs,” he wrote on the forum. Instead, retreats are meant to appeal to those who may agree with EA on some level but have not yet acted on it, and giving them time to “move closer to the values they <em>previously already wanted to live by</em>.”</span></p><p><span>Since EA was born, it has been very deliberate about the image it projects. The name “effective altruism” was itself the product of a long debate: “This has been such an obstacle in the utilitarianesque community — ‘do-gooder’ is the current term, and it sucks,” MacAskill, the philosopher, wrote in a 2011 email chain. What followed was a period of brainstorming — fusing terms like “utilitarian” and philanthropist” with “alliance” and “institute” — and a series of votes to establish a name for both “the type of person we wanted to refer to, and for the name of the organization we were setting up.”</span></p><p><span>Now, countless blogs and forum posts are dedicated to determining how best to recruit new members to the EA community. In December 2021, for instance, Jurkovic wrote a post on the EA forum describing an “organic” way to pitch EA to students.</span></p><p><span>“Person: What do you want to study? Me: Not sure, I’m trying to find what to study so I can have as good of an impact as possible,” he wrote in an example dialogue. “If their level of enthusiasm stays high or grows, pitch an intro fellowship or a reading group to them.”</span></p><div class=\"css-4kxa28\"><div class=\"shortcode-large shortcode-center\"><img src=\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/144535_1362612.JPG.1500x1000_q95_crop-smart_upscale.jpg\" alt=\"Daniela R. Shuman ’24, a Computer Science and Statistics concentrator interested in urban development, is one of the chairs of the Arete fellowship.\" class=\"css-8atqhb\"/><div class=\"css-8pot\"><span>Daniela R. Shuman ’24, a Computer Science and Statistics concentrator interested in urban development, is one of the chairs of the Arete fellowship.<!-- --> </span><span class=\"css-1ys3e0l\">By <a href=\"/writer/1217257/Marina__Qu/\" to=\"/writer/1217257/Marina__Qu/\" class=\"css-1fvj8jv\">Marina Qu</a></span></div></div></div><p><span>Even if some people choose not to become effective altruists, Shuman tells us, they could still take away valuable ideas from the movement.</span></p><p><span>“The point is not to create a club, but rather to create a world of people that want to do the most good, and EA just has a set of tools that it thinks are probably the most good,” Shuman says. “We want everybody to think in these terms.”</span></p><h1><span>Daedalus House</span></h1><p><span class=\"css-bp9mjk\">I</span><span>’m gonna talk from a removed, omniscient perspective,” Mysoré says, kicking his chair back and folding his arms behind his head. EA spends a lot of money on space, food, and socials, he tells us. “At a certain point you have to ask yourself: What is effective about that?”</span></p><p><span>Most of Harvard EA’s money comes from larger EA organizations like Open Philanthropy, a grantmaking foundation largely financed by Cari Tuna and Dustin Moskovitz, the latter of whom co-founded Facebook. Open Philanthropy distributes money to a range of EA-related causes. Put simply, it is an organization that “cares about making the world better,” Jurkovic says.</span></p><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_AllArticles_300x250_Add08\"></div></div><p><span>We ask him how Harvard EA uses its grant money.</span></p><p><span>“It’s not my area of expertise,” Jurkovic says. “But ...” He pauses for 15 seconds. “Yeah, just sometimes we get funding for club activities.”</span></p><p><span>In 2022, we later find out, part of an Open Philanthropy grant was used to send Arete fellows and the University-wide EA group on a weekend trip to Essex Woods, a serene, Thoreauesque venue an hour north of campus that charges about $5,000 per night. According to GiveWell, donating $10,000 to the nonprofit Malaria Consortium could save the lives of five people.</span></p><p><span>The schedule was similar to that of a corporate retreat: workshops, games, dinner, hot tub, Hamming circles. Well, maybe not the last one. Hamming circles are an activity where three to five participants sit down together and talk through one problem facing each member in 20-minute chunks. It’s “similar to what happens in a pair debug,” a post on an EA-related forum explains. These problems might vary, the post says, from “Is it possible for me, specifically, to have a meaningful impact on existential risk” to “I need to secure $250,000 in seed funding for my startup” to “I’m expected to speak at my father’s funeral and I have nothing but scathing, bitter, angry things to say.”</span></p><p><span>Open Philanthropy also issued a $250,000 grant for the Centre for Effective Altruism to “rent and refurbish” an office for the Harvard AI Safety Team in Harvard Square for one year.</span></p><p><span>Davies, the HAIST co-founder, tells us that the HAIST office is “pretty research-y.”</span></p><p><span>“People are often at whiteboards, talking about problems with each other,” he says. “I think it feels like people are really trying to make progress on this technical problem, which I find exciting. It’s maybe a little startup-y in vibe.” In a post on the EA forum in December, Davies wrote that “investing effort into making the space fun and convenient to use helped improve programming, social events, and sense of community.”</span></p><p><span>In August, Open Philanthropy recommended an $8.9 million grant for the Center for Effective Altruism to lease an EA office space for five years in Harvard Square. While the space would have been unaffiliated with Harvard EA, a forum post announcing the office promised that part of it would contain “meeting spaces for students at Harvard and other Boston-area schools,” and thanked Levin and Jurkovic for their help in developing the project.</span></p><p><span>Forum members, including Levin and Jurkovic, threw out potential names for the space. Some of the suggestions were mythological — “Daedalus,” who advised Icarus not to fly too close to the sun — some cosmological — “Supercluster,” “Earthrise” — and some silly, like “Aardvark,” from a user who argued the name sounded similar to “Harvard” and would show up first in alphabetical lists.</span></p><p><span>“Don’t like including the actual words EA in the name of the space,” Levin (who, for his part, liked “Apollo”) wrote in the comments. “It increases the chances of hypocrisy charges (from people who haven’t thought much about the effects of nice offices on productivity) for getting a nice central office space while ostensibly being altruistic.”</span></p><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_AllArticles_300x250_Add09\"></div></div><p><span>But the Apollo House — or Aardvark House, or Supercluster — never materialized. According to Levin, after CEA signed the lease and began preparing the space, Open Philanthropy notified them that the grant was under review. As a result, Levin tells us, CEA is now trying to sublease the space to get the money back.</span></p><p><span>CEA and Open Philanthropy did not respond to questions about the current status of the Harvard Square office grant.</span></p><p><span>In addition to money for spaces and retreats, Open Philanthropy has an open request form for university group funding, and regularly recommends grants to undergraduate organizers. Other EA-affiliated organizations also fund events and projects. Huml, the Global Health Programming Lead, tells us that this is part of what makes EA a valuable community in which to pursue global health work.</span></p><p><span>“To be totally transparent, I don’t 100 percent align with the values,” Huml tells us. “I think that they are an incredible platform and have a lot of resources — and those resources are financial, they are access to experts in very specific fields.”</span></p><p><span>Three Harvard students, including Davies and Gabrieli, were recipients of Open Philanthropy’s fall 2022 University Organizer Fellowship, for which the organization recommended a total of $3.1 million across 116 recipients. Gabrieli declined to be interviewed for this article. Davies says he doesn’t know if he’s allowed to disclose how much money he actually got, but that he considers the grant to be “an hourly wage,” since he quit previous jobs to focus on developing HAIST.</span></p><p><span>In February 2022, Open Philanthropy recommended a $75,000 grant to Pollack, the other HAIST co-founder, “to support her work organizing the effective altruism community at Harvard University.”</span></p><p><span>When we reach out to Pollack, she tells us over email that she is “no longer organizing for the Harvard Effective Altruism group,” but has spent about $14,000 of the grant on HAIST expenses with Open Philanthropy’s approval: $7,200 to monthly software costs like Airtable and Squarespace, and most of the rest to accommodations for a workshop that HAIST hosted with the MIT AI safety group in Arlington, Virginia.</span></p><p><span>Harvard EA is aware that this allocation of money can appear at odds with its stated mission. After the Essex Woods retreat, organizers sent out a feedback form. “How much did the spending of money at this retreat make you feel uncomfortable?” one question asked.</span></p><p><span>We talk to Levin, the University EA co-president, and he likens it to the way that companies spend money on recruitment. “The idea is that there are problems that are much more talent-constrained than money-constrained,” he tells us. AI safety, a problem that relatively few people are working on, is an extreme example of this, he says. “The question then becomes, ‘Okay, well, if we have money and not people, how do we convert between the two?’”</span></p><p><span>Levin pauses and corrects himself: “My train of thought there sounded kind of like I was saying, well, if you have a bunch of money, what do you do with it, right? That is not what I think.” What he does believe is that physical environments like retreats can rapidly accelerate the rate — by up to 100 times, he writes on the forums — at which people get on board with EA principles.</span></p><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_AllArticles_300x250_Add10\"></div></div><p><span>Several people in EA, Levin guesses, joined because of their experiences on a retreat. “That is absolutely something that we would have paid this money for,” he says.</span></p><p><span>Shuman believes that this outreach is particularly effective in Cambridge because of its highly motivated, change-driven student body. “Harvard and MIT have done the vast majority of vetting for people who are highly ambitious,” she says.</span></p><p><span>Shuman tells us that the international EA community places a lot of importance on this kind of community building. “If they can invest $1,000 in getting five high-potential individuals to, instead of doing AI research, do AI safety research, that’s a pretty good use of money,” she says. “It could save a lot of lives, potentially.”</span></p><h1><span>‘A Skewed Pipeline’</span></h1><p><span class=\"css-bp9mjk\">I</span><span>n a 2020 survey of EA Forum members, 76 percent of respondents were white and 71 percent were male. Though there is an imbalance, the Centre for Effective Altruism argues that diversity is important for several reasons.</span></p><p><span>“If an EA-aligned newcomer concludes that effective altruism is not for ‘people like me,’ they may not get involved, and the EA community may be less effective,” its website reads. “We don’t want to miss important perspectives.”</span></p><p><span>We ask Jurkovic if he’s aware of demographic imbalances within EA groups at Harvard. He pauses. “I think it is quite important to have a community which is welcoming to everyone,” he says. “EA sometimes shares a problem with the cause areas that it tackles” — meaning STEM fields — “which is that many of them have more males in them than average.”</span></p><p><span>Shuman — “the only she/her” leading an Arete section, she tells us — echoes this sentiment, saying that these numbers reflect existing disparities in STEM and philanthropic fields.</span></p><p><span>“It’s just a skewed pipeline,” she says, “which is a problem.”</span></p><p><span>“I can say from personal experience that we’ve had quite diverse groups,” Nickols says. “In terms of gender, it does tend to be more male-skewed, and that’s something that we’re continually working on.” He acknowledges that Harvard EA “is probably predominantly white and Asian, but not more so to Harvard’s general population.” (The organization does not keep demographic records of its members, so we can’t verify this.)</span></p><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_AllArticles_300x250_Add11\"></div></div><p><span>Nickols says that the applicants for Harvard EA’s fellowships also tend to skew male. “Given that word of mouth is our biggest kind of spreader, it might just be possible that guys who have done it in the past are friends with more guys and tell them about it,” he adds.</span></p><p><span>In recent months, the EA movement has been embroiled in controversies related to race and gender in its communities.</span></p><p><span>One of these controversies revolved around Nick Bostrom, a philosopher whose ideas led to the development of longtermism; four of his works are cited in the syllabus for Harvard EA’s Precipice Fellowship.</span></p><p><span>In January, Bostrom posted a letter to his website apologizing for a comment he wrote on a forum in the mid-90s, which claims that Black people “are more stupid than whites” and contains the n-word. In the letter, Bostrom castigates his past self for using the slur and writes that the comment “does not accurately represent my views, then or now,” but does not reject the possibility of genetic cognitive differences between races. He leaves this question to “others, who have more relevant knowledge.” The letter continues with a section about bioethics that opens: “What about eugenics? Do I support eugenics? No, not as the term is commonly understood.”</span></p><p><span>In March, Time magazine <a href=\"https://time.com/6252617/effective-altruism-sexual-harassment/\"> interviewed seven women</a> who said they had been sexually harassed, coerced, or assaulted within EA spaces, particularly in the Bay Area. The scene’s overwhelming maleness, tech-bro culture, and impulse to quantify and rationalize messy real-world dynamics created a deeply unsafe environment, the women said. One described having dinner with a prominent researcher nearly twice her age who told her that “pedophilic relationships between very young women and older men was a good way to transfer knowledge.”</span></p><p><span>“We were of course upset by both of these issues,” Jurkovic wrote in an email to us about the Bostrom letter and Time investigation, “and have spent time figuring out how we can improve our diversity and make sure we're a welcoming community to women and people of color.”</span></p><div class=\"css-4kxa28\"><div class=\"shortcode-large shortcode-center\"><img src=\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/144857_1362613.JPG.1500x1000_q95_crop-smart_upscale.jpg\" alt=\"Will A. Nickols ’24 is an Arete Fellowship chair.\" class=\"css-8atqhb\"/><div class=\"css-8pot\"><span>Will A. Nickols ’24 is an Arete Fellowship chair.<!-- --> </span><span class=\"css-1ys3e0l\">By <a href=\"/writer/1217257/Marina__Qu/\" to=\"/writer/1217257/Marina__Qu/\" class=\"css-1fvj8jv\">Marina Qu</a></span></div></div></div><p><span>Although some of EA’s focus areas deal with global health and economic growth in underdeveloped countries, its frameworks generally do not foreground race or gender. A version of the spring 2023 Arete syllabus posted on the Harvard EA website only mentions race in the overview of Week Four: Animal Welfare.</span></p><p><span>“One of the most important ways we can fail to identify the most important moral issues of our time is by unfairly shrinking our <em>moral circle</em>: the set of beings we deem worthy of our moral concern,” the syllabus reads. “For example, many whites in the US failed to identify that slavery was <em>the</em> moral issue of their age by excluding Blacks from their moral circle. To truly make the world better, we must look beyond the traditional moral horizon for those who are unfairly neglected by mainstream society. This week, we discuss one such group of beings: nonhuman animals.”</span></p><p><span>We ask Nickols, the Arete co-chair, about this framing. He tells us that it is important to keep the quote “in the context of where it was originally formulated.”</span></p><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_AllArticles_300x250_Add12\"></div></div><p><span>“Obviously the idea here is not to equate certain racial groups with animals or anything like that,” Nickols says. “Over time, though, the expanding moral circle idea is that white people who, before, held these extremely racist and terrible views — as the generations went on, and as culture shifted — began to see people, regardless of their race, as all morally equal.”</span></p><p><span>“We have not reached a point where racism is totally gone,” Nickols says, “but there is definitely a shift in the right direction here. And more generally, the idea is that as time goes on, it is quite possible that the circle will continue to increase.”</span></p><h1><span>Paperclips</span></h1><p><span class=\"css-bp9mjk\">O</span><span>ne thought experiment designed to demonstrate the danger of misaligned AI goes like this: Say the owner of a paperclip factory obtains an ultrapowerful AI and instructs it to maximize paperclip output. Although the AI is programmed to pursue a seemingly harmless goal, it might — if its understanding of values is not quite the same as ours — turn everything in the world into paper clips. That this scenario seems kind of silly is part of the point. Researchers are not concerned that AI will be “evil” per se, but that its pursuit of any objective, including “good” ones, might have unintended consequences.</span></p><p><span>“AI systems donʼt always do what their developers intend,” the Arete syllabus reads. “They replicate human biases, achieve their goals in surprising and destructive ways, and are vulnerable to external manipulation.” As a call to action, this is a compelling place to start. As a taxonomy, though, it is less an observation about AI than it is about <em>systems</em>. Existential risks like climate change might first destroy the people who did the least to create them; any movement created by people is, in a sense, only human.</span></p><p><span>This point came up in the wake of the FTX collapse — what did it mean that a group seeking to fundamentally change the world relied so heavily on existing distributions of power? — and it has come up again in the months since, in the course of writing this article. Can you optimize your life? What if the thing we construct in our idealized image turns out not to look so different from us after all?</span></p><p><span>For Andrew N. Garber ’23, a former Arete leader, considering questions is the point of EA. There is a common misconception that effective altruism is a destination, when really it’s more of a framework, he tells us: “It is more concerned about the question than any one specific answer.”</span></p><p><span>In any event, when we ask Jurkovic what he hopes EA will look like in the future, his response is straightforward. “The goal is to help people make the world better,” he says, half smiling. “As much as possible.”</span></p><p><span><em>— Associate Magazine Editor Bea Wall-Feng can be reached at bea.wall-feng@thecrimson.com. Follow them on Twitter <a href=\"https://twitter.com/wallfeng/\">@wallfeng</a>.</em></span></p><p><span><em>— Magazine writer Sophia C. Scott can be reached at sophia.scott@thecrimson.com. Follow her on Twitter <a href=\"https://twitter.com/ScottSophia_\">@ScottSophia_</a>.</em></span></p></div></div><style data-emotion-css=\"wf1v9y\">.css-wf1v9y{max-width:600px;margin:0px auto;padding:0px 20px;box-sizing:content-box;padding:20px 20px;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:20px auto;}</style><div class=\"css-wf1v9y\"><style data-emotion-css=\"1vg2o64\">.css-1vg2o64{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0px auto;}@media (max-width:425px){.css-1vg2o64{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;}}</style><div class=\"css-1vg2o64\"><style data-emotion-css=\"1oeze8g\">.css-1oeze8g{display:block;margin-right:20px;font:0.85em/1.5em 'Colaborate Light';text-transform:uppercase;}</style><span class=\"css-1oeze8g\">Tags</span><div><style data-emotion-css=\"1a0sfva\">.css-1a0sfva{display:inline-block;background-color:#aaa;border-radius:5px;padding:2px 4px;text-transform:uppercase;font:13px 'Colaborate Light';margin:2px;color:white;}</style><a href=\"/tag/scrutiny/\" to=\"/tag/scrutiny/\" class=\"css-1a0sfva\">Scrutiny</a><a href=\"/tag/front-middle-feature/\" to=\"/tag/front-middle-feature/\" class=\"css-1a0sfva\">Front Middle Feature</a></div></div></div><style data-emotion-css=\"1vqu4gu\">.css-1vqu4gu{margin-bottom:24px;}@media (max-width:768px){.css-1vqu4gu{display:none;}}</style><div class=\"css-1vqu4gu\"><style data-emotion-css=\"12m1k7q\">.css-12m1k7q{display:block;margin:0px auto;width:100%;max-width:100%;background:transparent;padding:10px 0px;}.css-12m1k7q .ad-tag{font-family:'Colaborate Light','Lucida Sans Unicode',sans-serif;color:#333333;text-align:center;font-size:0.7em;text-transform:uppercase;display:block;}.css-12m1k7q .ad-body{width:-webkit-fit-content;width:-moz-fit-content;width:fit-content;margin:0px auto;text-align:center;}</style><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_AllArticles_BTF_728x90\"></div></div></div><style data-emotion-css=\"jd14k9\">.css-jd14k9{display:none;margin-bottom:24px;}@media (max-width:768px){.css-jd14k9{display:block;}}</style><div class=\"css-jd14k9\"><style data-emotion-css=\"12m1k7q\">.css-12m1k7q{display:block;margin:0px auto;width:100%;max-width:100%;background:transparent;padding:10px 0px;}.css-12m1k7q .ad-tag{font-family:'Colaborate Light','Lucida Sans Unicode',sans-serif;color:#333333;text-align:center;font-size:0.7em;text-transform:uppercase;display:block;}.css-12m1k7q .ad-body{width:-webkit-fit-content;width:-moz-fit-content;width:fit-content;margin:0px auto;text-align:center;}</style><div class=\"advertisement undefined css-12m1k7q\"><div class=\"ad-tag\">Advertisement</div><div class=\"ad-body\" id=\"div-gpt-TheCrimson_Mobile_Leaderboard_BTF\"></div></div></div><style data-emotion-css=\"138id2w\">.css-138id2w{line-height:1.3em;background:#A50A0E;}.css-138id2w .top-bar .container{max-width:1000px;margin:0 auto;}.css-138id2w .top-bar li{list-style:none;padding:20px 76px 0px;}.css-138id2w .top-bar a{font-size:1.5em;font-family:'Big Moore';}.css-138id2w .top-bar .established{font-size:10px;padding:0 76px 5px;margin:0;}.css-138id2w .bottom-bar{max-width:1000px;margin:0px auto;padding:20px 60px 0px 60px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;font-family:'Roboto',sans-serif;box-sizing:border-box;}.css-138id2w .copyright{padding-bottom:20px;font-size:12.4px;line-height:18.72px;text-align:center;}@media (max-width:600px){.css-138id2w .top-bar li{padding:20px 60px 0px;}.css-138id2w .bottom-bar{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;padding:0px 60px 0px 60px;}}.css-138id2w .social-icons{margin:auto;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin-bottom:10px;max-width:200px;}.css-138id2w .social-icons svg{height:31px;width:100%;}.css-138id2w .social-icons .icon-container{-webkit-flex:1 1 0;-ms-flex:1 1 0;flex:1 1 0;}.css-138id2w a{color:white;}.css-138id2w .bottom-bar h1{color:white;}.css-138id2w .top-bar{background:#790B0B;}.css-138id2w .top-bar a{color:white;}.css-138id2w .top-bar .established{color:white;}.css-138id2w .copyright{color:white;}</style><footer class=\"css-138id2w\"><div class=\"top-bar\"><div class=\"container\"><li><a href=\"/\" to=\"/\">The Harvard Crimson</a></li><li class=\"established\"><em>The University Daily, Est. 1873</em></li></div></div><div class=\"bottom-bar\"><style data-emotion-css=\"2b7uf2\">.css-2b7uf2{height:-webkit-min-content;height:-moz-min-content;height:min-content;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-flex-basis:150px;-ms-flex-preferred-size:150px;flex-basis:150px;margin-bottom:50px;}.css-2b7uf2 h1{font-family:Lato;font-style:normal;font-weight:800;font-size:13px;text-transform:uppercase;line-height:0.95em;margin:0;padding-left:16px;padding-bottom:5px;padding-top:4px;}.css-2b7uf2 ul{list-style:none;padding:0;margin:0;font-weight:300;}.css-2b7uf2 ul a{font-family:Lato;font-style:normal;font-weight:300;font-size:14px;line-height:20px;}.css-2b7uf2 ul li{padding:3px 0;padding-left:16px;}@media (max-width:600px){.css-2b7uf2{text-align:left;-webkit-flex-basis:0px;-ms-flex-preferred-size:0px;flex-basis:0px;border-bottom:1px solid white;padding-top:15px;padding-bottom:15px;margin-bottom:10px;}.css-2b7uf2 h1{padding-left:0;font-size:15px;cursor:pointer;}.css-2b7uf2 ul li{padding-left:0;}.css-2b7uf2:not(:first-of-type){border-left:none;}.css-2b7uf2 .footer-list{max-height:0;overflow:hidden;-webkit-transition:max-height 0.2s ease-out;transition:max-height 0.2s ease-out;}}</style><div class=\"css-2b7uf2\"><h1 class=\"mobile-footer-nav\">Sections</h1><ul class=\"footer-list\"><li><a href=\"/section/news/\" to=\"/section/news/\">News</a></li><li><a href=\"/section/opinion/\" to=\"/section/opinion/\">Opinion</a></li><li><a href=\"/section/arts/\" to=\"/section/arts/\">Arts</a></li><li><a href=\"/flyby/\" to=\"/flyby/\">Blog</a></li><li><a href=\"/section/fm/\" to=\"/section/fm/\">Magazine</a></li><li><a href=\"/section/media/\" to=\"/section/media/\">Videos</a></li><li><a href=\"/section/sports/\" to=\"/section/sports/\">Sports</a></li></ul></div><div class=\"css-2b7uf2\"><h1 class=\"mobile-footer-nav\">About</h1><ul class=\"footer-list\"><li><a href=\"/about/\" to=\"/about/\">General</a></li><li><a href=\"/about/diversity-and-inclusion\" to=\"/about/diversity-and-inclusion\">Diversity &amp; Inclusion</a></li><li><a href=\"/about/privacy/\" to=\"/about/privacy/\">Privacy Policy</a></li><li><a href=\"/about/permissions/\" to=\"/about/permissions/\">Rights &amp; Permissions</a></li><li><a href=\"/sitemap/\" to=\"/sitemap/\">Sitemap</a></li></ul></div><div class=\"css-2b7uf2\"><h1 class=\"mobile-footer-nav\">Resources</h1><ul class=\"footer-list\"><li><a href=\"/advertising/\" to=\"/advertising/\">Advertising</a></li><li><a href=\"/subscribe/\" to=\"/subscribe/\">Newsletters</a></li><li><a href=\"https://www.gp.thecrimson.com/\" to=\"https://www.gp.thecrimson.com/\">Journalism Programs</a></li></ul></div><div class=\"css-2b7uf2\"><h1 class=\"mobile-footer-nav\">Contact Us</h1><ul class=\"footer-list\"><li><a href=\"/contact/\" to=\"/contact/\">Corrections</a></li></ul></div><br/></div><div class=\"social-icons\"><div class=\"icon-container\"><a href=\"https://www.youtube.com/user/TheHarvardCrimson\" to=\"https://www.youtube.com/user/TheHarvardCrimson\"><style data-emotion-css=\"qtbphl\">.css-qtbphl path{fill:white;}</style><svg width=\"43\" height=\"31\" viewBox=\"0 0 43 31\" fill=\"none\" class=\"css-qtbphl\"><path d=\"M34.8706 0.312108C28.4636 -0.104883 14.1936 -0.103188 7.79545 0.312108C0.867544 0.763001 0.0515548 4.75323 0 15.256C0.0515548 25.7401 0.860433 29.7473 7.79545 30.1999C14.1954 30.6152 28.4636 30.6168 34.8706 30.1999C41.7985 29.749 42.6145 25.7587 42.6661 15.256C42.6145 4.77188 41.8056 0.764696 34.8706 0.312108ZM15.9998 22.0363V8.47564L30.2218 15.2441L15.9998 22.0363Z\" fill=\"white\"></path></svg></a></div><div class=\"icon-container\"><a href=\"https://www.facebook.com/TheHarvardCrimson/\" to=\"https://www.facebook.com/TheHarvardCrimson/\"><svg width=\"32\" height=\"31\" viewBox=\"0 0 32 31\" fill=\"none\" class=\"css-qtbphl\"><path d=\"M30.2333 0H1.76667C0.790667 0 0 0.7539 0 1.68451V28.8287C0 29.7581 0.790667 30.512 1.76667 30.512H17.0933V18.6962H12.9227V14.0914H17.0933V10.6957C17.0933 6.75459 19.6173 4.60858 23.3053 4.60858C25.072 4.60858 26.5893 4.73444 27.032 4.79038V8.90949L24.4747 8.91076C22.4693 8.91076 22.0813 9.81977 22.0813 11.1521V14.0927H26.864L26.2413 18.6975H22.0813V30.512H30.236C31.2093 30.512 32 29.7581 32 28.8274V1.68451C32 0.7539 31.2093 0 30.2333 0Z\" fill=\"white\"></path></svg></a></div><div class=\"icon-container\"><a href=\"https://twitter.com/thecrimson\" to=\"https://twitter.com/thecrimson\"><svg width=\"1200\" height=\"1227\" viewBox=\"0 0 1200 1227\" fill=\"none\" class=\"css-qtbphl\"><path d=\"M714.163 519.284L1160.89 0H1055.03L667.137 450.887L357.328 0H0L468.492 681.821L0 1226.37H105.866L515.491 750.218L842.672 1226.37H1200L714.137 519.284H714.163ZM569.165 687.828L521.697 619.934L144.011 79.6944H306.615L611.412 515.685L658.88 583.579L1055.08 1150.3H892.476L569.165 687.854V687.828Z\" fill=\"white\"></path></svg></a></div><div class=\"icon-container\"><a href=\"https://www.instagram.com/theharvardcrimson/\" to=\"https://www.instagram.com/theharvardcrimson/\"><svg id=\"Layer_1\" data-name=\"Layer 1\" viewBox=\"0 0 511.07 511.16\" class=\"css-qtbphl\"><defs><style>\n      .cls-1 {\n        fill: #a92832;\n        stroke-width: 0px;\n      }\n    </style></defs><g id=\"_72a08e.tif\" data-name=\"72a08e.tif\"><g><path class=\"cls-1\" d=\"m382.31,511.16h-253.52c-5.05-.89-10.13-1.67-15.16-2.7C47.1,494.81.22,437.68.09,369.72-.05,293.7-.03,217.67.17,141.65c.02-9.25.8-18.67,2.62-27.73C16.14,47.13,73.48.19,141.7.08c75.84-.12,151.69-.12,227.53.09,9.42.03,19.07.61,28.2,2.71,57.05,13.14,93.63,48.13,109.45,104.55,1.95,6.97,2.82,14.24,4.18,21.38v253.57c-.89,5.03-1.53,10.12-2.7,15.08-13.39,57-48.21,93.64-104.61,109.5-6.99,1.96-14.29,2.83-21.45,4.2ZM255.54,45.82c0-.08,0-.16,0-.23-37.09,0-74.19.02-111.28-.01-11.37-.01-22.5,1.24-33.36,4.84-36.86,12.21-65.75,47.85-65.45,93.39.5,74.53.14,149.07.12,223.61,0,11.2,1.29,22.17,4.83,32.87,12.24,36.98,48.01,65.77,93.37,65.47,74.52-.5,149.04-.15,223.57-.12,11.2,0,22.17-1.28,32.86-4.84,37.14-12.35,65.67-47.99,65.43-92.89-.4-74.87-.15-149.74-.08-224.61.02-16.07-2.81-31.49-10.51-45.63-17.29-31.76-43.88-50.43-80.25-51.56-39.72-1.23-79.5-.28-119.26-.28Z\"></path><path class=\"cls-1\" d=\"m387.26,255.44c.1,72.45-58.96,131.72-131.42,131.89-72.63.17-132.05-59.16-131.99-131.82.05-72.48,59.21-131.62,131.67-131.65,72.45-.02,131.65,59.1,131.75,131.57Zm-131.55,86.26c47.79-.14,86.16-38.72,85.92-86.39-.24-47.65-38.41-85.74-86-85.81-47.73-.07-86.2,38.38-86.16,86.14.03,47.78,38.53,86.19,86.24,86.06Z\"></path><path class=\"cls-1\" d=\"m392.98,151.87c-18.39.1-33.49-14.78-33.54-33.08-.05-18.16,15.22-33.45,33.35-33.41,18.19.04,33.43,15.36,33.31,33.48-.11,18.09-14.98,32.91-33.13,33.01Z\"></path></g></g></svg></a></div></div><div class=\"copyright\">Copyright © <!-- -->2024<!-- --> The Harvard Crimson, Inc.</div></footer></div>\n          <div id=\"modal\"></div>\n          <script>\n            window.__APOLLO_STATE__={\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023})\":{\"__typename\":\"ArticleGQL\",\"title\":\"What is Going On With Effective Altruism?\",\"subtitle\":\"\",\"description\":\"“Most of us want to improve the world. We see suffering, injustice, and death and feel moved to do something about it,” the Harvard EA website says. “But figuring out what that ‘something’ is, let alone actually doing it, can be a difficult and disheartening challenge. Effective altruism is a response to this challenge.” Can it live up to that goal?\",\"contributorOverride\":\"\",\"createdOn\":\"2023-04-01T01:30:05.235477+00:00\",\"modifiedOn\":\"2023-11-10T03:54:30.953663+00:00\",\"showAds\":true,\"searchable\":true,\"slug\":\"ea-scrut\",\"url\":\"/article/2023/3/30/ea-scrut/\",\"group\":null,\"contributors\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).contributors.0\",\"typename\":\"ContributorGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).contributors.1\",\"typename\":\"ContributorGQL\"}],\"multimediaContributors\":[],\"tags\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).tags.0\",\"typename\":\"TagGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).tags.1\",\"typename\":\"TagGQL\"}],\"issue\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).issue\",\"typename\":\"IssueGQL\"},\"section\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).section\",\"typename\":\"SectionGQL\"},\"subsection\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).subsection\",\"typename\":\"TagGQL\"},\"fmSubsection\":\"Scrutiny\",\"flybySubsection\":null,\"relContent\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.0\",\"typename\":\"ImageGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.1\",\"typename\":\"ImageGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.2\",\"typename\":\"ImageGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.3\",\"typename\":\"ImageGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.4\",\"typename\":\"ImageGQL\"}],\"mainContent\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).mainContent\",\"typename\":\"ImageGQL\"},\"layout\":\"HPAC_TEMPLATE\",\"paragraphs\":{\"type\":\"json\",\"json\":[\"\\u003cp>{shortcode-63eec514b5e11ad6d3729f9f9ad4bd516201edea}\\u003c/p>\",\"\\u003cp>{shortcode-8c0dd475ea3269f67b1a4d37d27db5cc232a1fc2}hen we meet Nikola Jurkovic ’25 on Zoom, he’s sitting in front of a whiteboard covered with equations. We chat about his interest in folk punk music; his headphones push his hair back into a kind of emo swoop. Jurkovic comes across as friendly, but also guarded: He seems to want to make a good impression.\\u003c/p>\",\"\\u003cp>Jurkovic tells us he first learned about the philosophical movement called effective altruism from the comments section of a YouTube video about veganism during his gap year. When he moved from Croatia to Harvard, he joined the Harvard Undergraduate EA group and eventually became its president. “I think I’ve been looking for ways to make the world better for a really long time,” he says, “I think as far back as I can remember.”\\u003c/p>\",\"\\u003cp>EA, its proponents will tell you, is aimed at “doing good better.” It starts with the problem that the ways that we approach charity, aid, and other kinds of good-doing are clouded by human biases, and tries to find the best solution using statistical tools. For example, an effective altruist might argue that donating money to foundations that provide malaria treatments maximizes your lives saved per dollar spent.\\u003c/p>\",\"\\u003cp>The movement began in the early 2010s as the brainchild of the philosopher Will MacAskill. In the years since, it has flourished in both in-person spaces, like conferences, as well as blogs and forums. Users map out their worldviews in long, technical posts, debating what exactly different EA principles look like in practice, the merits of various criticisms of the movement, how to adapt to the more meritorious criticisms, and how to counter the less meritorious ones.\\u003c/p>\",\"\\u003cp>EA has also garnered support from a number of high-profile acolytes.\\u003c/p>\",\"\\u003cp>“This is a close match for my philosophy,” Elon Musk \\u003ca href=\\\"https://twitter.com/elonmusk/status/1554335028313718784\\\">tweeted\\u003c/a> last August about MacAskill’s new book.\\u003c/p>\",\"\\u003cp>“Effective altruism — efforts that actually help people rather than making you feel good or helping you show off — is one of the great new ideas of the 21st century,” celebrity academic and Harvard psychology professor Steven Pinker wrote.\\u003c/p>\",\"\\u003cp>One of the highest-profile effective altruists was the former billionaire Sam Bankman-Fried, whose commitment to donate much of his wealth to EA causes turned out to be impossible after his cryptocurrency company, FTX, collapsed due to alleged fraud. His commitment also turned out to be disingenuous: In a postmortem interview with Vox, he described his apparent embrace of ethics as “this dumb game we woke westerners play where we say all the right shibboleths and so everyone likes us.”\\u003c/p>\",\"\\u003cp>Nevertheless, EA’s profile has continued to rise. Even at Harvard, some students find the pull inescapable: “Effective altruism is a \\u003cem>huge\\u003c/em> trend on campus, seeping into everything,” Henry Haimo ’24 told the New Yorker in March, in an \\u003ca href=\\\"https://www.newyorker.com/magazine/2023/03/06/the-end-of-the-english-major\\\">article\\u003c/a> ostensibly about the decline of the humanities.\\u003c/p>\",\"\\u003cp>When Harvard’s undergraduate EA group started on campus a decade ago, though, EA was not so widespread. At first, getting students to join was a challenge. “I realized that the idea of HEA seemed crazy: ‘join us and we’ll try to figure out how to maximize the good we do in the world!’” Ben S. Kuhn ’15, the co-president at the time, wrote in a 2014 blog post. “But my mistake was \\u003cem>letting other people know\\u003c/em> that I knew HEA seemed crazy. As soon as they realized that I myself felt goofy, it was game over for convincing them to get involved.” He described expanding the group’s influence through a combination of savvy advertising tactics and a slate of famous speakers such as the philosopher Peter Singer, one of the most prominent intellectuals associated with the movement.\\u003c/p>\",\"\\u003cp>{shortcode-b730cf4004f8af7d74d9289183f0c71fc9976a48}\\u003c/p>\",\"\\u003cp>Today, there are several different EA groups across the University, including at the College, law school, and GSAS, as well as in the Boston area. (In this article, “Harvard EA” refers to the undergraduate group unless otherwise specified.) Jurkovic guesses there are “between 20 and 50” students involved in Harvard EA’s regular programming; there are dozens more in its introductory fellowships and hundreds more on the mailing lists.\\u003c/p>\",\"\\u003cp>Harvard markets itself as “developing leaders who make a difference globally,” and pop culture spins this principle into myth: Here is a university whose students will go on to save the world. EA is a movement that claims to know how to do it, from shifts in diet — a lot of the people we meet are vegan, in recognition of the statistically unmatched suffering that factory farming inflicts on chickens — to shifts in career, like the students who have devoted their futures to trying to prevent human extinction by AI.\\u003c/p>\",\"\\u003cp>“Most of us want to improve the world,” the Harvard EA website says. “But figuring out what that ‘something’ is, let alone actually doing it, can be a difficult and disheartening challenge.\\u003ca href=\\\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism/\\\"> Effective altruism\\u003c/a> is a response to this challenge.” Can it live up to that goal?\\u003c/p>\",\"\\u003ch1>Counterfactual Impact\\u003c/h1>\",\"\\u003cp>{shortcode-be29865d8a9c7908fa05930b7f2d42574eaa573c}f you are not already a committed effective altruist — and if you are interested in a discussion group that zigzags from deworming methodology to Bayesian statistics to the number of animals killed for food in the U.S. in the last 30 seconds — your first involvement with EA at Harvard might come through the introductory Arete Fellowship, a seven-week program that explores different effective altruism topics every week.\\u003c/p>\",\"\\u003cp>Daniela R. Shuman ’24, a Computer Science and Statistics concentrator interested in urban development, is one of the Arete chairs. She did the fellowship herself a year ago and “fell in love with the whole concept,” she says. She guesses there were around 40 fellows last fall.\\u003c/p>\",\"\\u003cp>We ask Will A. Nickols ’24, the other chair, if we can sit in on one of the sections. After some back-and-forth, Nickols suggests we attend the second week of the fellowship, which focuses on global health.\\u003c/p>\",\"\\u003cp>We get there before the meeting room opens. The discussion leaders, Nick C. Gabrieli ’24 and Jorge O. Guerra Jr. ’24, a former Crimson News editor, are sitting in near-darkness on the stairs outside the Adams House Upper Common Room, chatting: “What else are you up to this semester?” “What do you see as the limits of randomized control trials?” The vaulted blue ceiling looks like the night sky.\\u003c/p>\",\"\\u003cp>Claire Guo ’24, one of the Arete fellows and a former Crimson News editor, wanders up wearing two baseball caps; she wants to know if the leaders are participating in the junior class-wide game of Assassins.\\u003c/p>\",\"\\u003cp>No, Gabrieli tells her, adding, “I feel like I’m in the market for developing more hobbies.”\\u003c/p>\",\"\\u003cp>At 6 p.m., we file into the meeting room and assign ourselves to armchairs. There are three fellows in attendance, and another will show up halfway through. Gabrieli and Guerra walk us through the vocabulary terms from the readings — two blog posts, a lecture by an Oxford Professor, a TED talk, and several charts about life expectancy in different countries — asking the group to define and react to them.\\u003c/p>\",\"\\u003cp>This week, the discussion involves identifying your “problem”: So you’ve already decided you want to do good. How do you figure out \\u003cem>what\\u003c/em> to do?\\u003c/p>\",\"\\u003cp>EA takes cues from rationalism, a commitment to logic rather than feelings as a basis for decision-making. For the Arete fellows, this requires learning a lot of terminology. For example, the “counterfactual impact” of an action is the result of doing that action, relative to the result of not doing it. Say you want to volunteer in a soup kitchen — a lot of EA readings use this example — but there are plenty of other volunteers; if you don’t do it, someone else will. Unless you are extremely good at serving soup, the amount of soup served in a world where you are a server is probably not that different from the amount of soup served in a world where you are not, and you might want to focus your altruistic intentions elsewhere.\\u003c/p>\",\"\\u003cp>{shortcode-953d08b19087841b5ba3344e1e3970f242595182}\\u003c/p>\",\"\\u003cp>At the discussion leaders’ prompting, the fellows — Guo; Nathanael Tjandra ’26; and Kai C. Hostin ’25 — talk among themselves, trying to recall concepts from the readings. More than anything, it feels like a class section.\\u003c/p>\",\"\\u003cp>Gabrieli asks, “What is ‘importance?’” Guo and Hostin look at each other with uncertainty. “It should be able to be inferred, I think, from the word itself,” he continues. “What is the absolute magnitude of the thing we’re interested in?”\\u003c/p>\",\"\\u003cp>There are a lot of math terms, too. One of the readings constructs a complicated-looking product of derivatives to make the conceptual point that importance, tractability, and neglectedness are all vital considerations. Importance is how important a problem is; tractability is how much good can be done relative to effort; neglectedness is how much other people are already working in that area.\\u003c/p>\",\"\\u003cp>Still, the fellows have questions, and Gabrieli and Guerra are happy to discuss them. Should your personal interests play a role in what you decide to focus on? What about problems that require immediate responses, rather than careful calculation of impact? They talk but don’t come to any singular answer.\\u003c/p>\",\"\\u003cp>Near the end of the hour, Guerra brings up GiveWell, a well-known EA organization that maintains a ranked list of recommended charities. He asks the group: Of the top four causes on the website, which one would you donate to, and why?\\u003c/p>\",\"\\u003cp>First on the list is Malaria Consortium, which provides a kind of medicine called seasonal malaria chemoprevention. Malaria treatments are high-importance, and tractable because they are relatively cheap; GiveWell estimates that it costs $7 to protect a child from malaria. Second on the list is the Against Malaria Foundation, which provides malaria nets for about $5 each.\\u003c/p>\",\"\\u003cp>Tjandra, a Crimson Multimedia editor, ultimately chooses the fourth-ranked charity, which provides cash incentives to caregivers in Nigeria who vaccinate their babies. Unlike malaria treatments, he reasons, vaccines are “more generalizable to poor people everywhere.”\\u003c/p>\",\"\\u003cp>Guo asks if instead of providing Vitamin A supplements to areas in sub-Saharan Africa, as the third-ranked charity does, we could try to integrate vegetables rich in Vitamin A into those communities. “‘Eat some yams,’” she says, laughing.\\u003c/p>\",\"\\u003cp>Hostin concurs: “I don’t want to be like ‘Oh, here’s a fish’ rather than teach them how to fish, in a way.”\\u003c/p>\",\"\\u003ch1>A One in Ten Risk of Extinction\\u003c/h1>\",\"\\u003cp>{shortcode-21cc3534b02e5a90dd1b6e61be0fe28423896a7e}t the end of the Arete fellowship, you are given a $10 gift card to the charity donation site Every.org. If you donate $10 of your own money, you receive another gift card. But what happens next is up to you: Since Harvard EA has no centralized meetings or agenda, continued membership means “joining the various side groups that we have, depending on what you’re interested in,” Shuman says.\\u003c/p>\",\"\\u003cp>Donating large parts of your income to charity — or earning to give, in the style of Bankman-Fried — are largely out of reach for college students, Jurkovic tells us. Instead, we find out, undergrad EA groups tend to focus on research and recruitment.\\u003c/p>\",\"\\u003cp>Still, the issues Harvard EA focuses on don’t always line up with the picture of EA suggested by the Arete fellowship. Zazie Huml ’25, Harvard EA’s Global Health Programming Lead and one of four people on its board, said when they joined Harvard EA there was “no major initiative” in global health or international development — two of the five major topics covered in the Arete fellowship — and there were only “a couple people” involved with animal rights, a third focus area.\\u003c/p>\",\"\\u003cp>“In Harvard EA we try not to present unfairly biased opinions towards any particular world problem,” Jurkovic says. “We aim to present the facts about the world problems and also give people useful decision-making tools so that they can examine the facts themselves.”\\u003c/p>\",\"\\u003cp>“My entire experience with EA at Harvard last semester was, ‘Oh, this is not for me, this is not my community. They’re not interested in the same things as me,’” Huml says, until someone they met in the organization encouraged them to take another look. “If I was to take initiative within the system, there would be resources to support me,” they recall the person saying. Huml has since led several global health initiatives under the umbrella of Harvard EA, including a “comprehensive study on source apportionment of lead exposure” in lower-income countries in partnership with the Lead Exposure Elimination Project.\\u003c/p>\",\"\\u003cp>So what was Harvard EA doing? “It was only focused on longtermism and AI,” Huml remembers.\\u003c/p>\",\"\\u003cp>Longtermism — week five on the Arete syllabus — is the view that people who will be alive in the future warrant the same moral consideration as people alive today. “If we want to do the most good, that means we want to help the most people,” Shuman says, “and the most people is not at the specific time that we’re living in.” Since future people will significantly outnumber today's people, barring a mass extinction event, longtermists argue that we should devote more resources to preventing “existential risks” like nuclear warfare or engineered pandemics.\\u003c/p>\",\"\\u003cp>This is something of a departure from EA principles in other areas, which our interviewees explain are a refinement of natural human instincts: You already want to do good, and EA just teaches you how to be smart about it. For a longtermist, though, what is at stake is the future of human existence.\\u003c/p>\",\"\\u003cp>In recent years, longtermists have turned their attention to the field of AI safety. As AI models grow increasingly powerful, EA researchers have argued, the existential threat they pose may become insurmountable. They call this the problem of “AI alignment”: ensuring that if or when a superhuman AI comes into existence, its values align with our own, so that it does not kill everyone on Earth.\\u003c/p>\",\"\\u003cp>Some AI safety researchers have been sounding this alarm for decades — but this past year, thanks to the rise of shockingly powerful, publicly available AI models like \\u003ca href=\\\"https://www.thecrimson.com/article/2023/2/23/chatgpt-scrut/\\\">ChatGPT\\u003c/a>, the issue has made it into the mainstream. ChatGPT is already prone to spreading misinformation. A “sufficiently powerful” AI could be much worse, researcher Elizier Yudkowsky argued in a recent \\u003ca href=\\\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\\\">Time magazine op-ed\\u003c/a> calling for an indefinite moratorium on AI development. “In today’s world you can email DNA strings to laboratories that will produce proteins on demand allowing an AI initially confined to the internet to build artificial life forms or bootstrap straight to postbiological molecular manufacturing,” he wrote.\\u003c/p>\",\"\\u003cp>How do you quantify incalculable destruction? “The Precipice: Existential Risk and the Future of Humanity,” a book by Toby Ord about existential risk, for which Harvard EA runs a reading group, puts the risk of extinction in the next century from unaligned AI at 1 in 10, higher than any other source of risk. Last July, Jurkovic wrote in a comment on the EA forum that “existential risks are high up on the list of most probable causes of death for college aged-people”: Assume that the probability of achieving superhuman AI by 2045 is 50 percent, and assume that the probability of death given superhuman AI is at least 10 percent. Then the probability of death by AI in the next few years might be comparable to around 1 in 6000, he wrote, explaining that this probability is similar to the two largest causes of death for “college-aged people in the US,” suicide and vehicle accidents, although he did not write out the calculations leading to this conclusion.\\u003c/p>\",\"\\u003cp>Jurkovic guesses that there are more people in Harvard EA working on AI than other problems, but points out the existence of organizations with other EA-related focuses, including Harvard College Animal Advocates.\\u003c/p>\",\"\\u003cp>Harvard EA doesn’t have an AI safety program itself, but there are several related organizations that do. One of these is the Harvard AI Safety Team, which was founded in 2022 by Xander L. Davies ’23 and Fiona E. Pollack ’25. Though HAIST is not an EA organization, many members of Harvard EA work with HAIST, and HAIST also receives funding from national EA philanthropy organizations. HAIST and Harvard EA also share members with other local AI groups, including Cambridge Boston Alignment Initiative and MIT AI Alignment.\\u003c/p>\",\"\\u003cp>HAIST hosts reading groups and talks by professors, with a focus on machine learning research, Davies tells us on Zoom. He has curly hair and a black hoodie; the “L” in his name stands for Laser. Although he doesn’t believe that AI right now is poised to destroy humanity, he also doesn’t believe we have the tools to stop it. “I think how I look at it is, it’s currently impossible to get our AIs to not do things,” Davies says, referring to the ease with which users have bypassed ChatGPT-like models’ built-in filters against violent speech and misinformation.\\u003c/p>\",\"\\u003cp>“Rapid progress in AI is becoming more and more economically useful, becoming more and more trusted, while at the same time this stark lack of progress on actually understanding how these systems work, on getting confidence that we actually know how to make these systems do what we want, is very startling to me,” Davies says. “And I think it should be a core priority on the global stage.”\\u003c/p>\",\"\\u003ch1>‘The Warm Fuzzy Feeling Just Doesn’t Matter as Much’\\u003c/h1>\",\"\\u003cp>{shortcode-be29865d8a9c7908fa05930b7f2d42574eaa573c}n some respects, EA seems fairly intuitive: Who doesn’t want to minimize suffering as much as possible? In other respects, it pushes you to rethink your intuitions.\\u003c/p>\",\"\\u003cp>Take, for instance, a thought experiment effective altruists often use to illustrate the unique way this philosophy navigates moral quandaries. It’s called “\\u003ca href=\\\"https://www.harvardea.org/blog/2022/lena-ashooh-the-child-in-the-pond\\\">the drowning child scenario\\u003c/a>,” originally formulated by Peter Singer. Imagine you’re on your way to an important event, and you notice a child is drowning in a nearby pond. Do you jump in to save the child?\\u003c/p>\",\"\\u003cp>Barring circumstances like an inability to swim, most people answer yes. But then, you’re asked the question again and again. Each time, the stakes are higher: What if you will ruin your clothes and waterlog your phone by jumping in? What if you already saved a drowning child last week? What if this child was an undocumented immigrant? What if the pond was far enough away that you would have to spend gas money to get there?\\u003c/p>\",\"\\u003cp>As the hypotheticals escalate, generally, participants continue to decide to save the drowning child each time. But in the real world, when facing situations like determining healthcare access for immigrants or people in other countries, the increased distance leads some people to make what is in effect the opposite choice. EA wants to know: Why should you value those peoples’ lives less than those of people closer to you?\\u003c/p>\",\"\\u003cp>When we spoke to Marka F. X. Ellertson ’23, then the president of Harvard EA, last September, she told us that with EA efforts, “the warm fuzzy feeling just doesn’t matter as much to me as the rational thought that I know that I’ve had a bigger impact.”\\u003c/p>\",\"\\u003cp>“And I actually still do want that warm fuzzy feeling,” Ellertson added, explaining that she donates to local causes that are particularly meaningful to her.\\u003c/p>\",\"\\u003cp>Joshua D. Greene ’97, Harvard EA’s faculty advisor, disputes the idea that EA strips away the warmth of charity work.\\u003c/p>\",\"\\u003cp>Utilitarianism might make you think of “the things that kind of serve a function but don’t nurture our souls or to speak to our heart’s greatest desires, right? And utilitarianism is not just about cold functionality,” he says. “It’s about everything that makes life good or bad, everything that makes life worth living, everything that makes life meaningful.”\\u003c/p>\",\"\\u003cp>Harold H. Klapper ’25, who participated in a Harvard EA fellowship last year, tells us that in some EA dialogues about utilitarianism “get really wild.”\\u003c/p>\",\"\\u003cp>At a Boston-area EA event, for instance, “I’ve had conversations arguing about whether we should kill all wild animals, because they have negative lives,” Klapper says. “An ant colony must just have negative utility in the sense that they’re just not enjoying life, and so it’d be better if we just eliminated them.”\\u003c/p>\",\"\\u003cp>“When things are a movement, you kind of have to buy into the whole thing, and when you buy into the whole thing, you get really wacky and fucked up answers to problems,” Klapper adds.\\u003c/p>\",\"\\u003cp>Effective altruists seek to apply EA principles to personal decisions: what to study, where to work. If you are a college student interested in building EA communities, you might “consider not going to Harvard, as there are a bunch of people there doing great things,” Jurkovic wrote on the EA forum in December, suggesting that going to other colleges without strong EA movements could be better. (Was this something Jurkovic himself considered when applying to Harvard? “No,” he says, laughing.)\\u003c/p>\",\"\\u003cp>A lot of EA discourse revolves around career choice: You will probably work for around 80,000 hours in your lifetime — several of the people we talk to cite this estimate — and you should spend them doing things that count, even if they may not be things you enjoy.\\u003c/p>\",\"\\u003cp>Harvard EA, Shuman says, focuses mainly on “getting high-potential individuals into careers where they can spend their 80,000 hours of their career on solving these issues.”\\u003c/p>\",\"\\u003cp>{shortcode-ccfa245c56f41c3dd4f50e1d2e97b2e846b0727a}\\u003c/p>\",\"\\u003cp>To get them to that point, EA might also influence Harvard students’ concentration decisions. One such student is Jōsh P. Mysoré ’26; when we meet him outside of Blackbird, he’s reading Giovanni Boccaccio’s “The Decameron” for class. Mysoré completed the Arete fellowship last fall and is considering becoming a discussion leader at some point in the future.\\u003c/p>\",\"\\u003cp>“I love poetry. I loooove poetry,” he tells us. “Will I be going into poetry? No. Because I don’t think it will actually do good for people.” At the moment, Mysoré plans to concentrate in Computer Science and Linguistics.\\u003c/p>\",\"\\u003cp>Computer Science, as well as Statistics and Applied Mathematics, are fairly common concentration choices among the people we meet. Klapper tells us he knows someone in Harvard EA who studies Computer Science and dislikes it, but continues in the field because they believe it’s the most effective use of their time.\\u003c/p>\",\"\\u003cp>Mysoré “was given a certain amount of privilege in my life to even get to this point,” he tells us. “I do think I owe something to the greater good of humanity to do something that impacts more people in a tangible way.”\\u003c/p>\",\"\\u003cp>Does he think anyone should go into poetry, we ask.\\u003c/p>\",\"\\u003cp>“I don't think it’s a contradiction to say that I can hold two opposing viewpoints at the same time,” Mysoré says. “Like in my heart, I’m a humanist, and I’m very romantic.” He tells us that he joined EA specifically to challenge these humanist viewpoints, but his perspective might flip again. “Honestly, I do think there should be poets,” he says.\\u003c/p>\",\"\\u003cp>Mysoré tells us he still believes that EA has a noble mission, even if he disagrees with some of its particular approaches. “I think at the baseline, EA is creating dialogue,” he says. “That is really what counts.”\\u003c/p>\",\"\\u003ch1>‘Not To Create a Club, But Rather to Create a World’\\u003c/h1>\",\"\\u003cp>{shortcode-24643cedbe14221289878261864001a8ceef067a}n Saturdays, Harvard EA throws socials in a house near campus where Jurkovic lives with four of his friends. Half the time, the socials are just for Harvard affiliates; every other week, they are open to students from other Boston-area schools.\\u003c/p>\",\"\\u003cp>Of course, the socials are designed to be fun, but they have a functional purpose as well. “One important part of having a community is that the people talk to each other and have time spent together, so that they can collaborate and talk about their projects,” Jurkovic says.\\u003c/p>\",\"\\u003cp>Though Jurkovic declined our request to attend a social on the record, we can try to reconstruct the vibe from a guide that he posted on the EA forum called “How to Organize a Social.” Indeed, in the post, he records every step of preparing for a social in granular detail, providing recommendations for everything from grocery lists — CLIF Bars, Diet Coke, several varieties of Vitaminwater — to music, such as the Spotify-curated playlist “my life is a movie.” Jurkovic suggests you make it easy for guests to find answers to anticipated questions: “The shoes on/off policy? Where the bathroom is? Where one can get water? What the wi-fi password is?”\\u003c/p>\",\"\\u003cp>Last year, Trevor J. Levin ’19, who is currently on leave as the co-president of the university-wide EA group, also created a list of recommendations for effective retreats: They should happen in the beginning of the semester, when people are less busy; include lots of time for one-on-one interactions and a “structured vulnerable/emotional thing”; and include a healthy mix of new recruits and “moderately charismatic people for whom EA is a major consideration in how they make decisions.” These suggestions were embedded in a long post, which, citing feedback from Ellertson, Davies, Jurkovic, and others, argues that college EA groups should focus more on retreats as a method of bonding.\\u003c/p>\",\"\\u003cp>For Levin, a former Crimson editor, this kind of immersive social situation is vital to capturing those who might be interested in EA but don’t prioritize it.\\u003c/p>\",\"\\u003cp>“While most of the important cognition that happens is social/emotional, this is not the same thing as tricking or manipulating people into being EAs,” he wrote on the forum. Instead, retreats are meant to appeal to those who may agree with EA on some level but have not yet acted on it, and giving them time to “move closer to the values they \\u003cem>previously already wanted to live by\\u003c/em>.”\\u003c/p>\",\"\\u003cp>Since EA was born, it has been very deliberate about the image it projects. The name “effective altruism” was itself the product of a long debate: “This has been such an obstacle in the utilitarianesque community — ‘do-gooder’ is the current term, and it sucks,” MacAskill, the philosopher, wrote in a 2011 email chain. What followed was a period of brainstorming — fusing terms like “utilitarian” and philanthropist” with “alliance” and “institute” — and a series of votes to establish a name for both “the type of person we wanted to refer to, and for the name of the organization we were setting up.”\\u003c/p>\",\"\\u003cp>Now, countless blogs and forum posts are dedicated to determining how best to recruit new members to the EA community. In December 2021, for instance, Jurkovic wrote a post on the EA forum describing an “organic” way to pitch EA to students.\\u003c/p>\",\"\\u003cp>“Person: What do you want to study? Me: Not sure, I’m trying to find what to study so I can have as good of an impact as possible,” he wrote in an example dialogue. “If their level of enthusiasm stays high or grows, pitch an intro fellowship or a reading group to them.”\\u003c/p>\",\"\\u003cp>{shortcode-caec2638cd0e69a267b65142b3f32cd802e78a8f}\\u003c/p>\",\"\\u003cp>Even if some people choose not to become effective altruists, Shuman tells us, they could still take away valuable ideas from the movement.\\u003c/p>\",\"\\u003cp>“The point is not to create a club, but rather to create a world of people that want to do the most good, and EA just has a set of tools that it thinks are probably the most good,” Shuman says. “We want everybody to think in these terms.”\\u003c/p>\",\"\\u003ch1>Daedalus House\\u003c/h1>\",\"\\u003cp>{shortcode-be29865d8a9c7908fa05930b7f2d42574eaa573c}’m gonna talk from a removed, omniscient perspective,” Mysoré says, kicking his chair back and folding his arms behind his head. EA spends a lot of money on space, food, and socials, he tells us. “At a certain point you have to ask yourself: What is effective about that?”\\u003c/p>\",\"\\u003cp>Most of Harvard EA’s money comes from larger EA organizations like Open Philanthropy, a grantmaking foundation largely financed by Cari Tuna and Dustin Moskovitz, the latter of whom co-founded Facebook. Open Philanthropy distributes money to a range of EA-related causes. Put simply, it is an organization that “cares about making the world better,” Jurkovic says.\\u003c/p>\",\"\\u003cp>We ask him how Harvard EA uses its grant money.\\u003c/p>\",\"\\u003cp>“It’s not my area of expertise,” Jurkovic says. “But ...” He pauses for 15 seconds. “Yeah, just sometimes we get funding for club activities.”\\u003c/p>\",\"\\u003cp>In 2022, we later find out, part of an Open Philanthropy grant was used to send Arete fellows and the University-wide EA group on a weekend trip to Essex Woods, a serene, Thoreauesque venue an hour north of campus that charges about $5,000 per night. According to GiveWell, donating $10,000 to the nonprofit Malaria Consortium could save the lives of five people.\\u003c/p>\",\"\\u003cp>The schedule was similar to that of a corporate retreat: workshops, games, dinner, hot tub, Hamming circles. Well, maybe not the last one. Hamming circles are an activity where three to five participants sit down together and talk through one problem facing each member in 20-minute chunks. It’s “similar to what happens in a pair debug,” a post on an EA-related forum explains. These problems might vary, the post says, from “Is it possible for me, specifically, to have a meaningful impact on existential risk” to “I need to secure $250,000 in seed funding for my startup” to “I’m expected to speak at my father’s funeral and I have nothing but scathing, bitter, angry things to say.”\\u003c/p>\",\"\\u003cp>Open Philanthropy also issued a $250,000 grant for the Centre for Effective Altruism to “rent and refurbish” an office for the Harvard AI Safety Team in Harvard Square for one year.\\u003c/p>\",\"\\u003cp>Davies, the HAIST co-founder, tells us that the HAIST office is “pretty research-y.”\\u003c/p>\",\"\\u003cp>“People are often at whiteboards, talking about problems with each other,” he says. “I think it feels like people are really trying to make progress on this technical problem, which I find exciting. It’s maybe a little startup-y in vibe.” In a post on the EA forum in December, Davies wrote that “investing effort into making the space fun and convenient to use helped improve programming, social events, and sense of community.”\\u003c/p>\",\"\\u003cp>In August, Open Philanthropy recommended an $8.9 million grant for the Center for Effective Altruism to lease an EA office space for five years in Harvard Square. While the space would have been unaffiliated with Harvard EA, a forum post announcing the office promised that part of it would contain “meeting spaces for students at Harvard and other Boston-area schools,” and thanked Levin and Jurkovic for their help in developing the project.\\u003c/p>\",\"\\u003cp>Forum members, including Levin and Jurkovic, threw out potential names for the space. Some of the suggestions were mythological — “Daedalus,” who advised Icarus not to fly too close to the sun — some cosmological — “Supercluster,” “Earthrise” — and some silly, like “Aardvark,” from a user who argued the name sounded similar to “Harvard” and would show up first in alphabetical lists.\\u003c/p>\",\"\\u003cp>“Don’t like including the actual words EA in the name of the space,” Levin (who, for his part, liked “Apollo”) wrote in the comments. “It increases the chances of hypocrisy charges (from people who haven’t thought much about the effects of nice offices on productivity) for getting a nice central office space while ostensibly being altruistic.”\\u003c/p>\",\"\\u003cp>But the Apollo House — or Aardvark House, or Supercluster — never materialized. According to Levin, after CEA signed the lease and began preparing the space, Open Philanthropy notified them that the grant was under review. As a result, Levin tells us, CEA is now trying to sublease the space to get the money back.\\u003c/p>\",\"\\u003cp>CEA and Open Philanthropy did not respond to questions about the current status of the Harvard Square office grant.\\u003c/p>\",\"\\u003cp>In addition to money for spaces and retreats, Open Philanthropy has an open request form for university group funding, and regularly recommends grants to undergraduate organizers. Other EA-affiliated organizations also fund events and projects. Huml, the Global Health Programming Lead, tells us that this is part of what makes EA a valuable community in which to pursue global health work.\\u003c/p>\",\"\\u003cp>“To be totally transparent, I don’t 100 percent align with the values,” Huml tells us. “I think that they are an incredible platform and have a lot of resources — and those resources are financial, they are access to experts in very specific fields.”\\u003c/p>\",\"\\u003cp>Three Harvard students, including Davies and Gabrieli, were recipients of Open Philanthropy’s fall 2022 University Organizer Fellowship, for which the organization recommended a total of $3.1 million across 116 recipients. Gabrieli declined to be interviewed for this article. Davies says he doesn’t know if he’s allowed to disclose how much money he actually got, but that he considers the grant to be “an hourly wage,” since he quit previous jobs to focus on developing HAIST.\\u003c/p>\",\"\\u003cp>In February 2022, Open Philanthropy recommended a $75,000 grant to Pollack, the other HAIST co-founder, “to support her work organizing the effective altruism community at Harvard University.”\\u003c/p>\",\"\\u003cp>When we reach out to Pollack, she tells us over email that she is “no longer organizing for the Harvard Effective Altruism group,” but has spent about $14,000 of the grant on HAIST expenses with Open Philanthropy’s approval: $7,200 to monthly software costs like Airtable and Squarespace, and most of the rest to accommodations for a workshop that HAIST hosted with the MIT AI safety group in Arlington, Virginia.\\u003c/p>\",\"\\u003cp>Harvard EA is aware that this allocation of money can appear at odds with its stated mission. After the Essex Woods retreat, organizers sent out a feedback form. “How much did the spending of money at this retreat make you feel uncomfortable?” one question asked.\\u003c/p>\",\"\\u003cp>We talk to Levin, the University EA co-president, and he likens it to the way that companies spend money on recruitment. “The idea is that there are problems that are much more talent-constrained than money-constrained,” he tells us. AI safety, a problem that relatively few people are working on, is an extreme example of this, he says. “The question then becomes, ‘Okay, well, if we have money and not people, how do we convert between the two?’”\\u003c/p>\",\"\\u003cp>Levin pauses and corrects himself: “My train of thought there sounded kind of like I was saying, well, if you have a bunch of money, what do you do with it, right? That is not what I think.” What he does believe is that physical environments like retreats can rapidly accelerate the rate — by up to 100 times, he writes on the forums — at which people get on board with EA principles.\\u003c/p>\",\"\\u003cp>Several people in EA, Levin guesses, joined because of their experiences on a retreat. “That is absolutely something that we would have paid this money for,” he says.\\u003c/p>\",\"\\u003cp>Shuman believes that this outreach is particularly effective in Cambridge because of its highly motivated, change-driven student body. “Harvard and MIT have done the vast majority of vetting for people who are highly ambitious,” she says.\\u003c/p>\",\"\\u003cp>Shuman tells us that the international EA community places a lot of importance on this kind of community building. “If they can invest $1,000 in getting five high-potential individuals to, instead of doing AI research, do AI safety research, that’s a pretty good use of money,” she says. “It could save a lot of lives, potentially.”\\u003c/p>\",\"\\u003ch1>‘A Skewed Pipeline’\\u003c/h1>\",\"\\u003cp>{shortcode-be29865d8a9c7908fa05930b7f2d42574eaa573c}n a 2020 survey of EA Forum members, 76 percent of respondents were white and 71 percent were male. Though there is an imbalance, the Centre for Effective Altruism argues that diversity is important for several reasons.\\u003c/p>\",\"\\u003cp>“If an EA-aligned newcomer concludes that effective altruism is not for ‘people like me,’ they may not get involved, and the EA community may be less effective,” its website reads. “We don’t want to miss important perspectives.”\\u003c/p>\",\"\\u003cp>We ask Jurkovic if he’s aware of demographic imbalances within EA groups at Harvard. He pauses. “I think it is quite important to have a community which is welcoming to everyone,” he says. “EA sometimes shares a problem with the cause areas that it tackles” — meaning STEM fields — “which is that many of them have more males in them than average.”\\u003c/p>\",\"\\u003cp>Shuman — “the only she/her” leading an Arete section, she tells us — echoes this sentiment, saying that these numbers reflect existing disparities in STEM and philanthropic fields.\\u003c/p>\",\"\\u003cp>“It’s just a skewed pipeline,” she says, “which is a problem.”\\u003c/p>\",\"\\u003cp>“I can say from personal experience that we’ve had quite diverse groups,” Nickols says. “In terms of gender, it does tend to be more male-skewed, and that’s something that we’re continually working on.” He acknowledges that Harvard EA “is probably predominantly white and Asian, but not more so to Harvard’s general population.” (The organization does not keep demographic records of its members, so we can’t verify this.)\\u003c/p>\",\"\\u003cp>Nickols says that the applicants for Harvard EA’s fellowships also tend to skew male. “Given that word of mouth is our biggest kind of spreader, it might just be possible that guys who have done it in the past are friends with more guys and tell them about it,” he adds.\\u003c/p>\",\"\\u003cp>In recent months, the EA movement has been embroiled in controversies related to race and gender in its communities.\\u003c/p>\",\"\\u003cp>One of these controversies revolved around Nick Bostrom, a philosopher whose ideas led to the development of longtermism; four of his works are cited in the syllabus for Harvard EA’s Precipice Fellowship.\\u003c/p>\",\"\\u003cp>In January, Bostrom posted a letter to his website apologizing for a comment he wrote on a forum in the mid-90s, which claims that Black people “are more stupid than whites” and contains the n-word. In the letter, Bostrom castigates his past self for using the slur and writes that the comment “does not accurately represent my views, then or now,” but does not reject the possibility of genetic cognitive differences between races. He leaves this question to “others, who have more relevant knowledge.” The letter continues with a section about bioethics that opens: “What about eugenics? Do I support eugenics? No, not as the term is commonly understood.”\\u003c/p>\",\"\\u003cp>In March, Time magazine \\u003ca href=\\\"https://time.com/6252617/effective-altruism-sexual-harassment/\\\"> interviewed seven women\\u003c/a> who said they had been sexually harassed, coerced, or assaulted within EA spaces, particularly in the Bay Area. The scene’s overwhelming maleness, tech-bro culture, and impulse to quantify and rationalize messy real-world dynamics created a deeply unsafe environment, the women said. One described having dinner with a prominent researcher nearly twice her age who told her that “pedophilic relationships between very young women and older men was a good way to transfer knowledge.”\\u003c/p>\",\"\\u003cp>“We were of course upset by both of these issues,” Jurkovic wrote in an email to us about the Bostrom letter and Time investigation, “and have spent time figuring out how we can improve our diversity and make sure we're a welcoming community to women and people of color.”\\u003c/p>\",\"\\u003cp>{shortcode-70922b68dcc4d0816e2c010925135376dae995cb}\\u003c/p>\",\"\\u003cp>Although some of EA’s focus areas deal with global health and economic growth in underdeveloped countries, its frameworks generally do not foreground race or gender. A version of the spring 2023 Arete syllabus posted on the Harvard EA website only mentions race in the overview of Week Four: Animal Welfare.\\u003c/p>\",\"\\u003cp>“One of the most important ways we can fail to identify the most important moral issues of our time is by unfairly shrinking our \\u003cem>moral circle\\u003c/em>: the set of beings we deem worthy of our moral concern,” the syllabus reads. “For example, many whites in the US failed to identify that slavery was \\u003cem>the\\u003c/em> moral issue of their age by excluding Blacks from their moral circle. To truly make the world better, we must look beyond the traditional moral horizon for those who are unfairly neglected by mainstream society. This week, we discuss one such group of beings: nonhuman animals.”\\u003c/p>\",\"\\u003cp>We ask Nickols, the Arete co-chair, about this framing. He tells us that it is important to keep the quote “in the context of where it was originally formulated.”\\u003c/p>\",\"\\u003cp>“Obviously the idea here is not to equate certain racial groups with animals or anything like that,” Nickols says. “Over time, though, the expanding moral circle idea is that white people who, before, held these extremely racist and terrible views — as the generations went on, and as culture shifted — began to see people, regardless of their race, as all morally equal.”\\u003c/p>\",\"\\u003cp>“We have not reached a point where racism is totally gone,” Nickols says, “but there is definitely a shift in the right direction here. And more generally, the idea is that as time goes on, it is quite possible that the circle will continue to increase.”\\u003c/p>\",\"\\u003ch1>Paperclips\\u003c/h1>\",\"\\u003cp>{shortcode-24643cedbe14221289878261864001a8ceef067a}ne thought experiment designed to demonstrate the danger of misaligned AI goes like this: Say the owner of a paperclip factory obtains an ultrapowerful AI and instructs it to maximize paperclip output. Although the AI is programmed to pursue a seemingly harmless goal, it might — if its understanding of values is not quite the same as ours — turn everything in the world into paper clips. That this scenario seems kind of silly is part of the point. Researchers are not concerned that AI will be “evil” per se, but that its pursuit of any objective, including “good” ones, might have unintended consequences.\\u003c/p>\",\"\\u003cp>“AI systems donʼt always do what their developers intend,” the Arete syllabus reads. “They replicate human biases, achieve their goals in surprising and destructive ways, and are vulnerable to external manipulation.” As a call to action, this is a compelling place to start. As a taxonomy, though, it is less an observation about AI than it is about \\u003cem>systems\\u003c/em>. Existential risks like climate change might first destroy the people who did the least to create them; any movement created by people is, in a sense, only human.\\u003c/p>\",\"\\u003cp>This point came up in the wake of the FTX collapse — what did it mean that a group seeking to fundamentally change the world relied so heavily on existing distributions of power? — and it has come up again in the months since, in the course of writing this article. Can you optimize your life? What if the thing we construct in our idealized image turns out not to look so different from us after all?\\u003c/p>\",\"\\u003cp>For Andrew N. Garber ’23, a former Arete leader, considering questions is the point of EA. There is a common misconception that effective altruism is a destination, when really it’s more of a framework, he tells us: “It is more concerned about the question than any one specific answer.”\\u003c/p>\",\"\\u003cp>In any event, when we ask Jurkovic what he hopes EA will look like in the future, his response is straightforward. “The goal is to help people make the world better,” he says, half smiling. “As much as possible.”\\u003c/p>\",\"\\u003cp>\\u003cem>— Associate Magazine Editor Bea Wall-Feng can be reached at bea.wall-feng@thecrimson.com. Follow them on Twitter \\u003ca href=\\\"https://twitter.com/wallfeng/\\\">@wallfeng\\u003c/a>.\\u003c/em>\\u003c/p>\",\"\\u003cp>\\u003cem>— Magazine writer Sophia C. Scott can be reached at sophia.scott@thecrimson.com. Follow her on Twitter \\u003ca href=\\\"https://twitter.com/ScottSophia_\\\">@ScottSophia_\\u003c/a>.\\u003c/em>\\u003c/p>\"]},\"authorDescript\":\"\",\"bylineType\":\"Crimson Staff Writer\",\"mainTag\":null,\"hasJump\":false,\"textBeforeJump\":\"\\u003cp>{image id=1362611 align=center size=large caption=true byline=true}\\u003c/p>\\r\\n\\r\\n\\u003cp>{dropcap text=W color=AF2234}hen we meet Nikola Jurkovic &rsquo;25 on Zoom, he&rsquo;s sitting in front of a whiteboard covered with equations. We chat about his interest in folk punk music; his headphones push his hair back into a kind of emo swoop. Jurkovic comes across as friendly, but also guarded: He seems to want to make a good impression.\\u003c/p>\\r\\n\\r\\n\\u003cp>Jurkovic tells us he first learned about the philosophical movement called effective altruism from the comments section of a YouTube video about veganism during his gap year. When he moved from Croatia to Harvard, he joined the Harvard Undergraduate EA group and eventually became its president. &ldquo;I think I&rsquo;ve been looking for ways to make the world better for a really long time,&rdquo; he says, &ldquo;I think as far back as I can remember.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>EA, its proponents will tell you, is aimed at &ldquo;doing good better.&rdquo; It starts with the problem that the ways that we approach charity, aid, and other kinds of good-doing are clouded by human biases, and tries to find the best solution using statistical tools. For example, an effective altruist might argue that donating money to foundations that provide malaria treatments maximizes your lives saved per dollar spent.\\u003c/p>\\r\\n\\r\\n\\u003cp>The movement began in the early 2010s as the brainchild of the philosopher Will MacAskill. In the years since, it has flourished in both in-person spaces, like conferences, as well as blogs and forums. Users map out their worldviews in long, technical posts, debating what exactly different EA principles look like in practice, the merits of various criticisms of the movement, how to adapt to the more meritorious criticisms, and how to counter the less meritorious ones.\\u003c/p>\\r\\n\\r\\n\\u003cp>EA has also garnered support from a number of high-profile acolytes.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;This is a close match for my philosophy,&rdquo; Elon Musk \\u003ca href=\\\"https://twitter.com/elonmusk/status/1554335028313718784\\\">tweeted\\u003c/a> last August about MacAskill&rsquo;s new book.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;Effective altruism &mdash; efforts that actually help people rather than making you feel good or helping you show off &mdash; is one of the great new ideas of the 21st century,&rdquo; celebrity academic and Harvard psychology professor Steven Pinker wrote.\\u003c/p>\\r\\n\\r\\n\\u003cp>One of the highest-profile effective altruists was the former billionaire Sam Bankman-Fried, whose commitment to donate much of his wealth to EA causes turned out to be impossible after his cryptocurrency company, FTX, collapsed due to alleged fraud. His commitment also turned out to be disingenuous: In a postmortem interview with Vox, he described his apparent embrace of ethics as &ldquo;this dumb game we woke westerners play where we say all the right shibboleths and so everyone likes us.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>Nevertheless, EA&rsquo;s profile has continued to rise. Even at Harvard, some students find the pull inescapable: &ldquo;Effective altruism is a \\u003cem>huge\\u003c/em> trend on campus, seeping into everything,&rdquo; Henry Haimo &rsquo;24 told the New Yorker in March, in an \\u003ca href=\\\"https://www.newyorker.com/magazine/2023/03/06/the-end-of-the-english-major\\\">article\\u003c/a> ostensibly about the decline of the humanities.\\u003c/p>\\r\\n\\r\\n\\u003cp>When Harvard&rsquo;s undergraduate EA group started on campus a decade ago, though, EA was not so widespread. At first, getting students to join was a challenge. &ldquo;I realized that the idea of HEA seemed crazy: &lsquo;join us and we&rsquo;ll try to figure out how to maximize the good we do in the world!&rsquo;&rdquo; Ben S. Kuhn &rsquo;15, the co-president at the time, wrote in a 2014 blog post. &ldquo;But my mistake was \\u003cem>letting other people know\\u003c/em> that I knew HEA seemed crazy. As soon as they realized that I myself felt goofy, it was game over for convincing them to get involved.&rdquo; He described expanding the group&rsquo;s influence through a combination of savvy advertising tactics and a slate of famous speakers such as the philosopher Peter Singer, one of the most prominent intellectuals associated with the movement.\\u003c/p>\\r\\n\\r\\n\\u003cp>{image id=1362617 align=center size=large caption=true byline=true}\\u003c/p>\\r\\n\\r\\n\\u003cp>Today, there are several different EA groups across the University, including at the College, law school, and GSAS, as well as in the Boston area. (In this article, &ldquo;Harvard EA&rdquo; refers to the undergraduate group unless otherwise specified.) Jurkovic guesses there are &ldquo;between 20 and 50&rdquo; students involved in Harvard EA&rsquo;s regular programming; there are dozens more in its introductory fellowships and hundreds more on the mailing lists.\\u003c/p>\\r\\n\\r\\n\\u003cp>Harvard markets itself as &ldquo;developing leaders who make a difference globally,&rdquo; and pop culture spins this principle into myth: Here is a university whose students will go on to save the world. EA is a movement that claims to know how to do it, from shifts in diet &mdash; a lot of the people we meet are vegan, in recognition of the statistically unmatched suffering that factory farming inflicts on chickens &mdash; to shifts in career, like the students who have devoted their futures to trying to prevent human extinction by AI.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;Most of us want to improve the world,&rdquo; the Harvard EA website says. &ldquo;But figuring out what that &lsquo;something&rsquo; is, let alone actually doing it, can be a difficult and disheartening challenge.\\u003ca href=\\\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism/\\\"> Effective altruism\\u003c/a> is a response to this challenge.&rdquo; Can it live up to that goal?\\u003c/p>\\r\\n\\r\\n\\u003ch1>Counterfactual Impact\\u003c/h1>\\r\\n\\r\\n\\u003cp>{dropcap text=I color=AF2234}f you are not already a committed effective altruist &mdash; and if you are interested in a discussion group that zigzags from deworming methodology to Bayesian statistics to the number of animals killed for food in the U.S. in the last 30 seconds &mdash; your first involvement with EA at Harvard might come through the introductory Arete Fellowship, a seven-week program that explores different effective altruism topics every week.\\u003c/p>\\r\\n\\r\\n\\u003cp>Daniela R. Shuman &rsquo;24, a Computer Science and Statistics concentrator interested in urban development, is one of the Arete chairs. She did the fellowship herself a year ago and &ldquo;fell in love with the whole concept,&rdquo; she says. She guesses there were around 40 fellows last fall.\\u003c/p>\\r\\n\\r\\n\\u003cp>We ask Will A. Nickols &rsquo;24, the other chair, if we can sit in on one of the sections. After some back-and-forth, Nickols suggests we attend the second week of the fellowship, which focuses on global health.\\u003c/p>\\r\\n\\r\\n\\u003cp>We get there before the meeting room opens. The discussion leaders, Nick C. Gabrieli &rsquo;24 and Jorge O. Guerra Jr. &rsquo;24, a former Crimson News editor, are sitting in near-darkness on the stairs outside the Adams House Upper Common Room, chatting: &ldquo;What else are you up to this semester?&rdquo; &ldquo;What do you see as the limits of randomized control trials?&rdquo; The vaulted blue ceiling looks like the night sky.\\u003c/p>\\r\\n\\r\\n\\u003cp>Claire Guo &rsquo;24, one of the Arete fellows and a former Crimson News editor, wanders up wearing two baseball caps; she wants to know if the leaders are participating in the junior class-wide game of Assassins.\\u003c/p>\\r\\n\\r\\n\\u003cp>No, Gabrieli tells her, adding, &ldquo;I feel like I&rsquo;m in the market for developing more hobbies.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>At 6 p.m., we file into the meeting room and assign ourselves to armchairs. There are three fellows in attendance, and another will show up halfway through. Gabrieli and Guerra walk us through the vocabulary terms from the readings &mdash; two blog posts, a lecture by an Oxford Professor, a TED talk, and several charts about life expectancy in different countries &mdash; asking the group to define and react to them.\\u003c/p>\\r\\n\\r\\n\\u003cp>This week, the discussion involves identifying your &ldquo;problem&rdquo;: So you&rsquo;ve already decided you want to do good. How do you figure out \\u003cem>what\\u003c/em> to do?\\u003c/p>\\r\\n\\r\\n\\u003cp>EA takes cues from rationalism, a commitment to logic rather than feelings as a basis for decision-making. For the Arete fellows, this requires learning a lot of terminology. For example, the &ldquo;counterfactual impact&rdquo; of an action is the result of doing that action, relative to the result of not doing it. Say you want to volunteer in a soup kitchen &mdash; a lot of EA readings use this example &mdash; but there are plenty of other volunteers; if you don&rsquo;t do it, someone else will. Unless you are extremely good at serving soup, the amount of soup served in a world where you are a server is probably not that different from the amount of soup served in a world where you are not, and you might want to focus your altruistic intentions elsewhere.\\u003c/p>\\r\\n\\r\\n\\u003cp>{image id=1343308 align=center size=large caption=&quot;The Arete Fellowship discussion section takes place in the Adams House Upper Common Room. The Arete Fellows learn a lot of terminology to help them think about how they can do the most good.&quot; byline=true}\\u003c/p>\\r\\n\\r\\n\\u003cp>At the discussion leaders&rsquo; prompting, the fellows &mdash; Guo; Nathanael Tjandra &rsquo;26; and Kai C. Hostin &rsquo;25 &mdash; talk among themselves, trying to recall concepts from the readings. More than anything, it feels like a class section.\\u003c/p>\\r\\n\\r\\n\\u003cp>Gabrieli asks, &ldquo;What is &lsquo;importance?&rsquo;&rdquo; Guo and Hostin look at each other with uncertainty. &ldquo;It should be able to be inferred, I think, from the word itself,&rdquo; he continues. &ldquo;What is the absolute magnitude of the thing we&rsquo;re interested in?&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>There are a lot of math terms, too. One of the readings constructs a complicated-looking product of derivatives to make the conceptual point that importance, tractability, and neglectedness are all vital considerations. Importance is how important a problem is; tractability is how much good can be done relative to effort; neglectedness is how much other people are already working in that area.\\u003c/p>\\r\\n\\r\\n\\u003cp>Still, the fellows have questions, and Gabrieli and Guerra are happy to discuss them. Should your personal interests play a role in what you decide to focus on? What about problems that require immediate responses, rather than careful calculation of impact? They talk but don&rsquo;t come to any singular answer.\\u003c/p>\\r\\n\\r\\n\\u003cp>Near the end of the hour, Guerra brings up GiveWell, a well-known EA organization that maintains a ranked list of recommended charities. He asks the group: Of the top four causes on the website, which one would you donate to, and why?\\u003c/p>\\r\\n\\r\\n\\u003cp>First on the list is Malaria Consortium, which provides a kind of medicine called seasonal malaria chemoprevention. Malaria treatments are high-importance, and tractable because they are relatively cheap; GiveWell estimates that it costs $7 to protect a child from malaria. Second on the list is the Against Malaria Foundation, which provides malaria nets for about $5 each.\\u003c/p>\\r\\n\\r\\n\\u003cp>Tjandra, a Crimson Multimedia editor, ultimately chooses the fourth-ranked charity, which provides cash incentives to caregivers in Nigeria who vaccinate their babies. Unlike malaria treatments, he reasons, vaccines are &ldquo;more generalizable to poor people everywhere.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>Guo asks if instead of providing Vitamin A supplements to areas in sub-Saharan Africa, as the third-ranked charity does, we could try to integrate vegetables rich in Vitamin A into those communities. &ldquo;&lsquo;Eat some yams,&rsquo;&rdquo; she says, laughing.\\u003c/p>\\r\\n\\r\\n\\u003cp>Hostin concurs: &ldquo;I don&rsquo;t want to be like &lsquo;Oh, here&rsquo;s a fish&rsquo; rather than teach them how to fish, in a way.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003ch1>A One in Ten Risk of Extinction\\u003c/h1>\\r\\n\\r\\n\\u003cp>{dropcap text=A color=AF2234}t the end of the Arete fellowship, you are given a $10 gift card to the charity donation site Every.org. If you donate $10 of your own money, you receive another gift card. But what happens next is up to you: Since Harvard EA has no centralized meetings or agenda, continued membership means &ldquo;joining the various side groups that we have, depending on what you&rsquo;re interested in,&rdquo; Shuman says.\\u003c/p>\\r\\n\\r\\n\\u003cp>Donating large parts of your income to charity &mdash; or earning to give, in the style of Bankman-Fried &mdash; are largely out of reach for college students, Jurkovic tells us. Instead, we find out, undergrad EA groups tend to focus on research and recruitment.\\u003c/p>\\r\\n\\r\\n\\u003cp>Still, the issues Harvard EA focuses on don&rsquo;t always line up with the picture of EA suggested by the Arete fellowship. Zazie Huml &rsquo;25, Harvard EA&rsquo;s Global Health Programming Lead and one of four people on its board, said when they joined Harvard EA there was &ldquo;no major initiative&rdquo; in global health or international development &mdash; two of the five major topics covered in the Arete fellowship &mdash; and there were only &ldquo;a couple people&rdquo; involved with animal rights, a third focus area.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;In Harvard EA we try not to present unfairly biased opinions towards any particular world problem,&rdquo; Jurkovic says. &ldquo;We aim to present the facts about the world problems and also give people useful decision-making tools so that they can examine the facts themselves.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;My entire experience with EA at Harvard last semester was, &lsquo;Oh, this is not for me, this is not my community. They&rsquo;re not interested in the same things as me,&rsquo;&rdquo; Huml says, until someone they met in the organization encouraged them to take another look. &ldquo;If I was to take initiative within the system, there would be resources to support me,&rdquo; they recall the person saying. Huml has since led several global health initiatives under the umbrella of Harvard EA, including a &ldquo;comprehensive study on source apportionment of lead exposure&rdquo; in lower-income countries in partnership with the Lead Exposure Elimination Project.\\u003c/p>\\r\\n\\r\\n\\u003cp>So what was Harvard EA doing? &ldquo;It was only focused on longtermism and AI,&rdquo; Huml remembers.\\u003c/p>\\r\\n\\r\\n\\u003cp>Longtermism &mdash; week five on the Arete syllabus &mdash; is the view that people who will be alive in the future warrant the same moral consideration as people alive today. &ldquo;If we want to do the most good, that means we want to help the most people,&rdquo; Shuman says, &ldquo;and the most people is not at the specific time that we&rsquo;re living in.&rdquo; Since future people will significantly outnumber today&#39;s people, barring a mass extinction event, longtermists argue that we should devote more resources to preventing &ldquo;existential risks&rdquo; like nuclear warfare or engineered pandemics.\\u003c/p>\\r\\n\\r\\n\\u003cp>This is something of a departure from EA principles in other areas, which our interviewees explain are a refinement of natural human instincts: You already want to do good, and EA just teaches you how to be smart about it. For a longtermist, though, what is at stake is the future of human existence.\\u003c/p>\\r\\n\\r\\n\\u003cp>In recent years, longtermists have turned their attention to the field of AI safety. As AI models grow increasingly powerful, EA researchers have argued, the existential threat they pose may become insurmountable. They call this the problem of &ldquo;AI alignment&rdquo;: ensuring that if or when a superhuman AI comes into existence, its values align with our own, so that it does not kill everyone on Earth.\\u003c/p>\\r\\n\\r\\n\\u003cp>Some AI safety researchers have been sounding this alarm for decades &mdash; but this past year, thanks to the rise of shockingly powerful, publicly available AI models like \\u003ca href=\\\"https://www.thecrimson.com/article/2023/2/23/chatgpt-scrut/\\\">ChatGPT\\u003c/a>, the issue has made it into the mainstream. ChatGPT is already prone to spreading misinformation. A &ldquo;sufficiently powerful&rdquo; AI could be much worse, researcher Elizier Yudkowsky argued in a recent \\u003ca href=\\\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\\\">Time magazine op-ed\\u003c/a> calling for an indefinite moratorium on AI development. &ldquo;In today&rsquo;s world you can email DNA strings to laboratories that will produce proteins on demand allowing an AI initially confined to the internet to build artificial life forms or bootstrap straight to postbiological molecular manufacturing,&rdquo; he wrote.\\u003c/p>\\r\\n\\r\\n\\u003cp>How do you quantify incalculable destruction? &ldquo;The Precipice: Existential Risk and the Future of Humanity,&rdquo; a book by Toby Ord about existential risk, for which Harvard EA runs a reading group, puts the risk of extinction in the next century from unaligned AI at 1 in 10, higher than any other source of risk. Last July, Jurkovic wrote in a comment on the EA forum that &ldquo;existential risks are high up on the list of most probable causes of death for college aged-people&rdquo;: Assume that the probability of achieving superhuman AI by 2045 is 50 percent, and assume that the probability of death given superhuman AI is at least 10 percent. Then the probability of death by AI in the next few years might be comparable to around 1 in 6000, he wrote, explaining that this probability is similar to the two largest causes of death for &ldquo;college-aged people in the US,&rdquo; suicide and vehicle accidents, although he did not write out the calculations leading to this conclusion.\\u003c/p>\\r\\n\\r\\n\\u003cp>Jurkovic guesses that there are more people in Harvard EA working on AI than other problems, but points out the existence of organizations with other EA-related focuses, including Harvard College Animal Advocates.\\u003c/p>\\r\\n\\r\\n\\u003cp>Harvard EA doesn&rsquo;t have an AI safety program itself, but there are several related organizations that do. One of these is the Harvard AI Safety Team, which was founded in 2022 by Xander L. Davies &rsquo;23 and Fiona E. Pollack &rsquo;25. Though HAIST is not an EA organization, many members of Harvard EA work with HAIST, and HAIST also receives funding from national EA philanthropy organizations. HAIST and Harvard EA also share members with other local AI groups, including Cambridge Boston Alignment Initiative and MIT AI Alignment.\\u003c/p>\\r\\n\\r\\n\\u003cp>HAIST hosts reading groups and talks by professors, with a focus on machine learning research, Davies tells us on Zoom. He has curly hair and a black hoodie; the &ldquo;L&rdquo; in his name stands for Laser. Although he doesn&rsquo;t believe that AI right now is poised to destroy humanity, he also doesn&rsquo;t believe we have the tools to stop it. &ldquo;I think how I look at it is, it&rsquo;s currently impossible to get our AIs to not do things,&rdquo; Davies says, referring to the ease with which users have bypassed ChatGPT-like models&rsquo; built-in filters against violent speech and misinformation.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;Rapid progress in AI is becoming more and more economically useful, becoming more and more trusted, while at the same time this stark lack of progress on actually understanding how these systems work, on getting confidence that we actually know how to make these systems do what we want, is very startling to me,&rdquo; Davies says. &ldquo;And I think it should be a core priority on the global stage.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003ch1>&lsquo;The Warm Fuzzy Feeling Just Doesn&rsquo;t Matter as Much&rsquo;\\u003c/h1>\\r\\n\\r\\n\\u003cp>{dropcap text=I color=AF2234}n some respects, EA seems fairly intuitive: Who doesn&rsquo;t want to minimize suffering as much as possible? In other respects, it pushes you to rethink your intuitions.\\u003c/p>\\r\\n\\r\\n\\u003cp>Take, for instance, a thought experiment effective altruists often use to illustrate the unique way this philosophy navigates moral quandaries. It&rsquo;s called &ldquo;\\u003ca href=\\\"https://www.harvardea.org/blog/2022/lena-ashooh-the-child-in-the-pond\\\">the drowning child scenario\\u003c/a>,&rdquo; originally formulated by Peter Singer. Imagine you&rsquo;re on your way to an important event, and you notice a child is drowning in a nearby pond. Do you jump in to save the child?\\u003c/p>\\r\\n\\r\\n\\u003cp>Barring circumstances like an inability to swim, most people answer yes. But then, you&rsquo;re asked the question again and again. Each time, the stakes are higher: What if you will ruin your clothes and waterlog your phone by jumping in? What if you already saved a drowning child last week? What if this child was an undocumented immigrant? What if the pond was far enough away that you would have to spend gas money to get there?\\u003c/p>\\r\\n\\r\\n\\u003cp>As the hypotheticals escalate, generally, participants continue to decide to save the drowning child each time. But in the real world, when facing situations like determining healthcare access for immigrants or people in other countries, the increased distance leads some people to make what is in effect the opposite choice. EA wants to know: Why should you value those peoples&rsquo; lives less than those of people closer to you?\\u003c/p>\\r\\n\\r\\n\\u003cp>When we spoke to Marka F. X. Ellertson &rsquo;23, then the president of Harvard EA, last September, she told us that with EA efforts, &ldquo;the warm fuzzy feeling just doesn&rsquo;t matter as much to me as the rational thought that I know that I&rsquo;ve had a bigger impact.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;And I actually still do want that warm fuzzy feeling,&rdquo; Ellertson added, explaining that she donates to local causes that are particularly meaningful to her.\\u003c/p>\\r\\n\\r\\n\\u003cp>Joshua D. Greene &rsquo;97, Harvard EA&rsquo;s faculty advisor, disputes the idea that EA strips away the warmth of charity work.\\u003c/p>\\r\\n\\r\\n\\u003cp>Utilitarianism might make you think of &ldquo;the things that kind of serve a function but don&rsquo;t nurture our souls or to speak to our heart&rsquo;s greatest desires, right? And utilitarianism is not just about cold functionality,&rdquo; he says. &ldquo;It&rsquo;s about everything that makes life good or bad, everything that makes life worth living, everything that makes life meaningful.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>Harold H. Klapper &rsquo;25, who participated in a Harvard EA fellowship last year, tells us that in some EA dialogues about utilitarianism &ldquo;get really wild.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>At a Boston-area EA event, for instance, &ldquo;I&rsquo;ve had conversations arguing about whether we should kill all wild animals, because they have negative lives,&rdquo; Klapper says. &ldquo;An ant colony must just have negative utility in the sense that they&rsquo;re just not enjoying life, and so it&rsquo;d be better if we just eliminated them.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;When things are a movement, you kind of have to buy into the whole thing, and when you buy into the whole thing, you get really wacky and fucked up answers to problems,&rdquo; Klapper adds.\\u003c/p>\\r\\n\\r\\n\\u003cp>Effective altruists seek to apply EA principles to personal decisions: what to study, where to work. If you are a college student interested in building EA communities, you might &ldquo;consider not going to Harvard, as there are a bunch of people there doing great things,&rdquo; Jurkovic wrote on the EA forum in December, suggesting that going to other colleges without strong EA movements could be better. (Was this something Jurkovic himself considered when applying to Harvard? &ldquo;No,&rdquo; he says, laughing.)\\u003c/p>\\r\\n\\r\\n\\u003cp>A lot of EA discourse revolves around career choice: You will probably work for around 80,000 hours in your lifetime &mdash; several of the people we talk to cite this estimate &mdash; and you should spend them doing things that count, even if they may not be things you enjoy.\\u003c/p>\\r\\n\\r\\n\\u003cp>Harvard EA, Shuman says, focuses mainly on &ldquo;getting high-potential individuals into careers where they can spend their 80,000 hours of their career on solving these issues.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>{image id=1362618 align=center size=large caption=true byline=true}\\u003c/p>\\r\\n\\r\\n\\u003cp>To get them to that point, EA might also influence Harvard students&rsquo; concentration decisions. One such student is Jōsh P. Mysor&eacute; &rsquo;26; when we meet him outside of Blackbird, he&rsquo;s reading Giovanni Boccaccio&rsquo;s &ldquo;The Decameron&rdquo; for class. Mysor&eacute; completed the Arete fellowship last fall and is considering becoming a discussion leader at some point in the future.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;I love poetry. I loooove poetry,&rdquo; he tells us. &ldquo;Will I be going into poetry? No. Because I don&rsquo;t think it will actually do good for people.&rdquo; At the moment, Mysor&eacute; plans to concentrate in Computer Science and Linguistics.\\u003c/p>\\r\\n\\r\\n\\u003cp>Computer Science, as well as Statistics and Applied Mathematics, are fairly common concentration choices among the people we meet. Klapper tells us he knows someone in Harvard EA who studies Computer Science and dislikes it, but continues in the field because they believe it&rsquo;s the most effective use of their time.\\u003c/p>\\r\\n\\r\\n\\u003cp>Mysor&eacute; &ldquo;was given a certain amount of privilege in my life to even get to this point,&rdquo; he tells us. &ldquo;I do think I owe something to the greater good of humanity to do something that impacts more people in a tangible way.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>Does he think anyone should go into poetry, we ask.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;I don&#39;t think it&rsquo;s a contradiction to say that I can hold two opposing viewpoints at the same time,&rdquo; Mysor&eacute; says. &ldquo;Like in my heart, I&rsquo;m a humanist, and I&rsquo;m very romantic.&rdquo; He tells us that he joined EA specifically to challenge these humanist viewpoints, but his perspective might flip again. &ldquo;Honestly, I do think there should be poets,&rdquo; he says.\\u003c/p>\\r\\n\\r\\n\\u003cp>Mysor&eacute; tells us he still believes that EA has a noble mission, even if he disagrees with some of its particular approaches. &ldquo;I think at the baseline, EA is creating dialogue,&rdquo; he says. &ldquo;That is really what counts.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003ch1>&lsquo;Not To Create a Club, But Rather to Create a World&rsquo;\\u003c/h1>\\r\\n\\r\\n\\u003cp>{dropcap text=O color=AF2234}n Saturdays, Harvard EA throws socials in a house near campus where Jurkovic lives with four of his friends. Half the time, the socials are just for Harvard affiliates; every other week, they are open to students from other Boston-area schools.\\u003c/p>\\r\\n\\r\\n\\u003cp>Of course, the socials are designed to be fun, but they have a functional purpose as well. &ldquo;One important part of having a community is that the people talk to each other and have time spent together, so that they can collaborate and talk about their projects,&rdquo; Jurkovic says.\\u003c/p>\\r\\n\\r\\n\\u003cp>Though Jurkovic declined our request to attend a social on the record, we can try to reconstruct the vibe from a guide that he posted on the EA forum called &ldquo;How to Organize a Social.&rdquo; Indeed, in the post, he records every step of preparing for a social in granular detail, providing recommendations for everything from grocery lists &mdash; CLIF Bars, Diet Coke, several varieties of Vitaminwater &mdash; to music, such as the Spotify-curated playlist &ldquo;my life is a movie.&rdquo; Jurkovic suggests you make it easy for guests to find answers to anticipated questions: &ldquo;The shoes on/off policy? Where the bathroom is? Where one can get water? What the wi-fi password is?&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>Last year, Trevor J. Levin &rsquo;19, who is currently on leave as the co-president of the university-wide EA group, also created a list of recommendations for effective retreats: They should happen in the beginning of the semester, when people are less busy; include lots of time for one-on-one interactions and a &ldquo;structured vulnerable/emotional thing&rdquo;; and include a healthy mix of new recruits and &ldquo;moderately charismatic people for whom EA is a major consideration in how they make decisions.&rdquo; These suggestions were embedded in a long post, which, citing feedback from Ellertson, Davies, Jurkovic, and others, argues that college EA groups should focus more on retreats as a method of bonding.\\u003c/p>\\r\\n\\r\\n\\u003cp>For Levin, a former Crimson editor, this kind of immersive social situation is vital to capturing those who might be interested in EA but don&rsquo;t prioritize it.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;While most of the important cognition that happens is social/emotional, this is not the same thing as tricking or manipulating people into being EAs,&rdquo; he wrote on the forum. Instead, retreats are meant to appeal to those who may agree with EA on some level but have not yet acted on it, and giving them time to &ldquo;move closer to the values they \\u003cem>previously already wanted to live by\\u003c/em>.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>Since EA was born, it has been very deliberate about the image it projects. The name &ldquo;effective altruism&rdquo; was itself the product of a long debate: &ldquo;This has been such an obstacle in the utilitarianesque community &mdash; &lsquo;do-gooder&rsquo; is the current term, and it sucks,&rdquo; MacAskill, the philosopher, wrote in a 2011 email chain. What followed was a period of brainstorming &mdash; fusing terms like &ldquo;utilitarian&rdquo; and philanthropist&rdquo; with &ldquo;alliance&rdquo; and &ldquo;institute&rdquo; &mdash; and a series of votes to establish a name for both &ldquo;the type of person we wanted to refer to, and for the name of the organization we were setting up.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>Now, countless blogs and forum posts are dedicated to determining how best to recruit new members to the EA community. In December 2021, for instance, Jurkovic wrote a post on the EA forum describing an &ldquo;organic&rdquo; way to pitch EA to students.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;Person: What do you want to study? Me: Not sure, I&rsquo;m trying to find what to study so I can have as good of an impact as possible,&rdquo; he wrote in an example dialogue. &ldquo;If their level of enthusiasm stays high or grows, pitch an intro fellowship or a reading group to them.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>{image id=1362612 align=center size=large caption=true byline=true}\\u003c/p>\\r\\n\\r\\n\\u003cp>Even if some people choose not to become effective altruists, Shuman tells us, they could still take away valuable ideas from the movement.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;The point is not to create a club, but rather to create a world of people that want to do the most good, and EA just has a set of tools that it thinks are probably the most good,&rdquo; Shuman says. &ldquo;We want everybody to think in these terms.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003ch1>Daedalus House\\u003c/h1>\\r\\n\\r\\n\\u003cp>{dropcap text=I color=AF2234}&rsquo;m gonna talk from a removed, omniscient perspective,&rdquo; Mysor&eacute; says, kicking his chair back and folding his arms behind his head. EA spends a lot of money on space, food, and socials, he tells us. &ldquo;At a certain point you have to ask yourself: What is effective about that?&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>Most of Harvard EA&rsquo;s money comes from larger EA organizations like Open Philanthropy, a grantmaking foundation largely financed by Cari Tuna and Dustin Moskovitz, the latter of whom co-founded Facebook. Open Philanthropy distributes money to a range of EA-related causes. Put simply, it is an organization that &ldquo;cares about making the world better,&rdquo; Jurkovic says.\\u003c/p>\\r\\n\\r\\n\\u003cp>We ask him how Harvard EA uses its grant money.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;It&rsquo;s not my area of expertise,&rdquo; Jurkovic says. &ldquo;But ...&rdquo; He pauses for 15 seconds. &ldquo;Yeah, just sometimes we get funding for club activities.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>In 2022, we later find out, part of an Open Philanthropy grant was used to send Arete fellows and the University-wide EA group on a weekend trip to Essex Woods, a serene, Thoreauesque venue an hour north of campus that charges about $5,000 per night. According to GiveWell, donating $10,000 to the nonprofit Malaria Consortium could save the lives of five people.\\u003c/p>\\r\\n\\r\\n\\u003cp>The schedule was similar to that of a corporate retreat: workshops, games, dinner, hot tub, Hamming circles. Well, maybe not the last one. Hamming circles are an activity where three to five participants sit down together and talk through one problem facing each member in 20-minute chunks. It&rsquo;s &ldquo;similar to what happens in a pair debug,&rdquo; a post on an EA-related forum explains. These problems might vary, the post says, from &ldquo;Is it possible for me, specifically, to have a meaningful impact on existential risk&rdquo; to &ldquo;I need to secure $250,000 in seed funding for my startup&rdquo; to &ldquo;I&rsquo;m expected to speak at my father&rsquo;s funeral and I have nothing but scathing, bitter, angry things to say.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>Open Philanthropy also issued a $250,000 grant for the Centre for Effective Altruism to &ldquo;rent and refurbish&rdquo; an office for the Harvard AI Safety Team in Harvard Square for one year.\\u003c/p>\\r\\n\\r\\n\\u003cp>Davies, the HAIST co-founder, tells us that the HAIST office is &ldquo;pretty research-y.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;People are often at whiteboards, talking about problems with each other,&rdquo; he says. &ldquo;I think it feels like people are really trying to make progress on this technical problem, which I find exciting. It&rsquo;s maybe a little startup-y in vibe.&rdquo; In a post on the EA forum in December, Davies wrote that &ldquo;investing effort into making the space fun and convenient to use helped improve programming, social events, and sense of community.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>In August, Open Philanthropy recommended an $8.9 million grant for the Center for Effective Altruism to lease an EA office space for five years in Harvard Square. While the space would have been unaffiliated with Harvard EA, a forum post announcing the office promised that part of it would contain &ldquo;meeting spaces for students at Harvard and other Boston-area schools,&rdquo; and thanked Levin and Jurkovic for their help in developing the project.\\u003c/p>\\r\\n\\r\\n\\u003cp>Forum members, including Levin and Jurkovic, threw out potential names for the space. Some of the suggestions were mythological &mdash; &ldquo;Daedalus,&rdquo; who advised Icarus not to fly too close to the sun &mdash; some cosmological &mdash; &ldquo;Supercluster,&rdquo; &ldquo;Earthrise&rdquo; &mdash; and some silly, like &ldquo;Aardvark,&rdquo; from a user who argued the name sounded similar to &ldquo;Harvard&rdquo; and would show up first in alphabetical lists.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;Don&rsquo;t like including the actual words EA in the name of the space,&rdquo; Levin (who, for his part, liked &ldquo;Apollo&rdquo;) wrote in the comments. &ldquo;It increases the chances of hypocrisy charges (from people who haven&rsquo;t thought much about the effects of nice offices on productivity) for getting a nice central office space while ostensibly being altruistic.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>But the Apollo House &mdash; or Aardvark House, or Supercluster &mdash; never materialized. According to Levin, after CEA signed the lease and began preparing the space, Open Philanthropy notified them that the grant was under review. As a result, Levin tells us, CEA is now trying to sublease the space to get the money back.\\u003c/p>\\r\\n\\r\\n\\u003cp>CEA and Open Philanthropy did not respond to questions about the current status of the Harvard Square office grant.\\u003c/p>\\r\\n\\r\\n\\u003cp>In addition to money for spaces and retreats, Open Philanthropy has an open request form for university group funding, and regularly recommends grants to undergraduate organizers. Other EA-affiliated organizations also fund events and projects. Huml, the Global Health Programming Lead, tells us that this is part of what makes EA a valuable community in which to pursue global health work.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;To be totally transparent, I don&rsquo;t 100 percent align with the values,&rdquo; Huml tells us. &ldquo;I think that they are an incredible platform and have a lot of resources &mdash; and those resources are financial, they are access to experts in very specific fields.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>Three Harvard students, including Davies and Gabrieli, were recipients of Open Philanthropy&rsquo;s fall 2022 University Organizer Fellowship, for which the organization recommended a total of $3.1 million across 116 recipients. Gabrieli declined to be interviewed for this article. Davies says he doesn&rsquo;t know if he&rsquo;s allowed to disclose how much money he actually got, but that he considers the grant to be &ldquo;an hourly wage,&rdquo; since he quit previous jobs to focus on developing HAIST.\\u003c/p>\\r\\n\\r\\n\\u003cp>In February 2022, Open Philanthropy recommended a $75,000 grant to Pollack, the other HAIST co-founder, &ldquo;to support her work organizing the effective altruism community at Harvard University.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>When we reach out to Pollack, she tells us over email that she is &ldquo;no longer organizing for the Harvard Effective Altruism group,&rdquo; but has spent about $14,000 of the grant on HAIST expenses with Open Philanthropy&rsquo;s approval: $7,200 to monthly software costs like Airtable and Squarespace, and most of the rest to accommodations for a workshop that HAIST hosted with the MIT AI safety group in Arlington, Virginia.\\u003c/p>\\r\\n\\r\\n\\u003cp>Harvard EA is aware that this allocation of money can appear at odds with its stated mission. After the Essex Woods retreat, organizers sent out a feedback form. &ldquo;How much did the spending of money at this retreat make you feel uncomfortable?&rdquo; one question asked.\\u003c/p>\\r\\n\\r\\n\\u003cp>We talk to Levin, the University EA co-president, and he likens it to the way that companies spend money on recruitment. &ldquo;The idea is that there are problems that are much more talent-constrained than money-constrained,&rdquo; he tells us. AI safety, a problem that relatively few people are working on, is an extreme example of this, he says. &ldquo;The question then becomes, &lsquo;Okay, well, if we have money and not people, how do we convert between the two?&rsquo;&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>Levin pauses and corrects himself: &ldquo;My train of thought there sounded kind of like I was saying, well, if you have a bunch of money, what do you do with it, right? That is not what I think.&rdquo; What he does believe is that physical environments like retreats can rapidly accelerate the rate &mdash; by up to 100 times, he writes on the forums &mdash; at which people get on board with EA principles.\\u003c/p>\\r\\n\\r\\n\\u003cp>Several people in EA, Levin guesses, joined because of their experiences on a retreat. &ldquo;That is absolutely something that we would have paid this money for,&rdquo; he says.\\u003c/p>\\r\\n\\r\\n\\u003cp>Shuman believes that this outreach is particularly effective in Cambridge because of its highly motivated, change-driven student body. &ldquo;Harvard and MIT have done the vast majority of vetting for people who are highly ambitious,&rdquo; she says.\\u003c/p>\\r\\n\\r\\n\\u003cp>Shuman tells us that the international EA community places a lot of importance on this kind of community building. &ldquo;If they can invest $1,000 in getting five high-potential individuals to, instead of doing AI research, do AI safety research, that&rsquo;s a pretty good use of money,&rdquo; she says. &ldquo;It could save a lot of lives, potentially.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003ch1>&lsquo;A Skewed Pipeline&rsquo;\\u003c/h1>\\r\\n\\r\\n\\u003cp>{dropcap text=I color=AF2234}n a 2020 survey of EA Forum members, 76 percent of respondents were white and 71 percent were male. Though there is an imbalance, the Centre for Effective Altruism argues that diversity is important for several reasons.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;If an EA-aligned newcomer concludes that effective altruism is not for &lsquo;people like me,&rsquo; they may not get involved, and the EA community may be less effective,&rdquo; its website reads. &ldquo;We don&rsquo;t want to miss important perspectives.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>We ask Jurkovic if he&rsquo;s aware of demographic imbalances within EA groups at Harvard. He pauses. &ldquo;I think it is quite important to have a community which is welcoming to everyone,&rdquo; he says. &ldquo;EA sometimes shares a problem with the cause areas that it tackles&rdquo; &mdash; meaning STEM fields &mdash; &ldquo;which is that many of them have more males in them than average.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>Shuman &mdash; &ldquo;the only she/her&rdquo; leading an Arete section, she tells us &mdash; echoes this sentiment, saying that these numbers reflect existing disparities in STEM and philanthropic fields.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;It&rsquo;s just a skewed pipeline,&rdquo; she says, &ldquo;which is a problem.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;I can say from personal experience that we&rsquo;ve had quite diverse groups,&rdquo; Nickols says. &ldquo;In terms of gender, it does tend to be more male-skewed, and that&rsquo;s something that we&rsquo;re continually working on.&rdquo; He acknowledges that Harvard EA &ldquo;is probably predominantly white and Asian, but not more so to Harvard&rsquo;s general population.&rdquo; (The organization does not keep demographic records of its members, so we can&rsquo;t verify this.)\\u003c/p>\\r\\n\\r\\n\\u003cp>Nickols says that the applicants for Harvard EA&rsquo;s fellowships also tend to skew male. &ldquo;Given that word of mouth is our biggest kind of spreader, it might just be possible that guys who have done it in the past are friends with more guys and tell them about it,&rdquo; he adds.\\u003c/p>\\r\\n\\r\\n\\u003cp>In recent months, the EA movement has been embroiled in controversies related to race and gender in its communities.\\u003c/p>\\r\\n\\r\\n\\u003cp>One of these controversies revolved around Nick Bostrom, a philosopher whose ideas led to the development of longtermism; four of his works are cited in the syllabus for Harvard EA&rsquo;s Precipice Fellowship.\\u003c/p>\\r\\n\\r\\n\\u003cp>In January, Bostrom posted a letter to his website apologizing for a comment he wrote on a forum in the mid-90s, which claims that Black people &ldquo;are more stupid than whites&rdquo; and contains the n-word. In the letter, Bostrom castigates his past self for using the slur and writes that the comment &ldquo;does not accurately represent my views, then or now,&rdquo; but does not reject the possibility of genetic cognitive differences between races. He leaves this question to &ldquo;others, who have more relevant knowledge.&rdquo; The letter continues with a section about bioethics that opens: &ldquo;What about eugenics? Do I support eugenics? No, not as the term is commonly understood.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>In March, Time magazine \\u003ca href=\\\"https://time.com/6252617/effective-altruism-sexual-harassment/\\\"> interviewed seven women\\u003c/a> who said they had been sexually harassed, coerced, or assaulted within EA spaces, particularly in the Bay Area. The scene&rsquo;s overwhelming maleness, tech-bro culture, and impulse to quantify and rationalize messy real-world dynamics created a deeply unsafe environment, the women said. One described having dinner with a prominent researcher nearly twice her age who told her that &ldquo;pedophilic relationships between very young women and older men was a good way to transfer knowledge.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;We were of course upset by both of these issues,&rdquo; Jurkovic wrote in an email to us about the Bostrom letter and Time investigation, &ldquo;and have spent time figuring out how we can improve our diversity and make sure we&#39;re a welcoming community to women and people of color.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>{image id=1362613 align=center size=large caption=true byline=true}\\u003c/p>\\r\\n\\r\\n\\u003cp>Although some of EA&rsquo;s focus areas deal with global health and economic growth in underdeveloped countries, its frameworks generally do not foreground race or gender. A version of the spring 2023 Arete syllabus posted on the Harvard EA website only mentions race in the overview of Week Four: Animal Welfare.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;One of the most important ways we can fail to identify the most important moral issues of our time is by unfairly shrinking our \\u003cem>moral circle\\u003c/em>: the set of beings we deem worthy of our moral concern,&rdquo; the syllabus reads. &ldquo;For example, many whites in the US failed to identify that slavery was \\u003cem>the\\u003c/em> moral issue of their age by excluding Blacks from their moral circle. To truly make the world better, we must look beyond the traditional moral horizon for those who are unfairly neglected by mainstream society. This week, we discuss one such group of beings: nonhuman animals.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>We ask Nickols, the Arete co-chair, about this framing. He tells us that it is important to keep the quote &ldquo;in the context of where it was originally formulated.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;Obviously the idea here is not to equate certain racial groups with animals or anything like that,&rdquo; Nickols says. &ldquo;Over time, though, the expanding moral circle idea is that white people who, before, held these extremely racist and terrible views &mdash; as the generations went on, and as culture shifted &mdash; began to see people, regardless of their race, as all morally equal.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;We have not reached a point where racism is totally gone,&rdquo; Nickols says, &ldquo;but there is definitely a shift in the right direction here. And more generally, the idea is that as time goes on, it is quite possible that the circle will continue to increase.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003ch1>Paperclips\\u003c/h1>\\r\\n\\r\\n\\u003cp>{dropcap text=O color=AF2234}ne thought experiment designed to demonstrate the danger of misaligned AI goes like this: Say the owner of a paperclip factory obtains an ultrapowerful AI and instructs it to maximize paperclip output. Although the AI is programmed to pursue a seemingly harmless goal, it might &mdash; if its understanding of values is not quite the same as ours &mdash; turn everything in the world into paper clips. That this scenario seems kind of silly is part of the point. Researchers are not concerned that AI will be &ldquo;evil&rdquo; per se, but that its pursuit of any objective, including &ldquo;good&rdquo; ones, might have unintended consequences.\\u003c/p>\\r\\n\\r\\n\\u003cp>&ldquo;AI systems donʼt always do what their developers intend,&rdquo; the Arete syllabus reads. &ldquo;They replicate human biases, achieve their goals in surprising and destructive ways, and are vulnerable to external manipulation.&rdquo; As a call to action, this is a compelling place to start. As a taxonomy, though, it is less an observation about AI than it is about \\u003cem>systems\\u003c/em>. Existential risks like climate change might first destroy the people who did the least to create them; any movement created by people is, in a sense, only human.\\u003c/p>\\r\\n\\r\\n\\u003cp>This point came up in the wake of the FTX collapse &mdash; what did it mean that a group seeking to fundamentally change the world relied so heavily on existing distributions of power? &mdash; and it has come up again in the months since, in the course of writing this article. Can you optimize your life? What if the thing we construct in our idealized image turns out not to look so different from us after all?\\u003c/p>\\r\\n\\r\\n\\u003cp>For Andrew N. Garber &rsquo;23, a former Arete leader, considering questions is the point of EA. There is a common misconception that effective altruism is a destination, when really it&rsquo;s more of a framework, he tells us: &ldquo;It is more concerned about the question than any one specific answer.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>In any event, when we ask Jurkovic what he hopes EA will look like in the future, his response is straightforward. &ldquo;The goal is to help people make the world better,&rdquo; he says, half smiling. &ldquo;As much as possible.&rdquo;\\u003c/p>\\r\\n\\r\\n\\u003cp>\\u003cem>&mdash; Associate Magazine Editor Bea Wall-Feng can be reached at bea.wall-feng@thecrimson.com. Follow them on Twitter \\u003ca href=\\\"https://twitter.com/wallfeng/\\\">@wallfeng\\u003c/a>.\\u003c/em>\\u003c/p>\\r\\n\\r\\n\\u003cp>\\u003cem>&mdash; Magazine writer Sophia C. Scott can be reached at sophia.scott@thecrimson.com. Follow her on Twitter \\u003ca href=\\\"https://twitter.com/ScottSophia_\\\">@ScottSophia_\\u003c/a>.\\u003c/em>\\u003c/p>\",\"shortcodes\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.0\",\"typename\":\"ShortcodeImageGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.1\",\"typename\":\"ShortcodeDropcapGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.2\",\"typename\":\"ShortcodeImageGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.3\",\"typename\":\"ShortcodeDropcapGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.4\",\"typename\":\"ShortcodeImageGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.5\",\"typename\":\"ShortcodeDropcapGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.6\",\"typename\":\"ShortcodeDropcapGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.7\",\"typename\":\"ShortcodeImageGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.8\",\"typename\":\"ShortcodeDropcapGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.9\",\"typename\":\"ShortcodeImageGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.10\",\"typename\":\"ShortcodeDropcapGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.11\",\"typename\":\"ShortcodeDropcapGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.12\",\"typename\":\"ShortcodeImageGQL\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.13\",\"typename\":\"ShortcodeDropcapGQL\"}],\"recArticles\":[],\"prevArticle\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).prevArticle\",\"typename\":\"ArticleGQL\"}},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).contributors.0\":{\"name\":\"Sophia C. Scott\",\"url\":\"/writer/1217270/Sophia_C._Scott/\",\"__typename\":\"ContributorGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).contributors.1\":{\"name\":\"Bea Wall-Feng\",\"url\":\"/writer/1217332/Bea__Wall-Feng/\",\"__typename\":\"ContributorGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).tags.0\":{\"text\":\"Scrutiny\",\"url\":\"/tag/scrutiny/\",\"__typename\":\"TagGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).tags.1\":{\"text\":\"Front Middle Feature\",\"url\":\"/tag/front-middle-feature/\",\"__typename\":\"TagGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).issue\":{\"issueDate\":\"2023-03-30\",\"__typename\":\"IssueGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).section\":{\"name\":\"Magazine\",\"__typename\":\"SectionGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).subsection\":{\"text\":\"Scrutiny\",\"url\":\"/tag/scrutiny/\",\"__typename\":\"TagGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.0\":{\"title\":\"Nikola Jurkovic Portrait\",\"url\":\"/image/2023/4/1/nikola-jurkovic-portrait/\",\"contributors\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.0.contributors.0\",\"typename\":\"ContributorGQL\"}],\"__typename\":\"ImageGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.0.contributors.0\":{\"name\":\"Marina Qu\",\"url\":\"/writer/1217257/Marina__Qu/\",\"__typename\":\"ContributorGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.1\":{\"title\":\"Adams Upper Common Room\",\"url\":\"/flyby/image/2020/3/9/adams-ucr/\",\"contributors\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.1.contributors.0\",\"typename\":\"ContributorGQL\"}],\"__typename\":\"ImageGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.1.contributors.0\":{\"name\":\"Zadoc I.N. Gee\",\"url\":\"/writer/1215627/Zadoc_I.N._Gee/\",\"__typename\":\"ContributorGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.2\":{\"title\":\"Jōsh Mysoré Portrait\",\"url\":\"/image/2023/4/1/josh-mysore-portrait/\",\"contributors\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.2.contributors.0\",\"typename\":\"ContributorGQL\"}],\"__typename\":\"ImageGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.2.contributors.0\":{\"name\":\"Marina Qu\",\"url\":\"/writer/1217257/Marina__Qu/\",\"__typename\":\"ContributorGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.3\":{\"title\":\"Daniela Shuman Portrait\",\"url\":\"/image/2023/4/1/daniela-shuman-portrait/\",\"contributors\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.3.contributors.0\",\"typename\":\"ContributorGQL\"}],\"__typename\":\"ImageGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.3.contributors.0\":{\"name\":\"Marina Qu\",\"url\":\"/writer/1217257/Marina__Qu/\",\"__typename\":\"ContributorGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.4\":{\"title\":\"Will Nickols Portrait\",\"url\":\"/image/2023/4/1/will-nickols-portrait/\",\"contributors\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.4.contributors.0\",\"typename\":\"ContributorGQL\"}],\"__typename\":\"ImageGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).relContent.4.contributors.0\":{\"name\":\"Marina Qu\",\"url\":\"/writer/1217257/Marina__Qu/\",\"__typename\":\"ContributorGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).mainContent\":{\"__typename\":\"ImageGQL\",\"description\":\"“Most of us want to improve the world. We see suffering, injustice, and death and feel moved to do something about it,” the Harvard EA website says. “But figuring out what that ‘something’ is, let alone actually doing it, can be a difficult and disheartening challenge. Effective altruism is a response to this challenge.” Can it live up to that goal?\",\"contributors\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).mainContent.contributors.0\",\"typename\":\"ContributorGQL\"}],\"imgUrl({\\\"height\\\":2000,\\\"width\\\":2000})\":\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/143735_1362611.jpeg.2000x1294_q95_crop-smart_upscale.jpg\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).mainContent.contributors.0\":{\"name\":\"Pema Choedon\",\"url\":\"/writer/1217921/Pema_Choedon_/\",\"__typename\":\"ContributorGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.0\":{\"__typename\":\"ShortcodeImageGQL\",\"key\":\"{shortcode-63eec514b5e11ad6d3729f9f9ad4bd516201edea}\",\"byline\":true,\"imageUrl\":\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/143735_1362611.jpeg.1500x970_q95_crop-smart_upscale.jpg\",\"caption\":\"“Most of us want to improve the world. We see suffering, injustice, and death and feel moved to do something about it,” the Harvard EA website says. “But figuring out what that ‘something’ is, let alone actually doing it, can be a difficult and disheartening challenge. Effective altruism is a response to this challenge.” Can it live up to that goal?\",\"contributors\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.0.contributors.0\",\"typename\":\"ContributorGQL\"}],\"pos\":\"center\",\"size\":\"large\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.0.contributors.0\":{\"name\":\"Pema Choedon\",\"url\":\"/writer/1217921/Pema_Choedon_/\",\"__typename\":\"ContributorGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.1\":{\"__typename\":\"ShortcodeDropcapGQL\",\"key\":\"{shortcode-8c0dd475ea3269f67b1a4d37d27db5cc232a1fc2}\",\"text\":\"W\",\"color\":\"AF2234\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.2\":{\"__typename\":\"ShortcodeImageGQL\",\"key\":\"{shortcode-b730cf4004f8af7d74d9289183f0c71fc9976a48}\",\"byline\":true,\"imageUrl\":\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/214432_1362617.JPG.1500x1000_q95_crop-smart_upscale.jpg\",\"caption\":\"Nikola Jurkovic ’25 is the president of Harvard Undergraduate Effective Altruism.\",\"contributors\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.2.contributors.0\",\"typename\":\"ContributorGQL\"}],\"pos\":\"center\",\"size\":\"large\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.2.contributors.0\":{\"name\":\"Marina Qu\",\"url\":\"/writer/1217257/Marina__Qu/\",\"__typename\":\"ContributorGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.3\":{\"__typename\":\"ShortcodeDropcapGQL\",\"key\":\"{shortcode-be29865d8a9c7908fa05930b7f2d42574eaa573c}\",\"text\":\"I\",\"color\":\"AF2234\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.4\":{\"__typename\":\"ShortcodeImageGQL\",\"key\":\"{shortcode-953d08b19087841b5ba3344e1e3970f242595182}\",\"byline\":true,\"imageUrl\":\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2020/03/09/004053_1343308.jpg.1500x844_q95_crop-smart_upscale.jpg\",\"caption\":\"The Arete Fellowship discussion section takes place in the Adams House Upper Common Room. The Arete Fellows learn a lot of terminology to help them think about how they can do the most good.\",\"contributors\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.4.contributors.0\",\"typename\":\"ContributorGQL\"}],\"pos\":\"center\",\"size\":\"large\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.4.contributors.0\":{\"name\":\"Zadoc I.N. Gee\",\"url\":\"/writer/1215627/Zadoc_I.N._Gee/\",\"__typename\":\"ContributorGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.5\":{\"__typename\":\"ShortcodeDropcapGQL\",\"key\":\"{shortcode-21cc3534b02e5a90dd1b6e61be0fe28423896a7e}\",\"text\":\"A\",\"color\":\"AF2234\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.6\":{\"__typename\":\"ShortcodeDropcapGQL\",\"key\":\"{shortcode-be29865d8a9c7908fa05930b7f2d42574eaa573c}\",\"text\":\"I\",\"color\":\"AF2234\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.7\":{\"__typename\":\"ShortcodeImageGQL\",\"key\":\"{shortcode-ccfa245c56f41c3dd4f50e1d2e97b2e846b0727a}\",\"byline\":true,\"imageUrl\":\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/214644_1362618.JPG.1500x1000_q95_crop-smart_upscale.jpg\",\"caption\":\"Jōsh P. Mysoré ’26 completed the Arete fellowship last fall and is considering becoming a discussion leader at some point in the future. Mysoré loves poetry, but is planning on concentrating in Computer Science and Linguistics.\",\"contributors\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.7.contributors.0\",\"typename\":\"ContributorGQL\"}],\"pos\":\"center\",\"size\":\"large\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.7.contributors.0\":{\"name\":\"Marina Qu\",\"url\":\"/writer/1217257/Marina__Qu/\",\"__typename\":\"ContributorGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.8\":{\"__typename\":\"ShortcodeDropcapGQL\",\"key\":\"{shortcode-24643cedbe14221289878261864001a8ceef067a}\",\"text\":\"O\",\"color\":\"AF2234\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.9\":{\"__typename\":\"ShortcodeImageGQL\",\"key\":\"{shortcode-caec2638cd0e69a267b65142b3f32cd802e78a8f}\",\"byline\":true,\"imageUrl\":\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/144535_1362612.JPG.1500x1000_q95_crop-smart_upscale.jpg\",\"caption\":\"Daniela R. Shuman ’24, a Computer Science and Statistics concentrator interested in urban development, is one of the chairs of the Arete fellowship.\",\"contributors\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.9.contributors.0\",\"typename\":\"ContributorGQL\"}],\"pos\":\"center\",\"size\":\"large\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.9.contributors.0\":{\"name\":\"Marina Qu\",\"url\":\"/writer/1217257/Marina__Qu/\",\"__typename\":\"ContributorGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.10\":{\"__typename\":\"ShortcodeDropcapGQL\",\"key\":\"{shortcode-be29865d8a9c7908fa05930b7f2d42574eaa573c}\",\"text\":\"I\",\"color\":\"AF2234\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.11\":{\"__typename\":\"ShortcodeDropcapGQL\",\"key\":\"{shortcode-be29865d8a9c7908fa05930b7f2d42574eaa573c}\",\"text\":\"I\",\"color\":\"AF2234\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.12\":{\"__typename\":\"ShortcodeImageGQL\",\"key\":\"{shortcode-70922b68dcc4d0816e2c010925135376dae995cb}\",\"byline\":true,\"imageUrl\":\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/144857_1362613.JPG.1500x1000_q95_crop-smart_upscale.jpg\",\"caption\":\"Will A. Nickols ’24 is an Arete Fellowship chair.\",\"contributors\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.12.contributors.0\",\"typename\":\"ContributorGQL\"}],\"pos\":\"center\",\"size\":\"large\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.12.contributors.0\":{\"name\":\"Marina Qu\",\"url\":\"/writer/1217257/Marina__Qu/\",\"__typename\":\"ContributorGQL\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).shortcodes.13\":{\"__typename\":\"ShortcodeDropcapGQL\",\"key\":\"{shortcode-24643cedbe14221289878261864001a8ceef067a}\",\"text\":\"O\",\"color\":\"AF2234\"},\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023}).prevArticle\":{\"title\":\"“I Don’t Chase, I Attract”: The Lure of Manifestation TikTok\",\"url\":\"/article/2023/3/30/manifestation-tiktok/\",\"description\":\"On manifestation TikTok, superstitions and unfounded techniques promise the fruition of anything you desire. The more I scrolled, the more I was told to repeat phrases, to embrace my delusions, or to use an audio for good luck.\",\"__typename\":\"ArticleGQL\"},\"ROOT_QUERY\":{\"content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023})\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.content({\\\"day\\\":30,\\\"month\\\":3,\\\"slug\\\":\\\"ea-scrut\\\",\\\"year\\\":2023})\",\"typename\":\"ArticleGQL\"}}}\n          </script>\n        </body>\n      </html>","oembed":false,"readabilityObject":{"title":"What is Going On With Effective Altruism? | Magazine | The Harvard Crimson","content":"<div id=\"readability-page-1\" class=\"page\"><div><p><span>W</span><span>hen we meet Nikola Jurkovic ’25 on Zoom, he’s sitting in front of a whiteboard covered with equations. We chat about his interest in folk punk music; his headphones push his hair back into a kind of emo swoop. Jurkovic comes across as friendly, but also guarded: He seems to want to make a good impression.</span></p><p><span>Jurkovic tells us he first learned about the philosophical movement called effective altruism from the comments section of a YouTube video about veganism during his gap year. When he moved from Croatia to Harvard, he joined the Harvard Undergraduate EA group and eventually became its president. “I think I’ve been looking for ways to make the world better for a really long time,” he says, “I think as far back as I can remember.”</span></p><p><span>EA, its proponents will tell you, is aimed at “doing good better.” It starts with the problem that the ways that we approach charity, aid, and other kinds of good-doing are clouded by human biases, and tries to find the best solution using statistical tools. For example, an effective altruist might argue that donating money to foundations that provide malaria treatments maximizes your lives saved per dollar spent.</span></p><p><span>The movement began in the early 2010s as the brainchild of the philosopher Will MacAskill. In the years since, it has flourished in both in-person spaces, like conferences, as well as blogs and forums. Users map out their worldviews in long, technical posts, debating what exactly different EA principles look like in practice, the merits of various criticisms of the movement, how to adapt to the more meritorious criticisms, and how to counter the less meritorious ones.</span></p><p><span>EA has also garnered support from a number of high-profile acolytes.</span></p><p><span>“This is a close match for my philosophy,” Elon Musk <a href=\"https://twitter.com/elonmusk/status/1554335028313718784\">tweeted</a> last August about MacAskill’s new book.</span></p><p><span>“Effective altruism — efforts that actually help people rather than making you feel good or helping you show off — is one of the great new ideas of the 21st century,” celebrity academic and Harvard psychology professor Steven Pinker wrote.</span></p><p><span>One of the highest-profile effective altruists was the former billionaire Sam Bankman-Fried, whose commitment to donate much of his wealth to EA causes turned out to be impossible after his cryptocurrency company, FTX, collapsed due to alleged fraud. His commitment also turned out to be disingenuous: In a postmortem interview with Vox, he described his apparent embrace of ethics as “this dumb game we woke westerners play where we say all the right shibboleths and so everyone likes us.”</span></p><p><span>Nevertheless, EA’s profile has continued to rise. Even at Harvard, some students find the pull inescapable: “Effective altruism is a <em>huge</em> trend on campus, seeping into everything,” Henry Haimo ’24 told the New Yorker in March, in an <a href=\"https://www.newyorker.com/magazine/2023/03/06/the-end-of-the-english-major\">article</a> ostensibly about the decline of the humanities.</span></p><p><span>When Harvard’s undergraduate EA group started on campus a decade ago, though, EA was not so widespread. At first, getting students to join was a challenge. “I realized that the idea of HEA seemed crazy: ‘join us and we’ll try to figure out how to maximize the good we do in the world!’” Ben S. Kuhn ’15, the co-president at the time, wrote in a 2014 blog post. “But my mistake was <em>letting other people know</em> that I knew HEA seemed crazy. As soon as they realized that I myself felt goofy, it was game over for convincing them to get involved.” He described expanding the group’s influence through a combination of savvy advertising tactics and a slate of famous speakers such as the philosopher Peter Singer, one of the most prominent intellectuals associated with the movement.</span></p><div><p><img src=\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/214432_1362617.JPG.1500x1000_q95_crop-smart_upscale.jpg\" alt=\"Nikola Jurkovic ’25 is the president of Harvard Undergraduate Effective Altruism.\"></p><p><span>Nikola Jurkovic ’25 is the president of Harvard Undergraduate Effective Altruism.<!-- --> </span><span>By <a href=\"/writer/1217257/Marina__Qu/\" to=\"/writer/1217257/Marina__Qu/\">Marina Qu</a></span></p></div><p><span>Today, there are several different EA groups across the University, including at the College, law school, and GSAS, as well as in the Boston area. (In this article, “Harvard EA” refers to the undergraduate group unless otherwise specified.) Jurkovic guesses there are “between 20 and 50” students involved in Harvard EA’s regular programming; there are dozens more in its introductory fellowships and hundreds more on the mailing lists.</span></p><p><span>Harvard markets itself as “developing leaders who make a difference globally,” and pop culture spins this principle into myth: Here is a university whose students will go on to save the world. EA is a movement that claims to know how to do it, from shifts in diet — a lot of the people we meet are vegan, in recognition of the statistically unmatched suffering that factory farming inflicts on chickens — to shifts in career, like the students who have devoted their futures to trying to prevent human extinction by AI.</span></p><p><span>“Most of us want to improve the world,” the Harvard EA website says. “But figuring out what that ‘something’ is, let alone actually doing it, can be a difficult and disheartening challenge.<a href=\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism/\"> Effective altruism</a> is a response to this challenge.” Can it live up to that goal?</span></p><h2><span>Counterfactual Impact</span></h2><p><span>I</span><span>f you are not already a committed effective altruist — and if you are interested in a discussion group that zigzags from deworming methodology to Bayesian statistics to the number of animals killed for food in the U.S. in the last 30 seconds — your first involvement with EA at Harvard might come through the introductory Arete Fellowship, a seven-week program that explores different effective altruism topics every week.</span></p><p><span>Daniela R. Shuman ’24, a Computer Science and Statistics concentrator interested in urban development, is one of the Arete chairs. She did the fellowship herself a year ago and “fell in love with the whole concept,” she says. She guesses there were around 40 fellows last fall.</span></p><p><span>We ask Will A. Nickols ’24, the other chair, if we can sit in on one of the sections. After some back-and-forth, Nickols suggests we attend the second week of the fellowship, which focuses on global health.</span></p><p><span>We get there before the meeting room opens. The discussion leaders, Nick C. Gabrieli ’24 and Jorge O. Guerra Jr. ’24, a former Crimson News editor, are sitting in near-darkness on the stairs outside the Adams House Upper Common Room, chatting: “What else are you up to this semester?” “What do you see as the limits of randomized control trials?” The vaulted blue ceiling looks like the night sky.</span></p><p><span>Claire Guo ’24, one of the Arete fellows and a former Crimson News editor, wanders up wearing two baseball caps; she wants to know if the leaders are participating in the junior class-wide game of Assassins.</span></p><p><span>No, Gabrieli tells her, adding, “I feel like I’m in the market for developing more hobbies.”</span></p><p><span>At 6 p.m., we file into the meeting room and assign ourselves to armchairs. There are three fellows in attendance, and another will show up halfway through. Gabrieli and Guerra walk us through the vocabulary terms from the readings — two blog posts, a lecture by an Oxford Professor, a TED talk, and several charts about life expectancy in different countries — asking the group to define and react to them.</span></p><p><span>This week, the discussion involves identifying your “problem”: So you’ve already decided you want to do good. How do you figure out <em>what</em> to do?</span></p><p><span>EA takes cues from rationalism, a commitment to logic rather than feelings as a basis for decision-making. For the Arete fellows, this requires learning a lot of terminology. For example, the “counterfactual impact” of an action is the result of doing that action, relative to the result of not doing it. Say you want to volunteer in a soup kitchen — a lot of EA readings use this example — but there are plenty of other volunteers; if you don’t do it, someone else will. Unless you are extremely good at serving soup, the amount of soup served in a world where you are a server is probably not that different from the amount of soup served in a world where you are not, and you might want to focus your altruistic intentions elsewhere.</span></p><div><p><img src=\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2020/03/09/004053_1343308.jpg.1500x844_q95_crop-smart_upscale.jpg\" alt=\"The Arete Fellowship discussion section takes place in the Adams House Upper Common Room. The Arete Fellows learn a lot of terminology to help them think about how they can do the most good.\"></p><p><span>The Arete Fellowship discussion section takes place in the Adams House Upper Common Room. The Arete Fellows learn a lot of terminology to help them think about how they can do the most good.<!-- --> </span><span>By <a href=\"/writer/1215627/Zadoc_I.N._Gee/\" to=\"/writer/1215627/Zadoc_I.N._Gee/\">Zadoc I.N. Gee</a></span></p></div><p><span>At the discussion leaders’ prompting, the fellows — Guo; Nathanael Tjandra ’26; and Kai C. Hostin ’25 — talk among themselves, trying to recall concepts from the readings. More than anything, it feels like a class section.</span></p><p><span>Gabrieli asks, “What is ‘importance?’” Guo and Hostin look at each other with uncertainty. “It should be able to be inferred, I think, from the word itself,” he continues. “What is the absolute magnitude of the thing we’re interested in?”</span></p><p><span>There are a lot of math terms, too. One of the readings constructs a complicated-looking product of derivatives to make the conceptual point that importance, tractability, and neglectedness are all vital considerations. Importance is how important a problem is; tractability is how much good can be done relative to effort; neglectedness is how much other people are already working in that area.</span></p><p><span>Still, the fellows have questions, and Gabrieli and Guerra are happy to discuss them. Should your personal interests play a role in what you decide to focus on? What about problems that require immediate responses, rather than careful calculation of impact? They talk but don’t come to any singular answer.</span></p><p><span>Near the end of the hour, Guerra brings up GiveWell, a well-known EA organization that maintains a ranked list of recommended charities. He asks the group: Of the top four causes on the website, which one would you donate to, and why?</span></p><p><span>First on the list is Malaria Consortium, which provides a kind of medicine called seasonal malaria chemoprevention. Malaria treatments are high-importance, and tractable because they are relatively cheap; GiveWell estimates that it costs $7 to protect a child from malaria. Second on the list is the Against Malaria Foundation, which provides malaria nets for about $5 each.</span></p><p><span>Tjandra, a Crimson Multimedia editor, ultimately chooses the fourth-ranked charity, which provides cash incentives to caregivers in Nigeria who vaccinate their babies. Unlike malaria treatments, he reasons, vaccines are “more generalizable to poor people everywhere.”</span></p><p><span>Guo asks if instead of providing Vitamin A supplements to areas in sub-Saharan Africa, as the third-ranked charity does, we could try to integrate vegetables rich in Vitamin A into those communities. “‘Eat some yams,’” she says, laughing.</span></p><p><span>Hostin concurs: “I don’t want to be like ‘Oh, here’s a fish’ rather than teach them how to fish, in a way.”</span></p><h2><span>A One in Ten Risk of Extinction</span></h2><p><span>A</span><span>t the end of the Arete fellowship, you are given a $10 gift card to the charity donation site Every.org. If you donate $10 of your own money, you receive another gift card. But what happens next is up to you: Since Harvard EA has no centralized meetings or agenda, continued membership means “joining the various side groups that we have, depending on what you’re interested in,” Shuman says.</span></p><p><span>Donating large parts of your income to charity — or earning to give, in the style of Bankman-Fried — are largely out of reach for college students, Jurkovic tells us. Instead, we find out, undergrad EA groups tend to focus on research and recruitment.</span></p><p><span>Still, the issues Harvard EA focuses on don’t always line up with the picture of EA suggested by the Arete fellowship. Zazie Huml ’25, Harvard EA’s Global Health Programming Lead and one of four people on its board, said when they joined Harvard EA there was “no major initiative” in global health or international development — two of the five major topics covered in the Arete fellowship — and there were only “a couple people” involved with animal rights, a third focus area.</span></p><p><span>“In Harvard EA we try not to present unfairly biased opinions towards any particular world problem,” Jurkovic says. “We aim to present the facts about the world problems and also give people useful decision-making tools so that they can examine the facts themselves.”</span></p><p><span>“My entire experience with EA at Harvard last semester was, ‘Oh, this is not for me, this is not my community. They’re not interested in the same things as me,’” Huml says, until someone they met in the organization encouraged them to take another look. “If I was to take initiative within the system, there would be resources to support me,” they recall the person saying. Huml has since led several global health initiatives under the umbrella of Harvard EA, including a “comprehensive study on source apportionment of lead exposure” in lower-income countries in partnership with the Lead Exposure Elimination Project.</span></p><p><span>So what was Harvard EA doing? “It was only focused on longtermism and AI,” Huml remembers.</span></p><p><span>Longtermism — week five on the Arete syllabus — is the view that people who will be alive in the future warrant the same moral consideration as people alive today. “If we want to do the most good, that means we want to help the most people,” Shuman says, “and the most people is not at the specific time that we’re living in.” Since future people will significantly outnumber today's people, barring a mass extinction event, longtermists argue that we should devote more resources to preventing “existential risks” like nuclear warfare or engineered pandemics.</span></p><p><span>This is something of a departure from EA principles in other areas, which our interviewees explain are a refinement of natural human instincts: You already want to do good, and EA just teaches you how to be smart about it. For a longtermist, though, what is at stake is the future of human existence.</span></p><p><span>In recent years, longtermists have turned their attention to the field of AI safety. As AI models grow increasingly powerful, EA researchers have argued, the existential threat they pose may become insurmountable. They call this the problem of “AI alignment”: ensuring that if or when a superhuman AI comes into existence, its values align with our own, so that it does not kill everyone on Earth.</span></p><p><span>Some AI safety researchers have been sounding this alarm for decades — but this past year, thanks to the rise of shockingly powerful, publicly available AI models like <a href=\"https://www.thecrimson.com/article/2023/2/23/chatgpt-scrut/\">ChatGPT</a>, the issue has made it into the mainstream. ChatGPT is already prone to spreading misinformation. A “sufficiently powerful” AI could be much worse, researcher Elizier Yudkowsky argued in a recent <a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\">Time magazine op-ed</a> calling for an indefinite moratorium on AI development. “In today’s world you can email DNA strings to laboratories that will produce proteins on demand allowing an AI initially confined to the internet to build artificial life forms or bootstrap straight to postbiological molecular manufacturing,” he wrote.</span></p><p><span>How do you quantify incalculable destruction? “The Precipice: Existential Risk and the Future of Humanity,” a book by Toby Ord about existential risk, for which Harvard EA runs a reading group, puts the risk of extinction in the next century from unaligned AI at 1 in 10, higher than any other source of risk. Last July, Jurkovic wrote in a comment on the EA forum that “existential risks are high up on the list of most probable causes of death for college aged-people”: Assume that the probability of achieving superhuman AI by 2045 is 50 percent, and assume that the probability of death given superhuman AI is at least 10 percent. Then the probability of death by AI in the next few years might be comparable to around 1 in 6000, he wrote, explaining that this probability is similar to the two largest causes of death for “college-aged people in the US,” suicide and vehicle accidents, although he did not write out the calculations leading to this conclusion.</span></p><p><span>Jurkovic guesses that there are more people in Harvard EA working on AI than other problems, but points out the existence of organizations with other EA-related focuses, including Harvard College Animal Advocates.</span></p><p><span>Harvard EA doesn’t have an AI safety program itself, but there are several related organizations that do. One of these is the Harvard AI Safety Team, which was founded in 2022 by Xander L. Davies ’23 and Fiona E. Pollack ’25. Though HAIST is not an EA organization, many members of Harvard EA work with HAIST, and HAIST also receives funding from national EA philanthropy organizations. HAIST and Harvard EA also share members with other local AI groups, including Cambridge Boston Alignment Initiative and MIT AI Alignment.</span></p><p><span>HAIST hosts reading groups and talks by professors, with a focus on machine learning research, Davies tells us on Zoom. He has curly hair and a black hoodie; the “L” in his name stands for Laser. Although he doesn’t believe that AI right now is poised to destroy humanity, he also doesn’t believe we have the tools to stop it. “I think how I look at it is, it’s currently impossible to get our AIs to not do things,” Davies says, referring to the ease with which users have bypassed ChatGPT-like models’ built-in filters against violent speech and misinformation.</span></p><p><span>“Rapid progress in AI is becoming more and more economically useful, becoming more and more trusted, while at the same time this stark lack of progress on actually understanding how these systems work, on getting confidence that we actually know how to make these systems do what we want, is very startling to me,” Davies says. “And I think it should be a core priority on the global stage.”</span></p><h2><span>‘The Warm Fuzzy Feeling Just Doesn’t Matter as Much’</span></h2><p><span>I</span><span>n some respects, EA seems fairly intuitive: Who doesn’t want to minimize suffering as much as possible? In other respects, it pushes you to rethink your intuitions.</span></p><p><span>Take, for instance, a thought experiment effective altruists often use to illustrate the unique way this philosophy navigates moral quandaries. It’s called “<a href=\"https://www.harvardea.org/blog/2022/lena-ashooh-the-child-in-the-pond\">the drowning child scenario</a>,” originally formulated by Peter Singer. Imagine you’re on your way to an important event, and you notice a child is drowning in a nearby pond. Do you jump in to save the child?</span></p><p><span>Barring circumstances like an inability to swim, most people answer yes. But then, you’re asked the question again and again. Each time, the stakes are higher: What if you will ruin your clothes and waterlog your phone by jumping in? What if you already saved a drowning child last week? What if this child was an undocumented immigrant? What if the pond was far enough away that you would have to spend gas money to get there?</span></p><p><span>As the hypotheticals escalate, generally, participants continue to decide to save the drowning child each time. But in the real world, when facing situations like determining healthcare access for immigrants or people in other countries, the increased distance leads some people to make what is in effect the opposite choice. EA wants to know: Why should you value those peoples’ lives less than those of people closer to you?</span></p><p><span>When we spoke to Marka F. X. Ellertson ’23, then the president of Harvard EA, last September, she told us that with EA efforts, “the warm fuzzy feeling just doesn’t matter as much to me as the rational thought that I know that I’ve had a bigger impact.”</span></p><p><span>“And I actually still do want that warm fuzzy feeling,” Ellertson added, explaining that she donates to local causes that are particularly meaningful to her.</span></p><p><span>Joshua D. Greene ’97, Harvard EA’s faculty advisor, disputes the idea that EA strips away the warmth of charity work.</span></p><p><span>Utilitarianism might make you think of “the things that kind of serve a function but don’t nurture our souls or to speak to our heart’s greatest desires, right? And utilitarianism is not just about cold functionality,” he says. “It’s about everything that makes life good or bad, everything that makes life worth living, everything that makes life meaningful.”</span></p><p><span>Harold H. Klapper ’25, who participated in a Harvard EA fellowship last year, tells us that in some EA dialogues about utilitarianism “get really wild.”</span></p><p><span>At a Boston-area EA event, for instance, “I’ve had conversations arguing about whether we should kill all wild animals, because they have negative lives,” Klapper says. “An ant colony must just have negative utility in the sense that they’re just not enjoying life, and so it’d be better if we just eliminated them.”</span></p><p><span>“When things are a movement, you kind of have to buy into the whole thing, and when you buy into the whole thing, you get really wacky and fucked up answers to problems,” Klapper adds.</span></p><p><span>Effective altruists seek to apply EA principles to personal decisions: what to study, where to work. If you are a college student interested in building EA communities, you might “consider not going to Harvard, as there are a bunch of people there doing great things,” Jurkovic wrote on the EA forum in December, suggesting that going to other colleges without strong EA movements could be better. (Was this something Jurkovic himself considered when applying to Harvard? “No,” he says, laughing.)</span></p><p><span>A lot of EA discourse revolves around career choice: You will probably work for around 80,000 hours in your lifetime — several of the people we talk to cite this estimate — and you should spend them doing things that count, even if they may not be things you enjoy.</span></p><p><span>Harvard EA, Shuman says, focuses mainly on “getting high-potential individuals into careers where they can spend their 80,000 hours of their career on solving these issues.”</span></p><div><p><img src=\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/214644_1362618.JPG.1500x1000_q95_crop-smart_upscale.jpg\" alt=\"Jōsh P. Mysoré ’26 completed the Arete fellowship last fall and is considering becoming a discussion leader at some point in the future. Mysoré loves poetry, but is planning on concentrating in Computer Science and Linguistics.\"></p><p><span>Jōsh P. Mysoré ’26 completed the Arete fellowship last fall and is considering becoming a discussion leader at some point in the future. Mysoré loves poetry, but is planning on concentrating in Computer Science and Linguistics.<!-- --> </span><span>By <a href=\"/writer/1217257/Marina__Qu/\" to=\"/writer/1217257/Marina__Qu/\">Marina Qu</a></span></p></div><p><span>To get them to that point, EA might also influence Harvard students’ concentration decisions. One such student is Jōsh P. Mysoré ’26; when we meet him outside of Blackbird, he’s reading Giovanni Boccaccio’s “The Decameron” for class. Mysoré completed the Arete fellowship last fall and is considering becoming a discussion leader at some point in the future.</span></p><p><span>“I love poetry. I loooove poetry,” he tells us. “Will I be going into poetry? No. Because I don’t think it will actually do good for people.” At the moment, Mysoré plans to concentrate in Computer Science and Linguistics.</span></p><p><span>Computer Science, as well as Statistics and Applied Mathematics, are fairly common concentration choices among the people we meet. Klapper tells us he knows someone in Harvard EA who studies Computer Science and dislikes it, but continues in the field because they believe it’s the most effective use of their time.</span></p><p><span>Mysoré “was given a certain amount of privilege in my life to even get to this point,” he tells us. “I do think I owe something to the greater good of humanity to do something that impacts more people in a tangible way.”</span></p><p><span>Does he think anyone should go into poetry, we ask.</span></p><p><span>“I don't think it’s a contradiction to say that I can hold two opposing viewpoints at the same time,” Mysoré says. “Like in my heart, I’m a humanist, and I’m very romantic.” He tells us that he joined EA specifically to challenge these humanist viewpoints, but his perspective might flip again. “Honestly, I do think there should be poets,” he says.</span></p><p><span>Mysoré tells us he still believes that EA has a noble mission, even if he disagrees with some of its particular approaches. “I think at the baseline, EA is creating dialogue,” he says. “That is really what counts.”</span></p><h2><span>‘Not To Create a Club, But Rather to Create a World’</span></h2><p><span>O</span><span>n Saturdays, Harvard EA throws socials in a house near campus where Jurkovic lives with four of his friends. Half the time, the socials are just for Harvard affiliates; every other week, they are open to students from other Boston-area schools.</span></p><p><span>Of course, the socials are designed to be fun, but they have a functional purpose as well. “One important part of having a community is that the people talk to each other and have time spent together, so that they can collaborate and talk about their projects,” Jurkovic says.</span></p><p><span>Though Jurkovic declined our request to attend a social on the record, we can try to reconstruct the vibe from a guide that he posted on the EA forum called “How to Organize a Social.” Indeed, in the post, he records every step of preparing for a social in granular detail, providing recommendations for everything from grocery lists — CLIF Bars, Diet Coke, several varieties of Vitaminwater — to music, such as the Spotify-curated playlist “my life is a movie.” Jurkovic suggests you make it easy for guests to find answers to anticipated questions: “The shoes on/off policy? Where the bathroom is? Where one can get water? What the wi-fi password is?”</span></p><p><span>Last year, Trevor J. Levin ’19, who is currently on leave as the co-president of the university-wide EA group, also created a list of recommendations for effective retreats: They should happen in the beginning of the semester, when people are less busy; include lots of time for one-on-one interactions and a “structured vulnerable/emotional thing”; and include a healthy mix of new recruits and “moderately charismatic people for whom EA is a major consideration in how they make decisions.” These suggestions were embedded in a long post, which, citing feedback from Ellertson, Davies, Jurkovic, and others, argues that college EA groups should focus more on retreats as a method of bonding.</span></p><p><span>For Levin, a former Crimson editor, this kind of immersive social situation is vital to capturing those who might be interested in EA but don’t prioritize it.</span></p><p><span>“While most of the important cognition that happens is social/emotional, this is not the same thing as tricking or manipulating people into being EAs,” he wrote on the forum. Instead, retreats are meant to appeal to those who may agree with EA on some level but have not yet acted on it, and giving them time to “move closer to the values they <em>previously already wanted to live by</em>.”</span></p><p><span>Since EA was born, it has been very deliberate about the image it projects. The name “effective altruism” was itself the product of a long debate: “This has been such an obstacle in the utilitarianesque community — ‘do-gooder’ is the current term, and it sucks,” MacAskill, the philosopher, wrote in a 2011 email chain. What followed was a period of brainstorming — fusing terms like “utilitarian” and philanthropist” with “alliance” and “institute” — and a series of votes to establish a name for both “the type of person we wanted to refer to, and for the name of the organization we were setting up.”</span></p><p><span>Now, countless blogs and forum posts are dedicated to determining how best to recruit new members to the EA community. In December 2021, for instance, Jurkovic wrote a post on the EA forum describing an “organic” way to pitch EA to students.</span></p><p><span>“Person: What do you want to study? Me: Not sure, I’m trying to find what to study so I can have as good of an impact as possible,” he wrote in an example dialogue. “If their level of enthusiasm stays high or grows, pitch an intro fellowship or a reading group to them.”</span></p><div><p><img src=\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/144535_1362612.JPG.1500x1000_q95_crop-smart_upscale.jpg\" alt=\"Daniela R. Shuman ’24, a Computer Science and Statistics concentrator interested in urban development, is one of the chairs of the Arete fellowship.\"></p><p><span>Daniela R. Shuman ’24, a Computer Science and Statistics concentrator interested in urban development, is one of the chairs of the Arete fellowship.<!-- --> </span><span>By <a href=\"/writer/1217257/Marina__Qu/\" to=\"/writer/1217257/Marina__Qu/\">Marina Qu</a></span></p></div><p><span>Even if some people choose not to become effective altruists, Shuman tells us, they could still take away valuable ideas from the movement.</span></p><p><span>“The point is not to create a club, but rather to create a world of people that want to do the most good, and EA just has a set of tools that it thinks are probably the most good,” Shuman says. “We want everybody to think in these terms.”</span></p><h2><span>Daedalus House</span></h2><p><span>I</span><span>’m gonna talk from a removed, omniscient perspective,” Mysoré says, kicking his chair back and folding his arms behind his head. EA spends a lot of money on space, food, and socials, he tells us. “At a certain point you have to ask yourself: What is effective about that?”</span></p><p><span>Most of Harvard EA’s money comes from larger EA organizations like Open Philanthropy, a grantmaking foundation largely financed by Cari Tuna and Dustin Moskovitz, the latter of whom co-founded Facebook. Open Philanthropy distributes money to a range of EA-related causes. Put simply, it is an organization that “cares about making the world better,” Jurkovic says.</span></p><p><span>We ask him how Harvard EA uses its grant money.</span></p><p><span>“It’s not my area of expertise,” Jurkovic says. “But ...” He pauses for 15 seconds. “Yeah, just sometimes we get funding for club activities.”</span></p><p><span>In 2022, we later find out, part of an Open Philanthropy grant was used to send Arete fellows and the University-wide EA group on a weekend trip to Essex Woods, a serene, Thoreauesque venue an hour north of campus that charges about $5,000 per night. According to GiveWell, donating $10,000 to the nonprofit Malaria Consortium could save the lives of five people.</span></p><p><span>The schedule was similar to that of a corporate retreat: workshops, games, dinner, hot tub, Hamming circles. Well, maybe not the last one. Hamming circles are an activity where three to five participants sit down together and talk through one problem facing each member in 20-minute chunks. It’s “similar to what happens in a pair debug,” a post on an EA-related forum explains. These problems might vary, the post says, from “Is it possible for me, specifically, to have a meaningful impact on existential risk” to “I need to secure $250,000 in seed funding for my startup” to “I’m expected to speak at my father’s funeral and I have nothing but scathing, bitter, angry things to say.”</span></p><p><span>Open Philanthropy also issued a $250,000 grant for the Centre for Effective Altruism to “rent and refurbish” an office for the Harvard AI Safety Team in Harvard Square for one year.</span></p><p><span>Davies, the HAIST co-founder, tells us that the HAIST office is “pretty research-y.”</span></p><p><span>“People are often at whiteboards, talking about problems with each other,” he says. “I think it feels like people are really trying to make progress on this technical problem, which I find exciting. It’s maybe a little startup-y in vibe.” In a post on the EA forum in December, Davies wrote that “investing effort into making the space fun and convenient to use helped improve programming, social events, and sense of community.”</span></p><p><span>In August, Open Philanthropy recommended an $8.9 million grant for the Center for Effective Altruism to lease an EA office space for five years in Harvard Square. While the space would have been unaffiliated with Harvard EA, a forum post announcing the office promised that part of it would contain “meeting spaces for students at Harvard and other Boston-area schools,” and thanked Levin and Jurkovic for their help in developing the project.</span></p><p><span>Forum members, including Levin and Jurkovic, threw out potential names for the space. Some of the suggestions were mythological — “Daedalus,” who advised Icarus not to fly too close to the sun — some cosmological — “Supercluster,” “Earthrise” — and some silly, like “Aardvark,” from a user who argued the name sounded similar to “Harvard” and would show up first in alphabetical lists.</span></p><p><span>“Don’t like including the actual words EA in the name of the space,” Levin (who, for his part, liked “Apollo”) wrote in the comments. “It increases the chances of hypocrisy charges (from people who haven’t thought much about the effects of nice offices on productivity) for getting a nice central office space while ostensibly being altruistic.”</span></p><p><span>But the Apollo House — or Aardvark House, or Supercluster — never materialized. According to Levin, after CEA signed the lease and began preparing the space, Open Philanthropy notified them that the grant was under review. As a result, Levin tells us, CEA is now trying to sublease the space to get the money back.</span></p><p><span>CEA and Open Philanthropy did not respond to questions about the current status of the Harvard Square office grant.</span></p><p><span>In addition to money for spaces and retreats, Open Philanthropy has an open request form for university group funding, and regularly recommends grants to undergraduate organizers. Other EA-affiliated organizations also fund events and projects. Huml, the Global Health Programming Lead, tells us that this is part of what makes EA a valuable community in which to pursue global health work.</span></p><p><span>“To be totally transparent, I don’t 100 percent align with the values,” Huml tells us. “I think that they are an incredible platform and have a lot of resources — and those resources are financial, they are access to experts in very specific fields.”</span></p><p><span>Three Harvard students, including Davies and Gabrieli, were recipients of Open Philanthropy’s fall 2022 University Organizer Fellowship, for which the organization recommended a total of $3.1 million across 116 recipients. Gabrieli declined to be interviewed for this article. Davies says he doesn’t know if he’s allowed to disclose how much money he actually got, but that he considers the grant to be “an hourly wage,” since he quit previous jobs to focus on developing HAIST.</span></p><p><span>In February 2022, Open Philanthropy recommended a $75,000 grant to Pollack, the other HAIST co-founder, “to support her work organizing the effective altruism community at Harvard University.”</span></p><p><span>When we reach out to Pollack, she tells us over email that she is “no longer organizing for the Harvard Effective Altruism group,” but has spent about $14,000 of the grant on HAIST expenses with Open Philanthropy’s approval: $7,200 to monthly software costs like Airtable and Squarespace, and most of the rest to accommodations for a workshop that HAIST hosted with the MIT AI safety group in Arlington, Virginia.</span></p><p><span>Harvard EA is aware that this allocation of money can appear at odds with its stated mission. After the Essex Woods retreat, organizers sent out a feedback form. “How much did the spending of money at this retreat make you feel uncomfortable?” one question asked.</span></p><p><span>We talk to Levin, the University EA co-president, and he likens it to the way that companies spend money on recruitment. “The idea is that there are problems that are much more talent-constrained than money-constrained,” he tells us. AI safety, a problem that relatively few people are working on, is an extreme example of this, he says. “The question then becomes, ‘Okay, well, if we have money and not people, how do we convert between the two?’”</span></p><p><span>Levin pauses and corrects himself: “My train of thought there sounded kind of like I was saying, well, if you have a bunch of money, what do you do with it, right? That is not what I think.” What he does believe is that physical environments like retreats can rapidly accelerate the rate — by up to 100 times, he writes on the forums — at which people get on board with EA principles.</span></p><p><span>Several people in EA, Levin guesses, joined because of their experiences on a retreat. “That is absolutely something that we would have paid this money for,” he says.</span></p><p><span>Shuman believes that this outreach is particularly effective in Cambridge because of its highly motivated, change-driven student body. “Harvard and MIT have done the vast majority of vetting for people who are highly ambitious,” she says.</span></p><p><span>Shuman tells us that the international EA community places a lot of importance on this kind of community building. “If they can invest $1,000 in getting five high-potential individuals to, instead of doing AI research, do AI safety research, that’s a pretty good use of money,” she says. “It could save a lot of lives, potentially.”</span></p><h2><span>‘A Skewed Pipeline’</span></h2><p><span>I</span><span>n a 2020 survey of EA Forum members, 76 percent of respondents were white and 71 percent were male. Though there is an imbalance, the Centre for Effective Altruism argues that diversity is important for several reasons.</span></p><p><span>“If an EA-aligned newcomer concludes that effective altruism is not for ‘people like me,’ they may not get involved, and the EA community may be less effective,” its website reads. “We don’t want to miss important perspectives.”</span></p><p><span>We ask Jurkovic if he’s aware of demographic imbalances within EA groups at Harvard. He pauses. “I think it is quite important to have a community which is welcoming to everyone,” he says. “EA sometimes shares a problem with the cause areas that it tackles” — meaning STEM fields — “which is that many of them have more males in them than average.”</span></p><p><span>Shuman — “the only she/her” leading an Arete section, she tells us — echoes this sentiment, saying that these numbers reflect existing disparities in STEM and philanthropic fields.</span></p><p><span>“It’s just a skewed pipeline,” she says, “which is a problem.”</span></p><p><span>“I can say from personal experience that we’ve had quite diverse groups,” Nickols says. “In terms of gender, it does tend to be more male-skewed, and that’s something that we’re continually working on.” He acknowledges that Harvard EA “is probably predominantly white and Asian, but not more so to Harvard’s general population.” (The organization does not keep demographic records of its members, so we can’t verify this.)</span></p><p><span>Nickols says that the applicants for Harvard EA’s fellowships also tend to skew male. “Given that word of mouth is our biggest kind of spreader, it might just be possible that guys who have done it in the past are friends with more guys and tell them about it,” he adds.</span></p><p><span>In recent months, the EA movement has been embroiled in controversies related to race and gender in its communities.</span></p><p><span>One of these controversies revolved around Nick Bostrom, a philosopher whose ideas led to the development of longtermism; four of his works are cited in the syllabus for Harvard EA’s Precipice Fellowship.</span></p><p><span>In January, Bostrom posted a letter to his website apologizing for a comment he wrote on a forum in the mid-90s, which claims that Black people “are more stupid than whites” and contains the n-word. In the letter, Bostrom castigates his past self for using the slur and writes that the comment “does not accurately represent my views, then or now,” but does not reject the possibility of genetic cognitive differences between races. He leaves this question to “others, who have more relevant knowledge.” The letter continues with a section about bioethics that opens: “What about eugenics? Do I support eugenics? No, not as the term is commonly understood.”</span></p><p><span>In March, Time magazine <a href=\"https://time.com/6252617/effective-altruism-sexual-harassment/\"> interviewed seven women</a> who said they had been sexually harassed, coerced, or assaulted within EA spaces, particularly in the Bay Area. The scene’s overwhelming maleness, tech-bro culture, and impulse to quantify and rationalize messy real-world dynamics created a deeply unsafe environment, the women said. One described having dinner with a prominent researcher nearly twice her age who told her that “pedophilic relationships between very young women and older men was a good way to transfer knowledge.”</span></p><p><span>“We were of course upset by both of these issues,” Jurkovic wrote in an email to us about the Bostrom letter and Time investigation, “and have spent time figuring out how we can improve our diversity and make sure we're a welcoming community to women and people of color.”</span></p><div><p><img src=\"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/144857_1362613.JPG.1500x1000_q95_crop-smart_upscale.jpg\" alt=\"Will A. Nickols ’24 is an Arete Fellowship chair.\"></p><p><span>Will A. Nickols ’24 is an Arete Fellowship chair.<!-- --> </span><span>By <a href=\"/writer/1217257/Marina__Qu/\" to=\"/writer/1217257/Marina__Qu/\">Marina Qu</a></span></p></div><p><span>Although some of EA’s focus areas deal with global health and economic growth in underdeveloped countries, its frameworks generally do not foreground race or gender. A version of the spring 2023 Arete syllabus posted on the Harvard EA website only mentions race in the overview of Week Four: Animal Welfare.</span></p><p><span>“One of the most important ways we can fail to identify the most important moral issues of our time is by unfairly shrinking our <em>moral circle</em>: the set of beings we deem worthy of our moral concern,” the syllabus reads. “For example, many whites in the US failed to identify that slavery was <em>the</em> moral issue of their age by excluding Blacks from their moral circle. To truly make the world better, we must look beyond the traditional moral horizon for those who are unfairly neglected by mainstream society. This week, we discuss one such group of beings: nonhuman animals.”</span></p><p><span>We ask Nickols, the Arete co-chair, about this framing. He tells us that it is important to keep the quote “in the context of where it was originally formulated.”</span></p><p><span>“Obviously the idea here is not to equate certain racial groups with animals or anything like that,” Nickols says. “Over time, though, the expanding moral circle idea is that white people who, before, held these extremely racist and terrible views — as the generations went on, and as culture shifted — began to see people, regardless of their race, as all morally equal.”</span></p><p><span>“We have not reached a point where racism is totally gone,” Nickols says, “but there is definitely a shift in the right direction here. And more generally, the idea is that as time goes on, it is quite possible that the circle will continue to increase.”</span></p><h2><span>Paperclips</span></h2><p><span>O</span><span>ne thought experiment designed to demonstrate the danger of misaligned AI goes like this: Say the owner of a paperclip factory obtains an ultrapowerful AI and instructs it to maximize paperclip output. Although the AI is programmed to pursue a seemingly harmless goal, it might — if its understanding of values is not quite the same as ours — turn everything in the world into paper clips. That this scenario seems kind of silly is part of the point. Researchers are not concerned that AI will be “evil” per se, but that its pursuit of any objective, including “good” ones, might have unintended consequences.</span></p><p><span>“AI systems donʼt always do what their developers intend,” the Arete syllabus reads. “They replicate human biases, achieve their goals in surprising and destructive ways, and are vulnerable to external manipulation.” As a call to action, this is a compelling place to start. As a taxonomy, though, it is less an observation about AI than it is about <em>systems</em>. Existential risks like climate change might first destroy the people who did the least to create them; any movement created by people is, in a sense, only human.</span></p><p><span>This point came up in the wake of the FTX collapse — what did it mean that a group seeking to fundamentally change the world relied so heavily on existing distributions of power? — and it has come up again in the months since, in the course of writing this article. Can you optimize your life? What if the thing we construct in our idealized image turns out not to look so different from us after all?</span></p><p><span>For Andrew N. Garber ’23, a former Arete leader, considering questions is the point of EA. There is a common misconception that effective altruism is a destination, when really it’s more of a framework, he tells us: “It is more concerned about the question than any one specific answer.”</span></p><p><span>In any event, when we ask Jurkovic what he hopes EA will look like in the future, his response is straightforward. “The goal is to help people make the world better,” he says, half smiling. “As much as possible.”</span></p><p><span><em>— Associate Magazine Editor Bea Wall-Feng can be reached at bea.wall-feng@thecrimson.com. Follow them on Twitter <a href=\"https://twitter.com/wallfeng/\">@wallfeng</a>.</em></span></p><p><span><em>— Magazine writer Sophia C. Scott can be reached at sophia.scott@thecrimson.com. Follow her on Twitter <a href=\"https://twitter.com/ScottSophia_\">@ScottSophia_</a>.</em></span></p></div></div>","textContent":"When we meet Nikola Jurkovic ’25 on Zoom, he’s sitting in front of a whiteboard covered with equations. We chat about his interest in folk punk music; his headphones push his hair back into a kind of emo swoop. Jurkovic comes across as friendly, but also guarded: He seems to want to make a good impression.Jurkovic tells us he first learned about the philosophical movement called effective altruism from the comments section of a YouTube video about veganism during his gap year. When he moved from Croatia to Harvard, he joined the Harvard Undergraduate EA group and eventually became its president. “I think I’ve been looking for ways to make the world better for a really long time,” he says, “I think as far back as I can remember.”EA, its proponents will tell you, is aimed at “doing good better.” It starts with the problem that the ways that we approach charity, aid, and other kinds of good-doing are clouded by human biases, and tries to find the best solution using statistical tools. For example, an effective altruist might argue that donating money to foundations that provide malaria treatments maximizes your lives saved per dollar spent.The movement began in the early 2010s as the brainchild of the philosopher Will MacAskill. In the years since, it has flourished in both in-person spaces, like conferences, as well as blogs and forums. Users map out their worldviews in long, technical posts, debating what exactly different EA principles look like in practice, the merits of various criticisms of the movement, how to adapt to the more meritorious criticisms, and how to counter the less meritorious ones.EA has also garnered support from a number of high-profile acolytes.“This is a close match for my philosophy,” Elon Musk tweeted last August about MacAskill’s new book.“Effective altruism — efforts that actually help people rather than making you feel good or helping you show off — is one of the great new ideas of the 21st century,” celebrity academic and Harvard psychology professor Steven Pinker wrote.One of the highest-profile effective altruists was the former billionaire Sam Bankman-Fried, whose commitment to donate much of his wealth to EA causes turned out to be impossible after his cryptocurrency company, FTX, collapsed due to alleged fraud. His commitment also turned out to be disingenuous: In a postmortem interview with Vox, he described his apparent embrace of ethics as “this dumb game we woke westerners play where we say all the right shibboleths and so everyone likes us.”Nevertheless, EA’s profile has continued to rise. Even at Harvard, some students find the pull inescapable: “Effective altruism is a huge trend on campus, seeping into everything,” Henry Haimo ’24 told the New Yorker in March, in an article ostensibly about the decline of the humanities.When Harvard’s undergraduate EA group started on campus a decade ago, though, EA was not so widespread. At first, getting students to join was a challenge. “I realized that the idea of HEA seemed crazy: ‘join us and we’ll try to figure out how to maximize the good we do in the world!’” Ben S. Kuhn ’15, the co-president at the time, wrote in a 2014 blog post. “But my mistake was letting other people know that I knew HEA seemed crazy. As soon as they realized that I myself felt goofy, it was game over for convincing them to get involved.” He described expanding the group’s influence through a combination of savvy advertising tactics and a slate of famous speakers such as the philosopher Peter Singer, one of the most prominent intellectuals associated with the movement.Nikola Jurkovic ’25 is the president of Harvard Undergraduate Effective Altruism. By Marina QuToday, there are several different EA groups across the University, including at the College, law school, and GSAS, as well as in the Boston area. (In this article, “Harvard EA” refers to the undergraduate group unless otherwise specified.) Jurkovic guesses there are “between 20 and 50” students involved in Harvard EA’s regular programming; there are dozens more in its introductory fellowships and hundreds more on the mailing lists.Harvard markets itself as “developing leaders who make a difference globally,” and pop culture spins this principle into myth: Here is a university whose students will go on to save the world. EA is a movement that claims to know how to do it, from shifts in diet — a lot of the people we meet are vegan, in recognition of the statistically unmatched suffering that factory farming inflicts on chickens — to shifts in career, like the students who have devoted their futures to trying to prevent human extinction by AI.“Most of us want to improve the world,” the Harvard EA website says. “But figuring out what that ‘something’ is, let alone actually doing it, can be a difficult and disheartening challenge. Effective altruism is a response to this challenge.” Can it live up to that goal?Counterfactual ImpactIf you are not already a committed effective altruist — and if you are interested in a discussion group that zigzags from deworming methodology to Bayesian statistics to the number of animals killed for food in the U.S. in the last 30 seconds — your first involvement with EA at Harvard might come through the introductory Arete Fellowship, a seven-week program that explores different effective altruism topics every week.Daniela R. Shuman ’24, a Computer Science and Statistics concentrator interested in urban development, is one of the Arete chairs. She did the fellowship herself a year ago and “fell in love with the whole concept,” she says. She guesses there were around 40 fellows last fall.We ask Will A. Nickols ’24, the other chair, if we can sit in on one of the sections. After some back-and-forth, Nickols suggests we attend the second week of the fellowship, which focuses on global health.We get there before the meeting room opens. The discussion leaders, Nick C. Gabrieli ’24 and Jorge O. Guerra Jr. ’24, a former Crimson News editor, are sitting in near-darkness on the stairs outside the Adams House Upper Common Room, chatting: “What else are you up to this semester?” “What do you see as the limits of randomized control trials?” The vaulted blue ceiling looks like the night sky.Claire Guo ’24, one of the Arete fellows and a former Crimson News editor, wanders up wearing two baseball caps; she wants to know if the leaders are participating in the junior class-wide game of Assassins.No, Gabrieli tells her, adding, “I feel like I’m in the market for developing more hobbies.”At 6 p.m., we file into the meeting room and assign ourselves to armchairs. There are three fellows in attendance, and another will show up halfway through. Gabrieli and Guerra walk us through the vocabulary terms from the readings — two blog posts, a lecture by an Oxford Professor, a TED talk, and several charts about life expectancy in different countries — asking the group to define and react to them.This week, the discussion involves identifying your “problem”: So you’ve already decided you want to do good. How do you figure out what to do?EA takes cues from rationalism, a commitment to logic rather than feelings as a basis for decision-making. For the Arete fellows, this requires learning a lot of terminology. For example, the “counterfactual impact” of an action is the result of doing that action, relative to the result of not doing it. Say you want to volunteer in a soup kitchen — a lot of EA readings use this example — but there are plenty of other volunteers; if you don’t do it, someone else will. Unless you are extremely good at serving soup, the amount of soup served in a world where you are a server is probably not that different from the amount of soup served in a world where you are not, and you might want to focus your altruistic intentions elsewhere.The Arete Fellowship discussion section takes place in the Adams House Upper Common Room. The Arete Fellows learn a lot of terminology to help them think about how they can do the most good. By Zadoc I.N. GeeAt the discussion leaders’ prompting, the fellows — Guo; Nathanael Tjandra ’26; and Kai C. Hostin ’25 — talk among themselves, trying to recall concepts from the readings. More than anything, it feels like a class section.Gabrieli asks, “What is ‘importance?’” Guo and Hostin look at each other with uncertainty. “It should be able to be inferred, I think, from the word itself,” he continues. “What is the absolute magnitude of the thing we’re interested in?”There are a lot of math terms, too. One of the readings constructs a complicated-looking product of derivatives to make the conceptual point that importance, tractability, and neglectedness are all vital considerations. Importance is how important a problem is; tractability is how much good can be done relative to effort; neglectedness is how much other people are already working in that area.Still, the fellows have questions, and Gabrieli and Guerra are happy to discuss them. Should your personal interests play a role in what you decide to focus on? What about problems that require immediate responses, rather than careful calculation of impact? They talk but don’t come to any singular answer.Near the end of the hour, Guerra brings up GiveWell, a well-known EA organization that maintains a ranked list of recommended charities. He asks the group: Of the top four causes on the website, which one would you donate to, and why?First on the list is Malaria Consortium, which provides a kind of medicine called seasonal malaria chemoprevention. Malaria treatments are high-importance, and tractable because they are relatively cheap; GiveWell estimates that it costs $7 to protect a child from malaria. Second on the list is the Against Malaria Foundation, which provides malaria nets for about $5 each.Tjandra, a Crimson Multimedia editor, ultimately chooses the fourth-ranked charity, which provides cash incentives to caregivers in Nigeria who vaccinate their babies. Unlike malaria treatments, he reasons, vaccines are “more generalizable to poor people everywhere.”Guo asks if instead of providing Vitamin A supplements to areas in sub-Saharan Africa, as the third-ranked charity does, we could try to integrate vegetables rich in Vitamin A into those communities. “‘Eat some yams,’” she says, laughing.Hostin concurs: “I don’t want to be like ‘Oh, here’s a fish’ rather than teach them how to fish, in a way.”A One in Ten Risk of ExtinctionAt the end of the Arete fellowship, you are given a $10 gift card to the charity donation site Every.org. If you donate $10 of your own money, you receive another gift card. But what happens next is up to you: Since Harvard EA has no centralized meetings or agenda, continued membership means “joining the various side groups that we have, depending on what you’re interested in,” Shuman says.Donating large parts of your income to charity — or earning to give, in the style of Bankman-Fried — are largely out of reach for college students, Jurkovic tells us. Instead, we find out, undergrad EA groups tend to focus on research and recruitment.Still, the issues Harvard EA focuses on don’t always line up with the picture of EA suggested by the Arete fellowship. Zazie Huml ’25, Harvard EA’s Global Health Programming Lead and one of four people on its board, said when they joined Harvard EA there was “no major initiative” in global health or international development — two of the five major topics covered in the Arete fellowship — and there were only “a couple people” involved with animal rights, a third focus area.“In Harvard EA we try not to present unfairly biased opinions towards any particular world problem,” Jurkovic says. “We aim to present the facts about the world problems and also give people useful decision-making tools so that they can examine the facts themselves.”“My entire experience with EA at Harvard last semester was, ‘Oh, this is not for me, this is not my community. They’re not interested in the same things as me,’” Huml says, until someone they met in the organization encouraged them to take another look. “If I was to take initiative within the system, there would be resources to support me,” they recall the person saying. Huml has since led several global health initiatives under the umbrella of Harvard EA, including a “comprehensive study on source apportionment of lead exposure” in lower-income countries in partnership with the Lead Exposure Elimination Project.So what was Harvard EA doing? “It was only focused on longtermism and AI,” Huml remembers.Longtermism — week five on the Arete syllabus — is the view that people who will be alive in the future warrant the same moral consideration as people alive today. “If we want to do the most good, that means we want to help the most people,” Shuman says, “and the most people is not at the specific time that we’re living in.” Since future people will significantly outnumber today's people, barring a mass extinction event, longtermists argue that we should devote more resources to preventing “existential risks” like nuclear warfare or engineered pandemics.This is something of a departure from EA principles in other areas, which our interviewees explain are a refinement of natural human instincts: You already want to do good, and EA just teaches you how to be smart about it. For a longtermist, though, what is at stake is the future of human existence.In recent years, longtermists have turned their attention to the field of AI safety. As AI models grow increasingly powerful, EA researchers have argued, the existential threat they pose may become insurmountable. They call this the problem of “AI alignment”: ensuring that if or when a superhuman AI comes into existence, its values align with our own, so that it does not kill everyone on Earth.Some AI safety researchers have been sounding this alarm for decades — but this past year, thanks to the rise of shockingly powerful, publicly available AI models like ChatGPT, the issue has made it into the mainstream. ChatGPT is already prone to spreading misinformation. A “sufficiently powerful” AI could be much worse, researcher Elizier Yudkowsky argued in a recent Time magazine op-ed calling for an indefinite moratorium on AI development. “In today’s world you can email DNA strings to laboratories that will produce proteins on demand allowing an AI initially confined to the internet to build artificial life forms or bootstrap straight to postbiological molecular manufacturing,” he wrote.How do you quantify incalculable destruction? “The Precipice: Existential Risk and the Future of Humanity,” a book by Toby Ord about existential risk, for which Harvard EA runs a reading group, puts the risk of extinction in the next century from unaligned AI at 1 in 10, higher than any other source of risk. Last July, Jurkovic wrote in a comment on the EA forum that “existential risks are high up on the list of most probable causes of death for college aged-people”: Assume that the probability of achieving superhuman AI by 2045 is 50 percent, and assume that the probability of death given superhuman AI is at least 10 percent. Then the probability of death by AI in the next few years might be comparable to around 1 in 6000, he wrote, explaining that this probability is similar to the two largest causes of death for “college-aged people in the US,” suicide and vehicle accidents, although he did not write out the calculations leading to this conclusion.Jurkovic guesses that there are more people in Harvard EA working on AI than other problems, but points out the existence of organizations with other EA-related focuses, including Harvard College Animal Advocates.Harvard EA doesn’t have an AI safety program itself, but there are several related organizations that do. One of these is the Harvard AI Safety Team, which was founded in 2022 by Xander L. Davies ’23 and Fiona E. Pollack ’25. Though HAIST is not an EA organization, many members of Harvard EA work with HAIST, and HAIST also receives funding from national EA philanthropy organizations. HAIST and Harvard EA also share members with other local AI groups, including Cambridge Boston Alignment Initiative and MIT AI Alignment.HAIST hosts reading groups and talks by professors, with a focus on machine learning research, Davies tells us on Zoom. He has curly hair and a black hoodie; the “L” in his name stands for Laser. Although he doesn’t believe that AI right now is poised to destroy humanity, he also doesn’t believe we have the tools to stop it. “I think how I look at it is, it’s currently impossible to get our AIs to not do things,” Davies says, referring to the ease with which users have bypassed ChatGPT-like models’ built-in filters against violent speech and misinformation.“Rapid progress in AI is becoming more and more economically useful, becoming more and more trusted, while at the same time this stark lack of progress on actually understanding how these systems work, on getting confidence that we actually know how to make these systems do what we want, is very startling to me,” Davies says. “And I think it should be a core priority on the global stage.”‘The Warm Fuzzy Feeling Just Doesn’t Matter as Much’In some respects, EA seems fairly intuitive: Who doesn’t want to minimize suffering as much as possible? In other respects, it pushes you to rethink your intuitions.Take, for instance, a thought experiment effective altruists often use to illustrate the unique way this philosophy navigates moral quandaries. It’s called “the drowning child scenario,” originally formulated by Peter Singer. Imagine you’re on your way to an important event, and you notice a child is drowning in a nearby pond. Do you jump in to save the child?Barring circumstances like an inability to swim, most people answer yes. But then, you’re asked the question again and again. Each time, the stakes are higher: What if you will ruin your clothes and waterlog your phone by jumping in? What if you already saved a drowning child last week? What if this child was an undocumented immigrant? What if the pond was far enough away that you would have to spend gas money to get there?As the hypotheticals escalate, generally, participants continue to decide to save the drowning child each time. But in the real world, when facing situations like determining healthcare access for immigrants or people in other countries, the increased distance leads some people to make what is in effect the opposite choice. EA wants to know: Why should you value those peoples’ lives less than those of people closer to you?When we spoke to Marka F. X. Ellertson ’23, then the president of Harvard EA, last September, she told us that with EA efforts, “the warm fuzzy feeling just doesn’t matter as much to me as the rational thought that I know that I’ve had a bigger impact.”“And I actually still do want that warm fuzzy feeling,” Ellertson added, explaining that she donates to local causes that are particularly meaningful to her.Joshua D. Greene ’97, Harvard EA’s faculty advisor, disputes the idea that EA strips away the warmth of charity work.Utilitarianism might make you think of “the things that kind of serve a function but don’t nurture our souls or to speak to our heart’s greatest desires, right? And utilitarianism is not just about cold functionality,” he says. “It’s about everything that makes life good or bad, everything that makes life worth living, everything that makes life meaningful.”Harold H. Klapper ’25, who participated in a Harvard EA fellowship last year, tells us that in some EA dialogues about utilitarianism “get really wild.”At a Boston-area EA event, for instance, “I’ve had conversations arguing about whether we should kill all wild animals, because they have negative lives,” Klapper says. “An ant colony must just have negative utility in the sense that they’re just not enjoying life, and so it’d be better if we just eliminated them.”“When things are a movement, you kind of have to buy into the whole thing, and when you buy into the whole thing, you get really wacky and fucked up answers to problems,” Klapper adds.Effective altruists seek to apply EA principles to personal decisions: what to study, where to work. If you are a college student interested in building EA communities, you might “consider not going to Harvard, as there are a bunch of people there doing great things,” Jurkovic wrote on the EA forum in December, suggesting that going to other colleges without strong EA movements could be better. (Was this something Jurkovic himself considered when applying to Harvard? “No,” he says, laughing.)A lot of EA discourse revolves around career choice: You will probably work for around 80,000 hours in your lifetime — several of the people we talk to cite this estimate — and you should spend them doing things that count, even if they may not be things you enjoy.Harvard EA, Shuman says, focuses mainly on “getting high-potential individuals into careers where they can spend their 80,000 hours of their career on solving these issues.”Jōsh P. Mysoré ’26 completed the Arete fellowship last fall and is considering becoming a discussion leader at some point in the future. Mysoré loves poetry, but is planning on concentrating in Computer Science and Linguistics. By Marina QuTo get them to that point, EA might also influence Harvard students’ concentration decisions. One such student is Jōsh P. Mysoré ’26; when we meet him outside of Blackbird, he’s reading Giovanni Boccaccio’s “The Decameron” for class. Mysoré completed the Arete fellowship last fall and is considering becoming a discussion leader at some point in the future.“I love poetry. I loooove poetry,” he tells us. “Will I be going into poetry? No. Because I don’t think it will actually do good for people.” At the moment, Mysoré plans to concentrate in Computer Science and Linguistics.Computer Science, as well as Statistics and Applied Mathematics, are fairly common concentration choices among the people we meet. Klapper tells us he knows someone in Harvard EA who studies Computer Science and dislikes it, but continues in the field because they believe it’s the most effective use of their time.Mysoré “was given a certain amount of privilege in my life to even get to this point,” he tells us. “I do think I owe something to the greater good of humanity to do something that impacts more people in a tangible way.”Does he think anyone should go into poetry, we ask.“I don't think it’s a contradiction to say that I can hold two opposing viewpoints at the same time,” Mysoré says. “Like in my heart, I’m a humanist, and I’m very romantic.” He tells us that he joined EA specifically to challenge these humanist viewpoints, but his perspective might flip again. “Honestly, I do think there should be poets,” he says.Mysoré tells us he still believes that EA has a noble mission, even if he disagrees with some of its particular approaches. “I think at the baseline, EA is creating dialogue,” he says. “That is really what counts.”‘Not To Create a Club, But Rather to Create a World’On Saturdays, Harvard EA throws socials in a house near campus where Jurkovic lives with four of his friends. Half the time, the socials are just for Harvard affiliates; every other week, they are open to students from other Boston-area schools.Of course, the socials are designed to be fun, but they have a functional purpose as well. “One important part of having a community is that the people talk to each other and have time spent together, so that they can collaborate and talk about their projects,” Jurkovic says.Though Jurkovic declined our request to attend a social on the record, we can try to reconstruct the vibe from a guide that he posted on the EA forum called “How to Organize a Social.” Indeed, in the post, he records every step of preparing for a social in granular detail, providing recommendations for everything from grocery lists — CLIF Bars, Diet Coke, several varieties of Vitaminwater — to music, such as the Spotify-curated playlist “my life is a movie.” Jurkovic suggests you make it easy for guests to find answers to anticipated questions: “The shoes on/off policy? Where the bathroom is? Where one can get water? What the wi-fi password is?”Last year, Trevor J. Levin ’19, who is currently on leave as the co-president of the university-wide EA group, also created a list of recommendations for effective retreats: They should happen in the beginning of the semester, when people are less busy; include lots of time for one-on-one interactions and a “structured vulnerable/emotional thing”; and include a healthy mix of new recruits and “moderately charismatic people for whom EA is a major consideration in how they make decisions.” These suggestions were embedded in a long post, which, citing feedback from Ellertson, Davies, Jurkovic, and others, argues that college EA groups should focus more on retreats as a method of bonding.For Levin, a former Crimson editor, this kind of immersive social situation is vital to capturing those who might be interested in EA but don’t prioritize it.“While most of the important cognition that happens is social/emotional, this is not the same thing as tricking or manipulating people into being EAs,” he wrote on the forum. Instead, retreats are meant to appeal to those who may agree with EA on some level but have not yet acted on it, and giving them time to “move closer to the values they previously already wanted to live by.”Since EA was born, it has been very deliberate about the image it projects. The name “effective altruism” was itself the product of a long debate: “This has been such an obstacle in the utilitarianesque community — ‘do-gooder’ is the current term, and it sucks,” MacAskill, the philosopher, wrote in a 2011 email chain. What followed was a period of brainstorming — fusing terms like “utilitarian” and philanthropist” with “alliance” and “institute” — and a series of votes to establish a name for both “the type of person we wanted to refer to, and for the name of the organization we were setting up.”Now, countless blogs and forum posts are dedicated to determining how best to recruit new members to the EA community. In December 2021, for instance, Jurkovic wrote a post on the EA forum describing an “organic” way to pitch EA to students.“Person: What do you want to study? Me: Not sure, I’m trying to find what to study so I can have as good of an impact as possible,” he wrote in an example dialogue. “If their level of enthusiasm stays high or grows, pitch an intro fellowship or a reading group to them.”Daniela R. Shuman ’24, a Computer Science and Statistics concentrator interested in urban development, is one of the chairs of the Arete fellowship. By Marina QuEven if some people choose not to become effective altruists, Shuman tells us, they could still take away valuable ideas from the movement.“The point is not to create a club, but rather to create a world of people that want to do the most good, and EA just has a set of tools that it thinks are probably the most good,” Shuman says. “We want everybody to think in these terms.”Daedalus HouseI’m gonna talk from a removed, omniscient perspective,” Mysoré says, kicking his chair back and folding his arms behind his head. EA spends a lot of money on space, food, and socials, he tells us. “At a certain point you have to ask yourself: What is effective about that?”Most of Harvard EA’s money comes from larger EA organizations like Open Philanthropy, a grantmaking foundation largely financed by Cari Tuna and Dustin Moskovitz, the latter of whom co-founded Facebook. Open Philanthropy distributes money to a range of EA-related causes. Put simply, it is an organization that “cares about making the world better,” Jurkovic says.We ask him how Harvard EA uses its grant money.“It’s not my area of expertise,” Jurkovic says. “But ...” He pauses for 15 seconds. “Yeah, just sometimes we get funding for club activities.”In 2022, we later find out, part of an Open Philanthropy grant was used to send Arete fellows and the University-wide EA group on a weekend trip to Essex Woods, a serene, Thoreauesque venue an hour north of campus that charges about $5,000 per night. According to GiveWell, donating $10,000 to the nonprofit Malaria Consortium could save the lives of five people.The schedule was similar to that of a corporate retreat: workshops, games, dinner, hot tub, Hamming circles. Well, maybe not the last one. Hamming circles are an activity where three to five participants sit down together and talk through one problem facing each member in 20-minute chunks. It’s “similar to what happens in a pair debug,” a post on an EA-related forum explains. These problems might vary, the post says, from “Is it possible for me, specifically, to have a meaningful impact on existential risk” to “I need to secure $250,000 in seed funding for my startup” to “I’m expected to speak at my father’s funeral and I have nothing but scathing, bitter, angry things to say.”Open Philanthropy also issued a $250,000 grant for the Centre for Effective Altruism to “rent and refurbish” an office for the Harvard AI Safety Team in Harvard Square for one year.Davies, the HAIST co-founder, tells us that the HAIST office is “pretty research-y.”“People are often at whiteboards, talking about problems with each other,” he says. “I think it feels like people are really trying to make progress on this technical problem, which I find exciting. It’s maybe a little startup-y in vibe.” In a post on the EA forum in December, Davies wrote that “investing effort into making the space fun and convenient to use helped improve programming, social events, and sense of community.”In August, Open Philanthropy recommended an $8.9 million grant for the Center for Effective Altruism to lease an EA office space for five years in Harvard Square. While the space would have been unaffiliated with Harvard EA, a forum post announcing the office promised that part of it would contain “meeting spaces for students at Harvard and other Boston-area schools,” and thanked Levin and Jurkovic for their help in developing the project.Forum members, including Levin and Jurkovic, threw out potential names for the space. Some of the suggestions were mythological — “Daedalus,” who advised Icarus not to fly too close to the sun — some cosmological — “Supercluster,” “Earthrise” — and some silly, like “Aardvark,” from a user who argued the name sounded similar to “Harvard” and would show up first in alphabetical lists.“Don’t like including the actual words EA in the name of the space,” Levin (who, for his part, liked “Apollo”) wrote in the comments. “It increases the chances of hypocrisy charges (from people who haven’t thought much about the effects of nice offices on productivity) for getting a nice central office space while ostensibly being altruistic.”But the Apollo House — or Aardvark House, or Supercluster — never materialized. According to Levin, after CEA signed the lease and began preparing the space, Open Philanthropy notified them that the grant was under review. As a result, Levin tells us, CEA is now trying to sublease the space to get the money back.CEA and Open Philanthropy did not respond to questions about the current status of the Harvard Square office grant.In addition to money for spaces and retreats, Open Philanthropy has an open request form for university group funding, and regularly recommends grants to undergraduate organizers. Other EA-affiliated organizations also fund events and projects. Huml, the Global Health Programming Lead, tells us that this is part of what makes EA a valuable community in which to pursue global health work.“To be totally transparent, I don’t 100 percent align with the values,” Huml tells us. “I think that they are an incredible platform and have a lot of resources — and those resources are financial, they are access to experts in very specific fields.”Three Harvard students, including Davies and Gabrieli, were recipients of Open Philanthropy’s fall 2022 University Organizer Fellowship, for which the organization recommended a total of $3.1 million across 116 recipients. Gabrieli declined to be interviewed for this article. Davies says he doesn’t know if he’s allowed to disclose how much money he actually got, but that he considers the grant to be “an hourly wage,” since he quit previous jobs to focus on developing HAIST.In February 2022, Open Philanthropy recommended a $75,000 grant to Pollack, the other HAIST co-founder, “to support her work organizing the effective altruism community at Harvard University.”When we reach out to Pollack, she tells us over email that she is “no longer organizing for the Harvard Effective Altruism group,” but has spent about $14,000 of the grant on HAIST expenses with Open Philanthropy’s approval: $7,200 to monthly software costs like Airtable and Squarespace, and most of the rest to accommodations for a workshop that HAIST hosted with the MIT AI safety group in Arlington, Virginia.Harvard EA is aware that this allocation of money can appear at odds with its stated mission. After the Essex Woods retreat, organizers sent out a feedback form. “How much did the spending of money at this retreat make you feel uncomfortable?” one question asked.We talk to Levin, the University EA co-president, and he likens it to the way that companies spend money on recruitment. “The idea is that there are problems that are much more talent-constrained than money-constrained,” he tells us. AI safety, a problem that relatively few people are working on, is an extreme example of this, he says. “The question then becomes, ‘Okay, well, if we have money and not people, how do we convert between the two?’”Levin pauses and corrects himself: “My train of thought there sounded kind of like I was saying, well, if you have a bunch of money, what do you do with it, right? That is not what I think.” What he does believe is that physical environments like retreats can rapidly accelerate the rate — by up to 100 times, he writes on the forums — at which people get on board with EA principles.Several people in EA, Levin guesses, joined because of their experiences on a retreat. “That is absolutely something that we would have paid this money for,” he says.Shuman believes that this outreach is particularly effective in Cambridge because of its highly motivated, change-driven student body. “Harvard and MIT have done the vast majority of vetting for people who are highly ambitious,” she says.Shuman tells us that the international EA community places a lot of importance on this kind of community building. “If they can invest $1,000 in getting five high-potential individuals to, instead of doing AI research, do AI safety research, that’s a pretty good use of money,” she says. “It could save a lot of lives, potentially.”‘A Skewed Pipeline’In a 2020 survey of EA Forum members, 76 percent of respondents were white and 71 percent were male. Though there is an imbalance, the Centre for Effective Altruism argues that diversity is important for several reasons.“If an EA-aligned newcomer concludes that effective altruism is not for ‘people like me,’ they may not get involved, and the EA community may be less effective,” its website reads. “We don’t want to miss important perspectives.”We ask Jurkovic if he’s aware of demographic imbalances within EA groups at Harvard. He pauses. “I think it is quite important to have a community which is welcoming to everyone,” he says. “EA sometimes shares a problem with the cause areas that it tackles” — meaning STEM fields — “which is that many of them have more males in them than average.”Shuman — “the only she/her” leading an Arete section, she tells us — echoes this sentiment, saying that these numbers reflect existing disparities in STEM and philanthropic fields.“It’s just a skewed pipeline,” she says, “which is a problem.”“I can say from personal experience that we’ve had quite diverse groups,” Nickols says. “In terms of gender, it does tend to be more male-skewed, and that’s something that we’re continually working on.” He acknowledges that Harvard EA “is probably predominantly white and Asian, but not more so to Harvard’s general population.” (The organization does not keep demographic records of its members, so we can’t verify this.)Nickols says that the applicants for Harvard EA’s fellowships also tend to skew male. “Given that word of mouth is our biggest kind of spreader, it might just be possible that guys who have done it in the past are friends with more guys and tell them about it,” he adds.In recent months, the EA movement has been embroiled in controversies related to race and gender in its communities.One of these controversies revolved around Nick Bostrom, a philosopher whose ideas led to the development of longtermism; four of his works are cited in the syllabus for Harvard EA’s Precipice Fellowship.In January, Bostrom posted a letter to his website apologizing for a comment he wrote on a forum in the mid-90s, which claims that Black people “are more stupid than whites” and contains the n-word. In the letter, Bostrom castigates his past self for using the slur and writes that the comment “does not accurately represent my views, then or now,” but does not reject the possibility of genetic cognitive differences between races. He leaves this question to “others, who have more relevant knowledge.” The letter continues with a section about bioethics that opens: “What about eugenics? Do I support eugenics? No, not as the term is commonly understood.”In March, Time magazine  interviewed seven women who said they had been sexually harassed, coerced, or assaulted within EA spaces, particularly in the Bay Area. The scene’s overwhelming maleness, tech-bro culture, and impulse to quantify and rationalize messy real-world dynamics created a deeply unsafe environment, the women said. One described having dinner with a prominent researcher nearly twice her age who told her that “pedophilic relationships between very young women and older men was a good way to transfer knowledge.”“We were of course upset by both of these issues,” Jurkovic wrote in an email to us about the Bostrom letter and Time investigation, “and have spent time figuring out how we can improve our diversity and make sure we're a welcoming community to women and people of color.”Will A. Nickols ’24 is an Arete Fellowship chair. By Marina QuAlthough some of EA’s focus areas deal with global health and economic growth in underdeveloped countries, its frameworks generally do not foreground race or gender. A version of the spring 2023 Arete syllabus posted on the Harvard EA website only mentions race in the overview of Week Four: Animal Welfare.“One of the most important ways we can fail to identify the most important moral issues of our time is by unfairly shrinking our moral circle: the set of beings we deem worthy of our moral concern,” the syllabus reads. “For example, many whites in the US failed to identify that slavery was the moral issue of their age by excluding Blacks from their moral circle. To truly make the world better, we must look beyond the traditional moral horizon for those who are unfairly neglected by mainstream society. This week, we discuss one such group of beings: nonhuman animals.”We ask Nickols, the Arete co-chair, about this framing. He tells us that it is important to keep the quote “in the context of where it was originally formulated.”“Obviously the idea here is not to equate certain racial groups with animals or anything like that,” Nickols says. “Over time, though, the expanding moral circle idea is that white people who, before, held these extremely racist and terrible views — as the generations went on, and as culture shifted — began to see people, regardless of their race, as all morally equal.”“We have not reached a point where racism is totally gone,” Nickols says, “but there is definitely a shift in the right direction here. And more generally, the idea is that as time goes on, it is quite possible that the circle will continue to increase.”PaperclipsOne thought experiment designed to demonstrate the danger of misaligned AI goes like this: Say the owner of a paperclip factory obtains an ultrapowerful AI and instructs it to maximize paperclip output. Although the AI is programmed to pursue a seemingly harmless goal, it might — if its understanding of values is not quite the same as ours — turn everything in the world into paper clips. That this scenario seems kind of silly is part of the point. Researchers are not concerned that AI will be “evil” per se, but that its pursuit of any objective, including “good” ones, might have unintended consequences.“AI systems donʼt always do what their developers intend,” the Arete syllabus reads. “They replicate human biases, achieve their goals in surprising and destructive ways, and are vulnerable to external manipulation.” As a call to action, this is a compelling place to start. As a taxonomy, though, it is less an observation about AI than it is about systems. Existential risks like climate change might first destroy the people who did the least to create them; any movement created by people is, in a sense, only human.This point came up in the wake of the FTX collapse — what did it mean that a group seeking to fundamentally change the world relied so heavily on existing distributions of power? — and it has come up again in the months since, in the course of writing this article. Can you optimize your life? What if the thing we construct in our idealized image turns out not to look so different from us after all?For Andrew N. Garber ’23, a former Arete leader, considering questions is the point of EA. There is a common misconception that effective altruism is a destination, when really it’s more of a framework, he tells us: “It is more concerned about the question than any one specific answer.”In any event, when we ask Jurkovic what he hopes EA will look like in the future, his response is straightforward. “The goal is to help people make the world better,” he says, half smiling. “As much as possible.”— Associate Magazine Editor Bea Wall-Feng can be reached at bea.wall-feng@thecrimson.com. Follow them on Twitter @wallfeng.— Magazine writer Sophia C. Scott can be reached at sophia.scott@thecrimson.com. Follow her on Twitter @ScottSophia_.","length":42415,"excerpt":"“Most of us want to improve the world. We see suffering, injustice, and death and feel moved to do something about it,” the Harvard EA website says. “But figuring out what that ‘something’ is, let alone actually doing it, can be a difficult and disheartening challenge. Effective altruism is a response to this challenge.” Can it live up to that goal?","byline":null,"dir":null,"siteName":null,"lang":null},"finalizedMeta":{"title":"What is Going On With Effective Altruism? | Magazine | The Harvard Crimson","description":"“Most of us want to improve the world. We see suffering, injustice, and death and feel moved to do something about it,” the Harvard EA website says. “But figuring out what that ‘something’ is, let alone actually doing it, can be a difficult and disheartening challenge. Effective altruism is a response to this challenge.” Can it live up to that goal?","author":false,"creator":"","publisher":false,"date":"2024-08-29T04:36:05.853Z","topics":[]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"What is Going On With Effective Altruism? | Magazine | The Harvard Crimson","description":false,"canonical":"https://www.thecrimson.com/article/2023/3/30/ea-scrut/","keywords":[],"image":"data:image/png;base64,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","firstParagraph":"Sections"},"dublinCore":{},"opengraph":{"title":"What is Going On With Effective Altruism? | Magazine | The Harvard Crimson","description":"“Most of us want to improve the world. We see suffering, injustice, and death and feel moved to do something about it,” the Harvard EA website says. “But figuring out what that ‘something’ is, let alone actually doing it, can be a difficult and disheartening challenge. Effective altruism is a response to this challenge.” Can it live up to that goal?","url":"https://www.thecrimson.com/article/2023/3/30/ea-scrut/","site_name":false,"locale":false,"type":"Post","typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/143735_1362611.jpeg.2000x1294_q95_crop-smart_upscale.jpg"},"twitter":{"site":"@thecrimson","description":"What is Going On With Effective Altruism? | Magazine | The Harvard Crimson","card":"summary_large_image","creator":false,"title":"What is Going On With Effective Altruism? | Magazine | The Harvard Crimson","image":"https://s3.amazonaws.com/thumbnails.thecrimson.com/photos/2023/03/31/143735_1362611.jpeg.2000x1294_q95_crop-smart_upscale.jpg"},"archivedData":{"link":false,"wayback":false}}}