{"initialLink":"https://jamesg.blog/2024/06/05/responsible-web-crawling/","sanitizedLink":"https://jamesg.blog/2024/06/05/responsible-web-crawling/","finalLink":"https://jamesg.blog/2024/06/05/responsible-web-crawling/","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://jamesg.blog/2024/06/05/responsible-web-crawling/\" itemprop=\"url\">Notes on responsible web crawling | James' Coffee Blog</a></contexter-box-head></contexter-box-head><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2024-06-05T18:52:54.314Z\">6/5/2024</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>In my blog post brainstorming a new indie web search engine, I noted that running a web search engine is hard. With that in mind, I started to think that I haven't written too much about what I learned about web crawling when running IndieWeb Search, a search engine for the indie web. IndieWeb Search crawled a whitelist of websites, searching for pages, and indexed them for use in the search engine.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a is=\"contexter-link\" href=\"https://jamesg.blog/2024/06/05/responsible-web-crawling/\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"7b4a16a1dfbdc41f546163520236c1c847434f04","data":{"originalLink":"https://jamesg.blog/2024/06/05/responsible-web-crawling/","sanitizedLink":"https://jamesg.blog/2024/06/05/responsible-web-crawling/","canonical":"https://jamesg.blog/2024/06/05/responsible-web-crawling/","htmlText":"<!doctype html>\n<html lang=\"en\">\n  <!-- \n  Welcome, fellow web wanderer, to James' Coffee Blog: web developer edition!\n\n  Here, I have documented what various parts of my website mean. I haven't documented everything, but I hope what is below is useful to you.\n\n  In this HTML, you will be introduced to everything from microformats to Webmention to penguins.\n\n  This site is generated using a static site generator. Static site generators use logic to generate HTML.\n  \n  If you see a page that lists blog posts, for example, that list was generated with some static site generator magic.\n  \n  If you love making personal websites, you may like the IndieWeb community! Join us at https://indieweb.org/discuss.\n\n  If you have questions about my website and how it works, feel free to email readers [at] jamesg [dot] blog!\n\n  Please feel free to use the source code below as inspiration in your web adventures.\n\n  If you are unsure whether to start a personal website: I recommend you give it a try! Your site can be a single web page! You can grow it over time if\n  that interests you. If it doesn't, that's okay, too: your website is _your_ place on the web.\n\n  I hope you have a wonderful day, and that you continue wandering the web to explore more websites!\n\n  --------------\n\n  Now, a quote from Tomorrowland:\n  \n  Nix: How would your jet pack make the world a better place?\n\n  Young Frank Walker : Can't it just be fun?\n\n  --------------\n  -->\n  <head>\n    <meta charset=\"utf-8\" />\n\n    \n      <title>Notes on responsible web crawling | James' Coffee Blog</title>\n    \n\n    <!-- styles.css contains the CSS that styles most of this website. -->\n    <link rel=\"stylesheet\" href=\"https://jamesg.blog/assets/styles/styles.css\" />\n    <link rel=\"stylesheet\" href=\"https://jamesg.blog/assets/styles/annotate.css\" />\n\n    <!-- I use Webmention to receive comments on my website. Learn more at https://indieweb.org/Webmention -->\n    <link rel=\"webmention\" href=\"https://webmention.io/jamesg.blog/webmention\" />\n\n    <!-- rel=me lets me specify where to find me on other sites.\n      These links can be used with IndieAuth for authentication.\n      If jamesg.blog links to a profile page with rel=me, and that profile page links back to jamesg.blog,\n      it can be inferred that the profile page and jamesg.blog are controlled by the same person.\n      This allows rel=me to be used for authentication, when a two-way link is made.\n      Learn more about rel=me at https://indieweb.org/rel-me -->\n    <link href=\"mailto:jamesg@jamesg.blog\" rel=\"me\">\n    <link rel=\"me\" href=\"https://indieweb.social/@capjamesg\" />\n    <link rel=\"me\" href=\"https://news.ycombinator.com/user?id=zerojames\" />\n    <link rel=\"me\" href=\"https://orcid.org/0009-0006-3736-1653\" />\n\n    <!-- This is used to optimise my website for mobile use.\n      See https://developer.mozilla.org/en-US/docs/Web/HTML/Viewport_meta_tag for more information about what the tag means. -->\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n\n    <!-- These links let RSS readers discover my website's RSS feed\n      so that people can subscribe to this website! -->\n    \n      <link rel=\"alternate\" type=\"application/rss+xml\" title=\"RSS\" href=\"https://jamesg.blog/feeds/posts.xml\">\n    \n\n    <!-- This code sets the background for a page.\n      This code is here because the back-end of my site (the part you can't see!)\n      decides what background to use on every page load. -->\n    <style>\n        body {\n        \n            background-image: url(\"https://jamesg.blog/assets/g-room.png\");\n        \n            background-size: cover;\n            background-repeat: no-repeat;\n            background-position: center;\n            background-attachment: fixed;\n        }\n    </style>\n\n    <!-- This link specifies a manifest that states a search engine you can use on my site. -->\n    <link rel=\"search\" type=\"application/opensearchdescription+xml\" title=\"IndieWeb Search\" href=\"https://jamesg.blog/assets/opensearch.xml\">\n\n    <!-- This link contains information that mobile devices use to render progressive web apps (PWAs).\n        PWAs make websites feel more native to some mobile devices when a web page is added to a device home screen\n        or saved as a device app. -->\n    <link rel=\"manifest\" href=\"https://jamesg.blog/assets/manifest.json\">\n\n    \n      <meta name=\"description\" content=\"In my blog post brainstorming a new indie web search engine, I noted that running a web search engine is hard. With that in mind, I started to think that I haven't written too much about what I learned about web crawling when running IndieWeb Search, a search engine for the indie web. IndieWeb Search crawled a whitelist of websites, searching for pages, and indexed them for use in the search engine.\" />\n    \n\n    <!-- Microsub (https://indieweb.org/Microsub) is an experimental social reader protocol.\n    This header tag is used by the Microsub reader I use to find its associated server. -->\n    <link rel=\"microsub\" href=\"https://aperture.p3k.io/microsub/687\">\n    \n    \n      <!-- screenshots.jamesg.blog generates preview screenshots of my blog posts to show on social platforms.\n        See https://github.com/capjamesg/screenshots to set it up for yourself. -->\n      <meta property=\"og:image\" content=\"https://screenshots.jamesg.blog/?url=https://jamesg.blog/2024/06/05/responsible-web-crawling/\">\n    \n\n    <!-- This sets the icon that appears in your browser tab. -->\n    <link rel=\"icon\" href=\"https://jamesg.blog/favicon.ico\" />\n\n    <link rel=\"apple-touch-icon-precomposed\" href=\"https://jamesg.blog/favicon.ico\" />\n\n    <!-- rel=canonical tells search engines that this is the canonical page.\n      If someone triggers an easter egg on one of my blog posts and links to, for example,\n      https://jamesg.blog/2024/02/28/beginners-mind-empathy/?ts#ts,\n      the rel=canonical will tell search engines to ignore the ? and # and say that https://jamesg.blog/2024/02/28/beginners-mind-empathy/ is the main link.\n      Some sites use ?... to control the behaviour of a page, which is useful context for search engines. This is why rel=canonical is useful to specify. But you don't need to. -->\n    <link rel=\"canonical\" href=\"https://jamesg.blog/2024/06/05/responsible-web-crawling/\" />\n\n    \n    \n    \n    \n    <!-- This code powers the <recommend-firefox></recommend-firefox>\n      web component that appears at the bottom of this page.\n      A web component is a custom HTML element.\n      Learn more about how this component works at\n      https://jamesg.blog/2023/12/12/recommend-firefox/\n    -->\n    <script>\n        class RecommendFirefox extends HTMLElement {\n          constructor() {\n              super();\n              this.attachShadow({ mode: \"open\" });\n      \n              this.options = [\n                  \"<p>For a more privacy-focused browsing experience, I recommend <a href=\\\"https://www.mozilla.org/en-GB/firefox/new/\\\">Firefox</a>.</p>\",\n                  \"<p>Support browser diversity. Install <a href=\\\"https://www.mozilla.org/en-GB/firefox/new/\\\">Firefox</a>.</p>\",\n              ]\n          }\n      \n          connectedCallback() {\n               if (navigator.userAgent.indexOf(\"Chrome\") === -1) {\n                   return;\n               }\n                var message = this.getAttribute(\"data-message\") || this.options[Math.floor(Math.random() * this.options.length)];\n                this.shadowRoot.innerHTML = message;\n\n                var style = document.createElement('style');\n                style.innerHTML = `\n                    a {\n                        color: #006BD6;\n                        border-bottom: 2px dotted #006BD6;\n                        text-decoration: none;\n                    }\n                    a:hover {\n                        color: #0000CD;\n                        cursor: pointer;\n                    }\n                    p {\n                        font-size: 1.0625rem;\n                        line-height: 1.5em;\n                    }\n                    @media (prefers-color-scheme: dark) {\n                        a {\n                            color: lightgreen;\n                            border-bottom: 2px dotted lightgreen;\n                        }\n                        a:hover {\n                            color: #ccc;\n                        }\n                    }\n                `;\n                this.shadowRoot.appendChild(style);\n\n                if (localStorage.getItem('darkmode') == \"true\") {\n                    document.getElementById('darkmode').media = \"all\";\n                } else if (localStorage.getItem('darkmode') == \"false\") {\n                    style.innerHTML = `a { color: #006BD6 !important; border-bottom-color: #006BD6 !important; }` + style.innerHTML;\n                }\n          }\n      }\n      \n      customElements.define(\"recommend-firefox\", RecommendFirefox);\n    </script>\n\n    \n\n    <!-- The styles below show up when a user has dark mode enabled on my site.\n        The logic for when these styles are applied is in a script further down on the page. -->\n    <style id=\"darkmode\" media=\"none\">\n      html, .citation_box, details, summary, .social li, .ts {\n        background-color: #1F2937;\n      }\n      blockquote {\n        background-color: initial;\n      }\n      .ts {\n        color: white;\n      }\n      .highlight {\n        color: black;\n      }\n      .highlight a {\n        color: blue;\n        border-bottom: 2px dotted blue;\n      }\n      .highlight a:hover {\n        color: violet;\n      }\n      a {\n        color: lightgreen;\n        border-bottom: 2px dotted lightgreen;\n      }\n      a:hover {\n        color: #ccc;\n      }\n      h1, h2, h3 {\n        color: orange;\n      }\n      .social_interaction, .webmention, .hiring {\n        background: none;\n      }\n      p, li, dt, dd, .indieweb_footer, input, label {\n        color: white;\n      }\n      input, textarea, code, pre, main {\n        background-color: #0c2548;\n        color: #ccc;\n      }\n    </style>\n    <!-- prism.css is used by prism.js for code syntax highlighting. See https://prismjs.com/ to learn more about prism.js. -->\n    <link href=\"https://jamesg.blog/assets/styles/prism.css\" rel=\"stylesheet\" />\n  </head>\n  <body>\n    <main style=\"z-index: 2;\">\n        <!-- Skip link so that screen reader users can skip narration of the navigation bar. -->\n      <a href=\"#main\" class=\"accessibility_label\">Skip to main content</a>\n    \t<nav id=\"top_navigation\">\n\t<p><a href=\"/\" rel=\"home author\" class=\"p-author\">James' Coffee Blog <span aria-hidden=\"true\" class=\"seasonal_emoji\">‚òï</span></a></p>\n\t<ul>\n\t\t<li><a href=\"/indieweb/\">IndieWeb</a></li>\n\t\t<li><a href=\"/coffee/\">Coffee</a></li>\n\t\t<li><a href=\"/moments-of-joy/\">Moments of Joy</a></li>\n\t\t<li><a href=\"/advent-of-technical-writing/\">Technical Writing</a></li>\n\t\t<li><a href=\"/2024/01/03/software-technical-writing/\">Book</a></li>\n\t\t<li><a href=\"/2024/02/19/personal-website-ideas/\">Website Ideas</a></li>\n\t\t<li><a href=\"/explore/\">Explore</a></li>\n\t\t<li><a href=\"/wander/\">Wander</a></li>\n\t\t<li><a href=\"/feeds/posts.xml\">RSS</a></li>\n\t\t<!-- <li><a href=\"#\" onclick=\"document.querySelector('command-k').open(); return false;\">Search</a></li> -->\n\t</ul>\n</nav>\n      <!-- This code applies dark mode to a page if:\n            1. A visitor has dark mode enabled on their system and hasn't disabled dark mode using the button\n                in the site site navigation bar.\n            2. A visitor has enabled dark mode using the button in the site navigation bar.\n            Preferences for dark mode are saved across page views.\n            See https://github.com/capjamesg/darkmode.js for more info. -->\n      <script>\n        if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {\n          document.getElementById('darkmode').media = \"all\";\n        }\n\n        if (localStorage.getItem('darkmode') == \"true\") {\n          document.getElementById('darkmode').media = \"all\";\n        } else if (localStorage.getItem('darkmode') == \"false\") {\n          document.getElementById('darkmode').media = \"none\";\n        }\n\n        var navigation = document.getElementById(\"top_navigation\");\n        var ul = navigation.getElementsByTagName(\"ul\")[0];\n        \n        var li = document.createElement(\"li\");\n        var dark_style = document.getElementById(\"darkmode\");\n  \n        if (dark_style.media === \"all\") {\n          li.innerHTML = \"<a onclick='toggleTheme();'>Light Mode</a>\";\n        } else {\n          li.innerHTML = \"<a onclick='toggleTheme();'>Dark Mode</a>\";\n        }\n\n        var last_li = ul.getElementsByTagName(\"li\")[ul.getElementsByTagName(\"li\").length - 1];\n\n        ul.insertAfter(li, last_li);\n  \n        function toggleTheme() {\n          if (dark_style.media === \"all\") {\n            dark_style.media = \"none\";\n            li.innerHTML = \"<a onclick='toggleTheme();'>Dark Mode</a>\";\n            localStorage.setItem('darkmode', 'false');\n          } else {\n            dark_style.media = \"all\";\n            li.innerHTML = \"<a onclick='toggleTheme();'>Light Mode</a>\";\n            localStorage.setItem('darkmode', 'true');\n          }\n        }\n      </script>\n      <div id=\"main\">\n    \t <!-- This page uses microformats to structure different pieces of information.\n    I use h-entry to state this is a post. Any class name that starts with h-, p-, or -e is a microformat.\n    By specifying microformats, some web tools can better understand this post. For example, IndieNews can use\n    the p-name to figure out the title of the post (without the \"| James' Coffee Blog\" I add to the <title> tag of my website.\n    Learn more about h-entry: https://indieweb.org/h-entry\n-->\n<article class=\"h-entry\">\n\t<header>\n\t\t<h1 class=\"p-name\">Notes on responsible web crawling</h1>\n\t\t<p><em>Published on\n\t\t\t<a href='/2024/06'><time class=\"dt-published\" datetime=\"2024-06-05T00:00:00\">June 05, 2024</time></a>\n\t\t\t under the <a href=\"/coding\" class=\"p-category\">Coding</a> category.</em></p>\n\t\t  \n\t\t\n\t\t\n\t\t\n\t</header>\n\t<section class=\"e-content social_interaction\">\n\t\t<div class=\"special-image\">\n\t\t\t<p>In my blog post <a href=\"https://jamesg.blog/2024/06/05/indieweb-search-brainstorming/\">brainstorming a new indie web search engine</a>, I noted that running a web search engine is <em>hard</em>. With that in mind, I started to think that I haven't written too much about what I learned about web crawling when running <a href=\"https://github.com/capjamesg/indieweb-search\">IndieWeb Search</a>, a search engine for the indie web. IndieWeb Search crawled a whitelist of websites, searching for pages, and indexed them for use in the search engine.</p>\n<p>One challenge in particular when running search engines is ensuring that your search engine doesn't break someone's website. For example, suppose you are indexing a personal website. If your logic to parse URLs is incorrect, your crawler may spiral out of control and start crawling many pages that don't exist. A potential side-effect of this is that you put undue burden on someone's server, causing their site to be slower.</p>\n<p>This is why I outlined an indie web search engine that works by reading feeds rather than crawling (also called \"spidering\".</p>\n<p>Crawlers go from page to page and look for new links. They retrieve each page and index it. This is repeated until either a whole site has been indexed, or a crawl budget has been met.</p>\n<p>As soon as you get into crawling, there are many technical considerations to implement a responsible web crawler.</p>\n<p>With that said, I wanted to document what I learned in building a web search engine, and some of the ideas I have had since then pertaining to responsible web crawling. Below are <em>some of</em> the things you should do to ensure that your web crawler is responsible. There are likely other considerations that apply at different scales to the one on which I was working (~1,000 websites, ~500,000 pages).</p>\n<h2>URL canonicalisation</h2>\n<p>Ensure you have strong URL canonicalisation logic. This logic should take any URL on a site and normalise it into a standard form. For example, all the following URLs are valid, but equivalent when the domain being crawled is <code>https://jamesg.blog</code>:</p>\n<ul>\n<li><code>https://jamesg.blog</code></li>\n<li><code>https://jamesg.blog/</code></li>\n<li><code>/</code></li>\n</ul>\n<p>If you crawled all three of these pages, you have crawled three pages when you only needed to crawl one. This gets more complicated if a site has URL parameters that may or may not impact the page substantially. I decided to strip all URL parameters if I recall, but a large-scale search engine should respect them and identify whether URL parameters should be crawled and at what rate. That is out of scope for this guide.</p>\n<p>You should have well-tested logic that ensures URLs are canonicalized properly. Technically equivalent URLs should be consolidated. When you discover a new URL, it should be canonicalized to ensure you haven't already crawled it. If you haven't crawled the URL, you can put it in a queue. Of note, this is <em>not</em> related to <code>rel=canonical</code>. <code>rel=canonical</code> is a page stating that a given URL is canonical, but that is a consideration you arrive at when requesting a page.</p>\n<p>With poor URL canonicalization, you may end up crawling subtantially more pages than you need.</p>\n<h2>Redirects</h2>\n<p>Your search engine should have robust measures in place to manage redirects. Limit the number of redirects any URL can give. If you have a white list, don't crawl any URL that does not have a hostname allowed on your list. You should use a pre-existing library to check if hostnames match. Indeed, in general, lean on what others have written to write your crawler.</p>\n<p>For example, suppose a site is misconfigured and a URL that is <code>/example///</code> (with three slashes at the end) redirects to one with two slashes at the end (<code>/example//</code>), and that URL redirects back to the one with three slashes. Your crawler should be stop traversing these redirects and move on.</p>\n<h2>More tips</h2>\n<p>When you crawl a site, you should:</p>\n<ul>\n<li>Respect robots.txt. There are many parsers available that let you check if a URL is covered under a robots.txt policy given the user agent under which your search engine operates. Related: declare a user agent publicly and provide guidance on how people can limit or restrict crawls from your search engine. You are not a responsible search engine if you don't provide a clear means for people to limit crawling without their having to explicitly block your search engine.</li>\n<li>Respect the <code>Retry-After</code> header that states you should retry crawling a page after a certain period.</li>\n<li>Apply a timeout when you crawl URLs.</li>\n<li>Acknowledge 429s and make sure you update your crawl queue to prioritize other URLs that have not returned a 429.</li>\n<li>Look out for high incidence rates of 500s. This may indicate a site is running into stress, or has other technical issues. 500s are not useful in indexing, so you should back off and try again later.</li>\n<li>Crawl multiple sites at once, rather than crawling entire sites sequentially. If a site has 100,000 valid pages, you don't want to allocate all of your crawl capacity to that site all at once. Instead, you should crawl multiple sites at the same time. This will reduce the risk of running into 429s or causing problems.</li>\n<li>Have per-site crawl budgets. This could vary depending on the site. If you are making a small search engine, you may only choose to crawl 1,000 URLs from a site; for a larger search engine, this number may increase substantially.</li>\n<li>Use Last-Modified headers to check if a site has been modified since your last request.</li>\n<li>If a server is slowing down as you crawl, but no 429s are advertised, consider moving URLs from that site further back in your queue.</li>\n</ul>\n<p>These are some of the many considerations you should take into account when building a search engine that spiders between URLs.</p>\n<p>If you don't crawl web sites, and instead only download the content of feeds, the above considerations are less significant. This is why I outlined such an approach in a brainstorm for a new indie web search engin., If you are crawling 1000 feeds, and you only download the feed URLs rather than all the posts in each feed individually, there is substantially less risk of bringing down someone's site than if you are downloading thousands of URLs from their site. The considerations above are still useful if you decide to download any pages linked in a feed, though.</p>\n<p><em>If you have implemented a search crawler and have written advice on this topic, let me know by email. I can add a link to your post here.</em></p>\n\n\t\t\t\n\t\t\t<p><a href=\"https://news.ycombinator.com/submitlink?u=/https://jamesg.blog/2024/06/05/responsible-web-crawling/&t=Notes on responsible web crawling\">Share this post on Hacker News</a>.</p>\n\t\t<p><a href=\"https://lobste.rs/stories/new?url=/https://jamesg.blog/2024/06/05/responsible-web-crawling/&title=Notes on responsible web crawling\">Share this post on Lobste.rs</a>.</p>\n\t\t</div>\n\t\t\n\t\t\n\n\t\t<!-- My site has a cute mascot! Learn more about him at /mascot/.\n\t\tA friend suggested I come up with a name other than James, which I gave to the mascot in the aforelinked post. I thought about MJ, short for Mascot James.\n\t\tHerein, my mascot will be MJ and James. Or perhaps any of the myriad other nicknames I am likely to come up with over the years.\n\n\t\tI hired someone to design this mascot. The aforelinked post provides more information and a recommendation for the designer!\n\t\t--> \n\t\t<p><img src=\"/assets/mascot.svg\" alt=\"James' Coffee Blog mascot\" class=\"mascot\" /></p>\n\t\t<hr>\n\t\n\t\t<!-- Writing is a way for me to process the world around me. It is a craft that I enjoy refining. Please note: My words are written by me, not AI. -->\n\t\t<!-- On a related note, I ask many AI bots not to crawl my website: /robots.txt. This is my space. -->\n\t\t<p><a href=\"https://notbyai.fyi/\" class=\"notbyai_link\"><img src=\"/assets/ai.png\" height=\"42\" width=\"131\" alt=\"Written by human, not by AI\" class=\"notbyai\"/></a></p>\n\t</section>\n\t<section class=\"webmention\">\n    <h2>Responses</h2>\n    <div id=\"webmentions\"></div>\n    <h2>Comment on this post</h2>\n    <p>Respond to this post by sending a <a href=\"https://indieweb.org/Webmention\">Webmention</a>.</p>\n    <p>Have a comment? Email me at <a href=\"mailto:readers@jamesg.blog?subject=\">readers@jamesg.blog</a>.</p>  \n</section>\n</article>\n      </div>\n      <!-- This link makes it easy for users and screen readers to navigate back to the top\n        of this web page. -->\n      <p class=\"accessibility_label\"><a href=\"#main\">Go Back to the Top</a></p>\n      <footer>\n\t<ul>\n\t\t<li><a href=\"/time-machine/\">Time Machine</a></li>\n\t\t<li><a href=\"/coffee/maps/\">Coffee Maps</a></li>\n\t\t<li><a href=\"/projects/\">Projects</a></li>\n\t\t<li><a href=\"/talks/\">Talks</a></li>\n\t\t<li><a href=\"/archive/\">Archive</a></li>\n\t\t<li><a href=\"https://capjamesg.github.io/index/\">Index</a></li>\n\t\t<li><a href=\"/privacy/\" rel=\"privacy-policy\">Privacy</a></li>\n\t\t<li><a href=\"/#search\">Search</a></li>\n\t\t<li><a href=\"/contact/\">Contact</a></li>\n\t\t<li><a href=\"/feeds/posts.xml\">RSS</a></li>\n\t</ul>\n\t<div class=\"indieweb_footer\">\n\t\t<!-- Wait... James... <recommend-firefox> isn't a HTML element. Is it?\n\t\t\tYou're right: it's not! It's a web component: a custom HTML element.\n\t\t\tYou can learn more about what web components are and how this one works at\n\t\t\t/2023/12/12/recommend-firefox/\n\t\t-->\n\t\t<recommend-firefox></recommend-firefox>\n\t\t<a href=\"https://indieweb.org/Webmention\" class=\"button-link\"><img src=\"/assets/indieweb88x31-flat.png\" alt=\"Webmention logo indicating that you can send a webmention to this blog\" width=\"80\" height=\"15\" /></a>\n\t\t<a href=\"https://indieweb.org\" class=\"button-link\"><img src=\"/assets/indieweb_button.webp\" alt=\"IndieWeb logo\" width=\"80\" height=\"15\" /></a><br />\n\t\t<img src=\"/assets/coffee_button.gif\" alt=\"Coffee button\" width=\"80\" height=\"15\" />\n\t\t<a href=\"https://microformats.org\" class=\"button-link\"><img src=\"/assets/microformats_button.webp\" alt=\"Microformats logo\" width=\"80\" height=\"15\" /></a><br />\n\t\t<a href=\"/links#button\"><img src=\"/assets/blog_button.png\" alt=\"James' Coffee Blog button\" width=\"88\" height=\"31\" /></a><br />\n\t\t<div>\n\t\t\t<a href=\"https://xn--sr8hvo.ws/%F0%9F%98%85%F0%9F%8F%BD%E3%8A%97%EF%B8%8F/previous\">‚Üê</a>\n\t\t\tIndieWeb Webring\n\t\t\t<a href=\"https://xn--sr8hvo.ws/%F0%9F%98%85%F0%9F%8F%BD%E3%8A%97%EF%B8%8F/next\">‚Üí</a>\n\t\t\t<br>\n\t\t\t<!-- I use lights.png in my Christmas theme. It takes me a while to re-find the lights every year so I thought I'd leave them commented here. -->\n<!-- \t\t\t<img src=\"/assets/lights.png\" alt=\"\" /> -->\n\t\t</div>\n\t</div>\n</footer>\n\n\n\t<!-- This script shows Webmentions on my blog posts. Webmentions are sort of like comments or likes, but are published on someone's website first.\n\t\tThen, that site can notify you that they linked to you, so you can show their comment or like on your website.\n\t\tLearn more at https://indieweb.org/Webmention.\n\t\tHere's the script I use to show webmentions: https://github.com/PlaidWeb/webmention.js/\n\t-->\n\t<script src=\"/assets/webmention.min.js\" async></script>\n\n\n<!-- On certain days of the year, the coffee cup emoji that appears after \"James' Coffee Blog\" on my website is replaced. This script controls that functionality.\n\tThe script is documented and available for use at https://github.com/capjamesg/seasonal.js.\n-->\n<script src=\"/assets/js/seasonal.js\"></script>\n      \n        <!-- This script lets me add hovercards to links. When you hover over a link in an\n            article, a card should appear with some meta information about the link.\n            Read more about this script at https://jamesg.blog/2022/10/12/hovercards/ -->\n        <script src=\"https://jamesg.blog/assets/js/hovercard.js\"></script>\n      \n    </main>\n    <img src=\"/assets/penguin.webp\" height=\"100\" style=\"position: absolute; transform: translate(-50%, -50%); top: 50%; left: 50%; z-index: -1;\" id=\"penguin\" aria-hidden=\"true\">\n    \n    \n    <!-- Sometimes, what a website needs is a penguin.\n          The script below makes a penguin appear on the page when you press the `p` key on your keyboard.\n          You can make the penguin waddle around using the WASD keys.\n          If you move the penguin over a link for a few seconds, you will be taken to the page over which\n          the penguin is hovering!\n    \n          To learn how this script works, refer to https://jamesg.blog/2024/03/12/penguin-avatar/.\n\n          This script was made at IndieWebCamp Brighton 2024.\n    -->\n    <script>\n        var penguin = document.getElementById('penguin');\n        var nextAvatarState = \"penguin\";\n\n        var penguinLastMoved = Date.now();\n\n        document.addEventListener('keydown', function(event) {\n            // Assure the penguin does not escape the viewport.\n            // Add offset by %, make it work on scroll using WASD.\n            let rotationDegree = 10; // Degree to tilt for the waddle effect.\n        \n            if (event.key === 'w') {\n                penguin.style.top = parseInt(penguin.style.top) - 1 + '%';\n                // Reset rotation when moving up.\n                penguin.style.transform = 'rotate(0deg)';\n            }\n            if (event.key === 'a') {\n                penguin.style.left = parseInt(penguin.style.left) - 1 + '%';\n                // Rotate left for waddle.\n                penguin.style.transform = `rotate(-${rotationDegree}deg)`;\n            }\n            if (event.key === 's') {\n                penguin.style.top = parseInt(penguin.style.top) + 1 + '%';\n                // Reset rotation when moving down.\n                penguin.style.transform = 'rotate(0deg)';\n            }\n            if (event.key === 'd') {\n                penguin.style.left = parseInt(penguin.style.left) + 1 + '%';\n                // Rotate right for waddle.\n                penguin.style.transform = `rotate(${rotationDegree}deg)`;\n            }\n        \n            penguinLastMoved = Date.now();\n        \n            setTimeout(() => {\n                penguin.style.transform = 'rotate(0deg)';\n            }, 250);\n        });\n\n        document.addEventListener('keydown', function(event) {\n            if (event.key === 'p') {\n                if (nextAvatarState === \"penguin\") {\n                    penguin.src = \"https://jamesg.blog/assets/penguin.webp\";\n                    nextAvatarState = \"puffle\";\n                } else {\n                    penguin.src = \"https://jamesg.blog/assets/puffle.webp\";\n                    nextAvatarState = \"penguin\";\n                }\n                penguin.style.left = '2%';\n                penguin.style.top = '2%';\n                penguin.style.zIndex = 10;\n            }\n        });\n        var links = document.getElementsByTagName('a');\n\n        let intervalId = setInterval(function() {\n            for (var i = 0; i < links.length; i++) {\n                var link = links[i];\n                var linkRect = link.getBoundingClientRect();\n                var penguinRect = penguin.getBoundingClientRect();\n              \n                if (penguinRect.top < linkRect.bottom && penguinRect.bottom > linkRect.top && penguinRect.left < linkRect.right && penguinRect.right > linkRect.left && penguin.style.zIndex == 10) {\n                    const IMG_TAGS = ['svg', 'img', 'picture'];\n                    if (IMG_TAGS.includes(link.tagName.toLowerCase())) {\n                        return;\n                    }\n\n                    if (link.onclick) {\n                        return;\n                    }\n\n                    // if penguin hasn't moved in 500ms\n                    if (Date.now() - penguinLastMoved > 500) {\n                        link.click();\n\n                        clearInterval(intervalId);\n                    }\n                }\n            }\n        }, 100);\n    </script>\n    \n    <!-- incoming-links and outgoing-links are more web components!\n      These web components let me show incoming and outgoing links on my website.\n      To learn how they work, refer to the following page:\n      https://github.com/capjamesg/webmemex.js\n    -->\n    <incoming-links data-api-url=\"https://webmention.io/api/mentions.jf2?target=\"></incoming-links>\n    <outgoing-links data-api-url=\"https://jamesg.blog/lp/outgoing_links\"></outgoing-links>\n    <script src=\"https://capjamesg.github.io/webmemex.js/webmemex.js\"></script>\n    \n    <!-- This script enables code syntax highlighting on my blog post and code snippet pages. -->\n    <script src=\"https://jamesg.blog/assets/js/prism.js\"></script>\n\n    \n\n    <style>\n      .ac {\n        padding: 5px;\n        background-color: #f7f7f7;\n        border-radius: 10px;\n      }\n    </style>\n\n    <!-- This code powers a few hidden features.\n      Adding ?hn to the end of any page with a list of posts will show an orange\n    square next to the titles of all posts that have been featured on Hacker News.\n      Adding ?star will show all posts I have marked as starred. This classification\n      is not in active use, so few posts have a star.\n      Adding ?ac to any post URL will let you see how many posts you have read.\n      This box will appear above the https://jamesg.blog/assets/ai.png image on the page.\n    -->\n    <script>\n      function showElement (id) {\n        if (window.location.search.indexOf(id) > -1) {\n          var elements = document.getElementsByClassName(id);\n          for (var i = 0; i < elements.length; i++) {\n            elements[i].style.display = \"inline-block\";\n          }\n        }\n      }\n      showElement(\"hn\"); // show articles featured on Hacker News\n      showElement(\"ls\"); // show articles featured on Lobste.rs\n      showElement(\"star\"); // show starred articles\n      showElement(\"ts\"); // show most related Taylor Swift lyric to an article\n      \n      \n        // log that you read an article, in local storage\n        var current_article_count = localStorage.getItem(\"article_count\") || 0;\n\n        localStorage.setItem(\"article_count\", parseInt(current_article_count) + 1);\n      \n        if (window.location.search.indexOf(\"ac\") > -1) {\n            var current_article_count = localStorage.getItem(\"article_count\") || 0;\n    \n            var element = document.getElementsByClassName(\"e-content\")[0];\n            var p = document.createElement(\"p\");\n            p.innerHTML = \"You have read \" + current_article_count + \" articles on this site. Thank you!\";\n            p.classList.add(\"ac\");\n            element.appendChild(p);\n        }\n    </script>\n\n<!-- This code powers my background scavenger hunt.\n    There are four backgrounds on this site that you can find.\n    When you find them all, a message appears on the page informing you\n    that you have found all of the backgrounds.\n    The code below doesn't tell you what all the backgrounds are, so you can't cheat :D\n    Learn more about this quiz: https://jamesg.blog/2023/12/18/scavenger-hunt/\n-->\n<script>\n    function checkToAdd(value) {\n        var current_bgs_found = JSON.parse(localStorage.getItem(\"bg_found\")) || [];\n        if (current_bgs_found.indexOf(value) == -1) {\n            current_bgs_found.push(value);\n            localStorage.setItem(\"bg_found\", JSON.stringify(current_bgs_found));\n        }\n    }\n    var backgrounds = [\"box\", \"snow\", \"chair\", \"tool\"];\n\n    \n        checkToAdd(\"tool\");\n    \n    var storedData = localStorage.getItem(\"bg_found\");\n\n    var current_bgs_found = storedData ? JSON.parse(storedData) : [];\n\n    // if all bgs are in local storage, add achievement message\n    if (current_bgs_found.length == backgrounds.length) {\n        // insert \"achievement\" message before nav\n        var nav = document.getElementsByTagName(\"nav\")[0];\n        var message = document.createElement(\"div\");\n        message.innerHTML = \"<p>Achivement unlocked: You found all backgrounds! üéâ</p><p>Thank you for exploring my site. It means a lot!</p>\";\n        message.style = \"text-align: center;\";\n        nav.parentNode.insertBefore(message, nav);\n\n        // clear\n        localStorage.removeItem(\"bg_found\");\n    }\n</script>\n  </body>\n</html>","oembed":false,"readabilityObject":{"title":"Notes on responsible web crawling","content":"<div id=\"readability-page-1\" class=\"page\"><div id=\"main\">\n    \t <!-- This page uses microformats to structure different pieces of information.\n    I use h-entry to state this is a post. Any class name that starts with h-, p-, or -e is a microformat.\n    By specifying microformats, some web tools can better understand this post. For example, IndieNews can use\n    the p-name to figure out the title of the post (without the \"| James' Coffee Blog\" I add to the <title> tag of my website.\n    Learn more about h-entry: https://indieweb.org/h-entry\n-->\n<article>\n\t<header>\n\t\t\n\t\t<p><em>Published on\n\t\t\t<a href=\"/2024/06\"><time datetime=\"2024-06-05T00:00:00\">June 05, 2024</time></a>\n\t\t\t under the <a href=\"/coding\">Coding</a> category.</em></p>\n\t\t  \n\t\t\n\t\t\n\t\t\n\t</header>\n\t<section>\n\t\t<div>\n\t\t\t<p>In my blog post <a href=\"https://jamesg.blog/2024/06/05/indieweb-search-brainstorming/\">brainstorming a new indie web search engine</a>, I noted that running a web search engine is <em>hard</em>. With that in mind, I started to think that I haven't written too much about what I learned about web crawling when running <a href=\"https://github.com/capjamesg/indieweb-search\">IndieWeb Search</a>, a search engine for the indie web. IndieWeb Search crawled a whitelist of websites, searching for pages, and indexed them for use in the search engine.</p>\n<p>One challenge in particular when running search engines is ensuring that your search engine doesn't break someone's website. For example, suppose you are indexing a personal website. If your logic to parse URLs is incorrect, your crawler may spiral out of control and start crawling many pages that don't exist. A potential side-effect of this is that you put undue burden on someone's server, causing their site to be slower.</p>\n<p>This is why I outlined an indie web search engine that works by reading feeds rather than crawling (also called \"spidering\".</p>\n<p>Crawlers go from page to page and look for new links. They retrieve each page and index it. This is repeated until either a whole site has been indexed, or a crawl budget has been met.</p>\n<p>As soon as you get into crawling, there are many technical considerations to implement a responsible web crawler.</p>\n<p>With that said, I wanted to document what I learned in building a web search engine, and some of the ideas I have had since then pertaining to responsible web crawling. Below are <em>some of</em> the things you should do to ensure that your web crawler is responsible. There are likely other considerations that apply at different scales to the one on which I was working (~1,000 websites, ~500,000 pages).</p>\n<h2>URL canonicalisation</h2>\n<p>Ensure you have strong URL canonicalisation logic. This logic should take any URL on a site and normalise it into a standard form. For example, all the following URLs are valid, but equivalent when the domain being crawled is <code>https://jamesg.blog</code>:</p>\n<ul>\n<li><code>https://jamesg.blog</code></li>\n<li><code>https://jamesg.blog/</code></li>\n<li><code>/</code></li>\n</ul>\n<p>If you crawled all three of these pages, you have crawled three pages when you only needed to crawl one. This gets more complicated if a site has URL parameters that may or may not impact the page substantially. I decided to strip all URL parameters if I recall, but a large-scale search engine should respect them and identify whether URL parameters should be crawled and at what rate. That is out of scope for this guide.</p>\n<p>You should have well-tested logic that ensures URLs are canonicalized properly. Technically equivalent URLs should be consolidated. When you discover a new URL, it should be canonicalized to ensure you haven't already crawled it. If you haven't crawled the URL, you can put it in a queue. Of note, this is <em>not</em> related to <code>rel=canonical</code>. <code>rel=canonical</code> is a page stating that a given URL is canonical, but that is a consideration you arrive at when requesting a page.</p>\n<p>With poor URL canonicalization, you may end up crawling subtantially more pages than you need.</p>\n<h2>Redirects</h2>\n<p>Your search engine should have robust measures in place to manage redirects. Limit the number of redirects any URL can give. If you have a white list, don't crawl any URL that does not have a hostname allowed on your list. You should use a pre-existing library to check if hostnames match. Indeed, in general, lean on what others have written to write your crawler.</p>\n<p>For example, suppose a site is misconfigured and a URL that is <code>/example///</code> (with three slashes at the end) redirects to one with two slashes at the end (<code>/example//</code>), and that URL redirects back to the one with three slashes. Your crawler should be stop traversing these redirects and move on.</p>\n<h2>More tips</h2>\n<p>When you crawl a site, you should:</p>\n<ul>\n<li>Respect robots.txt. There are many parsers available that let you check if a URL is covered under a robots.txt policy given the user agent under which your search engine operates. Related: declare a user agent publicly and provide guidance on how people can limit or restrict crawls from your search engine. You are not a responsible search engine if you don't provide a clear means for people to limit crawling without their having to explicitly block your search engine.</li>\n<li>Respect the <code>Retry-After</code> header that states you should retry crawling a page after a certain period.</li>\n<li>Apply a timeout when you crawl URLs.</li>\n<li>Acknowledge 429s and make sure you update your crawl queue to prioritize other URLs that have not returned a 429.</li>\n<li>Look out for high incidence rates of 500s. This may indicate a site is running into stress, or has other technical issues. 500s are not useful in indexing, so you should back off and try again later.</li>\n<li>Crawl multiple sites at once, rather than crawling entire sites sequentially. If a site has 100,000 valid pages, you don't want to allocate all of your crawl capacity to that site all at once. Instead, you should crawl multiple sites at the same time. This will reduce the risk of running into 429s or causing problems.</li>\n<li>Have per-site crawl budgets. This could vary depending on the site. If you are making a small search engine, you may only choose to crawl 1,000 URLs from a site; for a larger search engine, this number may increase substantially.</li>\n<li>Use Last-Modified headers to check if a site has been modified since your last request.</li>\n<li>If a server is slowing down as you crawl, but no 429s are advertised, consider moving URLs from that site further back in your queue.</li>\n</ul>\n<p>These are some of the many considerations you should take into account when building a search engine that spiders between URLs.</p>\n<p>If you don't crawl web sites, and instead only download the content of feeds, the above considerations are less significant. This is why I outlined such an approach in a brainstorm for a new indie web search engin., If you are crawling 1000 feeds, and you only download the feed URLs rather than all the posts in each feed individually, there is substantially less risk of bringing down someone's site than if you are downloading thousands of URLs from their site. The considerations above are still useful if you decide to download any pages linked in a feed, though.</p>\n<p><em>If you have implemented a search crawler and have written advice on this topic, let me know by email. I can add a link to your post here.</em></p>\n\n\t\t\t\n\t\t\t<p><a href=\"https://news.ycombinator.com/submitlink?u=/https://jamesg.blog/2024/06/05/responsible-web-crawling/&amp;t=Notes%20on%20responsible%20web%20crawling\">Share this post on Hacker News</a>.</p>\n\t\t<p><a href=\"https://lobste.rs/stories/new?url=/https://jamesg.blog/2024/06/05/responsible-web-crawling/&amp;title=Notes%20on%20responsible%20web%20crawling\">Share this post on Lobste.rs</a>.</p>\n\t\t</div>\n\t\t\n\t\t\n\n\t\t<!-- My site has a cute mascot! Learn more about him at /mascot/.\n\t\tA friend suggested I come up with a name other than James, which I gave to the mascot in the aforelinked post. I thought about MJ, short for Mascot James.\n\t\tHerein, my mascot will be MJ and James. Or perhaps any of the myriad other nicknames I am likely to come up with over the years.\n\n\t\tI hired someone to design this mascot. The aforelinked post provides more information and a recommendation for the designer!\n\t\t--> \n\t\t<p><img src=\"/assets/mascot.svg\" alt=\"James' Coffee Blog mascot\"></p>\n\t\t<hr>\n\t\n\t\t<!-- Writing is a way for me to process the world around me. It is a craft that I enjoy refining. Please note: My words are written by me, not AI. -->\n\t\t<!-- On a related note, I ask many AI bots not to crawl my website: /robots.txt. This is my space. -->\n\t\t<p><a href=\"https://notbyai.fyi/\"><img src=\"/assets/ai.png\" height=\"42\" width=\"131\" alt=\"Written by human, not by AI\"></a></p>\n\t</section>\n\t<section>\n    <h2>Responses</h2>\n    \n    <h2>Comment on this post</h2>\n    <p>Respond to this post by sending a <a href=\"https://indieweb.org/Webmention\">Webmention</a>.</p>\n    <p>Have a comment? Email me at <a href=\"mailto:readers@jamesg.blog?subject=\">readers@jamesg.blog</a>.</p>  \n</section>\n</article>\n      </div></div>","textContent":"\n    \t \n\n\t\n\t\t\n\t\tPublished on\n\t\t\tJune 05, 2024\n\t\t\t under the Coding category.\n\t\t  \n\t\t\n\t\t\n\t\t\n\t\n\t\n\t\t\n\t\t\tIn my blog post brainstorming a new indie web search engine, I noted that running a web search engine is hard. With that in mind, I started to think that I haven't written too much about what I learned about web crawling when running IndieWeb Search, a search engine for the indie web. IndieWeb Search crawled a whitelist of websites, searching for pages, and indexed them for use in the search engine.\nOne challenge in particular when running search engines is ensuring that your search engine doesn't break someone's website. For example, suppose you are indexing a personal website. If your logic to parse URLs is incorrect, your crawler may spiral out of control and start crawling many pages that don't exist. A potential side-effect of this is that you put undue burden on someone's server, causing their site to be slower.\nThis is why I outlined an indie web search engine that works by reading feeds rather than crawling (also called \"spidering\".\nCrawlers go from page to page and look for new links. They retrieve each page and index it. This is repeated until either a whole site has been indexed, or a crawl budget has been met.\nAs soon as you get into crawling, there are many technical considerations to implement a responsible web crawler.\nWith that said, I wanted to document what I learned in building a web search engine, and some of the ideas I have had since then pertaining to responsible web crawling. Below are some of the things you should do to ensure that your web crawler is responsible. There are likely other considerations that apply at different scales to the one on which I was working (~1,000 websites, ~500,000 pages).\nURL canonicalisation\nEnsure you have strong URL canonicalisation logic. This logic should take any URL on a site and normalise it into a standard form. For example, all the following URLs are valid, but equivalent when the domain being crawled is https://jamesg.blog:\n\nhttps://jamesg.blog\nhttps://jamesg.blog/\n/\n\nIf you crawled all three of these pages, you have crawled three pages when you only needed to crawl one. This gets more complicated if a site has URL parameters that may or may not impact the page substantially. I decided to strip all URL parameters if I recall, but a large-scale search engine should respect them and identify whether URL parameters should be crawled and at what rate. That is out of scope for this guide.\nYou should have well-tested logic that ensures URLs are canonicalized properly. Technically equivalent URLs should be consolidated. When you discover a new URL, it should be canonicalized to ensure you haven't already crawled it. If you haven't crawled the URL, you can put it in a queue. Of note, this is not related to rel=canonical. rel=canonical is a page stating that a given URL is canonical, but that is a consideration you arrive at when requesting a page.\nWith poor URL canonicalization, you may end up crawling subtantially more pages than you need.\nRedirects\nYour search engine should have robust measures in place to manage redirects. Limit the number of redirects any URL can give. If you have a white list, don't crawl any URL that does not have a hostname allowed on your list. You should use a pre-existing library to check if hostnames match. Indeed, in general, lean on what others have written to write your crawler.\nFor example, suppose a site is misconfigured and a URL that is /example/// (with three slashes at the end) redirects to one with two slashes at the end (/example//), and that URL redirects back to the one with three slashes. Your crawler should be stop traversing these redirects and move on.\nMore tips\nWhen you crawl a site, you should:\n\nRespect robots.txt. There are many parsers available that let you check if a URL is covered under a robots.txt policy given the user agent under which your search engine operates. Related: declare a user agent publicly and provide guidance on how people can limit or restrict crawls from your search engine. You are not a responsible search engine if you don't provide a clear means for people to limit crawling without their having to explicitly block your search engine.\nRespect the Retry-After header that states you should retry crawling a page after a certain period.\nApply a timeout when you crawl URLs.\nAcknowledge 429s and make sure you update your crawl queue to prioritize other URLs that have not returned a 429.\nLook out for high incidence rates of 500s. This may indicate a site is running into stress, or has other technical issues. 500s are not useful in indexing, so you should back off and try again later.\nCrawl multiple sites at once, rather than crawling entire sites sequentially. If a site has 100,000 valid pages, you don't want to allocate all of your crawl capacity to that site all at once. Instead, you should crawl multiple sites at the same time. This will reduce the risk of running into 429s or causing problems.\nHave per-site crawl budgets. This could vary depending on the site. If you are making a small search engine, you may only choose to crawl 1,000 URLs from a site; for a larger search engine, this number may increase substantially.\nUse Last-Modified headers to check if a site has been modified since your last request.\nIf a server is slowing down as you crawl, but no 429s are advertised, consider moving URLs from that site further back in your queue.\n\nThese are some of the many considerations you should take into account when building a search engine that spiders between URLs.\nIf you don't crawl web sites, and instead only download the content of feeds, the above considerations are less significant. This is why I outlined such an approach in a brainstorm for a new indie web search engin., If you are crawling 1000 feeds, and you only download the feed URLs rather than all the posts in each feed individually, there is substantially less risk of bringing down someone's site than if you are downloading thousands of URLs from their site. The considerations above are still useful if you decide to download any pages linked in a feed, though.\nIf you have implemented a search crawler and have written advice on this topic, let me know by email. I can add a link to your post here.\n\n\t\t\t\n\t\t\tShare this post on Hacker News.\n\t\tShare this post on Lobste.rs.\n\t\t\n\t\t\n\t\t\n\n\t\t \n\t\t\n\t\t\n\t\n\t\t\n\t\t\n\t\t\n\t\n\t\n    Responses\n    \n    Comment on this post\n    Respond to this post by sending a Webmention.\n    Have a comment? Email me at readers@jamesg.blog.  \n\n\n      ","length":6561,"excerpt":"In my blog post brainstorming a new indie web search engine, I noted that running a web search engine is hard. With that in mind, I started to think that I haven't written too much about what I learned about web crawling when running IndieWeb Search, a search engine for the indie web. IndieWeb Search crawled a whitelist of websites, searching for pages, and indexed them for use in the search engine.","byline":"James' Coffee Blog ‚òï","dir":null,"siteName":null,"lang":"en"},"finalizedMeta":{"title":"Notes on responsible web crawling | James' Coffee Blog","description":"In my blog post brainstorming a new indie web search engine, I noted that running a web search engine is hard. With that in mind, I started to think that I haven't written too much about what I learned about web crawling when running IndieWeb Search, a search engine for the indie web. IndieWeb Search crawled a whitelist of websites, searching for pages, and indexed them for use in the search engine.","author":false,"creator":"","publisher":false,"date":"2024-06-05T18:52:54.314Z","topics":[]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"Notes on responsible web crawling | James' Coffee Blog","description":"In my blog post brainstorming a new indie web search engine, I noted that running a web search engine is hard. With that in mind, I started to think that I haven't written too much about what I learned about web crawling when running IndieWeb Search, a search engine for the indie web. IndieWeb Search crawled a whitelist of websites, searching for pages, and indexed them for use in the search engine.","canonical":"https://jamesg.blog/2024/06/05/responsible-web-crawling/","keywords":[],"image":"/assets/mascot.svg","firstParagraph":"James' Coffee Blog ‚òï"},"dublinCore":{},"opengraph":{"title":false,"description":false,"url":false,"site_name":false,"locale":false,"type":false,"typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":"https://screenshots.jamesg.blog/?url=https://jamesg.blog/2024/06/05/responsible-web-crawling/"},"twitter":{"site":false,"description":false,"card":false,"creator":false,"title":false,"image":false},"archivedData":{"link":false,"wayback":false}}}