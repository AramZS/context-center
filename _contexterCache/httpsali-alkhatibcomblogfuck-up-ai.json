{"initialLink":"https://ali-alkhatib.com/blog/fuck-up-ai","sanitizedLink":"https://ali-alkhatib.com/blog/fuck-up-ai","finalLink":"https://ali-alkhatib.com/blog/fuck-up-ai","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://ali-alkhatib.com/blog/fuck-up-ai\" itemprop=\"url\"> Ali Alkhatib: Destroy AI </a></contexter-box-head></contexter-box-head><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2025-07-27T23:12:35.369Z\">7/27/2025</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>I’ve been struggling to articulate this idea, and maybe the problem is that it’s actually kind of simple once you put it out there, and there’s really no good reason to unpack a whole case for it once you put the thought on paper.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a href=\"https://web.archive.org/web/20250603161015/https://ali-alkhatib.com/blog/fuck-up-ai\" is=\"contexter-link\" target=\"_blank\" rel=\"timemap\" class=\"read-link archive-link\" itemprop=\"archivedAt\" slot=\"archive-link\">Archived</a><a is=\"contexter-link\" href=\"https://ali-alkhatib.com/blog/fuck-up-ai\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"d671e2c9391a6d38a5ae3b0b2ec0c58d202fa04b","data":{"originalLink":"https://ali-alkhatib.com/blog/fuck-up-ai","sanitizedLink":"https://ali-alkhatib.com/blog/fuck-up-ai","canonical":"https://ali-alkhatib.com/blog/fuck-up-ai","htmlText":"<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n        <meta name=\"cf-2fa-verify\" content=\"a763b4d502d7860\">\n        <title> Ali Alkhatib: Destroy AI </title>\n        <meta charset=\"utf-8\">\n        <!-- <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n        <meta property=\"og:image\" content=\"/content/ali_profile_3k_sq_compressed.jpg\" />\n        \n        <link rel=\"icon\" type=\"image/png\" href=\"https://ali-alkhatib.com/favicon.png\"> -->\n        <link rel=\"icon\" type=\"image/png\" href=\"https://ali-alkhatib.com/favicon.png\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n\n        <link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css\">\n        <link rel=\"stylesheet\" href=\"//cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css\" integrity=\"sha512-MV7K8+y+gLIBoVD59lQIYicR65iaqukzvf/nwasF0nqhPay5w/9lJmVM2hMDcnK1OnMGCdVK+iQrJ7lzPJQd1w==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\" />\n        <link rel=\"stylesheet\" type=\"text/css\" href=\"/media/main.css\">\n\n        <!-- <meta name=\"twitter:image:src\" content=\"/content/ali_profile_3k_sq_compressed.jpg\">\n        \n        \n        <meta name=\"twitter:card\" content=\"summary\">\n        <meta name=\"twitter:site\" content=\"_alialkhatib\">\n        <meta name=\"twitter:creator\" content=\"_alialkhatib\">\n        <meta name=\"twitter:title\" content=\"Destroy AI\">\n        <meta name=\"twitter:description\" content=\"I’ve been struggling to articulate this idea, and maybe the problem is that it’s actually kind of simple once you put it out there, and there’s really no good reason to unpack a whole case for it once you put the thought on paper.\">\n        <meta name=\"og:type\" content=\"article\">\n        <meta name=\"og:title\" content=\"Destroy AI\">\n        <meta name=\"og:description\" content=\"I’ve been struggling to articulate this idea, and maybe the problem is that it’s actually kind of simple once you put it out there, and there’s really no good reason to unpack a whole case for it once you put the thought on paper.\">\n         -->\n    </head>\n<body>\n\n\n\n\n\n\n<div class=\"container\">\n\n\n\n      <div class=\"row\">\n        <header>\n          <div class=\"breadcrumb\">\n          <a href=\"/\" id=\"header-link\">Ali Alkhatib</a>\n          </div>\n        </header>\n      </div>\n\n\n<div class=\"row d-md-none\">\n      <div class=\"col-12\">\n        \n\n\n        <nav class=\"navbar text-nowrap flex-nowrap\">\n          \n          <a \n          \n          class=\"link-primary\"\n          \n              href=\"/research\"\n              >research</a>&nbsp;\n          <!-- <span class=\"d-sm-none d-lg-block\"></span> -->\n          \n          <a \n          \n          aria-current=\"page\" class=\"link-dark active\"\n          \n              href=\"/blog\"\n              >blog</a>&nbsp;\n          <!-- <span class=\"d-sm-none d-lg-block\"></span> -->\n          \n          <a \n          \n          class=\"link-primary\"\n          \n              href=\"/contact\"\n              >contact</a>&nbsp;\n          <!-- <span class=\"d-sm-none d-lg-block\"></span> -->\n          \n          <a \n          \n          class=\"link-primary\"\n          \n              href=\"/content/ali_alkhatib_cv.pdf\"\n               target=\"_blank\">CV</a>&nbsp;\n          <!-- <span class=\"d-sm-none d-lg-block\"></span> -->\n          \n        </nav>\n      </div>\n      <hr>\n</div>\n\n      <div class=\"row\">\n        <div class=\"col-sm-12 col-md-10\">\n        <!-- <h2>Blog</h2> -->\n<!-- <div class=\"with-tagline\"> -->\n        <!-- <a class=\"h2 fw-bold link-primary\" href=\"\"></a> -->\n    <!-- </div> -->\n    <!-- <span class=\"tagline fst-italic\"></span> -->\n\n<div>\n<span class=\"h2 fw-bold\">Destroy AI</span>\n</div>\n<span class=\"tagline fst-italic\">24 June 2024</span>\n\n\n<article class=\"full\">\n<p>I’ve been struggling to articulate this idea, and maybe the problem is that it’s actually kind of simple once you put it out there, and there’s really no good reason to unpack a whole case for it once you put the thought on paper.</p>\n\n<p>I’m gravitating away from the discourse of measuring and fixing unfair algorithmic systems, or making them more transparent, or accountable. Instead, I’m finding myself fixated on articulating the moral case for sabotaging, circumventing, and destroying “AI”, machine learning systems, and their surrounding political projects as valid responses to harm.</p>\n\n<p>In other words, I want us to internalize and develop a more rigorous appreciation of those who fuck up AI and its supporting systems.</p>\n\n<p>With hegemonic algorithmic systems (namely large language models and similar machine learning systems), and the overwhelming power of capital pushing these technologies on us, I’ve come to feel like human-centered design (HCD) and the overarching project of HCI has reached a state of abject failure. Maybe it’s been there for a while, but I think the field’s inability to rise forcefully to the ascent of large language models and the pervasive use of chatbots as panaceas to every conceivable problem is uncharitably illustrative of its current state.</p>\n\n<p>CHI and FAccT have failed to meaningfully respond to the deskilling of creative labor; or to the environmental or humanitarian crises these systems cause or exacerbate around the world; or even to the co-opting of our spaces by grifters and con artists making up “probabilities of doom”. Indeed, in some ways, these spaces have avoided the difficult conversations and welcomed the nonsense in to try to avoid the anguish of facing a genocide in which we are collectively implicated, or the disillusionment of confronting our own roles as agents of the corporate states that invade, surveil, displace, and kill people.</p>\n\n<p>I’m no longer interested in encouraging the design of more human-centered versions of these murderous technologies, or to inform the more humane administration of complex algorithmic systems that separate families, bomb schools and neighborhoods, that force people out of their homes or onto the streets, or that deny medical care at the moment people need it most. These systems exist to facilitate violence, and HCI researchers who have committed their careers to curl back that violence at the margins have considerably more of something in them than I have. I hope it’s patience and determination, and not self-interested greed.</p>\n\n<p>Regardless, there’s no way to make the administrative and bureaucratic systems of apartheid and violence more humane for the people subjugated by that system.</p>\n\n<p>I think we must forcefully put on the table the possibility that we will destroy systems that fail to make a compelling affirmative case for their existence. That threat must be credible. We should actively undermine and sabotage systems, and recognize that labor as a moral project that we engage in, the way luddites sabotaged machinery that tore people apart.</p>\n\n<p>This isn’t really a post or a conversation for the HCD or HCI community. I’ve grown weary (and wary, I suppose) of the design community because they ultimately seem committed to … <em>designing</em> systems - an ideological project antithetical to this one.</p>\n\n<p>If you think of yourself as a member of that community most people call “design”, I would ask you to pose a few challenging questions to yourself. Start with these:</p>\n\n<p>Do you work with systems, or people? Which would you follow, if the two paths diverge? What if if they’re in conflict and you can only follow or defend one? If you see a system dismantling a human being’s life, do you think that the system must be <em>fixed</em>, or that the system must be <em>destroyed</em>?</p>\n\n<hr />\n\n<p>I wanted to end with a few positive notes: I like projects like <a href=\"https://glaze.cs.uchicago.edu/\">Glaze at the University of Chicago</a>, and while I’ve been trying to write a piece about this I came across <a href=\"https://algorithmic-sabotage.github.io/asrg/manifesto-on-algorithmic_sabotage/\">a manifesto on mastodon</a> that I thought was very cool and uncannily relevant. I’ve also seen some really rad indigenous <a href=\"https://hyperallergic.com/878913/artist-chavis-marmol-crushes-tesla-with-colossal-olmec-head-sculpture/\">art</a>, and I know that there are other works that have explored the general space of destroying, sabotaging, and <em><a href=\"https://dl.acm.org/doi/10.1145/3442188.3445885\">poisoning</a></em> datasets and whatnot.</p>\n\n<p>I’d be surprised if nobody has ever thought to put this constellation of ideas together in the same blog post - please hit me up if you’ve seen additional thoughts like this. I’d love to hear from other people who are thinking about stuff like this.</p>\n\n<p>If you’ve been thinking along these lines, or if you’re one of the people I’ve linked to, then please take this as encouragement and an invitation to be in dialog with the other stuff I’ve written about and pointed to, because I think resistance is necessary, and mustn’t be captured by the design-brained people.</p>\n\n<p><em>edit notes</em>: Someone kindly pointed out that the “Manifesto on Algorithmic Sabotage” link was broken, and found the correct page for me. I updated it accordingly.</p>\n\n\n</article>\n\n<hr class=\"end-of-page\">\n\n<footer>If you have something to say about this, \n<!--<a title=\"email address\" href=\"mailto:hi@al2.in\">email me</a> or \n<a title=\"Twitter profile\" href=\"https://twitter.com/_alialkhatib\">tweet</a> at me.-->\n<a href=\"/contact\" title=\"contact page\">contact me</a>\n</footer>\n\n      </div>\n\n\n        <div class=\"col-md-2 col-sm-12 d-none d-md-block\">\n          <nav class=\"\">\n            \n            <a\n            \n            class=\"link-primary\"\n            \n                href=\"/research\"\n                >\n                research\n            </a>\n            <span class=\"d-sm-none d-lg-block\"></span>\n            \n            <a\n            \n            aria-current=\"page\" class=\"link-dark active\"\n            \n                href=\"/blog\"\n                >\n                blog\n            </a>\n            <span class=\"d-sm-none d-lg-block\"></span>\n            \n            <a\n            \n            class=\"link-primary\"\n            \n                href=\"/contact\"\n                >\n                contact\n            </a>\n            <span class=\"d-sm-none d-lg-block\"></span>\n            \n            <a\n            \n            class=\"link-primary\"\n            \n                href=\"/content/ali_alkhatib_cv.pdf\"\n                 target=\"_blank\">\n                CV\n            </a>\n            <span class=\"d-sm-none d-lg-block\"></span>\n            \n          </nav>\n        </div>\n\n        <div class=\"col-md-2 col-sm-12 d-md-none\">\n          <nav class=\"navbar text-nowrap flex-nowrap\">\n            \n            <a\n            \n          class=\"link-primary\"\n          \n                href=\"/research\"\n              >research</a>&nbsp;\n            </a>\n            \n            <a\n            \n          aria-current=\"page\" class=\"link-dark active\"\n          \n                href=\"/blog\"\n              >blog</a>&nbsp;\n            </a>\n            \n            <a\n            \n          class=\"link-primary\"\n          \n                href=\"/contact\"\n              >contact</a>&nbsp;\n            </a>\n            \n            <a\n            \n          class=\"link-primary\"\n          \n                href=\"/content/ali_alkhatib_cv.pdf\"\n               target=\"_blank\">CV</a>&nbsp;\n            </a>\n            \n          </nav>\n        </div>\n      \n      </div>\n</div>\n\n      \n\n</body>\n\n\n  \n  <script src=\"//cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js\" integrity=\"sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz\" crossorigin=\"anonymous\"></script>\n\n<script src=\"//cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js\"></script>\n<script src=\"//cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js\"></script>\n\n</html>\n","oembed":false,"readabilityObject":{"title":"Ali Alkhatib: Destroy AI","content":"<div id=\"readability-page-1\" class=\"page\"><article>\n<p>I’ve been struggling to articulate this idea, and maybe the problem is that it’s actually kind of simple once you put it out there, and there’s really no good reason to unpack a whole case for it once you put the thought on paper.</p>\n\n<p>I’m gravitating away from the discourse of measuring and fixing unfair algorithmic systems, or making them more transparent, or accountable. Instead, I’m finding myself fixated on articulating the moral case for sabotaging, circumventing, and destroying “AI”, machine learning systems, and their surrounding political projects as valid responses to harm.</p>\n\n<p>In other words, I want us to internalize and develop a more rigorous appreciation of those who fuck up AI and its supporting systems.</p>\n\n<p>With hegemonic algorithmic systems (namely large language models and similar machine learning systems), and the overwhelming power of capital pushing these technologies on us, I’ve come to feel like human-centered design (HCD) and the overarching project of HCI has reached a state of abject failure. Maybe it’s been there for a while, but I think the field’s inability to rise forcefully to the ascent of large language models and the pervasive use of chatbots as panaceas to every conceivable problem is uncharitably illustrative of its current state.</p>\n\n<p>CHI and FAccT have failed to meaningfully respond to the deskilling of creative labor; or to the environmental or humanitarian crises these systems cause or exacerbate around the world; or even to the co-opting of our spaces by grifters and con artists making up “probabilities of doom”. Indeed, in some ways, these spaces have avoided the difficult conversations and welcomed the nonsense in to try to avoid the anguish of facing a genocide in which we are collectively implicated, or the disillusionment of confronting our own roles as agents of the corporate states that invade, surveil, displace, and kill people.</p>\n\n<p>I’m no longer interested in encouraging the design of more human-centered versions of these murderous technologies, or to inform the more humane administration of complex algorithmic systems that separate families, bomb schools and neighborhoods, that force people out of their homes or onto the streets, or that deny medical care at the moment people need it most. These systems exist to facilitate violence, and HCI researchers who have committed their careers to curl back that violence at the margins have considerably more of something in them than I have. I hope it’s patience and determination, and not self-interested greed.</p>\n\n<p>Regardless, there’s no way to make the administrative and bureaucratic systems of apartheid and violence more humane for the people subjugated by that system.</p>\n\n<p>I think we must forcefully put on the table the possibility that we will destroy systems that fail to make a compelling affirmative case for their existence. That threat must be credible. We should actively undermine and sabotage systems, and recognize that labor as a moral project that we engage in, the way luddites sabotaged machinery that tore people apart.</p>\n\n<p>This isn’t really a post or a conversation for the HCD or HCI community. I’ve grown weary (and wary, I suppose) of the design community because they ultimately seem committed to … <em>designing</em> systems - an ideological project antithetical to this one.</p>\n\n<p>If you think of yourself as a member of that community most people call “design”, I would ask you to pose a few challenging questions to yourself. Start with these:</p>\n\n<p>Do you work with systems, or people? Which would you follow, if the two paths diverge? What if if they’re in conflict and you can only follow or defend one? If you see a system dismantling a human being’s life, do you think that the system must be <em>fixed</em>, or that the system must be <em>destroyed</em>?</p>\n\n<hr>\n\n<p>I wanted to end with a few positive notes: I like projects like <a href=\"https://glaze.cs.uchicago.edu/\">Glaze at the University of Chicago</a>, and while I’ve been trying to write a piece about this I came across <a href=\"https://algorithmic-sabotage.github.io/asrg/manifesto-on-algorithmic_sabotage/\">a manifesto on mastodon</a> that I thought was very cool and uncannily relevant. I’ve also seen some really rad indigenous <a href=\"https://hyperallergic.com/878913/artist-chavis-marmol-crushes-tesla-with-colossal-olmec-head-sculpture/\">art</a>, and I know that there are other works that have explored the general space of destroying, sabotaging, and <em><a href=\"https://dl.acm.org/doi/10.1145/3442188.3445885\">poisoning</a></em> datasets and whatnot.</p>\n\n<p>I’d be surprised if nobody has ever thought to put this constellation of ideas together in the same blog post - please hit me up if you’ve seen additional thoughts like this. I’d love to hear from other people who are thinking about stuff like this.</p>\n\n<p>If you’ve been thinking along these lines, or if you’re one of the people I’ve linked to, then please take this as encouragement and an invitation to be in dialog with the other stuff I’ve written about and pointed to, because I think resistance is necessary, and mustn’t be captured by the design-brained people.</p>\n\n<p><em>edit notes</em>: Someone kindly pointed out that the “Manifesto on Algorithmic Sabotage” link was broken, and found the correct page for me. I updated it accordingly.</p>\n\n\n</article></div>","textContent":"\nI’ve been struggling to articulate this idea, and maybe the problem is that it’s actually kind of simple once you put it out there, and there’s really no good reason to unpack a whole case for it once you put the thought on paper.\n\nI’m gravitating away from the discourse of measuring and fixing unfair algorithmic systems, or making them more transparent, or accountable. Instead, I’m finding myself fixated on articulating the moral case for sabotaging, circumventing, and destroying “AI”, machine learning systems, and their surrounding political projects as valid responses to harm.\n\nIn other words, I want us to internalize and develop a more rigorous appreciation of those who fuck up AI and its supporting systems.\n\nWith hegemonic algorithmic systems (namely large language models and similar machine learning systems), and the overwhelming power of capital pushing these technologies on us, I’ve come to feel like human-centered design (HCD) and the overarching project of HCI has reached a state of abject failure. Maybe it’s been there for a while, but I think the field’s inability to rise forcefully to the ascent of large language models and the pervasive use of chatbots as panaceas to every conceivable problem is uncharitably illustrative of its current state.\n\nCHI and FAccT have failed to meaningfully respond to the deskilling of creative labor; or to the environmental or humanitarian crises these systems cause or exacerbate around the world; or even to the co-opting of our spaces by grifters and con artists making up “probabilities of doom”. Indeed, in some ways, these spaces have avoided the difficult conversations and welcomed the nonsense in to try to avoid the anguish of facing a genocide in which we are collectively implicated, or the disillusionment of confronting our own roles as agents of the corporate states that invade, surveil, displace, and kill people.\n\nI’m no longer interested in encouraging the design of more human-centered versions of these murderous technologies, or to inform the more humane administration of complex algorithmic systems that separate families, bomb schools and neighborhoods, that force people out of their homes or onto the streets, or that deny medical care at the moment people need it most. These systems exist to facilitate violence, and HCI researchers who have committed their careers to curl back that violence at the margins have considerably more of something in them than I have. I hope it’s patience and determination, and not self-interested greed.\n\nRegardless, there’s no way to make the administrative and bureaucratic systems of apartheid and violence more humane for the people subjugated by that system.\n\nI think we must forcefully put on the table the possibility that we will destroy systems that fail to make a compelling affirmative case for their existence. That threat must be credible. We should actively undermine and sabotage systems, and recognize that labor as a moral project that we engage in, the way luddites sabotaged machinery that tore people apart.\n\nThis isn’t really a post or a conversation for the HCD or HCI community. I’ve grown weary (and wary, I suppose) of the design community because they ultimately seem committed to … designing systems - an ideological project antithetical to this one.\n\nIf you think of yourself as a member of that community most people call “design”, I would ask you to pose a few challenging questions to yourself. Start with these:\n\nDo you work with systems, or people? Which would you follow, if the two paths diverge? What if if they’re in conflict and you can only follow or defend one? If you see a system dismantling a human being’s life, do you think that the system must be fixed, or that the system must be destroyed?\n\n\n\nI wanted to end with a few positive notes: I like projects like Glaze at the University of Chicago, and while I’ve been trying to write a piece about this I came across a manifesto on mastodon that I thought was very cool and uncannily relevant. I’ve also seen some really rad indigenous art, and I know that there are other works that have explored the general space of destroying, sabotaging, and poisoning datasets and whatnot.\n\nI’d be surprised if nobody has ever thought to put this constellation of ideas together in the same blog post - please hit me up if you’ve seen additional thoughts like this. I’d love to hear from other people who are thinking about stuff like this.\n\nIf you’ve been thinking along these lines, or if you’re one of the people I’ve linked to, then please take this as encouragement and an invitation to be in dialog with the other stuff I’ve written about and pointed to, because I think resistance is necessary, and mustn’t be captured by the design-brained people.\n\nedit notes: Someone kindly pointed out that the “Manifesto on Algorithmic Sabotage” link was broken, and found the correct page for me. I updated it accordingly.\n\n\n","length":4930,"excerpt":"I’ve been struggling to articulate this idea, and maybe the problem is that it’s actually kind of simple once you put it out there, and there’s really no good reason to unpack a whole case for it once you put the thought on paper.","byline":null,"dir":null,"siteName":null,"lang":"en"},"finalizedMeta":{"title":" Ali Alkhatib: Destroy AI ","description":"I’ve been struggling to articulate this idea, and maybe the problem is that it’s actually kind of simple once you put it out there, and there’s really no good reason to unpack a whole case for it once you put the thought on paper.","author":false,"creator":"","publisher":false,"date":"2025-07-27T23:12:35.369Z","topics":[]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":false,"title":" Ali Alkhatib: Destroy AI ","description":false,"canonical":"https://ali-alkhatib.com/blog/fuck-up-ai","keywords":[],"image":false,"firstParagraph":"I’ve been struggling to articulate this idea, and maybe the problem is that it’s actually kind of simple once you put it out there, and there’s really no good reason to unpack a whole case for it once you put the thought on paper."},"dublinCore":{},"opengraph":{"title":false,"description":false,"url":false,"site_name":false,"locale":false,"type":false,"typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":false},"twitter":{"site":false,"description":false,"card":false,"creator":false,"title":false,"image":false},"archivedData":{"link":"https://web.archive.org/web/20250603161015/https://ali-alkhatib.com/blog/fuck-up-ai","wayback":"https://web.archive.org/web/20250603161015/https://ali-alkhatib.com/blog/fuck-up-ai"}}}