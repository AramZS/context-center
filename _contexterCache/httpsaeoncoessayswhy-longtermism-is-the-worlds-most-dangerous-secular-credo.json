{"initialLink":"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo","sanitizedLink":"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo","finalLink":"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo\" itemprop=\"url\">Why longtermism is the world’s most dangerous secular credo | Aeon Essays</a></contexter-box-head></contexter-box-head><contexter-byline class=\"p-author author\" slot=\"author\"><span class=\"p-name byline\" rel=\"author\" itemprop=\"author\">aeonmag</span></contexter-byline><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2024-08-29T04:36:06.666Z\">8/29/2024</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>It started as a fringe philosophical theory about humanity’s future. It’s now richly funded and increasingly dangerous</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a is=\"contexter-link\" href=\"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"2689a43486cc7c7fdba6ccfcee08a5988a410ccf","data":{"originalLink":"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo","sanitizedLink":"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo","canonical":"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo","htmlText":"<!DOCTYPE html><html lang=\"en\"><head><meta charSet=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width\"/><title>Why longtermism is the world’s most dangerous secular credo | Aeon Essays</title><meta name=\"robots\" content=\"index,follow,max-image-preview:large\"/><meta name=\"description\" content=\"It started as a fringe philosophical theory about humanity’s future. It’s now richly funded and increasingly dangerous\"/><meta property=\"og:title\" content=\"Why longtermism is the world’s most dangerous secular credo | Aeon Essays\"/><meta property=\"og:description\" content=\"It started as a fringe philosophical theory about humanity’s future. It’s now richly funded and increasingly dangerous\"/><meta property=\"og:url\" content=\"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo\"/><meta property=\"og:type\" content=\"article\"/><meta property=\"og:image\" content=\"https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=1200&amp;quality=75&amp;format=auto\"/><meta property=\"og:image:alt\" content=\"&lt;p&gt;Scarecrows keep away migratory birds from the dangers of the tailing ponds created by the exploitation on the tar sands at Fort McMurray, Alberta, Canada. &lt;em&gt;Photo by Larry Towell/Magnum&lt;/em&gt;&lt;/p&gt;\"/><meta property=\"og:image:width\" content=\"2000\"/><meta property=\"og:image:height\" content=\"1252\"/><link rel=\"canonical\" href=\"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo\"/><meta name=\"theme-color\" content=\"#0c776d\"/><meta name=\"twitter:label1\" content=\"Reading time\"/><meta name=\"twitter:data1\" content=\"31 min read\"/><link rel=\"alternate\" type=\"application/rss+xml\" title=\"Aeon | a world of ideas\" href=\"/feed.rss\"/><link rel=\"alternate\" type=\"application/atom+xml\" title=\"Aeon | a world of ideas\" href=\"/feed.atom\"/><script type=\"application/ld+json\">{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Philosophy\",\"item\":\"https://aeon.co/philosophy\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Thinkers and theories\",\"item\":\"https://aeon.co/philosophy/thinkers-and-theories\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"Against longtermism\",\"item\":\"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo\"}]}</script><script type=\"application/ld+json\">{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Essays\",\"item\":\"https://aeon.co/essays\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Against longtermism\",\"item\":\"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo\"}]}</script><script type=\"application/ld+json\">{\"@context\":\"https://schema.org\",\"@type\":\"Article\",\"datePublished\":\"2021-10-19\",\"description\":\"It started as a fringe philosophical theory about humanity’s future. It’s now richly funded and increasingly dangerous\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo\"},\"headline\":\"Why longtermism is the world’s most dangerous secular credo | Aeon Essays\",\"image\":[\"https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=1200&amp;quality=75&amp;format=auto\"],\"dateModified\":\"2021-10-19\",\"author\":[{\"@type\":\"Person\",\"name\":\"Émile P Torres\",\"url\":\"https://aeon.co/users/emile-p-torres\"}],\"publisher\":{\"@type\":\"Organization\",\"name\":\"Aeon Magazine\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https://aeon.co/logo.png\"}},\"isAccessibleForFree\":true,\"articleBody\":\"There seems to be a growing recognition that humanity might be approaching the ‘end times’. Dire predictions of catastrophe clutter the news. Social media videos of hellish wildfires, devastating floods and hospitals overflowing with COVID-19 patients dominate our timelines. Extinction Rebellion activists are shutting down cities in a desperate attempt to save the world. One survey even found that more than half of the people asked about humanity’s future ‘rated the risk of our way of life ending within the next 100 years at 50 per cent or greater.’ ‘Apocalypticism’, or the belief that the end times are imminent, is of course nothing new: people have warned that the end is nigh for millennia, and in fact many New Testament scholars believe that Jesus himself expected the world to end during his own lifetime. But the situation today is fundamentally different than in the past. The ‘eschatological’ scenarios now being discussed are based not on the revelations of religious prophets, or secular metanarratives of human history (as in the case of Marxism), but on robust scientific conclusions defended by leading experts in fields such as climatology, ecology, epidemiology and so on. We know, for example, that climate change poses a dire threat to civilisation. We know that biodiversity loss and the sixth mass extinction could precipitate sudden, irreversible, catastrophic shifts in the global ecosystem. A thermonuclear exchange could blot out the Sun for years or decades, bringing about the collapse of global agriculture. And whether or not SARS-CoV-2 came from a Wuhan laboratory or was cooked up in the kitchen of nature (the latter seems more probable right now), synthetic biology will soon enable bad actors to design pathogens far more lethal and contagious than anything Darwinian evolution could possibly invent. Some philosophers and scientists have also begun sounding the alarm about ‘emerging threats’ associated with machine superintelligence, molecular nanotechnology and stratospheric geoengineering, which look no less formidable. Such considerations have led many scholars to acknowledge that, as Stephen Hawking wrote in The Guardian in 2016, ‘we are at the most dangerous moment in the development of humanity.’ Lord Martin Rees, for example, estimates that civilisation has a 50/50 chance of making it to 2100. Noam Chomsky argues that the risk of annihilation is currently ‘unprecedented in the history of Homo sapiens’. And Max Tegmark contends that ‘it’s probably going to be within our lifetimes … that we’re either going to self-destruct or get our act together.’ Consistent with these dismal declarations, the Bulletin of the Atomic Scientists in 2020 set its iconic Doomsday Clock to a mere 100 seconds before midnight (or doom), the closest it’s been since the clock was created in 1947, and more than 11,000 scientists from around the world signed an article in 2020 stating ‘clearly and unequivocally that planet Earth is facing a climate emergency’, and without ‘an immense increase of scale in endeavours to conserve our biosphere [we risk] untold suffering due to the climate crisis.’ As the young climate activist Xiye Bastida summed up this existential mood in a Teen Vogue interview in 2019, the aim is to ‘make sure that we’re not the last generation’, because this now appears to be a very real possibility. Given the unprecedented dangers facing humanity today, one might expect philosophers to have spilled a considerable amount of ink on the ethical implications of our extinction, or related scenarios such as the permanent collapse of civilisation. How morally bad (or good) would our disappearance be, and for what reasons? Would it be wrong to prevent future generations from coming into existence? Does the value of past sacrifices, struggles and strivings depend on humanity continuing to exist for as long as Earth, or the Universe more generally, remains habitable? Yet this is not the case: the topic of our extinction has received little sustained attention from philosophers until recently, and even now remains at the fringe of philosophical discussion and debate. On the whole, they have been preoccupied with other matters. However, there is one notable exception to this rule: over the past two decades, a small group of theorists mostly based in Oxford have been busy working out the details of a new moral worldview called longtermism, which emphasizes how our actions affect the very long-term future of the universe – thousands, millions, billions, and even trillions of years from now. This has roots in the work of Nick Bostrom, who founded the grandiosely named Future of Humanity Institute (FHI) in 2005, and Nick Beckstead, a research associate at FHI and a programme officer at Open Philanthropy. It has been defended most publicly by the FHI philosopher Toby Ord, author of The Precipice: Existential Risk and the Future of Humanity (2020). Longtermism is the primary research focus of both the Global Priorities Institute (GPI), an FHI-linked organisation directed by Hilary Greaves, and the Forethought Foundation, run by William MacAskill, who also holds positions at FHI and GPI. Adding to the tangle of titles, names, institutes and acronyms, longtermism is one of the main ‘cause areas’ of the so-called effective altruism (EA) movement, which was introduced by Ord in around 2011 and now boasts of having a mind-boggling $46 billion in committed funding. It is difficult to overstate how influential longtermism has become. Karl Marx in 1845 declared that the point of philosophy isn’t merely to interpret the world but change it, and this is exactly what longtermists have been doing, with extraordinary success. Consider that Elon Musk, who has cited and endorsed Bostrom’s work, has donated $1.5 million dollars to FHI through its sister organisation, the even more grandiosely named Future of Life Institute (FLI). This was cofounded by the multimillionaire tech entrepreneur Jaan Tallinn, who, as I recently noted, doesn’t believe that climate change poses an ‘existential risk’ to humanity because of his adherence to the longtermist ideology. Meanwhile, the billionaire libertarian and Donald Trump supporter Peter Thiel, who once gave the keynote address at an EA conference, has donated large sums of money to the Machine Intelligence Research Institute, whose mission to save humanity from superintelligent machines is deeply intertwined with longtermist values. Other organisations such as GPI and the Forethought Foundation are funding essay contests and scholarships in an effort to draw young people into the community, while it’s an open secret that the Washington, DC-based Center for Security and Emerging Technologies (CSET) aims to place longtermists within high-level US government positions to shape national policy. In fact, CSET was established by Jason Matheny, a former research assistant at FHI who’s now the deputy assistant to US President Joe Biden for technology and national security. Ord himself has, astonishingly for a philosopher, ‘advised the World Health Organization, the World Bank, the World Economic Forum, the US National Intelligence Council, the UK Prime Minister’s Office, Cabinet Office, and Government Office for Science’, and he recently contributed to a report from the Secretary-General of the United Nations that specifically mentions ‘long-termism’. The point is that longtermism might be one of the most influential ideologies that few people outside of elite universities and Silicon Valley have ever heard about. I believe this needs to change because, as a former longtermist who published an entire book four years ago in defence of the general idea, I have come to see this worldview as quite possibly the most dangerous secular belief system in the world today. But to understand the nature of the beast, we need to first dissect it, examining its anatomical features and physiological functions. The initial thing to notice is that longtermism, as proposed by Bostrom and Beckstead, is not equivalent to ‘caring about the long term’ or ‘valuing the wellbeing of future generations’. It goes way beyond this. At its core is a simple – albeit flawed, in my opinion – analogy between individual persons and humanity as a whole. To illustrate the idea, consider the case of Frank Ramsey, a scholar at the University of Cambridge widely considered by his peers as among his generation’s most exceptional minds. ‘There was something of Newton about him,’ the belletrist Lytton Strachey once said. G E Moore wrote of Ramsey’s ‘very exceptional brilliance’. And John Maynard Keynes described a paper of Ramsey’s as ‘one of the most remarkable contributions to mathematical economics ever made’. But Ramsey’s story isn’t a happy one. On 19 January 1930, he died in a London hospital following a surgical procedure, the likely cause of death being a liver infection from swimming in the River Cam, which winds its way through Cambridge. Ramsey was only 26 years old. One could argue that there are two distinct reasons this outcome was tragic. The first is the most obvious: it cut short Ramsey’s life, depriving him of everything he could have experienced had he survived – the joys and happiness, the love and friendship: all that makes life worth living. In this sense, Ramsey’s early demise was a personal tragedy. But, secondly, his death also robbed the world of an intellectual superstar apparently destined to make even more extraordinary contributions to human knowledge. ‘The number of trails Ramsey laid was remarkable,’ writes Sir Partha Dasgupta. But how many more trails might he have blazed? ‘The loss to your generation is agonising to think of,’ Strachey lamented, ‘what a light has gone out’ – which leaves one wondering how Western intellectual history might have been different if Ramsey hadn’t died so young. From this perspective, one could argue that, although the personal tragedy of Ramsey’s death was truly terrible, the immensity of his potential to have changed the world for the better makes the second tragedy even worse. In other words, the badness of his death stems mostly, perhaps overwhelmingly, from his unfulfilled potential rather than the direct, personal harms that he experienced. Or so the argument goes. Longtermists would map these claims and conclusions on to humanity itself, as if humanity is an individual with its very own ‘potential’ to squander or fulfil, ruin or realise, over the course of ‘its lifetime’. So, on the one hand, a catastrophe that reduces the human population to zero would be tragic because of all the suffering it would inflict upon those alive at the time. Imagine the horror of starving to death in subfreezing temperatures, under pitch-black skies at noon, for years or decades after a thermonuclear war. This is the first tragedy, a personal tragedy for those directly affected. But there is, longtermists would argue, a second tragedy that is astronomically worse than the first, arising from the fact that our extinction would permanently foreclose what could be an extremely long and prosperous future over the next, say, ~10100 years (at which point the ‘heat death’ will make life impossible). In doing this, it would irreversibly destroy the ‘vast and glorious’ longterm potential of humanity, in Ord’s almost religious language – a ‘potential’ so huge, given the size of the Universe and the time left before reaching thermodynamic equilibrium, that the first tragedy would utterly pale in comparison. This immediately suggests another parallel between individuals and humanity: death isn’t the only way that someone’s potential could be left unfulfilled. Imagine that Ramsey hadn’t died young but, instead of studying, writing and publishing scholarly papers, he’d spent his days in the local bar playing pool and drinking. Same outcome, different failure mode. Applying this to humanity, longtermists would argue that there are failure modes that could leave our potential unfulfilled without us dying out, which I will return to below. On this view, a climate catastrophe will be a small blip – like a 90-year-old who stubbed his toe when he was two To summarise these ideas so far, humanity has a ‘potential’ of its own, one that transcends the potentials of each individual person, and failing to realise this potential would be extremely bad – indeed, as we will see, a moral catastrophe of literally cosmic proportions. This is the central dogma of longtermism: nothing matters more, ethically speaking, than fulfilling our potential as a species of ‘Earth-originating intelligent life’. It matters so much that longtermists have even coined the scary-sounding term ‘existential risk’ for any possibility of our potential being destroyed, and ‘existential catastrophe’ for any event that actually destroys this potential. Why do I think this ideology is so dangerous? The short answer is that elevating the fulfilment of humanity’s supposed potential above all else could nontrivially increase the probability that actual people – those alive today and in the near future – suffer extreme harms, even death. Consider that, as I noted elsewhere, the longtermist ideology inclines its adherents to take an insouciant attitude towards climate change. Why? Because even if climate change causes island nations to disappear, triggers mass migrations and kills millions of people, it probably isn’t going to compromise our longterm potential over the coming trillions of years. If one takes a cosmic view of the situation, even a climate catastrophe that cuts the human population by 75 per cent for the next two millennia will, in the grand scheme of things, be nothing more than a small blip – the equivalent of a 90-year-old man having stubbed his toe when he was two. Bostrom’s argument is that ‘a non-existential disaster causing the breakdown of global civilisation is, from the perspective of humanity as a whole, a potentially recoverable setback.’ It might be ‘a giant massacre for man’, he adds, but so long as humanity bounces back to fulfil its potential, it will ultimately register as little more than ‘a small misstep for mankind’. Elsewhere, he writes that the worst natural disasters and devastating atrocities in history become almost imperceptible trivialities when seen from this grand perspective. Referring to the two world wars, AIDS and the Chernobyl nuclear accident, he declares that ‘tragic as such events are to the people immediately affected, in the big picture of things … even the worst of these catastrophes are mere ripples on the surface of the great sea of life.’ This way of seeing the world, of assessing the badness of AIDS and the Holocaust, implies that future disasters of the same (non-existential) scope and intensity should also be categorised as ‘mere ripples’. If they don’t pose a direct existential risk, then we ought not to worry much about them, however tragic they might be to individuals. As Bostrom wrote in 2003, ‘priority number one, two, three and four should … be to reduce existential risk.’ He reiterated this several years later in arguing that we mustn’t ‘fritter … away’ our finite resources on ‘feel-good projects of suboptimal efficacy’ such as alleviating global poverty and reducing animal suffering, since neither threatens our longterm potential, and our longterm potential is what really matters. Ord echoes these views in arguing that, of all the problems facing humanity, our ‘first great task … is to reach a place of safety – a place where existential risk’ – as he defines it – ‘is low and stays low’, which he dubs ‘existential security’. More than anything else, what matters is doing everything necessary to ‘preserve’ and ‘protect’ our potential by ‘extracting ourselves from immediate danger’ and devising robust ‘safeguards that will defend humanity from dangers over the longterm future, so that it becomes impossible to fail.’ Although Ord gives a nod to climate change, he also claims – based on a dubious methodology – that the chance of climate change causing an existential catastrophe is only ∼1 in 1,000, which is a whole two orders of magnitude lower than the probability of superintelligent machines destroying humanity this century, according to Ord. What’s really notable here is that the central concern isn’t the effect of the climate catastrophe on actual people around the world (remember, in the grand scheme, this would be, in Bostrom’s words, a ‘small misstep for mankind’) but the slim possibility that, as Ord puts it in The Precipice, this catastrophe ‘poses a risk of an unrecoverable collapse of civilisation or even the complete extinction of humanity’. Again, the harms caused to actual people (especially those in the Global South) might be significant in absolute terms, but when compared to the ‘vastness’ and ‘glory’ of our longterm potential in the cosmos, they hardly even register. Yet the implications of longtermism are far more worrisome. If our top four priorities are to avoid an existential catastrophe – ie, to fulfil ‘our potential’ – then what’s not on the table for making this happen? Consider Thomas Nagel’s comment about how the notion of what we might call the ‘greater good’ has been used to ‘justify’ certain atrocities (eg, during war). If the ends ‘justify’ the means, he argues, and the ends are thought to be sufficiently large (eg, national security), then this ‘can be brought to bear to ease the consciences of those responsible for a certain number of charred babies’. Now imagine what might be ‘justified’ if the ‘greater good’ isn’t national security but the cosmic potential of Earth-originating intelligent life over the coming trillions of years? During the Second World War, 40 million civilians perished, but compare this number to the 1054 or more people (in Bostrom’s estimate) who could come to exist if we can avoid an existential catastrophe. What shouldn’t we do to ‘protect’ and ‘preserve’ this potential? To ensure that these unborn people come to exist? What means can’t be ‘justified’ by this cosmically significant moral end? Bostrom himself argued that we should seriously consider establishing a global, invasive surveillance system that monitors every person on the planet in realtime, to amplify the ‘capacities for preventive policing’ (eg, to prevent omnicidal terrorist attacks that could devastate civilisation). Elsewhere, he’s written that states should use preemptive violence/war to avoid existential catastrophes, and argued that saving billions of actual people is the moral equivalent of reducing existential risk by utterly minuscule amounts. In his words, even if there is ‘a mere 1 per cent chance’ of 1054 people existing in the future, then ‘the expected value of reducing existential risk by a mere one billionth of one billionth of one percentage point is worth 100 billion times as much as a billion human lives.’ Such fanaticism – a word that some longtermists embrace – has led a growing number of critics to worry about what might happen if political leaders in the real world were to take Bostrom’s view seriously. To quote the mathematical statistician Olle Häggström, who – perplexingly – tends otherwise to speak favourably of longtermism: I feel extremely uneasy about the prospect that [the calculations above] might become recognised among politicians and decision-makers as a guide to policy worth taking literally. It is simply too reminiscent of the old saying ‘If you want to make an omelette, you must be willing to break a few eggs,’ which has typically been used to explain that a bit of genocide or so might be a good thing, if it can contribute to the goal of creating a future utopia. Imagine a situation where the head of the CIA explains to the US president that they have credible evidence that somewhere in Germany, there is a lunatic who is working on a doomsday weapon and intends to use it to wipe out humanity, and that this lunatic has a one-in-a-million chance of succeeding. They have no further information on the identity or whereabouts of this lunatic. If the president has taken Bostrom’s argument to heart, and if he knows how to do the arithmetic, he may conclude that it is worthwhile conducting a full-scale nuclear assault on Germany to kill every single person within its borders.Here, then, are a few reasons I find longtermism to be profoundly dangerous. Yet there are additional, fundamental problems with this worldview that no one, to my knowledge, has previously noted in writing. For example, there’s a good case to make that the underlying commitments of longtermism are a major reason why humanity faces so many unprecedented risks to its survival in the first place. Longtermism might, in other words, be incompatible with the attainment of ‘existential security’, meaning that the only way to genuinely reduce the probability of extinction or collapse in the future might be to abandon the longtermist ideology entirely. To Bostrom and Ord, failing to become posthuman would prevent us from realising our vast, glorious potential To understand the argument, let’s first unpack what longtermists mean by our ‘longterm potential’, an expression that I have so far used without defining. We can analyse this concept into three main components: transhumanism, space expansionism, and a moral view closely associated with what philosophers call ‘total utilitarianism’. The first refers to the idea that we should use advanced technologies to reengineer our bodies and brains to create a ‘superior’ race of radically enhanced posthumans (which, confusingly, longtermists place within the category of ‘humanity’). Although Bostrom is perhaps the most prominent transhumanist today, longtermists have shied away from using the term ‘transhumanism’, probably because of its negative associations. Susan Levin, for example, points out that contemporary transhumanism has its roots in the Anglo-American eugenics movement, and transhumanists such as Julian Savulescu, who co-edited the book Human Enhancement (2009) with Bostrom, have literally argued for the consumption of ‘morality-boosting’ chemicals such as oxytocin to avoid an existential catastrophe (which he calls ‘ultimate harm’). As Savulescu writes with a colleague, ‘it is a matter of such urgency to improve humanity morally … that we should seek whatever means there are to effect this.’ Such claims are not only controversial but for many quite disturbing, and hence longtermists have attempted to distance themselves from such ideas, while nonetheless championing the ideology. Transhumanism claims that there are various ‘posthuman modes of being’ that are far better than our current human mode. We could, for instance, genetically alter ourselves to gain perfect control over our emotions, or access the internet via neural implants, or maybe even upload our minds to computer hardware to achieve ‘digital immortality’. As Ord urges in The Precipice, think of how awesome it would be to perceive the world via echolocation, like bats and dolphins, or magnetoreception, like red foxes and homing pigeons. ‘Such uncharted experiences,’ Ord writes, ‘exist in minds much less sophisticated than our own. What experiences, possibly of immense value, could be accessible, then, to minds much greater?’ Bostrom’s most fantastical exploration of these possibilities comes from his evocative ‘Letter from Utopia’ (2008), which depicts a techno-Utopian world full of superintelligent posthumans awash in so much ‘pleasure’ that, as the letter’s fictional posthuman writes, ‘we sprinkle it in our tea.’ The connection with longtermism is that, according to Bostrom and Ord, failing to become posthuman would seemingly prevent us from realising our vast and glorious potential, which would be existentially catastrophic. As Bostrom put it in 2012, ‘the permanent foreclosure of any possibility of this kind of transformative change of human biological nature may itself constitute an existential catastrophe.’ Similarly, Ord asserts that ‘forever preserving humanity as it is now may also squander our legacy, relinquishing the greater part of our potential.’ The second component of our potential – space expansionism – refers to the idea that we must colonise as much of our future light cone as possible: that is, the region of spacetime that is theoretically accessible to us. According to longtermists, our future light cone contains a huge quantity of exploitable resources, which they refer to as our ‘cosmic endowment’ of negentropy (or reverse entropy). The Milky Way alone, Ord writes, is ‘150,000 light years across, encompassing more than 100 billion stars, most with their own planets.’ Attaining humanity’s longterm potential, he continues, ‘requires only that [we] eventually travel to a nearby star and establish enough of a foothold to create a new flourishing society from which we could venture further.’ By spreading ‘just six light years at a time’, our posthuman descendants could make ‘almost all the stars of our galaxy … reachable’ since ‘each star system, including our own, would need to settle just the few nearest stars [for] the entire galaxy [to] eventually fill with life.’ The process could be exponential, resulting in ever-more ‘flourishing’ societies with each additional second our descendants hop from star to star. But why exactly would we want to do this? What’s so important about flooding the Universe with new posthuman civilisations? This leads to the third component: total utilitarianism, which I will refer to as ‘utilitarianism’ for short. Although some longtermists insist that they aren’t utilitarians, we should right away note that this is mostly a smoke-and-mirrors act to deflect criticisms that longtermism – and, more generally, the effective altruism (EA) movement from which it emerged – is nothing more than utilitarianism repackaged. The fact is that the EA movement is deeply utilitarian, at least in practice, and indeed, before it decided upon a name, the movement’s early members, including Ord, seriously considered calling it the ‘effective utilitarian community’. This being said, utilitarianism is an ethical theory that specifies our sole moral obligation as being to maximise the total amount of ‘intrinsic value’ in the world, as tallied up from a disembodied, impartial, cosmic vantage point called ‘the point of view of the Universe’. From this view, it doesn’t matter how value – which utilitarian hedonists equate with pleasure – is distributed among people across space and time. All that matters is the total net sum. For example, imagine that there are 1 trillion people who have lives of value ‘1’, meaning that they are just barely worth living. This gives a total value of 1 trillion. Now consider an alternative universe in which 1 billion people have lives with a value of ‘999’, meaning that their lives are extremely good. This gives a total value of 999 billion. Since 999 billion is less than 1 trillion, the first world full of lives hardly worth living would be morally better than the second world, and hence, if a utilitarian were forced to choose between these, she would pick the former. (This is called the ‘repugnant conclusion’, which longtermists such as Ord, MacAskill and Greaves recently argued shouldn’t be taken very seriously. For them, the first world really might be better!) Beckstead argued that we should prioritise the lives of people in rich countries over those in poor countries The underlying reasoning here is based on the idea that people – you and I – are nothing more than means to an end. We don’t matter in ourselves; we have no inherent value of our own. Instead, people are understood as the ‘containers’ of value, and hence we matter only insofar as we ‘contain’ value, and therefore contribute to the overall net amount of value in the Universe between the Big Bang and the heat death. Since utilitarianism tells us to maximise value, it follows that the more people (value containers) who exist with net-positive amounts of value (pleasure), the better the Universe will become, morally speaking. In a phrase: people exist for the sake of maximising value, rather than value existing for the sake of benefitting people. This is why longtermists are obsessed with calculating how many people could exist in the future if we were to colonise space and create vast computer simulations around stars in which unfathomably huge numbers of people live net-positive lives in virtual-reality environments. I already mentioned Bostrom’s estimate of 1054 future people, which includes many of these ‘digital people’, but in his bestseller Superintelligence (2014) he puts the number even higher at 1058 people, nearly all of whom would ‘live rich and happy lives while interacting with one another in virtual environments’. Greaves and MacAskill are similarly excited about this possibility, estimating that some 1045 conscious beings in computer simulations could exist within the Milky Way alone. That is what our ‘vast and glorious’ potential consists of: massive numbers of technologically enhanced digital posthumans inside huge computer simulations spread throughout our future light cone. It is for this goal that, in Häggström’s scenario, a longtermist politician would annihilate Germany. It is for this goal that we must not ‘fritter … away’ our resources on such things as solving global poverty. It is for this goal that we should consider implementing a global surveillance system, keep pre-emptive war on the table, and focus more on superintelligent machines than saving people in the Global South from the devastating effects of climate change (mostly caused by the Global North). In fact, Beckstead has even argued that, for the sake of attaining this goal, we should actually prioritise the lives of people in rich countries over those in poor countries, since influencing the long-term future is of ‘overwhelming importance’, and the former are more likely to influence the long-term future than the latter. To quote a passage from Beckstead’s 2013 PhD dissertation, which Ord enthusiastically praises as one of the most important contributions to the longtermist literature: Saving lives in poor countries may have significantly smaller ripple effects than saving and improving lives in rich countries. Why? Richer countries have substantially more innovation, and their workers are much more economically productive. [Consequently,] it now seems more plausible to me that saving a life in a rich country is substantially more important than saving a life in a poor country, other things being equal.This is just the tip of the iceberg. Consider the implications of this conception of ‘our potential’ for the development of technology and creation of new risks. Since realising our potential is the ultimate moral goal for humanity, and since our descendants cannot become posthuman, colonise space and create ~1058 people in computer simulations without technologies far more advanced than those around today, failing to develop more technology would itself constitute an existential catastrophe – a failure mode (comparable to Ramsey neglecting his talents by spending his days playing pool and drinking) that Bostrom calls ‘plateauing’. Indeed, Bostrom places this idea front-and-centre in his canonical definition of ‘existential risk’, which denotes any future event that would prevent humanity from reaching and/or sustaining a state of ‘technological maturity’, meaning ‘the attainment of capabilities affording a level of economic productivity and control over nature close to the maximum that could feasibly be achieved.’ Technological maturity is the linchpin here because controlling nature and increasing economic productivity to the absolute physical limits are ostensibly necessary for creating the maximum quantity of ‘value’ within our future light cone. But reflect for a moment on how humanity got itself into the current climatic and ecological crisis. Behind the extraction and burning of fossil fuels, decimation of ecosystems and extermination of species has been the notion that nature is something to be controlled, subjugated, exploited, vanquished, plundered, transformed, reconfigured and manipulated. As the technology theorist Langdon Winner writes in Autonomous Technology (1977), since the time of Francis Bacon our view of technology has been ‘inextricably bound to a single conception of the manner in which power is used – the style of absolute mastery, the despotic, one-way control of the master over the slave.’ He adds: There are seldom any reservations about man’s rightful role in conquering, vanquishing, and subjugating everything natural. This is his power and his glory. What would in other situations seem [to be] rather tawdry and despicable intentions are here the most honourable of virtues. Nature is the universal prey, to manipulate as humans see fit.This is precisely what we find in Bostrom’s account of existential risks and its associated normative futurology: nature, the entire Universe, our ‘cosmic endowment’ is there for the plundering, to be manipulated, transformed and converted into ‘value-structures, such as sentient beings living worthwhile lives’ in vast computer simulations, quoting Bostrom’s essay ‘Astronomical Waste’ (2003). Yet this Baconian, capitalist view is one of the most fundamental root causes of the unprecedented environmental crisis that now threatens to destroy large regions of the biosphere, Indigenous communities around the world, and perhaps even Western technological civilisation itself. While other longtermists have not been as explicit as Bostrom, there is a clear tendency to see the natural world the way utilitarianism sees people: as means to some abstract, impersonal end, and nothing more. MacAskill and a colleague, for example, write that the EA movement, and by implication longtermism, is ‘tentatively welfarist in that its tentative aim in doing good concerns promoting wellbeing only and not, say, protecting biodiversity or conserving natural beauty for their own sakes.’ On this account, every problem arises from too little rather than too much technology Just as worrisome is the longtermist demand that we must create ever-more powerful technologies, despite the agreed-upon fact that the overwhelming source of risk to human existence these days comes from these very technologies. In Ord’s words, ‘without serious efforts to protect humanity, there is strong reason to believe the risk will be higher this century, and increasing with each century that technological progress continues.’ Similarly, in 2012 Bostrom acknowledges that the great bulk of existential risk in the foreseeable future consists of anthropogenic existential risks – that is, arising from human activity. In particular, most of the biggest existential risks seem to be linked to potential future technological breakthroughs that may radically expand our ability to manipulate the external world or our own biology. As our powers expand, so will the scale of their potential consequences – intended and unintended, positive and negative.On this view, there is only one way forward – more technological development – even if this is the most dangerous path into the future. But how much sense does this make? Surely if we want to maximise our chances of survival, we should oppose the development of dangerous new dual-use technologies. If more technology equals greater risk – as history clearly shows and technological projections affirm – then perhaps the only way to actually attain a state of ‘existential security’ is to slow down or completely halt further technological innovation. But longtermists have an answer to this conundrum: the so-called ‘value-neutrality thesis’. This states that technology is a morally neutral object, ie, ‘just a tool’. The idea is most famously encapsulated in the NRA’s slogan ‘Guns don’t kill people, people kill people,’ which conveys the message that the consequences of technology, whether good or bad, beneficial or harmful, are entirely determined by the users, not the artefacts. As Bostrom put it in 2002, ‘we should not blame civilisation or technology for imposing big existential risks,’ adding that ‘because of the way we have defined existential risks, a failure to develop technological civilisation would imply that we had fallen victims of an existential disaster.’ Ord similarly argues that ‘the problem is not so much an excess of technology as a lack of wisdom,’ before going on to quote Carl Sagan’s book Pale Blue Dot (1994): ‘Many of the dangers we face indeed arise from science and technology but, more fundamentally, because we have become powerful without becoming commensurately wise.’ In other words, it is our fault for not being smarter, wiser and more ethical, a cluster of deficiencies that many longtermists believe, in a bit of twisted logic, could be rectified by technologically reengineering our cognitive systems and moral dispositions. Everything, on this account, is an engineering problem, and hence every problem arises from too little rather than too much technology. We can now begin to see how longtermism might be self-defeating. Not only could its ‘fanatical’ emphasis on fulfilling our longterm potential lead people to, eg, neglect non-existential climate change, prioritise the rich over the poor and perhaps even ‘justify’ pre-emptive violence and atrocities for the ‘greater cosmic good’ but it also contains within it the very tendencies – Baconianism, capitalism and value-neutrality – that have driven humanity inches away from the precipice of destruction. Longtermism tells us to maximise economic productivity, our control over nature, our presence in the Universe, the number of (simulated) people who exist in the future, the total amount of impersonal ‘value’ and so on. But to maximise, we must develop increasingly powerful – and dangerous – technologies; failing to do this would itself be an existential catastrophe. Not to worry, though, because technology is not responsible for our worsening predicament, and hence the fact that most risks stem directly from technology is no reason to stop creating more technology. Rather, the problem lies with us, which means only that we must create even more technology to transform ourselves into cognitively and morally enhanced posthumans. This looks like a recipe for disaster. Creating a new race of ‘wise and responsible’ posthumans is implausible and, if advanced technologies continue to be developed at the current rate, a global-scale catastrophe is almost certainly a matter of when rather than if. Yes, we will need advanced technologies if we wish to escape Earth before it’s sterilised by the Sun in a billion years or so. But the crucial fact that longtermists miss is that technology is far more likely to cause our extinction before this distant future event than to save us from it. If you, like me, value the continued survival and flourishing of humanity, you should care about the long term but reject the ideology of longtermism, which is not only dangerous and flawed but might be contributing to, and reinforcing, the risks that now threaten every person on the planet. \"}</script><link rel=\"preload\" as=\"image\" imageSrcSet=\"https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=640&amp;quality=75&amp;format=auto 640w, https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=750&amp;quality=75&amp;format=auto 750w, https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=828&amp;quality=75&amp;format=auto 828w, https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=1080&amp;quality=75&amp;format=auto 1080w, https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=1200&amp;quality=75&amp;format=auto 1200w, https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=1920&amp;quality=75&amp;format=auto 1920w, https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=2048&amp;quality=75&amp;format=auto 2048w, https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=3840&amp;quality=75&amp;format=auto 3840w\" imageSizes=\"(max-width: 1700px) 100vw, 1700px\" fetchpriority=\"high\"/><meta name=\"next-head-count\" content=\"23\"/><link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"/apple-touch-icon.png\"/><link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"/favicon-32x32.png\"/><link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"/favicon-16x16.png\"/><link rel=\"manifest\" href=\"/site.webmanifest\"/><link rel=\"mask-icon\" href=\"/safari-pinned-tab.svg\" color=\"#025744\"/><meta name=\"msapplication-TileColor\" content=\"#025744\"/><meta property=\"og:locale\" content=\"en_GB\"/><meta property=\"og:site_name\" content=\"Aeon\"/><meta name=\"twitter:site\" content=\"aeonmag\"/><meta name=\"twitter:creator\" content=\"aeonmag\"/><meta name=\"twitter:handle\" content=\"aeonmag\"/><meta name=\"twitter:card\" content=\"summary_large_image\"/><link data-next-font=\"\" rel=\"preconnect\" href=\"/\" crossorigin=\"anonymous\"/><link rel=\"preload\" href=\"/_next/static/css/6a711f7a2a843a17.css\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/6a711f7a2a843a17.css\" data-n-g=\"\"/><noscript data-n-css=\"\"></noscript><script defer=\"\" nomodule=\"\" src=\"/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js\"></script><script defer=\"\" src=\"/_next/static/chunks/4292-dbce675eb95e6722.js\"></script><script defer=\"\" src=\"/_next/static/chunks/5117.398ebcf9da6be7c5.js\"></script><script src=\"/_next/static/chunks/webpack-3b311b4c057cfe1c.js\" defer=\"\"></script><script src=\"/_next/static/chunks/framework-06a898fa5a45ca5c.js\" defer=\"\"></script><script src=\"/_next/static/chunks/main-087b8ca759d30a8f.js\" defer=\"\"></script><script src=\"/_next/static/chunks/pages/_app-41c09b8afd48f9f7.js\" defer=\"\"></script><script src=\"/_next/static/chunks/3105-aae8d0e348e5c1e2.js\" defer=\"\"></script><script src=\"/_next/static/chunks/7695-22af7ebecd7e00d8.js\" defer=\"\"></script><script src=\"/_next/static/chunks/6741-c392bfb23d7263d9.js\" defer=\"\"></script><script src=\"/_next/static/chunks/8813-45e92b766d39c3cc.js\" defer=\"\"></script><script src=\"/_next/static/chunks/8755-a84d11770b93b313.js\" defer=\"\"></script><script src=\"/_next/static/chunks/8162-247ab1b7f3caad51.js\" defer=\"\"></script><script src=\"/_next/static/chunks/7-f965b75f0aa5b234.js\" defer=\"\"></script><script src=\"/_next/static/chunks/9194-4bfc7f8e442b3d56.js\" defer=\"\"></script><script src=\"/_next/static/chunks/pages/essays/%5Bid%5D-3d663d4103d883ab.js\" defer=\"\"></script><script src=\"/_next/static/9mLhaGsqiGq6dMbedGMwT/_buildManifest.js\" defer=\"\"></script><script src=\"/_next/static/9mLhaGsqiGq6dMbedGMwT/_ssgManifest.js\" defer=\"\"></script><style data-styled=\"\" data-styled-version=\"6.0.8\">.jkOWJA{position:relative;overflow:hidden;}/*!sc*/\n@media only print{.jkOWJA{display:flex;flex-direction:column;}}/*!sc*/\ndata-styled.g1[id=\"sc-3328c936-0\"]{content:\"jkOWJA,\"}/*!sc*/\n.cSmhOU{position:relative;aspect-ratio:500/313;object-fit:cover;}/*!sc*/\n@media only print{.cSmhOU{display:none;}}/*!sc*/\ndata-styled.g2[id=\"sc-3328c936-1\"]{content:\"cSmhOU,\"}/*!sc*/\n.gCPQyl{margin:0;color:#0c776d;padding:14px 24px 0 24px;position:relative;}/*!sc*/\n@media only screen and (min-width:768px){.gCPQyl{display:flex;flex-direction:column;padding:0;grid-area:top-right;text-align:right;align-items:flex-end;justify-content:flex-start;position:relative;color:white;line-height:1.2;position:relative;z-index:1;min-width:9em;}.gCPQyl >*{position:relative;}.gCPQyl >*:before{content:'';display:block;position:absolute;top:-2em;left:-2em;width:calc(100% + 4em);height:calc(100% + 4em);background:rgba(0,0,0,0.6);box-shadow:0 0 140px 140px rgba(0,0,0,0.6);z-index:-1;opacity:0.3;}}/*!sc*/\n@media only print{.gCPQyl{color:#000;padding:0 24px;}.gCPQyl svg{display:none;}}/*!sc*/\ndata-styled.g3[id=\"sc-3328c936-2\"]{content:\"gCPQyl,\"}/*!sc*/\n@media only screen and (min-width:768px){.heKKOr{position:absolute;top:0;right:0;left:0;bottom:0;padding:60px 25px 60px;height:100%;box-sizing:border-box;display:grid;grid-template-areas:'top-left top-center top-right' 'center-left center-center center-right' 'bottom-left bottom-center bottom-right';max-height:100vh;max-width:100%;}}/*!sc*/\n@media only screen and (min-width:1280px){.heKKOr{left:50%;transform:translateX(-50%);width:140rem;}}/*!sc*/\n@media only print{.heKKOr{position:static;display:block;}}/*!sc*/\ndata-styled.g4[id=\"sc-3328c936-3\"]{content:\"heKKOr,\"}/*!sc*/\n.kmvKVn{font-family:\"Academica Book Pro\",Times,Georgia,serif;font-size:7.2rem;line-height:1;margin-bottom:2.4rem;font-weight:600;margin-top:4.24rem;}/*!sc*/\n@media only screen and (max-width:960px){.kmvKVn{font-size:4.2rem;}}/*!sc*/\n@media only screen and (max-width:767px){.kmvKVn{color:#000;font-size:4.2rem;margin-top:1rem;text-align:left;}}/*!sc*/\n@media print{.kmvKVn{font-size:4rem;}}/*!sc*/\ndata-styled.g5[id=\"sc-3328c936-4\"]{content:\"kmvKVn,\"}/*!sc*/\n.dkTvTa{font-size:2rem;font-weight:normal;line-height:1.4;max-width:48rem;margin:0 0 1.6rem 0;}/*!sc*/\n.dkTvTa .no-wrap{white-space:nowrap;}/*!sc*/\n@media only screen and (max-width:767px){.dkTvTa{color:#000;margin:0 0 2.4rem;font-size:1.8rem;text-align:left;}}/*!sc*/\ndata-styled.g6[id=\"sc-3328c936-5\"]{content:\"dkTvTa,\"}/*!sc*/\n.djYXGS{color:#000;margin:0 0 2.4rem;font-size:1.8rem;text-align:left;}/*!sc*/\n@media only screen and (min-width:640px){.djYXGS{display:none;}}/*!sc*/\n.djYXGS .toggleMobileCreditBtn{cursor:pointer;font-size:1.1rem;padding-left:1rem;font-weight:bold;background:#0c776d21;mix-blend-mode:multiply;padding:4px 6px 3px;border-radius:10px;white-space:nowrap;}/*!sc*/\n@media print{.djYXGS .toggleMobileCreditBtn{display:none;}}/*!sc*/\ndata-styled.g7[id=\"sc-3328c936-6\"]{content:\"djYXGS,\"}/*!sc*/\n.jnMzvZ{color:currentColor;display:flex;font-size:1.1rem;justify-content:flex-end;align-items:flex-end;padding:1rem;bottom:0;position:absolute;width:100%;z-index:500;}/*!sc*/\n.jnMzvZ p{margin:0 0 0.25rem;text-align:left;}/*!sc*/\n.jnMzvZ span{align-items:center;background-color:#fff;border-radius:50%;cursor:pointer;display:flex;font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;font-size:1.4rem;font-weight:bold;height:24px;justify-content:center;overflow:hidden;position:relative;min-width:24px;margin:0;line-height:1;color:#0c776d;}/*!sc*/\n.jnMzvZ span:before{background:#0c776d11;border-radius:50%;content:'';height:100%;left:0;pointer-events:none;position:absolute;top:0;width:100%;}/*!sc*/\n@media only print{.jnMzvZ span{display:none;}}/*!sc*/\n.jnMzvZ .sc-3328c936-8{opacity:0;display:none;}/*!sc*/\n@media print,screen and (min-width: 768px){.jnMzvZ{display:none;}}/*!sc*/\ndata-styled.g8[id=\"sc-3328c936-7\"]{content:\"jnMzvZ,\"}/*!sc*/\n.dVTstl{color:#999;font-size:1.2rem;margin:0;max-width:720px;padding:0rem 2rem;text-align:left;}/*!sc*/\n@media (min-width:640px){.dVTstl{display:none;}}/*!sc*/\ndata-styled.g9[id=\"sc-3328c936-8\"]{content:\"dVTstl,\"}/*!sc*/\n.ljcMFZ{position:relative;min-height:17rem;}/*!sc*/\n@media only screen and (min-width:768px){.ljcMFZ{display:grid;grid-template-areas:'main-tl main-tc main-tr' 'main-cl main-cc main-cr' 'main-bl main-bc main-br' 'attribution attribution attribution';grid-template-columns:0.5fr 1fr 0.5fr;grid-template-rows:0.5fr 1fr 0.5fr auto;}}/*!sc*/\n@media only screen and (min-width:1280px){.ljcMFZ{grid-template-areas:'main-tl main-tc main-tr' 'main-cl main-cc main-cr' 'main-bl main-bc main-br' 'attribution attribution attribution';}}/*!sc*/\ndata-styled.g11[id=\"sc-1da62608-1\"]{content:\"ljcMFZ,\"}/*!sc*/\n.ghhOJi{grid-area:main-tl/main-tl/main-br/main-br;}/*!sc*/\n@media print{.ghhOJi{grid-area:main-br/main-br;}}/*!sc*/\ndata-styled.g12[id=\"sc-1da62608-2\"]{content:\"ghhOJi,\"}/*!sc*/\n.lkpQNV{grid-area:attribution;display:none;min-height:5.4rem;justify-content:flex-end;}/*!sc*/\n@media (min-width:768px){.lkpQNV{display:flex;}}/*!sc*/\n@media print{.lkpQNV{display:none;}}/*!sc*/\ndata-styled.g13[id=\"sc-1da62608-3\"]{content:\"lkpQNV,\"}/*!sc*/\n.fnvdUn{color:#999;font-size:1.2rem;justify-self:flex-end;margin-bottom:0;margin-top:1rem;max-width:720px;padding-bottom:8px;padding-right:2rem;text-align:left;transition:opacity 0.6s;opacity:1;}/*!sc*/\n@media only screen and (min-width:1440px){.fnvdUn{margin-right:0;text-align:left;justify-self:flex-end;}}/*!sc*/\n.fnvdUn p{margin:0.4rem 2.4rem 2rem 2.4rem;transition:opacity 0.6s;opacity:1;}/*!sc*/\n@media only screen and (min-width:640px) and (max-width:768px){.fnvdUn p{margin:0 2rem;}}/*!sc*/\n@media only screen and (min-width:1440px){.fnvdUn p{max-width:70vw;}}/*!sc*/\n@media print{.fnvdUn{display:none;}}/*!sc*/\ndata-styled.g14[id=\"sc-1da62608-4\"]{content:\"fnvdUn,\"}/*!sc*/\n.dpJmnC{padding:0 1.8rem;}/*!sc*/\n@media screen and (max-width:640px){.dpJmnC{padding:0 0.6rem;}}/*!sc*/\ndata-styled.g24[id=\"sc-47bc2fa8-0\"]{content:\"dpJmnC,\"}/*!sc*/\n.bwTVbW{box-sizing:border-box;max-width:140rem;padding:0 1.8rem;display:flex;flex-direction:row;margin-left:auto;margin-right:auto;}/*!sc*/\n@media (max-width:1440px){.bwTVbW{max-width:120rem;}}/*!sc*/\ndata-styled.g25[id=\"sc-ae266dcd-0\"]{content:\"bwTVbW,\"}/*!sc*/\n.dmJOTI{margin-right:3.6rem;}/*!sc*/\n@media (max-width:960px){.dmJOTI{margin-bottom:2.4rem;}}/*!sc*/\n.gSiEIv{margin-right:0rem;}/*!sc*/\n@media (max-width:960px){.gSiEIv{margin-bottom:2.4rem;}}/*!sc*/\ndata-styled.g26[id=\"sc-bab0e3bb-0\"]{content:\"dmJOTI,gSiEIv,\"}/*!sc*/\n.hlaCbq{display:flex;font-size:1.6rem;flex-direction:column;line-height:1.6;}/*!sc*/\ndata-styled.g27[id=\"sc-bab0e3bb-1\"]{content:\"hlaCbq,\"}/*!sc*/\n.fQckeD{color:rgba(0,0,0,0.65);text-decoration:none;font-size:1.6rem;margin:0.2rem 0;padding:0.4rem 0;font-family:\"Atlas Grotesk\",\"Helvetica Neue\",Helvetica,Arial,sans-serif;}/*!sc*/\n.fQckeD:hover{color:#000;}/*!sc*/\ndata-styled.g28[id=\"sc-11b024c5-0\"]{content:\"fQckeD,\"}/*!sc*/\n.ithRRD{display:flex;order:0;flex-wrap:wrap;margin-right:3rem;height:100%;}/*!sc*/\ndata-styled.g29[id=\"sc-1b596d24-0\"]{content:\"ithRRD,\"}/*!sc*/\n.eajVBX{order:2;margin-right:3rem;}/*!sc*/\n@media (max-width:960px){.eajVBX{margin-bottom:2.4rem;}}/*!sc*/\ndata-styled.g31[id=\"sc-89e3ac83-0\"]{content:\"eajVBX,\"}/*!sc*/\n.pbUpk{display:flex;margin-top:0.5rem;}/*!sc*/\n.pbUpk a{display:flex;align-items:center;padding:0.8rem 2rem 0.8rem 0;}/*!sc*/\n.pbUpk svg{vertical-align:middle;}/*!sc*/\ndata-styled.g33[id=\"sc-89e3ac83-2\"]{content:\"pbUpk,\"}/*!sc*/\n.eHOrNe{margin:0;font-size:1.6rem;}/*!sc*/\ndata-styled.g34[id=\"sc-89e3ac83-3\"]{content:\"eHOrNe,\"}/*!sc*/\n.iqqVMN{display:flex;flex-direction:row;align-items:flex-start;position:relative;}/*!sc*/\n.ezDGGL{display:flex;flex-direction:row;align-items:flex-start;position:relative;}/*!sc*/\n@media screen and (max-width: 640px){.ezDGGL{display:flex;}}/*!sc*/\ndata-styled.g58[id=\"sc-b4707aca-0\"]{content:\"iqqVMN,ezDGGL,\"}/*!sc*/\n.jFRXWW{width:18px;height:18px;cursor:pointer;flex-grow:0;stroke:currentColor;}/*!sc*/\n@media only screen and (max-width:639px){.jFRXWW{position:relative;top:1px;height:18px;width:18px;}}/*!sc*/\n@media screen and (max-width:640px){.jFRXWW{margin-top:5px;}}/*!sc*/\n.jWSMHq{width:18px;height:18px;cursor:pointer;flex-grow:0;stroke:currentColor;}/*!sc*/\n@media only screen and (max-width:639px){.jWSMHq{position:relative;top:1px;height:18px;width:18px;}}/*!sc*/\ndata-styled.g59[id=\"sc-b4707aca-1\"]{content:\"jFRXWW,jWSMHq,\"}/*!sc*/\n.hWayZo{appearance:none;background-color:transparent;border:0;color:inherit;flex-grow:0;font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;font-size:1.6rem;height:100%;letter-spacing:0.1rem;margin:0;outline:0;padding:0 0 0 0.5rem;transition:width ease-in-out 50ms;vertical-align:top;width:15em;display:flex;position:absolute;left:23px;z-index:50;}/*!sc*/\n@media screen and (max-width:1280px){.hWayZo{width:12.5rem;}}/*!sc*/\n@media screen and (max-width:640px){.hWayZo{font-size:16px;padding:0 0 0 0.5rem;background-color:transparent;width:50vw;top:0;left:25px;background-color:white;}}/*!sc*/\n.smYs{appearance:none;background-color:transparent;border:0;color:inherit;flex-grow:0;font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;font-size:1.6rem;height:100%;letter-spacing:0.1rem;margin:0;outline:0;padding:0 0 0 0.5rem;transition:width ease-in-out 50ms;vertical-align:top;width:0;display:none;position:absolute;left:23px;z-index:50;}/*!sc*/\n@media screen and (max-width:640px){.smYs{font-size:16px;padding:0 0 0 0.5rem;background-color:transparent;}}/*!sc*/\ndata-styled.g60[id=\"sc-b4707aca-2\"]{content:\"hWayZo,smYs,\"}/*!sc*/\n.jyfgjC{padding:0 1.8rem;overflow-x:hidden;overflow-y:auto;height:100%;max-width:138rem;margin:0 auto;padding-top:10px;min-height:-webkit-fill-available;}/*!sc*/\n@media (min-width:640px){.jyfgjC{padding:0 8.181818181818182rem;min-height:60vh;}}/*!sc*/\n.jyfgjC a:hover{opacity:1;}/*!sc*/\ndata-styled.g61[id=\"sc-382a1301-0\"]{content:\"jyfgjC,\"}/*!sc*/\n.bkbYYv{display:flex;position:relative;}/*!sc*/\n.bkbYYv >*{width:50%;}/*!sc*/\n@media (max-width:720px){.bkbYYv >*:first-child{width:60%;}.bkbYYv >*:last-child{width:40%;}}/*!sc*/\ndata-styled.g62[id=\"sc-382a1301-1\"]{content:\"bkbYYv,\"}/*!sc*/\n.gYjtbO{background-color:#fff8;color:black;position:fixed;top:0;width:100%;min-height:100vh;min-height:-webkit-fill-available;z-index:2500;opacity:0;pointer-events:none;transition:opacity 0.2s cubic-bezier(.645,.045,.355,1);}/*!sc*/\ndata-styled.g63[id=\"sc-382a1301-2\"]{content:\"gYjtbO,\"}/*!sc*/\n.hAFhbJ{background-color:#fff;box-shadow:rgb(0,0,0,0.25) 0px 0px 3rem;}/*!sc*/\n@media (max-width:640px){.hAFhbJ{background-color:#fff;max-width:100%;}}/*!sc*/\ndata-styled.g64[id=\"sc-382a1301-3\"]{content:\"hAFhbJ,\"}/*!sc*/\n.dpKclR{display:flex;flex-direction:column;padding:0rem 0 1.8rem 0;}/*!sc*/\ndata-styled.g65[id=\"sc-382a1301-4\"]{content:\"dpKclR,\"}/*!sc*/\n.fxVzSw{color:#0c776d;font-size:4.2rem;font-weight:600;font-family:\"Atlas Grotesk\",\"Helvetica Neue\",Helvetica,Arial,sans-serif;padding:0;line-height:1.2;}/*!sc*/\n.fxVzSw:hover{text-decoration:underline;}/*!sc*/\n@media (max-width:720px){.fxVzSw{font-size:2.4rem;line-height:1.3;}}/*!sc*/\n.fGNXsA{color:#035a6d;font-size:4.2rem;font-weight:600;font-family:\"Atlas Grotesk\",\"Helvetica Neue\",Helvetica,Arial,sans-serif;padding:0;line-height:1.2;}/*!sc*/\n.fGNXsA:hover{text-decoration:underline;}/*!sc*/\n@media (max-width:720px){.fGNXsA{font-size:2.4rem;line-height:1.3;}}/*!sc*/\n.eWXizB{color:#940b52;font-size:4.2rem;font-weight:600;font-family:\"Atlas Grotesk\",\"Helvetica Neue\",Helvetica,Arial,sans-serif;padding:0;line-height:1.2;}/*!sc*/\n.eWXizB:hover{text-decoration:underline;}/*!sc*/\n@media (max-width:720px){.eWXizB{font-size:2.4rem;line-height:1.3;}}/*!sc*/\n.eSElSD{color:#9d120d;font-size:4.2rem;font-weight:600;font-family:\"Atlas Grotesk\",\"Helvetica Neue\",Helvetica,Arial,sans-serif;padding:0;line-height:1.2;}/*!sc*/\n.eSElSD:hover{text-decoration:underline;}/*!sc*/\n@media (max-width:720px){.eSElSD{font-size:2.4rem;line-height:1.3;}}/*!sc*/\n.fIRhhw{color:#c16e15;font-size:4.2rem;font-weight:600;font-family:\"Atlas Grotesk\",\"Helvetica Neue\",Helvetica,Arial,sans-serif;padding:0;line-height:1.2;}/*!sc*/\n.fIRhhw:hover{text-decoration:underline;}/*!sc*/\n@media (max-width:720px){.fIRhhw{font-size:2.4rem;line-height:1.3;}}/*!sc*/\ndata-styled.g66[id=\"sc-382a1301-5\"]{content:\"fxVzSw,fGNXsA,eWXizB,eSElSD,fIRhhw,\"}/*!sc*/\n.hXKHtL{display:flex;flex-direction:column;}/*!sc*/\n@media (max-width:640px),(max-height:700px){.hXKHtL{flex-direction:row;}.hXKHtL >*{flex:1;}}/*!sc*/\ndata-styled.g67[id=\"sc-382a1301-6\"]{content:\"hXKHtL,\"}/*!sc*/\n.kYQdxO{padding:0rem 0 1.8rem 0;display:flex;flex-direction:column;}/*!sc*/\ndata-styled.g68[id=\"sc-382a1301-7\"]{content:\"kYQdxO,\"}/*!sc*/\n.bbtlGE{font-size:3.2rem;line-height:1.4;padding:0;font-family:\"Atlas Grotesk\",\"Helvetica Neue\",Helvetica,Arial,sans-serif;}/*!sc*/\n.bbtlGE:hover{text-decoration:underline;}/*!sc*/\n@media (max-width:720px){.bbtlGE{font-size:2.2rem;line-height:1.5;}}/*!sc*/\ndata-styled.g69[id=\"sc-382a1301-8\"]{content:\"bbtlGE,\"}/*!sc*/\n.hdmxQR{padding:2.8rem 0 0;display:flex;flex-direction:column;}/*!sc*/\ndata-styled.g70[id=\"sc-382a1301-9\"]{content:\"hdmxQR,\"}/*!sc*/\n.dqDhaI{align-items:center;cursor:pointer;font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;font-size:1.2rem;grid-template-areas:'icon label';grid-template-columns:30px;justify-content:left;padding-top:1rem;color:#666;letter-spacing:0.2rem;margin-bottom:3.5vh;}/*!sc*/\n.dqDhaI:hover{opacity:0.4;}/*!sc*/\n.dqDhaI span{font-size:125%;}/*!sc*/\ndata-styled.g72[id=\"sc-382a1301-11\"]{content:\"dqDhaI,\"}/*!sc*/\n.deYgkm >a:first-child{margin-right:3rem;}/*!sc*/\n@media (max-width:720px){.deYgkm >a:first-child{margin-right:1rem;}}/*!sc*/\ndata-styled.g73[id=\"sc-382a1301-12\"]{content:\"deYgkm,\"}/*!sc*/\n.hDVXDg{padding:10rem 0;display:flex;flex-wrap:wrap;align-items:center;margin-top:auto;row-gap:0.4rem;}/*!sc*/\n.hDVXDg .sc-382a1301-10{font-size:1.7rem;text-transform:uppercase;margin-right:3rem;letter-spacing:0.1em;font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;line-height:1;}/*!sc*/\n@media (max-width: 720px){.hDVXDg .sc-382a1301-10{font-size:1.4rem;margin-right:1rem;}}/*!sc*/\n@media (min-width:640px){.hDVXDg{padding:4rem 0;row-gap:1rem;}}/*!sc*/\ndata-styled.g74[id=\"sc-382a1301-13\"]{content:\"hDVXDg,\"}/*!sc*/\n.kJATgS{display:flex;margin-bottom:3.5vh;padding-top:1.2rem;}/*!sc*/\n.kJATgS a{display:flex;padding-right:1rem;margin-right:1rem;}/*!sc*/\n.kJATgS a:hover{opacity:0.4;}/*!sc*/\n@media (max-width:720px){.kJATgS svg{transform:scale(0.8);}}/*!sc*/\ndata-styled.g75[id=\"sc-382a1301-14\"]{content:\"kJATgS,\"}/*!sc*/\n.fpvPoK{background-color:transparent;}/*!sc*/\n.fpvPoK svg{margin-top:0;}/*!sc*/\n@media screen and (max-width:640px){.fpvPoK input{max-width:100px;}}/*!sc*/\ndata-styled.g76[id=\"sc-382a1301-15\"]{content:\"fpvPoK,\"}/*!sc*/\n.kNqRaZ{width:94px;}/*!sc*/\n@media (max-width:720px){.kNqRaZ{width:70px;margin-top:0.8rem;}}/*!sc*/\ndata-styled.g77[id=\"sc-382a1301-16\"]{content:\"kNqRaZ,\"}/*!sc*/\n.gXfhvL{width:104px;}/*!sc*/\n@media (max-width:720px){.gXfhvL{width:73px;margin-top:0.8rem;}}/*!sc*/\ndata-styled.g78[id=\"sc-382a1301-17\"]{content:\"gXfhvL,\"}/*!sc*/\n.fxotvc{background-color:white;color:#000;border-bottom:1px solid rgba(0,0,0,0);height:44px;display:flex;flex-direction:column;justify-content:center;padding-top:0px;padding-bottom:-1px;top:0;visiblity:hidden;position:absolute;width:100%;z-index:1000;background:transparent;color:white;padding:0;top:0;}/*!sc*/\n.fxotvc a:hover{opacity:0.7;}/*!sc*/\n.fxotvc:before{content:'';background:linear-gradient(black,transparent);height:200%;opacity:0.2;pointer-events:none;position:absolute;top:0px;transition:opacity 0.5s cubic-bezier(0.19,1,0.22,1) 0s;width:100%;z-index:-1;max-width:1700px;left:50%;transform:translateX(-50%);display:none;}/*!sc*/\n.fxotvc:hover:before{display:block;opacity:0.45;}/*!sc*/\n@media (max-width:640px){.fxotvc:hover:before{display:none;}}/*!sc*/\n@media (max-width:640px){.fxotvc{background-color:white;color:black;}}/*!sc*/\n@media (max-width:640px){.fxotvc{height:42px;}}/*!sc*/\n@media print{.fxotvc{display:none;}}/*!sc*/\ndata-styled.g79[id=\"sc-fe6897d0-0\"]{content:\"fxotvc,\"}/*!sc*/\n.krRBWz{box-sizing:border-box;display:grid;font-size:1.44rem;letter-spacing:0.2rem;grid-row-gap:8px;grid-template-areas:'nav-left primary-logo nav-right' 'social-links primary-logo secondary-logo';grid-template-columns:322px auto 322px;grid-template-rows:auto;margin-left:auto;margin-right:auto;max-width:140rem;padding:0 2.4rem;position:relative;width:100%;max-height:44px;grid-template-areas:'nav-left social-links primary-logo nav-right';grid-template-rows:100%;grid-template-columns:125px 245px auto 365px;}/*!sc*/\n@media (max-width:720px){.krRBWz{align-content:center;justify-content:space-between;}}/*!sc*/\n@media (min-width:1281px){.krRBWz{grid-template-areas:'nav-left social-links primary-logo nav-right secondary-logo';grid-template-columns:125px 315px auto 365px 75px;}}/*!sc*/\n@media (min-width:640px){.krRBWz{height:calc( 44px - (0px) * 2 );}}/*!sc*/\n@media (max-width:960px){.krRBWz{font-size:1.3rem;grid-template-columns:35% auto 35%;grid-template-rows:100%;grid-template-columns:110px 100px auto 220px;}}/*!sc*/\n@media (max-width:720px){.krRBWz{grid-template-areas:'nav-left nav-left primary-logo nav-right nav-right' 'nav-left nav-left primary-logo nav-right nav-right';}}/*!sc*/\n@media (max-width:640px){.krRBWz{font-size:1.2rem;grid-template-areas:'nav-left primary-logo nav-right';grid-template-columns:auto;grid-template-columns:120px 1fr 120px;margin-top:0;padding:0 2.4rem;}}/*!sc*/\n@media (max-width:340px){.krRBWz{font-size:1rem;}}/*!sc*/\ndata-styled.g80[id=\"sc-fe6897d0-1\"]{content:\"krRBWz,\"}/*!sc*/\n.bncHuo{grid-area:primary-logo;text-align:center;color:#0c776d;line-height:0;display:flex;flex-direction:column;justify-content:center;align-items:center;}/*!sc*/\n.bncHuo:hover{opacity:1;}/*!sc*/\n.bncHuo svg path{fill:white;}/*!sc*/\n@media (max-width:640px){.bncHuo svg path{fill:#0c776d;}}/*!sc*/\n@media (max-width:640px){.bncHuo svg{height:20px;}}/*!sc*/\ndata-styled.g81[id=\"sc-fe6897d0-2\"]{content:\"bncHuo,\"}/*!sc*/\n.cgBUSz{grid-area:secondary-logo;text-align:right;display:flex;align-items:center;justify-content:flex-end;height:auto;}/*!sc*/\n.cgBUSz a{display:inline-block;margin-top:4px;}/*!sc*/\n.cgBUSz a::after{padding:0 1.2rem;content:'/';opacity:0.7;vertical-align:middle;position:relative;top:-0.5em;}/*!sc*/\n.cgBUSz a:last-child::after{display:none;}/*!sc*/\n.cgBUSz a:first-child{display:none;}/*!sc*/\n@media (max-width:1280px){.cgBUSz{display:none;}}/*!sc*/\n@media (max-width:640px){.cgBUSz{display:none;}}/*!sc*/\ndata-styled.g82[id=\"sc-fe6897d0-3\"]{content:\"cgBUSz,\"}/*!sc*/\n.ePSmzF{grid-area:nav-left;display:flex;align-items:center;justify-content:flex-start;font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;text-align:left;}/*!sc*/\n.gRAhjy{grid-area:nav-right;display:flex;align-items:center;justify-content:flex-end;font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;text-align:right;}/*!sc*/\ndata-styled.g83[id=\"sc-fe6897d0-4\"]{content:\"ePSmzF,gRAhjy,\"}/*!sc*/\n.fOgqRT{text-transform:uppercase;display:flex;cursor:pointer;align-items:center;line-height:1;height:100%;}/*!sc*/\n.fOgqRT:after{padding:0 0.6rem;opacity:0.7;content:'/';}/*!sc*/\n@media (max-width:768px){.fOgqRT{display:flex;height:25px;}.fOgqRT::after{padding:0 0.4rem;}.fOgqRT:last-child::after{content:'';display:none;}}/*!sc*/\n@media (max-width:1280px){.fOgqRT:last-child::after{content:'';display:none;}}/*!sc*/\n.gteQza{text-transform:uppercase;display:flex;cursor:pointer;align-items:center;line-height:1;height:100%;}/*!sc*/\n.gteQza:after{padding:0 0.6rem;opacity:0.7;content:'/';}/*!sc*/\n@media (max-width:768px){.gteQza{display:none;height:25px;}.gteQza::after{padding:0 0.4rem;}.gteQza:last-child::after{content:'';display:none;}}/*!sc*/\n@media (max-width:1280px){.gteQza:last-child::after{content:'';display:none;}}/*!sc*/\n.dcLccx{text-transform:uppercase;display:flex;cursor:pointer;align-items:center;line-height:1;height:100%;}/*!sc*/\n.dcLccx:after{padding:0 0.6rem;opacity:0.7;content:'/';}/*!sc*/\n@media (max-width:768px){.dcLccx{display:flex;height:25px;}.dcLccx::after{padding:0 0.4rem;}.dcLccx:last-child::after{content:'';display:none;}}/*!sc*/\n@media (max-width:1280px){.dcLccx:last-child::after{content:'';display:none;}}/*!sc*/\n@media (max-width:639px){.dcLccx:after{content:'';padding:0;}}/*!sc*/\n.hfBcHl{text-transform:uppercase;display:flex;cursor:pointer;align-items:center;line-height:1;height:100%;}/*!sc*/\n.hfBcHl:after{padding:0 0.6rem;opacity:0.7;content:'/';}/*!sc*/\n@media (max-width:768px){.hfBcHl{display:flex;height:25px;}.hfBcHl::after{padding:0 0.4rem;}.hfBcHl:last-child::after{content:'';display:none;}}/*!sc*/\n@media (max-width:1280px){.hfBcHl:last-child::after{content:'';display:none;}.hfBcHl:after{content:'';padding:0;}}/*!sc*/\ndata-styled.g84[id=\"sc-fe6897d0-5\"]{content:\"fOgqRT,gteQza,dcLccx,hfBcHl,\"}/*!sc*/\n.cdcWxf{display:flex;grid-area:social-links;height:auto;}/*!sc*/\n.cdcWxf a{padding-right:1.2rem;display:flex;align-items:center;}/*!sc*/\n@media (max-width:720px){.cdcWxf{display:none;}}/*!sc*/\ndata-styled.g86[id=\"sc-fe6897d0-7\"]{content:\"cdcWxf,\"}/*!sc*/\n.hLkmal{height:25px;}/*!sc*/\n@media only screen and (max-width:640px){.hLkmal{height:24px;}}/*!sc*/\ndata-styled.g88[id=\"sc-fe6897d0-9\"]{content:\"hLkmal,\"}/*!sc*/\n.jFSMVy{max-width:90%;}/*!sc*/\n.jFSMVy input{background-color:transparent;font-size:1.3rem;color:grey;}/*!sc*/\n@media (max-width:640px){.jFSMVy input{background-color:white;position:absolute;max-width:100%;top:0;}}/*!sc*/\ndata-styled.g89[id=\"sc-fe6897d0-10\"]{content:\"jFSMVy,\"}/*!sc*/\n.ebKSSz{padding-left:0.6rem;padding-top:0.3rem;}/*!sc*/\ndata-styled.g90[id=\"sc-fe6897d0-11\"]{content:\"ebKSSz,\"}/*!sc*/\n.cnuBHA{white-space:nowrap;}/*!sc*/\n@media only screen and (max-width:639px){.cnuBHA{padding:1rem 0 1rem 1rem;}}/*!sc*/\n.cnuBHA >div{display:inline;}/*!sc*/\n.cnuBHA >span{display:none;}/*!sc*/\n@media only screen and (min-width:640px){.cnuBHA >div{display:none;}.cnuBHA >span{display:inline;}}/*!sc*/\ndata-styled.g91[id=\"sc-fe6897d0-12\"]{content:\"cnuBHA,\"}/*!sc*/\n@font-face{font-family:'Atlas Grotesk';src:url(\"https://assets.aeonmedia.co/fonts/Atlas/AtlasGrotesk-Light.otf\") format(\"opentype\");font-style:normal;font-weight:100;font-display:swap;}/*!sc*/\n@font-face{font-family:'Atlas Grotesk';src:url(\"https://assets.aeonmedia.co/fonts/Atlas/AtlasGrotesk-LightItalic.otf\") format(\"opentype\");font-style:italic;font-weight:100;font-display:swap;}/*!sc*/\n@font-face{font-family:'Atlas Grotesk';src:url(https://assets.aeonmedia.co/fonts/Atlas/AtlasGrotesk-RegularItalic-Web.eot);src:url(https://assets.aeonmedia.co/fonts/Atlas/AtlasGrotesk-RegularItalic-Web.eot?#iefix) format(\"embedded-opentype\"),url(https://assets.aeonmedia.co/fonts/Atlas/AtlasGrotesk-RegularItalic-Web.woff2) format(\"woff2\"),url(https://assets.aeonmedia.co/fonts/Atlas/AtlasGrotesk-RegularItalic-Web.woff) format(\"woff\");font-style:italic;font-weight:400;font-display:swap;}/*!sc*/\n@font-face{font-family:'Atlas Grotesk';src:url(https://assets.aeonmedia.co/fonts/Atlas/AtlasGrotesk-Regular-Web.eot);src:url(https://assets.aeonmedia.co/fonts/Atlas/AtlasGrotesk-Regular-Web.eot?#iefix) format(\"embedded-opentype\"),url(https://assets.aeonmedia.co/fonts/Atlas/AtlasGrotesk-Regular-Web.woff2) format(\"woff2\"),url(https://assets.aeonmedia.co/fonts/Atlas/AtlasGrotesk-Regular-Web.woff) format(\"woff\");font-style:normal;font-weight:400;font-display:swap;}/*!sc*/\n@font-face{font-family:'Atlas Grotesk';src:url(https://assets.aeonmedia.co/fonts/Atlas/AtlasGrotesk-Bold-Web.eot);src:url(https://assets.aeonmedia.co/fonts/Atlas/AtlasGrotesk-Bold-Web.eot?#iefix) format(\"embedded-opentype\"),url(https://assets.aeonmedia.co/fonts/Atlas/AtlasGrotesk-Bold-Web.woff2) format(\"woff2\"),url(https://assets.aeonmedia.co/fonts/Atlas/AtlasGrotesk-Bold-Web.woff) format(\"woff\");font-style:normal;font-weight:700;font-display:swap;}/*!sc*/\n@font-face{font-family:'Atlas Typewriter';src:url(https://assets.aeonmedia.co/fonts/Atlas/AtlasTypewriter-Regular-Web.eot);src:url(https://assets.aeonmedia.co/fonts/Atlas/AtlasTypewriter-Regular-Web.eot?#iefix) format(\"embedded-opentype\"),url(https://assets.aeonmedia.co/fonts/Atlas/AtlasTypewriter-Regular-Web.woff2) format(\"woff2\"),url(https://assets.aeonmedia.co/fonts/Atlas/AtlasTypewriter-Regular-Web.woff) format(\"woff\");font-style:normal;font-weight:400;font-display:swap;}/*!sc*/\n@font-face{font-family:'Academica Book Pro';src:url(https://assets.aeonmedia.co/fonts/Academica/AcademicaBook.eot);src:url(https://assets.aeonmedia.co/fonts/Academica/AcademicaBook.eot?#iefix) format(\"embedded-opentype\"),url(https://assets.aeonmedia.co/fonts/Academica/AcademicaBook.woff) format(\"woff\");font-style:normal;font-weight:400;font-display:swap;}/*!sc*/\n@font-face{font-family:'Academica Book Pro';src:url(https://assets.aeonmedia.co/fonts/Academica/AcademicaBook-Bold.eot);src:url(https://assets.aeonmedia.co/fonts/Academica/AcademicaBook-Bold.eot?#iefix) format(\"embedded-opentype\"),url(https://assets.aeonmedia.co/fonts/Academica/AcademicaBook-Bold.woff) format(\"woff\");font-style:normal;font-weight:700;font-display:swap;}/*!sc*/\n@font-face{font-family:'Academica Book Pro';src:url(https://assets.aeonmedia.co/fonts/Academica/AcademicaBook-BoldItalic.eot);src:url(https://assets.aeonmedia.co/fonts/Academica/AcademicaBook-BoldItalic.eot?#iefix) format(\"embedded-opentype\"),url(https://assets.aeonmedia.co/fonts/Academica/AcademicaBook-BoldItalic.woff) format(\"woff\");font-style:italic;font-weight:700;font-display:swap;}/*!sc*/\n@font-face{font-family:'Academica Book Pro';src:url(https://assets.aeonmedia.co/fonts/Academica/AcademicaBook-Italic.eot);src:url(https://assets.aeonmedia.co/fonts/Academica/AcademicaBook-Italic.eot?#iefix) format(\"embedded-opentype\"),url(https://assets.aeonmedia.co/fonts/Academica/AcademicaBook-Italic.woff) format(\"woff\");font-style:italic;font-weight:400;font-display:swap;}/*!sc*/\n*{box-sizing:border-box;}/*!sc*/\n:root{font-size:10px;}/*!sc*/\n@media only screen and (max-width:640px){:root{font-size:9px;}}/*!sc*/\nbody{font-family:\"Atlas Grotesk\",\"Helvetica Neue\",Helvetica,Arial,sans-serif;line-height:1.4;margin:0;min-height:100vh;overflow-x:hidden;width:100vw;}/*!sc*/\nbody.no-scroll,body.open--newsletter-terms{overflow:hidden;}/*!sc*/\na{text-decoration:none;color:inherit;}/*!sc*/\ninput{font:inherit;}/*!sc*/\ninput::placeholder{color:inherit;}/*!sc*/\np{margin-bottom:1rem;}/*!sc*/\n@media print{body{width:100%;}}/*!sc*/\ndata-styled.g106[id=\"sc-global-cwtVrK1\"]{content:\"sc-global-cwtVrK1,\"}/*!sc*/\n.gakeCP{width:auto;order:3;margin-right:3rem;}/*!sc*/\n@media (max-width:960px){.gakeCP{margin-bottom:2.4rem;}}/*!sc*/\n@media (max-width:960px){.gakeCP{order:4;}}/*!sc*/\n.gakeCP .newsletter-signup div{background:inherit;padding:0;}/*!sc*/\n.gakeCP .newsletter-signup form{margin-left:0;}/*!sc*/\n.gakeCP .newsletter-signup h3{color:rgba(0,0,0,0.65);font-size:2rem;margin-top:0;}/*!sc*/\n.gakeCP .newsletter-signup h4{margin-top:0;color:rgba(0,0,0,0.65);font-size:1.4rem;margin-bottom:1.4rem;font-weight:normal;}/*!sc*/\n.gakeCP .newsletter-signup label{color:rgba(0,0,0,0.65);}/*!sc*/\n@media (min-width:960px){.gakeCP .newsletter-signup .frequency h3{margin-left:-0.5rem;}}/*!sc*/\n@media (min-width:960px){.gakeCP .newsletter-signup .submit{margin-right:2rem;margin-top:0.4rem;}}/*!sc*/\n.gakeCP .newsletter-signup .submit input{padding-bottom:0.95rem;padding-top:0.95rem;}/*!sc*/\n.gakeCP .newsletter-signup input[type='checkbox']:checked{background:rgba(0,0,0,0.65);}/*!sc*/\n@media (min-width:960px){.gakeCP .newsletter-signup .email{margin-right:2rem;}}/*!sc*/\n.gakeCP .newsletter-signup .email input{background:inherit;border:1px solid rgba(0,0,0,0.65);max-width:28rem;color:#444;padding:1.2rem;padding-bottom:1rem;}/*!sc*/\n.gakeCP .newsletter-signup .email input::placeholder{color:rgba(0,0,0,0.65);}/*!sc*/\n.gakeCP .newsletter-signup .submit~h3{margin-top:2.2rem;}/*!sc*/\ndata-styled.g107[id=\"sc-f1ffec09-0\"]{content:\"gakeCP,\"}/*!sc*/\n.hOpepC{background:rgba(0,0,0,0.03);color:rgba(0,0,0,0.65);font-size:1.4rem;line-height:1.6;margin-top:auto;overflow:hidden;padding-bottom:3.6rem;padding-top:3.6rem;position:relative;z-index:23;}/*!sc*/\n@media print{.hOpepC{display:none;}}/*!sc*/\ndata-styled.g108[id=\"sc-4c328164-0\"]{content:\"hOpepC,\"}/*!sc*/\n.kkIxMa{margin-bottom:4.8rem;}/*!sc*/\n@media (max-width:960px){.kkIxMa{flex-wrap:wrap;}}/*!sc*/\ndata-styled.g109[id=\"sc-4c328164-1\"]{content:\"kkIxMa,\"}/*!sc*/\n.ikAbKy{flex-direction:column;gap:0.5em;}/*!sc*/\ndata-styled.g110[id=\"sc-4c328164-2\"]{content:\"ikAbKy,\"}/*!sc*/\n.bAYwLt{color:rgba(0,0,0,0.6);font-size:1.2rem;}/*!sc*/\ndata-styled.g111[id=\"sc-4c328164-3\"]{content:\"bAYwLt,\"}/*!sc*/\n.dnIlCA{position:relative;flex-grow:1;width:100%;margin:0 auto;}/*!sc*/\n@media only print{.dnIlCA{width:21cm;}}/*!sc*/\ndata-styled.g117[id=\"sc-a70232b9-0\"]{content:\"dnIlCA,\"}/*!sc*/\n.bspASq{background-color:#fff;display:flex;flex-direction:column;min-height:100vh;position:relative;}/*!sc*/\ndata-styled.g118[id=\"sc-a70232b9-1\"]{content:\"bspASq,\"}/*!sc*/\n.kNEmjD{flex-grow:0;}/*!sc*/\ndata-styled.g119[id=\"sc-a70232b9-2\"]{content:\"kNEmjD,\"}/*!sc*/\n.jzMtbm{flex-grow:0;}/*!sc*/\ndata-styled.g120[id=\"sc-a70232b9-3\"]{content:\"jzMtbm,\"}/*!sc*/\n.kQGdcj{z-index:500;}/*!sc*/\n.ljOOuR{z-index:500;position:fixed;left:0;bottom:0;width:100%;box-shadow:0 0 30px #00000040;}/*!sc*/\ndata-styled.g122[id=\"sc-a70232b9-5\"]{content:\"kQGdcj,ljOOuR,\"}/*!sc*/\n.dAOxdm{height:5rem;max-width:30rem;background-color:#0c776d20;color:#0c776d;text-align:center;display:flex;align-items:center;justify-content:center;font-family:\"Atlas Grotesk\",\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:normal;position:relative;margin-bottom:40px;}/*!sc*/\n.dAOxdm:hover{background-color:#0c776d;color:white;cursor:pointer;}/*!sc*/\n.dAOxdm:hover:after{color:white;cursor:pointer;border-top:25px solid #0c776d;}/*!sc*/\n.dAOxdm:after{content:'';height:40px;width:40px;position:absolute;bottom:-25px;left:15px;width:0px;height:0px;border-left:0 solid transparent;border-right:20px solid transparent;border-top:25px solid #0c776d20;}/*!sc*/\n@media only screen and (max-width:768px){.dAOxdm{max-width:18rem;margin-top:2rem;}}/*!sc*/\ndata-styled.g220[id=\"sc-b6dddc87-0\"]{content:\"dAOxdm,\"}/*!sc*/\n.bkhChg{font-family:\"Atlas Grotesk\",\"Helvetica Neue\",Helvetica,Arial,sans-serif;color:#444;cursor:pointer;display:flex;flex-wrap:wrap;max-width:30rem;}/*!sc*/\n.bkhChg div{align-items:center;justify-content:center;box-sizing:border-box;display:flex;flex:0 0 50%;font:inherit;margin:1.3rem 0;}/*!sc*/\n.bkhChg div svg,.bkhChg div span{font-weight:normal;transition:all 0.5s cubic-bezier(0.19,1,0.22,1);}/*!sc*/\n.bkhChg div:hover svg,.bkhChg div:hover span{color:#0c776d;transition:all 0.5s cubic-bezier(0.19,1,0.22,1);}/*!sc*/\n@media only screen and (max-width:767px){.bkhChg div{justify-content:flex-start;}}/*!sc*/\n@media print{.bkhChg{display:none;}}/*!sc*/\ndata-styled.g221[id=\"sc-e2a1906f-0\"]{content:\"bkhChg,\"}/*!sc*/\n.bXtNKb{margin-right:10px;vertical-align:middle;}/*!sc*/\ndata-styled.g222[id=\"sc-e2a1906f-1\"]{content:\"bXtNKb,\"}/*!sc*/\n.hJXeEE{line-height:1;top:0.2rem;position:relative;}/*!sc*/\ndata-styled.g223[id=\"sc-e2a1906f-2\"]{content:\"hJXeEE,\"}/*!sc*/\n.cziXAF{flex-direction:column;font-size:1.6rem;font-weight:bold;position:sticky;display:none;}/*!sc*/\n@media only screen and (min-width:640px){.cziXAF{display:flex;gap:1rem;}}/*!sc*/\n@media print{.cziXAF{display:none;}}/*!sc*/\ndata-styled.g224[id=\"sc-c3e98e6e-0\"]{content:\"cziXAF,\"}/*!sc*/\n.jmDajv{display:flex;font-size:1.6rem;color:#444;}/*!sc*/\n.jmDajv >span{padding-right:1.4rem;}/*!sc*/\n@media only screen and (min-width:640px){.jmDajv{display:flex;}.jmDajv >span{padding-right:2.4rem;}}/*!sc*/\n@media print{.jmDajv{display:none;}}/*!sc*/\ndata-styled.g238[id=\"sc-95e65737-0\"]{content:\"jmDajv,\"}/*!sc*/\n.cIhWyi{display:inline-flex;column-gap:0.4rem;text-decoration:underline;}/*!sc*/\n@media only screen and (min-width:640px){.cIhWyi{display:none;}}/*!sc*/\ndata-styled.g239[id=\"sc-95e65737-1\"]{content:\"cIhWyi,\"}/*!sc*/\n.gjbGGR{display:inline-flex;column-gap:0.4rem;}/*!sc*/\n@media only screen and (min-width:640px){.gjbGGR{display:none;}}/*!sc*/\ndata-styled.g240[id=\"sc-95e65737-2\"]{content:\"gjbGGR,\"}/*!sc*/\n.jdutDe{padding-bottom:20px;color:#444;font-size:1.6rem;line-height:1.6;}/*!sc*/\n.jdutDe a{text-decoration:underline;}/*!sc*/\n.jdutDe p:first-of-type{display:inline;margin-top:0;}/*!sc*/\n.jdutDe p:last-of-type{margin-bottom:0;}/*!sc*/\ndata-styled.g252[id=\"sc-2e8621ab-0\"]{content:\"jdutDe,\"}/*!sc*/\n.gnJVZv{display:inline;margin-right:0.7ex;color:#444;font-size:1.6rem;line-height:1.6;}/*!sc*/\ndata-styled.g253[id=\"sc-2e8621ab-1\"]{content:\"gnJVZv,\"}/*!sc*/\n.cKyFvK{font-family:\"Academica Book Pro\",Times,Georgia,serif;font-size:2.2rem;line-height:1.4;color:#000;}/*!sc*/\n@media only screen and (min-width:640px){.cKyFvK{padding-right:3rem;}}/*!sc*/\n.cKyFvK a{border-bottom:1px solid;}/*!sc*/\n.cKyFvK a:hover{opacity:0.8;}/*!sc*/\n.cKyFvK .pullquote{color:#0c776d;display:block;font-size:3.2rem;line-height:1.3;margin-left:0;margin-top:2rem;padding-bottom:0.5rem;padding-left:6rem;padding-top:0.75rem;}/*!sc*/\n.cKyFvK .pullquote:first-letter{text-transform:uppercase;}/*!sc*/\n@media only screen and (max-width:640px){.cKyFvK .pullquote{padding-left:0;}}/*!sc*/\n@media print{.cKyFvK .pullquote{display:none;}}/*!sc*/\n.cKyFvK ol,.cKyFvK ul{counter-reset:item;padding-left:0.1em;}/*!sc*/\n.cKyFvK li{counter-increment:item;list-style-type:none;padding-left:1.9em;margin-bottom:1em;position:relative;}/*!sc*/\n.cKyFvK li:before{display:inline-block;width:1.65em;left:0;position:absolute;font-weight:bold;text-align:left;content:counter(item) '.';}/*!sc*/\n.cKyFvK ul li:before{content:\"•\";}/*!sc*/\n.cKyFvK blockquote{margin-left:2em;}/*!sc*/\n.cKyFvK code{font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;font-size:85%;background-color:#afb8c133;padding:0.2rem 0.4rem;border-radius:6px;white-space:break-spaces;}/*!sc*/\n@media not print{.cKyFvK .ld-dropcap,.cKyFvK .has-dropcap>p:first-of-type:first-letter{float:left;-webkit-font-feature-settings:'kern' 0;-webkit-font-kerning:none;-webkit-font-smoothing:antialiased;font-feature-settings:'kern' 0;font-kerning:none;font-size:3.25em;line-height:0.8;margin-bottom:-0.05em;margin-left:-0.05em;padding:0.05em 0.075em 0 0;position:relative;top:0.06em;background:inherit;}}/*!sc*/\n@media print{.cKyFvK .ld-dropcap,.cKyFvK .has-dropcap>p:first-of-type:first-letter{font-weight:bold;font-size:120%;margin-right:2px;}}/*!sc*/\n.cKyFvK .ld-image-block{max-width:100%;}/*!sc*/\n.cKyFvK .ld-image-gallery{display:none;}/*!sc*/\n.cKyFvK figure{margin:3rem auto;}/*!sc*/\n@media print{.cKyFvK figure{page-break-inside:avoid;}}/*!sc*/\n.cKyFvK figure iframe{aspect-ratio:16/9;width:100%;height:auto;}/*!sc*/\n@media not all and (min-resolution:0.001dpcm){@media{.cKyFvK figure iframe{min-height:420px;}}}/*!sc*/\n@media only screen and (max-width:1280px){@media not all and (min-resolution:0.001dpcm){@media{.cKyFvK figure iframe{min-height:260px;}}}}/*!sc*/\n@media only screen and (max-width:768px){@media not all and (min-resolution:0.001dpcm){@media{.cKyFvK figure iframe{min-height:200px;}}}}/*!sc*/\n@media only screen and (max-width:640px){@media not all and (min-resolution:0.001dpcm){@media{.cKyFvK figure iframe{min-height:260px;}}}}/*!sc*/\n@media only screen and (max-width:480px){@media not all and (min-resolution:0.001dpcm){@media{.cKyFvK figure iframe{min-height:160px;}}}}/*!sc*/\n.cKyFvK figure img{display:block;margin-left:auto;margin-right:auto;margin-top:0.5rem;max-width:100%;height:auto;width:auto;}/*!sc*/\n@media print{.cKyFvK figure img{max-width:10cm;max-height:10cm;}}/*!sc*/\n@media only screen and (max-width:480px){.cKyFvK figure .ld-embed-block{width:100%;height:100%;}}/*!sc*/\n.cKyFvK figcaption{grid-area:attribution;font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;color:#666;font-size:1.3rem;margin-top:2rem;position:relative;}/*!sc*/\n.cKyFvK figcaption:empty{display:none;}/*!sc*/\n.cKyFvK figcaption p{margin:0;}/*!sc*/\n.cKyFvK p{margin-bottom:2rem;overflow-wrap:anywhere;word-break:break-word;}/*!sc*/\n.cKyFvK p:first-of-type{margin-top:0;}/*!sc*/\n.cKyFvK p:last-of-type{margin-bottom:0;}/*!sc*/\n.cKyFvK img,.cKyFvK video{max-width:100%;}/*!sc*/\n.cKyFvK .ld-sans{font-family:'Atlas Grotesk','Helvetica Neue',Helvetica,Arial,sans-serif;}/*!sc*/\n.cKyFvK .subheading{font-size:2.6rem;padding-left:0;padding-top:0;padding-bottom:0;}/*!sc*/\n.cKyFvK .ld-superscript{position:relative;top:-0.4em;vertical-align:baseline;}/*!sc*/\n.cKyFvK .ld-subscript{position:relative;top:0.4em;vertical-align:baseline;}/*!sc*/\n.cKyFvK .ld-nowrap{white-space:nowrap;}/*!sc*/\n.cKyFvK .text-right{text-align:right;}/*!sc*/\n.cKyFvK .text-center{text-align:center;}/*!sc*/\n.cKyFvK .text-justify{text-align:justify;}/*!sc*/\n.cKyFvK .right-to-left{direction:rtl;}/*!sc*/\ndata-styled.g267[id=\"sc-8824d2be-1\"]{content:\"cKyFvK,\"}/*!sc*/\n.gDwTPy .page-mark{cursor:pointer;transition:background 0.2s linear,outline 0.2s linear;}/*!sc*/\n.gDwTPy .page-mark.mark-type-highlight{background:rgba(255, 125, 0, 0.5);}/*!sc*/\n.gDwTPy .page-mark.mark-type-annotation{background:rgba(255, 0, 125, 0.3);}/*!sc*/\n.gDwTPy .page-mark[data-annotation-number]::after{display:inline-block;content:attr(data-annotation-number);font-size:50%;font-weight:bold;transform:translateY(-75%);color:#9d1d20;margin-right:-1ex;}/*!sc*/\n.gDwTPy img.page-mark.mark-type-highlight,.gDwTPy iframe.page-mark.mark-type-highlight{outline:1rem solid rgba(255, 125, 0, 0.5);}/*!sc*/\n.gDwTPy img.page-mark.mark-type-annotation,.gDwTPy iframe.page-mark.mark-type-annotation{outline:1rem solid rgba(255, 0, 125, 0.3);}/*!sc*/\ndata-styled.g269[id=\"sc-8824d2be-3\"]{content:\"gDwTPy,\"}/*!sc*/\n.bhtfvC{background:#0c776d;border:none;color:white;cursor:pointer;display:inline-flex;flex-direction:column;font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;font-size:1.2rem;height:min-content;justify-content:center;letter-spacing:0.15rem;line-height:1.6;max-width:25rem;padding:1rem 2.4rem 0.8rem;position:relative;text-align:center;text-transform:uppercase;z-index:8;margin:0;background-color:#ececec;color:#666;}/*!sc*/\n.bhtfvC:hover{background-color:#0c776dc9;}/*!sc*/\n.bhtfvC:hover{background-color:#000;color:#fff;}/*!sc*/\ndata-styled.g271[id=\"sc-45649ee9-0\"]{content:\"bhtfvC,\"}/*!sc*/\n.cacrwE{visibility:visible;display:flex;align-items:center;}/*!sc*/\ndata-styled.g272[id=\"sc-45649ee9-1\"]{content:\"cacrwE,\"}/*!sc*/\n.kmtmVW{padding:2rem;}/*!sc*/\n.kmtmVW span{justify-content:center;}/*!sc*/\ndata-styled.g274[id=\"sc-dbbed3aa-0\"]{content:\"kmtmVW,\"}/*!sc*/\n.eiFojC{grid-area:syndication;margin:2rem 0;}/*!sc*/\n@media only print{.eiFojC{display:none;}}/*!sc*/\n@media (max-width:640px){.eiFojC{display:none;}}/*!sc*/\n.gROkhr{grid-area:syndication;margin:2rem 0;}/*!sc*/\n@media only print{.gROkhr{display:none;}}/*!sc*/\ndata-styled.g275[id=\"sc-dbbed3aa-1\"]{content:\"eiFojC,gROkhr,\"}/*!sc*/\n.eTKecW{flex-grow:0;flex-shrink:0;width:100%;background-color:#0c776d;display:block;overflow:hidden;position:relative;padding-top:55.56%;}/*!sc*/\n.eTKecW img{display:block;object-fit:cover;object-position:50% 50%;height:100%;width:100%;position:absolute;top:0;}/*!sc*/\n@media only print{.eTKecW{width:calc(21cm / 3);}}/*!sc*/\n.fycPaJ{flex-grow:0;flex-shrink:0;width:100%;background-color:#9d120d;display:block;overflow:hidden;position:relative;padding-top:55.56%;}/*!sc*/\n.fycPaJ img{display:block;object-fit:cover;object-position:50% 50%;height:100%;width:100%;position:absolute;top:0;}/*!sc*/\n@media only print{.fycPaJ{width:calc(21cm / 3);}}/*!sc*/\n.kQFYIz{flex-grow:0;flex-shrink:0;width:100%;background-color:#940b52;display:block;overflow:hidden;position:relative;padding-top:55.56%;}/*!sc*/\n.kQFYIz img{display:block;object-fit:cover;object-position:50% 50%;height:100%;width:100%;position:absolute;top:0;}/*!sc*/\n@media only print{.kQFYIz{width:calc(21cm / 3);}}/*!sc*/\n.gnJbce{flex-grow:0;flex-shrink:0;width:100%;background-color:#c16e15;display:block;overflow:hidden;position:relative;padding-top:55.56%;}/*!sc*/\n.gnJbce img{display:block;object-fit:cover;object-position:50% 50%;height:100%;width:100%;position:absolute;top:0;}/*!sc*/\n@media only print{.gnJbce{width:calc(21cm / 3);}}/*!sc*/\n.faKwuS{flex-grow:0;flex-shrink:0;width:100%;background-color:#035a6d;display:block;overflow:hidden;position:relative;padding-top:55.56%;}/*!sc*/\n.faKwuS img{display:block;object-fit:cover;object-position:50% 50%;height:100%;width:100%;position:absolute;top:0;}/*!sc*/\n@media only print{.faKwuS{width:calc(21cm / 3);}}/*!sc*/\ndata-styled.g278[id=\"sc-4c9ef492-2\"]{content:\"eTKecW,fycPaJ,kQFYIz,gnJbce,faKwuS,\"}/*!sc*/\n.fuyVND{transition:color 0.2s ease;}/*!sc*/\ndata-styled.g280[id=\"sc-59baed0b-0\"]{content:\"fuyVND,\"}/*!sc*/\n.HnFfS{box-sizing:border-box;position:relative;z-index:1;display:grid;grid-template-areas:'topic-wrap topic-wrap' 'title title' 'standfirst standfirst' 'author author';align-content:center;padding-top:24px;transition:all 0.3s cubic-bezier(.645,.045,.355,1);width:100%;margin:0;}/*!sc*/\n@media only screen and (max-width:640px){.HnFfS{padding-left:2.4rem;padding-right:2.4rem;}}/*!sc*/\n@media only print{.HnFfS{padding-top:2rem;padding-right:2rem;width:100%;}}/*!sc*/\ndata-styled.g281[id=\"sc-59baed0b-1\"]{content:\"HnFfS,\"}/*!sc*/\n.ePEXnI{font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;font-size:1.35rem;margin:0;display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;overflow:hidden;text-overflow:ellipsis;}/*!sc*/\ndata-styled.g282[id=\"sc-59baed0b-2\"]{content:\"ePEXnI,\"}/*!sc*/\n.cAOLoR{font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;font-size:1.35rem;justify-self:end;margin:0 0.6rem 0 0;text-transform:capitalize;transition:all 0.2s ease;overflow:hidden;max-width:5.5rem;padding-right:2rem;position:relative;display:inline-flex;}/*!sc*/\n.cAOLoR:after{display:block;content:' / ';opacity:0.6;height:100%;padding-left:0.4rem;}/*!sc*/\ndata-styled.g283[id=\"sc-59baed0b-3\"]{content:\"cAOLoR,\"}/*!sc*/\n.hfpWCv{color:#444;top:0.2rem;position:relative;font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;font-size:1.35rem;margin:0;display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;overflow:hidden;text-overflow:ellipsis;}/*!sc*/\ndata-styled.g284[id=\"sc-59baed0b-4\"]{content:\"hfpWCv,\"}/*!sc*/\n.efwOzb{display:flex;align-items:center;padding:2rem 2.2rem 2.2rem 2rem;margin:-2rem;}/*!sc*/\n.efwOzb .sc-59baed0b-4{display:none;}/*!sc*/\n.efwOzb:hover .sc-59baed0b-4{display:inline;color:#0c776d;}/*!sc*/\n.efwOzb:hover ~p{display:none;}/*!sc*/\n.dVkVdY{display:flex;align-items:center;padding:2rem 2.2rem 2.2rem 2rem;margin:-2rem;}/*!sc*/\n.dVkVdY .sc-59baed0b-4{display:none;}/*!sc*/\n.dVkVdY:hover .sc-59baed0b-4{display:inline;color:#9d120d;}/*!sc*/\n.dVkVdY:hover ~p{display:none;}/*!sc*/\n.bgoMLi{display:flex;align-items:center;padding:2rem 2.2rem 2.2rem 2rem;margin:-2rem;}/*!sc*/\n.bgoMLi .sc-59baed0b-4{display:none;}/*!sc*/\n.bgoMLi:hover .sc-59baed0b-4{display:inline;color:#940b52;}/*!sc*/\n.bgoMLi:hover ~p{display:none;}/*!sc*/\n.hXIPur{display:flex;align-items:center;padding:2rem 2.2rem 2.2rem 2rem;margin:-2rem;}/*!sc*/\n.hXIPur .sc-59baed0b-4{display:none;}/*!sc*/\n.hXIPur:hover .sc-59baed0b-4{display:inline;color:#c16e15;}/*!sc*/\n.hXIPur:hover ~p{display:none;}/*!sc*/\n.enJAtv{display:flex;align-items:center;padding:2rem 2.2rem 2.2rem 2rem;margin:-2rem;}/*!sc*/\n.enJAtv .sc-59baed0b-4{display:none;}/*!sc*/\n.enJAtv:hover .sc-59baed0b-4{display:inline;color:#035a6d;}/*!sc*/\n.enJAtv:hover ~p{display:none;}/*!sc*/\ndata-styled.g285[id=\"sc-59baed0b-5\"]{content:\"efwOzb,dVkVdY,bgoMLi,hXIPur,enJAtv,\"}/*!sc*/\n.dtyrwz{display:flex;flex-direction:column;position:absolute;top:0;right:0;}/*!sc*/\ndata-styled.g286[id=\"sc-59baed0b-6\"]{content:\"dtyrwz,\"}/*!sc*/\n.dBlrnI{width:2.6rem;height:2.6rem;background:white;display:flex;align-items:center;justify-content:center;border-radius:50%;margin:0.6rem 0.6rem 0 0;}/*!sc*/\ndata-styled.g287[id=\"sc-59baed0b-7\"]{content:\"dBlrnI,\"}/*!sc*/\n.bIyzxX{grid-area:topic-wrap;display:flex;align-items:center;font-size:1.4rem;}/*!sc*/\ndata-styled.g288[id=\"sc-59baed0b-8\"]{content:\"bIyzxX,\"}/*!sc*/\n.emuUeA{display:flex;flex-direction:column;position:relative;cursor:pointer;}/*!sc*/\n.emuUeA img{transition:all 0.2s ease;}/*!sc*/\n.emuUeA:hover .sc-59baed0b-0{color:#0c776d;}/*!sc*/\n.emuUeA:hover .sc-59baed0b-3{max-width:0;margin:0;padding-right:0;}/*!sc*/\n.emuUeA:hover img{mix-blend-mode:screen;filter:grayscale(100%);}/*!sc*/\n.emuUeA:hover svg{color:#0c776d;}/*!sc*/\n.nvsYc{display:flex;flex-direction:column;position:relative;cursor:pointer;}/*!sc*/\n.nvsYc img{transition:all 0.2s ease;}/*!sc*/\n.nvsYc:hover .sc-59baed0b-0{color:#9d120d;}/*!sc*/\n.nvsYc:hover .sc-59baed0b-3{max-width:0;margin:0;padding-right:0;}/*!sc*/\n.nvsYc:hover img{mix-blend-mode:screen;filter:grayscale(100%);}/*!sc*/\n.nvsYc:hover svg{color:#9d120d;}/*!sc*/\n.koxOCA{display:flex;flex-direction:column;position:relative;cursor:pointer;}/*!sc*/\n.koxOCA img{transition:all 0.2s ease;}/*!sc*/\n.koxOCA:hover .sc-59baed0b-0{color:#940b52;}/*!sc*/\n.koxOCA:hover .sc-59baed0b-3{max-width:0;margin:0;padding-right:0;}/*!sc*/\n.koxOCA:hover img{mix-blend-mode:screen;filter:grayscale(100%);}/*!sc*/\n.koxOCA:hover svg{color:#940b52;}/*!sc*/\n.reA-do{display:flex;flex-direction:column;position:relative;cursor:pointer;}/*!sc*/\n.reA-do img{transition:all 0.2s ease;}/*!sc*/\n.reA-do:hover .sc-59baed0b-0{color:#c16e15;}/*!sc*/\n.reA-do:hover .sc-59baed0b-3{max-width:0;margin:0;padding-right:0;}/*!sc*/\n.reA-do:hover img{mix-blend-mode:screen;filter:grayscale(100%);}/*!sc*/\n.reA-do:hover svg{color:#c16e15;}/*!sc*/\n.fNUksc{display:flex;flex-direction:column;position:relative;cursor:pointer;}/*!sc*/\n.fNUksc img{transition:all 0.2s ease;}/*!sc*/\n.fNUksc:hover .sc-59baed0b-0{color:#035a6d;}/*!sc*/\n.fNUksc:hover .sc-59baed0b-3{max-width:0;margin:0;padding-right:0;}/*!sc*/\n.fNUksc:hover img{mix-blend-mode:screen;filter:grayscale(100%);}/*!sc*/\n.fNUksc:hover svg{color:#035a6d;}/*!sc*/\ndata-styled.g289[id=\"sc-59baed0b-9\"]{content:\"emuUeA,nvsYc,koxOCA,reA-do,fNUksc,\"}/*!sc*/\n.gqsGJu{grid-area:title;font-family:\"Academica Book Pro\",Times,Georgia,serif;font-size:2.8rem;font-weight:700;line-height:1.2;margin:1.4rem 0 1.2rem 0;}/*!sc*/\ndata-styled.g290[id=\"sc-59baed0b-10\"]{content:\"gqsGJu,\"}/*!sc*/\n.llxLtL{grid-area:standfirst;font-family:\"Atlas Grotesk\",\"Helvetica Neue\",Helvetica,Arial,sans-serif;margin:0 0 14px;font-size:1.6rem;letter-spacing:0;line-height:1.5;}/*!sc*/\ndata-styled.g291[id=\"sc-59baed0b-11\"]{content:\"llxLtL,\"}/*!sc*/\n.cWJVNA{grid-area:author;margin:0;font-size:1.6rem;letter-spacing:0;opacity:0.65;}/*!sc*/\n.cWJVNA:hover{opacity:1;}/*!sc*/\ndata-styled.g292[id=\"sc-59baed0b-12\"]{content:\"cWJVNA,\"}/*!sc*/\n.cFULRC{display:grid;grid-template-columns:1fr 1fr;grid-row-gap:0;grid-column-gap:2.4rem;grid-row-gap:3.6rem;padding:2.4rem;}/*!sc*/\n@media only screen and (min-width:960px){.cFULRC{grid-template-columns:1fr 1fr 1fr;grid-column-gap:3.6rem;grid-row-gap:5.2rem;padding:3.6rem;}}/*!sc*/\n@media only screen and (max-width:640px){.cFULRC{grid-template-columns:1fr;grid-column-gap:0;grid-row-gap:6rem;padding:0;}}/*!sc*/\ndata-styled.g294[id=\"sc-c83e4c92-0\"]{content:\"cFULRC,\"}/*!sc*/\n.bMtwqF{margin:0 auto;max-width:120rem;}/*!sc*/\n@media (min-width:1280px){.bMtwqF{max-width:140rem;}}/*!sc*/\ndata-styled.g295[id=\"sc-d9870e8f-0\"]{content:\"bMtwqF,\"}/*!sc*/\n.uZAaZ{margin:0 auto;max-width:1700px;}/*!sc*/\ndata-styled.g296[id=\"sc-d9870e8f-1\"]{content:\"uZAaZ,\"}/*!sc*/\n.gKeKzI{display:grid;grid-template-areas:'header' 'aside' 'main' 'social' 'appendix';padding:0 2.4rem;}/*!sc*/\n@media only screen and (min-width:640px){.gKeKzI{grid-template-areas:\"header header\" \"aside main\" \"aside appendix\";grid-column-gap:48px;grid-template-columns:170px 1fr;min-height:400px;padding-bottom:30px;position:relative;}}/*!sc*/\n@media only screen and (min-width:768px){.gKeKzI{grid-template-columns:calc(100% * .9 / 3) 1fr;grid-column-gap:32px;grid-template-columns:calc(100% * .9 / 3) 1fr;}}/*!sc*/\n@media only screen and (min-width:1280px){.gKeKzI{grid-column-gap:36px;}}/*!sc*/\n@media print{.gKeKzI{display:block;}}/*!sc*/\n@media print,screen and (min-width:640px){.gKeKzI .SidebarWrapper{display:block;height:auto!important;}.gKeKzI .SidebarWrapper >div{display:block!important;}}/*!sc*/\n.gKeKzI .editorial-info-wrapper{margin-top:5rem;}/*!sc*/\n@media only print{.gKeKzI .editorial-info-wrapper{display:none;}}/*!sc*/\ndata-styled.g300[id=\"sc-d9870e8f-5\"]{content:\"gKeKzI,\"}/*!sc*/\n.CYCPy{align-self:start;width:100%;position:relative;z-index:1;max-width:90%;}/*!sc*/\ndata-styled.g302[id=\"sc-d9870e8f-7\"]{content:\"CYCPy,\"}/*!sc*/\n.kICVIU{background:linear-gradient( rgba(0,0,0,0.135) 0,rgba(0,0,0,0.02) 12rem );color:#666;display:flex;align-items:flex-start;font-size:1.3rem;}/*!sc*/\n.kICVIU >svg{max-width:11rem;height:auto;}/*!sc*/\n.kICVIU >div{padding:1.5rem 1rem 1rem 1rem;}/*!sc*/\n.kICVIU >div p{margin:1rem 0 0.6rem 0;}/*!sc*/\n.kICVIU >div a{text-decoration:underline;}/*!sc*/\n.kICVIU >div >div:last-child{margin-top:1rem;}/*!sc*/\n@media (max-width:640px){.kICVIU img{height:50px;width:auto;}}/*!sc*/\n@media (min-width:640px) and (max-width:860px){.kICVIU{flex-direction:column;}}/*!sc*/\n@media print{.kICVIU{display:none;}}/*!sc*/\ndata-styled.g303[id=\"sc-d9870e8f-8\"]{content:\"kICVIU,\"}/*!sc*/\n.fVeAYS p{color:#444;font-size:1.6rem;line-height:1.6;margin:0 0 20px 0;position:relative;z-index:1;}/*!sc*/\n.fVeAYS p a{margin-left:0.7ex;text-decoration:underline;}/*!sc*/\n@media only screen and (min-width:640px){.fVeAYS p{margin-bottom:15px;}}/*!sc*/\ndata-styled.g304[id=\"sc-d9870e8f-9\"]{content:\"fVeAYS,\"}/*!sc*/\n.ceECyG{grid-area:main;-moz-osx-font-smoothing:grayscale;font-family:\"Academica Book Pro\",Times,Georgia,serif;font-size:2rem;margin:2rem 0 4rem;}/*!sc*/\n@media only screen and (min-width:768px){.ceECyG{font-size:2.2rem;padding-bottom:0;margin-top:0;}}/*!sc*/\n@media only screen and (min-width:1280px){.ceECyG{padding-bottom:0;}}/*!sc*/\n@page{margin:1.9cm;}/*!sc*/\n@media print{.ceECyG p{padding:0.4cm;}}/*!sc*/\ndata-styled.g305[id=\"sc-d9870e8f-10\"]{content:\"ceECyG,\"}/*!sc*/\n.eWSurm{grid-area:aside;display:flex;flex-direction:column;justify-content:flex-start;margin-bottom:0;}/*!sc*/\n.eWSurm a:hover{text-decoration:none;}/*!sc*/\n@media (min-width:640px){.eWSurm{margin-bottom:3.2rem;}}/*!sc*/\n@media print{.eWSurm{margin-bottom:0;padding:0.4cm;}}/*!sc*/\ndata-styled.g307[id=\"sc-d9870e8f-12\"]{content:\"eWSurm,\"}/*!sc*/\n.cRzdyJ{grid-area:social;align-self:start;position:sticky;top:0;min-height:100vh;margin-top:2rem;padding-bottom:4rem;z-index:0;display:none;flex-direction:column;justify-content:flex-end;grid-area:social;align-self:start;}/*!sc*/\n@media (max-width:768px){.cRzdyJ{margin-top:0;min-height:0;}}/*!sc*/\n@media (min-width:640px){.cRzdyJ{display:flex;}}/*!sc*/\n@media print{.cRzdyJ{display:none;}}/*!sc*/\ndata-styled.g308[id=\"sc-d9870e8f-13\"]{content:\"cRzdyJ,\"}/*!sc*/\n@media only screen and (min-width:640px){.cSFNDd{display:none;}}/*!sc*/\ndata-styled.g309[id=\"sc-d9870e8f-14\"]{content:\"cSFNDd,\"}/*!sc*/\n@media only screen and (max-width:640px){.dfGLcH{max-width:100%;}.dfGLcH div{flex-basis:25%;margin-right:1.3rem;}}/*!sc*/\ndata-styled.g310[id=\"sc-d9870e8f-15\"]{content:\"dfGLcH,\"}/*!sc*/\n.bipmAa{font-size:1.5rem;font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;display:flex;flex-direction:column;margin-bottom:3rem;color:#999;}/*!sc*/\n@media only screen and (min-width:640px){.bipmAa{margin-top:2rem;}}/*!sc*/\n@media print{.bipmAa{flex-direction:row-reverse;justify-content:space-between;margin-top:3.8rem;}.bipmAa:after{display:inline-block;content:'aeon.co';}}/*!sc*/\ndata-styled.g311[id=\"sc-d9870e8f-16\"]{content:\"bipmAa,\"}/*!sc*/\n.gzvGIX{display:flex;flex-wrap:wrap;max-width:100%;margin-bottom:3rem;text-decoration:underline;}/*!sc*/\n.gzvGIX >a{padding-right:2rem;}/*!sc*/\n@media only screen and (min-width:640px){.gzvGIX{margin-bottom:none;max-width:75%;}}/*!sc*/\n@media print{.gzvGIX{display:none;}}/*!sc*/\ndata-styled.g312[id=\"sc-d9870e8f-17\"]{content:\"gzvGIX,\"}/*!sc*/\n.chrJAL{display:block;margin-top:2rem;font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;font-size:1.5rem;color:#0c776d;border-bottom:1px solid #0c776d;}/*!sc*/\n.eNPUtH{display:block;margin-top:2rem;font-family:\"Atlas Typewriter\",\"Courier New\",Courier,monospace;font-size:1.5rem;color:#9d120d;border-bottom:1px solid #9d120d;}/*!sc*/\ndata-styled.g313[id=\"sc-d9870e8f-18\"]{content:\"chrJAL,eNPUtH,\"}/*!sc*/\n.bCLvsT{width:100%;display:flex;flex-direction:column;justify-content:space-between;}/*!sc*/\ndata-styled.g314[id=\"sc-d9870e8f-19\"]{content:\"bCLvsT,\"}/*!sc*/\n@media only screen and (max-width:640px){.jkIrCU{display:none;}}/*!sc*/\ndata-styled.g315[id=\"sc-d9870e8f-20\"]{content:\"jkIrCU,\"}/*!sc*/\n.imycrC{grid-area:appendix;display:flex;flex-direction:column;row-gap:2.4rem;}/*!sc*/\n.imycrC a{text-decoration:none;border-bottom:none;}/*!sc*/\ndata-styled.g316[id=\"sc-d9870e8f-21\"]{content:\"imycrC,\"}/*!sc*/\n.clHWPp{display:flex;flex-direction:column;row-gap:2rem;}/*!sc*/\n@media only screen and (min-width:640px){.clHWPp{flex-direction:row;justify-content:space-between;}}/*!sc*/\ndata-styled.g317[id=\"sc-d9870e8f-22\"]{content:\"clHWPp,\"}/*!sc*/\n.ktQJNm{height:42px;background-color:white;}/*!sc*/\n@media (min-width:640px){.ktQJNm{display:none;}}/*!sc*/\ndata-styled.g318[id=\"sc-d9870e8f-23\"]{content:\"ktQJNm,\"}/*!sc*/\n@media only print{.cTqnQQ{display:none;}}/*!sc*/\ndata-styled.g319[id=\"sc-d9870e8f-24\"]{content:\"cTqnQQ,\"}/*!sc*/\n.iPkEru{margin-bottom:3rem;margin-top:0rem;}/*!sc*/\n@media (max-width:640px){.iPkEru{margin-top:2rem;margin-bottom:0rem;}}/*!sc*/\ndata-styled.g320[id=\"sc-d9870e8f-25\"]{content:\"iPkEru,\"}/*!sc*/\n.jZhtJo{position:relative;}/*!sc*/\ndata-styled.g321[id=\"sc-d9870e8f-26\"]{content:\"jZhtJo,\"}/*!sc*/\n.kmfslc{user-select:none;color:#000;font-family:\"Atlas Grotesk\",\"Helvetica Neue\",Helvetica,Arial,sans-serif;display:flex;align-items:center;}/*!sc*/\n.kmfslc svg{cursor:pointer;}/*!sc*/\n.kmfslc svg:hover{opacity:0.6;}/*!sc*/\ndata-styled.g331[id=\"sc-a4123286-0\"]{content:\"kmfslc,\"}/*!sc*/\n.EQsqR{align-items:center;cursor:pointer;display:flex;font-size:1.6rem;}/*!sc*/\n.EQsqR:hover{opacity:0.6;}/*!sc*/\n.EQsqR:hover svg{opacity:1;}/*!sc*/\n.EQsqR span{position:relative;top:0.1rem;left:0.5rem;}/*!sc*/\ndata-styled.g334[id=\"sc-a4123286-3\"]{content:\"EQsqR,\"}/*!sc*/\n</style></head><body><noscript><iframe title=\"googletagmanager\" src=\"https://analytics.aeon.co/ns.html?id=GTM-MHVZTPR\" height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe></noscript><div id=\"__next\"><div id=\"top-portal\" class=\"sc-a70232b9-5 kQGdcj\"></div><div class=\"sc-a70232b9-1 bspASq\"><header class=\"sc-fe6897d0-0 fxotvc\"><div class=\"sc-382a1301-2 gYjtbO\"><div class=\"sc-382a1301-3 hAFhbJ\"><div class=\"sc-382a1301-0 jyfgjC\"><div class=\"sc-382a1301-1 bkbYYv\"><div class=\"sc-382a1301-11 dqDhaI\"><span>×</span> CLOSE</div><div class=\"sc-382a1301-14 kJATgS\"><a href=\"https://www.facebook.com/aeonmag\" target=\"_blank\" rel=\"noopener noreferrer\" title=\"Facebook\"><svg viewBox=\"0 0 1024 1024\" fill=\"currentColor\" stroke=\"none\" width=\"18\" height=\"18\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-facebook\" aria-hidden=\"true\"><g><path d=\"M1024,512C1024,229.23,794.77,0,512,0S0,229.23,0,512c0,255.55,187.23,467.37,432,505.78V660H302V512H432V399.2C432,270.88,508.44,200,625.39,200c56,0,114.61,10,114.61,10V336H675.44c-63.6,0-83.44,39.47-83.44,80v96H734L711.3,660H592v357.78C836.77,979.37,1024,767.55,1024,512Z\"></path><path d=\"M711.3,660,734,512H592V416c0-40.49,19.84-80,83.44-80H740V210s-58.59-10-114.61-10C508.44,200,432,270.88,432,399.2V512H302V660H432v357.78a517.58,517.58,0,0,0,160,0V660Z\" fill=\"transparent\"></path></g></svg></a><a href=\"https://www.instagram.com/aeonmag\" target=\"_blank\" rel=\"noopener noreferrer\" title=\"Instagram\"><svg viewBox=\"0 0 24 25\" fill=\"currentColor\" stroke=\"none\" width=\"20\" height=\"20\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-instagram\" aria-hidden=\"true\"><path d=\"M17.216.5c4.456.203 6.887 2.633 7.09 7.09v10.126c-.203 4.456-2.634 6.887-7.09 7.09H7.09C2.633 24.603.203 22.172 0 17.716V7.59C.203 3.133 2.633.703 7.09.5zm-5.063 5.874c-3.444 0-6.28 2.835-6.28 6.279 0 3.443 2.836 6.279 6.28 6.279 3.443 0 6.279-2.836 6.279-6.28 0-3.443-2.836-6.278-6.28-6.278zm0 2.228a4.063 4.063 0 0 1 4.05 4.05 4.063 4.063 0 0 1-4.05 4.052 4.063 4.063 0 0 1-4.051-4.051 4.063 4.063 0 0 1 4.05-4.051zm6.481-3.849c-.81 0-1.418.608-1.418 1.418 0 .81.608 1.418 1.418 1.418.81 0 1.418-.608 1.418-1.418 0-.81-.608-1.418-1.418-1.418z\"></path></svg></a><a href=\"https://twitter.com/aeonmag\" target=\"_blank\" rel=\"noopener noreferrer\" title=\"X (Formerly Twitter)\"><svg viewBox=\"0 0 450 450\" fill=\"currentColor\" stroke=\"none\" width=\"20\" height=\"20\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-twitter\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M225 450C349.264 450 450 349.264 450 225C450 100.736 349.264 0 225 0C100.736 0 0 100.736 0 225C0 349.264 100.736 450 225 450ZM349.373 92L250.332 204.646L358 358H278.785L206.256 254.71L115.461 358H92L195.855 239.888L92 92H171.215L239.878 189.789L325.912 92H349.373ZM159.972 109.311H123.929L289.842 341.539H325.894L159.972 109.311Z\"></path></svg></a><a href=\"https://www.youtube.com/@AeonVideo\" target=\"_blank\" rel=\"noopener noreferrer\" title=\"YouTube\"><svg viewBox=\"0 0 22 20\" fill=\"currentColor\" stroke=\"none\" width=\"22\" height=\"20\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-youtube\" aria-hidden=\"true\"><g><path d=\"M21.5,4.7c-0.3-1-1-1.7-1.9-2C17.9,2.2,11,2.2,11,2.2s-6.9,0-8.6,0.5c-1,0.3-1.7,1-1.9,2C0,6.4,0,10,0,10s0,3.6,0.5,5.3  c0.3,1,1,1.7,1.9,2c1.7,0.5,8.6,0.5,8.6,0.5s6.9,0,8.6-0.5c1-0.3,1.7-1,1.9-2C22,13.6,22,10,22,10S22,6.4,21.5,4.7z M8.8,13.3V6.7  l5.8,3.3L8.8,13.3z\"></path></g></svg></a></div></div><div class=\"sc-382a1301-1 bkbYYv\"><div class=\"sc-382a1301-4 dpKclR\"><a href=\"/philosophy\" class=\"sc-382a1301-5 fxVzSw\">Philosophy</a><a href=\"/science\" class=\"sc-382a1301-5 fGNXsA\">Science</a><a href=\"/psychology\" class=\"sc-382a1301-5 eWXizB\">Psychology</a><a href=\"/society\" class=\"sc-382a1301-5 eSElSD\">Society</a><a href=\"/culture\" class=\"sc-382a1301-5 fIRhhw\">Culture</a></div><div class=\"sc-382a1301-6 hXKHtL\"><div class=\"sc-382a1301-7 kYQdxO\"><a href=\"/essays\" class=\"sc-382a1301-8 bbtlGE\">Essays</a><a href=\"/videos\" class=\"sc-382a1301-8 bbtlGE\">Videos</a><a href=\"/audio\" class=\"sc-382a1301-8 bbtlGE\">Audio</a><br/><br/><a href=\"/popular\" class=\"sc-382a1301-8 sc-382a1301-10 bbtlGE lctIKH\">Popular</a><br/><br/><a href=\"/about\" class=\"sc-382a1301-8 sc-382a1301-10 bbtlGE lctIKH\">About</a></div><div class=\"sc-382a1301-9 hdmxQR\"></div></div></div><div class=\"sc-382a1301-1 bkbYYv\"><div class=\"sc-382a1301-13 hDVXDg\"><a data-ga-select-prompt=\"aeon_menu_donate\" href=\"/donate\" class=\"sc-382a1301-8 sc-382a1301-10 bbtlGE lctIKH\">Donate</a><a href=\"/newsletter\" class=\"sc-382a1301-8 sc-382a1301-10 bbtlGE lctIKH\">Newsletter</a><div class=\"sc-382a1301-12 deYgkm\"><a target=\"_blank\" href=\"https://psyche.co\" style=\"line-height:1\" title=\"Psyche\"><svg viewBox=\"0 0 138 28\" fill=\"#025744\" stroke=\"none\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-psyche\" aria-hidden=\"true\" class=\"sc-382a1301-16 kNqRaZ\"><path stroke=\"none\" d=\"M33.4463639,1.03028697e-13 C35.4638444,1.03028697e-13 37.1784471,0.275980571 38.5905508,0.826994629 C40.0024651,1.37857694 41.1385883,2.17242359 41.999678,3.20739809 C42.7884893,4.15681924 43.3288473,5.27001901 43.620898,6.5468515 L43.6943919,6.89913545 L38.7316665,8.31123919 C38.5161099,7.28933447 38.160006,6.45533141 37.6625969,5.80979827 C37.1646196,5.16426513 36.5531815,4.69394271 35.8269567,4.39769452 C35.100732,4.10201459 34.2534697,3.95389049 33.28517,3.95389049 C31.8590494,3.95389049 30.7363748,4.25013868 29.9160098,4.84149856 C29.0952659,5.43342669 28.685557,6.29394813 28.685557,7.42363112 C28.685557,8.28472081 28.9138044,8.96359135 29.3714359,9.46100039 C29.8286886,9.95897769 30.4942999,10.3688761 31.3684594,10.6916427 C32.2424294,11.0144092 33.3253264,11.3510032 34.6165821,11.7002882 C36.7952565,12.2651297 38.5835424,12.8500494 39.9823869,13.4552367 C41.3806632,14.060424 42.4234038,14.8472622 43.1092828,15.8153725 C43.7951617,16.7838617 44.1380065,18.074928 44.1380065,19.6887608 C44.1380065,22.3250666 43.2503985,24.3688761 41.4753717,25.8213256 C39.7001556,27.2737752 37.1310928,28 33.7691304,28 C30.5679831,28 28.053283,27.3614753 26.2244619,26.0834788 C24.4782265,24.8642962 23.406295,23.1484036 23.0086672,20.9361304 L22.9564503,20.6167147 L27.9593322,19.2449568 C28.3086171,20.8322712 28.9408911,22.0223783 29.8555858,22.8154673 C30.7697122,23.6091245 32.1013137,24.0057637 33.8498221,24.0057637 C35.4094817,24.0057637 36.6268647,23.6829971 37.5012136,23.037464 C38.3749942,22.3919308 38.8123581,21.4374586 38.8123581,20.1729107 C38.8123581,19.3123892 38.60381,18.6465885 38.1869032,18.1756978 C37.7696175,17.7053754 37.1242738,17.3083573 36.2504932,16.9855908 C35.3761443,16.6628242 34.2129345,16.3135393 32.7604849,15.9365994 C31.4152454,15.5873145 30.1783523,15.1971154 29.0486693,14.7665706 C27.9189863,14.336594 26.9306084,13.8321765 26.0833462,13.2535076 C25.2358945,12.6754069 24.5835424,11.9561906 24.1264791,11.0951009 C23.6688477,10.2345794 23.4406002,9.17232888 23.4406002,7.90778098 C23.4406002,5.4870317 24.3143808,3.56425972 26.063268,2.13832853 C27.8112081,0.712965594 30.2723032,1.03028697e-13 33.4463639,1.03028697e-13 Z M80.0217694,0.564803615 C81.703603,0.564803615 83.2661038,0.796081774 84.7104085,1.25863809 C86.0096007,1.67493878 87.2353011,2.29023528 88.3869572,3.1036991 L88.7680995,3.3821921 L84.9416866,7.33494608 L84.7314338,7.33494608 C83.8063211,5.90522656 82.7971073,4.81910947 81.703603,4.07602657 C80.6104775,3.33351193 79.475112,2.96168635 78.2976959,2.96168635 C77.0920567,2.96168635 76.0616282,3.29809095 75.2069787,3.97090014 C74.3513821,4.64370933 73.6857707,5.56219237 73.2095764,6.72521276 C72.732435,7.8888014 72.4945272,9.22722193 72.4945272,10.7410426 C72.4945272,13.2364222 72.8798014,15.4020268 73.6511074,17.2378563 C74.4216558,19.0742542 75.5151601,20.4901463 76.9310522,21.4849643 C78.346376,22.4803507 79.9797189,22.9777597 81.8297547,22.9777597 C83.3153522,22.9777597 84.6819959,22.704431 85.9298751,22.1577735 C87.0808894,21.6531666 88.0354838,20.8680522 88.7932115,19.8019833 L88.9785418,19.5296126 L89.3149464,19.6557644 C88.7821163,22.3752243 87.7024394,24.4146771 86.0768627,25.7741229 C84.4507178,27.134137 82.1938142,27.8135758 79.3069097,27.8135758 C76.9520775,27.8135758 74.8495488,27.2320656 72.9991341,26.068477 C71.1490982,24.9054566 69.6983534,23.3075347 68.6470891,21.2747115 C67.5958247,19.2424565 67.0701925,16.8948222 67.0701925,14.2312403 C67.0701925,11.6241047 67.6166606,9.29029779 68.7101649,7.22981965 C69.8032904,5.16934151 71.3239301,3.5437648 73.2726522,2.35195302 C75.2208061,1.1607095 77.4705118,0.564803615 80.0217694,0.564803615 Z M53.7404825,0.943201959 L53.7404825,1.19550541 L52.8530638,4.46749469 L58.6442992,14.7876906 L63.5611858,4.46863119 L62.5288631,1.19550541 L62.5288631,0.943201959 L69.1309927,0.943201959 L69.1309927,1.19550541 L66.6106101,4.55614185 L59.6275629,17.3696337 L59.6275629,23.860765 L61.3934976,27.1827603 L61.3934976,27.4350638 L51.9743583,27.4350638 L51.9743583,27.1827603 L53.7404825,23.860765 L53.7404825,17.4272165 L46.3395814,4.4754502 L44.4893561,1.19550541 L44.4893561,0.943201959 L53.7404825,0.943201959 Z M97.7102109,0.564803615 L97.7102109,11.7002503 L108.805312,11.7002503 L108.805312,0.564803615 L114.050079,0.564803615 L114.050079,27.4351206 L108.805312,27.4351206 L108.805312,16.057599 L97.7102109,16.057599 L97.7102109,27.4351206 L92.5056,27.4351206 L92.5056,0.564803615 L97.7102109,0.564803615 Z M137.571731,0.443766151 L137.571731,5.36595635 L124.943489,5.36595635 L124.943489,11.3371379 L136.845506,11.3371379 L136.845506,16.0172532 L124.943489,16.0172532 L124.943489,22.5129304 L137.894497,22.5129304 L137.894497,27.4351206 L118.891616,27.4351206 L118.891616,0.443766151 L137.571731,0.443766151 Z M9.0778098,0.443747209 C13.0317003,0.443747209 16.0305503,1.16997199 18.074928,2.62242156 C20.1187374,4.07487113 21.1413998,6.40166958 21.1413998,9.60224865 C21.1413998,11.7544046 20.6837683,13.5095426 19.7694524,14.8672836 C18.8547577,16.2259718 17.503078,17.2276089 15.7147921,17.873142 C14.0450181,18.4756396 12.0066268,18.7969717 9.59931027,18.8371382 L9.0778098,18.8414417 L6.0518732,18.8414417 L6.0518732,27.4351017 L3.05533376e-12,27.4351017 L3.05533376e-12,0.443747209 L9.0778098,0.443747209 Z M8.87627011,5.20455413 L6.0518732,5.20455413 L6.0518732,14.0806348 L8.87627011,14.0806348 C10.1938548,14.0806348 11.3100892,13.9463382 12.2247839,13.6771766 C13.1389103,13.4085833 13.8317977,12.9509518 14.3026884,12.3054187 C14.7728214,11.6598855 15.008835,10.7590183 15.008835,9.60224865 C15.008835,8.44604727 14.7665706,7.55162019 14.2826102,6.91915682 C13.7982709,6.28745112 13.1055729,5.84383651 12.2047057,5.58793414 C11.3032701,5.3324106 10.1938548,5.20455413 8.87627011,5.20455413 Z\"></path></svg></a><a style=\"position:relative;top:0.4em\" href=\"https://sophiaclub.co\" target=\"_blank\" title=\"Sophia Club\"><svg viewBox=\"0 0 1285 287\" fill=\"#F74D41\" stroke=\"none\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-sophiaclub\" aria-hidden=\"true\" class=\"sc-382a1301-17 gXfhvL\"><path d=\"M148.139 76.5496C147.287 53.4902 146.11 42.2773 143.633 22.1777C143.469 20.8338 142.819 19.5963 141.805 18.6992C140.792 17.8021 139.485 17.3073 138.131 17.3084H135.902C135.124 17.3099 134.354 17.4751 133.643 17.7933C132.932 18.1115 132.296 18.5756 131.776 19.1555C131.257 19.7354 130.864 20.4181 130.625 21.1594C130.386 21.9007 130.305 22.684 130.388 23.4585C121.987 17.5501 110.071 12.282 94.3009 12.282C58.6064 12.282 39.3401 38.731 39.3401 63.6336C39.3401 85.648 52.7845 97.9424 65.782 109.826L88.1285 130.366C102.455 143.591 111.079 151.577 111.079 168.064C111.079 188.393 96.3001 210.275 63.855 210.275C46.4186 210.275 32.0864 202.874 17.6516 186.333C18.6169 174.225 18.8328 162.069 18.2978 149.934C18.2311 148.51 17.6184 147.166 16.587 146.181C15.5557 145.197 14.185 144.648 12.7594 144.648H10.5489C9.79112 144.647 9.04129 144.802 8.34571 145.103C7.65013 145.403 7.02367 145.844 6.50511 146.397C5.9865 146.949 5.58686 147.603 5.33078 148.316C5.07477 149.029 4.96782 149.788 5.01658 150.544C6.31513 170.819 4.41261 194.35 0.184837 210.499C-0.0166776 211.26 -0.0539232 212.056 0.0755735 212.833C0.205071 213.61 0.498344 214.35 0.935919 215.005C1.37349 215.66 1.94535 216.214 2.6135 216.63C3.28166 217.047 4.03082 217.317 4.81123 217.422L7.0459 217.7C8.36718 217.867 9.70459 217.554 10.8136 216.816C11.9226 216.078 12.7291 214.966 13.0856 213.682C13.901 210.758 14.6982 207.091 15.4471 202.911C27.0674 213.948 42.0639 223.33 63.8673 223.33C93.951 223.33 124.409 204.349 124.409 168.076C124.409 145.554 110.518 132.97 95.817 119.643L75.6931 101.162C62.8221 89.3636 52.6513 80.0479 52.6513 63.6274C52.6513 38.7552 74.1104 25.3313 94.2953 25.3313C111.575 25.3313 126.07 34.5987 132.768 43.5883C133.831 54.1 134.332 63.8872 134.821 76.9606C134.876 78.3936 135.483 79.7496 136.516 80.7439C137.549 81.7386 138.926 82.2947 140.36 82.2952H142.594C143.34 82.2962 144.077 82.1471 144.764 81.8565C145.45 81.5654 146.07 81.139 146.587 80.6024C147.104 80.0658 147.508 79.4304 147.773 78.7339C148.039 78.0374 148.161 77.2948 148.133 76.5496H148.139Z\" fill=\"#F74D41\"></path> <path d=\"M229.885 187.282C242.332 166.554 249.767 139.954 249.767 116.115C249.767 90.6505 236.945 74.8281 216.308 74.8281C197.736 74.8281 179.828 87.6176 165.876 110.853C153.428 131.58 145.993 158.187 145.993 182.02C145.993 207.49 158.816 223.312 179.454 223.312C198.026 223.331 215.933 210.535 229.885 187.282ZM198.835 203.231C192.42 207.846 185.717 210.287 179.454 210.287C161.939 210.287 159.311 192.592 159.311 182.038C159.311 148.267 175.83 110.013 196.932 94.9396C203.334 90.3241 210.038 87.8836 216.308 87.8836C233.823 87.8836 236.45 105.585 236.45 116.133C236.45 149.904 219.931 188.158 198.847 203.225L198.835 203.231Z\" fill=\"#F74D41\"></path> <path d=\"M381.541 129.285C381.541 92.1608 364.629 74.8701 328.258 74.8701C327.453 74.8701 326.648 74.8824 325.842 74.9064L332.081 57.3928C332.381 56.5559 332.474 55.6595 332.355 54.779C332.235 53.8986 331.905 53.0596 331.394 52.3329C330.883 51.6061 330.204 51.0133 329.415 50.6038C328.627 50.1944 327.752 49.9804 326.863 49.98H324.629C323.484 49.9819 322.367 50.3375 321.432 50.9983C320.497 51.6589 319.788 52.5927 319.404 53.671L311.196 76.8094C286.862 83.274 265.844 102.099 256.278 117.383C255.513 118.602 255.251 120.071 255.549 121.479C255.846 122.887 256.679 124.124 257.872 124.929L259.551 126.046C260.158 126.453 260.839 126.736 261.555 126.878C262.271 127.02 263.009 127.019 263.725 126.874C264.44 126.73 265.121 126.445 265.726 126.037C266.332 125.629 266.85 125.104 267.252 124.494C274.934 112.846 289.109 99.507 305.616 92.3601L242.773 273.957H221.634C220.43 273.959 219.259 274.352 218.298 275.078C217.337 275.804 216.638 276.822 216.307 277.98L215.745 279.932C215.51 280.757 215.469 281.626 215.626 282.47C215.783 283.314 216.134 284.109 216.651 284.795C217.168 285.48 217.837 286.036 218.605 286.419C219.373 286.801 220.22 287 221.078 287H288.614C289.819 287 290.99 286.609 291.953 285.884C292.915 285.159 293.615 284.141 293.947 282.982L294.503 281.025C294.737 280.2 294.777 279.332 294.619 278.489C294.462 277.646 294.112 276.851 293.595 276.167C293.079 275.482 292.412 274.926 291.645 274.543C290.878 274.159 290.033 273.959 289.176 273.957H256.761L274.433 222.986C344.771 221.035 381.541 174.794 381.541 129.285ZM328.258 87.9075C357.019 87.9075 368.223 99.507 368.223 129.285C368.223 167.526 337.578 206.354 278.89 209.786L320.932 88.2581C323.209 88.0044 325.746 87.8835 328.258 87.8835V87.9075Z\" fill=\"#F74D41\"></path> <path d=\"M519.5 167.949L517.825 166.832C516.692 166.079 515.319 165.769 513.972 165.96C512.624 166.151 511.396 166.832 510.517 167.871C510.306 168.118 489.608 192.676 481.799 200.76C477.806 204.753 472.135 210.021 468.161 210.293C468.584 207.599 470.764 202.071 472.715 197.135L505.378 115.124C507.189 110.574 510.245 101.767 510.245 93.7916C510.245 84.6332 504.9 74.846 489.904 74.846C472.582 74.846 449.475 91.7924 429.399 113.456L469.683 7.51545C470.001 6.67588 470.11 5.77193 470.001 4.88095C469.893 3.98999 469.57 3.13861 469.06 2.39977C468.551 1.66092 467.87 1.05663 467.076 0.638678C466.282 0.220728 465.399 0.00157566 464.501 0H462.544C461.891 0.00252852 461.243 0.119061 460.63 0.344356C452.363 3.24345 443.667 4.73223 434.907 4.7485H428.191C426.987 4.75044 425.816 5.14401 424.855 5.86977C423.895 6.59552 423.196 7.61416 422.864 8.77205L422.308 10.7234C422.073 11.5484 422.032 12.4167 422.189 13.2602C422.346 14.1036 422.696 14.8992 423.213 15.5843C423.729 16.2694 424.397 16.8253 425.164 17.2083C425.932 17.5914 426.778 17.7911 427.635 17.7917H434.056C440.201 17.7733 446.331 17.1664 452.362 15.9793L377.228 213.531C376.895 214.375 376.774 215.288 376.876 216.189C376.978 217.091 377.3 217.954 377.812 218.702C378.325 219.45 379.013 220.062 379.817 220.482C380.621 220.902 381.515 221.119 382.423 221.113H384.651C385.775 221.114 386.872 220.772 387.797 220.135C388.723 219.497 389.433 218.593 389.833 217.543L414.179 153.595C438.337 117.311 472.202 87.8835 489.904 87.8835C496.409 87.8835 496.928 90.445 496.928 93.7916C496.928 98.2442 495.587 103.899 492.858 111.01L461.016 190.906L459.808 193.927C456.988 201.019 454.765 206.614 454.765 211.073C454.765 218.28 460.141 223.318 467.847 223.318C476.508 223.318 484.426 216.564 491.094 209.901C499.223 201.485 519.782 177.138 520.689 176.105C521.186 175.508 521.55 174.812 521.76 174.063C521.97 173.315 522.021 172.531 521.913 171.762C521.801 170.993 521.524 170.256 521.109 169.599C520.694 168.942 520.146 168.38 519.5 167.949Z\" fill=\"#F74D41\"></path> <path d=\"M597.394 33.2275C595.745 33.2264 594.136 33.7143 592.763 34.6296C591.389 35.545 590.324 36.8467 589.688 38.3699C589.058 39.8932 588.894 41.5697 589.212 43.1874C589.535 44.805 590.329 46.2911 591.492 47.4576C592.66 48.6242 594.141 49.4188 595.76 49.741C597.379 50.0632 599.054 49.8984 600.576 49.2676C602.098 48.6367 603.404 47.5682 604.316 46.197C605.233 44.8259 605.725 43.2138 605.725 41.5646C605.72 39.355 604.844 37.2363 603.281 35.6733C601.719 34.1103 599.603 33.2307 597.394 33.2275Z\" fill=\"#F74D41\"></path> <path d=\"M598.511 167.949L596.83 166.832C595.688 166.07 594.304 165.76 592.947 165.962C591.589 166.164 590.354 166.864 589.483 167.925C589.278 168.173 568.866 192.985 561.093 201.038C555.145 206.989 550.472 210.1 547.203 210.293C547.762 207.327 550.744 200.627 552.424 196.851C552.947 195.673 553.454 194.543 553.93 193.444L589.944 108.744C593.464 100.256 596.236 93.5502 596.236 87.9137C596.236 79.8485 591.005 74.8342 582.597 74.8342C573.308 74.8342 566.012 81.3768 560.074 87.8112C551.584 97.127 542.295 111.409 530.956 129.986C530.213 131.208 529.967 132.67 530.28 134.067C530.587 135.465 531.423 136.688 532.611 137.483L534.287 138.601C534.907 139.021 535.614 139.311 536.351 139.451C537.089 139.592 537.848 139.58 538.585 139.417C539.318 139.254 540.015 138.944 540.625 138.504C541.234 138.065 541.747 137.506 542.136 136.861C553.613 117.8 562.671 104.636 569.752 96.6011C574.988 90.6443 579.077 87.8717 582.623 87.8717H582.94V87.9076C582.94 91.7985 579 100.981 577.504 104.437L541.506 189.016C540.973 190.254 540.368 191.583 539.743 192.942C536.818 199.383 533.79 206.04 533.79 211.066C533.79 218.389 539.052 223.312 546.87 223.312C553.956 223.312 561.416 219.156 570.331 210.233C578.534 202.035 598.88 177.084 599.746 176.027C600.222 175.428 600.576 174.735 600.775 173.994C600.975 173.252 601.016 172.477 600.904 171.717C600.786 170.958 600.514 170.231 600.104 169.582C599.689 168.934 599.146 168.378 598.511 167.949Z\" fill=\"#F74D41\"></path> <path d=\"M735.157 75.0632L733.21 74.5015C731.94 74.1361 730.587 74.2355 729.383 74.7813C728.184 75.3276 727.221 76.2845 726.662 77.4801L722.23 86.9588C717.998 81.0261 710.948 76.7975 700.967 76.7975C667.818 76.7975 604.69 149.39 604.69 198.228C604.69 216.733 615.055 223.324 624.754 223.324C639.556 223.324 656.239 211.894 671.333 196.615C669.473 201.079 668.069 205.393 668.069 209.12C668.069 217.881 673.403 223.324 681.964 223.324C691.335 223.324 699.046 216.902 705.983 209.967C714.238 201.708 733.287 176.969 734.097 175.918C734.558 175.315 734.896 174.624 735.08 173.887C735.265 173.15 735.296 172.383 735.173 171.634C735.05 170.884 734.773 170.168 734.358 169.53C733.948 168.892 733.405 168.345 732.775 167.925L731.099 166.807C729.936 166.026 728.517 165.717 727.134 165.941C725.75 166.165 724.5 166.907 723.645 168.015C723.455 168.257 704.749 192.441 697.027 200.433C690.141 207.326 685.633 210.262 681.964 210.262C681.744 210.262 681.57 210.262 681.431 210.262C681.38 209.884 681.354 209.502 681.36 209.12C681.36 207.435 682.477 203.079 687.82 191.909L738.631 82.724C738.969 81.9983 739.148 81.2081 739.148 80.407C739.154 79.6065 738.985 78.8147 738.652 78.0864C738.318 77.3582 737.832 76.7109 737.227 76.1886C736.617 75.6669 735.905 75.283 735.137 75.0632H735.157ZM624.754 210.286C623.079 210.286 618.006 210.286 618.006 198.234C618.006 155.147 676.912 89.8406 700.967 89.8406C712.603 89.8406 713.853 98.4614 713.853 102.171C713.812 103.414 713.638 104.65 713.331 105.856L700.138 134.45C687.99 160.494 649.178 210.286 624.754 210.286Z\" fill=\"#F74D41\"></path> <path d=\"M913.822 18.6198C913.53 17.2699 912.746 16.0777 911.619 15.2765C910.497 14.4754 909.113 14.1229 907.74 14.2882L905.506 14.56C904.748 14.6548 904.01 14.9065 903.354 15.2993C902.693 15.6921 902.125 16.2174 901.679 16.842C901.233 17.4666 900.921 18.1769 900.767 18.928C900.608 19.6791 900.608 20.4546 900.767 21.2055C901.525 24.7457 902.283 28.1047 903.047 31.3791C888.942 18.9037 870.246 12.2583 848.388 12.2583C791.214 12.2583 751.276 55.6473 751.276 117.776C751.276 180.891 790.753 223.3 849.505 223.3C889.557 223.3 915.943 200.827 926.41 181.585C927.102 180.312 927.266 178.819 926.872 177.425C926.472 176.032 925.55 174.849 924.289 174.13L922.337 173.006C921.697 172.639 920.985 172.404 920.252 172.316C919.514 172.227 918.771 172.287 918.059 172.492C917.347 172.696 916.681 173.041 916.107 173.506C915.528 173.971 915.051 174.547 914.698 175.199C905.342 192.647 881.559 210.239 849.505 210.239C798.71 210.288 764.602 173.121 764.602 117.806C764.602 63.362 799.028 25.3318 848.399 25.3318C879.453 25.3318 902.391 41.009 913.022 69.4818L916.373 78.4228C916.635 79.125 917.039 79.7661 917.557 80.3078C918.074 80.8495 918.699 81.2805 919.386 81.5752C920.078 81.8699 920.815 82.0221 921.569 82.0221C922.317 82.0226 923.06 81.8709 923.746 81.5768L925.698 80.7368C926.979 80.1884 928.009 79.1757 928.578 77.9006C929.146 76.626 929.213 75.1838 928.767 73.8621C923.649 58.8976 917.034 33.5299 913.822 18.6198Z\" fill=\"#F74D41\"></path> <path d=\"M989.651 208.052H973.132V6.64567C973.132 5.91879 972.989 5.19893 972.712 4.5272C972.435 3.85549 972.031 3.24509 971.513 2.73087L970.396 1.61322C969.571 0.795123 968.511 0.257485 967.363 0.0764193C966.215 -0.104646 965.037 0.0800354 964.002 0.604319C959.678 2.76712 954.749 4.7668 948.052 4.7668C946.264 4.74176 944.476 4.59035 942.713 4.3137C941.97 4.16742 941.212 4.17305 940.474 4.33026C939.737 4.48747 939.04 4.79296 938.425 5.22815C937.81 5.66331 937.293 6.21912 936.898 6.86164C936.504 7.50416 936.247 8.21997 936.14 8.96555L935.863 10.9169C935.663 12.3188 936.007 13.7444 936.821 14.9014C937.636 16.0583 938.866 16.8591 940.254 17.1395C942.831 17.5771 945.439 17.8014 948.052 17.8101C952.023 17.7942 955.978 17.2663 959.816 16.2393V208.052H945.537C944.066 208.052 942.657 208.635 941.617 209.674C940.582 210.713 939.993 212.122 939.993 213.592V215.549C939.993 217.02 940.577 218.431 941.617 219.471C942.657 220.511 944.066 221.095 945.537 221.095H989.625C991.095 221.094 992.504 220.509 993.545 219.469C994.585 218.429 995.164 217.019 995.164 215.549V213.592C995.164 212.127 994.585 210.723 993.55 209.685C992.52 208.647 991.116 208.06 989.651 208.052Z\" fill=\"#F74D41\"></path> <path d=\"M1139.29 203.865C1134.16 203.865 1127.18 205.073 1121.11 206.807V82.6212C1121.11 81.1503 1120.52 79.7399 1119.48 78.6996C1118.44 77.6597 1117.03 77.0754 1115.56 77.0754H1093.21C1092.49 77.0754 1091.76 77.2189 1091.09 77.4977C1090.42 77.7765 1089.81 78.185 1089.29 78.7001C1088.78 79.2151 1088.37 79.8265 1088.1 80.4995C1087.82 81.1724 1087.68 81.8929 1087.68 82.6212V84.5784C1087.68 85.3067 1087.82 86.0278 1088.1 86.7007C1088.37 87.3731 1088.78 87.9845 1089.29 88.4996C1089.81 89.0146 1090.42 89.4236 1091.09 89.7024C1091.76 89.9812 1092.49 90.1247 1093.21 90.1247H1107.77V187.886C1090.39 200.687 1075.3 210.269 1056.64 210.269C1044.02 210.269 1028.96 204.976 1028.96 179.79V82.6212C1028.96 81.8929 1028.82 81.1724 1028.54 80.4995C1028.26 79.8265 1027.85 79.2151 1027.34 78.7001C1026.82 78.185 1026.21 77.7765 1025.54 77.4977C1024.87 77.2189 1024.15 77.0754 1023.42 77.0754H1002.76C1002.03 77.0754 1001.31 77.2189 1000.64 77.4977C999.964 77.7765 999.355 78.185 998.837 78.7001C998.325 79.2151 997.915 79.8265 997.638 80.4995C997.362 81.1724 997.218 81.8929 997.218 82.6212V84.5784C997.218 85.3067 997.362 86.0278 997.638 86.7007C997.915 87.3731 998.325 87.9845 998.837 88.4996C999.355 89.0146 999.964 89.4236 1000.64 89.7024C1001.31 89.9812 1002.03 90.1247 1002.76 90.1247H1015.64V180.086C1015.64 207.164 1030.97 223.33 1056.64 223.33C1076.3 223.33 1091.94 215.054 1107.77 204.028V216.111C1107.77 216.942 1107.96 217.762 1108.32 218.511C1108.69 219.26 1109.21 219.918 1109.86 220.437L1111.25 221.554C1112.12 222.251 1113.19 222.668 1114.3 222.752C1115.41 222.836 1116.52 222.583 1117.49 222.025C1121.72 219.609 1133.11 216.902 1139.28 216.902C1140.75 216.902 1142.16 216.319 1143.19 215.28C1144.23 214.241 1144.82 212.832 1144.82 211.362V209.411C1144.82 207.942 1144.24 206.533 1143.2 205.494C1142.16 204.454 1140.76 203.868 1139.29 203.865Z\" fill=\"#F74D41\"></path> <path d=\"M1220.26 74.8466C1214.82 74.8466 1207.57 76.0063 1201.31 77.8672C1192.25 80.6701 1183.13 84.9597 1175.54 89.0192V6.64567C1175.54 5.91879 1175.4 5.19893 1175.12 4.5272C1174.84 3.85549 1174.43 3.24509 1173.92 2.73087L1172.8 1.61322C1171.98 0.795123 1170.92 0.257485 1169.77 0.0764193C1168.62 -0.104646 1167.44 0.0800354 1166.41 0.604319C1162.09 2.76712 1157.16 4.7668 1150.46 4.7668C1148.67 4.74176 1146.88 4.59035 1145.12 4.3137C1144.38 4.16654 1143.62 4.17156 1142.88 4.32845C1142.15 4.48535 1141.45 4.79083 1140.84 5.22625C1140.22 5.66162 1139.7 6.21778 1139.31 6.86071C1138.92 7.50365 1138.66 8.21986 1138.55 8.96555L1138.28 10.9229C1138.08 12.3238 1138.43 13.7463 1139.25 14.8988C1140.07 16.0512 1141.3 16.8464 1142.69 17.1214C1145.26 17.5584 1147.87 17.7828 1150.48 17.7919C1154.46 17.7765 1158.41 17.2486 1162.25 16.2212V209.411C1162.25 210.669 1162.68 211.889 1163.46 212.873L1164.58 214.269C1165.3 215.17 1166.29 215.823 1167.4 216.135C1184.53 220.969 1199.63 223.331 1213.57 223.331C1255.63 223.331 1285 191.88 1285 146.853C1284.98 100.419 1262 74.8466 1220.26 74.8466ZM1213.55 210.288C1198.74 210.288 1185.31 207.339 1175.54 204.663V103.911C1186.92 97.2659 1196.79 92.7589 1205.63 90.1432H1205.73C1210.43 88.704 1215.31 87.943 1220.23 87.8835C1254.34 87.8835 1271.64 107.723 1271.64 146.853C1271.67 184.201 1247.77 210.288 1213.55 210.288Z\" fill=\"#F74D41\"></path> </svg></a></div></div><div class=\"sc-382a1301-13 hDVXDg\"><div class=\"sc-382a1301-15 fpvPoK\"><form action=\"/search\" class=\"sc-b4707aca-0 iqqVMN\"><svg viewBox=\"0 0 13 13\" fill=\"currentColor\" stroke=\"none\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-search\" aria-hidden=\"true\" class=\"sc-b4707aca-1 jFRXWW\"><g fill=\"none\" fill-rule=\"evenodd\" stroke=\"inherit\" transform=\"translate(.5 .5)\"><path stroke-linecap=\"square\" d=\"M7.487 7.467L10.5 9.975\"></path><circle cx=\"4.2\" cy=\"4.2\" r=\"4.2\"></circle></g></svg><input type=\"search\" name=\"q\" placeholder=\"SEARCH\" autoComplete=\"off\" spellcheck=\"false\" autoCorrect=\"off\" class=\"sc-b4707aca-2 hWayZo\" value=\"\"/></form></div></div></div></div></div></div><div class=\"sc-fe6897d0-1 krRBWz\"><div class=\"sc-fe6897d0-4 ePSmzF\"><span tabindex=\"0\" role=\"button\" on=\"tap:sidebar-left.toggle\" class=\"sc-fe6897d0-5 fOgqRT\">Menu</span><span class=\"sc-fe6897d0-5 fOgqRT\"><form action=\"/search\" class=\"sc-b4707aca-0 ezDGGL\"><svg viewBox=\"0 0 13 13\" fill=\"currentColor\" stroke=\"none\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-search\" aria-hidden=\"true\" class=\"sc-b4707aca-1 jWSMHq\"><g fill=\"none\" fill-rule=\"evenodd\" stroke=\"inherit\" transform=\"translate(.5 .5)\"><path stroke-linecap=\"square\" d=\"M7.487 7.467L10.5 9.975\"></path><circle cx=\"4.2\" cy=\"4.2\" r=\"4.2\"></circle></g></svg><input type=\"search\" name=\"q\" placeholder=\"SEARCH\" autoComplete=\"off\" spellcheck=\"false\" autoCorrect=\"off\" class=\"sc-b4707aca-2 smYs\" value=\"\"/></form></span></div><div class=\"sc-fe6897d0-7 cdcWxf\"><a href=\"https://www.facebook.com/aeonmag\" target=\"_blank\" rel=\"noopener noreferrer\" title=\"Facebook\"><svg viewBox=\"0 0 1024 1024\" fill=\"currentColor\" stroke=\"none\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-facebook\" aria-hidden=\"true\"><g><path d=\"M1024,512C1024,229.23,794.77,0,512,0S0,229.23,0,512c0,255.55,187.23,467.37,432,505.78V660H302V512H432V399.2C432,270.88,508.44,200,625.39,200c56,0,114.61,10,114.61,10V336H675.44c-63.6,0-83.44,39.47-83.44,80v96H734L711.3,660H592v357.78C836.77,979.37,1024,767.55,1024,512Z\"></path><path d=\"M711.3,660,734,512H592V416c0-40.49,19.84-80,83.44-80H740V210s-58.59-10-114.61-10C508.44,200,432,270.88,432,399.2V512H302V660H432v357.78a517.58,517.58,0,0,0,160,0V660Z\" fill=\"transparent\"></path></g></svg></a><a href=\"https://www.instagram.com/aeonmag\" target=\"_blank\" rel=\"noopener noreferrer\" title=\"Instagram\"><svg viewBox=\"0 0 24 25\" fill=\"currentColor\" stroke=\"none\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-instagram\" aria-hidden=\"true\"><path d=\"M17.216.5c4.456.203 6.887 2.633 7.09 7.09v10.126c-.203 4.456-2.634 6.887-7.09 7.09H7.09C2.633 24.603.203 22.172 0 17.716V7.59C.203 3.133 2.633.703 7.09.5zm-5.063 5.874c-3.444 0-6.28 2.835-6.28 6.279 0 3.443 2.836 6.279 6.28 6.279 3.443 0 6.279-2.836 6.279-6.28 0-3.443-2.836-6.278-6.28-6.278zm0 2.228a4.063 4.063 0 0 1 4.05 4.05 4.063 4.063 0 0 1-4.05 4.052 4.063 4.063 0 0 1-4.051-4.051 4.063 4.063 0 0 1 4.05-4.051zm6.481-3.849c-.81 0-1.418.608-1.418 1.418 0 .81.608 1.418 1.418 1.418.81 0 1.418-.608 1.418-1.418 0-.81-.608-1.418-1.418-1.418z\"></path></svg></a><a href=\"https://twitter.com/aeonmag\" target=\"_blank\" rel=\"noopener noreferrer\" title=\"X (Formerly Twitter)\"><svg viewBox=\"0 0 450 450\" fill=\"currentColor\" stroke=\"none\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-twitter\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M225 450C349.264 450 450 349.264 450 225C450 100.736 349.264 0 225 0C100.736 0 0 100.736 0 225C0 349.264 100.736 450 225 450ZM349.373 92L250.332 204.646L358 358H278.785L206.256 254.71L115.461 358H92L195.855 239.888L92 92H171.215L239.878 189.789L325.912 92H349.373ZM159.972 109.311H123.929L289.842 341.539H325.894L159.972 109.311Z\"></path></svg></a><a href=\"https://www.youtube.com/@AeonVideo\" target=\"_blank\" rel=\"noopener noreferrer\" title=\"YouTube\"><svg viewBox=\"0 0 22 20\" fill=\"currentColor\" stroke=\"none\" width=\"22\" height=\"20\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-youtube\" aria-hidden=\"true\"><g><path d=\"M21.5,4.7c-0.3-1-1-1.7-1.9-2C17.9,2.2,11,2.2,11,2.2s-6.9,0-8.6,0.5c-1,0.3-1.7,1-1.9,2C0,6.4,0,10,0,10s0,3.6,0.5,5.3  c0.3,1,1,1.7,1.9,2c1.7,0.5,8.6,0.5,8.6,0.5s6.9,0,8.6-0.5c1-0.3,1.7-1,1.9-2C22,13.6,22,10,22,10S22,6.4,21.5,4.7z M8.8,13.3V6.7  l5.8,3.3L8.8,13.3z\"></path></g></svg></a></div><a href=\"/\" class=\"sc-fe6897d0-2 bncHuo\"><svg viewBox=\"-16 -5.118 174.65 61.413\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" aria-describedby=\"aeon-logo-title\" role=\"img\" class=\"sc-fe6897d0-9 hLkmal\"><title id=\"aeon-logo-title\">Aeon</title><path fill=\"currentColor\" d=\"M70.93 25.275c0 11.705 8.43 21.268 18.755 21.268 10.57 0 19.002-9.438 19.002-21.268 0-11.45-8.432-20.642-18.877-20.642-10.45 0-18.88 9.19-18.88 20.642m48.05.252c0 16.504-13.052 29.81-29.17 29.81-16.12 0-29.17-13.306-29.17-29.81 0-16.377 13.05-29.683 29.17-29.683 16.247 0 29.17 13.177 29.17 29.683M45.39 19.563c0-6.435-.266-10.854-.795-13.252-.525-2.398-1.35-4.217-2.47-5.456-.635-.72-1.48-1.08-2.535-1.08-1.575 0-2.856 1.02-3.853 3.06-1.782 3.56-2.674 8.435-2.674 14.63v2.1H45.39m11.146 4.017H33.29c.272 7.435 1.77 13.31 4.487 17.63 2.083 3.32 4.59 4.976 7.52 4.976 1.81 0 3.458-.673 4.936-2.01 1.485-1.34 3.07-3.75 4.76-7.225l1.544 1.322c-2.296 6.2-4.837 10.582-7.613 13.163-2.778 2.575-5.994 3.87-9.653 3.87-6.284 0-11.04-3.197-14.273-9.594C22.4 40.554 21.1 34.157 21.1 26.526c0-9.354 1.912-16.8 5.734-22.338 3.82-5.538 8.3-8.305 13.436-8.305 4.288 0 8.01 2.327 11.168 6.986 3.156 4.653 4.854 11.56 5.098 20.713M4.668 26.974c-6.007 2.037-9.014 5.404-9.014 9.97v4.566c0 2.88 1.323 5.046 3.967 5.046 2.883 0 5.048-2.524 5.048-5.648V26.974M14.876 55.32c-3.724 0-6.605-1.923-7.807-6.243C4.903 53.16 1.18 55.32-3.265 55.32c-6.725 0-11.29-5.165-11.29-14.05V38.5c0-10.572 8.168-15.615 19.223-19.7v-8.89c0-2.882-1.203-4.564-3.967-4.564-2.884 0-3.846 1.682-3.846 4.806V15.2h-9.73v-4.564c0-8.53 4.686-14.776 14.055-14.776 9.37 0 13.816 6.486 13.816 15.618V44.39c0 1.805.724 2.766 2.4 2.886v8.046h-2.522zM156.496 52.825v-39.79c0-7.597-.514-10.944-2.31-13.522-1.163-2.058-2.833-2.96-5.022-2.96-1.928 0-3.863 1.03-6.176 3.605-2.24 2.243-5.352 6.04-7.246 8.842.742-4.152.684-14.118.684-14.118-6.56 3.86-9.902 4.893-16.207 5.15V1.19c1.925.515 2.57.643 4.243 1.415.127 4.25.127 7.34.127 8.11v10.946l-.127 10.944V45.23c0 2.958 0 4.12-.127 7.596-1.416 1.283-1.93 1.413-3.73 2.184v1.285h18.78V55.01c-1.67-.774-2.187-1.028-3.474-2.184V10.973l.64-.64c2.063-1.933 3.99-3.092 5.28-3.092 2.697 0 3.47 2.06 3.47 8.5v37.085c-1.54 1.155-2.058 1.413-3.856 2.182v1.287h18.65v-1.542c-1.8-.643-2.315-.9-3.6-1.927\"></path></svg></a><div class=\"sc-fe6897d0-4 gRAhjy\"><span class=\"sc-fe6897d0-5 gteQza\"><a data-ga-select-prompt=\"aeon_topbar_button_donate\" href=\"/donate\">Donate</a></span><span data-ga-select-prompt=\"aeon_topbar_button_newsletter\" role=\"button\" tabindex=\"0\" on=\"tap:newsletter-popup\" class=\"sc-fe6897d0-5 sc-fe6897d0-6 dcLccx\">Newsletter</span><span role=\"button\" tabindex=\"0\" title=\"Sign in\" class=\"sc-fe6897d0-5 hfBcHl\"><span class=\"sc-fe6897d0-12 cnuBHA\"><div class=\"sc-fe6897d0-11 ebKSSz\"><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"18\" height=\"18\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-user\" aria-hidden=\"true\"><path d=\"M12 12c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm0 2c-2.67 0-8 1.34-8 4v1c0 .55.45 1 1 1h14c.55 0 1-.45 1-1v-1c0-2.66-5.33-4-8-4z\"></path></svg></div><span>SIGN IN</span></span></span></div><div class=\"sc-fe6897d0-3 cgBUSz\"><a style=\"position:relative;top:0.2em\" href=\"https://sophiaclub.co\" target=\"_blank\" title=\"Sophia Club\"><svg viewBox=\"0 0 1285 287\" fill=\"#F74D41\" stroke=\"none\" width=\"97\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-sophiaclub\" aria-hidden=\"true\"><path d=\"M148.139 76.5496C147.287 53.4902 146.11 42.2773 143.633 22.1777C143.469 20.8338 142.819 19.5963 141.805 18.6992C140.792 17.8021 139.485 17.3073 138.131 17.3084H135.902C135.124 17.3099 134.354 17.4751 133.643 17.7933C132.932 18.1115 132.296 18.5756 131.776 19.1555C131.257 19.7354 130.864 20.4181 130.625 21.1594C130.386 21.9007 130.305 22.684 130.388 23.4585C121.987 17.5501 110.071 12.282 94.3009 12.282C58.6064 12.282 39.3401 38.731 39.3401 63.6336C39.3401 85.648 52.7845 97.9424 65.782 109.826L88.1285 130.366C102.455 143.591 111.079 151.577 111.079 168.064C111.079 188.393 96.3001 210.275 63.855 210.275C46.4186 210.275 32.0864 202.874 17.6516 186.333C18.6169 174.225 18.8328 162.069 18.2978 149.934C18.2311 148.51 17.6184 147.166 16.587 146.181C15.5557 145.197 14.185 144.648 12.7594 144.648H10.5489C9.79112 144.647 9.04129 144.802 8.34571 145.103C7.65013 145.403 7.02367 145.844 6.50511 146.397C5.9865 146.949 5.58686 147.603 5.33078 148.316C5.07477 149.029 4.96782 149.788 5.01658 150.544C6.31513 170.819 4.41261 194.35 0.184837 210.499C-0.0166776 211.26 -0.0539232 212.056 0.0755735 212.833C0.205071 213.61 0.498344 214.35 0.935919 215.005C1.37349 215.66 1.94535 216.214 2.6135 216.63C3.28166 217.047 4.03082 217.317 4.81123 217.422L7.0459 217.7C8.36718 217.867 9.70459 217.554 10.8136 216.816C11.9226 216.078 12.7291 214.966 13.0856 213.682C13.901 210.758 14.6982 207.091 15.4471 202.911C27.0674 213.948 42.0639 223.33 63.8673 223.33C93.951 223.33 124.409 204.349 124.409 168.076C124.409 145.554 110.518 132.97 95.817 119.643L75.6931 101.162C62.8221 89.3636 52.6513 80.0479 52.6513 63.6274C52.6513 38.7552 74.1104 25.3313 94.2953 25.3313C111.575 25.3313 126.07 34.5987 132.768 43.5883C133.831 54.1 134.332 63.8872 134.821 76.9606C134.876 78.3936 135.483 79.7496 136.516 80.7439C137.549 81.7386 138.926 82.2947 140.36 82.2952H142.594C143.34 82.2962 144.077 82.1471 144.764 81.8565C145.45 81.5654 146.07 81.139 146.587 80.6024C147.104 80.0658 147.508 79.4304 147.773 78.7339C148.039 78.0374 148.161 77.2948 148.133 76.5496H148.139Z\" fill=\"#F74D41\"></path> <path d=\"M229.885 187.282C242.332 166.554 249.767 139.954 249.767 116.115C249.767 90.6505 236.945 74.8281 216.308 74.8281C197.736 74.8281 179.828 87.6176 165.876 110.853C153.428 131.58 145.993 158.187 145.993 182.02C145.993 207.49 158.816 223.312 179.454 223.312C198.026 223.331 215.933 210.535 229.885 187.282ZM198.835 203.231C192.42 207.846 185.717 210.287 179.454 210.287C161.939 210.287 159.311 192.592 159.311 182.038C159.311 148.267 175.83 110.013 196.932 94.9396C203.334 90.3241 210.038 87.8836 216.308 87.8836C233.823 87.8836 236.45 105.585 236.45 116.133C236.45 149.904 219.931 188.158 198.847 203.225L198.835 203.231Z\" fill=\"#F74D41\"></path> <path d=\"M381.541 129.285C381.541 92.1608 364.629 74.8701 328.258 74.8701C327.453 74.8701 326.648 74.8824 325.842 74.9064L332.081 57.3928C332.381 56.5559 332.474 55.6595 332.355 54.779C332.235 53.8986 331.905 53.0596 331.394 52.3329C330.883 51.6061 330.204 51.0133 329.415 50.6038C328.627 50.1944 327.752 49.9804 326.863 49.98H324.629C323.484 49.9819 322.367 50.3375 321.432 50.9983C320.497 51.6589 319.788 52.5927 319.404 53.671L311.196 76.8094C286.862 83.274 265.844 102.099 256.278 117.383C255.513 118.602 255.251 120.071 255.549 121.479C255.846 122.887 256.679 124.124 257.872 124.929L259.551 126.046C260.158 126.453 260.839 126.736 261.555 126.878C262.271 127.02 263.009 127.019 263.725 126.874C264.44 126.73 265.121 126.445 265.726 126.037C266.332 125.629 266.85 125.104 267.252 124.494C274.934 112.846 289.109 99.507 305.616 92.3601L242.773 273.957H221.634C220.43 273.959 219.259 274.352 218.298 275.078C217.337 275.804 216.638 276.822 216.307 277.98L215.745 279.932C215.51 280.757 215.469 281.626 215.626 282.47C215.783 283.314 216.134 284.109 216.651 284.795C217.168 285.48 217.837 286.036 218.605 286.419C219.373 286.801 220.22 287 221.078 287H288.614C289.819 287 290.99 286.609 291.953 285.884C292.915 285.159 293.615 284.141 293.947 282.982L294.503 281.025C294.737 280.2 294.777 279.332 294.619 278.489C294.462 277.646 294.112 276.851 293.595 276.167C293.079 275.482 292.412 274.926 291.645 274.543C290.878 274.159 290.033 273.959 289.176 273.957H256.761L274.433 222.986C344.771 221.035 381.541 174.794 381.541 129.285ZM328.258 87.9075C357.019 87.9075 368.223 99.507 368.223 129.285C368.223 167.526 337.578 206.354 278.89 209.786L320.932 88.2581C323.209 88.0044 325.746 87.8835 328.258 87.8835V87.9075Z\" fill=\"#F74D41\"></path> <path d=\"M519.5 167.949L517.825 166.832C516.692 166.079 515.319 165.769 513.972 165.96C512.624 166.151 511.396 166.832 510.517 167.871C510.306 168.118 489.608 192.676 481.799 200.76C477.806 204.753 472.135 210.021 468.161 210.293C468.584 207.599 470.764 202.071 472.715 197.135L505.378 115.124C507.189 110.574 510.245 101.767 510.245 93.7916C510.245 84.6332 504.9 74.846 489.904 74.846C472.582 74.846 449.475 91.7924 429.399 113.456L469.683 7.51545C470.001 6.67588 470.11 5.77193 470.001 4.88095C469.893 3.98999 469.57 3.13861 469.06 2.39977C468.551 1.66092 467.87 1.05663 467.076 0.638678C466.282 0.220728 465.399 0.00157566 464.501 0H462.544C461.891 0.00252852 461.243 0.119061 460.63 0.344356C452.363 3.24345 443.667 4.73223 434.907 4.7485H428.191C426.987 4.75044 425.816 5.14401 424.855 5.86977C423.895 6.59552 423.196 7.61416 422.864 8.77205L422.308 10.7234C422.073 11.5484 422.032 12.4167 422.189 13.2602C422.346 14.1036 422.696 14.8992 423.213 15.5843C423.729 16.2694 424.397 16.8253 425.164 17.2083C425.932 17.5914 426.778 17.7911 427.635 17.7917H434.056C440.201 17.7733 446.331 17.1664 452.362 15.9793L377.228 213.531C376.895 214.375 376.774 215.288 376.876 216.189C376.978 217.091 377.3 217.954 377.812 218.702C378.325 219.45 379.013 220.062 379.817 220.482C380.621 220.902 381.515 221.119 382.423 221.113H384.651C385.775 221.114 386.872 220.772 387.797 220.135C388.723 219.497 389.433 218.593 389.833 217.543L414.179 153.595C438.337 117.311 472.202 87.8835 489.904 87.8835C496.409 87.8835 496.928 90.445 496.928 93.7916C496.928 98.2442 495.587 103.899 492.858 111.01L461.016 190.906L459.808 193.927C456.988 201.019 454.765 206.614 454.765 211.073C454.765 218.28 460.141 223.318 467.847 223.318C476.508 223.318 484.426 216.564 491.094 209.901C499.223 201.485 519.782 177.138 520.689 176.105C521.186 175.508 521.55 174.812 521.76 174.063C521.97 173.315 522.021 172.531 521.913 171.762C521.801 170.993 521.524 170.256 521.109 169.599C520.694 168.942 520.146 168.38 519.5 167.949Z\" fill=\"#F74D41\"></path> <path d=\"M597.394 33.2275C595.745 33.2264 594.136 33.7143 592.763 34.6296C591.389 35.545 590.324 36.8467 589.688 38.3699C589.058 39.8932 588.894 41.5697 589.212 43.1874C589.535 44.805 590.329 46.2911 591.492 47.4576C592.66 48.6242 594.141 49.4188 595.76 49.741C597.379 50.0632 599.054 49.8984 600.576 49.2676C602.098 48.6367 603.404 47.5682 604.316 46.197C605.233 44.8259 605.725 43.2138 605.725 41.5646C605.72 39.355 604.844 37.2363 603.281 35.6733C601.719 34.1103 599.603 33.2307 597.394 33.2275Z\" fill=\"#F74D41\"></path> <path d=\"M598.511 167.949L596.83 166.832C595.688 166.07 594.304 165.76 592.947 165.962C591.589 166.164 590.354 166.864 589.483 167.925C589.278 168.173 568.866 192.985 561.093 201.038C555.145 206.989 550.472 210.1 547.203 210.293C547.762 207.327 550.744 200.627 552.424 196.851C552.947 195.673 553.454 194.543 553.93 193.444L589.944 108.744C593.464 100.256 596.236 93.5502 596.236 87.9137C596.236 79.8485 591.005 74.8342 582.597 74.8342C573.308 74.8342 566.012 81.3768 560.074 87.8112C551.584 97.127 542.295 111.409 530.956 129.986C530.213 131.208 529.967 132.67 530.28 134.067C530.587 135.465 531.423 136.688 532.611 137.483L534.287 138.601C534.907 139.021 535.614 139.311 536.351 139.451C537.089 139.592 537.848 139.58 538.585 139.417C539.318 139.254 540.015 138.944 540.625 138.504C541.234 138.065 541.747 137.506 542.136 136.861C553.613 117.8 562.671 104.636 569.752 96.6011C574.988 90.6443 579.077 87.8717 582.623 87.8717H582.94V87.9076C582.94 91.7985 579 100.981 577.504 104.437L541.506 189.016C540.973 190.254 540.368 191.583 539.743 192.942C536.818 199.383 533.79 206.04 533.79 211.066C533.79 218.389 539.052 223.312 546.87 223.312C553.956 223.312 561.416 219.156 570.331 210.233C578.534 202.035 598.88 177.084 599.746 176.027C600.222 175.428 600.576 174.735 600.775 173.994C600.975 173.252 601.016 172.477 600.904 171.717C600.786 170.958 600.514 170.231 600.104 169.582C599.689 168.934 599.146 168.378 598.511 167.949Z\" fill=\"#F74D41\"></path> <path d=\"M735.157 75.0632L733.21 74.5015C731.94 74.1361 730.587 74.2355 729.383 74.7813C728.184 75.3276 727.221 76.2845 726.662 77.4801L722.23 86.9588C717.998 81.0261 710.948 76.7975 700.967 76.7975C667.818 76.7975 604.69 149.39 604.69 198.228C604.69 216.733 615.055 223.324 624.754 223.324C639.556 223.324 656.239 211.894 671.333 196.615C669.473 201.079 668.069 205.393 668.069 209.12C668.069 217.881 673.403 223.324 681.964 223.324C691.335 223.324 699.046 216.902 705.983 209.967C714.238 201.708 733.287 176.969 734.097 175.918C734.558 175.315 734.896 174.624 735.08 173.887C735.265 173.15 735.296 172.383 735.173 171.634C735.05 170.884 734.773 170.168 734.358 169.53C733.948 168.892 733.405 168.345 732.775 167.925L731.099 166.807C729.936 166.026 728.517 165.717 727.134 165.941C725.75 166.165 724.5 166.907 723.645 168.015C723.455 168.257 704.749 192.441 697.027 200.433C690.141 207.326 685.633 210.262 681.964 210.262C681.744 210.262 681.57 210.262 681.431 210.262C681.38 209.884 681.354 209.502 681.36 209.12C681.36 207.435 682.477 203.079 687.82 191.909L738.631 82.724C738.969 81.9983 739.148 81.2081 739.148 80.407C739.154 79.6065 738.985 78.8147 738.652 78.0864C738.318 77.3582 737.832 76.7109 737.227 76.1886C736.617 75.6669 735.905 75.283 735.137 75.0632H735.157ZM624.754 210.286C623.079 210.286 618.006 210.286 618.006 198.234C618.006 155.147 676.912 89.8406 700.967 89.8406C712.603 89.8406 713.853 98.4614 713.853 102.171C713.812 103.414 713.638 104.65 713.331 105.856L700.138 134.45C687.99 160.494 649.178 210.286 624.754 210.286Z\" fill=\"#F74D41\"></path> <path d=\"M913.822 18.6198C913.53 17.2699 912.746 16.0777 911.619 15.2765C910.497 14.4754 909.113 14.1229 907.74 14.2882L905.506 14.56C904.748 14.6548 904.01 14.9065 903.354 15.2993C902.693 15.6921 902.125 16.2174 901.679 16.842C901.233 17.4666 900.921 18.1769 900.767 18.928C900.608 19.6791 900.608 20.4546 900.767 21.2055C901.525 24.7457 902.283 28.1047 903.047 31.3791C888.942 18.9037 870.246 12.2583 848.388 12.2583C791.214 12.2583 751.276 55.6473 751.276 117.776C751.276 180.891 790.753 223.3 849.505 223.3C889.557 223.3 915.943 200.827 926.41 181.585C927.102 180.312 927.266 178.819 926.872 177.425C926.472 176.032 925.55 174.849 924.289 174.13L922.337 173.006C921.697 172.639 920.985 172.404 920.252 172.316C919.514 172.227 918.771 172.287 918.059 172.492C917.347 172.696 916.681 173.041 916.107 173.506C915.528 173.971 915.051 174.547 914.698 175.199C905.342 192.647 881.559 210.239 849.505 210.239C798.71 210.288 764.602 173.121 764.602 117.806C764.602 63.362 799.028 25.3318 848.399 25.3318C879.453 25.3318 902.391 41.009 913.022 69.4818L916.373 78.4228C916.635 79.125 917.039 79.7661 917.557 80.3078C918.074 80.8495 918.699 81.2805 919.386 81.5752C920.078 81.8699 920.815 82.0221 921.569 82.0221C922.317 82.0226 923.06 81.8709 923.746 81.5768L925.698 80.7368C926.979 80.1884 928.009 79.1757 928.578 77.9006C929.146 76.626 929.213 75.1838 928.767 73.8621C923.649 58.8976 917.034 33.5299 913.822 18.6198Z\" fill=\"#F74D41\"></path> <path d=\"M989.651 208.052H973.132V6.64567C973.132 5.91879 972.989 5.19893 972.712 4.5272C972.435 3.85549 972.031 3.24509 971.513 2.73087L970.396 1.61322C969.571 0.795123 968.511 0.257485 967.363 0.0764193C966.215 -0.104646 965.037 0.0800354 964.002 0.604319C959.678 2.76712 954.749 4.7668 948.052 4.7668C946.264 4.74176 944.476 4.59035 942.713 4.3137C941.97 4.16742 941.212 4.17305 940.474 4.33026C939.737 4.48747 939.04 4.79296 938.425 5.22815C937.81 5.66331 937.293 6.21912 936.898 6.86164C936.504 7.50416 936.247 8.21997 936.14 8.96555L935.863 10.9169C935.663 12.3188 936.007 13.7444 936.821 14.9014C937.636 16.0583 938.866 16.8591 940.254 17.1395C942.831 17.5771 945.439 17.8014 948.052 17.8101C952.023 17.7942 955.978 17.2663 959.816 16.2393V208.052H945.537C944.066 208.052 942.657 208.635 941.617 209.674C940.582 210.713 939.993 212.122 939.993 213.592V215.549C939.993 217.02 940.577 218.431 941.617 219.471C942.657 220.511 944.066 221.095 945.537 221.095H989.625C991.095 221.094 992.504 220.509 993.545 219.469C994.585 218.429 995.164 217.019 995.164 215.549V213.592C995.164 212.127 994.585 210.723 993.55 209.685C992.52 208.647 991.116 208.06 989.651 208.052Z\" fill=\"#F74D41\"></path> <path d=\"M1139.29 203.865C1134.16 203.865 1127.18 205.073 1121.11 206.807V82.6212C1121.11 81.1503 1120.52 79.7399 1119.48 78.6996C1118.44 77.6597 1117.03 77.0754 1115.56 77.0754H1093.21C1092.49 77.0754 1091.76 77.2189 1091.09 77.4977C1090.42 77.7765 1089.81 78.185 1089.29 78.7001C1088.78 79.2151 1088.37 79.8265 1088.1 80.4995C1087.82 81.1724 1087.68 81.8929 1087.68 82.6212V84.5784C1087.68 85.3067 1087.82 86.0278 1088.1 86.7007C1088.37 87.3731 1088.78 87.9845 1089.29 88.4996C1089.81 89.0146 1090.42 89.4236 1091.09 89.7024C1091.76 89.9812 1092.49 90.1247 1093.21 90.1247H1107.77V187.886C1090.39 200.687 1075.3 210.269 1056.64 210.269C1044.02 210.269 1028.96 204.976 1028.96 179.79V82.6212C1028.96 81.8929 1028.82 81.1724 1028.54 80.4995C1028.26 79.8265 1027.85 79.2151 1027.34 78.7001C1026.82 78.185 1026.21 77.7765 1025.54 77.4977C1024.87 77.2189 1024.15 77.0754 1023.42 77.0754H1002.76C1002.03 77.0754 1001.31 77.2189 1000.64 77.4977C999.964 77.7765 999.355 78.185 998.837 78.7001C998.325 79.2151 997.915 79.8265 997.638 80.4995C997.362 81.1724 997.218 81.8929 997.218 82.6212V84.5784C997.218 85.3067 997.362 86.0278 997.638 86.7007C997.915 87.3731 998.325 87.9845 998.837 88.4996C999.355 89.0146 999.964 89.4236 1000.64 89.7024C1001.31 89.9812 1002.03 90.1247 1002.76 90.1247H1015.64V180.086C1015.64 207.164 1030.97 223.33 1056.64 223.33C1076.3 223.33 1091.94 215.054 1107.77 204.028V216.111C1107.77 216.942 1107.96 217.762 1108.32 218.511C1108.69 219.26 1109.21 219.918 1109.86 220.437L1111.25 221.554C1112.12 222.251 1113.19 222.668 1114.3 222.752C1115.41 222.836 1116.52 222.583 1117.49 222.025C1121.72 219.609 1133.11 216.902 1139.28 216.902C1140.75 216.902 1142.16 216.319 1143.19 215.28C1144.23 214.241 1144.82 212.832 1144.82 211.362V209.411C1144.82 207.942 1144.24 206.533 1143.2 205.494C1142.16 204.454 1140.76 203.868 1139.29 203.865Z\" fill=\"#F74D41\"></path> <path d=\"M1220.26 74.8466C1214.82 74.8466 1207.57 76.0063 1201.31 77.8672C1192.25 80.6701 1183.13 84.9597 1175.54 89.0192V6.64567C1175.54 5.91879 1175.4 5.19893 1175.12 4.5272C1174.84 3.85549 1174.43 3.24509 1173.92 2.73087L1172.8 1.61322C1171.98 0.795123 1170.92 0.257485 1169.77 0.0764193C1168.62 -0.104646 1167.44 0.0800354 1166.41 0.604319C1162.09 2.76712 1157.16 4.7668 1150.46 4.7668C1148.67 4.74176 1146.88 4.59035 1145.12 4.3137C1144.38 4.16654 1143.62 4.17156 1142.88 4.32845C1142.15 4.48535 1141.45 4.79083 1140.84 5.22625C1140.22 5.66162 1139.7 6.21778 1139.31 6.86071C1138.92 7.50365 1138.66 8.21986 1138.55 8.96555L1138.28 10.9229C1138.08 12.3238 1138.43 13.7463 1139.25 14.8988C1140.07 16.0512 1141.3 16.8464 1142.69 17.1214C1145.26 17.5584 1147.87 17.7828 1150.48 17.7919C1154.46 17.7765 1158.41 17.2486 1162.25 16.2212V209.411C1162.25 210.669 1162.68 211.889 1163.46 212.873L1164.58 214.269C1165.3 215.17 1166.29 215.823 1167.4 216.135C1184.53 220.969 1199.63 223.331 1213.57 223.331C1255.63 223.331 1285 191.88 1285 146.853C1284.98 100.419 1262 74.8466 1220.26 74.8466ZM1213.55 210.288C1198.74 210.288 1185.31 207.339 1175.54 204.663V103.911C1186.92 97.2659 1196.79 92.7589 1205.63 90.1432H1205.73C1210.43 88.704 1215.31 87.943 1220.23 87.8835C1254.34 87.8835 1271.64 107.723 1271.64 146.853C1271.67 184.201 1247.77 210.288 1213.55 210.288Z\" fill=\"#F74D41\"></path> </svg></a><a href=\"https://psyche.co\" target=\"_blank\" title=\"Psyche\"><svg viewBox=\"0 0 138 28\" fill=\"#025744\" stroke=\"none\" width=\"75\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-psyche\" aria-hidden=\"true\"><path stroke=\"none\" d=\"M33.4463639,1.03028697e-13 C35.4638444,1.03028697e-13 37.1784471,0.275980571 38.5905508,0.826994629 C40.0024651,1.37857694 41.1385883,2.17242359 41.999678,3.20739809 C42.7884893,4.15681924 43.3288473,5.27001901 43.620898,6.5468515 L43.6943919,6.89913545 L38.7316665,8.31123919 C38.5161099,7.28933447 38.160006,6.45533141 37.6625969,5.80979827 C37.1646196,5.16426513 36.5531815,4.69394271 35.8269567,4.39769452 C35.100732,4.10201459 34.2534697,3.95389049 33.28517,3.95389049 C31.8590494,3.95389049 30.7363748,4.25013868 29.9160098,4.84149856 C29.0952659,5.43342669 28.685557,6.29394813 28.685557,7.42363112 C28.685557,8.28472081 28.9138044,8.96359135 29.3714359,9.46100039 C29.8286886,9.95897769 30.4942999,10.3688761 31.3684594,10.6916427 C32.2424294,11.0144092 33.3253264,11.3510032 34.6165821,11.7002882 C36.7952565,12.2651297 38.5835424,12.8500494 39.9823869,13.4552367 C41.3806632,14.060424 42.4234038,14.8472622 43.1092828,15.8153725 C43.7951617,16.7838617 44.1380065,18.074928 44.1380065,19.6887608 C44.1380065,22.3250666 43.2503985,24.3688761 41.4753717,25.8213256 C39.7001556,27.2737752 37.1310928,28 33.7691304,28 C30.5679831,28 28.053283,27.3614753 26.2244619,26.0834788 C24.4782265,24.8642962 23.406295,23.1484036 23.0086672,20.9361304 L22.9564503,20.6167147 L27.9593322,19.2449568 C28.3086171,20.8322712 28.9408911,22.0223783 29.8555858,22.8154673 C30.7697122,23.6091245 32.1013137,24.0057637 33.8498221,24.0057637 C35.4094817,24.0057637 36.6268647,23.6829971 37.5012136,23.037464 C38.3749942,22.3919308 38.8123581,21.4374586 38.8123581,20.1729107 C38.8123581,19.3123892 38.60381,18.6465885 38.1869032,18.1756978 C37.7696175,17.7053754 37.1242738,17.3083573 36.2504932,16.9855908 C35.3761443,16.6628242 34.2129345,16.3135393 32.7604849,15.9365994 C31.4152454,15.5873145 30.1783523,15.1971154 29.0486693,14.7665706 C27.9189863,14.336594 26.9306084,13.8321765 26.0833462,13.2535076 C25.2358945,12.6754069 24.5835424,11.9561906 24.1264791,11.0951009 C23.6688477,10.2345794 23.4406002,9.17232888 23.4406002,7.90778098 C23.4406002,5.4870317 24.3143808,3.56425972 26.063268,2.13832853 C27.8112081,0.712965594 30.2723032,1.03028697e-13 33.4463639,1.03028697e-13 Z M80.0217694,0.564803615 C81.703603,0.564803615 83.2661038,0.796081774 84.7104085,1.25863809 C86.0096007,1.67493878 87.2353011,2.29023528 88.3869572,3.1036991 L88.7680995,3.3821921 L84.9416866,7.33494608 L84.7314338,7.33494608 C83.8063211,5.90522656 82.7971073,4.81910947 81.703603,4.07602657 C80.6104775,3.33351193 79.475112,2.96168635 78.2976959,2.96168635 C77.0920567,2.96168635 76.0616282,3.29809095 75.2069787,3.97090014 C74.3513821,4.64370933 73.6857707,5.56219237 73.2095764,6.72521276 C72.732435,7.8888014 72.4945272,9.22722193 72.4945272,10.7410426 C72.4945272,13.2364222 72.8798014,15.4020268 73.6511074,17.2378563 C74.4216558,19.0742542 75.5151601,20.4901463 76.9310522,21.4849643 C78.346376,22.4803507 79.9797189,22.9777597 81.8297547,22.9777597 C83.3153522,22.9777597 84.6819959,22.704431 85.9298751,22.1577735 C87.0808894,21.6531666 88.0354838,20.8680522 88.7932115,19.8019833 L88.9785418,19.5296126 L89.3149464,19.6557644 C88.7821163,22.3752243 87.7024394,24.4146771 86.0768627,25.7741229 C84.4507178,27.134137 82.1938142,27.8135758 79.3069097,27.8135758 C76.9520775,27.8135758 74.8495488,27.2320656 72.9991341,26.068477 C71.1490982,24.9054566 69.6983534,23.3075347 68.6470891,21.2747115 C67.5958247,19.2424565 67.0701925,16.8948222 67.0701925,14.2312403 C67.0701925,11.6241047 67.6166606,9.29029779 68.7101649,7.22981965 C69.8032904,5.16934151 71.3239301,3.5437648 73.2726522,2.35195302 C75.2208061,1.1607095 77.4705118,0.564803615 80.0217694,0.564803615 Z M53.7404825,0.943201959 L53.7404825,1.19550541 L52.8530638,4.46749469 L58.6442992,14.7876906 L63.5611858,4.46863119 L62.5288631,1.19550541 L62.5288631,0.943201959 L69.1309927,0.943201959 L69.1309927,1.19550541 L66.6106101,4.55614185 L59.6275629,17.3696337 L59.6275629,23.860765 L61.3934976,27.1827603 L61.3934976,27.4350638 L51.9743583,27.4350638 L51.9743583,27.1827603 L53.7404825,23.860765 L53.7404825,17.4272165 L46.3395814,4.4754502 L44.4893561,1.19550541 L44.4893561,0.943201959 L53.7404825,0.943201959 Z M97.7102109,0.564803615 L97.7102109,11.7002503 L108.805312,11.7002503 L108.805312,0.564803615 L114.050079,0.564803615 L114.050079,27.4351206 L108.805312,27.4351206 L108.805312,16.057599 L97.7102109,16.057599 L97.7102109,27.4351206 L92.5056,27.4351206 L92.5056,0.564803615 L97.7102109,0.564803615 Z M137.571731,0.443766151 L137.571731,5.36595635 L124.943489,5.36595635 L124.943489,11.3371379 L136.845506,11.3371379 L136.845506,16.0172532 L124.943489,16.0172532 L124.943489,22.5129304 L137.894497,22.5129304 L137.894497,27.4351206 L118.891616,27.4351206 L118.891616,0.443766151 L137.571731,0.443766151 Z M9.0778098,0.443747209 C13.0317003,0.443747209 16.0305503,1.16997199 18.074928,2.62242156 C20.1187374,4.07487113 21.1413998,6.40166958 21.1413998,9.60224865 C21.1413998,11.7544046 20.6837683,13.5095426 19.7694524,14.8672836 C18.8547577,16.2259718 17.503078,17.2276089 15.7147921,17.873142 C14.0450181,18.4756396 12.0066268,18.7969717 9.59931027,18.8371382 L9.0778098,18.8414417 L6.0518732,18.8414417 L6.0518732,27.4351017 L3.05533376e-12,27.4351017 L3.05533376e-12,0.443747209 L9.0778098,0.443747209 Z M8.87627011,5.20455413 L6.0518732,5.20455413 L6.0518732,14.0806348 L8.87627011,14.0806348 C10.1938548,14.0806348 11.3100892,13.9463382 12.2247839,13.6771766 C13.1389103,13.4085833 13.8317977,12.9509518 14.3026884,12.3054187 C14.7728214,11.6598855 15.008835,10.7590183 15.008835,9.60224865 C15.008835,8.44604727 14.7665706,7.55162019 14.2826102,6.91915682 C13.7982709,6.28745112 13.1055729,5.84383651 12.2047057,5.58793414 C11.3032701,5.3324106 10.1938548,5.20455413 8.87627011,5.20455413 Z\"></path></svg></a></div></div></header><div class=\"sc-a70232b9-0 dnIlCA\"><div class=\"sc-d9870e8f-23 ktQJNm\"></div><div class=\"sc-d9870e8f-26 jZhtJo\"><div class=\"sc-1da62608-1 ljcMFZ sc-d9870e8f-1 uZAaZ\"><div class=\"sc-3328c936-0 jkOWJA sc-1da62608-2 ghhOJi\"><div class=\"sc-3328c936-1 cSmhOU\"><img alt=\"Scarecrow figures in yellow protective suits on a desolate sandy landscape under a cloudy sky, with industrial smoke in the distance.\" fetchpriority=\"high\" decoding=\"async\" data-nimg=\"fill\" style=\"position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent\" sizes=\"(max-width: 1700px) 100vw, 1700px\" srcSet=\"https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=640&amp;quality=75&amp;format=auto 640w, https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=750&amp;quality=75&amp;format=auto 750w, https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=828&amp;quality=75&amp;format=auto 828w, https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=1080&amp;quality=75&amp;format=auto 1080w, https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=1200&amp;quality=75&amp;format=auto 1200w, https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=1920&amp;quality=75&amp;format=auto 1920w, https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=2048&amp;quality=75&amp;format=auto 2048w, https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=3840&amp;quality=75&amp;format=auto 3840w\" src=\"https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=3840&amp;quality=75&amp;format=auto\"/><div class=\"sc-3328c936-7 jnMzvZ\"><div class=\"sc-3328c936-8 dVTstl\"><p>Scarecrows keep away migratory birds from the dangers of the tailing ponds created by the exploitation on the tar sands at Fort McMurray, Alberta, Canada. <em>Photo by Larry Towell/Magnum</em></p></div><span>i</span></div></div><div class=\"sc-3328c936-3 heKKOr\"><div class=\"sc-3328c936-2 gCPQyl\"><h2 class=\"sc-3328c936-4 kmvKVn\">Against longtermism</h2><h1 class=\"sc-3328c936-5 dkTvTa\">It started as a fringe philosophical theory about humanity’s future. It’s now richly funded and increasingly dangerous</h1><p class=\"sc-3328c936-6 djYXGS\">by <!-- -->Émile P Torres<!-- --> <span class=\"toggleMobileCreditBtn\">+ BIO</span></p></div></div></div><div class=\"sc-1da62608-3 lkpQNV\"><small class=\"sc-1da62608-4 fnvdUn\"><p>Scarecrows keep away migratory birds from the dangers of the tailing ponds created by the exploitation on the tar sands at Fort McMurray, Alberta, Canada. <em>Photo by Larry Towell/Magnum</em></p></small></div></div></div><div class=\"sc-d9870e8f-0 bMtwqF\"><div class=\"sc-d9870e8f-5 gKeKzI\"><aside class=\"sc-d9870e8f-12 eWSurm\"><div aria-hidden=\"true\" class=\"rah-static rah-static--height-zero SidebarWrapper\" style=\"height:0;overflow:hidden\"><div><div class=\"sc-2e8621ab-0 jdutDe sc-d9870e8f-7 CYCPy\"><a href=\"/users/emile-p-torres\" class=\"sc-2e8621ab-1 gnJVZv\">Émile P Torres</a><span><p>is a PhD candidate in philosophy at Leibniz Universität Hannover in Germany. Their writing has appeared in <em>Philosophy Now</em>, <em>Nautilus</em>, Motherboard and the <em>Bulletin of the Atomic Scientists</em>, among others. They are the author of <em>The End: What Science and Religion Tell Us About the Apocalypse</em> (2016), <em>Morality, Foresight, and Human Flourishing: An Introduction to Existential Risks</em> (2017) and <em>Human Extinction: A History of the Science and Ethics of Annihilation</em> (forthcoming from Routledge).</p></span></div><br/><div class=\"sc-d9870e8f-9 fVeAYS\"><p>Edited by<a href=\"/users/samdresser\" class=\"sc-2e8621ab-1 gnJVZv\">Sam Dresser</a></p></div></div></div><div class=\"sc-95e65737-0 jmDajv\"><span>6,100<!-- --> words</span><span class=\"sc-95e65737-1 cIhWyi\"><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"16\" height=\"16\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-comment-bubble\" aria-hidden=\"true\"><path d=\"M21.303,16.553l1.107,3.783l-3.48-2.322c-0.689,0.234-1.422,0.371-2.193,0.371c-1.041,0-2.024-0.234-2.907-0.642 c2.034-1.697,3.333-4.275,3.333-7.166c0-1.994-0.625-3.836-1.677-5.347c0.406-0.073,0.823-0.117,1.251-0.117 c3.762,0,6.814,2.971,6.814,6.634c0,1.594-0.578,3.057-1.539,4.2L21.303,16.553z M15.255,10.684c0,3.861-3.213,6.99-7.176,6.99 c-0.812,0-1.583-0.144-2.31-0.391l-3.667,2.446l1.166-3.985l-0.744-0.638c-1.013-1.203-1.621-2.743-1.621-4.423 c0-3.858,3.215-6.987,7.176-6.987C12.042,3.696,15.255,6.825,15.255,10.684z\"></path></svg><span>85<!-- --> <!-- -->comments</span></span><span class=\"sc-95e65737-2 gjbGGR\"><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"17\" height=\"17\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-save\" aria-hidden=\"true\"><path d=\"M17 3H7c-1.1 0-1.99.9-1.99 2L5 21l7-3 7 3V5c0-1.1-.9-2-2-2zm0 15l-5-2.18L7 18V5h10v13z\"></path></svg><span>Save</span></span></div><br/><div class=\"sc-d9870e8f-8 kICVIU\"><img alt=\"curio logo\" loading=\"lazy\" width=\"100\" height=\"100\" decoding=\"async\" data-nimg=\"1\" style=\"color:transparent\" srcSet=\"/images/curio_logo_square.png?width=128&amp;quality=75&amp;format=auto 1x, /images/curio_logo_square.png?width=256&amp;quality=75&amp;format=auto 2x\" src=\"/images/curio_logo_square.png?width=256&amp;quality=75&amp;format=auto\"/><div class=\"sc-d9870e8f-19 bCLvsT\"><div class=\"sc-a4123286-0 kmfslc\"><audio preload=\"metadata\" src=\"\"></audio><div class=\"sc-a4123286-3 EQsqR\"><svg fill=\"#000000\" height=\"24\" viewBox=\"0 0 24 24\" width=\"24\"><path d=\"M0 0h24v24H0z\" fill=\"none\"></path><path d=\"M10 16.5l6-4.5-6-4.5v9zM12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 18c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8z\"></path></svg><span>Listen here</span></div></div><p>Brought to you by<!-- --> <a href=\"https://curioio.app.link/27QFqKu6TBb\" target=\"_blank\" rel=\"noreferrer\">Curio</a>, an Aeon partner<span class=\"sc-d9870e8f-20 jkIrCU\"><br/>Listen to more Aeon Essays <a href=\"/audio\">here</a></span></p></div></div><div class=\"sc-dbbed3aa-1 eiFojC\"><a class=\"sc-45649ee9-0 bhtfvC sc-dbbed3aa-0 kmtmVW\" href=\"/syndication?article_slug=why-longtermism-is-the-worlds-most-dangerous-secular-credo&amp;article_name=Against%20longtermism&amp;author=%C3%89mile%20P%20Torres&amp;date=19%20October%202021\"><span class=\"sc-45649ee9-1 cacrwE\">Syndicate this <!-- -->essay</span></a></div><div class=\"sc-c3e98e6e-0 cziXAF sc-d9870e8f-13 cRzdyJ\"><div class=\"sc-c3e98e6e-0 cziXAF\"><div><br/></div><div class=\"sc-b6dddc87-0 dAOxdm\"><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"24\" height=\"24\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-comment-bubble\" aria-hidden=\"true\"><path d=\"M21.303,16.553l1.107,3.783l-3.48-2.322c-0.689,0.234-1.422,0.371-2.193,0.371c-1.041,0-2.024-0.234-2.907-0.642 c2.034-1.697,3.333-4.275,3.333-7.166c0-1.994-0.625-3.836-1.677-5.347c0.406-0.073,0.823-0.117,1.251-0.117 c3.762,0,6.814,2.971,6.814,6.634c0,1.594-0.578,3.057-1.539,4.2L21.303,16.553z M15.255,10.684c0,3.861-3.213,6.99-7.176,6.99 c-0.812,0-1.583-0.144-2.31-0.391l-3.667,2.446l1.166-3.985l-0.744-0.638c-1.013-1.203-1.621-2.743-1.621-4.423 c0-3.858,3.215-6.987,7.176-6.987C12.042,3.696,15.255,6.825,15.255,10.684z\"></path></svg> <!-- -->85<!-- --> <!-- -->Comments</div><div class=\"sc-e2a1906f-0 bkhChg\"><div><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-email\" aria-hidden=\"true\" class=\"sc-e2a1906f-1 bXtNKb\"><path d=\"M22 6c0-1.1-.9-2-2-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6zm-2 0l-8 5-8-5h16zm0 12H4V8l8 5 8-5v10z\"></path></svg><span class=\"sc-e2a1906f-2 hJXeEE\">Email</span></div><div><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-save\" aria-hidden=\"true\" class=\"sc-e2a1906f-1 bXtNKb\"><path d=\"M17 3H7c-1.1 0-1.99.9-1.99 2L5 21l7-3 7 3V5c0-1.1-.9-2-2-2zm0 15l-5-2.18L7 18V5h10v13z\"></path></svg><span class=\"sc-e2a1906f-2 hJXeEE\">Save</span></div><div><svg viewBox=\"0 0 450 450\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-twitter\" aria-hidden=\"true\" class=\"sc-e2a1906f-1 bXtNKb\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M225 450C349.264 450 450 349.264 450 225C450 100.736 349.264 0 225 0C100.736 0 0 100.736 0 225C0 349.264 100.736 450 225 450ZM349.373 92L250.332 204.646L358 358H278.785L206.256 254.71L115.461 358H92L195.855 239.888L92 92H171.215L239.878 189.789L325.912 92H349.373ZM159.972 109.311H123.929L289.842 341.539H325.894L159.972 109.311Z\"></path></svg><span class=\"sc-e2a1906f-2 hJXeEE\">Post</span></div><div><svg viewBox=\"0 0 1024 1024\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-facebook\" aria-hidden=\"true\" class=\"sc-e2a1906f-1 bXtNKb\"><g><path d=\"M1024,512C1024,229.23,794.77,0,512,0S0,229.23,0,512c0,255.55,187.23,467.37,432,505.78V660H302V512H432V399.2C432,270.88,508.44,200,625.39,200c56,0,114.61,10,114.61,10V336H675.44c-63.6,0-83.44,39.47-83.44,80v96H734L711.3,660H592v357.78C836.77,979.37,1024,767.55,1024,512Z\"></path><path d=\"M711.3,660,734,512H592V416c0-40.49,19.84-80,83.44-80H740V210s-58.59-10-114.61-10C508.44,200,432,270.88,432,399.2V512H302V660H432v357.78a517.58,517.58,0,0,0,160,0V660Z\" fill=\"transparent\"></path></g></svg><span class=\"sc-e2a1906f-2 hJXeEE\">Share</span></div></div></div></div></aside><div class=\"sc-8824d2be-1 cKyFvK sc-d9870e8f-10 ceECyG\" id=\"article-content\" data-highlightable-root=\"true\"><div class=\"sc-8824d2be-3 gDwTPy has-dropcap\"><p>There seems to be a growing recognition that humanity might be approaching the ‘end times’. Dire predictions of catastrophe clutter the news. Social media videos of hellish wildfires, devastating floods and hospitals overflowing with <span class=\"ld-nowrap\">COVID-19</span> patients dominate our timelines. Extinction Rebellion activists are shutting down cities in a desperate attempt to save the world. One <a href=\"https://ro.uow.edu.au/cgi/viewcontent.cgi?article=1742&amp;context=buspapers\" target=\"_blank\" rel=\"noreferrer noopener\">survey</a> even found that more than half of the people asked about humanity’s future ‘rated the risk of our way of life ending within the next <span class=\"ld-nowrap\">100 years</span> at <span class=\"ld-nowrap\">50 per</span> cent or greater.’</p>\n<p>‘Apocalypticism’, or the belief that the end times are imminent, is of course nothing new: people have warned that the end is nigh for millennia, and in fact many New Testament scholars believe that Jesus himself expected the world to end during his own lifetime. But the situation today is fundamentally different than in the past. The ‘eschatological’ scenarios now being discussed are based not on the revelations of religious prophets, or secular metanarratives of human history (as in the case of Marxism), but on robust scientific conclusions defended by leading experts in fields such as climatology, ecology, epidemiology and so on.</p>\n<p>We know, for example, that climate change poses a dire threat to civilisation. We know that biodiversity loss and the sixth mass extinction could precipitate sudden, irreversible, catastrophic shifts in the global ecosystem. A thermonuclear exchange could blot out the Sun for years or decades, bringing about the collapse of global agriculture. And whether or not SARS-<span class=\"ld-nowrap\">CoV-2</span> came from a Wuhan laboratory or was cooked up in the kitchen of nature (the latter seems more <a href=\"https://www.medicalnewstoday.com/articles/covid-19-us-intelligence-rules-out-biological-weapon-origin\" target=\"_blank\" rel=\"noreferrer noopener\">probable</a> right now), synthetic biology will soon <a href=\"https://docs.wixstatic.com/ugd/d9aaad_4d3e08f426904b8c8be516230722087a.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">enable</a> bad actors to design pathogens far more lethal and contagious than anything Darwinian evolution could possibly invent. Some philosophers and scientists have also begun sounding the <a href=\"https://docs.wixstatic.com/ugd/d9aaad_b2e7f0f56bec40a195e551dd3e8c878e.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">alarm</a> about ‘emerging threats’ associated with machine superintelligence, molecular nanotechnology and stratospheric geoengineering, which look no less formidable.</p>\n<p>Such considerations have led many scholars to acknowledge that, as Stephen Hawking wrote in <em>The Guardian</em> in 2016, ‘we are at the most dangerous moment in the development of humanity.’ Lord Martin Rees, for example, estimates that civilisation has a 50/50 chance of making it to 2100. Noam Chomsky <a href=\"https://global.ilmanifesto.it/chomsky-republicans-are-a-danger-to-the-human-species/\" target=\"_blank\" rel=\"noreferrer noopener\">argues</a> that the risk of annihilation is currently ‘unprecedented in the history of <em>Homo sapiens</em>’. And Max Tegmark contends that ‘it’s probably going to be within our lifetimes … that we’re either going to self-destruct or get our act together.’ Consistent with these dismal declarations, the <em>Bulletin of the Atomic Scientists</em> in 2020 <a href=\"https://thebulletin.org/doomsday-clock/2020-doomsday-clock-statement/\" target=\"_blank\" rel=\"noreferrer noopener\">set</a> its iconic Doomsday Clock to a mere 100 seconds before midnight (or doom), the closest it’s been since the clock was created in 1947, and more than 11,000 scientists from around the world signed an <a href=\"https://hal.archives-ouvertes.fr/hal-02397151/document\" target=\"_blank\" rel=\"noreferrer noopener\">article</a> in 2020 stating ‘clearly and unequivocally that planet Earth is facing a climate emergency’, and without ‘an immense increase of scale in endeavours to conserve our biosphere [we risk] untold suffering due to the climate crisis.’ As the young climate activist Xiye Bastida summed up this existential mood in a <em>Teen Vogue</em> interview in 2019, the aim is to ‘make sure that we’re not the last generation’, because this now appears to be a very real possibility.</p>\n<p>Given the unprecedented dangers facing humanity today, one might expect philosophers to have spilled a considerable amount of ink on the ethical implications of our extinction, or related scenarios such as the permanent collapse of civilisation. How morally bad (or <a href=\"https://aeon.co/essays/having-children-is-not-life-affirming-its-immoral\" target=\"_blank\" rel=\"noopener\">good</a>) would our disappearance be, and for what reasons? Would it be wrong to prevent future generations from coming into existence? Does the value of past sacrifices, struggles and strivings depend on humanity continuing to exist for as long as Earth, or the Universe more generally, remains habitable?</p>\n<p>Yet this is not the case: the topic of our extinction has received little sustained attention from philosophers until recently, and even now remains at the fringe of philosophical discussion and debate. On the whole, they have been preoccupied with other matters. However, there is one notable exception to this rule: over the past two decades, a small group of theorists mostly based in Oxford have been busy working out the details of a new moral worldview called longtermism, which <a href=\"https://forum.effectivealtruism.org/posts/qZyshHCNkjs3TvSem/longtermism\" target=\"_blank\" rel=\"noreferrer noopener\">emphasizes</a> how our actions affect the very long-term future of the universe – thousands, millions, billions, and even trillions of years from now. This has roots in the work of <a href=\"https://aeon.co/essays/will-humans-be-around-in-a-billion-years-or-a-trillion\" target=\"_blank\" rel=\"noopener\">Nick Bostrom</a>, who founded the grandiosely named Future of Humanity Institute (FHI) in 2005, and Nick Beckstead, a research associate at FHI and a programme officer at Open Philanthropy. It has been defended most publicly by the FHI philosopher Toby Ord, author of <em>The Precipice: Existential Risk and the Future of Humanity</em> (2020). Longtermism is the primary research focus of both the Global Priorities Institute (GPI), an FHI-linked organisation directed by Hilary Greaves, and the Forethought Foundation, run by William MacAskill, who also holds positions at FHI and GPI. Adding to the tangle of titles, names, institutes and acronyms, longtermism is one of the main ‘cause areas’ of the so-called effective altruism (EA) movement, which was introduced by Ord in around 2011 and now <a href=\"https://80000hours.org/2021/07/effective-altruism-growing/\" target=\"_blank\" rel=\"noreferrer noopener\">boasts</a> of having a mind-boggling <span class=\"ld-nowrap\">$46 billion</span> in committed funding.</p>\n<p>It is difficult to overstate how influential longtermism has become. Karl Marx in 1845 declared that the point of philosophy isn’t merely to interpret the world but change it, and this is exactly what longtermists have been doing, with extraordinary success. Consider that <a href=\"https://aeon.co/essays/elon-musk-puts-his-case-for-a-multi-planet-civilisation\" target=\"_blank\" rel=\"noopener\">Elon Musk</a>, who has cited and endorsed Bostrom’s work, has donated $1.5 million dollars to FHI through its sister organisation, the even more grandiosely named Future of Life Institute (FLI). This was cofounded by the multimillionaire tech entrepreneur Jaan Tallinn, who, as I recently <a href=\"https://www.currentaffairs.org/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk?fbclid=IwAR1zhM1QqoEqF1bhNicGcZ5la7wGp3Q4hU0t9ytfbM2gBGrhOjGhFOl-NC8\" target=\"_blank\" rel=\"noreferrer noopener\">noted</a>, doesn’t believe that climate change poses an ‘existential risk’ to humanity because of his adherence to the longtermist ideology.</p>\n<p>Meanwhile, the billionaire libertarian and Donald Trump supporter Peter Thiel, who once gave the keynote address at an EA conference, has donated large sums of money to the Machine Intelligence Research Institute, whose mission to save humanity from superintelligent machines is deeply intertwined with longtermist values. Other organisations such as GPI and the Forethought Foundation are funding essay contests and scholarships in an effort to draw young people into the community, while it’s an open secret that the Washington, DC-based Center for Security and Emerging Technologies (CSET) aims to place longtermists within high-level US government positions to shape national policy. In fact, CSET was established by Jason Matheny, a former research assistant at FHI who’s now the deputy assistant to US President Joe Biden for technology and national security. Ord himself has, astonishingly for a philosopher, ‘advised the World Health Organization, the World Bank, the World Economic Forum, the US National Intelligence Council, the UK Prime Minister’s Office, Cabinet Office, and Government Office for Science’, and he recently <a href=\"https://forum.effectivealtruism.org/posts/Fwu2SLKeM5h5v95ww/major-un-report-discusses-existential-risk-and-future%23Context\" target=\"_blank\" rel=\"noreferrer noopener\">contributed</a> to a report from the Secretary-General of the United Nations that specifically mentions ‘long-termism’.</p>\n<p>The point is that longtermism might be one of the most influential ideologies that few people outside of elite universities and Silicon Valley have ever heard about. I believe this needs to change because, as a former longtermist who published an entire <a href=\"https://www.amazon.com/Morality-Foresight-Human-Flourishing-Introduction/dp/1634311426/ref=sr_1_1?dchild=1&amp;keywords=morality+foresight+torres&amp;qid=1625225082&amp;sr=8-1\" target=\"_blank\" rel=\"noreferrer noopener\">book</a> four years ago in defence of the general idea, I have come to see this worldview as quite possibly the most dangerous secular belief system in the world today. But to understand the nature of the beast, we need to first dissect it, examining its anatomical features and physiological functions.</p>\n<p><span class=\"ld-dropcap\">T</span>he initial thing to notice is that longtermism, as proposed by Bostrom and Beckstead, is not equivalent to ‘caring about the long term’ or ‘valuing the wellbeing of future generations’. It goes way beyond this. At its core is a simple – albeit flawed, in my opinion – analogy between individual persons and humanity as a whole. To illustrate the idea, consider the case of <a href=\"https://aeon.co/essays/what-is-truth-on-ramsey-wittgenstein-and-the-vienna-circle\" target=\"_blank\" rel=\"noopener\">Frank Ramsey</a>, a scholar at the University of Cambridge widely considered by his peers as among his generation’s most exceptional minds. ‘There was something of Newton about him,’ the belletrist Lytton Strachey once said. <span class=\"ld-nowrap\">G E Moore</span> wrote of Ramsey’s ‘very exceptional brilliance’. And John Maynard Keynes described a paper of Ramsey’s as ‘one of the most remarkable contributions to mathematical economics ever made’.</p>\n<p>But Ramsey’s story isn’t a happy one. On <span class=\"ld-nowrap\">19 January</span> 1930, he died in a London hospital following a surgical procedure, the likely cause of death being a liver infection from swimming in the River Cam, which winds its way through Cambridge. Ramsey was only <span class=\"ld-nowrap\">26 years</span> old.</p>\n<p>One could argue that there are two distinct reasons this outcome was tragic. The first is the most obvious: it cut short Ramsey’s life, depriving him of everything he could have experienced had he survived – the joys and happiness, the love and friendship: all that makes life worth living. In this sense, Ramsey’s early demise was a personal tragedy. But, secondly, his death also robbed the world of an intellectual superstar apparently destined to make even more extraordinary contributions to human knowledge. ‘The number of trails Ramsey laid was remarkable,’ writes Sir Partha Dasgupta. But how many more trails might he have blazed? ‘The loss to your generation is agonising to think of,’ Strachey lamented, ‘what a light has gone out’ – which leaves one wondering how Western intellectual history might have been different if Ramsey hadn’t died so young. From this perspective, one could argue that, although the personal tragedy of Ramsey’s death was truly terrible, the immensity of his potential to have changed the world for the better makes the second tragedy even worse. In other words, the badness of his death stems mostly, perhaps overwhelmingly, from his unfulfilled potential rather than the direct, personal harms that he experienced. Or so the argument goes.</p>\n<p>Longtermists would map these claims and conclusions on to humanity itself, as if humanity is an individual with its very own ‘potential’ to squander or fulfil, ruin or realise, over the course of ‘its lifetime’. So, on the one hand, a catastrophe that reduces the human population to zero would be tragic because of all the suffering it would inflict upon those alive at the time. Imagine the horror of starving to death in subfreezing temperatures, under pitch-black skies at noon, for years or decades after a thermonuclear war. This is the first tragedy, a personal tragedy for those directly affected. But there is, longtermists would argue, a second tragedy that is astronomically worse than the first, arising from the fact that our extinction would permanently foreclose what could be an extremely long and prosperous future over the next, say, ~10<sup class=\"ld-superscript\">100</sup> years (at which point the ‘heat death’ will <a href=\"https://aeon.co/essays/welcome-to-earth-2200-ad-pop-500-million-temp-180-f\" target=\"_blank\" rel=\"noopener\">make</a> life impossible). In doing this, it would irreversibly destroy the ‘vast and glorious’ longterm potential of humanity, in Ord’s almost religious language – a ‘potential’ so huge, given the size of the Universe and the time left before reaching thermodynamic equilibrium, that the first tragedy would utterly pale in comparison.</p>\n<p>This immediately suggests another parallel between individuals and humanity: death isn’t the only way that someone’s potential could be left unfulfilled. Imagine that Ramsey hadn’t died young but, instead of studying, writing and publishing scholarly papers, he’d spent his days in the local bar playing pool and drinking. Same outcome, different failure mode. Applying this to humanity, longtermists would argue that there are failure modes that could leave our potential unfulfilled without us dying out, which I will return to below.</p>\n<p class=\"pullquote\">On this view, a climate catastrophe will be a small blip – like a 90-year-old who stubbed his toe when he was two</p>\n<p>To summarise these ideas so far, humanity has a ‘potential’ of its own, one that transcends the potentials of each individual person, and failing to realise this potential would be extremely bad – indeed, as we will see, a moral catastrophe of literally cosmic proportions. This is the central dogma of longtermism: nothing matters more, ethically speaking, than fulfilling our potential as a species of ‘Earth-originating intelligent life’. It matters so much that longtermists have even <a href=\"https://nickbostrom.com/existential/risks.html\" target=\"_blank\" rel=\"noreferrer noopener\">coined</a> the scary-sounding term ‘existential risk’ for any possibility of our potential being destroyed, and ‘existential catastrophe’ for any event that actually destroys this potential.</p>\n<p>Why do I think this ideology is so dangerous? The short answer is that elevating the fulfilment of humanity’s supposed potential above all else could nontrivially increase the probability that actual people – those alive today and in the near future – suffer extreme harms, even death. Consider that, as I noted elsewhere, the longtermist ideology inclines its adherents to take an insouciant attitude towards climate change. Why? Because even if climate change causes island nations to disappear, triggers mass migrations and kills millions of people, it probably isn’t going to compromise our longterm potential over the coming trillions of years. If one takes a cosmic view of the situation, even a climate catastrophe that cuts the human population by <span class=\"ld-nowrap\">75 per</span> cent for the next two millennia will, in the grand scheme of things, be nothing more than a small blip – the equivalent of a 90-year-old man having stubbed his toe when he was two.</p>\n<p>Bostrom’s <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.304.7392&amp;rep=rep1&amp;type=pdf\" target=\"_blank\" rel=\"noreferrer noopener\">argument</a> is that ‘a non-existential disaster causing the breakdown of global civilisation is, from the perspective of humanity as a whole, a potentially recoverable setback.’ It might be ‘a giant massacre for man’, he adds, but so long as humanity bounces back to fulfil its potential, it will ultimately register as little more than ‘a small misstep for mankind’. Elsewhere, he writes that the worst natural disasters and devastating atrocities in history become almost imperceptible trivialities when seen from this grand perspective. Referring to the two world wars, AIDS and the Chernobyl nuclear accident, he declares that ‘tragic as such events are to the people immediately affected, in the big picture of things … even the worst of these catastrophes are mere ripples on the surface of the great sea of life.’</p>\n<p>This way of seeing the world, of assessing the badness of AIDS and the Holocaust, implies that future disasters of the same (non-existential) scope and intensity should also be categorised as ‘mere ripples’. If they don’t pose a direct existential risk, then we ought not to worry much about them, however tragic they might be to individuals. As Bostrom <a href=\"https://www.nickbostrom.com/astronomical/waste.html\" target=\"_blank\" rel=\"noreferrer noopener\">wrote</a> in 2003, ‘priority number one, two, three and four should … be to reduce existential risk.’ He <a href=\"https://www.existential-risk.org/concept.html\" target=\"_blank\" rel=\"noreferrer noopener\">reiterated</a> this several years later in arguing that we mustn’t ‘fritter … away’ our finite resources on ‘feel-good projects of suboptimal efficacy’ such as alleviating global poverty and reducing animal suffering, since neither threatens our longterm potential, and our longterm potential is what really matters.</p>\n<p>Ord echoes these views in arguing that, of all the problems facing humanity, our ‘first great task … is to reach a place of safety – a place where existential risk’ – as he defines it – ‘is low and stays low’, which he dubs ‘existential security’. More than anything else, what matters is doing everything necessary to ‘preserve’ and ‘protect’ our potential by ‘extracting ourselves from immediate danger’ and devising robust ‘safeguards that will defend humanity from dangers over the longterm future, so that it becomes impossible to fail.’ Although Ord gives a nod to climate change, he also claims – based on a dubious methodology – that the chance of climate change causing an existential catastrophe is only ∼1 in 1,000, which is a whole two orders of magnitude lower than the probability of superintelligent machines destroying humanity this century, according to Ord.</p>\n<p>What’s really notable here is that the central concern isn’t the effect of the climate catastrophe on actual people around the world (remember, in the grand scheme, this would be, in Bostrom’s words, a ‘small misstep for mankind’) but the slim possibility that, as Ord puts it in <em>The Precipice</em>, this catastrophe ‘poses a risk of an unrecoverable collapse of civilisation or even the complete extinction of humanity’. Again, the harms caused to actual people (especially those in the Global South) might be significant in absolute terms, but when compared to the ‘vastness’ and ‘glory’ of our longterm potential in the cosmos, they hardly even register.</p>\n<p><span class=\"ld-dropcap\">Y</span>et the implications of longtermism are far more worrisome. If our top four priorities are to avoid an existential catastrophe – ie, to fulfil ‘our potential’ – then what’s not on the table for making this happen? Consider <a href=\"https://aeon.co/essays/what-s-it-all-for-is-a-question-that-belongs-in-the-past\" target=\"_blank\" rel=\"noopener\">Thomas Nagel</a>’s comment about how the notion of what we might call the ‘greater good’ has been used to ‘justify’ certain atrocities (eg, during war). If the ends ‘justify’ the means, he argues, and the ends are thought to be sufficiently large (eg, national security), then this ‘can be brought to bear to ease the consciences of those responsible for a certain number of charred babies’. Now imagine what might be ‘justified’ if the ‘greater good’ isn’t national security but the cosmic potential of Earth-originating intelligent life over the coming trillions of years? During the Second World War, <span class=\"ld-nowrap\">40 million</span> civilians perished, but compare this number to the 10<sup class=\"ld-superscript\">54</sup> or more people (in Bostrom’s <a href=\"https://www.existential-risk.org/faq.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">estimate</a>) who could come to exist if we can avoid an existential catastrophe. What shouldn’t we do to ‘protect’ and ‘preserve’ this potential? To ensure that these unborn people come to exist? What means can’t be ‘justified’ by this cosmically significant moral end?</p>\n<p>Bostrom himself <a href=\"https://www.nickbostrom.com/papers/vulnerable.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">argued</a> that we should seriously consider establishing a global, invasive surveillance system that monitors every person on the planet in realtime, to amplify the ‘capacities for preventive policing’ (eg, to prevent omnicidal terrorist attacks that could devastate civilisation). Elsewhere, he’s written that states should use preemptive violence/war to avoid existential catastrophes, and argued that saving billions of actual people is the moral equivalent of reducing existential risk by utterly minuscule amounts. In his words, even if there is ‘a mere <span class=\"ld-nowrap\">1 per </span>cent chance’ of 10<sup class=\"ld-superscript\">54</sup> people existing in the future, then ‘the expected value of reducing existential risk by a mere <em>one billionth of one billionth of one percentage point</em> is worth <span class=\"ld-nowrap\">100 billion</span> times as much as a billion human lives.’ Such fanaticism – a word that some longtermists <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/Hayden-Wilkinson_In-defence-of-fanaticism.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">embrace</a> – has led a growing number of critics to worry about what might happen if political leaders in the real world were to take Bostrom’s view seriously. To <a href=\"https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780198723547.001.0001/acprof-9780198723547\" target=\"_blank\" rel=\"noreferrer noopener\">quote</a> the mathematical statistician Olle Häggström, who – perplexingly – tends otherwise to speak favourably of longtermism:</p>\n<blockquote>I feel extremely uneasy about the prospect that [the calculations above] might become recognised among politicians and decision-makers as a guide to policy worth taking literally. It is simply too reminiscent of the old saying ‘If you want to make an omelette, you must be willing to break a few eggs,’ which has typically been used to explain that a bit of genocide or so might be a good thing, if it can contribute to the goal of creating a future utopia. Imagine a situation where the head of the CIA explains to the US president that they have credible evidence that somewhere in Germany, there is a lunatic who is working on a doomsday weapon and intends to use it to wipe out humanity, and that this lunatic has a one-in-a-million chance of succeeding. They have no further information on the identity or whereabouts of this lunatic. If the president has taken Bostrom’s argument to heart, and if he knows how to do the arithmetic, he may conclude that it is worthwhile conducting a full-scale nuclear assault on Germany to kill every single person within its borders.</blockquote>\n<p>Here, then, are a few reasons I find longtermism to be profoundly dangerous. Yet there are additional, fundamental problems with this worldview that no one, to my knowledge, has previously noted in writing. For example, there’s a good case to make that the underlying commitments of longtermism are a major reason why humanity faces so many unprecedented risks to its survival in the first place. Longtermism might, in other words, be incompatible with the attainment of ‘existential security’, meaning that the only way to genuinely reduce the probability of extinction or collapse in the future might be to abandon the longtermist ideology entirely.</p>\n<p class=\"pullquote\">To Bostrom and Ord, failing to become posthuman would prevent us from realising our vast, glorious potential</p>\n<p>To understand the argument, let’s first unpack what longtermists mean by our ‘longterm potential’, an expression that I have so far used without defining. We can analyse this concept into three main components: transhumanism, space expansionism, and a moral view closely associated with what philosophers call ‘total utilitarianism’.</p>\n<p>The first refers to the <a href=\"https://aeon.co/essays/why-is-the-language-of-transhumanists-and-religion-so-similar\" target=\"_blank\" rel=\"noopener\">idea</a> that we should use advanced technologies to reengineer our bodies and brains to create a ‘superior’ race of radically enhanced posthumans (which, confusingly, longtermists place within the category of ‘humanity’). Although Bostrom is perhaps the most prominent transhumanist today, longtermists have shied away from using the term ‘transhumanism’, probably because of its negative associations. Susan Levin, for example, <a href=\"https://blog.oup.com/2021/01/playing-to-lose-transhumanism-autonomy-and-liberal-democracy-long-read/\" target=\"_blank\" rel=\"noreferrer noopener\">points out</a> that contemporary transhumanism has its roots in the Anglo-American eugenics movement, and transhumanists such as <a href=\"https://aeon.co/users/julian-savulescu\" target=\"_blank\" rel=\"noopener\">Julian Savulescu</a>, who co-edited the <a href=\"https://global.oup.com/academic/product/human-enhancement-9780199299720?cc=gb&amp;lang=en&amp;\" target=\"_blank\" rel=\"noreferrer noopener\">book</a> <em>Human Enhancement</em> (2009) with Bostrom, have literally argued for the consumption of ‘morality-boosting’ chemicals such as oxytocin to avoid an existential catastrophe (which he <a href=\"https://jme.bmj.com/content/37/7/441\" target=\"_blank\" rel=\"noreferrer noopener\">calls</a> ‘ultimate harm’). As Savulescu <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-8519.2011.01907.x\" target=\"_blank\" rel=\"noreferrer noopener\">writes</a> with a colleague, ‘it is a matter of such urgency to improve humanity morally … that we should seek <em>whatever</em> means there are to effect this.’ Such claims are not only controversial but for many quite disturbing, and hence longtermists have attempted to distance themselves from such ideas, while nonetheless championing the ideology.</p>\n<p>Transhumanism <a href=\"https://www.nickbostrom.com/posthuman.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">claims</a> that there are various ‘posthuman modes of being’ that are far better than our current human mode. We could, for instance, genetically alter ourselves to gain perfect control over our emotions, or access the internet via neural implants, or maybe even upload our minds to computer hardware to achieve ‘digital immortality’. As Ord urges in <em>The Precipice</em>, think of how awesome it would be to perceive the world via echolocation, like bats and dolphins, or magnetoreception, like red foxes and homing pigeons. ‘Such uncharted experiences,’ Ord writes, ‘exist in minds much less sophisticated than our own. What experiences, possibly of immense value, could be accessible, then, to minds much greater?’ Bostrom’s most fantastical exploration of these possibilities comes from his <a href=\"https://www.nickbostrom.com/utopia.html\" target=\"_blank\" rel=\"noreferrer noopener\">evocative</a> ‘Letter from Utopia’ (2008), which depicts a techno-Utopian world full of superintelligent posthumans awash in so much ‘pleasure’ that, as the letter’s fictional posthuman writes, ‘we sprinkle it in our tea.’</p>\n<p>The connection with longtermism is that, according to Bostrom and Ord, failing to become posthuman would seemingly prevent us from realising our vast and glorious potential, which would be existentially catastrophic. As Bostrom put it in 2012, ‘the permanent foreclosure of any possibility of this kind of transformative change of human biological nature may itself constitute an existential catastrophe.’ Similarly, Ord asserts that ‘forever preserving humanity as it is now may also squander our legacy, relinquishing the greater part of our potential.’</p>\n<p><span class=\"ld-dropcap\">T</span>he second component of our potential – space expansionism – refers to the idea that we must colonise as much of our future light cone as possible: that is, the region of spacetime that is theoretically accessible to us. According to longtermists, our future light cone contains a huge quantity of exploitable resources, which they refer to as our ‘cosmic endowment’ of negentropy (or reverse entropy). The Milky Way alone, Ord writes, is ‘150,000 light years across, encompassing more than <span class=\"ld-nowrap\">100 billion</span> stars, most with their own planets.’ Attaining humanity’s longterm potential, he continues, ‘requires only that [we] eventually travel to a nearby star and establish enough of a foothold to create a new flourishing society from which we could venture further.’ By spreading ‘just six light years at a time’, our posthuman descendants could make ‘almost all the stars of our galaxy … reachable’ since ‘each star system, including our own, would need to settle just the few nearest stars [for] the entire galaxy [to] eventually fill with life.’ The process could be exponential, resulting in ever-more ‘flourishing’ societies with each additional second our descendants hop from star to star.</p>\n<p>But why exactly would we want to do this? What’s so important about flooding the Universe with new posthuman civilisations? This leads to the third component: total utilitarianism, which I will refer to as ‘utilitarianism’ for short. Although some longtermists insist that they aren’t utilitarians, we should right away note that this is mostly a smoke-and-mirrors act to deflect criticisms that longtermism – and, more generally, the effective altruism (EA) movement from which it emerged – is nothing more than utilitarianism <a href=\"https://blog.apaonline.org/2021/03/29/is-effective-altruism-inherently-utilitarian/\" target=\"_blank\" rel=\"noreferrer noopener\">repackaged</a>. The fact is that the EA movement is deeply utilitarian, at least in practice, and indeed, before it decided upon a name, the movement’s early members, including Ord, seriously considered calling it the ‘effective utilitarian community’.</p>\n<p>This being said, utilitarianism is an ethical theory that specifies our sole moral obligation as being to maximise the total amount of ‘intrinsic value’ in the world, as tallied up from a disembodied, impartial, cosmic vantage point called ‘the point of view of the Universe’. From this view, it doesn’t matter how value – which utilitarian hedonists equate with pleasure – is distributed among people across space and time. All that matters is the total net sum. For example, imagine that there are <span class=\"ld-nowrap\">1 trillion</span> people who have lives of value ‘1’, meaning that they are just barely worth living. This gives a total value of <span class=\"ld-nowrap\">1 trillion</span>. Now consider an alternative universe in which <span class=\"ld-nowrap\">1 billion</span> people have lives with a value of ‘999’, meaning that their lives are extremely good. This gives a total value of <span class=\"ld-nowrap\">999 billion.</span> Since <span class=\"ld-nowrap\">999 billion</span> is less than <span class=\"ld-nowrap\">1 trillion,</span> the first world full of lives hardly worth living would be morally better than the second world, and hence, if a utilitarian were forced to choose between these, she would pick the former. (This is called the ‘repugnant conclusion’, which longtermists such as Ord, MacAskill and Greaves recently <a href=\"https://www.cambridge.org/core/journals/utilitas/article/what-should-we-agree-on-about-the-repugnant-conclusion/EB52C686BAFEF490CE37043A0A3DD075\" target=\"_blank\" rel=\"noreferrer noopener\">argued</a> shouldn’t be taken very seriously. For them, the first world really might be better!)</p>\n<p class=\"pullquote\">Beckstead argued that we should prioritise the lives of people in rich countries over those in poor countries</p>\n<p>The underlying reasoning here is based on the idea that people – you and I – are nothing more than means to an end. We don’t matter in ourselves; we have no inherent value of our own. Instead, people are understood as the ‘containers’ of value, and hence we matter only insofar as we ‘contain’ value, and therefore contribute to the overall net amount of value in the Universe between the Big Bang and the heat death. Since utilitarianism tells us to maximise value, it follows that the more people (value containers) who exist with net-positive amounts of value (pleasure), the better the Universe will become, morally speaking. In a phrase: people exist for the sake of maximising value, rather than value existing for the sake of benefitting people.</p>\n<p>This is why longtermists are obsessed with calculating how many people could exist in the future if we were to colonise space and create vast computer simulations around stars in which unfathomably huge numbers of people live net-positive lives in virtual-reality environments. I already mentioned Bostrom’s estimate of 10<sup class=\"ld-superscript\">54</sup> future people, which includes many of these ‘digital people’, but in his <a href=\"https://global.oup.com/academic/product/superintelligence-9780199678112?cc=gb&amp;lang=en&amp;\" target=\"_blank\" rel=\"noreferrer noopener\">bestseller</a> <em>Superintelligence</em> (2014) he puts the number even higher at 10<sup class=\"ld-superscript\">58</sup> people, nearly all of whom would ‘live rich and happy lives while interacting with one another in virtual environments’. Greaves and MacAskill are similarly excited about this possibility, <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">estimating</a> that some 10<sup class=\"ld-superscript\">45</sup> conscious beings in computer simulations could exist within the Milky Way alone.</p>\n<p>That is what our ‘vast and glorious’ potential consists of: massive numbers of technologically enhanced digital posthumans inside huge computer simulations spread throughout our future light cone. It is for this goal that, in Häggström’s scenario, a longtermist politician would annihilate Germany. It is for this goal that we must not ‘fritter … away’ our resources on such things as solving global poverty. It is for this goal that we should consider implementing a global surveillance system, keep pre-emptive war on the table, and focus more on superintelligent machines than saving people in the Global South from the devastating effects of climate change (mostly caused by the Global North). In fact, Beckstead has even argued that, for the sake of attaining this goal, we should actually prioritise the lives of people in rich countries over those in poor countries, since influencing the long-term future is of ‘overwhelming importance’, and the former are more likely to influence the long-term future than the latter. To <a href=\"https://www.proquest.com/docview/1442191960?fromopenview=tr%2522a%2520long%2520period%25E2%2580%2594perhaps%2520tens%2520of%2520thousands%2520of%2520years%25E2%2580%2594during%2520which%2520human%2520civilisation,%2520perhaps%2520with%2520the%2520aid%2520of%2520improved%2520cognitive%2520ability,%2520dedicates%2520itself%2520to%2520working%2520out%2520what%2520is%2520ultimately%2520of%2520value%2522ue&amp;pq-origsite=gscholar\" target=\"_blank\" rel=\"noreferrer noopener\">quote</a> a passage from Beckstead’s 2013 PhD dissertation, which Ord enthusiastically praises as one of the most important contributions to the longtermist literature:</p>\n<blockquote>Saving lives in poor countries may have significantly smaller ripple effects than saving and improving lives in rich countries. Why? Richer countries have substantially more innovation, and their workers are much more economically productive. [Consequently,] it now seems more plausible to me that saving a life in a rich country is substantially more important than saving a life in a poor country, other things being equal.</blockquote>\n<p><span class=\"ld-dropcap\">T</span>his is just the tip of the iceberg. Consider the implications of this conception of ‘our potential’ for the development of technology and creation of new risks. Since realising our potential is the ultimate moral goal for humanity, and since our descendants cannot become posthuman, colonise space and create ~10<sup class=\"ld-superscript\">58</sup> people in computer simulations without technologies far more advanced than those around today, failing to develop more technology would itself constitute an existential catastrophe – a failure mode (comparable to Ramsey neglecting his talents by spending his days playing pool and drinking) that Bostrom calls ‘plateauing’. Indeed, Bostrom places this idea front-and-centre in his canonical definition of ‘existential risk’, which denotes any future event that would prevent humanity from reaching and/or sustaining a state of ‘technological maturity’, meaning ‘the attainment of capabilities affording a level of economic productivity and control over nature close to the maximum that could feasibly be achieved.’ Technological maturity is the linchpin here because controlling nature and increasing economic productivity to the absolute physical limits are ostensibly necessary for creating the maximum quantity of ‘value’ within our future light cone.</p>\n<p>But reflect for a moment on how humanity got itself into the current climatic and ecological crisis. Behind the extraction and burning of fossil fuels, decimation of ecosystems and extermination of species has been the notion that nature is something to be controlled, subjugated, exploited, vanquished, plundered, transformed, reconfigured and manipulated. As the technology theorist Langdon Winner <a href=\"https://mitpress.mit.edu/books/autonomous-technology\" target=\"_blank\" rel=\"noreferrer noopener\">writes</a> in <em>Autonomous Technology</em> (1977), since the time of Francis Bacon our view of technology has been ‘inextricably bound to a single conception of the manner in which power is used – the style of absolute mastery, the despotic, one-way control of the master over the slave.’ He adds:</p>\n<blockquote>There are seldom any reservations about man’s rightful role in conquering, vanquishing, and subjugating everything natural. This is his power and his glory. What would in other situations seem [to be] rather tawdry and despicable intentions are here the most honourable of virtues. Nature is the universal prey, to manipulate as humans see fit.</blockquote>\n<p>This is precisely what we find in Bostrom’s account of existential risks and its associated normative futurology: nature, the entire Universe, our ‘cosmic endowment’ is there for the plundering, to be manipulated, transformed and converted into ‘value-structures, such as sentient beings living worthwhile lives’ in vast computer simulations, quoting Bostrom’s essay ‘Astronomical Waste’ (2003). Yet this Baconian, capitalist view is one of the most fundamental root causes of the unprecedented environmental crisis that now threatens to destroy large regions of the biosphere, Indigenous communities around the world, and perhaps even Western technological civilisation itself. While other longtermists have not been as explicit as Bostrom, there is a clear tendency to see the natural world the way utilitarianism sees people: as means to some abstract, impersonal end, and nothing more. MacAskill and a colleague, for example, <a href=\"https://philarchive.org/archive/PUMEAv3\" target=\"_blank\" rel=\"noreferrer noopener\">write</a> that the EA movement, and by implication longtermism, is ‘tentatively <em>welfarist</em> in that its tentative aim in doing good concerns promoting wellbeing only and not, say, protecting biodiversity or conserving natural beauty for their own sakes.’</p>\n<p class=\"pullquote\">On this account, every problem arises from too little rather than too much technology</p>\n<p>Just as worrisome is the longtermist demand that we must create ever-more powerful technologies, despite the agreed-upon fact that the overwhelming source of risk to human existence these days comes from these very technologies. In Ord’s words, ‘without serious efforts to protect humanity, there is strong reason to believe the risk will be higher this century, and increasing with each century that technological progress continues.’ Similarly, in 2012 Bostrom acknowledges that</p>\n<blockquote>the great bulk of existential risk in the foreseeable future consists of anthropogenic existential risks – that is, arising from human activity. In particular, most of the biggest existential risks seem to be linked to potential future technological breakthroughs that may radically expand our ability to manipulate the external world or our own biology. As our powers expand, so will the scale of their potential consequences – intended and unintended, positive and negative.</blockquote>\n<p>On this view, there is only one way forward – more technological development – even if this is the most dangerous path into the future. But how much sense does this make? Surely if we want to maximise our chances of survival, we should oppose the development of dangerous new dual-use technologies. If more technology equals greater risk – as history clearly shows and technological projections affirm – then perhaps the only way to actually attain a state of ‘existential security’ is to slow down or completely halt further technological innovation.</p>\n<p>But longtermists have an answer to this conundrum: the so-called ‘value-neutrality thesis’. This states that technology is a morally neutral object, ie, ‘just a tool’. The idea is most famously encapsulated in the NRA’s slogan ‘Guns don’t kill people, people kill people,’ which conveys the message that the consequences of technology, whether good or bad, beneficial or harmful, are entirely determined by the users, not the artefacts. As Bostrom put it in 2002, ‘we should not <em>blame</em> civilisation or technology for imposing big existential risks,’ adding that ‘because of the way we have defined existential risks, a failure to develop technological civilisation would imply that we had fallen victims of an existential disaster.’</p>\n<p>Ord similarly argues that ‘the problem is not so much an excess of technology as a lack of wisdom,’ before going on to quote Carl Sagan’s book <em>Pale Blue Dot</em> (1994): ‘Many of the dangers we face indeed arise from science and technology but, more fundamentally, because we have become powerful without becoming commensurately wise.’ In other words, it is our fault for not being smarter, wiser and more ethical, a cluster of deficiencies that many longtermists believe, in a bit of twisted logic, could be rectified by technologically reengineering our cognitive systems and moral dispositions. Everything, on this account, is an engineering problem, and hence every problem arises from too little rather than too much technology.</p>\n<p>We can now begin to see how longtermism might be self-defeating. Not only could its ‘fanatical’ emphasis on fulfilling our longterm potential lead people to, eg, neglect non-existential climate change, prioritise the rich over the poor and perhaps even ‘justify’ pre-emptive violence and atrocities for the ‘greater cosmic good’ but it also contains within it the very tendencies – Baconianism, capitalism and value-neutrality – that have driven humanity inches away from the precipice of destruction. Longtermism tells us to maximise economic productivity, our control over nature, our presence in the Universe, the number of (simulated) people who exist in the future, the total amount of impersonal ‘value’ and so on. But to maximise, we must develop increasingly powerful – and dangerous – technologies; failing to do this would itself be an existential catastrophe. Not to worry, though, because technology is not responsible for our worsening predicament, and hence the fact that most risks stem directly from technology is no reason to stop creating more technology. Rather, the problem lies with us, which means only that we must create even more technology to transform ourselves into cognitively and morally enhanced posthumans.</p>\n<p>This looks like a recipe for disaster. Creating a new race of ‘wise and responsible’ posthumans is implausible and, if advanced technologies continue to be developed at the current rate, a global-scale catastrophe is almost certainly a matter of when rather than if. Yes, we will need advanced technologies if we wish to escape Earth before it’s sterilised by the Sun in a billion years or so. But the crucial fact that longtermists miss is that <em>technology is far more likely to cause our extinction before this distant future event than to save us from it</em>. If you, like me, value the continued survival and flourishing of humanity, you should care about the long term but reject the ideology of longtermism, which is not only dangerous and flawed but might be contributing to, and reinforcing, the risks that now threaten every person on the planet.</p></div><div class=\"sc-d9870e8f-21 imycrC\"><div class=\"sc-d9870e8f-22 clHWPp\"><div class=\"sc-d9870e8f-17 gzvGIX\"><a href=\"/philosophy/thinkers-and-theories\" class=\"sc-d9870e8f-18 chrJAL\">Thinkers and theories</a><a href=\"/society/the-future\" class=\"sc-d9870e8f-18 eNPUtH\">The future</a><a href=\"/philosophy/values-beliefs\" class=\"sc-d9870e8f-18 chrJAL\">Values and beliefs</a></div><div class=\"sc-d9870e8f-16 bipmAa\">19 October 2021</div><div class=\"sc-d9870e8f-14 cSFNDd\"><div class=\"sc-e2a1906f-0 bkhChg sc-d9870e8f-15 dfGLcH\"><div><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-email\" aria-hidden=\"true\" class=\"sc-e2a1906f-1 bXtNKb\"><path d=\"M22 6c0-1.1-.9-2-2-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6zm-2 0l-8 5-8-5h16zm0 12H4V8l8 5 8-5v10z\"></path></svg><span class=\"sc-e2a1906f-2 hJXeEE\">Email</span></div><div><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-save\" aria-hidden=\"true\" class=\"sc-e2a1906f-1 bXtNKb\"><path d=\"M17 3H7c-1.1 0-1.99.9-1.99 2L5 21l7-3 7 3V5c0-1.1-.9-2-2-2zm0 15l-5-2.18L7 18V5h10v13z\"></path></svg><span class=\"sc-e2a1906f-2 hJXeEE\">Save</span></div><div><svg viewBox=\"0 0 450 450\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-twitter\" aria-hidden=\"true\" class=\"sc-e2a1906f-1 bXtNKb\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M225 450C349.264 450 450 349.264 450 225C450 100.736 349.264 0 225 0C100.736 0 0 100.736 0 225C0 349.264 100.736 450 225 450ZM349.373 92L250.332 204.646L358 358H278.785L206.256 254.71L115.461 358H92L195.855 239.888L92 92H171.215L239.878 189.789L325.912 92H349.373ZM159.972 109.311H123.929L289.842 341.539H325.894L159.972 109.311Z\"></path></svg><span class=\"sc-e2a1906f-2 hJXeEE\">Post</span></div><div><svg viewBox=\"0 0 1024 1024\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-facebook\" aria-hidden=\"true\" class=\"sc-e2a1906f-1 bXtNKb\"><g><path d=\"M1024,512C1024,229.23,794.77,0,512,0S0,229.23,0,512c0,255.55,187.23,467.37,432,505.78V660H302V512H432V399.2C432,270.88,508.44,200,625.39,200c56,0,114.61,10,114.61,10V336H675.44c-63.6,0-83.44,39.47-83.44,80v96H734L711.3,660H592v357.78C836.77,979.37,1024,767.55,1024,512Z\"></path><path d=\"M711.3,660,734,512H592V416c0-40.49,19.84-80,83.44-80H740V210s-58.59-10-114.61-10C508.44,200,432,270.88,432,399.2V512H302V660H432v357.78a517.58,517.58,0,0,0,160,0V660Z\" fill=\"transparent\"></path></g></svg><span class=\"sc-e2a1906f-2 hJXeEE\">Share</span></div></div></div></div><div class=\"sc-dbbed3aa-1 gROkhr sc-d9870e8f-25 iPkEru\"><a class=\"sc-45649ee9-0 bhtfvC sc-dbbed3aa-0 kmtmVW\" href=\"/syndication?article_slug=why-longtermism-is-the-worlds-most-dangerous-secular-credo&amp;article_name=Against%20longtermism&amp;author=%C3%89mile%20P%20Torres&amp;date=19%20October%202021\"><span class=\"sc-45649ee9-1 cacrwE\">Syndicate this <!-- -->essay</span></a></div></div></div></div></div><div class=\"sc-d9870e8f-24 cTqnQQ\"><div class=\"sc-d9870e8f-0 bMtwqF\"><div class=\"sc-c83e4c92-0 cFULRC\"><a href=\"/essays/c-l-r-james-foresaw-the-crisis-of-us-liberal-democracy\"><div class=\"sc-59baed0b-9 emuUeA\"><div type=\"essay\" class=\"sc-4c9ef492-2 eTKecW\"><img alt=\"Black-and-white photo of a man in a suit and hat grabbing another man by his collar in front of a bar with bottles.\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" style=\"position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent\" sizes=\"(max-width: 640px) 100vw, (max-width: 960px) 45vw, (max-width: 1440px) 30vw, 430px\" srcSet=\"https://images.aeonmedia.co/images/e7965c4b-cb81-41e6-a398-fb57fc2356c6/essay-the-public-enemy-.jpg?width=256&amp;quality=75&amp;format=auto 256w, https://images.aeonmedia.co/images/e7965c4b-cb81-41e6-a398-fb57fc2356c6/essay-the-public-enemy-.jpg?width=384&amp;quality=75&amp;format=auto 384w, https://images.aeonmedia.co/images/e7965c4b-cb81-41e6-a398-fb57fc2356c6/essay-the-public-enemy-.jpg?width=640&amp;quality=75&amp;format=auto 640w, https://images.aeonmedia.co/images/e7965c4b-cb81-41e6-a398-fb57fc2356c6/essay-the-public-enemy-.jpg?width=750&amp;quality=75&amp;format=auto 750w, https://images.aeonmedia.co/images/e7965c4b-cb81-41e6-a398-fb57fc2356c6/essay-the-public-enemy-.jpg?width=828&amp;quality=75&amp;format=auto 828w, https://images.aeonmedia.co/images/e7965c4b-cb81-41e6-a398-fb57fc2356c6/essay-the-public-enemy-.jpg?width=1080&amp;quality=75&amp;format=auto 1080w, https://images.aeonmedia.co/images/e7965c4b-cb81-41e6-a398-fb57fc2356c6/essay-the-public-enemy-.jpg?width=1200&amp;quality=75&amp;format=auto 1200w, https://images.aeonmedia.co/images/e7965c4b-cb81-41e6-a398-fb57fc2356c6/essay-the-public-enemy-.jpg?width=1920&amp;quality=75&amp;format=auto 1920w, https://images.aeonmedia.co/images/e7965c4b-cb81-41e6-a398-fb57fc2356c6/essay-the-public-enemy-.jpg?width=2048&amp;quality=75&amp;format=auto 2048w, https://images.aeonmedia.co/images/e7965c4b-cb81-41e6-a398-fb57fc2356c6/essay-the-public-enemy-.jpg?width=3840&amp;quality=75&amp;format=auto 3840w\" src=\"https://images.aeonmedia.co/images/e7965c4b-cb81-41e6-a398-fb57fc2356c6/essay-the-public-enemy-.jpg?width=3840&amp;quality=75&amp;format=auto\"/></div><div class=\"sc-59baed0b-1 HnFfS\"><div class=\"sc-59baed0b-8 bIyzxX\"><div class=\"sc-59baed0b-5 efwOzb\"><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-save\" aria-hidden=\"true\"><path d=\"M17 3H7c-1.1 0-1.99.9-1.99 2L5 21l7-3 7 3V5c0-1.1-.9-2-2-2zm0 15l-5-2.18L7 18V5h10v13z\"></path></svg><span class=\"sc-59baed0b-4 hfpWCv\">Save</span></div><p class=\"sc-59baed0b-0 sc-59baed0b-3 fuyVND cAOLoR\">essay</p><p class=\"sc-59baed0b-0 sc-59baed0b-2 fuyVND ePEXnI\">Political philosophy</p></div><p class=\"sc-59baed0b-10 gqsGJu\">C L R James and America</p><p class=\"sc-59baed0b-11 llxLtL\">The brilliant Trinidadian thinker is remembered as an admirer of the US but he also warned of its dark political future</p><p class=\"sc-59baed0b-0 sc-59baed0b-12 fuyVND cWJVNA\">Harvey Neptune</p></div><div class=\"sc-59baed0b-6 dtyrwz\"></div></div></a><a href=\"/essays/the-surprising-truth-about-wealth-and-inequality-in-the-west\"><div class=\"sc-59baed0b-9 nvsYc\"><div type=\"essay\" class=\"sc-4c9ef492-2 fycPaJ\"><img alt=\"A suburban street with mountains in the background, featuring a girl on a bike, parked cars, and old furniture on the sidewalk in front of a house.\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" style=\"position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent\" sizes=\"(max-width: 640px) 100vw, (max-width: 960px) 45vw, (max-width: 1440px) 30vw, 430px\" srcSet=\"https://images.aeonmedia.co/images/cf79ce56-22d5-485b-919e-43af3a6d508c/essay-v2-nn11492476.jpg?width=256&amp;quality=75&amp;format=auto 256w, https://images.aeonmedia.co/images/cf79ce56-22d5-485b-919e-43af3a6d508c/essay-v2-nn11492476.jpg?width=384&amp;quality=75&amp;format=auto 384w, https://images.aeonmedia.co/images/cf79ce56-22d5-485b-919e-43af3a6d508c/essay-v2-nn11492476.jpg?width=640&amp;quality=75&amp;format=auto 640w, https://images.aeonmedia.co/images/cf79ce56-22d5-485b-919e-43af3a6d508c/essay-v2-nn11492476.jpg?width=750&amp;quality=75&amp;format=auto 750w, https://images.aeonmedia.co/images/cf79ce56-22d5-485b-919e-43af3a6d508c/essay-v2-nn11492476.jpg?width=828&amp;quality=75&amp;format=auto 828w, https://images.aeonmedia.co/images/cf79ce56-22d5-485b-919e-43af3a6d508c/essay-v2-nn11492476.jpg?width=1080&amp;quality=75&amp;format=auto 1080w, https://images.aeonmedia.co/images/cf79ce56-22d5-485b-919e-43af3a6d508c/essay-v2-nn11492476.jpg?width=1200&amp;quality=75&amp;format=auto 1200w, https://images.aeonmedia.co/images/cf79ce56-22d5-485b-919e-43af3a6d508c/essay-v2-nn11492476.jpg?width=1920&amp;quality=75&amp;format=auto 1920w, https://images.aeonmedia.co/images/cf79ce56-22d5-485b-919e-43af3a6d508c/essay-v2-nn11492476.jpg?width=2048&amp;quality=75&amp;format=auto 2048w, https://images.aeonmedia.co/images/cf79ce56-22d5-485b-919e-43af3a6d508c/essay-v2-nn11492476.jpg?width=3840&amp;quality=75&amp;format=auto 3840w\" src=\"https://images.aeonmedia.co/images/cf79ce56-22d5-485b-919e-43af3a6d508c/essay-v2-nn11492476.jpg?width=3840&amp;quality=75&amp;format=auto\"/></div><div class=\"sc-59baed0b-1 HnFfS\"><div class=\"sc-59baed0b-8 bIyzxX\"><div class=\"sc-59baed0b-5 dVkVdY\"><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-save\" aria-hidden=\"true\"><path d=\"M17 3H7c-1.1 0-1.99.9-1.99 2L5 21l7-3 7 3V5c0-1.1-.9-2-2-2zm0 15l-5-2.18L7 18V5h10v13z\"></path></svg><span class=\"sc-59baed0b-4 hfpWCv\">Save</span></div><p class=\"sc-59baed0b-0 sc-59baed0b-3 fuyVND cAOLoR\">essay</p><p class=\"sc-59baed0b-0 sc-59baed0b-2 fuyVND ePEXnI\">Progress and modernity</p></div><p class=\"sc-59baed0b-10 gqsGJu\">The great wealth wave</p><p class=\"sc-59baed0b-11 llxLtL\">The tide has turned – evidence shows ordinary citizens in the Western world are now richer and more equal than ever before</p><p class=\"sc-59baed0b-0 sc-59baed0b-12 fuyVND cWJVNA\">Daniel Waldenström</p></div><div class=\"sc-59baed0b-6 dtyrwz\"><span class=\"sc-59baed0b-7 dBlrnI\"><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-comment-bubble\" aria-hidden=\"true\"><path d=\"M21.303,16.553l1.107,3.783l-3.48-2.322c-0.689,0.234-1.422,0.371-2.193,0.371c-1.041,0-2.024-0.234-2.907-0.642 c2.034-1.697,3.333-4.275,3.333-7.166c0-1.994-0.625-3.836-1.677-5.347c0.406-0.073,0.823-0.117,1.251-0.117 c3.762,0,6.814,2.971,6.814,6.634c0,1.594-0.578,3.057-1.539,4.2L21.303,16.553z M15.255,10.684c0,3.861-3.213,6.99-7.176,6.99 c-0.812,0-1.583-0.144-2.31-0.391l-3.667,2.446l1.166-3.985l-0.744-0.638c-1.013-1.203-1.621-2.743-1.621-4.423 c0-3.858,3.215-6.987,7.176-6.987C12.042,3.696,15.255,6.825,15.255,10.684z\"></path></svg></span></div></div></a><a href=\"/essays/how-a-warming-earth-is-changing-our-brains-bodies-and-minds\"><div class=\"sc-59baed0b-9 koxOCA\"><div type=\"essay\" class=\"sc-4c9ef492-2 kQFYIz\"><img alt=\"Silhouette of a person walking through a spray of water at sunset with cars and buildings in the background.\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" style=\"position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent\" sizes=\"(max-width: 640px) 100vw, (max-width: 960px) 45vw, (max-width: 1440px) 30vw, 430px\" srcSet=\"https://images.aeonmedia.co/images/25b67b4d-d49d-4f8e-b96d-1c8b33ed0a16/essay-rtr2g5k7.jpg?width=256&amp;quality=75&amp;format=auto 256w, https://images.aeonmedia.co/images/25b67b4d-d49d-4f8e-b96d-1c8b33ed0a16/essay-rtr2g5k7.jpg?width=384&amp;quality=75&amp;format=auto 384w, https://images.aeonmedia.co/images/25b67b4d-d49d-4f8e-b96d-1c8b33ed0a16/essay-rtr2g5k7.jpg?width=640&amp;quality=75&amp;format=auto 640w, https://images.aeonmedia.co/images/25b67b4d-d49d-4f8e-b96d-1c8b33ed0a16/essay-rtr2g5k7.jpg?width=750&amp;quality=75&amp;format=auto 750w, https://images.aeonmedia.co/images/25b67b4d-d49d-4f8e-b96d-1c8b33ed0a16/essay-rtr2g5k7.jpg?width=828&amp;quality=75&amp;format=auto 828w, https://images.aeonmedia.co/images/25b67b4d-d49d-4f8e-b96d-1c8b33ed0a16/essay-rtr2g5k7.jpg?width=1080&amp;quality=75&amp;format=auto 1080w, https://images.aeonmedia.co/images/25b67b4d-d49d-4f8e-b96d-1c8b33ed0a16/essay-rtr2g5k7.jpg?width=1200&amp;quality=75&amp;format=auto 1200w, https://images.aeonmedia.co/images/25b67b4d-d49d-4f8e-b96d-1c8b33ed0a16/essay-rtr2g5k7.jpg?width=1920&amp;quality=75&amp;format=auto 1920w, https://images.aeonmedia.co/images/25b67b4d-d49d-4f8e-b96d-1c8b33ed0a16/essay-rtr2g5k7.jpg?width=2048&amp;quality=75&amp;format=auto 2048w, https://images.aeonmedia.co/images/25b67b4d-d49d-4f8e-b96d-1c8b33ed0a16/essay-rtr2g5k7.jpg?width=3840&amp;quality=75&amp;format=auto 3840w\" src=\"https://images.aeonmedia.co/images/25b67b4d-d49d-4f8e-b96d-1c8b33ed0a16/essay-rtr2g5k7.jpg?width=3840&amp;quality=75&amp;format=auto\"/></div><div class=\"sc-59baed0b-1 HnFfS\"><div class=\"sc-59baed0b-8 bIyzxX\"><div class=\"sc-59baed0b-5 bgoMLi\"><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-save\" aria-hidden=\"true\"><path d=\"M17 3H7c-1.1 0-1.99.9-1.99 2L5 21l7-3 7 3V5c0-1.1-.9-2-2-2zm0 15l-5-2.18L7 18V5h10v13z\"></path></svg><span class=\"sc-59baed0b-4 hfpWCv\">Save</span></div><p class=\"sc-59baed0b-0 sc-59baed0b-3 fuyVND cAOLoR\">essay</p><p class=\"sc-59baed0b-0 sc-59baed0b-2 fuyVND ePEXnI\">Neuroscience</p></div><p class=\"sc-59baed0b-10 gqsGJu\">The melting brain</p><p class=\"sc-59baed0b-11 llxLtL\">It’s not just the planet and not just our health – the impact of a warming climate extends deep into our cortical fissures</p><p class=\"sc-59baed0b-0 sc-59baed0b-12 fuyVND cWJVNA\">Clayton Page Aldern</p></div><div class=\"sc-59baed0b-6 dtyrwz\"><span class=\"sc-59baed0b-7 dBlrnI\"><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-comment-bubble\" aria-hidden=\"true\"><path d=\"M21.303,16.553l1.107,3.783l-3.48-2.322c-0.689,0.234-1.422,0.371-2.193,0.371c-1.041,0-2.024-0.234-2.907-0.642 c2.034-1.697,3.333-4.275,3.333-7.166c0-1.994-0.625-3.836-1.677-5.347c0.406-0.073,0.823-0.117,1.251-0.117 c3.762,0,6.814,2.971,6.814,6.634c0,1.594-0.578,3.057-1.539,4.2L21.303,16.553z M15.255,10.684c0,3.861-3.213,6.99-7.176,6.99 c-0.812,0-1.583-0.144-2.31-0.391l-3.667,2.446l1.166-3.985l-0.744-0.638c-1.013-1.203-1.621-2.743-1.621-4.423 c0-3.858,3.215-6.987,7.176-6.987C12.042,3.696,15.255,6.825,15.255,10.684z\"></path></svg></span></div></div></a><a href=\"/essays/why-millions-of-britons-fell-in-love-with-suburban-life\"><div class=\"sc-59baed0b-9 reA-do\"><div type=\"essay\" class=\"sc-4c9ef492-2 gnJbce\"><img alt=\"A brick house with a tiled roof, surrounded by a well-maintained garden with bushes and colourful flowers.\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" style=\"position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent\" sizes=\"(max-width: 640px) 100vw, (max-width: 960px) 45vw, (max-width: 1440px) 30vw, 430px\" srcSet=\"https://images.aeonmedia.co/images/753e612a-9bae-4257-a34f-98ee4da12cc3/test-essay-roehouse.jpg?width=256&amp;quality=75&amp;format=auto 256w, https://images.aeonmedia.co/images/753e612a-9bae-4257-a34f-98ee4da12cc3/test-essay-roehouse.jpg?width=384&amp;quality=75&amp;format=auto 384w, https://images.aeonmedia.co/images/753e612a-9bae-4257-a34f-98ee4da12cc3/test-essay-roehouse.jpg?width=640&amp;quality=75&amp;format=auto 640w, https://images.aeonmedia.co/images/753e612a-9bae-4257-a34f-98ee4da12cc3/test-essay-roehouse.jpg?width=750&amp;quality=75&amp;format=auto 750w, https://images.aeonmedia.co/images/753e612a-9bae-4257-a34f-98ee4da12cc3/test-essay-roehouse.jpg?width=828&amp;quality=75&amp;format=auto 828w, https://images.aeonmedia.co/images/753e612a-9bae-4257-a34f-98ee4da12cc3/test-essay-roehouse.jpg?width=1080&amp;quality=75&amp;format=auto 1080w, https://images.aeonmedia.co/images/753e612a-9bae-4257-a34f-98ee4da12cc3/test-essay-roehouse.jpg?width=1200&amp;quality=75&amp;format=auto 1200w, https://images.aeonmedia.co/images/753e612a-9bae-4257-a34f-98ee4da12cc3/test-essay-roehouse.jpg?width=1920&amp;quality=75&amp;format=auto 1920w, https://images.aeonmedia.co/images/753e612a-9bae-4257-a34f-98ee4da12cc3/test-essay-roehouse.jpg?width=2048&amp;quality=75&amp;format=auto 2048w, https://images.aeonmedia.co/images/753e612a-9bae-4257-a34f-98ee4da12cc3/test-essay-roehouse.jpg?width=3840&amp;quality=75&amp;format=auto 3840w\" src=\"https://images.aeonmedia.co/images/753e612a-9bae-4257-a34f-98ee4da12cc3/test-essay-roehouse.jpg?width=3840&amp;quality=75&amp;format=auto\"/></div><div class=\"sc-59baed0b-1 HnFfS\"><div class=\"sc-59baed0b-8 bIyzxX\"><div class=\"sc-59baed0b-5 hXIPur\"><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-save\" aria-hidden=\"true\"><path d=\"M17 3H7c-1.1 0-1.99.9-1.99 2L5 21l7-3 7 3V5c0-1.1-.9-2-2-2zm0 15l-5-2.18L7 18V5h10v13z\"></path></svg><span class=\"sc-59baed0b-4 hfpWCv\">Save</span></div><p class=\"sc-59baed0b-0 sc-59baed0b-3 fuyVND cAOLoR\">essay</p><p class=\"sc-59baed0b-0 sc-59baed0b-2 fuyVND ePEXnI\">Home</p></div><p class=\"sc-59baed0b-10 gqsGJu\">Falling for suburbia</p><p class=\"sc-59baed0b-11 llxLtL\">Modernists and historians alike loathed the millions of new houses built in interwar Britain. But their owners loved them</p><p class=\"sc-59baed0b-0 sc-59baed0b-12 fuyVND cWJVNA\">Michael Gilson</p></div><div class=\"sc-59baed0b-6 dtyrwz\"></div></div></a><a href=\"/essays/john-rawls-liberalism-and-what-it-means-to-live-a-good-life\"><div class=\"sc-59baed0b-9 emuUeA\"><div type=\"essay\" class=\"sc-4c9ef492-2 eTKecW\"><img alt=\"An old photograph of a man pulling a small cart with a child and belongings, followed by a woman and three children; one child is pushing a stroller.\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" style=\"position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent\" sizes=\"(max-width: 640px) 100vw, (max-width: 960px) 45vw, (max-width: 1440px) 30vw, 430px\" srcSet=\"https://images.aeonmedia.co/images/6b44ba64-ea98-49b8-a00c-f1ea564aaf29/essay-family-with-five-children-master-pnp-fsa-8b38000-8b38700-8b38702u.jpg?width=256&amp;quality=75&amp;format=auto 256w, https://images.aeonmedia.co/images/6b44ba64-ea98-49b8-a00c-f1ea564aaf29/essay-family-with-five-children-master-pnp-fsa-8b38000-8b38700-8b38702u.jpg?width=384&amp;quality=75&amp;format=auto 384w, https://images.aeonmedia.co/images/6b44ba64-ea98-49b8-a00c-f1ea564aaf29/essay-family-with-five-children-master-pnp-fsa-8b38000-8b38700-8b38702u.jpg?width=640&amp;quality=75&amp;format=auto 640w, https://images.aeonmedia.co/images/6b44ba64-ea98-49b8-a00c-f1ea564aaf29/essay-family-with-five-children-master-pnp-fsa-8b38000-8b38700-8b38702u.jpg?width=750&amp;quality=75&amp;format=auto 750w, https://images.aeonmedia.co/images/6b44ba64-ea98-49b8-a00c-f1ea564aaf29/essay-family-with-five-children-master-pnp-fsa-8b38000-8b38700-8b38702u.jpg?width=828&amp;quality=75&amp;format=auto 828w, https://images.aeonmedia.co/images/6b44ba64-ea98-49b8-a00c-f1ea564aaf29/essay-family-with-five-children-master-pnp-fsa-8b38000-8b38700-8b38702u.jpg?width=1080&amp;quality=75&amp;format=auto 1080w, https://images.aeonmedia.co/images/6b44ba64-ea98-49b8-a00c-f1ea564aaf29/essay-family-with-five-children-master-pnp-fsa-8b38000-8b38700-8b38702u.jpg?width=1200&amp;quality=75&amp;format=auto 1200w, https://images.aeonmedia.co/images/6b44ba64-ea98-49b8-a00c-f1ea564aaf29/essay-family-with-five-children-master-pnp-fsa-8b38000-8b38700-8b38702u.jpg?width=1920&amp;quality=75&amp;format=auto 1920w, https://images.aeonmedia.co/images/6b44ba64-ea98-49b8-a00c-f1ea564aaf29/essay-family-with-five-children-master-pnp-fsa-8b38000-8b38700-8b38702u.jpg?width=2048&amp;quality=75&amp;format=auto 2048w, https://images.aeonmedia.co/images/6b44ba64-ea98-49b8-a00c-f1ea564aaf29/essay-family-with-five-children-master-pnp-fsa-8b38000-8b38700-8b38702u.jpg?width=3840&amp;quality=75&amp;format=auto 3840w\" src=\"https://images.aeonmedia.co/images/6b44ba64-ea98-49b8-a00c-f1ea564aaf29/essay-family-with-five-children-master-pnp-fsa-8b38000-8b38700-8b38702u.jpg?width=3840&amp;quality=75&amp;format=auto\"/></div><div class=\"sc-59baed0b-1 HnFfS\"><div class=\"sc-59baed0b-8 bIyzxX\"><div class=\"sc-59baed0b-5 efwOzb\"><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-save\" aria-hidden=\"true\"><path d=\"M17 3H7c-1.1 0-1.99.9-1.99 2L5 21l7-3 7 3V5c0-1.1-.9-2-2-2zm0 15l-5-2.18L7 18V5h10v13z\"></path></svg><span class=\"sc-59baed0b-4 hfpWCv\">Save</span></div><p class=\"sc-59baed0b-0 sc-59baed0b-3 fuyVND cAOLoR\">essay</p><p class=\"sc-59baed0b-0 sc-59baed0b-2 fuyVND ePEXnI\">Thinkers and theories</p></div><p class=\"sc-59baed0b-10 gqsGJu\">Rawls the redeemer</p><p class=\"sc-59baed0b-11 llxLtL\">For John Rawls, liberalism was more than a political project: it is the best way to fashion a life that is worthy of happiness</p><p class=\"sc-59baed0b-0 sc-59baed0b-12 fuyVND cWJVNA\">Alexandre Lefebvre</p></div><div class=\"sc-59baed0b-6 dtyrwz\"></div></div></a><a href=\"/essays/is-ai-our-salvation-our-undoing-or-just-more-of-the-same\"><div class=\"sc-59baed0b-9 fNUksc\"><div type=\"essay\" class=\"sc-4c9ef492-2 faKwuS\"><img alt=\"Close-up of a person’s hand using a smartphone in a dimly lit room with blurred lights in the background. The phone screen shows the text ‘How can I help you today?’ and a text input field.\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" style=\"position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent\" sizes=\"(max-width: 640px) 100vw, (max-width: 960px) 45vw, (max-width: 1440px) 30vw, 430px\" srcSet=\"https://images.aeonmedia.co/images/4512cf70-20be-4286-bbe3-7338389b95d5/essay-final-gettyimages-2152436402.jpg?width=256&amp;quality=75&amp;format=auto 256w, https://images.aeonmedia.co/images/4512cf70-20be-4286-bbe3-7338389b95d5/essay-final-gettyimages-2152436402.jpg?width=384&amp;quality=75&amp;format=auto 384w, https://images.aeonmedia.co/images/4512cf70-20be-4286-bbe3-7338389b95d5/essay-final-gettyimages-2152436402.jpg?width=640&amp;quality=75&amp;format=auto 640w, https://images.aeonmedia.co/images/4512cf70-20be-4286-bbe3-7338389b95d5/essay-final-gettyimages-2152436402.jpg?width=750&amp;quality=75&amp;format=auto 750w, https://images.aeonmedia.co/images/4512cf70-20be-4286-bbe3-7338389b95d5/essay-final-gettyimages-2152436402.jpg?width=828&amp;quality=75&amp;format=auto 828w, https://images.aeonmedia.co/images/4512cf70-20be-4286-bbe3-7338389b95d5/essay-final-gettyimages-2152436402.jpg?width=1080&amp;quality=75&amp;format=auto 1080w, https://images.aeonmedia.co/images/4512cf70-20be-4286-bbe3-7338389b95d5/essay-final-gettyimages-2152436402.jpg?width=1200&amp;quality=75&amp;format=auto 1200w, https://images.aeonmedia.co/images/4512cf70-20be-4286-bbe3-7338389b95d5/essay-final-gettyimages-2152436402.jpg?width=1920&amp;quality=75&amp;format=auto 1920w, https://images.aeonmedia.co/images/4512cf70-20be-4286-bbe3-7338389b95d5/essay-final-gettyimages-2152436402.jpg?width=2048&amp;quality=75&amp;format=auto 2048w, https://images.aeonmedia.co/images/4512cf70-20be-4286-bbe3-7338389b95d5/essay-final-gettyimages-2152436402.jpg?width=3840&amp;quality=75&amp;format=auto 3840w\" src=\"https://images.aeonmedia.co/images/4512cf70-20be-4286-bbe3-7338389b95d5/essay-final-gettyimages-2152436402.jpg?width=3840&amp;quality=75&amp;format=auto\"/></div><div class=\"sc-59baed0b-1 HnFfS\"><div class=\"sc-59baed0b-8 bIyzxX\"><div class=\"sc-59baed0b-5 enJAtv\"><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-save\" aria-hidden=\"true\"><path d=\"M17 3H7c-1.1 0-1.99.9-1.99 2L5 21l7-3 7 3V5c0-1.1-.9-2-2-2zm0 15l-5-2.18L7 18V5h10v13z\"></path></svg><span class=\"sc-59baed0b-4 hfpWCv\">Save</span></div><p class=\"sc-59baed0b-0 sc-59baed0b-3 fuyVND cAOLoR\">essay</p><p class=\"sc-59baed0b-0 sc-59baed0b-2 fuyVND ePEXnI\">Computing and artificial intelligence</p></div><p class=\"sc-59baed0b-10 gqsGJu\">Mere imitation</p><p class=\"sc-59baed0b-11 llxLtL\">Generative AI has lately set off public euphoria: the machines have learned to think! But just how intelligent is AI?</p><p class=\"sc-59baed0b-0 sc-59baed0b-12 fuyVND cWJVNA\">Deepak P</p></div><div class=\"sc-59baed0b-6 dtyrwz\"><span class=\"sc-59baed0b-7 dBlrnI\"><svg viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\" width=\"20px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-aeon-comment-bubble\" aria-hidden=\"true\"><path d=\"M21.303,16.553l1.107,3.783l-3.48-2.322c-0.689,0.234-1.422,0.371-2.193,0.371c-1.041,0-2.024-0.234-2.907-0.642 c2.034-1.697,3.333-4.275,3.333-7.166c0-1.994-0.625-3.836-1.677-5.347c0.406-0.073,0.823-0.117,1.251-0.117 c3.762,0,6.814,2.971,6.814,6.634c0,1.594-0.578,3.057-1.539,4.2L21.303,16.553z M15.255,10.684c0,3.861-3.213,6.99-7.176,6.99 c-0.812,0-1.583-0.144-2.31-0.391l-3.667,2.446l1.166-3.985l-0.744-0.638c-1.013-1.203-1.621-2.743-1.621-4.423 c0-3.858,3.215-6.987,7.176-6.987C12.042,3.696,15.255,6.825,15.255,10.684z\"></path></svg></span></div></div></a></div></div></div></div><footer class=\"sc-4c328164-0 hOpepC footer\"><div class=\"sc-ae266dcd-0 sc-4c328164-1 bwTVbW kkIxMa\"><div class=\"sc-1b596d24-0 ithRRD\"><div class=\"sc-47bc2fa8-0 sc-bab0e3bb-0 dpJmnC dmJOTI\"><nav class=\"sc-bab0e3bb-1 hlaCbq\"><a href=\"/essays\" class=\"sc-11b024c5-0 fQckeD\">Essays</a><a href=\"/ideas\" class=\"sc-11b024c5-0 fQckeD\">Ideas</a><a href=\"/videos\" class=\"sc-11b024c5-0 fQckeD\">Videos</a></nav></div><div class=\"sc-47bc2fa8-0 sc-bab0e3bb-0 dpJmnC dmJOTI\"><nav class=\"sc-bab0e3bb-1 hlaCbq\"><a href=\"/about\" class=\"sc-11b024c5-0 fQckeD\">About</a><a href=\"/contact\" class=\"sc-11b024c5-0 fQckeD\">Contact</a><a href=\"/support\" class=\"sc-11b024c5-0 fQckeD\">Support</a></nav></div><div class=\"sc-47bc2fa8-0 sc-bab0e3bb-0 dpJmnC gSiEIv\"><nav class=\"sc-bab0e3bb-1 hlaCbq\"><a href=\"/feed.rss\" target=\"_blank\" rel=\"noreferrer\" class=\"sc-11b024c5-0 fQckeD\">RSS Feed</a><a data-ga-select-prompt=\"aeon_footer_donate\" href=\"/donate\" class=\"sc-11b024c5-0 fQckeD\">Donate</a><a href=\"/community-guidelines\" class=\"sc-11b024c5-0 fQckeD\">Community Guidelines</a></nav></div></div><div class=\"sc-47bc2fa8-0 sc-89e3ac83-0 dpJmnC eajVBX\"><p class=\"sc-89e3ac83-3 eHOrNe\">Follow Aeon</p><div class=\"sc-89e3ac83-1 TwAna\"><p class=\"sc-89e3ac83-2 pbUpk\"><a href=\"https://www.facebook.com/aeonmag\" target=\"_blank\" rel=\"noopener noreferrer\" title=\"Facebook\"><svg viewBox=\"0 0 1024 1024\" fill=\"currentColor\" stroke=\"none\" width=\"20\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-facebook\" aria-hidden=\"true\"><g><path d=\"M1024,512C1024,229.23,794.77,0,512,0S0,229.23,0,512c0,255.55,187.23,467.37,432,505.78V660H302V512H432V399.2C432,270.88,508.44,200,625.39,200c56,0,114.61,10,114.61,10V336H675.44c-63.6,0-83.44,39.47-83.44,80v96H734L711.3,660H592v357.78C836.77,979.37,1024,767.55,1024,512Z\"></path><path d=\"M711.3,660,734,512H592V416c0-40.49,19.84-80,83.44-80H740V210s-58.59-10-114.61-10C508.44,200,432,270.88,432,399.2V512H302V660H432v357.78a517.58,517.58,0,0,0,160,0V660Z\" fill=\"transparent\"></path></g></svg></a><a href=\"https://www.instagram.com/aeonmag\" target=\"_blank\" rel=\"noopener noreferrer\" title=\"Instagram\"><svg viewBox=\"0 0 24 25\" fill=\"currentColor\" stroke=\"none\" width=\"19\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-instagram\" aria-hidden=\"true\"><path d=\"M17.216.5c4.456.203 6.887 2.633 7.09 7.09v10.126c-.203 4.456-2.634 6.887-7.09 7.09H7.09C2.633 24.603.203 22.172 0 17.716V7.59C.203 3.133 2.633.703 7.09.5zm-5.063 5.874c-3.444 0-6.28 2.835-6.28 6.279 0 3.443 2.836 6.279 6.28 6.279 3.443 0 6.279-2.836 6.279-6.28 0-3.443-2.836-6.278-6.28-6.278zm0 2.228a4.063 4.063 0 0 1 4.05 4.05 4.063 4.063 0 0 1-4.05 4.052 4.063 4.063 0 0 1-4.051-4.051 4.063 4.063 0 0 1 4.05-4.051zm6.481-3.849c-.81 0-1.418.608-1.418 1.418 0 .81.608 1.418 1.418 1.418.81 0 1.418-.608 1.418-1.418 0-.81-.608-1.418-1.418-1.418z\"></path></svg></a><a href=\"https://twitter.com/aeonmag\" target=\"_blank\" rel=\"noopener noreferrer\" title=\"X (Formerly Twitter)\"><svg viewBox=\"0 0 450 450\" fill=\"currentColor\" stroke=\"none\" width=\"22\" height=\"20.3\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-twitter\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M225 450C349.264 450 450 349.264 450 225C450 100.736 349.264 0 225 0C100.736 0 0 100.736 0 225C0 349.264 100.736 450 225 450ZM349.373 92L250.332 204.646L358 358H278.785L206.256 254.71L115.461 358H92L195.855 239.888L92 92H171.215L239.878 189.789L325.912 92H349.373ZM159.972 109.311H123.929L289.842 341.539H325.894L159.972 109.311Z\"></path></svg></a><a href=\"https://www.youtube.com/@AeonVideo\" target=\"_blank\" rel=\"noopener noreferrer\" title=\"YouTube\"><svg viewBox=\"0 0 22 20\" fill=\"currentColor\" stroke=\"none\" width=\"22\" height=\"20\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" data-cy=\"icon-youtube\" aria-hidden=\"true\"><g><path d=\"M21.5,4.7c-0.3-1-1-1.7-1.9-2C17.9,2.2,11,2.2,11,2.2s-6.9,0-8.6,0.5c-1,0.3-1.7,1-1.9,2C0,6.4,0,10,0,10s0,3.6,0.5,5.3  c0.3,1,1,1.7,1.9,2c1.7,0.5,8.6,0.5,8.6,0.5s6.9,0,8.6-0.5c1-0.3,1.7-1,1.9-2C22,13.6,22,10,22,10S22,6.4,21.5,4.7z M8.8,13.3V6.7  l5.8,3.3L8.8,13.3z\"></path></g></svg></a></p></div></div><div class=\"sc-f1ffec09-0 gakeCP\"><div class=\"sc-47bc2fa8-0 dpJmnC\"><div class=\"newsletter-signup\"><div class=\"flex justify-center bg-light-pink py-16\"><div class=\"mx-auto inline-block w-full font-sans max-w-[42rem]\"><form><h3 class=\"mb-3 text-xl\">Sign up to our newsletter</h3><h4 class=\"mb-0\">Updates on everything new at Aeon.</h4><div class=\"space-y-1 text-base inline-block\"><input class=\"appearance-none border-solid font-normal text-inherit placeholder:opacity-50 inline-block w-full min-w-60 rounded border border-grey-light px-3.5 py-3 text-base outline-none transition-colors focus:bg-grey-light-bg sm:text-base\" placeholder=\"Your email address\" id=\":R5ddq6:\" aria-invalid=\"false\" name=\"email\" value=\"\"/></div><div class=\"my-5 grid grid-cols-1 gap-x-6 gap-y-2 sm:grid-cols-[min-content,auto]\"><div class=\"space-y-1 text-base\"><label class=\"flex items-center gap-4\"><label class=\"after:-rotate-[48deg] after:-top-0.5 flex h-[1.8rem] w-[1.8rem] flex-shrink-0 items-center justify-center border border-black border-solid after:relative after:hidden after:h-[5px] after:w-[9px] after:border-0 after:border-white after:border-b-2 after:border-l-2 after:border-solid has-[:checked]:border-aeon-red has-[:checked]:bg-aeon-red has-[:checked]:after:block\"><input type=\"checkbox\" class=\"absolute appearance-none\" id=\":Rnddq6:\" aria-invalid=\"false\" name=\"frequency.daily\"/></label><div><span>Daily</span></div></label></div><div class=\"space-y-1 text-base\"><label class=\"flex items-center gap-4\"><label class=\"after:-rotate-[48deg] after:-top-0.5 flex h-[1.8rem] w-[1.8rem] flex-shrink-0 items-center justify-center border border-black border-solid after:relative after:hidden after:h-[5px] after:w-[9px] after:border-0 after:border-white after:border-b-2 after:border-l-2 after:border-solid has-[:checked]:border-aeon-red has-[:checked]:bg-aeon-red has-[:checked]:after:block\"><input type=\"checkbox\" class=\"absolute appearance-none\" id=\":R17ddq6:\" aria-invalid=\"false\" name=\"frequency.weekly\"/></label><div><span>Weekly</span></div></label></div></div><div class=\"mt-5 flex items-center gap-4\"><button class=\"cursor-pointer appearance-none border-none text-base relative inline-flex min-w-32 items-center justify-center gap-2 overflow-hidden bg-aeon-red px-5 py-3 text-white tracking-wide transition-colors hover:enabled:bg-black disabled:cursor-default disabled:opacity-50\" type=\"submit\">Subscribe<svg fill=\"#fff\" width=\"18\" height=\"18\" viewBox=\"0 0 24 24\"><path d=\"M0 0h24v24H0z\" fill=\"none\"></path><path d=\"M12 4l-1.41 1.41L16.17 11H4v2h12.17l-5.58 5.59L12 20l8-8z\"></path></svg></button></div><div class=\"mt-6 text-xs\">See our<!-- --> <a class=\"underline\" href=\"/newsletter-privacy\">newsletter privacy policy</a>.</div></form></div></div></div></div></div></div><div class=\"sc-ae266dcd-0 sc-4c328164-2 bwTVbW ikAbKy\"><div class=\"px-[1.8rem] text-black/60 text-xs max-[640px]:px-[0.6rem]\">© <!-- -->Aeon Media Group Ltd<!-- -->. 2012-<!-- -->2024<!-- -->.<!-- --> <a class=\"text-inherit hover:text-black\" href=\"/privacy\"><u>Privacy Policy</u></a>.<!-- --> <a class=\"text-inherit hover:text-black\" href=\"/terms-of-use\"><u>Terms of Use</u></a>.</div><div class=\"sc-47bc2fa8-0 sc-4c328164-3 dpJmnC bAYwLt\">Aeon is published by registered charity Aeon Media Group Ltd in association with Aeon America, a 501(c)(3) charity.</div></div></footer></div><div id=\"bottom-portal\" class=\"sc-a70232b9-5 ljOOuR\"></div></div><script id=\"__NEXT_DATA__\" type=\"application/json\">{\"props\":{\"pageProps\":{\"article\":{\"id\":\"4667\",\"title\":\"Against longtermism\",\"slug\":\"why-longtermism-is-the-worlds-most-dangerous-secular-credo\",\"type\":\"essay\",\"body\":\"\\u003cp\\u003eThere seems to be a growing recognition that humanity might be approaching the ‘end times’. Dire predictions of catastrophe clutter the news. Social media videos of hellish wildfires, devastating floods and hospitals overflowing with \\u003cspan class=\\\"ld-nowrap\\\"\\u003eCOVID-19\\u003c/span\\u003e patients dominate our timelines. Extinction Rebellion activists are shutting down cities in a desperate attempt to save the world. One \\u003ca href=\\\"https://ro.uow.edu.au/cgi/viewcontent.cgi?article=1742\\u0026amp;context=buspapers\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003esurvey\\u003c/a\\u003e even found that more than half of the people asked about humanity’s future ‘rated the risk of our way of life ending within the next \\u003cspan class=\\\"ld-nowrap\\\"\\u003e100 years\\u003c/span\\u003e at \\u003cspan class=\\\"ld-nowrap\\\"\\u003e50 per\\u003c/span\\u003e cent or greater.’\\u003c/p\\u003e\\n\\u003cp\\u003e‘Apocalypticism’, or the belief that the end times are imminent, is of course nothing new: people have warned that the end is nigh for millennia, and in fact many New Testament scholars believe that Jesus himself expected the world to end during his own lifetime. But the situation today is fundamentally different than in the past. The ‘eschatological’ scenarios now being discussed are based not on the revelations of religious prophets, or secular metanarratives of human history (as in the case of Marxism), but on robust scientific conclusions defended by leading experts in fields such as climatology, ecology, epidemiology and so on.\\u003c/p\\u003e\\n\\u003cp\\u003eWe know, for example, that climate change poses a dire threat to civilisation. We know that biodiversity loss and the sixth mass extinction could precipitate sudden, irreversible, catastrophic shifts in the global ecosystem. A thermonuclear exchange could blot out the Sun for years or decades, bringing about the collapse of global agriculture. And whether or not SARS-\\u003cspan class=\\\"ld-nowrap\\\"\\u003eCoV-2\\u003c/span\\u003e came from a Wuhan laboratory or was cooked up in the kitchen of nature (the latter seems more \\u003ca href=\\\"https://www.medicalnewstoday.com/articles/covid-19-us-intelligence-rules-out-biological-weapon-origin\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003eprobable\\u003c/a\\u003e right now), synthetic biology will soon \\u003ca href=\\\"https://docs.wixstatic.com/ugd/d9aaad_4d3e08f426904b8c8be516230722087a.pdf\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003eenable\\u003c/a\\u003e bad actors to design pathogens far more lethal and contagious than anything Darwinian evolution could possibly invent. Some philosophers and scientists have also begun sounding the \\u003ca href=\\\"https://docs.wixstatic.com/ugd/d9aaad_b2e7f0f56bec40a195e551dd3e8c878e.pdf\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003ealarm\\u003c/a\\u003e about ‘emerging threats’ associated with machine superintelligence, molecular nanotechnology and stratospheric geoengineering, which look no less formidable.\\u003c/p\\u003e\\n\\u003cp\\u003eSuch considerations have led many scholars to acknowledge that, as Stephen Hawking wrote in \\u003cem\\u003eThe Guardian\\u003c/em\\u003e in 2016, ‘we are at the most dangerous moment in the development of humanity.’ Lord Martin Rees, for example, estimates that civilisation has a 50/50 chance of making it to 2100. Noam Chomsky \\u003ca href=\\\"https://global.ilmanifesto.it/chomsky-republicans-are-a-danger-to-the-human-species/\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003eargues\\u003c/a\\u003e that the risk of annihilation is currently ‘unprecedented in the history of \\u003cem\\u003eHomo sapiens\\u003c/em\\u003e’. And Max Tegmark contends that ‘it’s probably going to be within our lifetimes … that we’re either going to self-destruct or get our act together.’ Consistent with these dismal declarations, the \\u003cem\\u003eBulletin of the Atomic Scientists\\u003c/em\\u003e in 2020 \\u003ca href=\\\"https://thebulletin.org/doomsday-clock/2020-doomsday-clock-statement/\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003eset\\u003c/a\\u003e its iconic Doomsday Clock to a mere 100 seconds before midnight (or doom), the closest it’s been since the clock was created in 1947, and more than 11,000 scientists from around the world signed an \\u003ca href=\\\"https://hal.archives-ouvertes.fr/hal-02397151/document\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003earticle\\u003c/a\\u003e in 2020 stating ‘clearly and unequivocally that planet Earth is facing a climate emergency’, and without ‘an immense increase of scale in endeavours to conserve our biosphere [we risk] untold suffering due to the climate crisis.’ As the young climate activist Xiye Bastida summed up this existential mood in a \\u003cem\\u003eTeen Vogue\\u003c/em\\u003e interview in 2019, the aim is to ‘make sure that we’re not the last generation’, because this now appears to be a very real possibility.\\u003c/p\\u003e\\n\\u003cp\\u003eGiven the unprecedented dangers facing humanity today, one might expect philosophers to have spilled a considerable amount of ink on the ethical implications of our extinction, or related scenarios such as the permanent collapse of civilisation. How morally bad (or \\u003ca href=\\\"https://aeon.co/essays/having-children-is-not-life-affirming-its-immoral\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\"\\u003egood\\u003c/a\\u003e) would our disappearance be, and for what reasons? Would it be wrong to prevent future generations from coming into existence? Does the value of past sacrifices, struggles and strivings depend on humanity continuing to exist for as long as Earth, or the Universe more generally, remains habitable?\\u003c/p\\u003e\\n\\u003cp\\u003eYet this is not the case: the topic of our extinction has received little sustained attention from philosophers until recently, and even now remains at the fringe of philosophical discussion and debate. On the whole, they have been preoccupied with other matters. However, there is one notable exception to this rule: over the past two decades, a small group of theorists mostly based in Oxford have been busy working out the details of a new moral worldview called longtermism, which \\u003ca href=\\\"https://forum.effectivealtruism.org/posts/qZyshHCNkjs3TvSem/longtermism\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003eemphasizes\\u003c/a\\u003e how our actions affect the very long-term future of the universe – thousands, millions, billions, and even trillions of years from now. This has roots in the work of \\u003ca href=\\\"https://aeon.co/essays/will-humans-be-around-in-a-billion-years-or-a-trillion\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\"\\u003eNick Bostrom\\u003c/a\\u003e, who founded the grandiosely named Future of Humanity Institute (FHI) in 2005, and Nick Beckstead, a research associate at FHI and a programme officer at Open Philanthropy. It has been defended most publicly by the FHI philosopher Toby Ord, author of \\u003cem\\u003eThe Precipice: Existential Risk and the Future of Humanity\\u003c/em\\u003e (2020). Longtermism is the primary research focus of both the Global Priorities Institute (GPI), an FHI-linked organisation directed by Hilary Greaves, and the Forethought Foundation, run by William MacAskill, who also holds positions at FHI and GPI. Adding to the tangle of titles, names, institutes and acronyms, longtermism is one of the main ‘cause areas’ of the so-called effective altruism (EA) movement, which was introduced by Ord in around 2011 and now \\u003ca href=\\\"https://80000hours.org/2021/07/effective-altruism-growing/\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003eboasts\\u003c/a\\u003e of having a mind-boggling \\u003cspan class=\\\"ld-nowrap\\\"\\u003e$46 billion\\u003c/span\\u003e in committed funding.\\u003c/p\\u003e\\n\\u003cp\\u003eIt is difficult to overstate how influential longtermism has become. Karl Marx in 1845 declared that the point of philosophy isn’t merely to interpret the world but change it, and this is exactly what longtermists have been doing, with extraordinary success. Consider that \\u003ca href=\\\"https://aeon.co/essays/elon-musk-puts-his-case-for-a-multi-planet-civilisation\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\"\\u003eElon Musk\\u003c/a\\u003e, who has cited and endorsed Bostrom’s work, has donated $1.5 million dollars to FHI through its sister organisation, the even more grandiosely named Future of Life Institute (FLI). This was cofounded by the multimillionaire tech entrepreneur Jaan Tallinn, who, as I recently \\u003ca href=\\\"https://www.currentaffairs.org/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk?fbclid=IwAR1zhM1QqoEqF1bhNicGcZ5la7wGp3Q4hU0t9ytfbM2gBGrhOjGhFOl-NC8\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003enoted\\u003c/a\\u003e, doesn’t believe that climate change poses an ‘existential risk’ to humanity because of his adherence to the longtermist ideology.\\u003c/p\\u003e\\n\\u003cp\\u003eMeanwhile, the billionaire libertarian and Donald Trump supporter Peter Thiel, who once gave the keynote address at an EA conference, has donated large sums of money to the Machine Intelligence Research Institute, whose mission to save humanity from superintelligent machines is deeply intertwined with longtermist values. Other organisations such as GPI and the Forethought Foundation are funding essay contests and scholarships in an effort to draw young people into the community, while it’s an open secret that the Washington, DC-based Center for Security and Emerging Technologies (CSET) aims to place longtermists within high-level US government positions to shape national policy. In fact, CSET was established by Jason Matheny, a former research assistant at FHI who’s now the deputy assistant to US President Joe Biden for technology and national security. Ord himself has, astonishingly for a philosopher, ‘advised the World Health Organization, the World Bank, the World Economic Forum, the US National Intelligence Council, the UK Prime Minister’s Office, Cabinet Office, and Government Office for Science’, and he recently \\u003ca href=\\\"https://forum.effectivealtruism.org/posts/Fwu2SLKeM5h5v95ww/major-un-report-discusses-existential-risk-and-future%23Context\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003econtributed\\u003c/a\\u003e to a report from the Secretary-General of the United Nations that specifically mentions ‘long-termism’.\\u003c/p\\u003e\\n\\u003cp\\u003eThe point is that longtermism might be one of the most influential ideologies that few people outside of elite universities and Silicon Valley have ever heard about. I believe this needs to change because, as a former longtermist who published an entire \\u003ca href=\\\"https://www.amazon.com/Morality-Foresight-Human-Flourishing-Introduction/dp/1634311426/ref=sr_1_1?dchild=1\\u0026amp;keywords=morality+foresight+torres\\u0026amp;qid=1625225082\\u0026amp;sr=8-1\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003ebook\\u003c/a\\u003e four years ago in defence of the general idea, I have come to see this worldview as quite possibly the most dangerous secular belief system in the world today. But to understand the nature of the beast, we need to first dissect it, examining its anatomical features and physiological functions.\\u003c/p\\u003e\\n\\u003cp\\u003e\\u003cspan class=\\\"ld-dropcap\\\"\\u003eT\\u003c/span\\u003ehe initial thing to notice is that longtermism, as proposed by Bostrom and Beckstead, is not equivalent to ‘caring about the long term’ or ‘valuing the wellbeing of future generations’. It goes way beyond this. At its core is a simple – albeit flawed, in my opinion – analogy between individual persons and humanity as a whole. To illustrate the idea, consider the case of \\u003ca href=\\\"https://aeon.co/essays/what-is-truth-on-ramsey-wittgenstein-and-the-vienna-circle\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\"\\u003eFrank Ramsey\\u003c/a\\u003e, a scholar at the University of Cambridge widely considered by his peers as among his generation’s most exceptional minds. ‘There was something of Newton about him,’ the belletrist Lytton Strachey once said. \\u003cspan class=\\\"ld-nowrap\\\"\\u003eG E Moore\\u003c/span\\u003e wrote of Ramsey’s ‘very exceptional brilliance’. And John Maynard Keynes described a paper of Ramsey’s as ‘one of the most remarkable contributions to mathematical economics ever made’.\\u003c/p\\u003e\\n\\u003cp\\u003eBut Ramsey’s story isn’t a happy one. On \\u003cspan class=\\\"ld-nowrap\\\"\\u003e19 January\\u003c/span\\u003e 1930, he died in a London hospital following a surgical procedure, the likely cause of death being a liver infection from swimming in the River Cam, which winds its way through Cambridge. Ramsey was only \\u003cspan class=\\\"ld-nowrap\\\"\\u003e26 years\\u003c/span\\u003e old.\\u003c/p\\u003e\\n\\u003cp\\u003eOne could argue that there are two distinct reasons this outcome was tragic. The first is the most obvious: it cut short Ramsey’s life, depriving him of everything he could have experienced had he survived – the joys and happiness, the love and friendship: all that makes life worth living. In this sense, Ramsey’s early demise was a personal tragedy. But, secondly, his death also robbed the world of an intellectual superstar apparently destined to make even more extraordinary contributions to human knowledge. ‘The number of trails Ramsey laid was remarkable,’ writes Sir Partha Dasgupta. But how many more trails might he have blazed? ‘The loss to your generation is agonising to think of,’ Strachey lamented, ‘what a light has gone out’ – which leaves one wondering how Western intellectual history might have been different if Ramsey hadn’t died so young. From this perspective, one could argue that, although the personal tragedy of Ramsey’s death was truly terrible, the immensity of his potential to have changed the world for the better makes the second tragedy even worse. In other words, the badness of his death stems mostly, perhaps overwhelmingly, from his unfulfilled potential rather than the direct, personal harms that he experienced. Or so the argument goes.\\u003c/p\\u003e\\n\\u003cp\\u003eLongtermists would map these claims and conclusions on to humanity itself, as if humanity is an individual with its very own ‘potential’ to squander or fulfil, ruin or realise, over the course of ‘its lifetime’. So, on the one hand, a catastrophe that reduces the human population to zero would be tragic because of all the suffering it would inflict upon those alive at the time. Imagine the horror of starving to death in subfreezing temperatures, under pitch-black skies at noon, for years or decades after a thermonuclear war. This is the first tragedy, a personal tragedy for those directly affected. But there is, longtermists would argue, a second tragedy that is astronomically worse than the first, arising from the fact that our extinction would permanently foreclose what could be an extremely long and prosperous future over the next, say, ~10\\u003csup class=\\\"ld-superscript\\\"\\u003e100\\u003c/sup\\u003e years (at which point the ‘heat death’ will \\u003ca href=\\\"https://aeon.co/essays/welcome-to-earth-2200-ad-pop-500-million-temp-180-f\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\"\\u003emake\\u003c/a\\u003e life impossible). In doing this, it would irreversibly destroy the ‘vast and glorious’ longterm potential of humanity, in Ord’s almost religious language – a ‘potential’ so huge, given the size of the Universe and the time left before reaching thermodynamic equilibrium, that the first tragedy would utterly pale in comparison.\\u003c/p\\u003e\\n\\u003cp\\u003eThis immediately suggests another parallel between individuals and humanity: death isn’t the only way that someone’s potential could be left unfulfilled. Imagine that Ramsey hadn’t died young but, instead of studying, writing and publishing scholarly papers, he’d spent his days in the local bar playing pool and drinking. Same outcome, different failure mode. Applying this to humanity, longtermists would argue that there are failure modes that could leave our potential unfulfilled without us dying out, which I will return to below.\\u003c/p\\u003e\\n\\u003cp class=\\\"pullquote\\\"\\u003eOn this view, a climate catastrophe will be a small blip – like a 90-year-old who stubbed his toe when he was two\\u003c/p\\u003e\\n\\u003cp\\u003eTo summarise these ideas so far, humanity has a ‘potential’ of its own, one that transcends the potentials of each individual person, and failing to realise this potential would be extremely bad – indeed, as we will see, a moral catastrophe of literally cosmic proportions. This is the central dogma of longtermism: nothing matters more, ethically speaking, than fulfilling our potential as a species of ‘Earth-originating intelligent life’. It matters so much that longtermists have even \\u003ca href=\\\"https://nickbostrom.com/existential/risks.html\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003ecoined\\u003c/a\\u003e the scary-sounding term ‘existential risk’ for any possibility of our potential being destroyed, and ‘existential catastrophe’ for any event that actually destroys this potential.\\u003c/p\\u003e\\n\\u003cp\\u003eWhy do I think this ideology is so dangerous? The short answer is that elevating the fulfilment of humanity’s supposed potential above all else could nontrivially increase the probability that actual people – those alive today and in the near future – suffer extreme harms, even death. Consider that, as I noted elsewhere, the longtermist ideology inclines its adherents to take an insouciant attitude towards climate change. Why? Because even if climate change causes island nations to disappear, triggers mass migrations and kills millions of people, it probably isn’t going to compromise our longterm potential over the coming trillions of years. If one takes a cosmic view of the situation, even a climate catastrophe that cuts the human population by \\u003cspan class=\\\"ld-nowrap\\\"\\u003e75 per\\u003c/span\\u003e cent for the next two millennia will, in the grand scheme of things, be nothing more than a small blip – the equivalent of a 90-year-old man having stubbed his toe when he was two.\\u003c/p\\u003e\\n\\u003cp\\u003eBostrom’s \\u003ca href=\\\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.304.7392\\u0026amp;rep=rep1\\u0026amp;type=pdf\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003eargument\\u003c/a\\u003e is that ‘a non-existential disaster causing the breakdown of global civilisation is, from the perspective of humanity as a whole, a potentially recoverable setback.’ It might be ‘a giant massacre for man’, he adds, but so long as humanity bounces back to fulfil its potential, it will ultimately register as little more than ‘a small misstep for mankind’. Elsewhere, he writes that the worst natural disasters and devastating atrocities in history become almost imperceptible trivialities when seen from this grand perspective. Referring to the two world wars, AIDS and the Chernobyl nuclear accident, he declares that ‘tragic as such events are to the people immediately affected, in the big picture of things … even the worst of these catastrophes are mere ripples on the surface of the great sea of life.’\\u003c/p\\u003e\\n\\u003cp\\u003eThis way of seeing the world, of assessing the badness of AIDS and the Holocaust, implies that future disasters of the same (non-existential) scope and intensity should also be categorised as ‘mere ripples’. If they don’t pose a direct existential risk, then we ought not to worry much about them, however tragic they might be to individuals. As Bostrom \\u003ca href=\\\"https://www.nickbostrom.com/astronomical/waste.html\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003ewrote\\u003c/a\\u003e in 2003, ‘priority number one, two, three and four should … be to reduce existential risk.’ He \\u003ca href=\\\"https://www.existential-risk.org/concept.html\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003ereiterated\\u003c/a\\u003e this several years later in arguing that we mustn’t ‘fritter … away’ our finite resources on ‘feel-good projects of suboptimal efficacy’ such as alleviating global poverty and reducing animal suffering, since neither threatens our longterm potential, and our longterm potential is what really matters.\\u003c/p\\u003e\\n\\u003cp\\u003eOrd echoes these views in arguing that, of all the problems facing humanity, our ‘first great task … is to reach a place of safety – a place where existential risk’ – as he defines it – ‘is low and stays low’, which he dubs ‘existential security’. More than anything else, what matters is doing everything necessary to ‘preserve’ and ‘protect’ our potential by ‘extracting ourselves from immediate danger’ and devising robust ‘safeguards that will defend humanity from dangers over the longterm future, so that it becomes impossible to fail.’ Although Ord gives a nod to climate change, he also claims – based on a dubious methodology – that the chance of climate change causing an existential catastrophe is only ∼1 in 1,000, which is a whole two orders of magnitude lower than the probability of superintelligent machines destroying humanity this century, according to Ord.\\u003c/p\\u003e\\n\\u003cp\\u003eWhat’s really notable here is that the central concern isn’t the effect of the climate catastrophe on actual people around the world (remember, in the grand scheme, this would be, in Bostrom’s words, a ‘small misstep for mankind’) but the slim possibility that, as Ord puts it in \\u003cem\\u003eThe Precipice\\u003c/em\\u003e, this catastrophe ‘poses a risk of an unrecoverable collapse of civilisation or even the complete extinction of humanity’. Again, the harms caused to actual people (especially those in the Global South) might be significant in absolute terms, but when compared to the ‘vastness’ and ‘glory’ of our longterm potential in the cosmos, they hardly even register.\\u003c/p\\u003e\\n\\u003cp\\u003e\\u003cspan class=\\\"ld-dropcap\\\"\\u003eY\\u003c/span\\u003eet the implications of longtermism are far more worrisome. If our top four priorities are to avoid an existential catastrophe – ie, to fulfil ‘our potential’ – then what’s not on the table for making this happen? Consider \\u003ca href=\\\"https://aeon.co/essays/what-s-it-all-for-is-a-question-that-belongs-in-the-past\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\"\\u003eThomas Nagel\\u003c/a\\u003e’s comment about how the notion of what we might call the ‘greater good’ has been used to ‘justify’ certain atrocities (eg, during war). If the ends ‘justify’ the means, he argues, and the ends are thought to be sufficiently large (eg, national security), then this ‘can be brought to bear to ease the consciences of those responsible for a certain number of charred babies’. Now imagine what might be ‘justified’ if the ‘greater good’ isn’t national security but the cosmic potential of Earth-originating intelligent life over the coming trillions of years? During the Second World War, \\u003cspan class=\\\"ld-nowrap\\\"\\u003e40 million\\u003c/span\\u003e civilians perished, but compare this number to the 10\\u003csup class=\\\"ld-superscript\\\"\\u003e54\\u003c/sup\\u003e or more people (in Bostrom’s \\u003ca href=\\\"https://www.existential-risk.org/faq.pdf\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003eestimate\\u003c/a\\u003e) who could come to exist if we can avoid an existential catastrophe. What shouldn’t we do to ‘protect’ and ‘preserve’ this potential? To ensure that these unborn people come to exist? What means can’t be ‘justified’ by this cosmically significant moral end?\\u003c/p\\u003e\\n\\u003cp\\u003eBostrom himself \\u003ca href=\\\"https://www.nickbostrom.com/papers/vulnerable.pdf\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003eargued\\u003c/a\\u003e that we should seriously consider establishing a global, invasive surveillance system that monitors every person on the planet in realtime, to amplify the ‘capacities for preventive policing’ (eg, to prevent omnicidal terrorist attacks that could devastate civilisation). Elsewhere, he’s written that states should use preemptive violence/war to avoid existential catastrophes, and argued that saving billions of actual people is the moral equivalent of reducing existential risk by utterly minuscule amounts. In his words, even if there is ‘a mere \\u003cspan class=\\\"ld-nowrap\\\"\\u003e1 per \\u003c/span\\u003ecent chance’ of 10\\u003csup class=\\\"ld-superscript\\\"\\u003e54\\u003c/sup\\u003e people existing in the future, then ‘the expected value of reducing existential risk by a mere \\u003cem\\u003eone billionth of one billionth of one percentage point\\u003c/em\\u003e is worth \\u003cspan class=\\\"ld-nowrap\\\"\\u003e100 billion\\u003c/span\\u003e times as much as a billion human lives.’ Such fanaticism – a word that some longtermists \\u003ca href=\\\"https://globalprioritiesinstitute.org/wp-content/uploads/Hayden-Wilkinson_In-defence-of-fanaticism.pdf\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003eembrace\\u003c/a\\u003e – has led a growing number of critics to worry about what might happen if political leaders in the real world were to take Bostrom’s view seriously. To \\u003ca href=\\\"https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780198723547.001.0001/acprof-9780198723547\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003equote\\u003c/a\\u003e the mathematical statistician Olle Häggström, who – perplexingly – tends otherwise to speak favourably of longtermism:\\u003c/p\\u003e\\n\\u003cblockquote\\u003eI feel extremely uneasy about the prospect that [the calculations above] might become recognised among politicians and decision-makers as a guide to policy worth taking literally. It is simply too reminiscent of the old saying ‘If you want to make an omelette, you must be willing to break a few eggs,’ which has typically been used to explain that a bit of genocide or so might be a good thing, if it can contribute to the goal of creating a future utopia. Imagine a situation where the head of the CIA explains to the US president that they have credible evidence that somewhere in Germany, there is a lunatic who is working on a doomsday weapon and intends to use it to wipe out humanity, and that this lunatic has a one-in-a-million chance of succeeding. They have no further information on the identity or whereabouts of this lunatic. If the president has taken Bostrom’s argument to heart, and if he knows how to do the arithmetic, he may conclude that it is worthwhile conducting a full-scale nuclear assault on Germany to kill every single person within its borders.\\u003c/blockquote\\u003e\\n\\u003cp\\u003eHere, then, are a few reasons I find longtermism to be profoundly dangerous. Yet there are additional, fundamental problems with this worldview that no one, to my knowledge, has previously noted in writing. For example, there’s a good case to make that the underlying commitments of longtermism are a major reason why humanity faces so many unprecedented risks to its survival in the first place. Longtermism might, in other words, be incompatible with the attainment of ‘existential security’, meaning that the only way to genuinely reduce the probability of extinction or collapse in the future might be to abandon the longtermist ideology entirely.\\u003c/p\\u003e\\n\\u003cp class=\\\"pullquote\\\"\\u003eTo Bostrom and Ord, failing to become posthuman would prevent us from realising our vast, glorious potential\\u003c/p\\u003e\\n\\u003cp\\u003eTo understand the argument, let’s first unpack what longtermists mean by our ‘longterm potential’, an expression that I have so far used without defining. We can analyse this concept into three main components: transhumanism, space expansionism, and a moral view closely associated with what philosophers call ‘total utilitarianism’.\\u003c/p\\u003e\\n\\u003cp\\u003eThe first refers to the \\u003ca href=\\\"https://aeon.co/essays/why-is-the-language-of-transhumanists-and-religion-so-similar\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\"\\u003eidea\\u003c/a\\u003e that we should use advanced technologies to reengineer our bodies and brains to create a ‘superior’ race of radically enhanced posthumans (which, confusingly, longtermists place within the category of ‘humanity’). Although Bostrom is perhaps the most prominent transhumanist today, longtermists have shied away from using the term ‘transhumanism’, probably because of its negative associations. Susan Levin, for example, \\u003ca href=\\\"https://blog.oup.com/2021/01/playing-to-lose-transhumanism-autonomy-and-liberal-democracy-long-read/\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003epoints out\\u003c/a\\u003e that contemporary transhumanism has its roots in the Anglo-American eugenics movement, and transhumanists such as \\u003ca href=\\\"https://aeon.co/users/julian-savulescu\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\"\\u003eJulian Savulescu\\u003c/a\\u003e, who co-edited the \\u003ca href=\\\"https://global.oup.com/academic/product/human-enhancement-9780199299720?cc=gb\\u0026amp;lang=en\\u0026amp;\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003ebook\\u003c/a\\u003e \\u003cem\\u003eHuman Enhancement\\u003c/em\\u003e (2009) with Bostrom, have literally argued for the consumption of ‘morality-boosting’ chemicals such as oxytocin to avoid an existential catastrophe (which he \\u003ca href=\\\"https://jme.bmj.com/content/37/7/441\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003ecalls\\u003c/a\\u003e ‘ultimate harm’). As Savulescu \\u003ca href=\\\"https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-8519.2011.01907.x\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003ewrites\\u003c/a\\u003e with a colleague, ‘it is a matter of such urgency to improve humanity morally … that we should seek \\u003cem\\u003ewhatever\\u003c/em\\u003e means there are to effect this.’ Such claims are not only controversial but for many quite disturbing, and hence longtermists have attempted to distance themselves from such ideas, while nonetheless championing the ideology.\\u003c/p\\u003e\\n\\u003cp\\u003eTranshumanism \\u003ca href=\\\"https://www.nickbostrom.com/posthuman.pdf\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003eclaims\\u003c/a\\u003e that there are various ‘posthuman modes of being’ that are far better than our current human mode. We could, for instance, genetically alter ourselves to gain perfect control over our emotions, or access the internet via neural implants, or maybe even upload our minds to computer hardware to achieve ‘digital immortality’. As Ord urges in \\u003cem\\u003eThe Precipice\\u003c/em\\u003e, think of how awesome it would be to perceive the world via echolocation, like bats and dolphins, or magnetoreception, like red foxes and homing pigeons. ‘Such uncharted experiences,’ Ord writes, ‘exist in minds much less sophisticated than our own. What experiences, possibly of immense value, could be accessible, then, to minds much greater?’ Bostrom’s most fantastical exploration of these possibilities comes from his \\u003ca href=\\\"https://www.nickbostrom.com/utopia.html\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003eevocative\\u003c/a\\u003e ‘Letter from Utopia’ (2008), which depicts a techno-Utopian world full of superintelligent posthumans awash in so much ‘pleasure’ that, as the letter’s fictional posthuman writes, ‘we sprinkle it in our tea.’\\u003c/p\\u003e\\n\\u003cp\\u003eThe connection with longtermism is that, according to Bostrom and Ord, failing to become posthuman would seemingly prevent us from realising our vast and glorious potential, which would be existentially catastrophic. As Bostrom put it in 2012, ‘the permanent foreclosure of any possibility of this kind of transformative change of human biological nature may itself constitute an existential catastrophe.’ Similarly, Ord asserts that ‘forever preserving humanity as it is now may also squander our legacy, relinquishing the greater part of our potential.’\\u003c/p\\u003e\\n\\u003cp\\u003e\\u003cspan class=\\\"ld-dropcap\\\"\\u003eT\\u003c/span\\u003ehe second component of our potential – space expansionism – refers to the idea that we must colonise as much of our future light cone as possible: that is, the region of spacetime that is theoretically accessible to us. According to longtermists, our future light cone contains a huge quantity of exploitable resources, which they refer to as our ‘cosmic endowment’ of negentropy (or reverse entropy). The Milky Way alone, Ord writes, is ‘150,000 light years across, encompassing more than \\u003cspan class=\\\"ld-nowrap\\\"\\u003e100 billion\\u003c/span\\u003e stars, most with their own planets.’ Attaining humanity’s longterm potential, he continues, ‘requires only that [we] eventually travel to a nearby star and establish enough of a foothold to create a new flourishing society from which we could venture further.’ By spreading ‘just six light years at a time’, our posthuman descendants could make ‘almost all the stars of our galaxy … reachable’ since ‘each star system, including our own, would need to settle just the few nearest stars [for] the entire galaxy [to] eventually fill with life.’ The process could be exponential, resulting in ever-more ‘flourishing’ societies with each additional second our descendants hop from star to star.\\u003c/p\\u003e\\n\\u003cp\\u003eBut why exactly would we want to do this? What’s so important about flooding the Universe with new posthuman civilisations? This leads to the third component: total utilitarianism, which I will refer to as ‘utilitarianism’ for short. Although some longtermists insist that they aren’t utilitarians, we should right away note that this is mostly a smoke-and-mirrors act to deflect criticisms that longtermism – and, more generally, the effective altruism (EA) movement from which it emerged – is nothing more than utilitarianism \\u003ca href=\\\"https://blog.apaonline.org/2021/03/29/is-effective-altruism-inherently-utilitarian/\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003erepackaged\\u003c/a\\u003e. The fact is that the EA movement is deeply utilitarian, at least in practice, and indeed, before it decided upon a name, the movement’s early members, including Ord, seriously considered calling it the ‘effective utilitarian community’.\\u003c/p\\u003e\\n\\u003cp\\u003eThis being said, utilitarianism is an ethical theory that specifies our sole moral obligation as being to maximise the total amount of ‘intrinsic value’ in the world, as tallied up from a disembodied, impartial, cosmic vantage point called ‘the point of view of the Universe’. From this view, it doesn’t matter how value – which utilitarian hedonists equate with pleasure – is distributed among people across space and time. All that matters is the total net sum. For example, imagine that there are \\u003cspan class=\\\"ld-nowrap\\\"\\u003e1 trillion\\u003c/span\\u003e people who have lives of value ‘1’, meaning that they are just barely worth living. This gives a total value of \\u003cspan class=\\\"ld-nowrap\\\"\\u003e1 trillion\\u003c/span\\u003e. Now consider an alternative universe in which \\u003cspan class=\\\"ld-nowrap\\\"\\u003e1 billion\\u003c/span\\u003e people have lives with a value of ‘999’, meaning that their lives are extremely good. This gives a total value of \\u003cspan class=\\\"ld-nowrap\\\"\\u003e999 billion.\\u003c/span\\u003e Since \\u003cspan class=\\\"ld-nowrap\\\"\\u003e999 billion\\u003c/span\\u003e is less than \\u003cspan class=\\\"ld-nowrap\\\"\\u003e1 trillion,\\u003c/span\\u003e the first world full of lives hardly worth living would be morally better than the second world, and hence, if a utilitarian were forced to choose between these, she would pick the former. (This is called the ‘repugnant conclusion’, which longtermists such as Ord, MacAskill and Greaves recently \\u003ca href=\\\"https://www.cambridge.org/core/journals/utilitas/article/what-should-we-agree-on-about-the-repugnant-conclusion/EB52C686BAFEF490CE37043A0A3DD075\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003eargued\\u003c/a\\u003e shouldn’t be taken very seriously. For them, the first world really might be better!)\\u003c/p\\u003e\\n\\u003cp class=\\\"pullquote\\\"\\u003eBeckstead argued that we should prioritise the lives of people in rich countries over those in poor countries\\u003c/p\\u003e\\n\\u003cp\\u003eThe underlying reasoning here is based on the idea that people – you and I – are nothing more than means to an end. We don’t matter in ourselves; we have no inherent value of our own. Instead, people are understood as the ‘containers’ of value, and hence we matter only insofar as we ‘contain’ value, and therefore contribute to the overall net amount of value in the Universe between the Big Bang and the heat death. Since utilitarianism tells us to maximise value, it follows that the more people (value containers) who exist with net-positive amounts of value (pleasure), the better the Universe will become, morally speaking. In a phrase: people exist for the sake of maximising value, rather than value existing for the sake of benefitting people.\\u003c/p\\u003e\\n\\u003cp\\u003eThis is why longtermists are obsessed with calculating how many people could exist in the future if we were to colonise space and create vast computer simulations around stars in which unfathomably huge numbers of people live net-positive lives in virtual-reality environments. I already mentioned Bostrom’s estimate of 10\\u003csup class=\\\"ld-superscript\\\"\\u003e54\\u003c/sup\\u003e future people, which includes many of these ‘digital people’, but in his \\u003ca href=\\\"https://global.oup.com/academic/product/superintelligence-9780199678112?cc=gb\\u0026amp;lang=en\\u0026amp;\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003ebestseller\\u003c/a\\u003e \\u003cem\\u003eSuperintelligence\\u003c/em\\u003e (2014) he puts the number even higher at 10\\u003csup class=\\\"ld-superscript\\\"\\u003e58\\u003c/sup\\u003e people, nearly all of whom would ‘live rich and happy lives while interacting with one another in virtual environments’. Greaves and MacAskill are similarly excited about this possibility, \\u003ca href=\\\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003eestimating\\u003c/a\\u003e that some 10\\u003csup class=\\\"ld-superscript\\\"\\u003e45\\u003c/sup\\u003e conscious beings in computer simulations could exist within the Milky Way alone.\\u003c/p\\u003e\\n\\u003cp\\u003eThat is what our ‘vast and glorious’ potential consists of: massive numbers of technologically enhanced digital posthumans inside huge computer simulations spread throughout our future light cone. It is for this goal that, in Häggström’s scenario, a longtermist politician would annihilate Germany. It is for this goal that we must not ‘fritter … away’ our resources on such things as solving global poverty. It is for this goal that we should consider implementing a global surveillance system, keep pre-emptive war on the table, and focus more on superintelligent machines than saving people in the Global South from the devastating effects of climate change (mostly caused by the Global North). In fact, Beckstead has even argued that, for the sake of attaining this goal, we should actually prioritise the lives of people in rich countries over those in poor countries, since influencing the long-term future is of ‘overwhelming importance’, and the former are more likely to influence the long-term future than the latter. To \\u003ca href=\\\"https://www.proquest.com/docview/1442191960?fromopenview=tr%2522a%2520long%2520period%25E2%2580%2594perhaps%2520tens%2520of%2520thousands%2520of%2520years%25E2%2580%2594during%2520which%2520human%2520civilisation,%2520perhaps%2520with%2520the%2520aid%2520of%2520improved%2520cognitive%2520ability,%2520dedicates%2520itself%2520to%2520working%2520out%2520what%2520is%2520ultimately%2520of%2520value%2522ue\\u0026amp;pq-origsite=gscholar\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003equote\\u003c/a\\u003e a passage from Beckstead’s 2013 PhD dissertation, which Ord enthusiastically praises as one of the most important contributions to the longtermist literature:\\u003c/p\\u003e\\n\\u003cblockquote\\u003eSaving lives in poor countries may have significantly smaller ripple effects than saving and improving lives in rich countries. Why? Richer countries have substantially more innovation, and their workers are much more economically productive. [Consequently,] it now seems more plausible to me that saving a life in a rich country is substantially more important than saving a life in a poor country, other things being equal.\\u003c/blockquote\\u003e\\n\\u003cp\\u003e\\u003cspan class=\\\"ld-dropcap\\\"\\u003eT\\u003c/span\\u003ehis is just the tip of the iceberg. Consider the implications of this conception of ‘our potential’ for the development of technology and creation of new risks. Since realising our potential is the ultimate moral goal for humanity, and since our descendants cannot become posthuman, colonise space and create ~10\\u003csup class=\\\"ld-superscript\\\"\\u003e58\\u003c/sup\\u003e people in computer simulations without technologies far more advanced than those around today, failing to develop more technology would itself constitute an existential catastrophe – a failure mode (comparable to Ramsey neglecting his talents by spending his days playing pool and drinking) that Bostrom calls ‘plateauing’. Indeed, Bostrom places this idea front-and-centre in his canonical definition of ‘existential risk’, which denotes any future event that would prevent humanity from reaching and/or sustaining a state of ‘technological maturity’, meaning ‘the attainment of capabilities affording a level of economic productivity and control over nature close to the maximum that could feasibly be achieved.’ Technological maturity is the linchpin here because controlling nature and increasing economic productivity to the absolute physical limits are ostensibly necessary for creating the maximum quantity of ‘value’ within our future light cone.\\u003c/p\\u003e\\n\\u003cp\\u003eBut reflect for a moment on how humanity got itself into the current climatic and ecological crisis. Behind the extraction and burning of fossil fuels, decimation of ecosystems and extermination of species has been the notion that nature is something to be controlled, subjugated, exploited, vanquished, plundered, transformed, reconfigured and manipulated. As the technology theorist Langdon Winner \\u003ca href=\\\"https://mitpress.mit.edu/books/autonomous-technology\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003ewrites\\u003c/a\\u003e in \\u003cem\\u003eAutonomous Technology\\u003c/em\\u003e (1977), since the time of Francis Bacon our view of technology has been ‘inextricably bound to a single conception of the manner in which power is used – the style of absolute mastery, the despotic, one-way control of the master over the slave.’ He adds:\\u003c/p\\u003e\\n\\u003cblockquote\\u003eThere are seldom any reservations about man’s rightful role in conquering, vanquishing, and subjugating everything natural. This is his power and his glory. What would in other situations seem [to be] rather tawdry and despicable intentions are here the most honourable of virtues. Nature is the universal prey, to manipulate as humans see fit.\\u003c/blockquote\\u003e\\n\\u003cp\\u003eThis is precisely what we find in Bostrom’s account of existential risks and its associated normative futurology: nature, the entire Universe, our ‘cosmic endowment’ is there for the plundering, to be manipulated, transformed and converted into ‘value-structures, such as sentient beings living worthwhile lives’ in vast computer simulations, quoting Bostrom’s essay ‘Astronomical Waste’ (2003). Yet this Baconian, capitalist view is one of the most fundamental root causes of the unprecedented environmental crisis that now threatens to destroy large regions of the biosphere, Indigenous communities around the world, and perhaps even Western technological civilisation itself. While other longtermists have not been as explicit as Bostrom, there is a clear tendency to see the natural world the way utilitarianism sees people: as means to some abstract, impersonal end, and nothing more. MacAskill and a colleague, for example, \\u003ca href=\\\"https://philarchive.org/archive/PUMEAv3\\\" target=\\\"_blank\\\" rel=\\\"noreferrer noopener\\\"\\u003ewrite\\u003c/a\\u003e that the EA movement, and by implication longtermism, is ‘tentatively \\u003cem\\u003ewelfarist\\u003c/em\\u003e in that its tentative aim in doing good concerns promoting wellbeing only and not, say, protecting biodiversity or conserving natural beauty for their own sakes.’\\u003c/p\\u003e\\n\\u003cp class=\\\"pullquote\\\"\\u003eOn this account, every problem arises from too little rather than too much technology\\u003c/p\\u003e\\n\\u003cp\\u003eJust as worrisome is the longtermist demand that we must create ever-more powerful technologies, despite the agreed-upon fact that the overwhelming source of risk to human existence these days comes from these very technologies. In Ord’s words, ‘without serious efforts to protect humanity, there is strong reason to believe the risk will be higher this century, and increasing with each century that technological progress continues.’ Similarly, in 2012 Bostrom acknowledges that\\u003c/p\\u003e\\n\\u003cblockquote\\u003ethe great bulk of existential risk in the foreseeable future consists of anthropogenic existential risks – that is, arising from human activity. In particular, most of the biggest existential risks seem to be linked to potential future technological breakthroughs that may radically expand our ability to manipulate the external world or our own biology. As our powers expand, so will the scale of their potential consequences – intended and unintended, positive and negative.\\u003c/blockquote\\u003e\\n\\u003cp\\u003eOn this view, there is only one way forward – more technological development – even if this is the most dangerous path into the future. But how much sense does this make? Surely if we want to maximise our chances of survival, we should oppose the development of dangerous new dual-use technologies. If more technology equals greater risk – as history clearly shows and technological projections affirm – then perhaps the only way to actually attain a state of ‘existential security’ is to slow down or completely halt further technological innovation.\\u003c/p\\u003e\\n\\u003cp\\u003eBut longtermists have an answer to this conundrum: the so-called ‘value-neutrality thesis’. This states that technology is a morally neutral object, ie, ‘just a tool’. The idea is most famously encapsulated in the NRA’s slogan ‘Guns don’t kill people, people kill people,’ which conveys the message that the consequences of technology, whether good or bad, beneficial or harmful, are entirely determined by the users, not the artefacts. As Bostrom put it in 2002, ‘we should not \\u003cem\\u003eblame\\u003c/em\\u003e civilisation or technology for imposing big existential risks,’ adding that ‘because of the way we have defined existential risks, a failure to develop technological civilisation would imply that we had fallen victims of an existential disaster.’\\u003c/p\\u003e\\n\\u003cp\\u003eOrd similarly argues that ‘the problem is not so much an excess of technology as a lack of wisdom,’ before going on to quote Carl Sagan’s book \\u003cem\\u003ePale Blue Dot\\u003c/em\\u003e (1994): ‘Many of the dangers we face indeed arise from science and technology but, more fundamentally, because we have become powerful without becoming commensurately wise.’ In other words, it is our fault for not being smarter, wiser and more ethical, a cluster of deficiencies that many longtermists believe, in a bit of twisted logic, could be rectified by technologically reengineering our cognitive systems and moral dispositions. Everything, on this account, is an engineering problem, and hence every problem arises from too little rather than too much technology.\\u003c/p\\u003e\\n\\u003cp\\u003eWe can now begin to see how longtermism might be self-defeating. Not only could its ‘fanatical’ emphasis on fulfilling our longterm potential lead people to, eg, neglect non-existential climate change, prioritise the rich over the poor and perhaps even ‘justify’ pre-emptive violence and atrocities for the ‘greater cosmic good’ but it also contains within it the very tendencies – Baconianism, capitalism and value-neutrality – that have driven humanity inches away from the precipice of destruction. Longtermism tells us to maximise economic productivity, our control over nature, our presence in the Universe, the number of (simulated) people who exist in the future, the total amount of impersonal ‘value’ and so on. But to maximise, we must develop increasingly powerful – and dangerous – technologies; failing to do this would itself be an existential catastrophe. Not to worry, though, because technology is not responsible for our worsening predicament, and hence the fact that most risks stem directly from technology is no reason to stop creating more technology. Rather, the problem lies with us, which means only that we must create even more technology to transform ourselves into cognitively and morally enhanced posthumans.\\u003c/p\\u003e\\n\\u003cp\\u003eThis looks like a recipe for disaster. Creating a new race of ‘wise and responsible’ posthumans is implausible and, if advanced technologies continue to be developed at the current rate, a global-scale catastrophe is almost certainly a matter of when rather than if. Yes, we will need advanced technologies if we wish to escape Earth before it’s sterilised by the Sun in a billion years or so. But the crucial fact that longtermists miss is that \\u003cem\\u003etechnology is far more likely to cause our extinction before this distant future event than to save us from it\\u003c/em\\u003e. If you, like me, value the continued survival and flourishing of humanity, you should care about the long term but reject the ideology of longtermism, which is not only dangerous and flawed but might be contributing to, and reinforcing, the risks that now threaten every person on the planet.\\u003c/p\\u003e\",\"standfirstShort\":\"Why longtermism is the world’s most dangerous secular credo\",\"standfirstLong\":\"It started as a fringe philosophical theory about humanity’s future. It’s now richly funded and increasingly dangerous\",\"creativeCommons\":false,\"license\":{\"name\":\"Aeon Permissive (syndication allowed)\",\"short\":\"Aeon Permissive License\",\"url\":null,\"republicationAllowed\":true,\"commercialUseAllowed\":true},\"authors\":[{\"id\":\"120645\",\"name\":\"Émile P Torres\",\"authorBio\":\"\\u003cp\\u003eis a PhD candidate in philosophy at Leibniz Universität Hannover in Germany. Their writing has appeared in \\u003cem\\u003ePhilosophy Now\\u003c/em\\u003e, \\u003cem\\u003eNautilus\\u003c/em\\u003e, Motherboard and the \\u003cem\\u003eBulletin of the Atomic Scientists\\u003c/em\\u003e, among others. They are the author of \\u003cem\\u003eThe End: What Science and Religion Tell Us About the Apocalypse\\u003c/em\\u003e (2016), \\u003cem\\u003eMorality, Foresight, and Human Flourishing: An Introduction to Existential Risks\\u003c/em\\u003e (2017) and \\u003cem\\u003eHuman Extinction: A History of the Science and Ethics of Annihilation\\u003c/em\\u003e (forthcoming from Routledge).\\u003c/p\\u003e\",\"slug\":\"emile-p-torres\"}],\"image\":{\"caption\":\"\\u003cp\\u003eScarecrows keep away migratory birds from the dangers of the tailing ponds created by the exploitation on the tar sands at Fort McMurray, Alberta, Canada. \\u003cem\\u003ePhoto by Larry Towell/Magnum\\u003c/em\\u003e\\u003c/p\\u003e\",\"alt\":\"Scarecrow figures in yellow protective suits on a desolate sandy landscape under a cloudy sky, with industrial smoke in the distance.\",\"url\":\"https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg\",\"height\":1252,\"width\":2000},\"primaryTopic\":{\"title\":\"Thinkers and theories\",\"slug\":\"thinkers-and-theories\",\"url\":\"https://aeon.co/philosophy/thinkers-and-theories\"},\"topics\":[{\"slug\":\"thinkers-and-theories\",\"title\":\"Thinkers and theories\",\"url\":\"https://aeon.co/philosophy/thinkers-and-theories\",\"section\":{\"slug\":\"philosophy\",\"title\":\"Philosophy\",\"url\":\"https://aeon.co/philosophy\"}},{\"slug\":\"the-future\",\"title\":\"The future\",\"url\":\"https://aeon.co/society/the-future\",\"section\":{\"slug\":\"society\",\"title\":\"Society\",\"url\":\"https://aeon.co/society\"}},{\"slug\":\"values-beliefs\",\"title\":\"Values and beliefs\",\"url\":\"https://aeon.co/philosophy/values-beliefs\",\"section\":{\"slug\":\"philosophy\",\"title\":\"Philosophy\",\"url\":\"https://aeon.co/philosophy\"}}],\"settings\":{\"alignX\":\"right\",\"alignY\":\"top\",\"backdropStrength\":3,\"featureImgCaptionAlign\":\"right\"},\"section\":{\"title\":\"Philosophy\",\"slug\":\"philosophy\",\"url\":\"https://aeon.co/philosophy\"},\"editor\":{\"id\":\"16\",\"name\":\"Sam Dresser\",\"authorBio\":\"\\u003cp\\u003eis an editor at Aeon. He lives in New York.\\u003c/p\\u003e\",\"slug\":\"samdresser\"},\"audio\":{\"id\":\"1236\"},\"createdAt\":\"2021-10-01T13:12:36Z\",\"publishedAt\":\"2021-10-19T10:00:00Z\",\"scheduledAt\":null,\"readingTime\":31,\"wordCount\":\"6,100\",\"updatedAt\":\"2021-10-19T10:00:02Z\",\"lastUpdatedAt\":null,\"commentsEnabled\":true,\"commentsOpen\":true,\"commentAndRepliesCount\":85,\"newsletterHidden\":false,\"republishToken\":\"d197030d-085f-4dd1-ba70-a69f133c0c0e\",\"nonAudioPartners\":[]},\"relatedArticles\":[{\"id\":\"7429\",\"title\":\"C L R James and America\",\"slug\":\"c-l-r-james-foresaw-the-crisis-of-us-liberal-democracy\",\"type\":\"essay\",\"audio\":null,\"standfirstShort\":\"C L R James foresaw the crisis of US liberal democracy\",\"standfirstLong\":\"The brilliant Trinidadian thinker is remembered as an admirer of the US but he also warned of its dark political future\",\"duration\":0,\"creditsShort\":null,\"commentsEnabled\":false,\"settings\":{\"badgeColor\":null,\"cardPlayIconColor\":null,\"alignX\":\"center\",\"alignY\":\"center\",\"backdropStrength\":3},\"tags\":[],\"authors\":[{\"id\":\"203895\",\"name\":\"Harvey Neptune\"}],\"image\":{\"url\":\"https://images.aeonmedia.co/images/e7965c4b-cb81-41e6-a398-fb57fc2356c6/essay-the-public-enemy-.jpg\",\"alt\":\"Black-and-white photo of a man in a suit and hat grabbing another man by his collar in front of a bar with bottles.\",\"height\":1252,\"width\":2000},\"primaryTopic\":{\"title\":\"Political philosophy\",\"slug\":\"political-philosophy\"},\"topics\":[{\"slug\":\"political-philosophy\",\"title\":\"Political philosophy\"},{\"slug\":\"thinkers-and-theories\",\"title\":\"Thinkers and theories\"},{\"slug\":\"nations-and-empires\",\"title\":\"Nations and empires\"}],\"section\":{\"title\":\"Philosophy\",\"slug\":\"philosophy\"},\"createdAt\":\"2024-07-03T06:42:02Z\"},{\"id\":\"7475\",\"title\":\"The great wealth wave\",\"slug\":\"the-surprising-truth-about-wealth-and-inequality-in-the-west\",\"type\":\"essay\",\"audio\":null,\"standfirstShort\":\"The surprising truth about wealth and inequality in the West\",\"standfirstLong\":\"The tide has turned – evidence shows ordinary citizens in the Western world are now richer and more equal than ever before\",\"duration\":0,\"creditsShort\":null,\"commentsEnabled\":true,\"settings\":{\"badgeColor\":null,\"cardPlayIconColor\":null,\"alignX\":\"left\",\"alignY\":\"top\",\"backdropStrength\":3},\"tags\":[],\"authors\":[{\"id\":\"209435\",\"name\":\"Daniel Waldenström\"}],\"image\":{\"url\":\"https://images.aeonmedia.co/images/cf79ce56-22d5-485b-919e-43af3a6d508c/essay-v2-nn11492476.jpg\",\"alt\":\"A suburban street with mountains in the background, featuring a girl on a bike, parked cars, and old furniture on the sidewalk in front of a house.\",\"height\":1252,\"width\":2000},\"primaryTopic\":{\"title\":\"Progress and modernity\",\"slug\":\"progress-modernity\"},\"topics\":[{\"slug\":\"progress-modernity\",\"title\":\"Progress and modernity\"},{\"slug\":\"economic-history\",\"title\":\"Economic history\"},{\"slug\":\"economics\",\"title\":\"Economics\"}],\"section\":{\"title\":\"Society\",\"slug\":\"society\"},\"createdAt\":\"2024-08-01T10:42:34Z\"},{\"id\":\"7468\",\"title\":\"The melting brain\",\"slug\":\"how-a-warming-earth-is-changing-our-brains-bodies-and-minds\",\"type\":\"essay\",\"audio\":null,\"standfirstShort\":\"How a warming Earth is changing our brains, bodies and minds\",\"standfirstLong\":\"It’s not just the planet and not just our health – the impact of a warming climate extends deep into our cortical fissures\",\"duration\":0,\"creditsShort\":null,\"commentsEnabled\":true,\"settings\":{\"badgeColor\":null,\"cardPlayIconColor\":null,\"alignX\":\"center\",\"alignY\":\"center\",\"backdropStrength\":3},\"tags\":[],\"authors\":[{\"id\":\"207785\",\"name\":\"Clayton Page Aldern\"}],\"image\":{\"url\":\"https://images.aeonmedia.co/images/25b67b4d-d49d-4f8e-b96d-1c8b33ed0a16/essay-rtr2g5k7.jpg\",\"alt\":\"Silhouette of a person walking through a spray of water at sunset with cars and buildings in the background.\",\"height\":1252,\"width\":2000},\"primaryTopic\":{\"title\":\"Neuroscience\",\"slug\":\"neuroscience\"},\"topics\":[{\"slug\":\"neuroscience\",\"title\":\"Neuroscience\"},{\"slug\":\"earth-science-and-climate\",\"title\":\"Earth science and climate\"},{\"slug\":\"the-environment\",\"title\":\"The environment\"}],\"section\":{\"title\":\"Psychology\",\"slug\":\"psychology\"},\"createdAt\":\"2024-07-23T11:37:50Z\"},{\"id\":\"7451\",\"title\":\"Falling for suburbia\",\"slug\":\"why-millions-of-britons-fell-in-love-with-suburban-life\",\"type\":\"essay\",\"audio\":null,\"standfirstShort\":\"Why millions of Britons fell in love with suburban life\",\"standfirstLong\":\"Modernists and historians alike loathed the millions of new houses built in interwar Britain. But their owners loved them\",\"duration\":0,\"creditsShort\":null,\"commentsEnabled\":false,\"settings\":{\"badgeColor\":null,\"cardPlayIconColor\":null,\"alignX\":\"center\",\"alignY\":\"center\",\"backdropStrength\":3},\"tags\":[],\"authors\":[{\"id\":\"206706\",\"name\":\"Michael Gilson\"}],\"image\":{\"url\":\"https://images.aeonmedia.co/images/753e612a-9bae-4257-a34f-98ee4da12cc3/test-essay-roehouse.jpg\",\"alt\":\"A brick house with a tiled roof, surrounded by a well-maintained garden with bushes and colourful flowers.\",\"height\":1252,\"width\":2000},\"primaryTopic\":{\"title\":\"Home\",\"slug\":\"home\"},\"topics\":[{\"slug\":\"home\",\"title\":\"Home\"},{\"slug\":\"cities\",\"title\":\"Cities\"},{\"slug\":\"nature-and-landscape\",\"title\":\"Nature and landscape\"}],\"section\":{\"title\":\"Culture\",\"slug\":\"culture\"},\"createdAt\":\"2024-07-17T11:21:55Z\"},{\"id\":\"7335\",\"title\":\"Rawls the redeemer\",\"slug\":\"john-rawls-liberalism-and-what-it-means-to-live-a-good-life\",\"type\":\"essay\",\"audio\":null,\"standfirstShort\":\"John Rawls, liberalism and what it means to live a good life\",\"standfirstLong\":\"For John Rawls, liberalism was more than a political project: it is the best way to fashion a life that is worthy of happiness\",\"duration\":0,\"creditsShort\":null,\"commentsEnabled\":false,\"settings\":{\"badgeColor\":null,\"cardPlayIconColor\":null,\"alignX\":\"center\",\"alignY\":\"center\",\"backdropStrength\":3},\"tags\":[],\"authors\":[{\"id\":\"196759\",\"name\":\"Alexandre Lefebvre\"}],\"image\":{\"url\":\"https://images.aeonmedia.co/images/6b44ba64-ea98-49b8-a00c-f1ea564aaf29/essay-family-with-five-children-master-pnp-fsa-8b38000-8b38700-8b38702u.jpg\",\"alt\":\"An old photograph of a man pulling a small cart with a child and belongings, followed by a woman and three children; one child is pushing a stroller.\",\"height\":1252,\"width\":2000},\"primaryTopic\":{\"title\":\"Thinkers and theories\",\"slug\":\"thinkers-and-theories\"},\"topics\":[{\"slug\":\"thinkers-and-theories\",\"title\":\"Thinkers and theories\"},{\"slug\":\"meaning-the-good-life\",\"title\":\"Meaning and the good life\"},{\"slug\":\"political-philosophy\",\"title\":\"Political philosophy\"}],\"section\":{\"title\":\"Philosophy\",\"slug\":\"philosophy\"},\"createdAt\":\"2024-05-10T09:40:21Z\"},{\"id\":\"7463\",\"title\":\"Mere imitation\",\"slug\":\"is-ai-our-salvation-our-undoing-or-just-more-of-the-same\",\"type\":\"essay\",\"audio\":null,\"standfirstShort\":\"Is AI our salvation, our undoing, or just more of the same?\",\"standfirstLong\":\"Generative AI has lately set off public euphoria: the machines have learned to think! But just how intelligent is AI?\",\"duration\":0,\"creditsShort\":null,\"commentsEnabled\":true,\"settings\":{\"badgeColor\":null,\"cardPlayIconColor\":null,\"alignX\":\"right\",\"alignY\":\"center\",\"backdropStrength\":3},\"tags\":[],\"authors\":[{\"id\":\"207630\",\"name\":\"Deepak P\"}],\"image\":{\"url\":\"https://images.aeonmedia.co/images/4512cf70-20be-4286-bbe3-7338389b95d5/essay-final-gettyimages-2152436402.jpg\",\"alt\":\"Close-up of a person’s hand using a smartphone in a dimly lit room with blurred lights in the background. The phone screen shows the text ‘How can I help you today?’ and a text input field.\",\"height\":1252,\"width\":2000},\"primaryTopic\":{\"title\":\"Computing and artificial intelligence\",\"slug\":\"computing-artificial-intelligence\"},\"topics\":[{\"slug\":\"computing-artificial-intelligence\",\"title\":\"Computing and artificial intelligence\"},{\"slug\":\"history-of-science\",\"title\":\"History of science\"},{\"slug\":\"information-and-communication\",\"title\":\"Information and communication\"}],\"section\":{\"title\":\"Science\",\"slug\":\"science\"},\"createdAt\":\"2024-07-22T12:04:27Z\"}],\"preview\":false,\"geolocation\":null},\"__N_SSG\":true},\"page\":\"/essays/[id]\",\"query\":{\"id\":\"why-longtermism-is-the-worlds-most-dangerous-secular-credo\"},\"buildId\":\"9mLhaGsqiGq6dMbedGMwT\",\"isFallback\":false,\"isExperimentalCompile\":false,\"dynamicIds\":[27462,65117],\"gsp\":true,\"scriptLoader\":[]}</script></body></html>","oembed":false,"readabilityObject":{"title":"Why longtermism is the world’s most dangerous secular credo | Aeon Essays","content":"<div id=\"readability-page-1\" class=\"page\"><div><p>There seems to be a growing recognition that humanity might be approaching the ‘end times’. Dire predictions of catastrophe clutter the news. Social media videos of hellish wildfires, devastating floods and hospitals overflowing with <span>COVID-19</span> patients dominate our timelines. Extinction Rebellion activists are shutting down cities in a desperate attempt to save the world. One <a href=\"https://ro.uow.edu.au/cgi/viewcontent.cgi?article=1742&amp;context=buspapers\" target=\"_blank\" rel=\"noreferrer noopener\">survey</a> even found that more than half of the people asked about humanity’s future ‘rated the risk of our way of life ending within the next <span>100 years</span> at <span>50 per</span> cent or greater.’</p>\n<p>‘Apocalypticism’, or the belief that the end times are imminent, is of course nothing new: people have warned that the end is nigh for millennia, and in fact many New Testament scholars believe that Jesus himself expected the world to end during his own lifetime. But the situation today is fundamentally different than in the past. The ‘eschatological’ scenarios now being discussed are based not on the revelations of religious prophets, or secular metanarratives of human history (as in the case of Marxism), but on robust scientific conclusions defended by leading experts in fields such as climatology, ecology, epidemiology and so on.</p>\n<p>We know, for example, that climate change poses a dire threat to civilisation. We know that biodiversity loss and the sixth mass extinction could precipitate sudden, irreversible, catastrophic shifts in the global ecosystem. A thermonuclear exchange could blot out the Sun for years or decades, bringing about the collapse of global agriculture. And whether or not SARS-<span>CoV-2</span> came from a Wuhan laboratory or was cooked up in the kitchen of nature (the latter seems more <a href=\"https://www.medicalnewstoday.com/articles/covid-19-us-intelligence-rules-out-biological-weapon-origin\" target=\"_blank\" rel=\"noreferrer noopener\">probable</a> right now), synthetic biology will soon <a href=\"https://docs.wixstatic.com/ugd/d9aaad_4d3e08f426904b8c8be516230722087a.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">enable</a> bad actors to design pathogens far more lethal and contagious than anything Darwinian evolution could possibly invent. Some philosophers and scientists have also begun sounding the <a href=\"https://docs.wixstatic.com/ugd/d9aaad_b2e7f0f56bec40a195e551dd3e8c878e.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">alarm</a> about ‘emerging threats’ associated with machine superintelligence, molecular nanotechnology and stratospheric geoengineering, which look no less formidable.</p>\n<p>Such considerations have led many scholars to acknowledge that, as Stephen Hawking wrote in <em>The Guardian</em> in 2016, ‘we are at the most dangerous moment in the development of humanity.’ Lord Martin Rees, for example, estimates that civilisation has a 50/50 chance of making it to 2100. Noam Chomsky <a href=\"https://global.ilmanifesto.it/chomsky-republicans-are-a-danger-to-the-human-species/\" target=\"_blank\" rel=\"noreferrer noopener\">argues</a> that the risk of annihilation is currently ‘unprecedented in the history of <em>Homo sapiens</em>’. And Max Tegmark contends that ‘it’s probably going to be within our lifetimes … that we’re either going to self-destruct or get our act together.’ Consistent with these dismal declarations, the <em>Bulletin of the Atomic Scientists</em> in 2020 <a href=\"https://thebulletin.org/doomsday-clock/2020-doomsday-clock-statement/\" target=\"_blank\" rel=\"noreferrer noopener\">set</a> its iconic Doomsday Clock to a mere 100 seconds before midnight (or doom), the closest it’s been since the clock was created in 1947, and more than 11,000 scientists from around the world signed an <a href=\"https://hal.archives-ouvertes.fr/hal-02397151/document\" target=\"_blank\" rel=\"noreferrer noopener\">article</a> in 2020 stating ‘clearly and unequivocally that planet Earth is facing a climate emergency’, and without ‘an immense increase of scale in endeavours to conserve our biosphere [we risk] untold suffering due to the climate crisis.’ As the young climate activist Xiye Bastida summed up this existential mood in a <em>Teen Vogue</em> interview in 2019, the aim is to ‘make sure that we’re not the last generation’, because this now appears to be a very real possibility.</p>\n<p>Given the unprecedented dangers facing humanity today, one might expect philosophers to have spilled a considerable amount of ink on the ethical implications of our extinction, or related scenarios such as the permanent collapse of civilisation. How morally bad (or <a href=\"https://aeon.co/essays/having-children-is-not-life-affirming-its-immoral\" target=\"_blank\" rel=\"noopener\">good</a>) would our disappearance be, and for what reasons? Would it be wrong to prevent future generations from coming into existence? Does the value of past sacrifices, struggles and strivings depend on humanity continuing to exist for as long as Earth, or the Universe more generally, remains habitable?</p>\n<p>Yet this is not the case: the topic of our extinction has received little sustained attention from philosophers until recently, and even now remains at the fringe of philosophical discussion and debate. On the whole, they have been preoccupied with other matters. However, there is one notable exception to this rule: over the past two decades, a small group of theorists mostly based in Oxford have been busy working out the details of a new moral worldview called longtermism, which <a href=\"https://forum.effectivealtruism.org/posts/qZyshHCNkjs3TvSem/longtermism\" target=\"_blank\" rel=\"noreferrer noopener\">emphasizes</a> how our actions affect the very long-term future of the universe – thousands, millions, billions, and even trillions of years from now. This has roots in the work of <a href=\"https://aeon.co/essays/will-humans-be-around-in-a-billion-years-or-a-trillion\" target=\"_blank\" rel=\"noopener\">Nick Bostrom</a>, who founded the grandiosely named Future of Humanity Institute (FHI) in 2005, and Nick Beckstead, a research associate at FHI and a programme officer at Open Philanthropy. It has been defended most publicly by the FHI philosopher Toby Ord, author of <em>The Precipice: Existential Risk and the Future of Humanity</em> (2020). Longtermism is the primary research focus of both the Global Priorities Institute (GPI), an FHI-linked organisation directed by Hilary Greaves, and the Forethought Foundation, run by William MacAskill, who also holds positions at FHI and GPI. Adding to the tangle of titles, names, institutes and acronyms, longtermism is one of the main ‘cause areas’ of the so-called effective altruism (EA) movement, which was introduced by Ord in around 2011 and now <a href=\"https://80000hours.org/2021/07/effective-altruism-growing/\" target=\"_blank\" rel=\"noreferrer noopener\">boasts</a> of having a mind-boggling <span>$46 billion</span> in committed funding.</p>\n<p>It is difficult to overstate how influential longtermism has become. Karl Marx in 1845 declared that the point of philosophy isn’t merely to interpret the world but change it, and this is exactly what longtermists have been doing, with extraordinary success. Consider that <a href=\"https://aeon.co/essays/elon-musk-puts-his-case-for-a-multi-planet-civilisation\" target=\"_blank\" rel=\"noopener\">Elon Musk</a>, who has cited and endorsed Bostrom’s work, has donated $1.5 million dollars to FHI through its sister organisation, the even more grandiosely named Future of Life Institute (FLI). This was cofounded by the multimillionaire tech entrepreneur Jaan Tallinn, who, as I recently <a href=\"https://www.currentaffairs.org/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk?fbclid=IwAR1zhM1QqoEqF1bhNicGcZ5la7wGp3Q4hU0t9ytfbM2gBGrhOjGhFOl-NC8\" target=\"_blank\" rel=\"noreferrer noopener\">noted</a>, doesn’t believe that climate change poses an ‘existential risk’ to humanity because of his adherence to the longtermist ideology.</p>\n<p>Meanwhile, the billionaire libertarian and Donald Trump supporter Peter Thiel, who once gave the keynote address at an EA conference, has donated large sums of money to the Machine Intelligence Research Institute, whose mission to save humanity from superintelligent machines is deeply intertwined with longtermist values. Other organisations such as GPI and the Forethought Foundation are funding essay contests and scholarships in an effort to draw young people into the community, while it’s an open secret that the Washington, DC-based Center for Security and Emerging Technologies (CSET) aims to place longtermists within high-level US government positions to shape national policy. In fact, CSET was established by Jason Matheny, a former research assistant at FHI who’s now the deputy assistant to US President Joe Biden for technology and national security. Ord himself has, astonishingly for a philosopher, ‘advised the World Health Organization, the World Bank, the World Economic Forum, the US National Intelligence Council, the UK Prime Minister’s Office, Cabinet Office, and Government Office for Science’, and he recently <a href=\"https://forum.effectivealtruism.org/posts/Fwu2SLKeM5h5v95ww/major-un-report-discusses-existential-risk-and-future%23Context\" target=\"_blank\" rel=\"noreferrer noopener\">contributed</a> to a report from the Secretary-General of the United Nations that specifically mentions ‘long-termism’.</p>\n<p>The point is that longtermism might be one of the most influential ideologies that few people outside of elite universities and Silicon Valley have ever heard about. I believe this needs to change because, as a former longtermist who published an entire <a href=\"https://www.amazon.com/Morality-Foresight-Human-Flourishing-Introduction/dp/1634311426/ref=sr_1_1?dchild=1&amp;keywords=morality+foresight+torres&amp;qid=1625225082&amp;sr=8-1\" target=\"_blank\" rel=\"noreferrer noopener\">book</a> four years ago in defence of the general idea, I have come to see this worldview as quite possibly the most dangerous secular belief system in the world today. But to understand the nature of the beast, we need to first dissect it, examining its anatomical features and physiological functions.</p>\n<p><span>T</span>he initial thing to notice is that longtermism, as proposed by Bostrom and Beckstead, is not equivalent to ‘caring about the long term’ or ‘valuing the wellbeing of future generations’. It goes way beyond this. At its core is a simple – albeit flawed, in my opinion – analogy between individual persons and humanity as a whole. To illustrate the idea, consider the case of <a href=\"https://aeon.co/essays/what-is-truth-on-ramsey-wittgenstein-and-the-vienna-circle\" target=\"_blank\" rel=\"noopener\">Frank Ramsey</a>, a scholar at the University of Cambridge widely considered by his peers as among his generation’s most exceptional minds. ‘There was something of Newton about him,’ the belletrist Lytton Strachey once said. <span>G E Moore</span> wrote of Ramsey’s ‘very exceptional brilliance’. And John Maynard Keynes described a paper of Ramsey’s as ‘one of the most remarkable contributions to mathematical economics ever made’.</p>\n<p>But Ramsey’s story isn’t a happy one. On <span>19 January</span> 1930, he died in a London hospital following a surgical procedure, the likely cause of death being a liver infection from swimming in the River Cam, which winds its way through Cambridge. Ramsey was only <span>26 years</span> old.</p>\n<p>One could argue that there are two distinct reasons this outcome was tragic. The first is the most obvious: it cut short Ramsey’s life, depriving him of everything he could have experienced had he survived – the joys and happiness, the love and friendship: all that makes life worth living. In this sense, Ramsey’s early demise was a personal tragedy. But, secondly, his death also robbed the world of an intellectual superstar apparently destined to make even more extraordinary contributions to human knowledge. ‘The number of trails Ramsey laid was remarkable,’ writes Sir Partha Dasgupta. But how many more trails might he have blazed? ‘The loss to your generation is agonising to think of,’ Strachey lamented, ‘what a light has gone out’ – which leaves one wondering how Western intellectual history might have been different if Ramsey hadn’t died so young. From this perspective, one could argue that, although the personal tragedy of Ramsey’s death was truly terrible, the immensity of his potential to have changed the world for the better makes the second tragedy even worse. In other words, the badness of his death stems mostly, perhaps overwhelmingly, from his unfulfilled potential rather than the direct, personal harms that he experienced. Or so the argument goes.</p>\n<p>Longtermists would map these claims and conclusions on to humanity itself, as if humanity is an individual with its very own ‘potential’ to squander or fulfil, ruin or realise, over the course of ‘its lifetime’. So, on the one hand, a catastrophe that reduces the human population to zero would be tragic because of all the suffering it would inflict upon those alive at the time. Imagine the horror of starving to death in subfreezing temperatures, under pitch-black skies at noon, for years or decades after a thermonuclear war. This is the first tragedy, a personal tragedy for those directly affected. But there is, longtermists would argue, a second tragedy that is astronomically worse than the first, arising from the fact that our extinction would permanently foreclose what could be an extremely long and prosperous future over the next, say, ~10<sup>100</sup> years (at which point the ‘heat death’ will <a href=\"https://aeon.co/essays/welcome-to-earth-2200-ad-pop-500-million-temp-180-f\" target=\"_blank\" rel=\"noopener\">make</a> life impossible). In doing this, it would irreversibly destroy the ‘vast and glorious’ longterm potential of humanity, in Ord’s almost religious language – a ‘potential’ so huge, given the size of the Universe and the time left before reaching thermodynamic equilibrium, that the first tragedy would utterly pale in comparison.</p>\n<p>This immediately suggests another parallel between individuals and humanity: death isn’t the only way that someone’s potential could be left unfulfilled. Imagine that Ramsey hadn’t died young but, instead of studying, writing and publishing scholarly papers, he’d spent his days in the local bar playing pool and drinking. Same outcome, different failure mode. Applying this to humanity, longtermists would argue that there are failure modes that could leave our potential unfulfilled without us dying out, which I will return to below.</p>\n<p>On this view, a climate catastrophe will be a small blip – like a 90-year-old who stubbed his toe when he was two</p>\n<p>To summarise these ideas so far, humanity has a ‘potential’ of its own, one that transcends the potentials of each individual person, and failing to realise this potential would be extremely bad – indeed, as we will see, a moral catastrophe of literally cosmic proportions. This is the central dogma of longtermism: nothing matters more, ethically speaking, than fulfilling our potential as a species of ‘Earth-originating intelligent life’. It matters so much that longtermists have even <a href=\"https://nickbostrom.com/existential/risks.html\" target=\"_blank\" rel=\"noreferrer noopener\">coined</a> the scary-sounding term ‘existential risk’ for any possibility of our potential being destroyed, and ‘existential catastrophe’ for any event that actually destroys this potential.</p>\n<p>Why do I think this ideology is so dangerous? The short answer is that elevating the fulfilment of humanity’s supposed potential above all else could nontrivially increase the probability that actual people – those alive today and in the near future – suffer extreme harms, even death. Consider that, as I noted elsewhere, the longtermist ideology inclines its adherents to take an insouciant attitude towards climate change. Why? Because even if climate change causes island nations to disappear, triggers mass migrations and kills millions of people, it probably isn’t going to compromise our longterm potential over the coming trillions of years. If one takes a cosmic view of the situation, even a climate catastrophe that cuts the human population by <span>75 per</span> cent for the next two millennia will, in the grand scheme of things, be nothing more than a small blip – the equivalent of a 90-year-old man having stubbed his toe when he was two.</p>\n<p>Bostrom’s <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.304.7392&amp;rep=rep1&amp;type=pdf\" target=\"_blank\" rel=\"noreferrer noopener\">argument</a> is that ‘a non-existential disaster causing the breakdown of global civilisation is, from the perspective of humanity as a whole, a potentially recoverable setback.’ It might be ‘a giant massacre for man’, he adds, but so long as humanity bounces back to fulfil its potential, it will ultimately register as little more than ‘a small misstep for mankind’. Elsewhere, he writes that the worst natural disasters and devastating atrocities in history become almost imperceptible trivialities when seen from this grand perspective. Referring to the two world wars, AIDS and the Chernobyl nuclear accident, he declares that ‘tragic as such events are to the people immediately affected, in the big picture of things … even the worst of these catastrophes are mere ripples on the surface of the great sea of life.’</p>\n<p>This way of seeing the world, of assessing the badness of AIDS and the Holocaust, implies that future disasters of the same (non-existential) scope and intensity should also be categorised as ‘mere ripples’. If they don’t pose a direct existential risk, then we ought not to worry much about them, however tragic they might be to individuals. As Bostrom <a href=\"https://www.nickbostrom.com/astronomical/waste.html\" target=\"_blank\" rel=\"noreferrer noopener\">wrote</a> in 2003, ‘priority number one, two, three and four should … be to reduce existential risk.’ He <a href=\"https://www.existential-risk.org/concept.html\" target=\"_blank\" rel=\"noreferrer noopener\">reiterated</a> this several years later in arguing that we mustn’t ‘fritter … away’ our finite resources on ‘feel-good projects of suboptimal efficacy’ such as alleviating global poverty and reducing animal suffering, since neither threatens our longterm potential, and our longterm potential is what really matters.</p>\n<p>Ord echoes these views in arguing that, of all the problems facing humanity, our ‘first great task … is to reach a place of safety – a place where existential risk’ – as he defines it – ‘is low and stays low’, which he dubs ‘existential security’. More than anything else, what matters is doing everything necessary to ‘preserve’ and ‘protect’ our potential by ‘extracting ourselves from immediate danger’ and devising robust ‘safeguards that will defend humanity from dangers over the longterm future, so that it becomes impossible to fail.’ Although Ord gives a nod to climate change, he also claims – based on a dubious methodology – that the chance of climate change causing an existential catastrophe is only ∼1 in 1,000, which is a whole two orders of magnitude lower than the probability of superintelligent machines destroying humanity this century, according to Ord.</p>\n<p>What’s really notable here is that the central concern isn’t the effect of the climate catastrophe on actual people around the world (remember, in the grand scheme, this would be, in Bostrom’s words, a ‘small misstep for mankind’) but the slim possibility that, as Ord puts it in <em>The Precipice</em>, this catastrophe ‘poses a risk of an unrecoverable collapse of civilisation or even the complete extinction of humanity’. Again, the harms caused to actual people (especially those in the Global South) might be significant in absolute terms, but when compared to the ‘vastness’ and ‘glory’ of our longterm potential in the cosmos, they hardly even register.</p>\n<p><span>Y</span>et the implications of longtermism are far more worrisome. If our top four priorities are to avoid an existential catastrophe – ie, to fulfil ‘our potential’ – then what’s not on the table for making this happen? Consider <a href=\"https://aeon.co/essays/what-s-it-all-for-is-a-question-that-belongs-in-the-past\" target=\"_blank\" rel=\"noopener\">Thomas Nagel</a>’s comment about how the notion of what we might call the ‘greater good’ has been used to ‘justify’ certain atrocities (eg, during war). If the ends ‘justify’ the means, he argues, and the ends are thought to be sufficiently large (eg, national security), then this ‘can be brought to bear to ease the consciences of those responsible for a certain number of charred babies’. Now imagine what might be ‘justified’ if the ‘greater good’ isn’t national security but the cosmic potential of Earth-originating intelligent life over the coming trillions of years? During the Second World War, <span>40 million</span> civilians perished, but compare this number to the 10<sup>54</sup> or more people (in Bostrom’s <a href=\"https://www.existential-risk.org/faq.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">estimate</a>) who could come to exist if we can avoid an existential catastrophe. What shouldn’t we do to ‘protect’ and ‘preserve’ this potential? To ensure that these unborn people come to exist? What means can’t be ‘justified’ by this cosmically significant moral end?</p>\n<p>Bostrom himself <a href=\"https://www.nickbostrom.com/papers/vulnerable.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">argued</a> that we should seriously consider establishing a global, invasive surveillance system that monitors every person on the planet in realtime, to amplify the ‘capacities for preventive policing’ (eg, to prevent omnicidal terrorist attacks that could devastate civilisation). Elsewhere, he’s written that states should use preemptive violence/war to avoid existential catastrophes, and argued that saving billions of actual people is the moral equivalent of reducing existential risk by utterly minuscule amounts. In his words, even if there is ‘a mere <span>1 per </span>cent chance’ of 10<sup>54</sup> people existing in the future, then ‘the expected value of reducing existential risk by a mere <em>one billionth of one billionth of one percentage point</em> is worth <span>100 billion</span> times as much as a billion human lives.’ Such fanaticism – a word that some longtermists <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/Hayden-Wilkinson_In-defence-of-fanaticism.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">embrace</a> – has led a growing number of critics to worry about what might happen if political leaders in the real world were to take Bostrom’s view seriously. To <a href=\"https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780198723547.001.0001/acprof-9780198723547\" target=\"_blank\" rel=\"noreferrer noopener\">quote</a> the mathematical statistician Olle Häggström, who – perplexingly – tends otherwise to speak favourably of longtermism:</p>\n<blockquote>I feel extremely uneasy about the prospect that [the calculations above] might become recognised among politicians and decision-makers as a guide to policy worth taking literally. It is simply too reminiscent of the old saying ‘If you want to make an omelette, you must be willing to break a few eggs,’ which has typically been used to explain that a bit of genocide or so might be a good thing, if it can contribute to the goal of creating a future utopia. Imagine a situation where the head of the CIA explains to the US president that they have credible evidence that somewhere in Germany, there is a lunatic who is working on a doomsday weapon and intends to use it to wipe out humanity, and that this lunatic has a one-in-a-million chance of succeeding. They have no further information on the identity or whereabouts of this lunatic. If the president has taken Bostrom’s argument to heart, and if he knows how to do the arithmetic, he may conclude that it is worthwhile conducting a full-scale nuclear assault on Germany to kill every single person within its borders.</blockquote>\n<p>Here, then, are a few reasons I find longtermism to be profoundly dangerous. Yet there are additional, fundamental problems with this worldview that no one, to my knowledge, has previously noted in writing. For example, there’s a good case to make that the underlying commitments of longtermism are a major reason why humanity faces so many unprecedented risks to its survival in the first place. Longtermism might, in other words, be incompatible with the attainment of ‘existential security’, meaning that the only way to genuinely reduce the probability of extinction or collapse in the future might be to abandon the longtermist ideology entirely.</p>\n<p>To Bostrom and Ord, failing to become posthuman would prevent us from realising our vast, glorious potential</p>\n<p>To understand the argument, let’s first unpack what longtermists mean by our ‘longterm potential’, an expression that I have so far used without defining. We can analyse this concept into three main components: transhumanism, space expansionism, and a moral view closely associated with what philosophers call ‘total utilitarianism’.</p>\n<p>The first refers to the <a href=\"https://aeon.co/essays/why-is-the-language-of-transhumanists-and-religion-so-similar\" target=\"_blank\" rel=\"noopener\">idea</a> that we should use advanced technologies to reengineer our bodies and brains to create a ‘superior’ race of radically enhanced posthumans (which, confusingly, longtermists place within the category of ‘humanity’). Although Bostrom is perhaps the most prominent transhumanist today, longtermists have shied away from using the term ‘transhumanism’, probably because of its negative associations. Susan Levin, for example, <a href=\"https://blog.oup.com/2021/01/playing-to-lose-transhumanism-autonomy-and-liberal-democracy-long-read/\" target=\"_blank\" rel=\"noreferrer noopener\">points out</a> that contemporary transhumanism has its roots in the Anglo-American eugenics movement, and transhumanists such as <a href=\"https://aeon.co/users/julian-savulescu\" target=\"_blank\" rel=\"noopener\">Julian Savulescu</a>, who co-edited the <a href=\"https://global.oup.com/academic/product/human-enhancement-9780199299720?cc=gb&amp;lang=en&amp;\" target=\"_blank\" rel=\"noreferrer noopener\">book</a> <em>Human Enhancement</em> (2009) with Bostrom, have literally argued for the consumption of ‘morality-boosting’ chemicals such as oxytocin to avoid an existential catastrophe (which he <a href=\"https://jme.bmj.com/content/37/7/441\" target=\"_blank\" rel=\"noreferrer noopener\">calls</a> ‘ultimate harm’). As Savulescu <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-8519.2011.01907.x\" target=\"_blank\" rel=\"noreferrer noopener\">writes</a> with a colleague, ‘it is a matter of such urgency to improve humanity morally … that we should seek <em>whatever</em> means there are to effect this.’ Such claims are not only controversial but for many quite disturbing, and hence longtermists have attempted to distance themselves from such ideas, while nonetheless championing the ideology.</p>\n<p>Transhumanism <a href=\"https://www.nickbostrom.com/posthuman.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">claims</a> that there are various ‘posthuman modes of being’ that are far better than our current human mode. We could, for instance, genetically alter ourselves to gain perfect control over our emotions, or access the internet via neural implants, or maybe even upload our minds to computer hardware to achieve ‘digital immortality’. As Ord urges in <em>The Precipice</em>, think of how awesome it would be to perceive the world via echolocation, like bats and dolphins, or magnetoreception, like red foxes and homing pigeons. ‘Such uncharted experiences,’ Ord writes, ‘exist in minds much less sophisticated than our own. What experiences, possibly of immense value, could be accessible, then, to minds much greater?’ Bostrom’s most fantastical exploration of these possibilities comes from his <a href=\"https://www.nickbostrom.com/utopia.html\" target=\"_blank\" rel=\"noreferrer noopener\">evocative</a> ‘Letter from Utopia’ (2008), which depicts a techno-Utopian world full of superintelligent posthumans awash in so much ‘pleasure’ that, as the letter’s fictional posthuman writes, ‘we sprinkle it in our tea.’</p>\n<p>The connection with longtermism is that, according to Bostrom and Ord, failing to become posthuman would seemingly prevent us from realising our vast and glorious potential, which would be existentially catastrophic. As Bostrom put it in 2012, ‘the permanent foreclosure of any possibility of this kind of transformative change of human biological nature may itself constitute an existential catastrophe.’ Similarly, Ord asserts that ‘forever preserving humanity as it is now may also squander our legacy, relinquishing the greater part of our potential.’</p>\n<p><span>T</span>he second component of our potential – space expansionism – refers to the idea that we must colonise as much of our future light cone as possible: that is, the region of spacetime that is theoretically accessible to us. According to longtermists, our future light cone contains a huge quantity of exploitable resources, which they refer to as our ‘cosmic endowment’ of negentropy (or reverse entropy). The Milky Way alone, Ord writes, is ‘150,000 light years across, encompassing more than <span>100 billion</span> stars, most with their own planets.’ Attaining humanity’s longterm potential, he continues, ‘requires only that [we] eventually travel to a nearby star and establish enough of a foothold to create a new flourishing society from which we could venture further.’ By spreading ‘just six light years at a time’, our posthuman descendants could make ‘almost all the stars of our galaxy … reachable’ since ‘each star system, including our own, would need to settle just the few nearest stars [for] the entire galaxy [to] eventually fill with life.’ The process could be exponential, resulting in ever-more ‘flourishing’ societies with each additional second our descendants hop from star to star.</p>\n<p>But why exactly would we want to do this? What’s so important about flooding the Universe with new posthuman civilisations? This leads to the third component: total utilitarianism, which I will refer to as ‘utilitarianism’ for short. Although some longtermists insist that they aren’t utilitarians, we should right away note that this is mostly a smoke-and-mirrors act to deflect criticisms that longtermism – and, more generally, the effective altruism (EA) movement from which it emerged – is nothing more than utilitarianism <a href=\"https://blog.apaonline.org/2021/03/29/is-effective-altruism-inherently-utilitarian/\" target=\"_blank\" rel=\"noreferrer noopener\">repackaged</a>. The fact is that the EA movement is deeply utilitarian, at least in practice, and indeed, before it decided upon a name, the movement’s early members, including Ord, seriously considered calling it the ‘effective utilitarian community’.</p>\n<p>This being said, utilitarianism is an ethical theory that specifies our sole moral obligation as being to maximise the total amount of ‘intrinsic value’ in the world, as tallied up from a disembodied, impartial, cosmic vantage point called ‘the point of view of the Universe’. From this view, it doesn’t matter how value – which utilitarian hedonists equate with pleasure – is distributed among people across space and time. All that matters is the total net sum. For example, imagine that there are <span>1 trillion</span> people who have lives of value ‘1’, meaning that they are just barely worth living. This gives a total value of <span>1 trillion</span>. Now consider an alternative universe in which <span>1 billion</span> people have lives with a value of ‘999’, meaning that their lives are extremely good. This gives a total value of <span>999 billion.</span> Since <span>999 billion</span> is less than <span>1 trillion,</span> the first world full of lives hardly worth living would be morally better than the second world, and hence, if a utilitarian were forced to choose between these, she would pick the former. (This is called the ‘repugnant conclusion’, which longtermists such as Ord, MacAskill and Greaves recently <a href=\"https://www.cambridge.org/core/journals/utilitas/article/what-should-we-agree-on-about-the-repugnant-conclusion/EB52C686BAFEF490CE37043A0A3DD075\" target=\"_blank\" rel=\"noreferrer noopener\">argued</a> shouldn’t be taken very seriously. For them, the first world really might be better!)</p>\n<p>Beckstead argued that we should prioritise the lives of people in rich countries over those in poor countries</p>\n<p>The underlying reasoning here is based on the idea that people – you and I – are nothing more than means to an end. We don’t matter in ourselves; we have no inherent value of our own. Instead, people are understood as the ‘containers’ of value, and hence we matter only insofar as we ‘contain’ value, and therefore contribute to the overall net amount of value in the Universe between the Big Bang and the heat death. Since utilitarianism tells us to maximise value, it follows that the more people (value containers) who exist with net-positive amounts of value (pleasure), the better the Universe will become, morally speaking. In a phrase: people exist for the sake of maximising value, rather than value existing for the sake of benefitting people.</p>\n<p>This is why longtermists are obsessed with calculating how many people could exist in the future if we were to colonise space and create vast computer simulations around stars in which unfathomably huge numbers of people live net-positive lives in virtual-reality environments. I already mentioned Bostrom’s estimate of 10<sup>54</sup> future people, which includes many of these ‘digital people’, but in his <a href=\"https://global.oup.com/academic/product/superintelligence-9780199678112?cc=gb&amp;lang=en&amp;\" target=\"_blank\" rel=\"noreferrer noopener\">bestseller</a> <em>Superintelligence</em> (2014) he puts the number even higher at 10<sup>58</sup> people, nearly all of whom would ‘live rich and happy lives while interacting with one another in virtual environments’. Greaves and MacAskill are similarly excited about this possibility, <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">estimating</a> that some 10<sup>45</sup> conscious beings in computer simulations could exist within the Milky Way alone.</p>\n<p>That is what our ‘vast and glorious’ potential consists of: massive numbers of technologically enhanced digital posthumans inside huge computer simulations spread throughout our future light cone. It is for this goal that, in Häggström’s scenario, a longtermist politician would annihilate Germany. It is for this goal that we must not ‘fritter … away’ our resources on such things as solving global poverty. It is for this goal that we should consider implementing a global surveillance system, keep pre-emptive war on the table, and focus more on superintelligent machines than saving people in the Global South from the devastating effects of climate change (mostly caused by the Global North). In fact, Beckstead has even argued that, for the sake of attaining this goal, we should actually prioritise the lives of people in rich countries over those in poor countries, since influencing the long-term future is of ‘overwhelming importance’, and the former are more likely to influence the long-term future than the latter. To <a href=\"https://www.proquest.com/docview/1442191960?fromopenview=tr%2522a%2520long%2520period%25E2%2580%2594perhaps%2520tens%2520of%2520thousands%2520of%2520years%25E2%2580%2594during%2520which%2520human%2520civilisation,%2520perhaps%2520with%2520the%2520aid%2520of%2520improved%2520cognitive%2520ability,%2520dedicates%2520itself%2520to%2520working%2520out%2520what%2520is%2520ultimately%2520of%2520value%2522ue&amp;pq-origsite=gscholar\" target=\"_blank\" rel=\"noreferrer noopener\">quote</a> a passage from Beckstead’s 2013 PhD dissertation, which Ord enthusiastically praises as one of the most important contributions to the longtermist literature:</p>\n<blockquote>Saving lives in poor countries may have significantly smaller ripple effects than saving and improving lives in rich countries. Why? Richer countries have substantially more innovation, and their workers are much more economically productive. [Consequently,] it now seems more plausible to me that saving a life in a rich country is substantially more important than saving a life in a poor country, other things being equal.</blockquote>\n<p><span>T</span>his is just the tip of the iceberg. Consider the implications of this conception of ‘our potential’ for the development of technology and creation of new risks. Since realising our potential is the ultimate moral goal for humanity, and since our descendants cannot become posthuman, colonise space and create ~10<sup>58</sup> people in computer simulations without technologies far more advanced than those around today, failing to develop more technology would itself constitute an existential catastrophe – a failure mode (comparable to Ramsey neglecting his talents by spending his days playing pool and drinking) that Bostrom calls ‘plateauing’. Indeed, Bostrom places this idea front-and-centre in his canonical definition of ‘existential risk’, which denotes any future event that would prevent humanity from reaching and/or sustaining a state of ‘technological maturity’, meaning ‘the attainment of capabilities affording a level of economic productivity and control over nature close to the maximum that could feasibly be achieved.’ Technological maturity is the linchpin here because controlling nature and increasing economic productivity to the absolute physical limits are ostensibly necessary for creating the maximum quantity of ‘value’ within our future light cone.</p>\n<p>But reflect for a moment on how humanity got itself into the current climatic and ecological crisis. Behind the extraction and burning of fossil fuels, decimation of ecosystems and extermination of species has been the notion that nature is something to be controlled, subjugated, exploited, vanquished, plundered, transformed, reconfigured and manipulated. As the technology theorist Langdon Winner <a href=\"https://mitpress.mit.edu/books/autonomous-technology\" target=\"_blank\" rel=\"noreferrer noopener\">writes</a> in <em>Autonomous Technology</em> (1977), since the time of Francis Bacon our view of technology has been ‘inextricably bound to a single conception of the manner in which power is used – the style of absolute mastery, the despotic, one-way control of the master over the slave.’ He adds:</p>\n<blockquote>There are seldom any reservations about man’s rightful role in conquering, vanquishing, and subjugating everything natural. This is his power and his glory. What would in other situations seem [to be] rather tawdry and despicable intentions are here the most honourable of virtues. Nature is the universal prey, to manipulate as humans see fit.</blockquote>\n<p>This is precisely what we find in Bostrom’s account of existential risks and its associated normative futurology: nature, the entire Universe, our ‘cosmic endowment’ is there for the plundering, to be manipulated, transformed and converted into ‘value-structures, such as sentient beings living worthwhile lives’ in vast computer simulations, quoting Bostrom’s essay ‘Astronomical Waste’ (2003). Yet this Baconian, capitalist view is one of the most fundamental root causes of the unprecedented environmental crisis that now threatens to destroy large regions of the biosphere, Indigenous communities around the world, and perhaps even Western technological civilisation itself. While other longtermists have not been as explicit as Bostrom, there is a clear tendency to see the natural world the way utilitarianism sees people: as means to some abstract, impersonal end, and nothing more. MacAskill and a colleague, for example, <a href=\"https://philarchive.org/archive/PUMEAv3\" target=\"_blank\" rel=\"noreferrer noopener\">write</a> that the EA movement, and by implication longtermism, is ‘tentatively <em>welfarist</em> in that its tentative aim in doing good concerns promoting wellbeing only and not, say, protecting biodiversity or conserving natural beauty for their own sakes.’</p>\n<p>On this account, every problem arises from too little rather than too much technology</p>\n<p>Just as worrisome is the longtermist demand that we must create ever-more powerful technologies, despite the agreed-upon fact that the overwhelming source of risk to human existence these days comes from these very technologies. In Ord’s words, ‘without serious efforts to protect humanity, there is strong reason to believe the risk will be higher this century, and increasing with each century that technological progress continues.’ Similarly, in 2012 Bostrom acknowledges that</p>\n<blockquote>the great bulk of existential risk in the foreseeable future consists of anthropogenic existential risks – that is, arising from human activity. In particular, most of the biggest existential risks seem to be linked to potential future technological breakthroughs that may radically expand our ability to manipulate the external world or our own biology. As our powers expand, so will the scale of their potential consequences – intended and unintended, positive and negative.</blockquote>\n<p>On this view, there is only one way forward – more technological development – even if this is the most dangerous path into the future. But how much sense does this make? Surely if we want to maximise our chances of survival, we should oppose the development of dangerous new dual-use technologies. If more technology equals greater risk – as history clearly shows and technological projections affirm – then perhaps the only way to actually attain a state of ‘existential security’ is to slow down or completely halt further technological innovation.</p>\n<p>But longtermists have an answer to this conundrum: the so-called ‘value-neutrality thesis’. This states that technology is a morally neutral object, ie, ‘just a tool’. The idea is most famously encapsulated in the NRA’s slogan ‘Guns don’t kill people, people kill people,’ which conveys the message that the consequences of technology, whether good or bad, beneficial or harmful, are entirely determined by the users, not the artefacts. As Bostrom put it in 2002, ‘we should not <em>blame</em> civilisation or technology for imposing big existential risks,’ adding that ‘because of the way we have defined existential risks, a failure to develop technological civilisation would imply that we had fallen victims of an existential disaster.’</p>\n<p>Ord similarly argues that ‘the problem is not so much an excess of technology as a lack of wisdom,’ before going on to quote Carl Sagan’s book <em>Pale Blue Dot</em> (1994): ‘Many of the dangers we face indeed arise from science and technology but, more fundamentally, because we have become powerful without becoming commensurately wise.’ In other words, it is our fault for not being smarter, wiser and more ethical, a cluster of deficiencies that many longtermists believe, in a bit of twisted logic, could be rectified by technologically reengineering our cognitive systems and moral dispositions. Everything, on this account, is an engineering problem, and hence every problem arises from too little rather than too much technology.</p>\n<p>We can now begin to see how longtermism might be self-defeating. Not only could its ‘fanatical’ emphasis on fulfilling our longterm potential lead people to, eg, neglect non-existential climate change, prioritise the rich over the poor and perhaps even ‘justify’ pre-emptive violence and atrocities for the ‘greater cosmic good’ but it also contains within it the very tendencies – Baconianism, capitalism and value-neutrality – that have driven humanity inches away from the precipice of destruction. Longtermism tells us to maximise economic productivity, our control over nature, our presence in the Universe, the number of (simulated) people who exist in the future, the total amount of impersonal ‘value’ and so on. But to maximise, we must develop increasingly powerful – and dangerous – technologies; failing to do this would itself be an existential catastrophe. Not to worry, though, because technology is not responsible for our worsening predicament, and hence the fact that most risks stem directly from technology is no reason to stop creating more technology. Rather, the problem lies with us, which means only that we must create even more technology to transform ourselves into cognitively and morally enhanced posthumans.</p>\n<p>This looks like a recipe for disaster. Creating a new race of ‘wise and responsible’ posthumans is implausible and, if advanced technologies continue to be developed at the current rate, a global-scale catastrophe is almost certainly a matter of when rather than if. Yes, we will need advanced technologies if we wish to escape Earth before it’s sterilised by the Sun in a billion years or so. But the crucial fact that longtermists miss is that <em>technology is far more likely to cause our extinction before this distant future event than to save us from it</em>. If you, like me, value the continued survival and flourishing of humanity, you should care about the long term but reject the ideology of longtermism, which is not only dangerous and flawed but might be contributing to, and reinforcing, the risks that now threaten every person on the planet.</p></div></div>","textContent":"There seems to be a growing recognition that humanity might be approaching the ‘end times’. Dire predictions of catastrophe clutter the news. Social media videos of hellish wildfires, devastating floods and hospitals overflowing with COVID-19 patients dominate our timelines. Extinction Rebellion activists are shutting down cities in a desperate attempt to save the world. One survey even found that more than half of the people asked about humanity’s future ‘rated the risk of our way of life ending within the next 100 years at 50 per cent or greater.’\n‘Apocalypticism’, or the belief that the end times are imminent, is of course nothing new: people have warned that the end is nigh for millennia, and in fact many New Testament scholars believe that Jesus himself expected the world to end during his own lifetime. But the situation today is fundamentally different than in the past. The ‘eschatological’ scenarios now being discussed are based not on the revelations of religious prophets, or secular metanarratives of human history (as in the case of Marxism), but on robust scientific conclusions defended by leading experts in fields such as climatology, ecology, epidemiology and so on.\nWe know, for example, that climate change poses a dire threat to civilisation. We know that biodiversity loss and the sixth mass extinction could precipitate sudden, irreversible, catastrophic shifts in the global ecosystem. A thermonuclear exchange could blot out the Sun for years or decades, bringing about the collapse of global agriculture. And whether or not SARS-CoV-2 came from a Wuhan laboratory or was cooked up in the kitchen of nature (the latter seems more probable right now), synthetic biology will soon enable bad actors to design pathogens far more lethal and contagious than anything Darwinian evolution could possibly invent. Some philosophers and scientists have also begun sounding the alarm about ‘emerging threats’ associated with machine superintelligence, molecular nanotechnology and stratospheric geoengineering, which look no less formidable.\nSuch considerations have led many scholars to acknowledge that, as Stephen Hawking wrote in The Guardian in 2016, ‘we are at the most dangerous moment in the development of humanity.’ Lord Martin Rees, for example, estimates that civilisation has a 50/50 chance of making it to 2100. Noam Chomsky argues that the risk of annihilation is currently ‘unprecedented in the history of Homo sapiens’. And Max Tegmark contends that ‘it’s probably going to be within our lifetimes … that we’re either going to self-destruct or get our act together.’ Consistent with these dismal declarations, the Bulletin of the Atomic Scientists in 2020 set its iconic Doomsday Clock to a mere 100 seconds before midnight (or doom), the closest it’s been since the clock was created in 1947, and more than 11,000 scientists from around the world signed an article in 2020 stating ‘clearly and unequivocally that planet Earth is facing a climate emergency’, and without ‘an immense increase of scale in endeavours to conserve our biosphere [we risk] untold suffering due to the climate crisis.’ As the young climate activist Xiye Bastida summed up this existential mood in a Teen Vogue interview in 2019, the aim is to ‘make sure that we’re not the last generation’, because this now appears to be a very real possibility.\nGiven the unprecedented dangers facing humanity today, one might expect philosophers to have spilled a considerable amount of ink on the ethical implications of our extinction, or related scenarios such as the permanent collapse of civilisation. How morally bad (or good) would our disappearance be, and for what reasons? Would it be wrong to prevent future generations from coming into existence? Does the value of past sacrifices, struggles and strivings depend on humanity continuing to exist for as long as Earth, or the Universe more generally, remains habitable?\nYet this is not the case: the topic of our extinction has received little sustained attention from philosophers until recently, and even now remains at the fringe of philosophical discussion and debate. On the whole, they have been preoccupied with other matters. However, there is one notable exception to this rule: over the past two decades, a small group of theorists mostly based in Oxford have been busy working out the details of a new moral worldview called longtermism, which emphasizes how our actions affect the very long-term future of the universe – thousands, millions, billions, and even trillions of years from now. This has roots in the work of Nick Bostrom, who founded the grandiosely named Future of Humanity Institute (FHI) in 2005, and Nick Beckstead, a research associate at FHI and a programme officer at Open Philanthropy. It has been defended most publicly by the FHI philosopher Toby Ord, author of The Precipice: Existential Risk and the Future of Humanity (2020). Longtermism is the primary research focus of both the Global Priorities Institute (GPI), an FHI-linked organisation directed by Hilary Greaves, and the Forethought Foundation, run by William MacAskill, who also holds positions at FHI and GPI. Adding to the tangle of titles, names, institutes and acronyms, longtermism is one of the main ‘cause areas’ of the so-called effective altruism (EA) movement, which was introduced by Ord in around 2011 and now boasts of having a mind-boggling $46 billion in committed funding.\nIt is difficult to overstate how influential longtermism has become. Karl Marx in 1845 declared that the point of philosophy isn’t merely to interpret the world but change it, and this is exactly what longtermists have been doing, with extraordinary success. Consider that Elon Musk, who has cited and endorsed Bostrom’s work, has donated $1.5 million dollars to FHI through its sister organisation, the even more grandiosely named Future of Life Institute (FLI). This was cofounded by the multimillionaire tech entrepreneur Jaan Tallinn, who, as I recently noted, doesn’t believe that climate change poses an ‘existential risk’ to humanity because of his adherence to the longtermist ideology.\nMeanwhile, the billionaire libertarian and Donald Trump supporter Peter Thiel, who once gave the keynote address at an EA conference, has donated large sums of money to the Machine Intelligence Research Institute, whose mission to save humanity from superintelligent machines is deeply intertwined with longtermist values. Other organisations such as GPI and the Forethought Foundation are funding essay contests and scholarships in an effort to draw young people into the community, while it’s an open secret that the Washington, DC-based Center for Security and Emerging Technologies (CSET) aims to place longtermists within high-level US government positions to shape national policy. In fact, CSET was established by Jason Matheny, a former research assistant at FHI who’s now the deputy assistant to US President Joe Biden for technology and national security. Ord himself has, astonishingly for a philosopher, ‘advised the World Health Organization, the World Bank, the World Economic Forum, the US National Intelligence Council, the UK Prime Minister’s Office, Cabinet Office, and Government Office for Science’, and he recently contributed to a report from the Secretary-General of the United Nations that specifically mentions ‘long-termism’.\nThe point is that longtermism might be one of the most influential ideologies that few people outside of elite universities and Silicon Valley have ever heard about. I believe this needs to change because, as a former longtermist who published an entire book four years ago in defence of the general idea, I have come to see this worldview as quite possibly the most dangerous secular belief system in the world today. But to understand the nature of the beast, we need to first dissect it, examining its anatomical features and physiological functions.\nThe initial thing to notice is that longtermism, as proposed by Bostrom and Beckstead, is not equivalent to ‘caring about the long term’ or ‘valuing the wellbeing of future generations’. It goes way beyond this. At its core is a simple – albeit flawed, in my opinion – analogy between individual persons and humanity as a whole. To illustrate the idea, consider the case of Frank Ramsey, a scholar at the University of Cambridge widely considered by his peers as among his generation’s most exceptional minds. ‘There was something of Newton about him,’ the belletrist Lytton Strachey once said. G E Moore wrote of Ramsey’s ‘very exceptional brilliance’. And John Maynard Keynes described a paper of Ramsey’s as ‘one of the most remarkable contributions to mathematical economics ever made’.\nBut Ramsey’s story isn’t a happy one. On 19 January 1930, he died in a London hospital following a surgical procedure, the likely cause of death being a liver infection from swimming in the River Cam, which winds its way through Cambridge. Ramsey was only 26 years old.\nOne could argue that there are two distinct reasons this outcome was tragic. The first is the most obvious: it cut short Ramsey’s life, depriving him of everything he could have experienced had he survived – the joys and happiness, the love and friendship: all that makes life worth living. In this sense, Ramsey’s early demise was a personal tragedy. But, secondly, his death also robbed the world of an intellectual superstar apparently destined to make even more extraordinary contributions to human knowledge. ‘The number of trails Ramsey laid was remarkable,’ writes Sir Partha Dasgupta. But how many more trails might he have blazed? ‘The loss to your generation is agonising to think of,’ Strachey lamented, ‘what a light has gone out’ – which leaves one wondering how Western intellectual history might have been different if Ramsey hadn’t died so young. From this perspective, one could argue that, although the personal tragedy of Ramsey’s death was truly terrible, the immensity of his potential to have changed the world for the better makes the second tragedy even worse. In other words, the badness of his death stems mostly, perhaps overwhelmingly, from his unfulfilled potential rather than the direct, personal harms that he experienced. Or so the argument goes.\nLongtermists would map these claims and conclusions on to humanity itself, as if humanity is an individual with its very own ‘potential’ to squander or fulfil, ruin or realise, over the course of ‘its lifetime’. So, on the one hand, a catastrophe that reduces the human population to zero would be tragic because of all the suffering it would inflict upon those alive at the time. Imagine the horror of starving to death in subfreezing temperatures, under pitch-black skies at noon, for years or decades after a thermonuclear war. This is the first tragedy, a personal tragedy for those directly affected. But there is, longtermists would argue, a second tragedy that is astronomically worse than the first, arising from the fact that our extinction would permanently foreclose what could be an extremely long and prosperous future over the next, say, ~10100 years (at which point the ‘heat death’ will make life impossible). In doing this, it would irreversibly destroy the ‘vast and glorious’ longterm potential of humanity, in Ord’s almost religious language – a ‘potential’ so huge, given the size of the Universe and the time left before reaching thermodynamic equilibrium, that the first tragedy would utterly pale in comparison.\nThis immediately suggests another parallel between individuals and humanity: death isn’t the only way that someone’s potential could be left unfulfilled. Imagine that Ramsey hadn’t died young but, instead of studying, writing and publishing scholarly papers, he’d spent his days in the local bar playing pool and drinking. Same outcome, different failure mode. Applying this to humanity, longtermists would argue that there are failure modes that could leave our potential unfulfilled without us dying out, which I will return to below.\nOn this view, a climate catastrophe will be a small blip – like a 90-year-old who stubbed his toe when he was two\nTo summarise these ideas so far, humanity has a ‘potential’ of its own, one that transcends the potentials of each individual person, and failing to realise this potential would be extremely bad – indeed, as we will see, a moral catastrophe of literally cosmic proportions. This is the central dogma of longtermism: nothing matters more, ethically speaking, than fulfilling our potential as a species of ‘Earth-originating intelligent life’. It matters so much that longtermists have even coined the scary-sounding term ‘existential risk’ for any possibility of our potential being destroyed, and ‘existential catastrophe’ for any event that actually destroys this potential.\nWhy do I think this ideology is so dangerous? The short answer is that elevating the fulfilment of humanity’s supposed potential above all else could nontrivially increase the probability that actual people – those alive today and in the near future – suffer extreme harms, even death. Consider that, as I noted elsewhere, the longtermist ideology inclines its adherents to take an insouciant attitude towards climate change. Why? Because even if climate change causes island nations to disappear, triggers mass migrations and kills millions of people, it probably isn’t going to compromise our longterm potential over the coming trillions of years. If one takes a cosmic view of the situation, even a climate catastrophe that cuts the human population by 75 per cent for the next two millennia will, in the grand scheme of things, be nothing more than a small blip – the equivalent of a 90-year-old man having stubbed his toe when he was two.\nBostrom’s argument is that ‘a non-existential disaster causing the breakdown of global civilisation is, from the perspective of humanity as a whole, a potentially recoverable setback.’ It might be ‘a giant massacre for man’, he adds, but so long as humanity bounces back to fulfil its potential, it will ultimately register as little more than ‘a small misstep for mankind’. Elsewhere, he writes that the worst natural disasters and devastating atrocities in history become almost imperceptible trivialities when seen from this grand perspective. Referring to the two world wars, AIDS and the Chernobyl nuclear accident, he declares that ‘tragic as such events are to the people immediately affected, in the big picture of things … even the worst of these catastrophes are mere ripples on the surface of the great sea of life.’\nThis way of seeing the world, of assessing the badness of AIDS and the Holocaust, implies that future disasters of the same (non-existential) scope and intensity should also be categorised as ‘mere ripples’. If they don’t pose a direct existential risk, then we ought not to worry much about them, however tragic they might be to individuals. As Bostrom wrote in 2003, ‘priority number one, two, three and four should … be to reduce existential risk.’ He reiterated this several years later in arguing that we mustn’t ‘fritter … away’ our finite resources on ‘feel-good projects of suboptimal efficacy’ such as alleviating global poverty and reducing animal suffering, since neither threatens our longterm potential, and our longterm potential is what really matters.\nOrd echoes these views in arguing that, of all the problems facing humanity, our ‘first great task … is to reach a place of safety – a place where existential risk’ – as he defines it – ‘is low and stays low’, which he dubs ‘existential security’. More than anything else, what matters is doing everything necessary to ‘preserve’ and ‘protect’ our potential by ‘extracting ourselves from immediate danger’ and devising robust ‘safeguards that will defend humanity from dangers over the longterm future, so that it becomes impossible to fail.’ Although Ord gives a nod to climate change, he also claims – based on a dubious methodology – that the chance of climate change causing an existential catastrophe is only ∼1 in 1,000, which is a whole two orders of magnitude lower than the probability of superintelligent machines destroying humanity this century, according to Ord.\nWhat’s really notable here is that the central concern isn’t the effect of the climate catastrophe on actual people around the world (remember, in the grand scheme, this would be, in Bostrom’s words, a ‘small misstep for mankind’) but the slim possibility that, as Ord puts it in The Precipice, this catastrophe ‘poses a risk of an unrecoverable collapse of civilisation or even the complete extinction of humanity’. Again, the harms caused to actual people (especially those in the Global South) might be significant in absolute terms, but when compared to the ‘vastness’ and ‘glory’ of our longterm potential in the cosmos, they hardly even register.\nYet the implications of longtermism are far more worrisome. If our top four priorities are to avoid an existential catastrophe – ie, to fulfil ‘our potential’ – then what’s not on the table for making this happen? Consider Thomas Nagel’s comment about how the notion of what we might call the ‘greater good’ has been used to ‘justify’ certain atrocities (eg, during war). If the ends ‘justify’ the means, he argues, and the ends are thought to be sufficiently large (eg, national security), then this ‘can be brought to bear to ease the consciences of those responsible for a certain number of charred babies’. Now imagine what might be ‘justified’ if the ‘greater good’ isn’t national security but the cosmic potential of Earth-originating intelligent life over the coming trillions of years? During the Second World War, 40 million civilians perished, but compare this number to the 1054 or more people (in Bostrom’s estimate) who could come to exist if we can avoid an existential catastrophe. What shouldn’t we do to ‘protect’ and ‘preserve’ this potential? To ensure that these unborn people come to exist? What means can’t be ‘justified’ by this cosmically significant moral end?\nBostrom himself argued that we should seriously consider establishing a global, invasive surveillance system that monitors every person on the planet in realtime, to amplify the ‘capacities for preventive policing’ (eg, to prevent omnicidal terrorist attacks that could devastate civilisation). Elsewhere, he’s written that states should use preemptive violence/war to avoid existential catastrophes, and argued that saving billions of actual people is the moral equivalent of reducing existential risk by utterly minuscule amounts. In his words, even if there is ‘a mere 1 per cent chance’ of 1054 people existing in the future, then ‘the expected value of reducing existential risk by a mere one billionth of one billionth of one percentage point is worth 100 billion times as much as a billion human lives.’ Such fanaticism – a word that some longtermists embrace – has led a growing number of critics to worry about what might happen if political leaders in the real world were to take Bostrom’s view seriously. To quote the mathematical statistician Olle Häggström, who – perplexingly – tends otherwise to speak favourably of longtermism:\nI feel extremely uneasy about the prospect that [the calculations above] might become recognised among politicians and decision-makers as a guide to policy worth taking literally. It is simply too reminiscent of the old saying ‘If you want to make an omelette, you must be willing to break a few eggs,’ which has typically been used to explain that a bit of genocide or so might be a good thing, if it can contribute to the goal of creating a future utopia. Imagine a situation where the head of the CIA explains to the US president that they have credible evidence that somewhere in Germany, there is a lunatic who is working on a doomsday weapon and intends to use it to wipe out humanity, and that this lunatic has a one-in-a-million chance of succeeding. They have no further information on the identity or whereabouts of this lunatic. If the president has taken Bostrom’s argument to heart, and if he knows how to do the arithmetic, he may conclude that it is worthwhile conducting a full-scale nuclear assault on Germany to kill every single person within its borders.\nHere, then, are a few reasons I find longtermism to be profoundly dangerous. Yet there are additional, fundamental problems with this worldview that no one, to my knowledge, has previously noted in writing. For example, there’s a good case to make that the underlying commitments of longtermism are a major reason why humanity faces so many unprecedented risks to its survival in the first place. Longtermism might, in other words, be incompatible with the attainment of ‘existential security’, meaning that the only way to genuinely reduce the probability of extinction or collapse in the future might be to abandon the longtermist ideology entirely.\nTo Bostrom and Ord, failing to become posthuman would prevent us from realising our vast, glorious potential\nTo understand the argument, let’s first unpack what longtermists mean by our ‘longterm potential’, an expression that I have so far used without defining. We can analyse this concept into three main components: transhumanism, space expansionism, and a moral view closely associated with what philosophers call ‘total utilitarianism’.\nThe first refers to the idea that we should use advanced technologies to reengineer our bodies and brains to create a ‘superior’ race of radically enhanced posthumans (which, confusingly, longtermists place within the category of ‘humanity’). Although Bostrom is perhaps the most prominent transhumanist today, longtermists have shied away from using the term ‘transhumanism’, probably because of its negative associations. Susan Levin, for example, points out that contemporary transhumanism has its roots in the Anglo-American eugenics movement, and transhumanists such as Julian Savulescu, who co-edited the book Human Enhancement (2009) with Bostrom, have literally argued for the consumption of ‘morality-boosting’ chemicals such as oxytocin to avoid an existential catastrophe (which he calls ‘ultimate harm’). As Savulescu writes with a colleague, ‘it is a matter of such urgency to improve humanity morally … that we should seek whatever means there are to effect this.’ Such claims are not only controversial but for many quite disturbing, and hence longtermists have attempted to distance themselves from such ideas, while nonetheless championing the ideology.\nTranshumanism claims that there are various ‘posthuman modes of being’ that are far better than our current human mode. We could, for instance, genetically alter ourselves to gain perfect control over our emotions, or access the internet via neural implants, or maybe even upload our minds to computer hardware to achieve ‘digital immortality’. As Ord urges in The Precipice, think of how awesome it would be to perceive the world via echolocation, like bats and dolphins, or magnetoreception, like red foxes and homing pigeons. ‘Such uncharted experiences,’ Ord writes, ‘exist in minds much less sophisticated than our own. What experiences, possibly of immense value, could be accessible, then, to minds much greater?’ Bostrom’s most fantastical exploration of these possibilities comes from his evocative ‘Letter from Utopia’ (2008), which depicts a techno-Utopian world full of superintelligent posthumans awash in so much ‘pleasure’ that, as the letter’s fictional posthuman writes, ‘we sprinkle it in our tea.’\nThe connection with longtermism is that, according to Bostrom and Ord, failing to become posthuman would seemingly prevent us from realising our vast and glorious potential, which would be existentially catastrophic. As Bostrom put it in 2012, ‘the permanent foreclosure of any possibility of this kind of transformative change of human biological nature may itself constitute an existential catastrophe.’ Similarly, Ord asserts that ‘forever preserving humanity as it is now may also squander our legacy, relinquishing the greater part of our potential.’\nThe second component of our potential – space expansionism – refers to the idea that we must colonise as much of our future light cone as possible: that is, the region of spacetime that is theoretically accessible to us. According to longtermists, our future light cone contains a huge quantity of exploitable resources, which they refer to as our ‘cosmic endowment’ of negentropy (or reverse entropy). The Milky Way alone, Ord writes, is ‘150,000 light years across, encompassing more than 100 billion stars, most with their own planets.’ Attaining humanity’s longterm potential, he continues, ‘requires only that [we] eventually travel to a nearby star and establish enough of a foothold to create a new flourishing society from which we could venture further.’ By spreading ‘just six light years at a time’, our posthuman descendants could make ‘almost all the stars of our galaxy … reachable’ since ‘each star system, including our own, would need to settle just the few nearest stars [for] the entire galaxy [to] eventually fill with life.’ The process could be exponential, resulting in ever-more ‘flourishing’ societies with each additional second our descendants hop from star to star.\nBut why exactly would we want to do this? What’s so important about flooding the Universe with new posthuman civilisations? This leads to the third component: total utilitarianism, which I will refer to as ‘utilitarianism’ for short. Although some longtermists insist that they aren’t utilitarians, we should right away note that this is mostly a smoke-and-mirrors act to deflect criticisms that longtermism – and, more generally, the effective altruism (EA) movement from which it emerged – is nothing more than utilitarianism repackaged. The fact is that the EA movement is deeply utilitarian, at least in practice, and indeed, before it decided upon a name, the movement’s early members, including Ord, seriously considered calling it the ‘effective utilitarian community’.\nThis being said, utilitarianism is an ethical theory that specifies our sole moral obligation as being to maximise the total amount of ‘intrinsic value’ in the world, as tallied up from a disembodied, impartial, cosmic vantage point called ‘the point of view of the Universe’. From this view, it doesn’t matter how value – which utilitarian hedonists equate with pleasure – is distributed among people across space and time. All that matters is the total net sum. For example, imagine that there are 1 trillion people who have lives of value ‘1’, meaning that they are just barely worth living. This gives a total value of 1 trillion. Now consider an alternative universe in which 1 billion people have lives with a value of ‘999’, meaning that their lives are extremely good. This gives a total value of 999 billion. Since 999 billion is less than 1 trillion, the first world full of lives hardly worth living would be morally better than the second world, and hence, if a utilitarian were forced to choose between these, she would pick the former. (This is called the ‘repugnant conclusion’, which longtermists such as Ord, MacAskill and Greaves recently argued shouldn’t be taken very seriously. For them, the first world really might be better!)\nBeckstead argued that we should prioritise the lives of people in rich countries over those in poor countries\nThe underlying reasoning here is based on the idea that people – you and I – are nothing more than means to an end. We don’t matter in ourselves; we have no inherent value of our own. Instead, people are understood as the ‘containers’ of value, and hence we matter only insofar as we ‘contain’ value, and therefore contribute to the overall net amount of value in the Universe between the Big Bang and the heat death. Since utilitarianism tells us to maximise value, it follows that the more people (value containers) who exist with net-positive amounts of value (pleasure), the better the Universe will become, morally speaking. In a phrase: people exist for the sake of maximising value, rather than value existing for the sake of benefitting people.\nThis is why longtermists are obsessed with calculating how many people could exist in the future if we were to colonise space and create vast computer simulations around stars in which unfathomably huge numbers of people live net-positive lives in virtual-reality environments. I already mentioned Bostrom’s estimate of 1054 future people, which includes many of these ‘digital people’, but in his bestseller Superintelligence (2014) he puts the number even higher at 1058 people, nearly all of whom would ‘live rich and happy lives while interacting with one another in virtual environments’. Greaves and MacAskill are similarly excited about this possibility, estimating that some 1045 conscious beings in computer simulations could exist within the Milky Way alone.\nThat is what our ‘vast and glorious’ potential consists of: massive numbers of technologically enhanced digital posthumans inside huge computer simulations spread throughout our future light cone. It is for this goal that, in Häggström’s scenario, a longtermist politician would annihilate Germany. It is for this goal that we must not ‘fritter … away’ our resources on such things as solving global poverty. It is for this goal that we should consider implementing a global surveillance system, keep pre-emptive war on the table, and focus more on superintelligent machines than saving people in the Global South from the devastating effects of climate change (mostly caused by the Global North). In fact, Beckstead has even argued that, for the sake of attaining this goal, we should actually prioritise the lives of people in rich countries over those in poor countries, since influencing the long-term future is of ‘overwhelming importance’, and the former are more likely to influence the long-term future than the latter. To quote a passage from Beckstead’s 2013 PhD dissertation, which Ord enthusiastically praises as one of the most important contributions to the longtermist literature:\nSaving lives in poor countries may have significantly smaller ripple effects than saving and improving lives in rich countries. Why? Richer countries have substantially more innovation, and their workers are much more economically productive. [Consequently,] it now seems more plausible to me that saving a life in a rich country is substantially more important than saving a life in a poor country, other things being equal.\nThis is just the tip of the iceberg. Consider the implications of this conception of ‘our potential’ for the development of technology and creation of new risks. Since realising our potential is the ultimate moral goal for humanity, and since our descendants cannot become posthuman, colonise space and create ~1058 people in computer simulations without technologies far more advanced than those around today, failing to develop more technology would itself constitute an existential catastrophe – a failure mode (comparable to Ramsey neglecting his talents by spending his days playing pool and drinking) that Bostrom calls ‘plateauing’. Indeed, Bostrom places this idea front-and-centre in his canonical definition of ‘existential risk’, which denotes any future event that would prevent humanity from reaching and/or sustaining a state of ‘technological maturity’, meaning ‘the attainment of capabilities affording a level of economic productivity and control over nature close to the maximum that could feasibly be achieved.’ Technological maturity is the linchpin here because controlling nature and increasing economic productivity to the absolute physical limits are ostensibly necessary for creating the maximum quantity of ‘value’ within our future light cone.\nBut reflect for a moment on how humanity got itself into the current climatic and ecological crisis. Behind the extraction and burning of fossil fuels, decimation of ecosystems and extermination of species has been the notion that nature is something to be controlled, subjugated, exploited, vanquished, plundered, transformed, reconfigured and manipulated. As the technology theorist Langdon Winner writes in Autonomous Technology (1977), since the time of Francis Bacon our view of technology has been ‘inextricably bound to a single conception of the manner in which power is used – the style of absolute mastery, the despotic, one-way control of the master over the slave.’ He adds:\nThere are seldom any reservations about man’s rightful role in conquering, vanquishing, and subjugating everything natural. This is his power and his glory. What would in other situations seem [to be] rather tawdry and despicable intentions are here the most honourable of virtues. Nature is the universal prey, to manipulate as humans see fit.\nThis is precisely what we find in Bostrom’s account of existential risks and its associated normative futurology: nature, the entire Universe, our ‘cosmic endowment’ is there for the plundering, to be manipulated, transformed and converted into ‘value-structures, such as sentient beings living worthwhile lives’ in vast computer simulations, quoting Bostrom’s essay ‘Astronomical Waste’ (2003). Yet this Baconian, capitalist view is one of the most fundamental root causes of the unprecedented environmental crisis that now threatens to destroy large regions of the biosphere, Indigenous communities around the world, and perhaps even Western technological civilisation itself. While other longtermists have not been as explicit as Bostrom, there is a clear tendency to see the natural world the way utilitarianism sees people: as means to some abstract, impersonal end, and nothing more. MacAskill and a colleague, for example, write that the EA movement, and by implication longtermism, is ‘tentatively welfarist in that its tentative aim in doing good concerns promoting wellbeing only and not, say, protecting biodiversity or conserving natural beauty for their own sakes.’\nOn this account, every problem arises from too little rather than too much technology\nJust as worrisome is the longtermist demand that we must create ever-more powerful technologies, despite the agreed-upon fact that the overwhelming source of risk to human existence these days comes from these very technologies. In Ord’s words, ‘without serious efforts to protect humanity, there is strong reason to believe the risk will be higher this century, and increasing with each century that technological progress continues.’ Similarly, in 2012 Bostrom acknowledges that\nthe great bulk of existential risk in the foreseeable future consists of anthropogenic existential risks – that is, arising from human activity. In particular, most of the biggest existential risks seem to be linked to potential future technological breakthroughs that may radically expand our ability to manipulate the external world or our own biology. As our powers expand, so will the scale of their potential consequences – intended and unintended, positive and negative.\nOn this view, there is only one way forward – more technological development – even if this is the most dangerous path into the future. But how much sense does this make? Surely if we want to maximise our chances of survival, we should oppose the development of dangerous new dual-use technologies. If more technology equals greater risk – as history clearly shows and technological projections affirm – then perhaps the only way to actually attain a state of ‘existential security’ is to slow down or completely halt further technological innovation.\nBut longtermists have an answer to this conundrum: the so-called ‘value-neutrality thesis’. This states that technology is a morally neutral object, ie, ‘just a tool’. The idea is most famously encapsulated in the NRA’s slogan ‘Guns don’t kill people, people kill people,’ which conveys the message that the consequences of technology, whether good or bad, beneficial or harmful, are entirely determined by the users, not the artefacts. As Bostrom put it in 2002, ‘we should not blame civilisation or technology for imposing big existential risks,’ adding that ‘because of the way we have defined existential risks, a failure to develop technological civilisation would imply that we had fallen victims of an existential disaster.’\nOrd similarly argues that ‘the problem is not so much an excess of technology as a lack of wisdom,’ before going on to quote Carl Sagan’s book Pale Blue Dot (1994): ‘Many of the dangers we face indeed arise from science and technology but, more fundamentally, because we have become powerful without becoming commensurately wise.’ In other words, it is our fault for not being smarter, wiser and more ethical, a cluster of deficiencies that many longtermists believe, in a bit of twisted logic, could be rectified by technologically reengineering our cognitive systems and moral dispositions. Everything, on this account, is an engineering problem, and hence every problem arises from too little rather than too much technology.\nWe can now begin to see how longtermism might be self-defeating. Not only could its ‘fanatical’ emphasis on fulfilling our longterm potential lead people to, eg, neglect non-existential climate change, prioritise the rich over the poor and perhaps even ‘justify’ pre-emptive violence and atrocities for the ‘greater cosmic good’ but it also contains within it the very tendencies – Baconianism, capitalism and value-neutrality – that have driven humanity inches away from the precipice of destruction. Longtermism tells us to maximise economic productivity, our control over nature, our presence in the Universe, the number of (simulated) people who exist in the future, the total amount of impersonal ‘value’ and so on. But to maximise, we must develop increasingly powerful – and dangerous – technologies; failing to do this would itself be an existential catastrophe. Not to worry, though, because technology is not responsible for our worsening predicament, and hence the fact that most risks stem directly from technology is no reason to stop creating more technology. Rather, the problem lies with us, which means only that we must create even more technology to transform ourselves into cognitively and morally enhanced posthumans.\nThis looks like a recipe for disaster. Creating a new race of ‘wise and responsible’ posthumans is implausible and, if advanced technologies continue to be developed at the current rate, a global-scale catastrophe is almost certainly a matter of when rather than if. Yes, we will need advanced technologies if we wish to escape Earth before it’s sterilised by the Sun in a billion years or so. But the crucial fact that longtermists miss is that technology is far more likely to cause our extinction before this distant future event than to save us from it. If you, like me, value the continued survival and flourishing of humanity, you should care about the long term but reject the ideology of longtermism, which is not only dangerous and flawed but might be contributing to, and reinforcing, the risks that now threaten every person on the planet.","length":39271,"excerpt":"It started as a fringe philosophical theory about humanity’s future. It’s now richly funded and increasingly dangerous","byline":"Émile P Torres","dir":null,"siteName":"Aeon Magazine","lang":"en"},"finalizedMeta":{"title":"Why longtermism is the world’s most dangerous secular credo | Aeon Essays","description":"It started as a fringe philosophical theory about humanity’s future. It’s now richly funded and increasingly dangerous","author":false,"creator":"aeonmag","publisher":false,"date":"2024-08-29T04:36:06.666Z","topics":[]},"jsonLd":{"@type":"BreadcrumbList","headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"@context":"https://schema.org","itemListElement":[{"@type":"ListItem","position":1,"name":"Philosophy","item":"https://aeon.co/philosophy"},{"@type":"ListItem","position":2,"name":"Thinkers and theories","item":"https://aeon.co/philosophy/thinkers-and-theories"},{"@type":"ListItem","position":3,"name":"Against longtermism","item":"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo"}]},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"Why longtermism is the world’s most dangerous secular credo | Aeon Essays","description":"It started as a fringe philosophical theory about humanity’s future. It’s now richly funded and increasingly dangerous","canonical":"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo","keywords":[],"image":"https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=3840&quality=75&format=auto","firstParagraph":"Scarecrows keep away migratory birds from the dangers of the tailing ponds created by the exploitation on the tar sands at Fort McMurray, Alberta, Canada. Photo by Larry Towell/Magnum"},"dublinCore":{},"opengraph":{"title":"Why longtermism is the world’s most dangerous secular credo | Aeon Essays","description":"It started as a fringe philosophical theory about humanity’s future. It’s now richly funded and increasingly dangerous","url":"https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo","site_name":"Aeon","locale":"en_GB","type":"article","typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":"https://images.aeonmedia.co/images/58d9e359-d77e-4d11-98b0-e1374794b4d2/essay-final-nn11440305.jpg?width=1200&quality=75&format=auto","image:alt":"<p>Scarecrows keep away migratory birds from the dangers of the tailing ponds created by the exploitation on the tar sands at Fort McMurray, Alberta, Canada. <em>Photo by Larry Towell/Magnum</em></p>","image:width":"2000","image:height":"1252"},"twitter":{"site":"aeonmag","description":false,"card":"summary_large_image","creator":"aeonmag","title":false,"image":false,"label1":"Reading time","data1":"31 min read","handle":"aeonmag"},"archivedData":{"link":false,"wayback":false}}}