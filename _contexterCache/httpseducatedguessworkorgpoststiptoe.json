{"initialLink":"https://educatedguesswork.org/posts/tiptoe/","sanitizedLink":"https://educatedguesswork.org/posts/tiptoe/","finalLink":"https://educatedguesswork.org/posts/tiptoe/","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://educatedguesswork.org/posts/tiptoe/\" itemprop=\"url\">Maybe someday we'll actually be able to search the Web privately</a></contexter-box-head></contexter-box-head><contexter-byline class=\"p-author author\" slot=\"author\"><span class=\"p-name byline\" rel=\"author\" itemprop=\"author\">@ekr____</span></contexter-byline><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2023-10-03T14:27:10.906Z\">10/3/2023</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>A look at the new Tiptoe encrypted search system</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a href=\"https://web.archive.org/web/20231003142716/https://educatedguesswork.org/posts/tiptoe/\" is=\"contexter-link\" target=\"_blank\" rel=\"timemap\" class=\"read-link archive-link\" itemprop=\"archivedAt\" slot=\"archive-link\">Archived</a><a is=\"contexter-link\" href=\"https://educatedguesswork.org/posts/tiptoe/\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"f805a1857d5005123495b2a12e1f4b2da2a69652","data":{"originalLink":"https://educatedguesswork.org/posts/tiptoe/","sanitizedLink":"https://educatedguesswork.org/posts/tiptoe/","canonical":"https://educatedguesswork.org/posts/tiptoe/","htmlText":"<!doctype html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Maybe someday we&#39;ll actually be able to search the Web privately</title>\n    <meta name=\"description\" content=\"\">\n\n    <meta name=\"twitter:card\" content=\"summary\" />\n    <meta name=\"twitter:creator\" content=\"@ekr____\" />\n    <meta property=\"og:url\" content=\"https://educatedguesswork.org/posts/tiptoe/\" />\n    <meta property=\"og:title\" content=\"Maybe someday we&#39;ll actually be able to search the Web privately\" />\n    <meta property=\"og:description\" content=\"A look at the new Tiptoe encrypted search system\" />\n    <meta property=\"og:image\" content=\"https://educatedguesswork.org/img/private-search-illustration.jpg\" />\n    <meta property=\"og:type\" content=\"article\" />\n\n    <link rel=\"stylesheet\" href=\"/css/prism-base16-monokai.dark.css\">\n    <link rel=\"stylesheet\" href=\"/css/index.css\">\n    <link rel=\"alternate\" href=\"/feed/feed.xml\" type=\"application/atom+xml\" title=\"Educated Guesswork\">\n    <link rel=\"alternate\" href=\"/feed/feed.json\" type=\"application/json\" title=\"Educated Guesswork\">\n  </head>\n  <body>\n      <header>\n        <div class=\"wrapper\">\n          <h1 class=\"nav-title\"><a href=\"/\">Educated Guesswork</a></h1>\n          <ul class=\"nav\">\n            <li class=\"nav-item\"><a  href=\"/posts/\">Archive</a></li>\n            <li class=\"nav-item\"><a  href=\"/about/\">About Me</a></li>\n            <li class=\"nav-item desktop-only\"><a href=\"mailto:ekr@rtfm.com\">Contact</a></li>\n            <li class=\"nav-item desktop-only\"><a href=\"https://twitter.com/ekr____\">Follow on Twitter</a></li>\n            <li class=\"nav-item highlight\"><a href=\"/subscribe\">Subscribe</a></li>\n          </ul>\n        </div>\n      </header>\n        <div class=\"wrapper\">\n          <main class=\"tmpl-post\">\n            \n<div class=\"grid\">\n  <div class=\"grid-main\">\n    <article>\n        <h1>Maybe someday we&#39;ll actually be able to search the Web privately</h1><p class=\"subtitle\">A look at the new Tiptoe encrypted search system</subtitle>\n        <p class=\"dateline\">Posted by <a target=\"_blank\" href=\"https://twitter.com/ekr____\">ekr</a> on 02 Oct 2023</p>\n\n        <script>\nwindow.MathJax = {\n  tex: {\n    inlineMath: [['$', '$'], ['\\\\(', '\\\\)']]\n  }\n  }\n</script>\n<script type=\"text/javascript\" id=\"MathJax-script\" async\n  src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js\">\n</script>\n<p><img src=\"/img/private-search-illustration.jpg\" alt=\"Cover llustration of someone searching\"></p>\n<p>The privacy of Web search is tragically bad. For those of you who\nhaven't thought about it, the way that search works is that your\nquery (i.e., whatever you typed in the URL bar) is sent to the\nsearch engine, which responds with a <em>search results page (SERP)</em>\ncontaining the engine's results. The result\nis that the search engine gets to learn everything you search\nfor. The privacy risks here should be obvious\nbecause people routinely type sensitive queries into their search\nengine (e.g., &quot;what is this rash?&quot;,\n&quot;<a href=\"https://b985.fm/new-englands-most-embarrassing-google-searches/\">Why do I sweat so much</a>&quot;, or\neven &quot;<a href=\"https://www.cnn.com/2023/01/18/us/brian-walshe-ana-walshe-google-searches/index.html\">Dismemberment and the best ways to dispose of a body</a>)&quot;, and you're really\njust trusting the search engine not to reveal your browsing history.</p>\n<p>In addition to learning about your search query itself,\nbrowsers and search engines offer a feature called &quot;search suggestions&quot;\nin which the search engine tries to guess what you are looking\nfor from the beginning of your query. The way this works is that\nas you start typing stuff into the search bar, the browser sends\nthe characters typed so far to the search engine, which responds\nwith things it thinks you might be interested in searching for.\nFor instance, if I type the letter &quot;f&quot; into Firefox, this is what\nI get:</p>\n<p><img src=\"/img/search-suggest.png\" alt=\"Search suggestions in Firefox\"></p>\n<p>Everything in the red box is a search suggestion from Google.\nThe stuff below that is from a Firefox-specific mechanism\ncalled <a href=\"https://support.mozilla.org/en-US/kb/firefox-suggest-faq\">Firefox Suggest</a>\nwhich searches your history or—depending on your settings—might\nask Mozilla's servers for suggestions.\nThe important thing to realize here is that <em>anything</em> you type into\nthe search bar might get sent to the server for autocompletion, which\nmeans that even in situations where you are obviously just typing the\nname of a site, as in &quot;facebook&quot;<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup></p>\n<p>Your privacy in this setting basically consists of trusting the search\nengine; even if the search engine has a relatively good <a href=\"https://duckduckgo.com/privacy\">privacy\npolicy</a>, this is still an\nuncomfortable position.  Note that while Firefox and Safari—but\nnot Chrome!—have a lot of anti-tracking features, they don't do\nmuch about this risk because they are oriented towards ad networks\ntracking you cross sites, but all of this interaction is with a single\nsite (e.g., Google.)  There are some mechanisms for protecting your\nprivacy in this situation—primarily <a href=\"posts/traffic-relaying/\">concealing your IP\naddress</a>—but they're clunky, generally\nnot available for free, and require trusting some third party to\nconceal your identity.</p>\n<p>This situation is well-known to most people who work on browsers—and\nto pretty much anyone who thinks about it for a minute—of course\nyou have to send your search queries to the search engine, if it doesn't\nhave your query, it can't fulfill your request. <a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Film/TheCore\"><strong>But what if it could?</strong></a></p>\n<p>This is the question raised by a really cool new <a href=\"https://eprint.iacr.org/2023/1438\">paper</a>\nby Henzinger, Dauterman, Corrigan-Gibbs, and Zeldovich about an encrypted\nsearch system called &quot;Tiptoe&quot;.<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>\nTiptoe promises fully private (in the sense\nthat the server learns nothing about what you are searching for) search\nfor the low low price of 56.9 MiB of communication and 145 core-seconds of\nserver compute time. Let's take a look.</p>\n<h2 id=\"background%3A-embeddings\">Background: Embeddings <a class=\"direct-link\" href=\"#background%3A-embeddings\">#</a></h2>\n<p>In order to understand how Tiptoe works, we need some background on\nwhat's called an\n<a href=\"https://en.wikipedia.org/w/index.php?title=Word_embedding&amp;oldid=1173409586\">embedding</a>.\nThe basic idea behind an embedding is that you can take a piece of\ncontent such as a document or image and convert it into a short(er)\nvector of numbers that preserves most of the semantic (meaningful)\nproperties of the input. They key property here is that two similar\ninputs will have similar embedding vectors.</p>\n<p>As an intuition pump, consider what would happen if we were to\nsimply count the number of times the <a href=\"https://www.sketchengine.eu/wp-content/uploads/word-list/english/english-word-list-total.csv\">500 most common English language words</a> appear in the text. For example, look at this sentence:</p>\n<blockquote>\n<p>I went to the store with my mother</p>\n</blockquote>\n<p>This contains the following words from the top 500 list (the\nnumbers in parentheses are the appearance on the list with\n0 being the most common):</p>\n<ul>\n<li>the(0)</li>\n<li>to(2)</li>\n<li>with(12)</li>\n<li>my(41)</li>\n<li>went(327)</li>\n</ul>\n<p>We can turn this into a vector of numbers by just making a list\nwhere each entry is the number of times the corresponding word\nis present, so in this case it's a vector of 500 components\n(dimension 500), as in:</p>\n<pre><code>1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n</code></pre>\n<p>That's a lot of zeroes, so let's stick to the following form which\nlists the words that are present:</p>\n<pre><code>[the(0) to(2) with(12) my(41) went(327)]\n</code></pre>\n<p>Let's consider a few more sentences:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Number</th>\n<th style=\"text-align:left\">Sentence</th>\n<th style=\"text-align:left\">Embedding</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">1</td>\n<td style=\"text-align:left\">I went to the store with my mother</td>\n<td style=\"text-align:left\"><code>[the(0) to(2) with(12) my(41) went(327)]</code></td>\n</tr>\n<tr>\n<td style=\"text-align:left\">2</td>\n<td style=\"text-align:left\">I went to the store with my sister</td>\n<td style=\"text-align:left\"><code>[the(0) to(2) with(12) my(41) went(327)]</code></td>\n</tr>\n<tr>\n<td style=\"text-align:left\">3</td>\n<td style=\"text-align:left\">I went to the store with your sister</td>\n<td style=\"text-align:left\"><code>[the(0) to(2) with(12) your(23) went(327)]</code></td>\n</tr>\n<tr>\n<td style=\"text-align:left\">4</td>\n<td style=\"text-align:left\">I am going to create the website</td>\n<td style=\"text-align:left\"><code>[the(0) to(2) going(140) am(157) website(321) create(345)]</code></td>\n</tr>\n</tbody>\n</table>\n<p>As you can see, sentences 1 and 2 have exactly the same embedding,\nwhereas sentence 3 has a similar but not identical embedding, because\nI went with <em>your</em> sister rather than with <em>my</em> (mother, sister).\nThis nicely illustrates several key\npoints about embeddings, namely that (1) similar inputs have\nsimilar embeddings and (2) that embeddings necessarily destroy\nsome information (technically term: they are <em>lossy</em>). In this\ncase, you'll notice that they have also destroyed the information\nabout where I went with (your, my) (mother, sister, friend).\nBy contrast, sentence (4) is a totally different sentence and\nhas a much smaller overlap, consisting of only the two common\nwords &quot;the&quot; and &quot;to&quot;<sup class=\"footnote-ref\"><a href=\"#fn3\" id=\"fnref3\">[3]</a></sup></p>\n<p>Once we have computed an embedding, we can easily use it to assess how\nsimilar two sentences are. One conventional procedure\n(and the one we'll be using for the rest of this post)\nis to instead take what's called the <a href=\"https://en.wikipedia.org/wiki/Dot_product\">inner product</a>\nof the two vectors, which means that you take the sum of the pairwise product of\nthe corresponding values in each vector (i.e., we multiply component 1 in vector 1 times component 1 in vector 2,\ncomponent 2 times component 2, and so on). I.e.,</p>\n<p>$$\nP = \\sum_i V_1[i] * V_2[i]\n$$</p>\n<p>The way this works is that we start by looking at the most common\nword (&quot;the&quot;). Each sentence has one &quot;the&quot;, so that component is one\nin each vector. We multiply them to get 1.\nWe then move on to the second most common English word (which happens to be &quot;and&quot;). Neither\nsentence has &quot;and&quot;, so in both vectors this is a 0, and 0*0 = 0. Next\nwe look at the third-most common word (&quot;to&quot;), and so on. We can\ndraw this like so, for the inner product of S1 and S2.</p>\n<p>$$\n\\begin{matrix}\nthe \\\\\nand \\\\\nto  \\\\\n... \\\\\nwith \\\\\n... \\\\\nmy \\\\\n.... \\\\\nwent \\\\\n\\end{matrix}\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n= (1 + 1 + 1 + 1 + 1) = 5\n$$</p>\n<p>By contrast, if we take S1 and S3 we get:</p>\n<p>$$\n\\begin{matrix}\nthe \\\\\nand \\\\\nto  \\\\\n... \\\\\nwith \\\\\n... \\\\\nyour \\\\\n... \\\\\nmy \\\\\n.... \\\\\nwent \\\\\n\\end{matrix}\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n0 \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n... \\\\\n0 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n= (1 + 1 + 1 + 0 + 0 + 1) = 4\n$$</p>\n<p>This value is lower because one sentence has &quot;your&quot;  and the\nother has &quot;my&quot; but neither has both &quot;your&quot; and &quot;my&quot;. Finally, if we take S1 and S4, we get:</p>\n<p>$$\n\\begin{matrix}\nthe \\\\\nand \\\\\nto  \\\\\n... \\\\\nwith \\\\\n... \\\\\nmy \\\\\n.... \\\\\nwent \\\\\n\\end{matrix}\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n0 \\\\\n... \\\\\n0 \\\\\n... \\\\\n0 \\\\\n\\end{bmatrix}\n= (1 + 1 + 0 + 0 + 0) = 3\n$$</p>\n<p>What you should be noticing here is that the more similar (the\nmore words they have in common) the  embedding vectors are, the higher the inner product.\nThe conventional interpretation is that each embedding vector represents\na <em>d</em>-dimensional vector where <em>n</em> is the number of components and that\nthe closer the angle between the two vectors (the more the point in the\nsame direction) the more similar they are. Conveniently, the inner product\nis equal to the <a href=\"https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&amp;page=Sine_and_cosine&amp;id=1175052829&amp;wpFormIdentifier=titleform\">cosine</a> of the angle, which is 1 when the angle is 0\nand 0 when the angle is 90 degrees, and so can be used as a measure\nof vector similarity. Personally, I don't think well in hundreds of dimensions\nso I've never found this interpretation as helpful as one might like,\nbut maybe you will find it more intuitive, and it's good to know anyway.</p>\n<h3 id=\"normalization\">Normalization <a class=\"direct-link\" href=\"#normalization\">#</a></h3>\n<p>I've cheated a little bit in the way I constructed these sentences,\nbecause using this definition sentences which have more of the\ncommon English words (e.g., longer sentences) will tend to look more similar than those which do not. For instance,\nif instead I had used the sentences:</p>\n<blockquote>\n<p>S5: I have been to the store with my sister</p>\n</blockquote>\n<blockquote>\n<p>S6: I have been to the store with your sister</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Number</th>\n<th style=\"text-align:left\">Sentence</th>\n<th style=\"text-align:left\">Embedding</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">2</td>\n<td style=\"text-align:left\">I went to the store with my mother</td>\n<td style=\"text-align:left\"><code>[the(0) to(2) with(12) my(41) went(327)]</code></td>\n</tr>\n<tr>\n<td style=\"text-align:left\">5</td>\n<td style=\"text-align:left\">I have been to the store with my sister</td>\n<td style=\"text-align:left\"><code>[the(0) to(2) with(12) have(19) my(41) been(60)]</code></td>\n</tr>\n<tr>\n<td style=\"text-align:left\">6</td>\n<td style=\"text-align:left\">I have been to the store with your sister</td>\n<td style=\"text-align:left\"><code>[the(0) to(2) with(12) have(19) your(23) been(60)]</code></td>\n</tr>\n</tbody>\n</table>\n<p>You'll notice that sentences 2 and 5 have four words in common (the, to, with, my),\nwhereas 5 and 6 have five words in common (the, to, with, have, been), even though\nthey (at least arguably) have quite a different meaning (who I went to the store with)\nrather than just differing in grammatical tense (have been versus went).</p>\n<p>The standard way to fix this is to <em>normalize</em> the vectors so that the\nthe larger the values of components in aggregate, the less the value of\neach individual component matters. For mathematical reasons, this is\ndone by setting magnitude of the vector (the square root of the\nsum of the squares of each component) to 1, which you can do by dividing\neach component by the magnitude. When we do this, we\nget the following result:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Sentence Pair</th>\n<th style=\"text-align:left\">Un-normalized Inner Product</th>\n<th style=\"text-align:left\">Normalized Inner Product</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">S2 and S5</td>\n<td style=\"text-align:left\">4</td>\n<td style=\"text-align:left\">0.73</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">S2 and S6</td>\n<td style=\"text-align:left\">5</td>\n<td style=\"text-align:left\">0.55</td>\n</tr>\n</tbody>\n</table>\n<p>This matches our intuition that sentences 2 and 5 are more similar than sentences\n2 and 6.</p>\n<p><img src=\"/img/ml-linear-algebra.png\" alt=\"ML always has been\"></p>\n<h3 id=\"real-world-embeddings\">Real-world Embeddings <a class=\"direct-link\" href=\"#real-world-embeddings\">#</a></h3>\n<p>Obviously, I'm massively oversimplifying here and in the real world an\nembedding would be a lot fancier than just counting common words.\nTypically embeddings are computed\nusing some fancier algorithm like <a href=\"https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&amp;page=Word2vec&amp;id=1173932050&amp;wpFormIdentifier=titleform\">Word2vec</a>,\nwhich itself might use a neural network. However, the cool thing here\nis that however you compute the embedding, you can still compute\nthe similarity of two embeddings in the same way, which means\nthat you can just build a system that depends on having <em>some</em> embedding\nmechanism and then work out that embedding separately. This is very\nconvenient for a system like Tiptoe where we can just assume there is\nan embedding and work out cryptography that will work generically for\nany embedding.</p>\n<h2 id=\"tiptoe\">Tiptoe <a class=\"direct-link\" href=\"#tiptoe\">#</a></h2>\n<p>With this background in mind, we are ready to take a look at Tiptoe.</p>\n<h3 id=\"naive-embedding-based-search\">Naive Embedding Based Search <a class=\"direct-link\" href=\"#naive-embedding-based-search\">#</a></h3>\n<p>Let's start by looking at how you could use embeddings to build a search\nengine. The basic intuition here is simple. You have a corpus of documents (e.g., Web pages)\n$D_1, D_2 ... D_n$. For each document, you compute a corresponding\nembedding for the document $Embed(D_1), Embed(D_2), ... Embed(D_n)$. When the user sends\nin their search query $Q$ you compute $Embed(Q)$ and return the document(s)\nthat are closest to $Embed(Q)$, which is to say have the highest inner products.<sup class=\"footnote-ref\"><a href=\"#fn4\" id=\"fnref4\">[4]</a></sup>\nNaively, you just compute the inner product of the embedded query against every\ndocument embedding and then take the top values, though of course there\nare more efficient algorithms.</p>\n<p>The figure below shows a trivial example. In this case, the client's\nembedded query is most similar to $Embed(D_4)$, and so the server sends $D_4$\n(or, in the case of search, its URL) in response.</p>\n<p><img src=\"/img/EmbeddingSearch.png\" alt=\"Example of search with embeddings\"></p>\n<p>This is actually a very simplified version of how modern systems such\nas <a href=\"https://arxiv.org/pdf/2004.12832.pdf\">ColBERT</a> work.</p>\n<p>Of course the problem with this system is the same as the problem we started\nwith, because you have to send your query to the server so it can compute\nthe embedding. There are two obvious ways to address this:</p>\n<ul>\n<li>Compute the embedding on the client and send it to the server.</li>\n<li>Send the entire database to the client</li>\n</ul>\n<p>The first of these doesn't work because the embedding contains lots of\ninformation about the query (otherwise the search engine couldn't\ndo its job). The second doesn't work because the embedding database\nis far too big to send to the client. What we need is a way to do this\nsame computation on the server without sending the client's cleartext\nquery or its embedding to the server.</p>\n<h3 id=\"naive-tiptoe%3A-inner-products-with-homomorphic-encryption\">Naive Tiptoe: Inner Products with Homomorphic Encryption <a class=\"direct-link\" href=\"#naive-tiptoe%3A-inner-products-with-homomorphic-encryption\">#</a></h3>\n<p>Tiptoe addresses this problem by splitting it up into two pieces.\nFirst, the client uses a <a href=\"https://en.wikipedia.org/w/index.php?title=Homomorphic_encryption&amp;oldid=1085790826\">homomorphic encryption</a>\nencryption system to get the server to compute the inner product for\neach document without allowing the server to see the query.</p>\n<p><img src=\"/img/Tiptoe-diagram.png\" alt=\"Tiptoe private ranking\"></p>\n<p>The client then ranks each results by its inner product, which gives\nit a list of the results that are most relevant (e.g., results <code>1, 3, 9</code>).\nThe indices themselves aren't useful: the client needs the URL\nfor each result, so it uses a <a href=\"/posts/pir\">Private Information Retrieval (PIR)</a> scheme to retrieve\nthe URLs associated with the top results from the server.</p>\n<p>The reason for this two-stage design is that the URLs themselves\nare fairly large, and so having the server provide the URL\nfor each result is inefficient, as most of the results will\nbe ranked low and so the user will never see them.\nThe server can also embed the type of preview metainformation\nthat typically appears on the SERP (e.g., a text snippet)\nif it wanted to, but because PIR is\nexpensive, you want the results to be as small as possible.\nOnce the client has the URLs, the it can just go directly to\nwhichever site the user selects.</p>\n<p>I already explained PIR in a previous\n<a href=\"/posts/pir\">post</a>, so this post will just focus on the ranking\nsystem. This system uses some similar concepts to PIR, so you\nmay also want to go review that post.\nYou may recall from that <a href=\"/posts/pir\">post</a> that a homomorphic\nencryption scheme is one in which you can operate on encrypted data.\nSpecifically, if you have two plaintext messages $M_1$ and $M_2$ and\ntheir corresponding ciphertexts $E(M_1)$ and $E(M_2)$ then the\nencryption is homomorphic with respect to a function $F$ if</p>\n<p>$$\nF(E(M_1), E(M_2)) = E(F(M_1, M_2))\n$$</p>\n<p>So, for instance, if you were to have an encryption function which is\nhomomorphic with respect to addition, that would mean you could add\nup the ciphertexts and the result would be the encryption of the\nsum of the plaintexts. I.e.,</p>\n<p>$$\nE(A) + E(B) = E(A + B)\n$$</p>\n<p>Homomorphic encryption allows you to give\nsome encrypted values to another party, have it operate on them\nand give you the result, and then you can decrypt it to get the\nsame result as if they had just operated on the plaintext values,\nbut without them learning anything about the values they are operating\non.</p>\n<p>We can apply homomorphic encryption to this problem as follows. First,\nthe client computes the embedding of the query\ngiving it an embedding vector $V$ and each element of it $i$,\n$V_i$. The client then encrypts each element of $V$ with a homomorphic\nencryption system. Call this $E(V)$ and each element $E(V_i)$.\nThe client sends $E(V)$ to the server.</p>\n<p>The server iterates over each URL $U_j$ and its corresponding embedding\nvalue $D_j$ and computes the inner product of $D_j$ and $E(V)$. Specifically,\nfor each element $i$, it computes the pairwise product $I_{j, i}$:</p>\n<p>$$\nE(I_{j,i}) = D_{j,i} * E(V_i)\n$$</p>\n<p>It then sums up all these values, to get the encrypted inner product for URL $j$.</p>\n<p>$$\nE(I_j) = \\sum_i E(IP_{j,i}) = \\begin{matrix}E(V_1 * D_1) \\\\\n+ \\\\\nE(V_2 * D_2) \\\\\n+ \\\\<br>\nE(V_3 * D_3) \\\\\n+ \\\\<br>\nE(V_4 * D_4) \\\\\n+ \\\\\nE(V_5 * D_5)\n\\end{matrix}\n$$</p>\n<p>Written in pseudo-matrix notation, we get:</p>\n<p>$$\n\\begin{bmatrix}\nE(V_1) \\\\\nE(V_2) \\\\\nE(V_3) \\\\\nE(V_4) \\\\\nE(V_5) \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nD_1 \\\\\nD_2 \\\\\nD_3 \\\\\nD_4 \\\\\nD_5 \\\\\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\nE(V_1 * D_1) \\\\\nE(V_2 * D_2) \\\\\nE(V_3 * D_3) \\\\\nE(V_4 * D_4) \\\\\nE(V_5 * D_5) \\\\\n\\end{bmatrix}\n\\rightarrow\n\\sum_i E(V_i * D_i)\n$$</p>\n<p>The server then sends back the encrypted inner product values to the\nclient (one per document in the corpus). The client decrypts them to\nrecover the inner product values (again, one per document). It can\nthen just pick the highest ones which are the best matches and\nretrieve their URLs via PIR (effectively, &quot;give me the URLs for\ndocuments 1, 3, 9&quot;, etc.). It then dereferences the URLs as normal.\nBecause this is all done under encryption, the server never learns\nyour search query, the matching documents, or the URLs you eventually\ndecide to dereference (though of course those servers see when\nyou visit them). Importantly, these guarantees are cryptographic, so you don't have to\ntrust the server or anyone else not to cheat. This is different\nform proxying systems, where the proxy and the server can collude\nto link up your searches and your identity.</p>\n<div class=\"callout\">\n<h4 id=\"ciphertext-size-matters\">Ciphertext Size Matters <a class=\"direct-link\" href=\"#ciphertext-size-matters\">#</a></h4>\n<p>For instance:</p>\n<ul>\n<li>\n<p>If each value in the embedding vector is a 32-bit floating point number\nand the embedding vector has dimension 700ish, then the embedding\nvalues for each document is around 2800 bits.</p>\n</li>\n<li>\n<p>If we naively use <a href=\"/posts/pir/#detail%3A-homomorphic-encryption-using-elgamal\">ElGamal encryption</a>,\nthen each ciphertext will be around 64 bytes (480 bits).</p>\n</li>\n</ul>\n<p>This is an improvement of a factor of 7 or so, but at the cost\nof doing $N$ encryption operations, which is quite a lot.</p>\n</div>\n<h3 id=\"clustering\">Clustering <a class=\"direct-link\" href=\"#clustering\">#</a></h3>\n<p>Let's take stock of where we are now. The client sends a relatively\nshort value, consisting of $T$ ciphertexts where $T$ is the number\nelements in the embedding vector. The server responds with $N$\nciphertexts, where $N$ is the number of URLs in its corpus and has\nto do $T*N$ multiplications. Depending on the homomorphic encryption algorithm, this might or\nmight not be an improvement on the total communication bandwidth,\nbut it's still linear in the number of documents, which is quite\nbad.</p>\n<p>It's not really possible to reduce the number of operations on the\nserver below linear. The reason for this is that the server\nneeds to operate on the embedding for each document; otherwise\nthe server could determine which embeddings the client <em>isn't</em>\ninterested in by which ones it doesn't have to look at it\nin order to satisfy the client's query. However, it <em>is</em> possible\nto significantly improve the amount of bandwidth consumed by the\nserver's response.</p>\n<p>The trick here is that the server breaks up the corpus of documents\ninto clusters of approximately $\\sqrt N$ size (hence there are approximately\n$\\sqrt N$ clusters). These clusters are arranged so that they\nhave nearby embedding vectors, and hence the documents are\nare theoretically similar. The server publishes the embedding vector for\nthe center of the cluster, and this allows the client to request\n<em>only</em> the inner products for the closest cluster. This reduces\nthe amount of data that the server by a factor of $\\sqrt N$ to order\n$\\sqrt N$. There's just one problem: if the client only queries one cluster, then\ndoesn't the server know which cluster the client is interested in?</p>\n<p>We fix this by having the client send a separate encrypted query for\neach cluster, like so:</p>\n<p>$$\n\\begin{bmatrix}\nE(0) &amp; \\color{red}{E(V_1)} &amp; E(0)  \\\\\nE(0) &amp; \\color{red}{E(V_2)} &amp; E(0)  \\\\\nE(0) &amp; \\color{red}{E(V_3)} &amp; E(0)  \\\\\nE(0) &amp; \\color{red}{E(V_4)} &amp; E(0)  \\\\\nE(0) &amp; \\color{red}{E(V_5)} &amp; E(0)  \\\\\n\\end{bmatrix}\n$$</p>\n<p>In this diagram, each column represents one cluster (and hence there are\n$\\sqrt N$ columns), and each row is\na different embedding component. The column corresponding to the cluster\n(column $q$) of interest (in red) contains the encryption of the client's actual\nquery embedding vector, whereas the rest of the columns just contain\nthe encryption of 0 (the encryption is randomized so that they are\nnot readily identifiable).</p>\n<p>The server takes each column of the client's query and computes the\ninner product for each document the corresponding cluster, as before.\nI.e., for document $j$ in cluster $c$, it computes $E(I_{c, j})$.\n<em>Then</em>, however, the server adds up the\ninner product values across the clusters, with one report for the\nthe sum of the values for 1st URL in each cluster, one for the\nthe sum of the inner products for the 2nd URL, and the cluster,\nand so on, so that the server still only returns the same\nnumber of of ciphertexts as before. I.e., it reports:</p>\n<p>$$\nE(I_j) = \\sum_c E(I_{c, j})\n$$</p>\n<p>Ordinarily the sum of these would be useless, but\nthe trick<sup class=\"footnote-ref\"><a href=\"#fn5\" id=\"fnref5\">[5]</a></sup>\nhere is that because the other columns—corresponding\nto the cluster that is not of interest—are the encryption of\n0, their inner products are <em>also</em> zero, which means that the\nresult sent back to the client only includes the inner products\nfor the column of interest (column $q$).</p>\n<p>The resulting scheme has much better communication overhead:</p>\n<ul>\n<li>The server sends the list of the centers of the embeddings ($\\sqrt N$).</li>\n<li>The client sends a list of $d$ encrypted components for each cluster\n($d \\sqrt N$).</li>\n<li>The server sends a single encrypted inner product value for each\ndocument in the cluster ($\\sqrt N$).</li>\n</ul>\n<p>This is dramatically better than the naive scheme in which the client\nsends $d$ values and the server sends $N$, although at the cost of\npushing some of the transmission cost onto the client, for a total\ntransmission that scales as a factor of $(d+2)\\sqrt N$. Of course,\nthat's still pretty big and the constant factor is <em>also</em> pretty big\n(~512 bits per document for ElGamal). The Tiptoe paper uses some clever\ntricks to bring the size down some  (see <a href=\"#cost\">below</a> for cost numbers) but the end result is still fairly large\n(see <a href=\"#cost\">cost</a> below).</p>\n<h2 id=\"performance\">Performance <a class=\"direct-link\" href=\"#performance\">#</a></h2>\n<p>As should be clear from the previous section it's <em>possible</em> to build\nprivacy-preserving search, but how well does it actually do? This\nactually comes down to two questions:</p>\n<ol>\n<li>How good are the answers?</li>\n<li>How much does it cost?</li>\n</ol>\n<h3 id=\"accuracy\">Accuracy <a class=\"direct-link\" href=\"#accuracy\">#</a></h3>\n<p>First, let's take a look at accuracy. Obviously, a private search\nmechanism will be no better than a non private search mechanism,\nbecause if it were you could just remove the privacy pieces and\nget the same accuracy. However, realistically we should expect worse\naccuracy, just on general principle (i.e., we are hiding information\nfrom the server). In this specific case we\nshould expect worse accuracy because the server is just operating\non the (encrypted) embedding of the query, rather than the whole\nquery, and computing the embedding destroys some information.</p>\n<p>The metric the authors use for performance is something called &quot;MRR@100&quot;, which stands\nfor &quot;mean reciprocal rank at 100&quot;. The way this works is that for each\nquery you determine which result people would have ranked at number\n1 and then ask what position the search algorithm returned it in.\nYou then compute a score that is the inverse of that position,\nso, for instance, if the document were found in position 5, then the\nscore would be $1/5$. The &quot;mean&quot; part is that you average out the\nresults over the document corpus. The &quot;at 100&quot; part is that if the\nsearch algorithm doesn't return the result in the top 100 values,\nyou get a score of zero. In other words:</p>\n<p>$$\nMRR =\n\\frac{\\sum_i^N\n\\begin{cases}\n\\frac{1}{Rank_i} &amp; \\text{if } R_i \\leq 100 \\\\\n0 &amp;\\text{otherwise}\n\\end{cases}\n}\n{N}\n$$</p>\n<p>Note that this score really rewards getting the top result, because even\ngetting it in second place only gets you a per-document score of $1/2$.</p>\n<p>The results look like this:</p>\n<figure class=\"img-center\">\n<p><img src=\"/img/mrr-scores-tiptoe.png\" alt=\"Tiptoe MRR Results\"></p>\n<figcaption>\nSource: Tiptoe paper.\n</figcaption>\n</figure>\n<p>The graph on the left provides MRR@100 comparisons to a number of\nalgorithms, including:</p>\n<ul>\n<li>A modern search algorithm (<a href=\"https://arxiv.org/pdf/2004.12832.pdf\">ColBERT</a>)</li>\n<li>Two somewhat older systems (BM25 and tf-idf)</li>\n</ul>\n<p>As you can see, ColBERT performs the best and Tiptoe gets pretty close\nto tf-idf but is still significantly worse than BM-25. The &quot;embeddings&quot;\nis the result if you don't use the clustering trick described <a href=\"#clustering\">above</a>.\nNotice here that &quot;embeddings&quot; does very well, and in fact is better than\nBM25, so the clustering really does have quite a significant impact on\nthe quality of the results.</p>\n<p>The graph on the right shows the cumulative probability that the\nbest result will be found at an index less than $i$ (i.e., that\nit is found in the top $i$ results). The dotted line shows the\nchance that the best result is in the cluster Tiptoe receives at\nall; which reflects the best result Tiptoe could deliver even if\nit always picked the best result out of the cluster (about 1/3 of the\ntime).</p>\n<p>On the one hand, this is a fairly large regression from the state of the\nart, but on the other hand, it means that there is a lot of room for\nimprovement just by improving the clustering algorithm on the server.\nObviously, there's also room for improvement in terms of ranking within\nthe cluster. With the current design the client just gets the\ninner product so all it can do is rank them, but there might be some\nthings you could do, such as proactively retrieving the first 10 documents\nor so (there is a very steep improvement curve within the first 10)\nand running some local ranking algorithm on their content.</p>\n<h3 id=\"cost\">Cost <a class=\"direct-link\" href=\"#cost\">#</a></h3>\n<p>So how much will all this cost. The answer is &quot;quite a bit\nbut not as much as you would think&quot;. Here's Figure 8, which\nshows the estimated cost of Tiptoe for various document sizes:</p>\n<figure class=\"img-center\">\n<p><img src=\"/img/tiptoe-cost.png\" alt=\"The cost of Tiptoe\"></p>\n<figcaption>\nSource: Tiptoe paper.\n</figcaption>\n</figure>\n<p>The server CPU cost is linear in the number of documents in the corpus\nand would require around 1500 core seconds for something like Google.</p>\n<p>The communication cost is sublinear in the number of\ndocuments but has a very high fixed cost of around 55\nMiB for a query on a corpus the size of\nthe Common Crawl data set (~360 million documents)\nand around 125 MiB for a Google sized system (~8 billion documents).\nTiptoe uses a number of tricks to frontload this\ncost; most of the communication isn't dependent on the\nquery, so that the client and server can exchange it\nin advance without it being in the critical path.\nThe server also has to send the client the\nembedding algorithm, which can be quite large (e.g,\n200+ MiB) but that is reused for multiple queries and\nso can be amortized out.</p>\n<p>Using Amazon's list price costs, the overall cost is around 0.10 USD/query\nfor a system the size of Google. Google doesn't publish their numbers\nbut 9to5Google estimates it at <a href=\"https://9to5google.com/2023/02/23/google-bard-ai-cost-report/#:~:text=An%20estimate%20by%20Morgan%20Stanley%20pins%20down%20a,but%20the%20number%20would%20skyrocket%20when%20using%20AI.\">.002 USD/query</a>.\nThis is 50 times less, which is a big difference, but actually that\nprobably overstates the difference because Google isn't paying list\nprice for their compute costs, so the difference is probably\nquite a bit less. In either case, this is actually a smaller difference\nthan you would expect given the enormous improvement in privacy.</p>\n<h2 id=\"the-bigger-picture\">The Bigger Picture <a class=\"direct-link\" href=\"#the-bigger-picture\">#</a></h2>\n<p>The lesson you should be taking home here is <em>not</em> that Tiptoe is\ngoing to replace Google search tomorrow. Not only are private search\ntechniques like this more expensive than non-private techniques, they\nare inherently less flexible. Google's SERP is a lot more than just\na list of results. For instance here's the top of the search page for\n&quot;tiptoe&quot;:</p>\n<p><img src=\"/img/tiptoe-serp.png\" alt=\"Tiptoe SERP\"></p>\n<p>Note that the first entry is actually a dictionary definition, the info-box\non the right, and the alternate questions. The first website result\nis below all that. Obviously, one could imagine\nenhancing a system like Tiptoe to provide at least some of these\nfeatures, though at yet more cost.</p>\n<p>There are two stories here that are true at the same time. The\nfirst is about technical capabilities: in most cases, private systems are inherently less flexible and\npowerful than their non-flexible counterparts. It's always easier\nto just tell the server everything and let it sort out what to do,\nboth because the server can just unilaterally add new features\nwithout any help from the client and because it's often difficult\nto figure out how to provide a feature privately (just look at all\nthe fancy cryptography that's required to provide a simple list\nof prioritized URLs). This will almost always be true, with the\nonly real exception being cases where the data is so sensitive\nthat it's simply unacceptable to send it to the server at all, and\nso private mechanisms are the only way to go. However, I think the lesson\nof the past 20 years is that people are actually quite willing to\ntell their deepest secrets to some computer, so those cases are quite\nrare.</p>\n<p>The other story is about path dependence. Google search didn't get\nthis fancy at all once; the original search page was much simpler\n(basically a list of URLs with a snippet from the page) and features\nwere added over time. If we imagine a world in which privacy had been\nprioritized right from the start, then we would have a much richer\nprivate search ecosystem—though most likely not as powerful as\nthe one we have now. The entry barrier to increased data collection\nfor slightly better features would most likely be a lot higher than it\nis today. But because we started out with a design that wasn't private,\nit led us naturally to where we are today, where every keystroke you\ntype in the URL/search bar just gets fed to the search provider.</p>\n<p>I'm not under any illusions that it will be easy to reverse course here:\neven in the much simpler situation of protecting your Web traffic\nin transit, it's taken decades to get out from under the weight of\nthe early decisions to do almost everything in the clear and we're\nstill not completely done. Moreover, that was a situation where we had the technology\nto do it for a long time, and it was just a matter of deployment and\ncost. However, the first step to actually changing things is knowing\nhow to do it, and so it's really exciting to see people taking up\nthe challenge.</p>\n<h2 id=\"acknowledgement\">Acknowledgement <a class=\"direct-link\" href=\"#acknowledgement\">#</a></h2>\n<p>Thanks to Henry <a href=\"https://people.csail.mit.edu/henrycg/\">Corrigan-Gibbs</a> for assistance with this post. All mistakes are of course mine.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>Firefox, at least, does make some\nattempt to omit <em>pure</em> navigational queries, so if you type &quot;http://&quot;\nin the Firefox search box, this gets sent to the server, but\n&quot;<a href=\"http://f\">http://f</a>&quot; does not. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>\nDisclosure: this work was partially funded by a grant from\nMozilla, in a program operated by my department. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn3\" class=\"footnote-item\"><p>\nIn a real-world example, one might well prune out these\ncommon not-very-meaningful words. <a href=\"#fnref3\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn4\" class=\"footnote-item\"><p>\nNote that you might use a different algorithm to compute the embeddings\non the documents as on the queries, for instance if you are doing\ntext search over images. For the purposes of this post, however,\nthis is not important. <a href=\"#fnref4\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn5\" class=\"footnote-item\"><p>Note that this is basically\nthe same trick that PIR schemes use. <a href=\"#fnref5\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n\n    </article>\n\n    <section class=\"header-line\">\n      <h3>Keep Reading</h3>\n      <ul><li>Previous: <a href=\"/posts/desolation-wilderness/\">Desolation Wilderness Seven^H^H^H^H^HTwo Summits</a></li>\n      </ul>\n    </section>\n      \n  </div>\n  <aside class=\"grid-sidebar\">\n\n    <section class=\"box\">\n      <h2>More from Educated Guesswork</h2>\n      <p>Tech, tech policy, and sometimes running by <a href=\"/about\">Eric Rescorla</a>. Subscribe to the newsletter for more like this:</p>\n      <form class=\"inline-form\" action=\"https://educatedguesswork-subscribe.fly.dev/subscribe\" method=\"post\">\n  <input type=\"email\" placeholder=\"Your e-mail address...\" id=\"email\" name=\"email\">\n  <button class=\"subscribe-button\">Subscribe</button>\n</form>\n\n\n    </section>\n\n    <section>\n      <h3>Recent Posts</h3>\n\n      \n      <ul reversed class=\"postlist\"}\">\n\n  <li class=\"postlist-item\">\n    <a href=\"/posts/tiptoe/\" class=\"postlist-link\">Maybe someday we&#39;ll actually be able to search the Web privately</a>\n    <time class=\"postlist-date\" datetime=\"2023-10-02\">02 Oct 2023</time>\n    <span class=\"post-tags\">\n    \n      <a href=\"/tags/networking/\" class=\"post-tag\">networking</a>\n      <a href=\"/tags/security/\" class=\"post-tag\">security</a>\n      <a href=\"/tags/privacy/\" class=\"post-tag\">privacy</a>\n      <a href=\"/tags/pir/\" class=\"post-tag\">pir</a>\n      <a href=\"/tags/eli15/\" class=\"post-tag\">eli15</a>\n    </span>\n  </li>\n\n  <li class=\"postlist-item\">\n    <a href=\"/posts/desolation-wilderness/\" class=\"postlist-link\">Desolation Wilderness Seven^H^H^H^H^HTwo Summits</a>\n    <time class=\"postlist-date\" datetime=\"2023-09-05\">05 Sep 2023</time>\n    <span class=\"post-tags\">\n    \n      <a href=\"/tags/running/\" class=\"post-tag\">running</a>\n      <a href=\"/tags/race report/\" class=\"post-tag\">race report</a>\n    </span>\n  </li>\n\n  <li class=\"postlist-item\">\n    <a href=\"/posts/private-access-tokens/\" class=\"postlist-link\">Private Access Tokens, also not great</a>\n    <time class=\"postlist-date\" datetime=\"2023-08-29\">29 Aug 2023</time>\n    <span class=\"post-tags\">\n    \n      <a href=\"/tags/networking/\" class=\"post-tag\">networking</a>\n      <a href=\"/tags/privacy/\" class=\"post-tag\">privacy</a>\n      <a href=\"/tags/web/\" class=\"post-tag\">web</a>\n    </span>\n  </li>\n\n  <li class=\"postlist-item\">\n    <a href=\"/posts/wei/\" class=\"postlist-link\">The endpoint of Web Environment Integrity is a closed Web</a>\n    <time class=\"postlist-date\" datetime=\"2023-08-18\">18 Aug 2023</time>\n    <span class=\"post-tags\">\n    \n      <a href=\"/tags/networking/\" class=\"post-tag\">networking</a>\n      <a href=\"/tags/privacy/\" class=\"post-tag\">privacy</a>\n      <a href=\"/tags/web/\" class=\"post-tag\">web</a>\n    </span>\n  </li>\n\n  <li class=\"postlist-item\">\n    <a href=\"/posts/nat-part-4/\" class=\"postlist-link\">How NATs Work, Part IV: TURN Relaying</a>\n    <time class=\"postlist-date\" datetime=\"2023-07-17\">17 Jul 2023</time>\n    <span class=\"post-tags\">\n    \n      <a href=\"/tags/networking/\" class=\"post-tag\">networking</a>\n      <a href=\"/tags/nat/\" class=\"post-tag\">nat</a>\n    </span>\n  </li>\n\n</ul>\n\n      <p><a href=\"/posts/\">Full archives ...</a></p>\n    </section>\n\n  </aside>\n</div>\n\n<!-- Cloudflare Web Analytics --><script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{\"token\": \"8950ce09297145caa48662b027542727\"}'></script><!-- End Cloudflare Web Analytics -->\n\n\n          </main>\n        </div>\n\n      <footer>\n        <div class=\"wrapper\">\n          <p>Subscribe to the newsletter for more like this:</p>\n          <form class=\"inline-form\" action=\"https://educatedguesswork-subscribe.fly.dev/subscribe\" method=\"post\">\n  <input type=\"email\" placeholder=\"Your e-mail address...\" id=\"email\" name=\"email\">\n  <button class=\"subscribe-button\">Subscribe</button>\n</form>\n\n\n          </div>\n      </footer>\n    </div>\n    <!-- Current page: /posts/tiptoe/ -->\n  </body>\n</html>\n","oembed":false,"readabilityObject":{"title":"Maybe someday we'll actually be able to search the Web privately","content":"<div id=\"readability-page-1\" class=\"page\"><div>\n          <main>\n            \n<div>\n    <article>\n        <p>A look at the new Tiptoe encrypted search system\n        </p>\n\n        \n\n<p><img src=\"/img/private-search-illustration.jpg\" alt=\"Cover llustration of someone searching\"></p>\n<p>The privacy of Web search is tragically bad. For those of you who\nhaven't thought about it, the way that search works is that your\nquery (i.e., whatever you typed in the URL bar) is sent to the\nsearch engine, which responds with a <em>search results page (SERP)</em>\ncontaining the engine's results. The result\nis that the search engine gets to learn everything you search\nfor. The privacy risks here should be obvious\nbecause people routinely type sensitive queries into their search\nengine (e.g., \"what is this rash?\",\n\"<a href=\"https://b985.fm/new-englands-most-embarrassing-google-searches/\">Why do I sweat so much</a>\", or\neven \"<a href=\"https://www.cnn.com/2023/01/18/us/brian-walshe-ana-walshe-google-searches/index.html\">Dismemberment and the best ways to dispose of a body</a>)\", and you're really\njust trusting the search engine not to reveal your browsing history.</p>\n<p>In addition to learning about your search query itself,\nbrowsers and search engines offer a feature called \"search suggestions\"\nin which the search engine tries to guess what you are looking\nfor from the beginning of your query. The way this works is that\nas you start typing stuff into the search bar, the browser sends\nthe characters typed so far to the search engine, which responds\nwith things it thinks you might be interested in searching for.\nFor instance, if I type the letter \"f\" into Firefox, this is what\nI get:</p>\n<p><img src=\"/img/search-suggest.png\" alt=\"Search suggestions in Firefox\"></p>\n<p>Everything in the red box is a search suggestion from Google.\nThe stuff below that is from a Firefox-specific mechanism\ncalled <a href=\"https://support.mozilla.org/en-US/kb/firefox-suggest-faq\">Firefox Suggest</a>\nwhich searches your history or—depending on your settings—might\nask Mozilla's servers for suggestions.\nThe important thing to realize here is that <em>anything</em> you type into\nthe search bar might get sent to the server for autocompletion, which\nmeans that even in situations where you are obviously just typing the\nname of a site, as in \"facebook\"<sup><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup></p>\n<p>Your privacy in this setting basically consists of trusting the search\nengine; even if the search engine has a relatively good <a href=\"https://duckduckgo.com/privacy\">privacy\npolicy</a>, this is still an\nuncomfortable position.  Note that while Firefox and Safari—but\nnot Chrome!—have a lot of anti-tracking features, they don't do\nmuch about this risk because they are oriented towards ad networks\ntracking you cross sites, but all of this interaction is with a single\nsite (e.g., Google.)  There are some mechanisms for protecting your\nprivacy in this situation—primarily <a href=\"posts/traffic-relaying/\">concealing your IP\naddress</a>—but they're clunky, generally\nnot available for free, and require trusting some third party to\nconceal your identity.</p>\n<p>This situation is well-known to most people who work on browsers—and\nto pretty much anyone who thinks about it for a minute—of course\nyou have to send your search queries to the search engine, if it doesn't\nhave your query, it can't fulfill your request. <a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Film/TheCore\"><strong>But what if it could?</strong></a></p>\n<p>This is the question raised by a really cool new <a href=\"https://eprint.iacr.org/2023/1438\">paper</a>\nby Henzinger, Dauterman, Corrigan-Gibbs, and Zeldovich about an encrypted\nsearch system called \"Tiptoe\".<sup><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>\nTiptoe promises fully private (in the sense\nthat the server learns nothing about what you are searching for) search\nfor the low low price of 56.9 MiB of communication and 145 core-seconds of\nserver compute time. Let's take a look.</p>\n<h2 id=\"background%3A-embeddings\">Background: Embeddings <a href=\"#background%3A-embeddings\">#</a></h2>\n<p>In order to understand how Tiptoe works, we need some background on\nwhat's called an\n<a href=\"https://en.wikipedia.org/w/index.php?title=Word_embedding&amp;oldid=1173409586\">embedding</a>.\nThe basic idea behind an embedding is that you can take a piece of\ncontent such as a document or image and convert it into a short(er)\nvector of numbers that preserves most of the semantic (meaningful)\nproperties of the input. They key property here is that two similar\ninputs will have similar embedding vectors.</p>\n<p>As an intuition pump, consider what would happen if we were to\nsimply count the number of times the <a href=\"https://www.sketchengine.eu/wp-content/uploads/word-list/english/english-word-list-total.csv\">500 most common English language words</a> appear in the text. For example, look at this sentence:</p>\n<blockquote>\n<p>I went to the store with my mother</p>\n</blockquote>\n<p>This contains the following words from the top 500 list (the\nnumbers in parentheses are the appearance on the list with\n0 being the most common):</p>\n<ul>\n<li>the(0)</li>\n<li>to(2)</li>\n<li>with(12)</li>\n<li>my(41)</li>\n<li>went(327)</li>\n</ul>\n<p>We can turn this into a vector of numbers by just making a list\nwhere each entry is the number of times the corresponding word\nis present, so in this case it's a vector of 500 components\n(dimension 500), as in:</p>\n<pre><code>1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n</code></pre>\n<p>That's a lot of zeroes, so let's stick to the following form which\nlists the words that are present:</p>\n<pre><code>[the(0) to(2) with(12) my(41) went(327)]\n</code></pre>\n<p>Let's consider a few more sentences:</p>\n<table>\n<thead>\n<tr>\n<th>Number</th>\n<th>Sentence</th>\n<th>Embedding</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>I went to the store with my mother</td>\n<td><code>[the(0) to(2) with(12) my(41) went(327)]</code></td>\n</tr>\n<tr>\n<td>2</td>\n<td>I went to the store with my sister</td>\n<td><code>[the(0) to(2) with(12) my(41) went(327)]</code></td>\n</tr>\n<tr>\n<td>3</td>\n<td>I went to the store with your sister</td>\n<td><code>[the(0) to(2) with(12) your(23) went(327)]</code></td>\n</tr>\n<tr>\n<td>4</td>\n<td>I am going to create the website</td>\n<td><code>[the(0) to(2) going(140) am(157) website(321) create(345)]</code></td>\n</tr>\n</tbody>\n</table>\n<p>As you can see, sentences 1 and 2 have exactly the same embedding,\nwhereas sentence 3 has a similar but not identical embedding, because\nI went with <em>your</em> sister rather than with <em>my</em> (mother, sister).\nThis nicely illustrates several key\npoints about embeddings, namely that (1) similar inputs have\nsimilar embeddings and (2) that embeddings necessarily destroy\nsome information (technically term: they are <em>lossy</em>). In this\ncase, you'll notice that they have also destroyed the information\nabout where I went with (your, my) (mother, sister, friend).\nBy contrast, sentence (4) is a totally different sentence and\nhas a much smaller overlap, consisting of only the two common\nwords \"the\" and \"to\"<sup><a href=\"#fn3\" id=\"fnref3\">[3]</a></sup></p>\n<p>Once we have computed an embedding, we can easily use it to assess how\nsimilar two sentences are. One conventional procedure\n(and the one we'll be using for the rest of this post)\nis to instead take what's called the <a href=\"https://en.wikipedia.org/wiki/Dot_product\">inner product</a>\nof the two vectors, which means that you take the sum of the pairwise product of\nthe corresponding values in each vector (i.e., we multiply component 1 in vector 1 times component 1 in vector 2,\ncomponent 2 times component 2, and so on). I.e.,</p>\n<p>$$\nP = \\sum_i V_1[i] * V_2[i]\n$$</p>\n<p>The way this works is that we start by looking at the most common\nword (\"the\"). Each sentence has one \"the\", so that component is one\nin each vector. We multiply them to get 1.\nWe then move on to the second most common English word (which happens to be \"and\"). Neither\nsentence has \"and\", so in both vectors this is a 0, and 0*0 = 0. Next\nwe look at the third-most common word (\"to\"), and so on. We can\ndraw this like so, for the inner product of S1 and S2.</p>\n<p>$$\n\\begin{matrix}\nthe \\\\\nand \\\\\nto  \\\\\n... \\\\\nwith \\\\\n... \\\\\nmy \\\\\n.... \\\\\nwent \\\\\n\\end{matrix}\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n= (1 + 1 + 1 + 1 + 1) = 5\n$$</p>\n<p>By contrast, if we take S1 and S3 we get:</p>\n<p>$$\n\\begin{matrix}\nthe \\\\\nand \\\\\nto  \\\\\n... \\\\\nwith \\\\\n... \\\\\nyour \\\\\n... \\\\\nmy \\\\\n.... \\\\\nwent \\\\\n\\end{matrix}\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n0 \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n... \\\\\n0 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n= (1 + 1 + 1 + 0 + 0 + 1) = 4\n$$</p>\n<p>This value is lower because one sentence has \"your\"  and the\nother has \"my\" but neither has both \"your\" and \"my\". Finally, if we take S1 and S4, we get:</p>\n<p>$$\n\\begin{matrix}\nthe \\\\\nand \\\\\nto  \\\\\n... \\\\\nwith \\\\\n... \\\\\nmy \\\\\n.... \\\\\nwent \\\\\n\\end{matrix}\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n0 \\\\\n... \\\\\n0 \\\\\n... \\\\\n0 \\\\\n\\end{bmatrix}\n= (1 + 1 + 0 + 0 + 0) = 3\n$$</p>\n<p>What you should be noticing here is that the more similar (the\nmore words they have in common) the  embedding vectors are, the higher the inner product.\nThe conventional interpretation is that each embedding vector represents\na <em>d</em>-dimensional vector where <em>n</em> is the number of components and that\nthe closer the angle between the two vectors (the more the point in the\nsame direction) the more similar they are. Conveniently, the inner product\nis equal to the <a href=\"https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&amp;page=Sine_and_cosine&amp;id=1175052829&amp;wpFormIdentifier=titleform\">cosine</a> of the angle, which is 1 when the angle is 0\nand 0 when the angle is 90 degrees, and so can be used as a measure\nof vector similarity. Personally, I don't think well in hundreds of dimensions\nso I've never found this interpretation as helpful as one might like,\nbut maybe you will find it more intuitive, and it's good to know anyway.</p>\n<h3 id=\"normalization\">Normalization <a href=\"#normalization\">#</a></h3>\n<p>I've cheated a little bit in the way I constructed these sentences,\nbecause using this definition sentences which have more of the\ncommon English words (e.g., longer sentences) will tend to look more similar than those which do not. For instance,\nif instead I had used the sentences:</p>\n<blockquote>\n<p>S5: I have been to the store with my sister</p>\n</blockquote>\n<blockquote>\n<p>S6: I have been to the store with your sister</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Number</th>\n<th>Sentence</th>\n<th>Embedding</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2</td>\n<td>I went to the store with my mother</td>\n<td><code>[the(0) to(2) with(12) my(41) went(327)]</code></td>\n</tr>\n<tr>\n<td>5</td>\n<td>I have been to the store with my sister</td>\n<td><code>[the(0) to(2) with(12) have(19) my(41) been(60)]</code></td>\n</tr>\n<tr>\n<td>6</td>\n<td>I have been to the store with your sister</td>\n<td><code>[the(0) to(2) with(12) have(19) your(23) been(60)]</code></td>\n</tr>\n</tbody>\n</table>\n<p>You'll notice that sentences 2 and 5 have four words in common (the, to, with, my),\nwhereas 5 and 6 have five words in common (the, to, with, have, been), even though\nthey (at least arguably) have quite a different meaning (who I went to the store with)\nrather than just differing in grammatical tense (have been versus went).</p>\n<p>The standard way to fix this is to <em>normalize</em> the vectors so that the\nthe larger the values of components in aggregate, the less the value of\neach individual component matters. For mathematical reasons, this is\ndone by setting magnitude of the vector (the square root of the\nsum of the squares of each component) to 1, which you can do by dividing\neach component by the magnitude. When we do this, we\nget the following result:</p>\n<table>\n<thead>\n<tr>\n<th>Sentence Pair</th>\n<th>Un-normalized Inner Product</th>\n<th>Normalized Inner Product</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>S2 and S5</td>\n<td>4</td>\n<td>0.73</td>\n</tr>\n<tr>\n<td>S2 and S6</td>\n<td>5</td>\n<td>0.55</td>\n</tr>\n</tbody>\n</table>\n<p>This matches our intuition that sentences 2 and 5 are more similar than sentences\n2 and 6.</p>\n<p><img src=\"/img/ml-linear-algebra.png\" alt=\"ML always has been\"></p>\n<h3 id=\"real-world-embeddings\">Real-world Embeddings <a href=\"#real-world-embeddings\">#</a></h3>\n<p>Obviously, I'm massively oversimplifying here and in the real world an\nembedding would be a lot fancier than just counting common words.\nTypically embeddings are computed\nusing some fancier algorithm like <a href=\"https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&amp;page=Word2vec&amp;id=1173932050&amp;wpFormIdentifier=titleform\">Word2vec</a>,\nwhich itself might use a neural network. However, the cool thing here\nis that however you compute the embedding, you can still compute\nthe similarity of two embeddings in the same way, which means\nthat you can just build a system that depends on having <em>some</em> embedding\nmechanism and then work out that embedding separately. This is very\nconvenient for a system like Tiptoe where we can just assume there is\nan embedding and work out cryptography that will work generically for\nany embedding.</p>\n<h2 id=\"tiptoe\">Tiptoe <a href=\"#tiptoe\">#</a></h2>\n<p>With this background in mind, we are ready to take a look at Tiptoe.</p>\n<h3 id=\"naive-embedding-based-search\">Naive Embedding Based Search <a href=\"#naive-embedding-based-search\">#</a></h3>\n<p>Let's start by looking at how you could use embeddings to build a search\nengine. The basic intuition here is simple. You have a corpus of documents (e.g., Web pages)\n$D_1, D_2 ... D_n$. For each document, you compute a corresponding\nembedding for the document $Embed(D_1), Embed(D_2), ... Embed(D_n)$. When the user sends\nin their search query $Q$ you compute $Embed(Q)$ and return the document(s)\nthat are closest to $Embed(Q)$, which is to say have the highest inner products.<sup><a href=\"#fn4\" id=\"fnref4\">[4]</a></sup>\nNaively, you just compute the inner product of the embedded query against every\ndocument embedding and then take the top values, though of course there\nare more efficient algorithms.</p>\n<p>The figure below shows a trivial example. In this case, the client's\nembedded query is most similar to $Embed(D_4)$, and so the server sends $D_4$\n(or, in the case of search, its URL) in response.</p>\n<p><img src=\"/img/EmbeddingSearch.png\" alt=\"Example of search with embeddings\"></p>\n<p>This is actually a very simplified version of how modern systems such\nas <a href=\"https://arxiv.org/pdf/2004.12832.pdf\">ColBERT</a> work.</p>\n<p>Of course the problem with this system is the same as the problem we started\nwith, because you have to send your query to the server so it can compute\nthe embedding. There are two obvious ways to address this:</p>\n<ul>\n<li>Compute the embedding on the client and send it to the server.</li>\n<li>Send the entire database to the client</li>\n</ul>\n<p>The first of these doesn't work because the embedding contains lots of\ninformation about the query (otherwise the search engine couldn't\ndo its job). The second doesn't work because the embedding database\nis far too big to send to the client. What we need is a way to do this\nsame computation on the server without sending the client's cleartext\nquery or its embedding to the server.</p>\n<h3 id=\"naive-tiptoe%3A-inner-products-with-homomorphic-encryption\">Naive Tiptoe: Inner Products with Homomorphic Encryption <a href=\"#naive-tiptoe%3A-inner-products-with-homomorphic-encryption\">#</a></h3>\n<p>Tiptoe addresses this problem by splitting it up into two pieces.\nFirst, the client uses a <a href=\"https://en.wikipedia.org/w/index.php?title=Homomorphic_encryption&amp;oldid=1085790826\">homomorphic encryption</a>\nencryption system to get the server to compute the inner product for\neach document without allowing the server to see the query.</p>\n<p><img src=\"/img/Tiptoe-diagram.png\" alt=\"Tiptoe private ranking\"></p>\n<p>The client then ranks each results by its inner product, which gives\nit a list of the results that are most relevant (e.g., results <code>1, 3, 9</code>).\nThe indices themselves aren't useful: the client needs the URL\nfor each result, so it uses a <a href=\"/posts/pir\">Private Information Retrieval (PIR)</a> scheme to retrieve\nthe URLs associated with the top results from the server.</p>\n<p>The reason for this two-stage design is that the URLs themselves\nare fairly large, and so having the server provide the URL\nfor each result is inefficient, as most of the results will\nbe ranked low and so the user will never see them.\nThe server can also embed the type of preview metainformation\nthat typically appears on the SERP (e.g., a text snippet)\nif it wanted to, but because PIR is\nexpensive, you want the results to be as small as possible.\nOnce the client has the URLs, the it can just go directly to\nwhichever site the user selects.</p>\n<p>I already explained PIR in a previous\n<a href=\"/posts/pir\">post</a>, so this post will just focus on the ranking\nsystem. This system uses some similar concepts to PIR, so you\nmay also want to go review that post.\nYou may recall from that <a href=\"/posts/pir\">post</a> that a homomorphic\nencryption scheme is one in which you can operate on encrypted data.\nSpecifically, if you have two plaintext messages $M_1$ and $M_2$ and\ntheir corresponding ciphertexts $E(M_1)$ and $E(M_2)$ then the\nencryption is homomorphic with respect to a function $F$ if</p>\n<p>$$\nF(E(M_1), E(M_2)) = E(F(M_1, M_2))\n$$</p>\n<p>So, for instance, if you were to have an encryption function which is\nhomomorphic with respect to addition, that would mean you could add\nup the ciphertexts and the result would be the encryption of the\nsum of the plaintexts. I.e.,</p>\n<p>$$\nE(A) + E(B) = E(A + B)\n$$</p>\n<p>Homomorphic encryption allows you to give\nsome encrypted values to another party, have it operate on them\nand give you the result, and then you can decrypt it to get the\nsame result as if they had just operated on the plaintext values,\nbut without them learning anything about the values they are operating\non.</p>\n<p>We can apply homomorphic encryption to this problem as follows. First,\nthe client computes the embedding of the query\ngiving it an embedding vector $V$ and each element of it $i$,\n$V_i$. The client then encrypts each element of $V$ with a homomorphic\nencryption system. Call this $E(V)$ and each element $E(V_i)$.\nThe client sends $E(V)$ to the server.</p>\n<p>The server iterates over each URL $U_j$ and its corresponding embedding\nvalue $D_j$ and computes the inner product of $D_j$ and $E(V)$. Specifically,\nfor each element $i$, it computes the pairwise product $I_{j, i}$:</p>\n<p>$$\nE(I_{j,i}) = D_{j,i} * E(V_i)\n$$</p>\n<p>It then sums up all these values, to get the encrypted inner product for URL $j$.</p>\n<p>$$\nE(I_j) = \\sum_i E(IP_{j,i}) = \\begin{matrix}E(V_1 * D_1) \\\\\n+ \\\\\nE(V_2 * D_2) \\\\\n+ \\\\<br>\nE(V_3 * D_3) \\\\\n+ \\\\<br>\nE(V_4 * D_4) \\\\\n+ \\\\\nE(V_5 * D_5)\n\\end{matrix}\n$$</p>\n<p>Written in pseudo-matrix notation, we get:</p>\n<p>$$\n\\begin{bmatrix}\nE(V_1) \\\\\nE(V_2) \\\\\nE(V_3) \\\\\nE(V_4) \\\\\nE(V_5) \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nD_1 \\\\\nD_2 \\\\\nD_3 \\\\\nD_4 \\\\\nD_5 \\\\\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\nE(V_1 * D_1) \\\\\nE(V_2 * D_2) \\\\\nE(V_3 * D_3) \\\\\nE(V_4 * D_4) \\\\\nE(V_5 * D_5) \\\\\n\\end{bmatrix}\n\\rightarrow\n\\sum_i E(V_i * D_i)\n$$</p>\n<p>The server then sends back the encrypted inner product values to the\nclient (one per document in the corpus). The client decrypts them to\nrecover the inner product values (again, one per document). It can\nthen just pick the highest ones which are the best matches and\nretrieve their URLs via PIR (effectively, \"give me the URLs for\ndocuments 1, 3, 9\", etc.). It then dereferences the URLs as normal.\nBecause this is all done under encryption, the server never learns\nyour search query, the matching documents, or the URLs you eventually\ndecide to dereference (though of course those servers see when\nyou visit them). Importantly, these guarantees are cryptographic, so you don't have to\ntrust the server or anyone else not to cheat. This is different\nform proxying systems, where the proxy and the server can collude\nto link up your searches and your identity.</p>\n<div>\n<h4 id=\"ciphertext-size-matters\">Ciphertext Size Matters <a href=\"#ciphertext-size-matters\">#</a></h4>\n<p>For instance:</p>\n<ul>\n<li>\n<p>If each value in the embedding vector is a 32-bit floating point number\nand the embedding vector has dimension 700ish, then the embedding\nvalues for each document is around 2800 bits.</p>\n</li>\n<li>\n<p>If we naively use <a href=\"/posts/pir/#detail%3A-homomorphic-encryption-using-elgamal\">ElGamal encryption</a>,\nthen each ciphertext will be around 64 bytes (480 bits).</p>\n</li>\n</ul>\n<p>This is an improvement of a factor of 7 or so, but at the cost\nof doing $N$ encryption operations, which is quite a lot.</p>\n</div>\n<h3 id=\"clustering\">Clustering <a href=\"#clustering\">#</a></h3>\n<p>Let's take stock of where we are now. The client sends a relatively\nshort value, consisting of $T$ ciphertexts where $T$ is the number\nelements in the embedding vector. The server responds with $N$\nciphertexts, where $N$ is the number of URLs in its corpus and has\nto do $T*N$ multiplications. Depending on the homomorphic encryption algorithm, this might or\nmight not be an improvement on the total communication bandwidth,\nbut it's still linear in the number of documents, which is quite\nbad.</p>\n<p>It's not really possible to reduce the number of operations on the\nserver below linear. The reason for this is that the server\nneeds to operate on the embedding for each document; otherwise\nthe server could determine which embeddings the client <em>isn't</em>\ninterested in by which ones it doesn't have to look at it\nin order to satisfy the client's query. However, it <em>is</em> possible\nto significantly improve the amount of bandwidth consumed by the\nserver's response.</p>\n<p>The trick here is that the server breaks up the corpus of documents\ninto clusters of approximately $\\sqrt N$ size (hence there are approximately\n$\\sqrt N$ clusters). These clusters are arranged so that they\nhave nearby embedding vectors, and hence the documents are\nare theoretically similar. The server publishes the embedding vector for\nthe center of the cluster, and this allows the client to request\n<em>only</em> the inner products for the closest cluster. This reduces\nthe amount of data that the server by a factor of $\\sqrt N$ to order\n$\\sqrt N$. There's just one problem: if the client only queries one cluster, then\ndoesn't the server know which cluster the client is interested in?</p>\n<p>We fix this by having the client send a separate encrypted query for\neach cluster, like so:</p>\n<p>$$\n\\begin{bmatrix}\nE(0) &amp; \\color{red}{E(V_1)} &amp; E(0)  \\\\\nE(0) &amp; \\color{red}{E(V_2)} &amp; E(0)  \\\\\nE(0) &amp; \\color{red}{E(V_3)} &amp; E(0)  \\\\\nE(0) &amp; \\color{red}{E(V_4)} &amp; E(0)  \\\\\nE(0) &amp; \\color{red}{E(V_5)} &amp; E(0)  \\\\\n\\end{bmatrix}\n$$</p>\n<p>In this diagram, each column represents one cluster (and hence there are\n$\\sqrt N$ columns), and each row is\na different embedding component. The column corresponding to the cluster\n(column $q$) of interest (in red) contains the encryption of the client's actual\nquery embedding vector, whereas the rest of the columns just contain\nthe encryption of 0 (the encryption is randomized so that they are\nnot readily identifiable).</p>\n<p>The server takes each column of the client's query and computes the\ninner product for each document the corresponding cluster, as before.\nI.e., for document $j$ in cluster $c$, it computes $E(I_{c, j})$.\n<em>Then</em>, however, the server adds up the\ninner product values across the clusters, with one report for the\nthe sum of the values for 1st URL in each cluster, one for the\nthe sum of the inner products for the 2nd URL, and the cluster,\nand so on, so that the server still only returns the same\nnumber of of ciphertexts as before. I.e., it reports:</p>\n<p>$$\nE(I_j) = \\sum_c E(I_{c, j})\n$$</p>\n<p>Ordinarily the sum of these would be useless, but\nthe trick<sup><a href=\"#fn5\" id=\"fnref5\">[5]</a></sup>\nhere is that because the other columns—corresponding\nto the cluster that is not of interest—are the encryption of\n0, their inner products are <em>also</em> zero, which means that the\nresult sent back to the client only includes the inner products\nfor the column of interest (column $q$).</p>\n<p>The resulting scheme has much better communication overhead:</p>\n<ul>\n<li>The server sends the list of the centers of the embeddings ($\\sqrt N$).</li>\n<li>The client sends a list of $d$ encrypted components for each cluster\n($d \\sqrt N$).</li>\n<li>The server sends a single encrypted inner product value for each\ndocument in the cluster ($\\sqrt N$).</li>\n</ul>\n<p>This is dramatically better than the naive scheme in which the client\nsends $d$ values and the server sends $N$, although at the cost of\npushing some of the transmission cost onto the client, for a total\ntransmission that scales as a factor of $(d+2)\\sqrt N$. Of course,\nthat's still pretty big and the constant factor is <em>also</em> pretty big\n(~512 bits per document for ElGamal). The Tiptoe paper uses some clever\ntricks to bring the size down some  (see <a href=\"#cost\">below</a> for cost numbers) but the end result is still fairly large\n(see <a href=\"#cost\">cost</a> below).</p>\n<h2 id=\"performance\">Performance <a href=\"#performance\">#</a></h2>\n<p>As should be clear from the previous section it's <em>possible</em> to build\nprivacy-preserving search, but how well does it actually do? This\nactually comes down to two questions:</p>\n<ol>\n<li>How good are the answers?</li>\n<li>How much does it cost?</li>\n</ol>\n<h3 id=\"accuracy\">Accuracy <a href=\"#accuracy\">#</a></h3>\n<p>First, let's take a look at accuracy. Obviously, a private search\nmechanism will be no better than a non private search mechanism,\nbecause if it were you could just remove the privacy pieces and\nget the same accuracy. However, realistically we should expect worse\naccuracy, just on general principle (i.e., we are hiding information\nfrom the server). In this specific case we\nshould expect worse accuracy because the server is just operating\non the (encrypted) embedding of the query, rather than the whole\nquery, and computing the embedding destroys some information.</p>\n<p>The metric the authors use for performance is something called \"MRR@100\", which stands\nfor \"mean reciprocal rank at 100\". The way this works is that for each\nquery you determine which result people would have ranked at number\n1 and then ask what position the search algorithm returned it in.\nYou then compute a score that is the inverse of that position,\nso, for instance, if the document were found in position 5, then the\nscore would be $1/5$. The \"mean\" part is that you average out the\nresults over the document corpus. The \"at 100\" part is that if the\nsearch algorithm doesn't return the result in the top 100 values,\nyou get a score of zero. In other words:</p>\n<p>$$\nMRR =\n\\frac{\\sum_i^N\n\\begin{cases}\n\\frac{1}{Rank_i} &amp; \\text{if } R_i \\leq 100 \\\\\n0 &amp;\\text{otherwise}\n\\end{cases}\n}\n{N}\n$$</p>\n<p>Note that this score really rewards getting the top result, because even\ngetting it in second place only gets you a per-document score of $1/2$.</p>\n<p>The results look like this:</p>\n<figure>\n<p><img src=\"/img/mrr-scores-tiptoe.png\" alt=\"Tiptoe MRR Results\"></p>\n<figcaption>\nSource: Tiptoe paper.\n</figcaption>\n</figure>\n<p>The graph on the left provides MRR@100 comparisons to a number of\nalgorithms, including:</p>\n<ul>\n<li>A modern search algorithm (<a href=\"https://arxiv.org/pdf/2004.12832.pdf\">ColBERT</a>)</li>\n<li>Two somewhat older systems (BM25 and tf-idf)</li>\n</ul>\n<p>As you can see, ColBERT performs the best and Tiptoe gets pretty close\nto tf-idf but is still significantly worse than BM-25. The \"embeddings\"\nis the result if you don't use the clustering trick described <a href=\"#clustering\">above</a>.\nNotice here that \"embeddings\" does very well, and in fact is better than\nBM25, so the clustering really does have quite a significant impact on\nthe quality of the results.</p>\n<p>The graph on the right shows the cumulative probability that the\nbest result will be found at an index less than $i$ (i.e., that\nit is found in the top $i$ results). The dotted line shows the\nchance that the best result is in the cluster Tiptoe receives at\nall; which reflects the best result Tiptoe could deliver even if\nit always picked the best result out of the cluster (about 1/3 of the\ntime).</p>\n<p>On the one hand, this is a fairly large regression from the state of the\nart, but on the other hand, it means that there is a lot of room for\nimprovement just by improving the clustering algorithm on the server.\nObviously, there's also room for improvement in terms of ranking within\nthe cluster. With the current design the client just gets the\ninner product so all it can do is rank them, but there might be some\nthings you could do, such as proactively retrieving the first 10 documents\nor so (there is a very steep improvement curve within the first 10)\nand running some local ranking algorithm on their content.</p>\n<h3 id=\"cost\">Cost <a href=\"#cost\">#</a></h3>\n<p>So how much will all this cost. The answer is \"quite a bit\nbut not as much as you would think\". Here's Figure 8, which\nshows the estimated cost of Tiptoe for various document sizes:</p>\n<figure>\n<p><img src=\"/img/tiptoe-cost.png\" alt=\"The cost of Tiptoe\"></p>\n<figcaption>\nSource: Tiptoe paper.\n</figcaption>\n</figure>\n<p>The server CPU cost is linear in the number of documents in the corpus\nand would require around 1500 core seconds for something like Google.</p>\n<p>The communication cost is sublinear in the number of\ndocuments but has a very high fixed cost of around 55\nMiB for a query on a corpus the size of\nthe Common Crawl data set (~360 million documents)\nand around 125 MiB for a Google sized system (~8 billion documents).\nTiptoe uses a number of tricks to frontload this\ncost; most of the communication isn't dependent on the\nquery, so that the client and server can exchange it\nin advance without it being in the critical path.\nThe server also has to send the client the\nembedding algorithm, which can be quite large (e.g,\n200+ MiB) but that is reused for multiple queries and\nso can be amortized out.</p>\n<p>Using Amazon's list price costs, the overall cost is around 0.10 USD/query\nfor a system the size of Google. Google doesn't publish their numbers\nbut 9to5Google estimates it at <a href=\"https://9to5google.com/2023/02/23/google-bard-ai-cost-report/#:~:text=An%20estimate%20by%20Morgan%20Stanley%20pins%20down%20a,but%20the%20number%20would%20skyrocket%20when%20using%20AI.\">.002 USD/query</a>.\nThis is 50 times less, which is a big difference, but actually that\nprobably overstates the difference because Google isn't paying list\nprice for their compute costs, so the difference is probably\nquite a bit less. In either case, this is actually a smaller difference\nthan you would expect given the enormous improvement in privacy.</p>\n<h2 id=\"the-bigger-picture\">The Bigger Picture <a href=\"#the-bigger-picture\">#</a></h2>\n<p>The lesson you should be taking home here is <em>not</em> that Tiptoe is\ngoing to replace Google search tomorrow. Not only are private search\ntechniques like this more expensive than non-private techniques, they\nare inherently less flexible. Google's SERP is a lot more than just\na list of results. For instance here's the top of the search page for\n\"tiptoe\":</p>\n<p><img src=\"/img/tiptoe-serp.png\" alt=\"Tiptoe SERP\"></p>\n<p>Note that the first entry is actually a dictionary definition, the info-box\non the right, and the alternate questions. The first website result\nis below all that. Obviously, one could imagine\nenhancing a system like Tiptoe to provide at least some of these\nfeatures, though at yet more cost.</p>\n<p>There are two stories here that are true at the same time. The\nfirst is about technical capabilities: in most cases, private systems are inherently less flexible and\npowerful than their non-flexible counterparts. It's always easier\nto just tell the server everything and let it sort out what to do,\nboth because the server can just unilaterally add new features\nwithout any help from the client and because it's often difficult\nto figure out how to provide a feature privately (just look at all\nthe fancy cryptography that's required to provide a simple list\nof prioritized URLs). This will almost always be true, with the\nonly real exception being cases where the data is so sensitive\nthat it's simply unacceptable to send it to the server at all, and\nso private mechanisms are the only way to go. However, I think the lesson\nof the past 20 years is that people are actually quite willing to\ntell their deepest secrets to some computer, so those cases are quite\nrare.</p>\n<p>The other story is about path dependence. Google search didn't get\nthis fancy at all once; the original search page was much simpler\n(basically a list of URLs with a snippet from the page) and features\nwere added over time. If we imagine a world in which privacy had been\nprioritized right from the start, then we would have a much richer\nprivate search ecosystem—though most likely not as powerful as\nthe one we have now. The entry barrier to increased data collection\nfor slightly better features would most likely be a lot higher than it\nis today. But because we started out with a design that wasn't private,\nit led us naturally to where we are today, where every keystroke you\ntype in the URL/search bar just gets fed to the search provider.</p>\n<p>I'm not under any illusions that it will be easy to reverse course here:\neven in the much simpler situation of protecting your Web traffic\nin transit, it's taken decades to get out from under the weight of\nthe early decisions to do almost everything in the clear and we're\nstill not completely done. Moreover, that was a situation where we had the technology\nto do it for a long time, and it was just a matter of deployment and\ncost. However, the first step to actually changing things is knowing\nhow to do it, and so it's really exciting to see people taking up\nthe challenge.</p>\n<h2 id=\"acknowledgement\">Acknowledgement <a href=\"#acknowledgement\">#</a></h2>\n<p>Thanks to Henry <a href=\"https://people.csail.mit.edu/henrycg/\">Corrigan-Gibbs</a> for assistance with this post. All mistakes are of course mine.</p>\n<hr>\n<section>\n<ol>\n<li id=\"fn1\"><p>Firefox, at least, does make some\nattempt to omit <em>pure</em> navigational queries, so if you type \"http://\"\nin the Firefox search box, this gets sent to the server, but\n\"<a href=\"http://f/\">http://f</a>\" does not. <a href=\"#fnref1\">↩︎</a></p>\n</li>\n<li id=\"fn2\"><p>\nDisclosure: this work was partially funded by a grant from\nMozilla, in a program operated by my department. <a href=\"#fnref2\">↩︎</a></p>\n</li>\n<li id=\"fn3\"><p>\nIn a real-world example, one might well prune out these\ncommon not-very-meaningful words. <a href=\"#fnref3\">↩︎</a></p>\n</li>\n<li id=\"fn4\"><p>\nNote that you might use a different algorithm to compute the embeddings\non the documents as on the queries, for instance if you are doing\ntext search over images. For the purposes of this post, however,\nthis is not important. <a href=\"#fnref4\">↩︎</a></p>\n</li>\n<li id=\"fn5\"><p>Note that this is basically\nthe same trick that PIR schemes use. <a href=\"#fnref5\">↩︎</a></p>\n</li>\n</ol>\n</section>\n\n    </article>\n\n    \n      \n  </div>\n\n<!-- Cloudflare Web Analytics --><!-- End Cloudflare Web Analytics -->\n\n\n          </main>\n        </div></div>","textContent":"\n          \n            \n\n    \n        A look at the new Tiptoe encrypted search system\n        \n\n        \n\n\nThe privacy of Web search is tragically bad. For those of you who\nhaven't thought about it, the way that search works is that your\nquery (i.e., whatever you typed in the URL bar) is sent to the\nsearch engine, which responds with a search results page (SERP)\ncontaining the engine's results. The result\nis that the search engine gets to learn everything you search\nfor. The privacy risks here should be obvious\nbecause people routinely type sensitive queries into their search\nengine (e.g., \"what is this rash?\",\n\"Why do I sweat so much\", or\neven \"Dismemberment and the best ways to dispose of a body)\", and you're really\njust trusting the search engine not to reveal your browsing history.\nIn addition to learning about your search query itself,\nbrowsers and search engines offer a feature called \"search suggestions\"\nin which the search engine tries to guess what you are looking\nfor from the beginning of your query. The way this works is that\nas you start typing stuff into the search bar, the browser sends\nthe characters typed so far to the search engine, which responds\nwith things it thinks you might be interested in searching for.\nFor instance, if I type the letter \"f\" into Firefox, this is what\nI get:\n\nEverything in the red box is a search suggestion from Google.\nThe stuff below that is from a Firefox-specific mechanism\ncalled Firefox Suggest\nwhich searches your history or—depending on your settings—might\nask Mozilla's servers for suggestions.\nThe important thing to realize here is that anything you type into\nthe search bar might get sent to the server for autocompletion, which\nmeans that even in situations where you are obviously just typing the\nname of a site, as in \"facebook\"[1]\nYour privacy in this setting basically consists of trusting the search\nengine; even if the search engine has a relatively good privacy\npolicy, this is still an\nuncomfortable position.  Note that while Firefox and Safari—but\nnot Chrome!—have a lot of anti-tracking features, they don't do\nmuch about this risk because they are oriented towards ad networks\ntracking you cross sites, but all of this interaction is with a single\nsite (e.g., Google.)  There are some mechanisms for protecting your\nprivacy in this situation—primarily concealing your IP\naddress—but they're clunky, generally\nnot available for free, and require trusting some third party to\nconceal your identity.\nThis situation is well-known to most people who work on browsers—and\nto pretty much anyone who thinks about it for a minute—of course\nyou have to send your search queries to the search engine, if it doesn't\nhave your query, it can't fulfill your request. But what if it could?\nThis is the question raised by a really cool new paper\nby Henzinger, Dauterman, Corrigan-Gibbs, and Zeldovich about an encrypted\nsearch system called \"Tiptoe\".[2]\nTiptoe promises fully private (in the sense\nthat the server learns nothing about what you are searching for) search\nfor the low low price of 56.9 MiB of communication and 145 core-seconds of\nserver compute time. Let's take a look.\nBackground: Embeddings #\nIn order to understand how Tiptoe works, we need some background on\nwhat's called an\nembedding.\nThe basic idea behind an embedding is that you can take a piece of\ncontent such as a document or image and convert it into a short(er)\nvector of numbers that preserves most of the semantic (meaningful)\nproperties of the input. They key property here is that two similar\ninputs will have similar embedding vectors.\nAs an intuition pump, consider what would happen if we were to\nsimply count the number of times the 500 most common English language words appear in the text. For example, look at this sentence:\n\nI went to the store with my mother\n\nThis contains the following words from the top 500 list (the\nnumbers in parentheses are the appearance on the list with\n0 being the most common):\n\nthe(0)\nto(2)\nwith(12)\nmy(41)\nwent(327)\n\nWe can turn this into a vector of numbers by just making a list\nwhere each entry is the number of times the corresponding word\nis present, so in this case it's a vector of 500 components\n(dimension 500), as in:\n1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\nThat's a lot of zeroes, so let's stick to the following form which\nlists the words that are present:\n[the(0) to(2) with(12) my(41) went(327)]\n\nLet's consider a few more sentences:\n\n\n\nNumber\nSentence\nEmbedding\n\n\n\n\n1\nI went to the store with my mother\n[the(0) to(2) with(12) my(41) went(327)]\n\n\n2\nI went to the store with my sister\n[the(0) to(2) with(12) my(41) went(327)]\n\n\n3\nI went to the store with your sister\n[the(0) to(2) with(12) your(23) went(327)]\n\n\n4\nI am going to create the website\n[the(0) to(2) going(140) am(157) website(321) create(345)]\n\n\n\nAs you can see, sentences 1 and 2 have exactly the same embedding,\nwhereas sentence 3 has a similar but not identical embedding, because\nI went with your sister rather than with my (mother, sister).\nThis nicely illustrates several key\npoints about embeddings, namely that (1) similar inputs have\nsimilar embeddings and (2) that embeddings necessarily destroy\nsome information (technically term: they are lossy). In this\ncase, you'll notice that they have also destroyed the information\nabout where I went with (your, my) (mother, sister, friend).\nBy contrast, sentence (4) is a totally different sentence and\nhas a much smaller overlap, consisting of only the two common\nwords \"the\" and \"to\"[3]\nOnce we have computed an embedding, we can easily use it to assess how\nsimilar two sentences are. One conventional procedure\n(and the one we'll be using for the rest of this post)\nis to instead take what's called the inner product\nof the two vectors, which means that you take the sum of the pairwise product of\nthe corresponding values in each vector (i.e., we multiply component 1 in vector 1 times component 1 in vector 2,\ncomponent 2 times component 2, and so on). I.e.,\n$$\nP = \\sum_i V_1[i] * V_2[i]\n$$\nThe way this works is that we start by looking at the most common\nword (\"the\"). Each sentence has one \"the\", so that component is one\nin each vector. We multiply them to get 1.\nWe then move on to the second most common English word (which happens to be \"and\"). Neither\nsentence has \"and\", so in both vectors this is a 0, and 0*0 = 0. Next\nwe look at the third-most common word (\"to\"), and so on. We can\ndraw this like so, for the inner product of S1 and S2.\n$$\n\\begin{matrix}\nthe \\\\\nand \\\\\nto  \\\\\n... \\\\\nwith \\\\\n... \\\\\nmy \\\\\n.... \\\\\nwent \\\\\n\\end{matrix}\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n= (1 + 1 + 1 + 1 + 1) = 5\n$$\nBy contrast, if we take S1 and S3 we get:\n$$\n\\begin{matrix}\nthe \\\\\nand \\\\\nto  \\\\\n... \\\\\nwith \\\\\n... \\\\\nyour \\\\\n... \\\\\nmy \\\\\n.... \\\\\nwent \\\\\n\\end{matrix}\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n0 \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n... \\\\\n0 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n= (1 + 1 + 1 + 0 + 0 + 1) = 4\n$$\nThis value is lower because one sentence has \"your\"  and the\nother has \"my\" but neither has both \"your\" and \"my\". Finally, if we take S1 and S4, we get:\n$$\n\\begin{matrix}\nthe \\\\\nand \\\\\nto  \\\\\n... \\\\\nwith \\\\\n... \\\\\nmy \\\\\n.... \\\\\nwent \\\\\n\\end{matrix}\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n... \\\\\n1 \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n1  \\\\\n... \\\\\n0 \\\\\n... \\\\\n0 \\\\\n... \\\\\n0 \\\\\n\\end{bmatrix}\n= (1 + 1 + 0 + 0 + 0) = 3\n$$\nWhat you should be noticing here is that the more similar (the\nmore words they have in common) the  embedding vectors are, the higher the inner product.\nThe conventional interpretation is that each embedding vector represents\na d-dimensional vector where n is the number of components and that\nthe closer the angle between the two vectors (the more the point in the\nsame direction) the more similar they are. Conveniently, the inner product\nis equal to the cosine of the angle, which is 1 when the angle is 0\nand 0 when the angle is 90 degrees, and so can be used as a measure\nof vector similarity. Personally, I don't think well in hundreds of dimensions\nso I've never found this interpretation as helpful as one might like,\nbut maybe you will find it more intuitive, and it's good to know anyway.\nNormalization #\nI've cheated a little bit in the way I constructed these sentences,\nbecause using this definition sentences which have more of the\ncommon English words (e.g., longer sentences) will tend to look more similar than those which do not. For instance,\nif instead I had used the sentences:\n\nS5: I have been to the store with my sister\n\n\nS6: I have been to the store with your sister\n\n\n\n\nNumber\nSentence\nEmbedding\n\n\n\n\n2\nI went to the store with my mother\n[the(0) to(2) with(12) my(41) went(327)]\n\n\n5\nI have been to the store with my sister\n[the(0) to(2) with(12) have(19) my(41) been(60)]\n\n\n6\nI have been to the store with your sister\n[the(0) to(2) with(12) have(19) your(23) been(60)]\n\n\n\nYou'll notice that sentences 2 and 5 have four words in common (the, to, with, my),\nwhereas 5 and 6 have five words in common (the, to, with, have, been), even though\nthey (at least arguably) have quite a different meaning (who I went to the store with)\nrather than just differing in grammatical tense (have been versus went).\nThe standard way to fix this is to normalize the vectors so that the\nthe larger the values of components in aggregate, the less the value of\neach individual component matters. For mathematical reasons, this is\ndone by setting magnitude of the vector (the square root of the\nsum of the squares of each component) to 1, which you can do by dividing\neach component by the magnitude. When we do this, we\nget the following result:\n\n\n\nSentence Pair\nUn-normalized Inner Product\nNormalized Inner Product\n\n\n\n\nS2 and S5\n4\n0.73\n\n\nS2 and S6\n5\n0.55\n\n\n\nThis matches our intuition that sentences 2 and 5 are more similar than sentences\n2 and 6.\n\nReal-world Embeddings #\nObviously, I'm massively oversimplifying here and in the real world an\nembedding would be a lot fancier than just counting common words.\nTypically embeddings are computed\nusing some fancier algorithm like Word2vec,\nwhich itself might use a neural network. However, the cool thing here\nis that however you compute the embedding, you can still compute\nthe similarity of two embeddings in the same way, which means\nthat you can just build a system that depends on having some embedding\nmechanism and then work out that embedding separately. This is very\nconvenient for a system like Tiptoe where we can just assume there is\nan embedding and work out cryptography that will work generically for\nany embedding.\nTiptoe #\nWith this background in mind, we are ready to take a look at Tiptoe.\nNaive Embedding Based Search #\nLet's start by looking at how you could use embeddings to build a search\nengine. The basic intuition here is simple. You have a corpus of documents (e.g., Web pages)\n$D_1, D_2 ... D_n$. For each document, you compute a corresponding\nembedding for the document $Embed(D_1), Embed(D_2), ... Embed(D_n)$. When the user sends\nin their search query $Q$ you compute $Embed(Q)$ and return the document(s)\nthat are closest to $Embed(Q)$, which is to say have the highest inner products.[4]\nNaively, you just compute the inner product of the embedded query against every\ndocument embedding and then take the top values, though of course there\nare more efficient algorithms.\nThe figure below shows a trivial example. In this case, the client's\nembedded query is most similar to $Embed(D_4)$, and so the server sends $D_4$\n(or, in the case of search, its URL) in response.\n\nThis is actually a very simplified version of how modern systems such\nas ColBERT work.\nOf course the problem with this system is the same as the problem we started\nwith, because you have to send your query to the server so it can compute\nthe embedding. There are two obvious ways to address this:\n\nCompute the embedding on the client and send it to the server.\nSend the entire database to the client\n\nThe first of these doesn't work because the embedding contains lots of\ninformation about the query (otherwise the search engine couldn't\ndo its job). The second doesn't work because the embedding database\nis far too big to send to the client. What we need is a way to do this\nsame computation on the server without sending the client's cleartext\nquery or its embedding to the server.\nNaive Tiptoe: Inner Products with Homomorphic Encryption #\nTiptoe addresses this problem by splitting it up into two pieces.\nFirst, the client uses a homomorphic encryption\nencryption system to get the server to compute the inner product for\neach document without allowing the server to see the query.\n\nThe client then ranks each results by its inner product, which gives\nit a list of the results that are most relevant (e.g., results 1, 3, 9).\nThe indices themselves aren't useful: the client needs the URL\nfor each result, so it uses a Private Information Retrieval (PIR) scheme to retrieve\nthe URLs associated with the top results from the server.\nThe reason for this two-stage design is that the URLs themselves\nare fairly large, and so having the server provide the URL\nfor each result is inefficient, as most of the results will\nbe ranked low and so the user will never see them.\nThe server can also embed the type of preview metainformation\nthat typically appears on the SERP (e.g., a text snippet)\nif it wanted to, but because PIR is\nexpensive, you want the results to be as small as possible.\nOnce the client has the URLs, the it can just go directly to\nwhichever site the user selects.\nI already explained PIR in a previous\npost, so this post will just focus on the ranking\nsystem. This system uses some similar concepts to PIR, so you\nmay also want to go review that post.\nYou may recall from that post that a homomorphic\nencryption scheme is one in which you can operate on encrypted data.\nSpecifically, if you have two plaintext messages $M_1$ and $M_2$ and\ntheir corresponding ciphertexts $E(M_1)$ and $E(M_2)$ then the\nencryption is homomorphic with respect to a function $F$ if\n$$\nF(E(M_1), E(M_2)) = E(F(M_1, M_2))\n$$\nSo, for instance, if you were to have an encryption function which is\nhomomorphic with respect to addition, that would mean you could add\nup the ciphertexts and the result would be the encryption of the\nsum of the plaintexts. I.e.,\n$$\nE(A) + E(B) = E(A + B)\n$$\nHomomorphic encryption allows you to give\nsome encrypted values to another party, have it operate on them\nand give you the result, and then you can decrypt it to get the\nsame result as if they had just operated on the plaintext values,\nbut without them learning anything about the values they are operating\non.\nWe can apply homomorphic encryption to this problem as follows. First,\nthe client computes the embedding of the query\ngiving it an embedding vector $V$ and each element of it $i$,\n$V_i$. The client then encrypts each element of $V$ with a homomorphic\nencryption system. Call this $E(V)$ and each element $E(V_i)$.\nThe client sends $E(V)$ to the server.\nThe server iterates over each URL $U_j$ and its corresponding embedding\nvalue $D_j$ and computes the inner product of $D_j$ and $E(V)$. Specifically,\nfor each element $i$, it computes the pairwise product $I_{j, i}$:\n$$\nE(I_{j,i}) = D_{j,i} * E(V_i)\n$$\nIt then sums up all these values, to get the encrypted inner product for URL $j$.\n$$\nE(I_j) = \\sum_i E(IP_{j,i}) = \\begin{matrix}E(V_1 * D_1) \\\\\n+ \\\\\nE(V_2 * D_2) \\\\\n+ \\\\\nE(V_3 * D_3) \\\\\n+ \\\\\nE(V_4 * D_4) \\\\\n+ \\\\\nE(V_5 * D_5)\n\\end{matrix}\n$$\nWritten in pseudo-matrix notation, we get:\n$$\n\\begin{bmatrix}\nE(V_1) \\\\\nE(V_2) \\\\\nE(V_3) \\\\\nE(V_4) \\\\\nE(V_5) \\\\\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nD_1 \\\\\nD_2 \\\\\nD_3 \\\\\nD_4 \\\\\nD_5 \\\\\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\nE(V_1 * D_1) \\\\\nE(V_2 * D_2) \\\\\nE(V_3 * D_3) \\\\\nE(V_4 * D_4) \\\\\nE(V_5 * D_5) \\\\\n\\end{bmatrix}\n\\rightarrow\n\\sum_i E(V_i * D_i)\n$$\nThe server then sends back the encrypted inner product values to the\nclient (one per document in the corpus). The client decrypts them to\nrecover the inner product values (again, one per document). It can\nthen just pick the highest ones which are the best matches and\nretrieve their URLs via PIR (effectively, \"give me the URLs for\ndocuments 1, 3, 9\", etc.). It then dereferences the URLs as normal.\nBecause this is all done under encryption, the server never learns\nyour search query, the matching documents, or the URLs you eventually\ndecide to dereference (though of course those servers see when\nyou visit them). Importantly, these guarantees are cryptographic, so you don't have to\ntrust the server or anyone else not to cheat. This is different\nform proxying systems, where the proxy and the server can collude\nto link up your searches and your identity.\n\nCiphertext Size Matters #\nFor instance:\n\n\nIf each value in the embedding vector is a 32-bit floating point number\nand the embedding vector has dimension 700ish, then the embedding\nvalues for each document is around 2800 bits.\n\n\nIf we naively use ElGamal encryption,\nthen each ciphertext will be around 64 bytes (480 bits).\n\n\nThis is an improvement of a factor of 7 or so, but at the cost\nof doing $N$ encryption operations, which is quite a lot.\n\nClustering #\nLet's take stock of where we are now. The client sends a relatively\nshort value, consisting of $T$ ciphertexts where $T$ is the number\nelements in the embedding vector. The server responds with $N$\nciphertexts, where $N$ is the number of URLs in its corpus and has\nto do $T*N$ multiplications. Depending on the homomorphic encryption algorithm, this might or\nmight not be an improvement on the total communication bandwidth,\nbut it's still linear in the number of documents, which is quite\nbad.\nIt's not really possible to reduce the number of operations on the\nserver below linear. The reason for this is that the server\nneeds to operate on the embedding for each document; otherwise\nthe server could determine which embeddings the client isn't\ninterested in by which ones it doesn't have to look at it\nin order to satisfy the client's query. However, it is possible\nto significantly improve the amount of bandwidth consumed by the\nserver's response.\nThe trick here is that the server breaks up the corpus of documents\ninto clusters of approximately $\\sqrt N$ size (hence there are approximately\n$\\sqrt N$ clusters). These clusters are arranged so that they\nhave nearby embedding vectors, and hence the documents are\nare theoretically similar. The server publishes the embedding vector for\nthe center of the cluster, and this allows the client to request\nonly the inner products for the closest cluster. This reduces\nthe amount of data that the server by a factor of $\\sqrt N$ to order\n$\\sqrt N$. There's just one problem: if the client only queries one cluster, then\ndoesn't the server know which cluster the client is interested in?\nWe fix this by having the client send a separate encrypted query for\neach cluster, like so:\n$$\n\\begin{bmatrix}\nE(0) & \\color{red}{E(V_1)} & E(0)  \\\\\nE(0) & \\color{red}{E(V_2)} & E(0)  \\\\\nE(0) & \\color{red}{E(V_3)} & E(0)  \\\\\nE(0) & \\color{red}{E(V_4)} & E(0)  \\\\\nE(0) & \\color{red}{E(V_5)} & E(0)  \\\\\n\\end{bmatrix}\n$$\nIn this diagram, each column represents one cluster (and hence there are\n$\\sqrt N$ columns), and each row is\na different embedding component. The column corresponding to the cluster\n(column $q$) of interest (in red) contains the encryption of the client's actual\nquery embedding vector, whereas the rest of the columns just contain\nthe encryption of 0 (the encryption is randomized so that they are\nnot readily identifiable).\nThe server takes each column of the client's query and computes the\ninner product for each document the corresponding cluster, as before.\nI.e., for document $j$ in cluster $c$, it computes $E(I_{c, j})$.\nThen, however, the server adds up the\ninner product values across the clusters, with one report for the\nthe sum of the values for 1st URL in each cluster, one for the\nthe sum of the inner products for the 2nd URL, and the cluster,\nand so on, so that the server still only returns the same\nnumber of of ciphertexts as before. I.e., it reports:\n$$\nE(I_j) = \\sum_c E(I_{c, j})\n$$\nOrdinarily the sum of these would be useless, but\nthe trick[5]\nhere is that because the other columns—corresponding\nto the cluster that is not of interest—are the encryption of\n0, their inner products are also zero, which means that the\nresult sent back to the client only includes the inner products\nfor the column of interest (column $q$).\nThe resulting scheme has much better communication overhead:\n\nThe server sends the list of the centers of the embeddings ($\\sqrt N$).\nThe client sends a list of $d$ encrypted components for each cluster\n($d \\sqrt N$).\nThe server sends a single encrypted inner product value for each\ndocument in the cluster ($\\sqrt N$).\n\nThis is dramatically better than the naive scheme in which the client\nsends $d$ values and the server sends $N$, although at the cost of\npushing some of the transmission cost onto the client, for a total\ntransmission that scales as a factor of $(d+2)\\sqrt N$. Of course,\nthat's still pretty big and the constant factor is also pretty big\n(~512 bits per document for ElGamal). The Tiptoe paper uses some clever\ntricks to bring the size down some  (see below for cost numbers) but the end result is still fairly large\n(see cost below).\nPerformance #\nAs should be clear from the previous section it's possible to build\nprivacy-preserving search, but how well does it actually do? This\nactually comes down to two questions:\n\nHow good are the answers?\nHow much does it cost?\n\nAccuracy #\nFirst, let's take a look at accuracy. Obviously, a private search\nmechanism will be no better than a non private search mechanism,\nbecause if it were you could just remove the privacy pieces and\nget the same accuracy. However, realistically we should expect worse\naccuracy, just on general principle (i.e., we are hiding information\nfrom the server). In this specific case we\nshould expect worse accuracy because the server is just operating\non the (encrypted) embedding of the query, rather than the whole\nquery, and computing the embedding destroys some information.\nThe metric the authors use for performance is something called \"MRR@100\", which stands\nfor \"mean reciprocal rank at 100\". The way this works is that for each\nquery you determine which result people would have ranked at number\n1 and then ask what position the search algorithm returned it in.\nYou then compute a score that is the inverse of that position,\nso, for instance, if the document were found in position 5, then the\nscore would be $1/5$. The \"mean\" part is that you average out the\nresults over the document corpus. The \"at 100\" part is that if the\nsearch algorithm doesn't return the result in the top 100 values,\nyou get a score of zero. In other words:\n$$\nMRR =\n\\frac{\\sum_i^N\n\\begin{cases}\n\\frac{1}{Rank_i} & \\text{if } R_i \\leq 100 \\\\\n0 &\\text{otherwise}\n\\end{cases}\n}\n{N}\n$$\nNote that this score really rewards getting the top result, because even\ngetting it in second place only gets you a per-document score of $1/2$.\nThe results look like this:\n\n\n\nSource: Tiptoe paper.\n\n\nThe graph on the left provides MRR@100 comparisons to a number of\nalgorithms, including:\n\nA modern search algorithm (ColBERT)\nTwo somewhat older systems (BM25 and tf-idf)\n\nAs you can see, ColBERT performs the best and Tiptoe gets pretty close\nto tf-idf but is still significantly worse than BM-25. The \"embeddings\"\nis the result if you don't use the clustering trick described above.\nNotice here that \"embeddings\" does very well, and in fact is better than\nBM25, so the clustering really does have quite a significant impact on\nthe quality of the results.\nThe graph on the right shows the cumulative probability that the\nbest result will be found at an index less than $i$ (i.e., that\nit is found in the top $i$ results). The dotted line shows the\nchance that the best result is in the cluster Tiptoe receives at\nall; which reflects the best result Tiptoe could deliver even if\nit always picked the best result out of the cluster (about 1/3 of the\ntime).\nOn the one hand, this is a fairly large regression from the state of the\nart, but on the other hand, it means that there is a lot of room for\nimprovement just by improving the clustering algorithm on the server.\nObviously, there's also room for improvement in terms of ranking within\nthe cluster. With the current design the client just gets the\ninner product so all it can do is rank them, but there might be some\nthings you could do, such as proactively retrieving the first 10 documents\nor so (there is a very steep improvement curve within the first 10)\nand running some local ranking algorithm on their content.\nCost #\nSo how much will all this cost. The answer is \"quite a bit\nbut not as much as you would think\". Here's Figure 8, which\nshows the estimated cost of Tiptoe for various document sizes:\n\n\n\nSource: Tiptoe paper.\n\n\nThe server CPU cost is linear in the number of documents in the corpus\nand would require around 1500 core seconds for something like Google.\nThe communication cost is sublinear in the number of\ndocuments but has a very high fixed cost of around 55\nMiB for a query on a corpus the size of\nthe Common Crawl data set (~360 million documents)\nand around 125 MiB for a Google sized system (~8 billion documents).\nTiptoe uses a number of tricks to frontload this\ncost; most of the communication isn't dependent on the\nquery, so that the client and server can exchange it\nin advance without it being in the critical path.\nThe server also has to send the client the\nembedding algorithm, which can be quite large (e.g,\n200+ MiB) but that is reused for multiple queries and\nso can be amortized out.\nUsing Amazon's list price costs, the overall cost is around 0.10 USD/query\nfor a system the size of Google. Google doesn't publish their numbers\nbut 9to5Google estimates it at .002 USD/query.\nThis is 50 times less, which is a big difference, but actually that\nprobably overstates the difference because Google isn't paying list\nprice for their compute costs, so the difference is probably\nquite a bit less. In either case, this is actually a smaller difference\nthan you would expect given the enormous improvement in privacy.\nThe Bigger Picture #\nThe lesson you should be taking home here is not that Tiptoe is\ngoing to replace Google search tomorrow. Not only are private search\ntechniques like this more expensive than non-private techniques, they\nare inherently less flexible. Google's SERP is a lot more than just\na list of results. For instance here's the top of the search page for\n\"tiptoe\":\n\nNote that the first entry is actually a dictionary definition, the info-box\non the right, and the alternate questions. The first website result\nis below all that. Obviously, one could imagine\nenhancing a system like Tiptoe to provide at least some of these\nfeatures, though at yet more cost.\nThere are two stories here that are true at the same time. The\nfirst is about technical capabilities: in most cases, private systems are inherently less flexible and\npowerful than their non-flexible counterparts. It's always easier\nto just tell the server everything and let it sort out what to do,\nboth because the server can just unilaterally add new features\nwithout any help from the client and because it's often difficult\nto figure out how to provide a feature privately (just look at all\nthe fancy cryptography that's required to provide a simple list\nof prioritized URLs). This will almost always be true, with the\nonly real exception being cases where the data is so sensitive\nthat it's simply unacceptable to send it to the server at all, and\nso private mechanisms are the only way to go. However, I think the lesson\nof the past 20 years is that people are actually quite willing to\ntell their deepest secrets to some computer, so those cases are quite\nrare.\nThe other story is about path dependence. Google search didn't get\nthis fancy at all once; the original search page was much simpler\n(basically a list of URLs with a snippet from the page) and features\nwere added over time. If we imagine a world in which privacy had been\nprioritized right from the start, then we would have a much richer\nprivate search ecosystem—though most likely not as powerful as\nthe one we have now. The entry barrier to increased data collection\nfor slightly better features would most likely be a lot higher than it\nis today. But because we started out with a design that wasn't private,\nit led us naturally to where we are today, where every keystroke you\ntype in the URL/search bar just gets fed to the search provider.\nI'm not under any illusions that it will be easy to reverse course here:\neven in the much simpler situation of protecting your Web traffic\nin transit, it's taken decades to get out from under the weight of\nthe early decisions to do almost everything in the clear and we're\nstill not completely done. Moreover, that was a situation where we had the technology\nto do it for a long time, and it was just a matter of deployment and\ncost. However, the first step to actually changing things is knowing\nhow to do it, and so it's really exciting to see people taking up\nthe challenge.\nAcknowledgement #\nThanks to Henry Corrigan-Gibbs for assistance with this post. All mistakes are of course mine.\n\n\n\nFirefox, at least, does make some\nattempt to omit pure navigational queries, so if you type \"http://\"\nin the Firefox search box, this gets sent to the server, but\n\"http://f\" does not. ↩︎\n\n\nDisclosure: this work was partially funded by a grant from\nMozilla, in a program operated by my department. ↩︎\n\n\nIn a real-world example, one might well prune out these\ncommon not-very-meaningful words. ↩︎\n\n\nNote that you might use a different algorithm to compute the embeddings\non the documents as on the queries, for instance if you are doing\ntext search over images. For the purposes of this post, however,\nthis is not important. ↩︎\n\nNote that this is basically\nthe same trick that PIR schemes use. ↩︎\n\n\n\n\n    \n\n    \n      \n  \n\n\n\n\n          \n        ","length":31236,"excerpt":"A look at the new Tiptoe encrypted search system","byline":"Posted by ekr on 02 Oct 2023","dir":null,"siteName":null,"lang":"en"},"finalizedMeta":{"title":"Maybe someday we'll actually be able to search the Web privately","description":"A look at the new Tiptoe encrypted search system","author":false,"creator":"@ekr____","publisher":false,"date":"2023-10-03T14:27:10.906Z","topics":[]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"Maybe someday we'll actually be able to search the Web privately","description":"","canonical":"https://educatedguesswork.org/posts/tiptoe/","keywords":[],"image":"/img/private-search-illustration.jpg","firstParagraph":"A look at the new Tiptoe encrypted search system\n        "},"dublinCore":{},"opengraph":{"title":"Maybe someday we'll actually be able to search the Web privately","description":"A look at the new Tiptoe encrypted search system","url":"https://educatedguesswork.org/posts/tiptoe/","site_name":false,"locale":false,"type":"article","typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":"https://educatedguesswork.org/img/private-search-illustration.jpg"},"twitter":{"site":false,"description":false,"card":"summary","creator":"@ekr____","title":false,"image":false},"archivedData":{"link":"https://web.archive.org/web/20231003142716/https://educatedguesswork.org/posts/tiptoe/","wayback":"https://web.archive.org/web/20231003142716/https://educatedguesswork.org/posts/tiptoe/"}}}