{"initialLink":"https://ollama.com/blog/continue-code-assistant","sanitizedLink":"https://ollama.com/blog/continue-code-assistant","finalLink":"https://ollama.com/blog/continue-code-assistant","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://ollama.com/blog/continue-code-assistant\" itemprop=\"url\">An entirely open-source AI code assistant inside your editor · Ollama Blog</a></contexter-box-head></contexter-box-head><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2024-07-10T15:32:44.083Z\">7/10/2024</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>Continue enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a is=\"contexter-link\" href=\"https://ollama.com/blog/continue-code-assistant\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"ceab6664cf92576805aa2c7efc5612066757c3c4","data":{"originalLink":"https://ollama.com/blog/continue-code-assistant","sanitizedLink":"https://ollama.com/blog/continue-code-assistant","canonical":"https://ollama.com/blog/continue-code-assistant","htmlText":"<!doctype html>\n<html>\n  <head>\n    <title>An entirely open-source AI code assistant inside your editor · Ollama Blog</title>\n    <meta name=\"description\" content=\"Continue enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs.\" />\n    <meta property=\"og:title\" content=\"An entirely open-source AI code assistant inside your editor · Ollama Blog\" />\n    <meta property=\"og:description\" content=\"Continue enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs.\" />\n    <meta property=\"og:url\" content=\"https://ollama.com/public/\" />\n    <meta property=\"og:image\" content=\"https://ollama.com/public/og.png\" />\n    <meta property=\"og:image:type\" content=\"image/png\" />\n    <meta property=\"og:image:width\" content=\"1200\">\n    <meta property=\"og:image:height\" content=\"628\">\n    <meta property=\"og:type\" content=\"website\" />\n\n    <meta property=\"twitter:card\" content=\"summary_large_image\" />\n    <meta property=\"twitter:site\" content=\"ollama\" />\n    <meta property=\"twitter:title\" content=\"An entirely open-source AI code assistant inside your editor · Ollama Blog\" />\n    <meta property=\"twitter:description\" content=\"Continue enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs.\" />\n    <meta property=\"twitter:image:src\" content=\"https://ollama.com/public/og.png\" />\n    <meta property=\"twitter:image:width\" content=\"1200\">\n    <meta property=\"twitter:image:height\" content=\"628\">\n    <link rel=\"stylesheet\" href=\"/public/vendor/highlight/highlight.min.css\">\n    \n<meta charset=\"utf-8\">\n<meta name=\"description\" content=\"Get up and running with large language models.\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<meta property=\"og:image\" content=\"https://ollama.com/public/og.png\" />\n<meta property=\"og:image:type\" content=\"image/png\" />\n<meta property=\"og:image:width\" content=\"1200\">\n<meta property=\"og:image:height\" content=\"628\">\n<meta property=\"og:type\" content=\"website\" />\n\n<meta property=\"twitter:card\" content=\"summary\" />\n<meta property=\"twitter:site\" content=\"ollama\" />\n\n<meta property=\"twitter:image:src\" content=\"https://ollama.com/public/og-twitter.png\" />\n<meta property=\"twitter:image:width\" content=\"1200\">\n<meta property=\"twitter:image:height\" content=\"628\">\n\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"/public/icon-16x16.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"/public/icon-32x32.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"48x48\" href=\"/public/icon-48x48.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"64x64\" href=\"/public/icon-64x64.png\">\n<link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"/public/apple-touch-icon.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=\"/public/android-chrome-icon-192x192.png\">\n<link rel=\"icon\" type=\"image/png\" sizes=\"512x512\" href=\"/public/android-chrome-icon-512x512.png\">\n\n  <link href=\"/public/tailwind.css\" rel=\"stylesheet\">\n\n<style>\n  body {\n    -webkit-font-smoothing: antialiased;\n    font-synthesis: none;\n    text-rendering: optimizeLegibility;\n    position: absolute;\n    min-height: 100vh;\n    width: 100%;\n    height: 100%;\n    margin: 0;\n\n    display: flex;\n    flex-direction: column;\n  }\n</style>\n<script type=\"application/ld+json\">\n  {\n    \"@context\": \"https://schema.org\",\n    \"@type\": \"WebSite\",\n    \"name\": \"Ollama\",\n    \"url\": \"https://ollama.com\"\n  }\n</script>\n<script src=\"/public/vendor/htmx/bundle.js\"></script>\n<script>\n  function copyToClipboard(element) {\n    const commandElement = element.closest('.language-none').querySelector('.command')\n    const copyText = commandElement.value === undefined ? commandElement.textContent.trim() : commandElement.value\n    const textArea = document.createElement(\"textarea\")\n\n    textArea.value = copyText\n    document.body.appendChild(textArea)\n\n    textArea.select()\n    document.execCommand(\"copy\")\n    document.body.removeChild(textArea)\n\n    const copyIcon = element.querySelector(\".copy-icon\")\n    const checkIcon = element.querySelector(\".check-icon\")\n\n    copyIcon.classList.add('hidden')\n    checkIcon.classList.remove('hidden')\n\n    setTimeout(() => {\n        copyIcon.classList.remove('hidden')\n        checkIcon.classList.add('hidden')\n    }, 2000)\n  }\n</script>\n\n  </head>\n  <body>\n    \n<header class=\"sticky top-0 md:static bg-white underline-offset-4 z-40\">\n  \n  <nav class=\"flex w-full px-6 py-4 items-center justify-between md:hidden\">\n    <a href=\"/\" class=\"z-50\">\n      <img src=\"/public/ollama.png\" class=\"w-6\" alt=\"Ollama\" />\n    </a>\n    <input type=\"checkbox\" id=\"menu\" class=\"hidden peer\">\n    <label for=\"menu\" class=\"peer-checked:hidden cursor-pointer z-50\">\n      <svg class=\"h-8 w-8\" fill=\"none\" viewBox=\"0 0 24 24\" stroke-width=\"1.5\" stroke=\"currentColor\" aria-hidden=\"true\">\n        <path stroke-linecap=\"round\" stroke-linejoin=\"round\" d=\"M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5\" />\n      </svg>\n    </label>\n    <label for=\"menu\" class=\"hidden peer-checked:block cursor-pointer z-50\">\n      <svg class=\"h-8 w-8\" fill=\"none\" viewBox=\"0 0 24 24\" stroke-width=\"1.5\" stroke=\"currentColor\" aria-hidden=\"true\">\n        <path stroke-linecap=\"round\" stroke-linejoin=\"round\" d=\"M6 18L18 6M6 6l12 12\" />\n      </svg>\n    </label>\n    <div class=\"peer-checked:flex fixed bg-white hidden top-0 pt-24 h-screen right-0 w-full sm:hidden flex-col px-6 py-4 text-3xl space-y-6 tracking-tight\">\n      <a href=\"/library\">Models</a>\n      <a href=\"/search\">Search</a>\n      <a href=\"https://discord.com/invite/ollama\">Discord</a>\n      <a href=\"https://github.com/ollama/ollama\">GitHub</a>\n      <a href=\"/download\">Download</a>\n      \n      \n      <a class=\"block\" href=\"/login\">Sign in</a>\n      \n    </div>\n  </nav>\n  <nav class=\"hidden select-none mx-auto md:flex justify-between items-center px-6 py-3.5 text-md md:text-lg\">\n    <div class=\"flex items-center space-x-8\">\n      <a class=\"mr-2\" href=\"/\"><img src=\"/public/ollama.png\" class=\"w-8\" alt=\"Ollama\" /></a>\n      <a class=\"hover:underline\" href=\"/blog\">Blog</a>\n      <a class=\"hover:underline\" target=\"_blank\" href=\"https://discord.com/invite/ollama\">\n        Discord\n      </a>\n      <a class=\"hover:underline\" target=\"_blank\" href=\"https://github.com/ollama/ollama\">\n        GitHub\n      </a>\n    </div>\n    <div class=\"flex space-x-8 items-center\">\n      \n      <div class=\"hidden lg:flex items-center flex-grow relative w-full max-w-[500px] pl-8\">\n        <div id=\"searchContainer\" class=\"relative flex items-center rounded-lg border border-neutral-200\">\n          <span id=\"searchIcon\" class=\"pl-2 text-2xl text-neutral-500\">\n            <svg class=\"w-4 h-4 ml-1 mt-0.25 fill-current\" viewBox=\"0 0 20 20\" xmlns=\"http://www.w3.org/2000/svg\">\n              <path d=\"m8.5 3c3.0375661 0 5.5 2.46243388 5.5 5.5 0 1.24832096-.4158777 2.3995085-1.1166416 3.3225711l4.1469717 4.1470988c.2928932.2928932.2928932.767767 0 1.0606602-.2662666.2662665-.6829303.2904726-.9765418.0726181l-.0841184-.0726181-4.1470988-4.1469717c-.9230626.7007639-2.07425014 1.1166416-3.3225711 1.1166416-3.03756612 0-5.5-2.4624339-5.5-5.5 0-3.03756612 2.46243388-5.5 5.5-5.5zm0 1.5c-2.209139 0-4 1.790861-4 4s1.790861 4 4 4 4-1.790861 4-4-1.790861-4-4-4z\"/></svg>\n            </svg>\n          </span>\n          <input id=\"search\" hx-get=\"/search-preview\" hx-trigger=\"keyup changed delay:800ms, focus\" hx-target=\"#popup\"\n            name=\"q\" class=\"w-full resize-none rounded-lg border-0 py-1.5 pr-10 text-sm\n                            focus:outline-none focus:ring-0\" placeholder=\"Search models\" autocomplete=\"off\"\n            value=\"\" />\n          <div id=\"popup\" class=\"absolute hidden left-0 right-0 top-full z-50\">\n            \n          </div>\n        </div>\n      </div>\n      \n      <a class=\"hover:underline\" href=\"/library\">Models</a>\n      \n        <a class=\"hover:underline min-w-fit\" href=\"/login\">Sign in</a>\n      \n      <a class=\"flex cursor-pointer items-center rounded-lg bg-neutral-800 px-4 py-1 text-white hover:bg-black\" href=\"/download\">\n        Download\n      </a>\n    </div>\n  </nav>\n</header>\n<script>\n  const searchInput = document.getElementById('search');\n  const popup = document.getElementById('popup');\n\n  document.addEventListener('DOMContentLoaded', function () {\n    var searchIcon = document.getElementById('searchIcon');\n    if (searchIcon !== null) {\n        searchIcon.addEventListener('click', function() {\n        searchInput.focus();\n      });\n    }\n\n\n    if (searchInput !== null) {\n      searchInput.addEventListener('focus', function () {\n        document.getElementById('searchContainer').classList.add('border-blue-400', 'outline-none', 'ring', 'ring-blue-300', 'ring-opacity-75');\n        popup.classList.remove('hidden');\n        searchInput.addEventListener('keydown', handleKeyPress);\n      });\n\n      searchInput.addEventListener('focusout', function (event) {\n        \n        setTimeout(function () {\n          \n          if (!popup.contains(event.relatedTarget)) {\n            hideSearchPopup();\n            searchInput.blur();\n          }\n        }, 150);\n      });\n\n      searchInput.addEventListener('keydown', function(event) {\n        if (event.key === 'Escape') {\n          hideSearchPopup();\n          searchInput.blur();\n        }\n      });\n    }\n  });\n\n  document.addEventListener('keydown', function(event) {\n    if (event.key === 'Tab') {\n      setTimeout(() => {\n        if (searchInput !== null && popup !== null && !searchInput.contains(document.activeElement) && !popup.contains(document.activeElement)) {\n          searchInput.blur();\n          hideSearchPopup();\n        }\n      }, 0);\n    }\n  });\n\n  function handleKeyPress(event) {\n    if (event.key === 'Enter') {\n      event.preventDefault();\n      var query = document.getElementById('search').value;\n      window.location.href = '/search?q=' + encodeURIComponent(query) + '&p=1';\n    }\n  }\n\n  function hideSearchPopup() {\n    document.getElementById('searchContainer').classList.remove('border-blue-400', 'outline-none', 'ring', 'ring-blue-300', 'ring-opacity-75');\n    document.getElementById('popup').classList.add('hidden');\n    searchInput.removeEventListener('keydown', handleKeyPress);\n  }\n</script>\n\n    <main class=\"mx-auto flex flex-1 max-w-2xl w-full flex-col space-y-3 px-6 py-16 md:px-0\">\n      <h1 class=\"text-4xl font-semibold tracking-tight\">An entirely open-source AI code assistant inside your editor</h1>\n      <h2 class=\"text-neutral-500\">May 31, 2024</h2>\n      <section\n        class=\"\n        prose\n        prose-p:mb-4\n        prose-p:mt-0\n        prose-p:leading-relaxed\n        prose-p:before:hidden\n        prose-p:after:hidden\n\n        prose-ul:mt-1\n        prose-ul:mb-4\n        prose-ul:pl-8\n        marker:prose-ul:text-black\n\n        prose-ol:pl-8\n        prose-ol:mt-1\n        prose-ol:mb-4\n        marker:prose-ol:text-black\n\n        prose-li:mt-0.5\n        prose-li:mb-0\n        prose-li:text-black\n        first:prose-li:mt-0\n\n        prose-headings:font-semibold\n        prose-headings:tracking-tight\n        prose-headings:mt-[1.25em]\n        prose-headings:mb-[0.7em]\n        prose-headings:py-0\n\n        prose-h1:text-[32px]\n        prose-h2:text-2xl\n        prose-h3:text-xl\n        prose-h4:text-lg\n        prose-h5:text-base\n\n        prose-img:mx-auto\n        prose-img:mt-6\n\n        prose-video:mt-6\n        prose-video:mb-2\n\n        prose-code:bg-neutral-100\n        prose-code:rounded-md\n        prose-code:px-2\n        prose-code:inline-block\n        prose-code:font-normal\n        prose-code:text-[85%]\n        prose-code:leading-relaxed\n        prose-code:text-black\n        prose-code:before:hidden\n        prose-code:after:hidden\n        prose-code:my-0\n\n        prose-pre:bg-neutral-100\n        prose-pre:text-black\n        prose-pre:mb-4\n        prose-pre:mt-0\n        prose-pre:whitespace-pre-wrap\n        prose-pre:rounded-lg\n        prose-pre:px-3\n        prose-pre:py-3\n        prose-pre:text-base\n\n        prose-blockquote:before:hidden\n        prose-blockquote:after:hidden\n        prose-blockquote:not-italic\n        prose-blockquote:font-normal\n        prose-blockquote:text-neutral-500\n\n        prose-a:font-normal\n        prose-a:underline-offset-2\n\n        prose-th:text-left\n\n        mb-20\n        max-w-none\n        break-words\n        py-5\n        text-black\"\n      >\n        <blockquote>\n<p>This is a guest post from Ty Dunn, Co-founder of Continue, that covers how to set up, explore, and figure out the best way to use Continue and Ollama together.</p>\n</blockquote>\n\n<p><img src=\"/public/blog/ollama-continue.png\" alt=\"Continue and Ollama\" /></p>\n\n<p><a href=\"https://continue.dev\">Continue</a> enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs. All this can run entirely on your own laptop or have Ollama deployed on a server to remotely power code completion and chat experiences based on your needs.</p>\n\n<p>To get set up, you&rsquo;ll want to install</p>\n\n<ul>\n<li><a href=\"https://docs.continue.dev/quickstart\">Continue</a> for <a href=\"https://marketplace.visualstudio.com/items?itemName=Continue.continue\">VS Code</a> or <a href=\"https://plugins.jetbrains.com/plugin/22707-continue\">JetBrains</a></li>\n<li><a href=\"https://github.com/ollama/ollama?tab=readme-ov-file#quickstart\">Ollama</a> for <a href=\"https://ollama.com/download/mac\">macOS</a>, <a href=\"https://ollama.com/download/linux\">Linux</a>, or <a href=\"https://ollama.com/download/windows\">Windows</a></li>\n</ul>\n\n<p>Once you have them downloaded, <strong>here&rsquo;s what we recommend exploring:</strong></p>\n\n<h3>Try out Mistral AI&rsquo;s Codestral 22B model for autocomplete and chat</h3>\n\n<p>As of the now, <a href=\"https://mistral.ai/news/codestral/\">Codestral</a> is our current favorite model capable of both autocomplete and chat. This model demonstrates how LLMs have improved for programming tasks. However, with 22B parameters and a <a href=\"https://mistral.ai/news/mistral-ai-non-production-license-mnpl/\">non-production license</a>, it requires quite a bit of VRAM and can only be used for research and testing purposes, so it might not be the best fit for daily local usage.</p>\n\n<p>a. Download and run Codestral in your terminal by running</p>\n\n<pre><code>ollama run codestral\n</code></pre>\n\n<p>b. Click on the gear icon in the bottom right corner of Continue to open your <code>config.json</code> and add</p>\n\n<pre><code class=\"language-json\">{\n  &quot;models&quot;: [\n    {\n      &quot;title&quot;: &quot;Codestral&quot;,\n      &quot;provider&quot;: &quot;ollama&quot;,\n      &quot;model&quot;: &quot;codestral&quot;\n    }\n  ],\n  &quot;tabAutocompleteModel&quot;: {\n    &quot;title&quot;: &quot;Codestral&quot;,\n    &quot;provider&quot;: &quot;ollama&quot;,\n    &quot;model&quot;: &quot;codestral&quot;\n  }\n}\n</code></pre>\n\n<p><img src=\"/public/blog/continue-settings-vscode.png\" alt=\"VS Code settings to change config.json\" /></p>\n\n<h3>Use DeepSeek Coder 6.7B for autocomplete and Llama 3 8B for chat</h3>\n\n<p>Depending on how much VRAM you have on your machine, you might be able to take advantage of Ollama&rsquo;s ability to run multiple models and handle multiple concurrent requests by using <a href=\"https://deepseekcoder.github.io/\">DeepSeek Coder 6.7B</a> for autocomplete and <a href=\"https://ai.meta.com/blog/meta-llama-3/\">Llama 3 8B</a> for chat. If your machine can&rsquo;t handle both at the same time, then try each of them and decide whether you prefer a local autocomplete or a local chat experience. You can then <a href=\"https://docs.continue.dev/setup/select-provider\">use a remotely hosted or SaaS model</a> for the other experience.</p>\n\n<p>a. Download and run DeepSeek Coder 6.7B in your terminal by running</p>\n\n<pre><code>ollama run deepseek-coder:6.7b-base\n</code></pre>\n\n<p>b. Download and run Llama 3 8B in another terminal window by running</p>\n\n<pre><code>ollama run llama3:8b\n</code></pre>\n\n<p>c. Click on the gear icon in the bottom right corner of Continue to open your <code>config.json</code> and add</p>\n\n<pre><code class=\"language-json\">{\n  &quot;models&quot;: [\n    {\n      &quot;title&quot;: &quot;Llama 3 8B&quot;,\n      &quot;provider&quot;: &quot;ollama&quot;,\n      &quot;model&quot;: &quot;llama3:8b&quot;\n    }\n  ],\n  &quot;tabAutocompleteModel&quot;: {\n    &quot;title&quot;: &quot;DeepSeek Coder 6.7B&quot;,\n    &quot;provider&quot;: &quot;ollama&quot;,\n    &quot;model&quot;: &quot;deepseek-coder:6.7b-base&quot;\n  }\n}\n</code></pre>\n\n<h3>Use <code>nomic-embed-text</code> embeddings with Ollama to power <code>@codebase</code></h3>\n\n<p>Continue comes with an <a href=\"https://docs.continue.dev/customization/context-providers#codebase-retrieval\">@codebase</a> context provider built-in, which lets you automatically retrieve the most relevant snippets from your codebase. Assuming you have a chat model set up already (e.g. Codestral, Llama 3), you can keep this entire experience local thanks to embeddings with Ollama and <a href=\"https://blog.lancedb.com/lancedb-x-continue/\">LanceDB</a>. As of now, we recommend using <code>nomic-embed-text</code> embeddings.</p>\n\n<p>a. Download <code>nomic-embed-text</code> in your terminal by running</p>\n\n<pre><code>ollama pull nomic-embed-text\n</code></pre>\n\n<p>b. Click on the gear icon on the bottom right corner of Continue to open your <code>config.json</code> and add</p>\n\n<pre><code class=\"language-json\">{\n  &quot;embeddingsProvider&quot;: {\n    &quot;provider&quot;: &quot;ollama&quot;,\n    &quot;model&quot;: &quot;nomic-embed-text&quot;\n  }\n}\n</code></pre>\n\n<p>c. Depending on the size of your codebase, it might take some time to index and then you can ask it questions with important codebase sections automatically being found and used in the answer (e.g. &ldquo;@codebase what is the default context length for Llama 3?&rdquo;)</p>\n\n<h3>Fine-tune StarCoder 2 on your development data and push it to the Ollama model library</h3>\n\n<p>When you use Continue, you automatically generate data on how you build software. By default, this <a href=\"https://docs.continue.dev/development-data\">development data</a> is saved to <code>.continue/dev_data</code> on your local machine. When combined with the code that you ultimately commit, it can be used to improve the LLM that you or your team use (if you allow). For example, you can use accepted autocomplete suggestions from your team to fine-tune a model like StarCoder 2 to give you better suggestions.</p>\n\n<p>a. <a href=\"https://github.com/dlt-hub/continue-dlt-demo/blob/main/continue-hf-pipeline.py\">Extract and load the &ldquo;accepted tab suggestions&rdquo; into Hugging Face Datasets</a></p>\n\n<p>b. <a href=\"https://colab.research.google.com/drive/1jjb14BDlEeGjRmeXnfm41gDBlTNvsscn\">Use Hugging Face Supervised Fine-tuning Trainer to fine-tune StarCoder 2</a></p>\n\n<p>c. <a href=\"https://ollama.com/oakela/starcoder2_continue\">Push the model to the Ollama model library for your team to use and measure how your acceptance rate changes</a></p>\n\n<h3>Learn more about Ollama by using <code>@docs</code> to ask questions with the help of Continue</h3>\n\n<p>Continue also comes with an <a href=\"https://docs.continue.dev/customization/context-providers#documentation\"><code>@docs</code></a> context provider built-in, which lets you index and retrieve snippets from any documentation site. Assuming you have a chat model set up already (e.g. Codestral, Llama 3), you can keep this entire experience local by providing a link to the Ollama README on GitHub and asking questions to learn more with it as context.</p>\n\n<p>a. Type <code>@docs</code> in the chat sidebar, select &ldquo;Add Docs&rdquo;, copy and paste &ldquo;<a href=\"https://github.com/ollama/ollama&quot;\">https://github.com/ollama/ollama&rdquo;</a> into the URL field, and type &ldquo;Ollama&rdquo; into the title field</p>\n\n<p>b. It should quickly index the Ollama README and then you can ask it questions with important sections automatically being found and used in the answer (e.g. &ldquo;@Ollama how do I run Llama 3?&rdquo;)</p>\n\n<h2>Join our Discord!</h2>\n\n<p>Now that you have tried these different explorations, you should hopefully have a much better sense of what is the best way for you to use Continue and Ollama. If you ran into problems along the way or have questions, join the <a href=\"https://discord.com/invite/EfJEfdFnDQ\">Continue Discord</a> or the <a href=\"https://discord.com/invite/ollama\">Ollama Discord</a> to get some help and answers.</p>\n\n      </section>\n    </main>\n    \n<footer class=\"hidden md:flex inset-x-0 bottom-0 bg-white underline-offset-4 z-50\">\n  <div class=\"w-full px-6 py-3.5 flex items-center justify-between\">\n    <div class=\"text-xs text-neutral-500\">© 2024 Ollama</div>\n    <div class=\"flex space-x-6 text-xs text-neutral-500\">\n      <a href=\"/blog\" class=\"hover:underline\">Blog</a>\n      <a href=\"https://github.com/ollama/ollama/tree/main/docs\" class=\"hover:underline\">Docs</a>\n      <a href=\"https://github.com/ollama/ollama\" class=\"hover:underline\">GitHub</a>\n      <a href=\"https://discord.com/invite/ollama\" class=\"hover:underline\">Discord</a>\n      <a href=\"https://twitter.com/ollama\" class=\"hover:underline\">X (Twitter)</a>\n      <a href=\"https://lu.ma/ollama\" class=\"hover:underline\">Meetups</a>\n    </div>\n  </div>\n</footer>\n<footer class=\"inset-x-0 flex bottom-0 md:hidden py-4 bg-white\">\n  <div class=\"flex flex-col items-center justify-center w-full\">\n    <ul class=\"flex flex-wrap justify-center items-center text-sm text-neutral-500\">\n      <li class=\"mx-2 my-1\"><a href=\"/blog\" class=\"hover:underline\">Blog</a></li>\n      <li class=\"mx-2 my-1\"><a href=\"https://github.com/ollama/ollama/tree/main/docs\" class=\"hover:underline\">Docs</a></li>\n      <li class=\"mx-2 my-1\"><a href=\"https://github.com/ollama/ollama\" class=\"hover:underline\">GitHub</a></li>\n    </ul>\n    <ul class=\"flex flex-wrap justify-center items-center text-sm text-neutral-500\">\n      <li class=\"mx-2 my-1\"><a href=\"https://discord.com/invite/ollama\" class=\"hover:underline\">Discord</a></li>\n      <li class=\"mx-2 my-1\"><a href=\"https://twitter.com/ollama\" class=\"hover:underline\">X (Twitter)</a></li>\n      <li class=\"mx-2 my-1\"><a href=\"https://lu.ma/ollama\" class=\"hover:underline\">Meetups</a></li>\n    </ul>\n    <div class=\"flex items-center justify-center mt-2 text-neutral-500 text-sm\">\n      © 2024 Ollama\n    </div>\n  </div>\n</footer>\n\n  </body>\n  <script src=\"/public/vendor/highlight/highlight.min.js\"></script>\n  <script>hljs.configure({languages:[]});hljs.initHighlightingOnLoad();</script>\n</html>\n","oembed":false,"readabilityObject":{"title":"An entirely open-source AI code assistant inside your editor · Ollama Blog","content":"<div id=\"readability-page-1\" class=\"page\"><section>\n        <blockquote>\n<p>This is a guest post from Ty Dunn, Co-founder of Continue, that covers how to set up, explore, and figure out the best way to use Continue and Ollama together.</p>\n</blockquote>\n\n<p><img src=\"/public/blog/ollama-continue.png\" alt=\"Continue and Ollama\"></p>\n\n<p><a href=\"https://continue.dev/\">Continue</a> enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs. All this can run entirely on your own laptop or have Ollama deployed on a server to remotely power code completion and chat experiences based on your needs.</p>\n\n<p>To get set up, you’ll want to install</p>\n\n<ul>\n<li><a href=\"https://docs.continue.dev/quickstart\">Continue</a> for <a href=\"https://marketplace.visualstudio.com/items?itemName=Continue.continue\">VS Code</a> or <a href=\"https://plugins.jetbrains.com/plugin/22707-continue\">JetBrains</a></li>\n<li><a href=\"https://github.com/ollama/ollama?tab=readme-ov-file#quickstart\">Ollama</a> for <a href=\"https://ollama.com/download/mac\">macOS</a>, <a href=\"https://ollama.com/download/linux\">Linux</a>, or <a href=\"https://ollama.com/download/windows\">Windows</a></li>\n</ul>\n\n<p>Once you have them downloaded, <strong>here’s what we recommend exploring:</strong></p>\n\n<h3>Try out Mistral AI’s Codestral 22B model for autocomplete and chat</h3>\n\n<p>As of the now, <a href=\"https://mistral.ai/news/codestral/\">Codestral</a> is our current favorite model capable of both autocomplete and chat. This model demonstrates how LLMs have improved for programming tasks. However, with 22B parameters and a <a href=\"https://mistral.ai/news/mistral-ai-non-production-license-mnpl/\">non-production license</a>, it requires quite a bit of VRAM and can only be used for research and testing purposes, so it might not be the best fit for daily local usage.</p>\n\n<p>a. Download and run Codestral in your terminal by running</p>\n\n<pre><code>ollama run codestral\n</code></pre>\n\n<p>b. Click on the gear icon in the bottom right corner of Continue to open your <code>config.json</code> and add</p>\n\n<pre><code>{\n  \"models\": [\n    {\n      \"title\": \"Codestral\",\n      \"provider\": \"ollama\",\n      \"model\": \"codestral\"\n    }\n  ],\n  \"tabAutocompleteModel\": {\n    \"title\": \"Codestral\",\n    \"provider\": \"ollama\",\n    \"model\": \"codestral\"\n  }\n}\n</code></pre>\n\n<p><img src=\"/public/blog/continue-settings-vscode.png\" alt=\"VS Code settings to change config.json\"></p>\n\n<h3>Use DeepSeek Coder 6.7B for autocomplete and Llama 3 8B for chat</h3>\n\n<p>Depending on how much VRAM you have on your machine, you might be able to take advantage of Ollama’s ability to run multiple models and handle multiple concurrent requests by using <a href=\"https://deepseekcoder.github.io/\">DeepSeek Coder 6.7B</a> for autocomplete and <a href=\"https://ai.meta.com/blog/meta-llama-3/\">Llama 3 8B</a> for chat. If your machine can’t handle both at the same time, then try each of them and decide whether you prefer a local autocomplete or a local chat experience. You can then <a href=\"https://docs.continue.dev/setup/select-provider\">use a remotely hosted or SaaS model</a> for the other experience.</p>\n\n<p>a. Download and run DeepSeek Coder 6.7B in your terminal by running</p>\n\n<pre><code>ollama run deepseek-coder:6.7b-base\n</code></pre>\n\n<p>b. Download and run Llama 3 8B in another terminal window by running</p>\n\n<pre><code>ollama run llama3:8b\n</code></pre>\n\n<p>c. Click on the gear icon in the bottom right corner of Continue to open your <code>config.json</code> and add</p>\n\n<pre><code>{\n  \"models\": [\n    {\n      \"title\": \"Llama 3 8B\",\n      \"provider\": \"ollama\",\n      \"model\": \"llama3:8b\"\n    }\n  ],\n  \"tabAutocompleteModel\": {\n    \"title\": \"DeepSeek Coder 6.7B\",\n    \"provider\": \"ollama\",\n    \"model\": \"deepseek-coder:6.7b-base\"\n  }\n}\n</code></pre>\n\n<h3>Use <code>nomic-embed-text</code> embeddings with Ollama to power <code>@codebase</code></h3>\n\n<p>Continue comes with an <a href=\"https://docs.continue.dev/customization/context-providers#codebase-retrieval\">@codebase</a> context provider built-in, which lets you automatically retrieve the most relevant snippets from your codebase. Assuming you have a chat model set up already (e.g. Codestral, Llama 3), you can keep this entire experience local thanks to embeddings with Ollama and <a href=\"https://blog.lancedb.com/lancedb-x-continue/\">LanceDB</a>. As of now, we recommend using <code>nomic-embed-text</code> embeddings.</p>\n\n<p>a. Download <code>nomic-embed-text</code> in your terminal by running</p>\n\n<pre><code>ollama pull nomic-embed-text\n</code></pre>\n\n<p>b. Click on the gear icon on the bottom right corner of Continue to open your <code>config.json</code> and add</p>\n\n<pre><code>{\n  \"embeddingsProvider\": {\n    \"provider\": \"ollama\",\n    \"model\": \"nomic-embed-text\"\n  }\n}\n</code></pre>\n\n<p>c. Depending on the size of your codebase, it might take some time to index and then you can ask it questions with important codebase sections automatically being found and used in the answer (e.g. “@codebase what is the default context length for Llama 3?”)</p>\n\n<h3>Fine-tune StarCoder 2 on your development data and push it to the Ollama model library</h3>\n\n<p>When you use Continue, you automatically generate data on how you build software. By default, this <a href=\"https://docs.continue.dev/development-data\">development data</a> is saved to <code>.continue/dev_data</code> on your local machine. When combined with the code that you ultimately commit, it can be used to improve the LLM that you or your team use (if you allow). For example, you can use accepted autocomplete suggestions from your team to fine-tune a model like StarCoder 2 to give you better suggestions.</p>\n\n<p>a. <a href=\"https://github.com/dlt-hub/continue-dlt-demo/blob/main/continue-hf-pipeline.py\">Extract and load the “accepted tab suggestions” into Hugging Face Datasets</a></p>\n\n<p>b. <a href=\"https://colab.research.google.com/drive/1jjb14BDlEeGjRmeXnfm41gDBlTNvsscn\">Use Hugging Face Supervised Fine-tuning Trainer to fine-tune StarCoder 2</a></p>\n\n<p>c. <a href=\"https://ollama.com/oakela/starcoder2_continue\">Push the model to the Ollama model library for your team to use and measure how your acceptance rate changes</a></p>\n\n<h3>Learn more about Ollama by using <code>@docs</code> to ask questions with the help of Continue</h3>\n\n<p>Continue also comes with an <a href=\"https://docs.continue.dev/customization/context-providers#documentation\"><code>@docs</code></a> context provider built-in, which lets you index and retrieve snippets from any documentation site. Assuming you have a chat model set up already (e.g. Codestral, Llama 3), you can keep this entire experience local by providing a link to the Ollama README on GitHub and asking questions to learn more with it as context.</p>\n\n<p>a. Type <code>@docs</code> in the chat sidebar, select “Add Docs”, copy and paste “<a href=\"https://github.com/ollama/ollama%22\">https://github.com/ollama/ollama”</a> into the URL field, and type “Ollama” into the title field</p>\n\n<p>b. It should quickly index the Ollama README and then you can ask it questions with important sections automatically being found and used in the answer (e.g. “@Ollama how do I run Llama 3?”)</p>\n\n<h2>Join our Discord!</h2>\n\n<p>Now that you have tried these different explorations, you should hopefully have a much better sense of what is the best way for you to use Continue and Ollama. If you ran into problems along the way or have questions, join the <a href=\"https://discord.com/invite/EfJEfdFnDQ\">Continue Discord</a> or the <a href=\"https://discord.com/invite/ollama\">Ollama Discord</a> to get some help and answers.</p>\n\n      </section></div>","textContent":"\n        \nThis is a guest post from Ty Dunn, Co-founder of Continue, that covers how to set up, explore, and figure out the best way to use Continue and Ollama together.\n\n\n\n\nContinue enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs. All this can run entirely on your own laptop or have Ollama deployed on a server to remotely power code completion and chat experiences based on your needs.\n\nTo get set up, you’ll want to install\n\n\nContinue for VS Code or JetBrains\nOllama for macOS, Linux, or Windows\n\n\nOnce you have them downloaded, here’s what we recommend exploring:\n\nTry out Mistral AI’s Codestral 22B model for autocomplete and chat\n\nAs of the now, Codestral is our current favorite model capable of both autocomplete and chat. This model demonstrates how LLMs have improved for programming tasks. However, with 22B parameters and a non-production license, it requires quite a bit of VRAM and can only be used for research and testing purposes, so it might not be the best fit for daily local usage.\n\na. Download and run Codestral in your terminal by running\n\nollama run codestral\n\n\nb. Click on the gear icon in the bottom right corner of Continue to open your config.json and add\n\n{\n  \"models\": [\n    {\n      \"title\": \"Codestral\",\n      \"provider\": \"ollama\",\n      \"model\": \"codestral\"\n    }\n  ],\n  \"tabAutocompleteModel\": {\n    \"title\": \"Codestral\",\n    \"provider\": \"ollama\",\n    \"model\": \"codestral\"\n  }\n}\n\n\n\n\nUse DeepSeek Coder 6.7B for autocomplete and Llama 3 8B for chat\n\nDepending on how much VRAM you have on your machine, you might be able to take advantage of Ollama’s ability to run multiple models and handle multiple concurrent requests by using DeepSeek Coder 6.7B for autocomplete and Llama 3 8B for chat. If your machine can’t handle both at the same time, then try each of them and decide whether you prefer a local autocomplete or a local chat experience. You can then use a remotely hosted or SaaS model for the other experience.\n\na. Download and run DeepSeek Coder 6.7B in your terminal by running\n\nollama run deepseek-coder:6.7b-base\n\n\nb. Download and run Llama 3 8B in another terminal window by running\n\nollama run llama3:8b\n\n\nc. Click on the gear icon in the bottom right corner of Continue to open your config.json and add\n\n{\n  \"models\": [\n    {\n      \"title\": \"Llama 3 8B\",\n      \"provider\": \"ollama\",\n      \"model\": \"llama3:8b\"\n    }\n  ],\n  \"tabAutocompleteModel\": {\n    \"title\": \"DeepSeek Coder 6.7B\",\n    \"provider\": \"ollama\",\n    \"model\": \"deepseek-coder:6.7b-base\"\n  }\n}\n\n\nUse nomic-embed-text embeddings with Ollama to power @codebase\n\nContinue comes with an @codebase context provider built-in, which lets you automatically retrieve the most relevant snippets from your codebase. Assuming you have a chat model set up already (e.g. Codestral, Llama 3), you can keep this entire experience local thanks to embeddings with Ollama and LanceDB. As of now, we recommend using nomic-embed-text embeddings.\n\na. Download nomic-embed-text in your terminal by running\n\nollama pull nomic-embed-text\n\n\nb. Click on the gear icon on the bottom right corner of Continue to open your config.json and add\n\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"ollama\",\n    \"model\": \"nomic-embed-text\"\n  }\n}\n\n\nc. Depending on the size of your codebase, it might take some time to index and then you can ask it questions with important codebase sections automatically being found and used in the answer (e.g. “@codebase what is the default context length for Llama 3?”)\n\nFine-tune StarCoder 2 on your development data and push it to the Ollama model library\n\nWhen you use Continue, you automatically generate data on how you build software. By default, this development data is saved to .continue/dev_data on your local machine. When combined with the code that you ultimately commit, it can be used to improve the LLM that you or your team use (if you allow). For example, you can use accepted autocomplete suggestions from your team to fine-tune a model like StarCoder 2 to give you better suggestions.\n\na. Extract and load the “accepted tab suggestions” into Hugging Face Datasets\n\nb. Use Hugging Face Supervised Fine-tuning Trainer to fine-tune StarCoder 2\n\nc. Push the model to the Ollama model library for your team to use and measure how your acceptance rate changes\n\nLearn more about Ollama by using @docs to ask questions with the help of Continue\n\nContinue also comes with an @docs context provider built-in, which lets you index and retrieve snippets from any documentation site. Assuming you have a chat model set up already (e.g. Codestral, Llama 3), you can keep this entire experience local by providing a link to the Ollama README on GitHub and asking questions to learn more with it as context.\n\na. Type @docs in the chat sidebar, select “Add Docs”, copy and paste “https://github.com/ollama/ollama” into the URL field, and type “Ollama” into the title field\n\nb. It should quickly index the Ollama README and then you can ask it questions with important sections automatically being found and used in the answer (e.g. “@Ollama how do I run Llama 3?”)\n\nJoin our Discord!\n\nNow that you have tried these different explorations, you should hopefully have a much better sense of what is the best way for you to use Continue and Ollama. If you ran into problems along the way or have questions, join the Continue Discord or the Ollama Discord to get some help and answers.\n\n      ","length":5479,"excerpt":"Continue enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs.","byline":null,"dir":null,"siteName":null,"lang":null},"finalizedMeta":{"title":"An entirely open-source AI code assistant inside your editor · Ollama Blog","description":"Continue enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs.","author":false,"creator":"","publisher":false,"date":"2024-07-10T15:32:44.083Z","topics":[]},"jsonLd":{"@type":"WebSite","headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"@context":"https://schema.org","name":"Ollama","url":"https://ollama.com"},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"An entirely open-source AI code assistant inside your editor · Ollama Blog","description":"Continue enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs.","canonical":"https://ollama.com/blog/continue-code-assistant","keywords":[],"image":"/public/ollama.png","firstParagraph":"This is a guest post from Ty Dunn, Co-founder of Continue, that covers how to set up, explore, and figure out the best way to use Continue and Ollama together."},"dublinCore":{},"opengraph":{"title":"An entirely open-source AI code assistant inside your editor · Ollama Blog","description":"Continue enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs.","url":"https://ollama.com/public/","site_name":false,"locale":false,"type":["website","website"],"typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":["https://ollama.com/public/og.png","https://ollama.com/public/og.png"],"image:type":["image/png","image/png"],"image:width":["1200","1200"],"image:height":["628","628"]},"twitter":{"site":false,"description":false,"card":false,"creator":false,"title":false,"image":false},"archivedData":{"link":false,"wayback":false}}}