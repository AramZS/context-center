{"initialLink":"https://wiby.me/about/guide.html","sanitizedLink":"https://wiby.me/about/guide.html","finalLink":"https://wiby.me/about/guide.html","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://wiby.me/about/guide.html\" itemprop=\"url\">Build your own Search Engine</a></contexter-box-head></contexter-box-head><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2022-11-07T05:32:19.794Z\">10/7/2022</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>The source code and instructions to create your own version of Wiby.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a href=\"https://web.archive.org/web/20221102202830/https://wiby.me/about/guide.html\" is=\"contexter-link\" target=\"_blank\" rel=\"timemap\" class=\"read-link archive-link\" itemprop=\"archivedAt\" slot=\"archive-link\">Archived</a><a is=\"contexter-link\" href=\"https://wiby.me/about/guide.html\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"263813aa358573bb4dcd35d7dde6d7f9e4c01579","data":{"originalLink":"https://wiby.me/about/guide.html","sanitizedLink":"https://wiby.me/about/guide.html","canonical":"https://wiby.me/about/guide.html","htmlText":"<!DOCTYPE html>    \r\n<html lang=\"en\">    \r\n<head>    \r\n<title>Build your own Search Engine</title>\r\n<style> \r\nh1 { color: #062DA1; }\r\nbody { background-color: #FAFAFA; }\r\na:link { COLOR: #062DA1;  font-size: 17px; text-decoration: none;}\r\na:visited { COLOR: #7900A7; }\r\na.tlink:visited { COLOR: #515151; }\r\na.title:link { COLOR: #7900A7; font-weight: bold; font-size: 29px;  text-decoration: none; font-family: \"Georgia\"; }\r\na.tiny { COLOR: #7900A7; font-size: 17px; }\r\na.pin1 { font-size:14px; COLOR: #7900A7; }\r\na.tlink { font-size: 21px; COLOR: #062DA1 }\r\na.more { font-size: 21px; COLOR: #7900A7 }\r\nh1 { margin:0px; line-height:96px; }\r\np { font-size:17px; margin-bottom:0px; margin-top:0px; }\r\n.titlep { COLOR: #7900A7; font-weight: bold; font-size: 83px; font-family: \"Georgia\"; }\r\n.url { font-size:15px; color: #3a5a0c; }\r\n.pin { font-size:14px; COLOR: #2e2e2e;}\r\ntextarea:focus, input:focus{ outline: none;}\r\nblockquote { width: 700px; }\r\npre { width:700px; white-space: pre-wrap; word-wrap: break-word; }\r\n</style> \r\n<meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\"/> \r\n<meta name=\"description\" content=\"The source code and instructions to create your own version of Wiby.\"/> \r\n</head>        \r\n<body> \r\n<blockquote>   \r\n<h1 align=\"center\">Build Your Own Search Engine</h1>\r\n<div align=\"center\">(Wiby Install Guide)</div>\r\n<br>\r\n<p>\r\n<a href=\"guide.html#overview\">Overview</a>\r\n<br>\r\n<a href=\"guide.html#install\">Installation</a>\r\n<br>\r\n<a href=\"guide.html#control\">Controlling</a>\r\n<br>\r\n<a href=\"guide.html#scale\">Scaling</a>\r\n\r\n<h2><a name=\"overview\">Overview</a></h2>\r\nWiby is a search engine for the World Wide Web. The source code is now free as of July 8, 2022 under the GPLv2 license. I have been longing for this day! You can watch a quick demo <a href=\"https://youtu.be/nCfWJqNBqHo\">here</a>.\r\n<br>\r\n<br>\r\nIt includes a web interface allowing guardians to control where, how far, and how often it crawls websites and follows hyperlinks. The search index is stored inside of a MySQL full-text index.\r\n<br>\r\n<br>\r\nFast queries are maintained by concurrently reading different sections of the index across multiple replication servers or across duplicate server connections, returning a list of top results from each connection, \r\nthen searching the combined list to ensure correct ordering. Replicas that fail are automatically excluded; new replicas are easy to include. \r\nAs new pages are crawled, they are stored randomly across the index, ensuring each replica can obtain relevant results.<br>\r\n<br>\r\nThe search engine is not meant to index the entire web and then sort it with a ranking algorithm. \r\nIt prefers to seed its index through human submissions made by guests, or by the guardian(s) of the search engine. \r\n<br>\r\n<br>\r\nThe software is designed for anyone with some extra computers (even a Pi), to host their own search engine catering to whatever niche matters to them. The search engine includes a simple API \r\nfor meta search engines to harness.\r\n<br>\r\n<br>\r\nI hope this will enable anyone with a love of computers to cheaply build and maintain a search engine of their own. \r\nI hope it can cultivate free and independent search engines, ensuring accessibility of ideas and information across the World Wide Web.\r\n<br>\r\n<br>\r\n<pre>\r\n\r\n       Web Traffic\r\n            |\r\n            |\r\n+-----------+-----------+\r\n| Reverse Proxy (nginx) |\r\n+-----------+-----------+\r\n            |\r\n            |\r\n+-----------+-----------+\r\n|  Wiby Core Server(s)  |+-----------------+----------------------------+\r\n|(Golang or PHP version)|                  |                            |\r\n+-----------+-----------+       +----------+----------+       +---------+---------+\r\n            |                   |Replication Databases|+-----+|Replication Tracker|\r\n            |                   +----------+----------+       +-------------------+\r\n+-----------+-----------+                  |\r\n|    Primary Database   |+-----------------+\r\n|   (MySQL or MariaDB)  |\r\n+----+-------------+----+\r\n     |             |  \r\n     |             |  \r\n+----+-----+  +----+----+\r\n|   Web    |  | Refresh |\r\n|Crawler(s)|  |Scheduler|\r\n+----------+  +---------+\r\n</pre>\r\n<br>\r\n<hr>\r\n<h2><a name=\"install\">Installation</a></h2>\r\nI can only provide manual install instructions at this time. \r\n<br>\r\n<br>\r\nNote that while the software is functionally complete, it is still in beta. Anticipate that some bugs will be discovered now that the source is released. \r\nEnsure that you isolate the search engine from your other important services, and if you are running parts of it out of your home, keep the servers \r\non a separate VLAN. Make sure this VLAN cannot access your router or switch interface. Continue this practise even when the software reaches \"1.0\".\r\n<br>\r\n<br>\r\nIf you have created a \"LAMP\", or rather a \"LEMP\" server before, this isn't much more complicated. If you've never done that, I suggest you find a \"LEMP\" tutorial.\r\n<br><br>\r\n<h3>Build a LEMP server</h3>\r\nDigital Ocean tutorials are usually pretty good so here is a link to one for <a href=\"https://www.digitalocean.com/community/tutorials/how-to-install-linux-nginx-mysql-php-lemp-stack-on-ubuntu-20-04\">Ubuntu 20</a> and \r\n<a href=\"https://www.digitalocean.com/community/tutorials/how-to-install-linux-nginx-mysql-php-lemp-stack-on-ubuntu-22-04\">Ubuntu 22</a>.\r\n<br>\r\n<br>\r\nFor the sake of simplicity, assume all instructions are for Ubuntu 20 or 22. If you are on a different distro, modify the install steps accordingly to suit your distro.\r\n<br>\r\n<br>\r\nIf you don't have a physical server, you can rent computing space by looking for a \"VPS provider\". This virtual computer will be your reverse proxy, and if you want, it can host everything else too.\r\n<br>\r\n<br>\r\n<h3>Install the following additional packages:</h3>\r\n<pre>apt install build-essential php-gd libcurl4-openssl-dev libmysqlclient-dev mysql-server golang git\r\n\r\nFor Ubuntu 20:\r\ngo get -u github.com/go-sql-driver/mysql\r\n\r\nFor Ubuntu 22 or latest Golang versions:\r\ngo install github.com/go-sql-driver/mysql@latest\r\ngo mod init mysql\r\ngo get github.com/go-sql-driver/mysql\r\n</pre>\r\n<br>\r\n<h3>Get Wiby Source Files</h3>\r\nDownload the source directly from Wiby <a href=\"/download/wibysource.zip\">here</a>, or from <a href=\"https://github.com/wibyweb/wiby/\">GitHub</a>. The source is released under the GPLv2 license. Copy the source files for Wiby to your server.\r\n<br>\r\n<br>\r\n\r\n<h3>Compile the crawler (cr), refresh scheduler (rs), replication tracker (rt):</h3>\r\n<pre>\r\ngcc cr.c -o cr -I/usr/include/mysql -lmysqlclient -lcurl -std=c99 -O3\r\ngcc rs.c -o rs -I/usr/include/mysql -lmysqlclient -std=c99 -O3\r\ngcc rt.c -o rt -I/usr/include/mysql -lmysqlclient -std=c99 -O3\r\n</pre>\r\nIf you get any compile errors, it is likely due to the path of the mysql or libcurl header files. \r\nThis could happen if you are not using Ubuntu 20. You might have to locate the correct path for curl.h, easy.h, and mysql.h.\r\n<br>\r\n<br>\r\n<h3>Build the core server application:</h3>\r\n<pre>\r\nInside the go folder:\r\n\r\ngo build core.go\r\ngo build 1core.go\r\n</pre>\r\nIf you are just starting out, you can use '1core'. If you are going to setup replication servers or you are using a computer with a lot of available cores, you can use 'core', but make sure to read the scaling section. \r\nYou can also use index.php in the root of the www directory and not use the Go version at all. Though the PHP version is used mainly for prototyping.\r\n<br>\r\n<br>\r\n<h3>Build the database:</h3>\r\nMake sure these lines are inside of /etc/mysql/my.cnf, then restart mysql\r\n<pre>\r\n[client]\r\ndefault-character-set=utf8mb4\r\n\r\n[mysql]\r\ndefault-character-set = utf8mb4\r\n\r\n[mysqld]\r\nmax_connections = 400\r\nft_min_word_len=2\r\nsql_mode = \"NO_BACKSLASH_ESCAPES\"\r\ncharacter-set-server = utf8mb4\r\ncollation-server = utf8mb4_0900_ai_ci\r\nskip-character-set-client-handshake\r\ndefault-authentication-plugin=mysql_native_password\r\nwait_timeout = 800\r\n\r\n#memory use settings, you should adjust this based on your hardware\r\ninnodb_buffer_pool_size = 1342177280\r\ninnodb_buffer_pool_instances = 2\r\n\r\n</pre>\r\nLogin to MySQL and type:\r\n<pre>\r\ncreate database wiby;\r\ncreate database wibytemp;\r\n</pre>\r\nImport the wiby and wibytemp database files:\r\n<pre>\r\nmysql -u root -p wiby < wiby.sql\r\nmysql -u root -p wibytemp < wibytemp.sql\r\n</pre>\r\nLogin to MySQL, create the following accounts and give them the correct access:\r\n<pre>\r\ncreate user 'guest'@'localhost' identified by 'qwer';\r\ncreate user 'approver'@'localhost' identified by 'foobar';\r\ncreate user 'crawler'@'localhost' identified by 'seekout';\r\nuse wiby;\r\ngrant select on accounts to 'approver'@'localhost';\r\ngrant select on reviewqueue to 'approver'@'localhost';\r\ngrant insert on indexqueue to 'approver'@'localhost';\r\ngrant delete on reviewqueue to 'approver'@'localhost';\r\ngrant update on reviewqueue to 'approver'@'localhost';\r\ngrant select on indexqueue to 'crawler'@'localhost';\r\ngrant insert on windex to 'crawler'@'localhost';\r\ngrant insert on indexqueue to 'crawler'@'localhost';\r\ngrant update on windex to 'crawler'@'localhost';\r\ngrant delete on indexqueue to 'crawler'@'localhost';\r\ngrant delete on windex to 'crawler'@'localhost';\r\ngrant select on windex to 'crawler'@'localhost';\r\ngrant insert on reviewqueue to 'crawler'@'localhost';\r\ngrant select on windex to 'guest'@'localhost';\r\ngrant insert on reviewqueue to 'guest'@'localhost';\r\ngrant insert on feedback to 'guest'@'localhost';\r\ngrant select on feedback to 'approver'@'localhost';\r\ngrant delete on feedback to 'approver'@'localhost';\r\ngrant insert on graveyard to 'approver'@'localhost';\r\ngrant update on graveyard to 'approver'@'localhost';\r\ngrant delete on graveyard to 'approver'@'localhost';\r\ngrant select on graveyard to 'approver'@'localhost';\r\ngrant update on accounts to 'approver'@'localhost';\r\ngrant insert on accounts to 'approver'@'localhost';\r\ngrant delete on accounts to 'approver'@'localhost';\r\nuse wibytemp;\r\ngrant select on titlecheck to 'crawler'@'localhost';\r\ngrant insert on titlecheck to 'crawler'@'localhost';\r\ngrant delete on titlecheck to 'crawler'@'localhost';\r\ngrant select on rejected to 'approver'@'localhost';\r\ngrant insert on rejected to 'approver'@'localhost';\r\ngrant delete on rejected to 'approver'@'localhost';\r\ngrant select on reserve_id to 'crawler'@'localhost';\r\ngrant insert on reserve_id to 'crawler'@'localhost';\r\ngrant delete on reserve_id to 'crawler'@'localhost';\r\nFLUSH PRIVILEGES;\r\n</pre>\r\n<h3>Copy the HTML files and PHP scripts to your web server</h3>\r\n<pre>Copy the contents of the the html directory into the nginx html directory (/var/www/html)</pre>\r\n\r\n<h3>Configure nginx for Wiby</h3>\r\nIn /etc/nginx/, create a directory called 'phpcache', and another one called 'cache'.\r\n<br>\r\nInstead of going through every detail, I will provide a template for you to try out as your default nginx config from inside /etc/nginx/sites-available/ of the source code.\r\n<br>\r\n<br>\r\nYou should learn nginx configuration on your own, this template is just to assist. \r\nIf you are using only the php version, comment all \"core app\" location entries to revert Wiby search to the php only version.\r\n<br>\r\nMake sure ssl_certificate and ssl_certificate_key have the path for your SSL files instead of the example paths. If you don't want to use SSL, just remove the server {} configuration for SSL connections (on port 443). \r\nAlso the example file references php7.4-fpm.sock, so if you are using a different version remember to update that as well (such as php8.1-fpm.sock on Ubuntu 22).\r\n<br>\r\n<br>\r\n<h3>Start the Refresh Scheduler</h3>\r\nThis program (rs) will make sure all pages indexed are refreshed at least once per week (or sooner depending on how you assign updates to an individual website). \r\nYou may want to run this on startup, easiest way to set that is with a cron job (crontab -e). Run './rs -h' to get more parameters.\r\n<br>\r\n<br>\r\n<h3>Start the Crawler</h3>\r\nIt is best to run the crawler in a screen session so that you can monitor its output. You can have more than one crawler running as long as you keep them in separate directories, include a symlink to the same robots folder, and also set the correct parameters on each. \r\nTo view the parameters, type './cr -h'. Without any parameters set, you can only run one crawler (which is probably all you need anyway). If necessary, you can change the database connection from 'localhost' to a different IP from inside cr.c, then rebuild.\r\n<br>\r\n<br>\r\nNote that you may need to change the crawler's user-agent if you have issues indexing some websites. Pages that fail to index are noted inside of abandoned.txt.\r\n<br> \r\n<br>\r\nMake sure the robots folder exists. All robots.txt files are stored in the robots folder. They are downloaded once and then referenced from that folder on future updates. Clear this folder every few weeks to ensure robots.txt files get refreshed from time to time. \r\nYou can turn off checking for robots.txt files by commenting out the line calling the \"checkrobots\" function inside of cr.c.\r\n<br>\r\n<br>\r\nIf crawling through hyperlinks on a page, the following file types are accepted: html, htm, txt, php, asp. Links containing parameters are ignored. These limitations do not apply to pages directly submitted by people.\r\n<br>\r\n<br>\r\n<h3>Start the core server</h3>\r\nIf you are just starting out, '1core' or the php version is easiest to start with. Use 'core' if you want to scale computer resources as the index grows or if you have a lot of available CPU cores. Make sure to read the scaling section. \r\nYou can run the core server on startup with a cron job.\r\n<br>\r\n<br>\r\n<h3>Set Administrator Password for the Web Interface</h3>\r\nThere is no default web login, you will have to set this manually the first time:\r\n<pre>\r\nRename the /html/hash folder to something private.\r\n\r\nEdit html/private_folder_name/hashmake.php and change 'secretpassword' to your preferred admin password. \r\n\r\nAccess /private_folder_name/hashmake.php from your browser and copy down the hash.\r\n\r\nAfter you have copied it down, delete or remove hashmake.php from your web server folder so that the hash cannot be discovered.\r\n</pre>\r\nLogin to MySQL and create the account:\r\n<pre>\r\nuse wiby;\r\nINSERT INTO accounts (name,hash,level) VALUES('your_username','your_password_hash','admin');\r\n</pre>\r\nYou can now access /accounts/ from your browser, login to create and manage all accounts for administrators and guardians of the search engine.\r\n<br>\r\n<br>\r\n<b>admin</b> - Can access all web forms for the search engine and use the /accounts/ page to create and delete accounts.\r\n<br>\r\n<br>\r\n<b>guardian</b> - The main role of a guardian is to gatekeep the index of the search engine. Can access all forms except for /readf/, and can only use the /accounts/ page to change their own password.\r\n<br>\r\n<br>\r\n<br>\r\n<hr>\r\n<h2><a name=\"control\">Controlling the Search Engine</a></h2>\r\n<br>\r\nThere are several forms to control the search engine. There is no central form linking everything together, just a collection of different folders that you can rename if you want.\r\n<br>\r\n<br>\r\n<h3>/submit/</h3> This public facing form allows users of the search engine to submit websites for indexing, provided they comply with your submission criteria, which you can modify on /submit/form.html.php.\r\n<br>\r\n<br>\r\n<h3>/accounts/</h3>\r\nThis is the account management page. Admins have options to create, lock, change account type, delete, and reset passwords. Guardians have the option to change their password.\r\n<br>\r\n<br>\r\n<h3>/review/</h3> This is the most important form, intended for you to verify website submissions meet your criteria. Up to 10 pages are assigned to each guardian or admin that accesses the form. The pages will remain assigned to that account for up to 30 minutes. \r\nFrom here you can control how much, how deep, and how often the web crawler will access each submission. Here is an example of the available options for a website submission:\r\n<br>\r\n<br>\r\n<a href=\"url_that_was_submitted\">url_that_was_submitted</a>\r\n<br>\r\n[Worksafe<input type=\"checkbox\" id=\"worksafe\" name=\"worksafe\" checked=\"checked\">]\r\n\t    [Surprise<input type=\"checkbox\" id=\"surprise\" name=\"surprise\">]\r\n            [Skip<input type=\"checkbox\" id=\"skip\" name=\"skip\" >]\r\n            [Bury<input type=\"checkbox\" id=\"bury\" name=\"bury\" >]\r\n\t    [Deny<input type=\"checkbox\" id=\"deny\" name=\"deny\" >]\r\n\t    [Updatable<select id=\"updatable\" name=\"updatable\"> \r\n\t\t\t<option value=1>1 WEEK</option>\r\n\t\t\t<option value=2>1 DAY</option> \r\n\t\t\t<option value=3>12 HOUR</option>\r\n\t\t\t<option value=4>6 HOUR</option>\r\n\t\t\t<option value=5>3 HOUR</option>\r\n\t\t\t<option value=6>1 HOUR</option>\r\n\t\t      </select>]\r\n\t\t<br>\r\n\t\t[Crawl: Depth <input type=\"number\" id=\"crawldepth\" name=\"crawldepth\" >\r\n\t\tPages <input type=\"number\" id=\"crawlpages\" name=\"crawlpages\" >\r\n\t\tType <select id=\"crawltype\" name=\"crawltype\">\r\n\t\t\t<option value=0>Local</option>\r\n\t\t\t<option value=1>All</option>\r\n\t\t\t<option value=2>External</option>\r\n\t\t\t</select>\r\n\t\tEnforce Rules<input type=\"checkbox\" id=\"forcerules\" name=\"forcerules\" >\r\n\t\tRepeat<input type=\"checkbox\" id=\"crawlrepeat\" name=\"crawlrepeat\" >]\r\n<br>\r\n<br>\r\nExplanation of the above options:\r\n<br>\r\n<br>\r\n<b>Worksafe</b> - Indicates if the website is safe for work. Set by the user who submitted the website, however you can change it based on your determination.\r\n<br>\r\n<br>\r\n<b>Surprise</b> - Checking this box will put it in the \"surprise me\" feature, where users get redirected to random websites when they click \"surprise me\". Note that this feature won't show NSFW websites even if they are set to surprise.\r\n<br>\r\n<br>\r\n<b>Skip</b> - Selecting this option will skip indexing the page and it will reappear on the review form after you submit the rest of the pages for crawling.\r\n<br>\r\n<br>\r\n<b>Bury</b> - Selecting this will move the page to a grave yard (/grave/), a holding place with the same options as /review/ for websites that might have stopped working but that you suspect may come back online. The crawler will detect this automatically and send the page back into review. When you click on the link and see a 404, you can be assured the crawler sent it back to review after failing two update cycles. This also happens if the title of the page changes. The crawler will only do this for pages directly submitted by people. This curtesy is not given to websites that are automatically crawled but then fail to work later on. For those sites, after two failed update cycles, the page will be removed.\r\n<br>\r\n<br>\r\n<b>Deny</b> - Select this to drop the page from being indexed. If the page does not meet your submission criteria, this would be the option to remove it from the queue.\r\n<br>\r\n<br>\r\n<b>Updatable</b> - The update cycle for the web crawler to return to the page. This only applies to pages submitted by people, pages found by link crawling always go on a 1 week update cycle.\r\n<br>\r\n<br>\r\n<b>------------------- Crawl -------------------</b> \r\n<br>\r\nThe options listed below control how the crawler indexes hyperlinks on the website. By default, the crawler does not index any hyperlinks, it will only index the page that is submitted.\r\n<br>\r\n<br>\r\n<b>Depth</b>  - How many layers of links to crawl through. You must set at least a depth of 1 if you want to crawl any hyperlinks. Setting a negative value = no limit. Be careful about that.\r\n<br>\r\n<br>\r\n<b>Pages</b> - How many pages to crawl on each link layer (depth). They will be randomly selected. You must set at least 1 if you want to crawl any hyperlinks. Setting a negative value = no limit. Be careful about that.\r\n<br>\r\n<br>\r\n<b>Type</b> - Indicates if you want to only crawl links local to the website, or links external to the website, or both.\r\n<br>\r\n<br>\r\n<b>Enforce rules</b> - This is a blunt tool that checks if pages have more than two scripts and/or css files. If the limit is exceded, the page will not be indexed. I don't use it and prefer to manually check based on more forgiving criteria.\r\n<br>\r\n<br>\r\n<b>Repeat</b> - While the crawler will always return to update each page in the index, it wont crawl through hyperlinks again unless you tell it to. Even so, it only crawls hyperlinks on the page at a depth of 1 when repeat is selected.\r\n<br>\r\n<br>\r\n<h3>/ban/</h3>\r\nYou can delete or ban individual URL's from the index with this form. Its pretty simple as I don't use it much. You can't delete an entire domain with it, for that you can build your own query in the MySQL console.\r\n<br>\r\n<br>\r\n<h3>/bulksubmit/</h3>\r\nAdmins/Guardians can import a list of URLs into the review queue with this form.\r\n<br>\r\n<br>\r\n<h3>/feedback/</h3>\r\nUsers can submit feedback for you with this form.\r\n<br>\r\n<br>\r\n<h3>/readf/</h3>\r\nWhere admin accounts can read feedback submitted by users.\r\n<br>\r\n<br>\r\n<h3>/grave/</h3>\r\nIt has the same features as /review/. Websites that you don't yet want to index but don't want to forget about are stored inside /grave/ by selecting 'bury' from inside /review/. The web crawler will (only for pages submitted directly by people), move 404'd pages or pages where the title has changed back to /review/ after two update cycles \r\nwhere the page does not return to normal. So after a few weeks you may notice dead pages appearing in /review/, you can decide to drop the page or to bury it where it will be moved to /grave/. The page might go back to normal at some point and you can check /grave/ to see if it resurrects. \r\n<br>\r\n<br>\r\n<h3>/insert/</h3>\r\nThis was the first form created back in late 2016 to populate the Wiby index and see if the search engine could even work as a proof of concept. It was meant to manually enter pages into the index as no crawler existed yet. \r\nIt is still useful if you want to manually index a page that refuses to permit the crawler to access it. In that case, set updatable to 0.\r\n<br>\r\n<br>\r\n<h3>/tags/</h3>\r\nIf you want to force a website to appear at the top rank for specific single word queries (like \"weather\"), you can force it by tagging the words to the target url.\r\n<br>\r\n<br>\r\n<h3>/json/</h3>\r\nThis is the JSON API developers can use to connect their services to the search engine. Instructions are located at that location.\r\n<br>\r\n<br>\r\n<h3>Additional Notes</h3>\r\nIf you need to stop the web crawler in a situation where it was accidently queued to index an unlimited number of pages, first stop the crawler program, truncate the indexqueue table 'truncate indexqueue;', then restart the crawler.\r\n<br>\r\n<br>\r\n<br>\r\n<hr>\r\n<h2><a name=\"scale\">Scaling the Search Engine</a></h2>\r\n<br>\r\nYou can help ensure sub-second search queries as your index grows by building MySQL replica servers on a local network close to eachother, run the core application AND replication tracker (rt) on one or more replica servers and point your reverse proxy to use it. \r\nEdit the servers.csv file for rt to indicate all available replica servers. If you have a machine with a huge amount of resources and cores, entering multiple duplicate entries to the same sever inside servers.csv (e.g. one for each core) works also.\r\n<br>\r\n<br>\r\nThe core application checks the replication tracker (rt) output to determine if any replicas are online, it will initiate a connection on those replicas and task each one to search a different section of the index, \r\ndrastically speeding up search speeds especially for multi-word queries. By default, single-word queries will not initiate multiple connections across replicas. To enable that on single-word queries, comment out the IF statement \r\non line 373 and rebuild the core application.\r\n<br>\r\n<br>\r\nThe number of available replicas must divide evenly into the search results per page limit (lim), OR, the search results per page limit must divide evenly into the number of available replicas. If there \r\nis an excess of available replicas such that they do not divide evenly, those will remain in synch but will not be used for searches unless another replica fails. You can adjust the search results per page limit (lim) to a different value (default 12), \r\nand then rebuild to make excess available replicas divide evenly (if necessary).\r\n<br>\r\n<br>\r\nThe reverse proxy and replica servers can be connected through a VPN such as wireguard or openvpn, however the IPs for servers.csv should be the local IPs for the LAN \r\nthe replicas are all connected on. <a href=\"https://www.digitalocean.com/community/tutorials/how-to-set-up-replication-in-mysql\">Here</a> is a tutorial for setting up MySQL replicas.\r\n<br><br>\r\n<b>Full instructions below:</b>\r\n<br>\r\n<br>\r\nOn the primary server add these lines to my.cnf under [mysqld] but only once you have a VPN to reach your replicas. Replace my.vpn.ip with your own.\r\n<pre>\r\n#setting up replication below\r\nbind-address = 127.0.0.1,my.vpn.ip\r\nserver-id = 1\r\nlog_bin = /var/log/mysql/mysql-bin.log\r\nbinlog_do_db = wiby\r\nbinlog_format = mixed\r\n</pre>\r\nIn MySQL on the primary server, create a user for replica access:\r\n<pre>\r\ncreate user 'slave_user'@'%' identified by 'd0gemuchw0w';\r\nGRANT REPLICATION SLAVE ON *.* TO 'slave_user'@'%';\r\nFLUSH PRIVILEGES;\r\n</pre>\r\nOn the replica server, ensure the following my.cnf configuration, set the server-id as a unique id for each replica, then restart mysql:\r\n<pre>\r\n[client]\r\ndefault-character-set=utf8mb4\r\n\r\n[mysql]\r\ndefault-character-set = utf8mb4\r\n\r\n[mysqld]\r\nmax_connections = 400\r\nft_min_word_len=2\r\nsql_mode = \"NO_BACKSLASH_ESCAPES\"\r\n#character-set-client-handshake = FALSE\r\ncharacter-set-server = utf8mb4\r\ncollation-server = utf8mb4_0900_ai_ci\r\nskip-character-set-client-handshake\r\ndefault-authentication-plugin=mysql_native_password\r\nwait_timeout = 800\r\n\r\n#memory use settings, you should adjust this based on your hardware\r\ninnodb_buffer_pool_size = 1342177280\r\ninnodb_buffer_pool_instances = 2\r\n\r\n#setting up replication below\r\nbind-address = 0.0.0.0\r\nserver-id = 2\r\nrelay_log_info_repository = TABLE\r\nrelay_log_recovery = ON\r\nsync_binlog=1\r\n</pre>\r\nMake sure only VPN and VLAN addresses can reach your replicas. The bind address of 0.0.0.0 can be replaced with '127.0.0.1,replica.vpn.ip' which is safer but also more crash prone if the VPN address is not available on startup.\r\n<br>\r\n<br>\r\nTo export the database to the replica server, on the primary server, stop the web crawler and hide any web forms that can accept new data, then open MySQL and do the following.\r\n<pre>\r\nUSE wiby;\r\nFLUSH TABLES WITH READ LOCK;\r\nSHOW MASTER STATUS;\r\n\r\n+------------------+----------+--------------+------------------+-------------------+\r\n| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |\r\n+------------------+----------+--------------+------------------+-------------------+\r\n| mysql-bin.000055 | 15871269 | wiby         |                  |                   |\r\n+------------------+----------+--------------+------------------+-------------------+\r\n</pre>\r\nKeep the above session window open (or run it in a screen session).\r\n<br>\r\nCopy down the information from that table. In a separate session window, export the database:\r\n<pre>\r\nmysqldump -u root -p wiby > wiby.sql\r\n</pre>\r\nOnce you have exported the database and recorded what you need, you can unlock the tables, and resume as normal. On the session window displaying the master status:\r\n<pre>\r\nUNLOCK TABLES;\r\n</pre>\r\nYou can now close that window if you want.\r\n<br>\r\n<br>\r\nOn the replica server, login to MySQL and create the database:\r\n<pre>\r\nCREATE DATABASE wiby;\r\nEXIT;\r\n</pre>\r\nImport the database:\r\n<pre>\r\nmysql -u root -p wiby < wiby.sql\r\n</pre>\r\nLogin to MySQL and type the following but replace the primary_server_ip, MASTER_LOG_FILE, and MASTER_LOG_POS with yours from the table:\r\n<pre>\r\nCHANGE MASTER TO MASTER_HOST='primary_server_ip',MASTER_USER='slave_user', MASTER_PASSWORD='d0gemuchw0w', MASTER_LOG_FILE='mysql-bin.000055', MASTER_LOG_POS=15871269;\r\nSTART SLAVE;\r\n</pre>\r\nTo verify that the replica is syncronized, type the following on the replica in MySQL:\r\n<pre>\r\nSHOW SLAVE STATUS\\G\r\n</pre>\r\nMake sure that:\r\n<pre>\r\nSlave_IO_Running: Yes\r\nSlave_SQL_Running: Yes\r\n</pre>\r\nIn MySQL on the replica, create the accounts required for the replication tracker and core application:\r\n<pre>\r\nuse wiby;\r\ncreate user 'remote_guest'@'%' identified by 'd0gemuchw0w';\r\ngrant select on windex to 'remote_guest'@'%';\r\ncreate user 'guest'@'localhost' identified by 'qwer';\r\ngrant select on windex to 'guest'@'localhost';\r\nFLUSH PRIVILEGES;\r\n\r\n</pre>\r\n<h3>Load Balancing</h3>\r\nYou should run the core application on one or more of your replicas and have nginx send traffic to it, this way you can reduce the burden on your VPS. The replication tracker (rt) must run on the same server \r\nand directory that the core application is running on (not required for 1core).\r\n<br>\r\n<br>\r\nAdd the replica server's VPN address/port to upstream remote_core {} from the default config for nginx (see the provided example template). You can use the VPS as a backup instead by adding 'backup' to its address (eg: server 127.0.0.1:8080 backup;)\r\n<br>\r\n</p>\r\n</blockquote>\r\n</body>    \r\n</html>\r\n","oembed":false,"readabilityObject":{"title":"Build your own Search Engine","content":"<div id=\"readability-page-1\" class=\"page\"><div>   \n\n<p>(Wiby Install Guide)</p>\n\n<p>\n<a href=\"guide.html#overview\">Overview</a>\n<br>\n<a href=\"guide.html#install\">Installation</a>\n<br>\n<a href=\"guide.html#control\">Controlling</a>\n<br>\n<a href=\"guide.html#scale\">Scaling</a>\n\n</p><h2><a name=\"overview\">Overview</a></h2>\nWiby is a search engine for the World Wide Web. The source code is now free as of July 8, 2022 under the GPLv2 license. I have been longing for this day! You can watch a quick demo <a href=\"https://youtu.be/nCfWJqNBqHo\">here</a>.\n<p>\n\nIt includes a web interface allowing guardians to control where, how far, and how often it crawls websites and follows hyperlinks. The search index is stored inside of a MySQL full-text index.\n</p><p>\n\nFast queries are maintained by concurrently reading different sections of the index across multiple replication servers or across duplicate server connections, returning a list of top results from each connection, \nthen searching the combined list to ensure correct ordering. Replicas that fail are automatically excluded; new replicas are easy to include. \nAs new pages are crawled, they are stored randomly across the index, ensuring each replica can obtain relevant results.</p><p>\n\nThe search engine is not meant to index the entire web and then sort it with a ranking algorithm. \nIt prefers to seed its index through human submissions made by guests, or by the guardian(s) of the search engine. \n</p><p>\n\nThe software is designed for anyone with some extra computers (even a Pi), to host their own search engine catering to whatever niche matters to them. The search engine includes a simple API \nfor meta search engines to harness.\n</p><p>\n\nI hope this will enable anyone with a love of computers to cheaply build and maintain a search engine of their own. \nI hope it can cultivate free and independent search engines, ensuring accessibility of ideas and information across the World Wide Web.\n</p><pre>\n       Web Traffic\n            |\n            |\n+-----------+-----------+\n| Reverse Proxy (nginx) |\n+-----------+-----------+\n            |\n            |\n+-----------+-----------+\n|  Wiby Core Server(s)  |+-----------------+----------------------------+\n|(Golang or PHP version)|                  |                            |\n+-----------+-----------+       +----------+----------+       +---------+---------+\n            |                   |Replication Databases|+-----+|Replication Tracker|\n            |                   +----------+----------+       +-------------------+\n+-----------+-----------+                  |\n|    Primary Database   |+-----------------+\n|   (MySQL or MariaDB)  |\n+----+-------------+----+\n     |             |  \n     |             |  \n+----+-----+  +----+----+\n|   Web    |  | Refresh |\n|Crawler(s)|  |Scheduler|\n+----------+  +---------+\n</pre>\n<br>\n<hr>\n<h2><a name=\"install\">Installation</a></h2>\nI can only provide manual install instructions at this time. \n<p>\n\nNote that while the software is functionally complete, it is still in beta. Anticipate that some bugs will be discovered now that the source is released. \nEnsure that you isolate the search engine from your other important services, and if you are running parts of it out of your home, keep the servers \non a separate VLAN. Make sure this VLAN cannot access your router or switch interface. Continue this practise even when the software reaches \"1.0\".\n</p><p>\n\nIf you have created a \"LAMP\", or rather a \"LEMP\" server before, this isn't much more complicated. If you've never done that, I suggest you find a \"LEMP\" tutorial.\n</p><h3>Build a LEMP server</h3>\nDigital Ocean tutorials are usually pretty good so here is a link to one for <a href=\"https://www.digitalocean.com/community/tutorials/how-to-install-linux-nginx-mysql-php-lemp-stack-on-ubuntu-20-04\">Ubuntu 20</a> and \n<a href=\"https://www.digitalocean.com/community/tutorials/how-to-install-linux-nginx-mysql-php-lemp-stack-on-ubuntu-22-04\">Ubuntu 22</a>.\n<p>\n\nFor the sake of simplicity, assume all instructions are for Ubuntu 20 or 22. If you are on a different distro, modify the install steps accordingly to suit your distro.\n</p><p>\n\nIf you don't have a physical server, you can rent computing space by looking for a \"VPS provider\". This virtual computer will be your reverse proxy, and if you want, it can host everything else too.\n</p><h3>Install the following additional packages:</h3>\n<pre>apt install build-essential php-gd libcurl4-openssl-dev libmysqlclient-dev mysql-server golang git\n\nFor Ubuntu 20:\ngo get -u github.com/go-sql-driver/mysql\n\nFor Ubuntu 22 or latest Golang versions:\ngo install github.com/go-sql-driver/mysql@latest\ngo mod init mysql\ngo get github.com/go-sql-driver/mysql\n</pre>\n<br>\n<h3>Get Wiby Source Files</h3>\nDownload the source directly from Wiby <a href=\"/download/wibysource.zip\">here</a>, or from <a href=\"https://github.com/wibyweb/wiby/\">GitHub</a>. The source is released under the GPLv2 license. Copy the source files for Wiby to your server.\n<h3>Compile the crawler (cr), refresh scheduler (rs), replication tracker (rt):</h3>\n<pre>gcc cr.c -o cr -I/usr/include/mysql -lmysqlclient -lcurl -std=c99 -O3\ngcc rs.c -o rs -I/usr/include/mysql -lmysqlclient -std=c99 -O3\ngcc rt.c -o rt -I/usr/include/mysql -lmysqlclient -std=c99 -O3\n</pre>\nIf you get any compile errors, it is likely due to the path of the mysql or libcurl header files. \nThis could happen if you are not using Ubuntu 20. You might have to locate the correct path for curl.h, easy.h, and mysql.h.\n<h3>Build the core server application:</h3>\n<pre>Inside the go folder:\n\ngo build core.go\ngo build 1core.go\n</pre>\nIf you are just starting out, you can use '1core'. If you are going to setup replication servers or you are using a computer with a lot of available cores, you can use 'core', but make sure to read the scaling section. \nYou can also use index.php in the root of the www directory and not use the Go version at all. Though the PHP version is used mainly for prototyping.\n<h3>Build the database:</h3>\nMake sure these lines are inside of /etc/mysql/my.cnf, then restart mysql\n<pre>[client]\ndefault-character-set=utf8mb4\n\n[mysql]\ndefault-character-set = utf8mb4\n\n[mysqld]\nmax_connections = 400\nft_min_word_len=2\nsql_mode = \"NO_BACKSLASH_ESCAPES\"\ncharacter-set-server = utf8mb4\ncollation-server = utf8mb4_0900_ai_ci\nskip-character-set-client-handshake\ndefault-authentication-plugin=mysql_native_password\nwait_timeout = 800\n\n#memory use settings, you should adjust this based on your hardware\ninnodb_buffer_pool_size = 1342177280\ninnodb_buffer_pool_instances = 2\n\n</pre>\nLogin to MySQL and type:\n<pre>create database wiby;\ncreate database wibytemp;\n</pre>\nImport the wiby and wibytemp database files:\n<pre>mysql -u root -p wiby &lt; wiby.sql\nmysql -u root -p wibytemp &lt; wibytemp.sql\n</pre>\nLogin to MySQL, create the following accounts and give them the correct access:\n<pre>create user 'guest'@'localhost' identified by 'qwer';\ncreate user 'approver'@'localhost' identified by 'foobar';\ncreate user 'crawler'@'localhost' identified by 'seekout';\nuse wiby;\ngrant select on accounts to 'approver'@'localhost';\ngrant select on reviewqueue to 'approver'@'localhost';\ngrant insert on indexqueue to 'approver'@'localhost';\ngrant delete on reviewqueue to 'approver'@'localhost';\ngrant update on reviewqueue to 'approver'@'localhost';\ngrant select on indexqueue to 'crawler'@'localhost';\ngrant insert on windex to 'crawler'@'localhost';\ngrant insert on indexqueue to 'crawler'@'localhost';\ngrant update on windex to 'crawler'@'localhost';\ngrant delete on indexqueue to 'crawler'@'localhost';\ngrant delete on windex to 'crawler'@'localhost';\ngrant select on windex to 'crawler'@'localhost';\ngrant insert on reviewqueue to 'crawler'@'localhost';\ngrant select on windex to 'guest'@'localhost';\ngrant insert on reviewqueue to 'guest'@'localhost';\ngrant insert on feedback to 'guest'@'localhost';\ngrant select on feedback to 'approver'@'localhost';\ngrant delete on feedback to 'approver'@'localhost';\ngrant insert on graveyard to 'approver'@'localhost';\ngrant update on graveyard to 'approver'@'localhost';\ngrant delete on graveyard to 'approver'@'localhost';\ngrant select on graveyard to 'approver'@'localhost';\ngrant update on accounts to 'approver'@'localhost';\ngrant insert on accounts to 'approver'@'localhost';\ngrant delete on accounts to 'approver'@'localhost';\nuse wibytemp;\ngrant select on titlecheck to 'crawler'@'localhost';\ngrant insert on titlecheck to 'crawler'@'localhost';\ngrant delete on titlecheck to 'crawler'@'localhost';\ngrant select on rejected to 'approver'@'localhost';\ngrant insert on rejected to 'approver'@'localhost';\ngrant delete on rejected to 'approver'@'localhost';\ngrant select on reserve_id to 'crawler'@'localhost';\ngrant insert on reserve_id to 'crawler'@'localhost';\ngrant delete on reserve_id to 'crawler'@'localhost';\nFLUSH PRIVILEGES;\n</pre>\n<h3>Copy the HTML files and PHP scripts to your web server</h3>\n<pre>Copy the contents of the the html directory into the nginx html directory (/var/www/html)</pre>\n\n<h3>Configure nginx for Wiby</h3>\nIn /etc/nginx/, create a directory called 'phpcache', and another one called 'cache'.\n<br>\nInstead of going through every detail, I will provide a template for you to try out as your default nginx config from inside /etc/nginx/sites-available/ of the source code.\n<p>\n\nYou should learn nginx configuration on your own, this template is just to assist. \nIf you are using only the php version, comment all \"core app\" location entries to revert Wiby search to the php only version.\n<br>\nMake sure ssl_certificate and ssl_certificate_key have the path for your SSL files instead of the example paths. If you don't want to use SSL, just remove the server {} configuration for SSL connections (on port 443). \nAlso the example file references php7.4-fpm.sock, so if you are using a different version remember to update that as well (such as php8.1-fpm.sock on Ubuntu 22).\n</p><h3>Start the Refresh Scheduler</h3>\nThis program (rs) will make sure all pages indexed are refreshed at least once per week (or sooner depending on how you assign updates to an individual website). \nYou may want to run this on startup, easiest way to set that is with a cron job (crontab -e). Run './rs -h' to get more parameters.\n<h3>Start the Crawler</h3>\nIt is best to run the crawler in a screen session so that you can monitor its output. You can have more than one crawler running as long as you keep them in separate directories, include a symlink to the same robots folder, and also set the correct parameters on each. \nTo view the parameters, type './cr -h'. Without any parameters set, you can only run one crawler (which is probably all you need anyway). If necessary, you can change the database connection from 'localhost' to a different IP from inside cr.c, then rebuild.\n<p>\n\nNote that you may need to change the crawler's user-agent if you have issues indexing some websites. Pages that fail to index are noted inside of abandoned.txt.\n</p><p> \n\nMake sure the robots folder exists. All robots.txt files are stored in the robots folder. They are downloaded once and then referenced from that folder on future updates. Clear this folder every few weeks to ensure robots.txt files get refreshed from time to time. \nYou can turn off checking for robots.txt files by commenting out the line calling the \"checkrobots\" function inside of cr.c.\n</p><p>\n\nIf crawling through hyperlinks on a page, the following file types are accepted: html, htm, txt, php, asp. Links containing parameters are ignored. These limitations do not apply to pages directly submitted by people.\n</p><h3>Start the core server</h3>\nIf you are just starting out, '1core' or the php version is easiest to start with. Use 'core' if you want to scale computer resources as the index grows or if you have a lot of available CPU cores. Make sure to read the scaling section. \nYou can run the core server on startup with a cron job.\n<h3>Set Administrator Password for the Web Interface</h3>\nThere is no default web login, you will have to set this manually the first time:\n<pre>Rename the /html/hash folder to something private.\n\nEdit html/private_folder_name/hashmake.php and change 'secretpassword' to your preferred admin password. \n\nAccess /private_folder_name/hashmake.php from your browser and copy down the hash.\n\nAfter you have copied it down, delete or remove hashmake.php from your web server folder so that the hash cannot be discovered.\n</pre>\nLogin to MySQL and create the account:\n<pre>use wiby;\nINSERT INTO accounts (name,hash,level) VALUES('your_username','your_password_hash','admin');\n</pre>\nYou can now access /accounts/ from your browser, login to create and manage all accounts for administrators and guardians of the search engine.\n<p>\n\n<b>admin</b> - Can access all web forms for the search engine and use the /accounts/ page to create and delete accounts.\n</p><p>\n\n<b>guardian</b> - The main role of a guardian is to gatekeep the index of the search engine. Can access all forms except for /readf/, and can only use the /accounts/ page to change their own password.\n</p><hr>\n<h2><a name=\"control\">Controlling the Search Engine</a></h2>\n<br>\nThere are several forms to control the search engine. There is no central form linking everything together, just a collection of different folders that you can rename if you want.\n<h3>/submit/</h3> This public facing form allows users of the search engine to submit websites for indexing, provided they comply with your submission criteria, which you can modify on /submit/form.html.php.\n<h3>/accounts/</h3>\nThis is the account management page. Admins have options to create, lock, change account type, delete, and reset passwords. Guardians have the option to change their password.\n<h3>/review/</h3> This is the most important form, intended for you to verify website submissions meet your criteria. Up to 10 pages are assigned to each guardian or admin that accesses the form. The pages will remain assigned to that account for up to 30 minutes. \nFrom here you can control how much, how deep, and how often the web crawler will access each submission. Here is an example of the available options for a website submission:\n<p>\n\n<a href=\"url_that_was_submitted\">url_that_was_submitted</a>\n<br>\n[Worksafe]\n\t    [Surprise]\n            [Skip]\n            [Bury]\n\t    [Deny]\n\t    [Updatable]\n\t\t<br>\n\t\t[Crawl: Depth \n\t\tPages \n\t\tType \n\t\tEnforce Rules\n\t\tRepeat]\n</p><p>\n\nExplanation of the above options:\n</p><p>\n\n<b>Worksafe</b> - Indicates if the website is safe for work. Set by the user who submitted the website, however you can change it based on your determination.\n</p><p>\n\n<b>Surprise</b> - Checking this box will put it in the \"surprise me\" feature, where users get redirected to random websites when they click \"surprise me\". Note that this feature won't show NSFW websites even if they are set to surprise.\n</p><p>\n\n<b>Skip</b> - Selecting this option will skip indexing the page and it will reappear on the review form after you submit the rest of the pages for crawling.\n</p><p>\n\n<b>Bury</b> - Selecting this will move the page to a grave yard (/grave/), a holding place with the same options as /review/ for websites that might have stopped working but that you suspect may come back online. The crawler will detect this automatically and send the page back into review. When you click on the link and see a 404, you can be assured the crawler sent it back to review after failing two update cycles. This also happens if the title of the page changes. The crawler will only do this for pages directly submitted by people. This curtesy is not given to websites that are automatically crawled but then fail to work later on. For those sites, after two failed update cycles, the page will be removed.\n</p><p>\n\n<b>Deny</b> - Select this to drop the page from being indexed. If the page does not meet your submission criteria, this would be the option to remove it from the queue.\n</p><p>\n\n<b>Updatable</b> - The update cycle for the web crawler to return to the page. This only applies to pages submitted by people, pages found by link crawling always go on a 1 week update cycle.\n</p><p>\n\n<b>------------------- Crawl -------------------</b> \n<br>\nThe options listed below control how the crawler indexes hyperlinks on the website. By default, the crawler does not index any hyperlinks, it will only index the page that is submitted.\n</p><p>\n\n<b>Depth</b>  - How many layers of links to crawl through. You must set at least a depth of 1 if you want to crawl any hyperlinks. Setting a negative value = no limit. Be careful about that.\n</p><p>\n\n<b>Pages</b> - How many pages to crawl on each link layer (depth). They will be randomly selected. You must set at least 1 if you want to crawl any hyperlinks. Setting a negative value = no limit. Be careful about that.\n</p><p>\n\n<b>Type</b> - Indicates if you want to only crawl links local to the website, or links external to the website, or both.\n</p><p>\n\n<b>Enforce rules</b> - This is a blunt tool that checks if pages have more than two scripts and/or css files. If the limit is exceded, the page will not be indexed. I don't use it and prefer to manually check based on more forgiving criteria.\n</p><p>\n\n<b>Repeat</b> - While the crawler will always return to update each page in the index, it wont crawl through hyperlinks again unless you tell it to. Even so, it only crawls hyperlinks on the page at a depth of 1 when repeat is selected.\n</p><h3>/ban/</h3>\nYou can delete or ban individual URL's from the index with this form. Its pretty simple as I don't use it much. You can't delete an entire domain with it, for that you can build your own query in the MySQL console.\n<h3>/bulksubmit/</h3>\nAdmins/Guardians can import a list of URLs into the review queue with this form.\n<h3>/feedback/</h3>\nUsers can submit feedback for you with this form.\n<h3>/readf/</h3>\nWhere admin accounts can read feedback submitted by users.\n<h3>/grave/</h3>\nIt has the same features as /review/. Websites that you don't yet want to index but don't want to forget about are stored inside /grave/ by selecting 'bury' from inside /review/. The web crawler will (only for pages submitted directly by people), move 404'd pages or pages where the title has changed back to /review/ after two update cycles \nwhere the page does not return to normal. So after a few weeks you may notice dead pages appearing in /review/, you can decide to drop the page or to bury it where it will be moved to /grave/. The page might go back to normal at some point and you can check /grave/ to see if it resurrects. \n<h3>/insert/</h3>\nThis was the first form created back in late 2016 to populate the Wiby index and see if the search engine could even work as a proof of concept. It was meant to manually enter pages into the index as no crawler existed yet. \nIt is still useful if you want to manually index a page that refuses to permit the crawler to access it. In that case, set updatable to 0.\n<h3>/tags/</h3>\nIf you want to force a website to appear at the top rank for specific single word queries (like \"weather\"), you can force it by tagging the words to the target url.\n<h3>/json/</h3>\nThis is the JSON API developers can use to connect their services to the search engine. Instructions are located at that location.\n<h3>Additional Notes</h3>\nIf you need to stop the web crawler in a situation where it was accidently queued to index an unlimited number of pages, first stop the crawler program, truncate the indexqueue table 'truncate indexqueue;', then restart the crawler.\n<hr>\n<h2><a name=\"scale\">Scaling the Search Engine</a></h2>\n<br>\nYou can help ensure sub-second search queries as your index grows by building MySQL replica servers on a local network close to eachother, run the core application AND replication tracker (rt) on one or more replica servers and point your reverse proxy to use it. \nEdit the servers.csv file for rt to indicate all available replica servers. If you have a machine with a huge amount of resources and cores, entering multiple duplicate entries to the same sever inside servers.csv (e.g. one for each core) works also.\n<p>\n\nThe core application checks the replication tracker (rt) output to determine if any replicas are online, it will initiate a connection on those replicas and task each one to search a different section of the index, \ndrastically speeding up search speeds especially for multi-word queries. By default, single-word queries will not initiate multiple connections across replicas. To enable that on single-word queries, comment out the IF statement \non line 373 and rebuild the core application.\n</p><p>\n\nThe number of available replicas must divide evenly into the search results per page limit (lim), OR, the search results per page limit must divide evenly into the number of available replicas. If there \nis an excess of available replicas such that they do not divide evenly, those will remain in synch but will not be used for searches unless another replica fails. You can adjust the search results per page limit (lim) to a different value (default 12), \nand then rebuild to make excess available replicas divide evenly (if necessary).\n</p><p>\n\nThe reverse proxy and replica servers can be connected through a VPN such as wireguard or openvpn, however the IPs for servers.csv should be the local IPs for the LAN \nthe replicas are all connected on. <a href=\"https://www.digitalocean.com/community/tutorials/how-to-set-up-replication-in-mysql\">Here</a> is a tutorial for setting up MySQL replicas.\n</p><p>\n<b>Full instructions below:</b></p><p>\n\nOn the primary server add these lines to my.cnf under [mysqld] but only once you have a VPN to reach your replicas. Replace my.vpn.ip with your own.\n</p><pre>#setting up replication below\nbind-address = 127.0.0.1,my.vpn.ip\nserver-id = 1\nlog_bin = /var/log/mysql/mysql-bin.log\nbinlog_do_db = wiby\nbinlog_format = mixed\n</pre>\nIn MySQL on the primary server, create a user for replica access:\n<pre>create user 'slave_user'@'%' identified by 'd0gemuchw0w';\nGRANT REPLICATION SLAVE ON *.* TO 'slave_user'@'%';\nFLUSH PRIVILEGES;\n</pre>\nOn the replica server, ensure the following my.cnf configuration, set the server-id as a unique id for each replica, then restart mysql:\n<pre>[client]\ndefault-character-set=utf8mb4\n\n[mysql]\ndefault-character-set = utf8mb4\n\n[mysqld]\nmax_connections = 400\nft_min_word_len=2\nsql_mode = \"NO_BACKSLASH_ESCAPES\"\n#character-set-client-handshake = FALSE\ncharacter-set-server = utf8mb4\ncollation-server = utf8mb4_0900_ai_ci\nskip-character-set-client-handshake\ndefault-authentication-plugin=mysql_native_password\nwait_timeout = 800\n\n#memory use settings, you should adjust this based on your hardware\ninnodb_buffer_pool_size = 1342177280\ninnodb_buffer_pool_instances = 2\n\n#setting up replication below\nbind-address = 0.0.0.0\nserver-id = 2\nrelay_log_info_repository = TABLE\nrelay_log_recovery = ON\nsync_binlog=1\n</pre>\nMake sure only VPN and VLAN addresses can reach your replicas. The bind address of 0.0.0.0 can be replaced with '127.0.0.1,replica.vpn.ip' which is safer but also more crash prone if the VPN address is not available on startup.\n<p>\n\nTo export the database to the replica server, on the primary server, stop the web crawler and hide any web forms that can accept new data, then open MySQL and do the following.\n</p><pre>USE wiby;\nFLUSH TABLES WITH READ LOCK;\nSHOW MASTER STATUS;\n\n+------------------+----------+--------------+------------------+-------------------+\n| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |\n+------------------+----------+--------------+------------------+-------------------+\n| mysql-bin.000055 | 15871269 | wiby         |                  |                   |\n+------------------+----------+--------------+------------------+-------------------+\n</pre>\nKeep the above session window open (or run it in a screen session).\n<br>\nCopy down the information from that table. In a separate session window, export the database:\n<pre>mysqldump -u root -p wiby &gt; wiby.sql\n</pre>\nOnce you have exported the database and recorded what you need, you can unlock the tables, and resume as normal. On the session window displaying the master status:\n<pre>UNLOCK TABLES;\n</pre>\nYou can now close that window if you want.\n<p>\n\nOn the replica server, login to MySQL and create the database:\n</p><pre>CREATE DATABASE wiby;\nEXIT;\n</pre>\nImport the database:\n<pre>mysql -u root -p wiby &lt; wiby.sql\n</pre>\nLogin to MySQL and type the following but replace the primary_server_ip, MASTER_LOG_FILE, and MASTER_LOG_POS with yours from the table:\n<pre>CHANGE MASTER TO MASTER_HOST='primary_server_ip',MASTER_USER='slave_user', MASTER_PASSWORD='d0gemuchw0w', MASTER_LOG_FILE='mysql-bin.000055', MASTER_LOG_POS=15871269;\nSTART SLAVE;\n</pre>\nTo verify that the replica is syncronized, type the following on the replica in MySQL:\n<pre>SHOW SLAVE STATUS\\G\n</pre>\nMake sure that:\n<pre>Slave_IO_Running: Yes\nSlave_SQL_Running: Yes\n</pre>\nIn MySQL on the replica, create the accounts required for the replication tracker and core application:\n<pre>use wiby;\ncreate user 'remote_guest'@'%' identified by 'd0gemuchw0w';\ngrant select on windex to 'remote_guest'@'%';\ncreate user 'guest'@'localhost' identified by 'qwer';\ngrant select on windex to 'guest'@'localhost';\nFLUSH PRIVILEGES;\n\n</pre>\n<h3>Load Balancing</h3>\nYou should run the core application on one or more of your replicas and have nginx send traffic to it, this way you can reduce the burden on your VPS. The replication tracker (rt) must run on the same server \nand directory that the core application is running on (not required for 1core).\n<p>\n\nAdd the replica server's VPN address/port to upstream remote_core {} from the default config for nginx (see the provided example template). You can use the VPS as a backup instead by adding 'backup' to its address (eg: server 127.0.0.1:8080 backup;)\n</p>\n</div></div>","textContent":"   \n\n(Wiby Install Guide)\n\n\nOverview\n\nInstallation\n\nControlling\n\nScaling\n\nOverview\nWiby is a search engine for the World Wide Web. The source code is now free as of July 8, 2022 under the GPLv2 license. I have been longing for this day! You can watch a quick demo here.\n\n\nIt includes a web interface allowing guardians to control where, how far, and how often it crawls websites and follows hyperlinks. The search index is stored inside of a MySQL full-text index.\n\n\nFast queries are maintained by concurrently reading different sections of the index across multiple replication servers or across duplicate server connections, returning a list of top results from each connection, \nthen searching the combined list to ensure correct ordering. Replicas that fail are automatically excluded; new replicas are easy to include. \nAs new pages are crawled, they are stored randomly across the index, ensuring each replica can obtain relevant results.\n\nThe search engine is not meant to index the entire web and then sort it with a ranking algorithm. \nIt prefers to seed its index through human submissions made by guests, or by the guardian(s) of the search engine. \n\n\nThe software is designed for anyone with some extra computers (even a Pi), to host their own search engine catering to whatever niche matters to them. The search engine includes a simple API \nfor meta search engines to harness.\n\n\nI hope this will enable anyone with a love of computers to cheaply build and maintain a search engine of their own. \nI hope it can cultivate free and independent search engines, ensuring accessibility of ideas and information across the World Wide Web.\n\n       Web Traffic\n            |\n            |\n+-----------+-----------+\n| Reverse Proxy (nginx) |\n+-----------+-----------+\n            |\n            |\n+-----------+-----------+\n|  Wiby Core Server(s)  |+-----------------+----------------------------+\n|(Golang or PHP version)|                  |                            |\n+-----------+-----------+       +----------+----------+       +---------+---------+\n            |                   |Replication Databases|+-----+|Replication Tracker|\n            |                   +----------+----------+       +-------------------+\n+-----------+-----------+                  |\n|    Primary Database   |+-----------------+\n|   (MySQL or MariaDB)  |\n+----+-------------+----+\n     |             |  \n     |             |  \n+----+-----+  +----+----+\n|   Web    |  | Refresh |\n|Crawler(s)|  |Scheduler|\n+----------+  +---------+\n\n\n\nInstallation\nI can only provide manual install instructions at this time. \n\n\nNote that while the software is functionally complete, it is still in beta. Anticipate that some bugs will be discovered now that the source is released. \nEnsure that you isolate the search engine from your other important services, and if you are running parts of it out of your home, keep the servers \non a separate VLAN. Make sure this VLAN cannot access your router or switch interface. Continue this practise even when the software reaches \"1.0\".\n\n\nIf you have created a \"LAMP\", or rather a \"LEMP\" server before, this isn't much more complicated. If you've never done that, I suggest you find a \"LEMP\" tutorial.\nBuild a LEMP server\nDigital Ocean tutorials are usually pretty good so here is a link to one for Ubuntu 20 and \nUbuntu 22.\n\n\nFor the sake of simplicity, assume all instructions are for Ubuntu 20 or 22. If you are on a different distro, modify the install steps accordingly to suit your distro.\n\n\nIf you don't have a physical server, you can rent computing space by looking for a \"VPS provider\". This virtual computer will be your reverse proxy, and if you want, it can host everything else too.\nInstall the following additional packages:\napt install build-essential php-gd libcurl4-openssl-dev libmysqlclient-dev mysql-server golang git\n\nFor Ubuntu 20:\ngo get -u github.com/go-sql-driver/mysql\n\nFor Ubuntu 22 or latest Golang versions:\ngo install github.com/go-sql-driver/mysql@latest\ngo mod init mysql\ngo get github.com/go-sql-driver/mysql\n\n\nGet Wiby Source Files\nDownload the source directly from Wiby here, or from GitHub. The source is released under the GPLv2 license. Copy the source files for Wiby to your server.\nCompile the crawler (cr), refresh scheduler (rs), replication tracker (rt):\ngcc cr.c -o cr -I/usr/include/mysql -lmysqlclient -lcurl -std=c99 -O3\ngcc rs.c -o rs -I/usr/include/mysql -lmysqlclient -std=c99 -O3\ngcc rt.c -o rt -I/usr/include/mysql -lmysqlclient -std=c99 -O3\n\nIf you get any compile errors, it is likely due to the path of the mysql or libcurl header files. \nThis could happen if you are not using Ubuntu 20. You might have to locate the correct path for curl.h, easy.h, and mysql.h.\nBuild the core server application:\nInside the go folder:\n\ngo build core.go\ngo build 1core.go\n\nIf you are just starting out, you can use '1core'. If you are going to setup replication servers or you are using a computer with a lot of available cores, you can use 'core', but make sure to read the scaling section. \nYou can also use index.php in the root of the www directory and not use the Go version at all. Though the PHP version is used mainly for prototyping.\nBuild the database:\nMake sure these lines are inside of /etc/mysql/my.cnf, then restart mysql\n[client]\ndefault-character-set=utf8mb4\n\n[mysql]\ndefault-character-set = utf8mb4\n\n[mysqld]\nmax_connections = 400\nft_min_word_len=2\nsql_mode = \"NO_BACKSLASH_ESCAPES\"\ncharacter-set-server = utf8mb4\ncollation-server = utf8mb4_0900_ai_ci\nskip-character-set-client-handshake\ndefault-authentication-plugin=mysql_native_password\nwait_timeout = 800\n\n#memory use settings, you should adjust this based on your hardware\ninnodb_buffer_pool_size = 1342177280\ninnodb_buffer_pool_instances = 2\n\n\nLogin to MySQL and type:\ncreate database wiby;\ncreate database wibytemp;\n\nImport the wiby and wibytemp database files:\nmysql -u root -p wiby < wiby.sql\nmysql -u root -p wibytemp < wibytemp.sql\n\nLogin to MySQL, create the following accounts and give them the correct access:\ncreate user 'guest'@'localhost' identified by 'qwer';\ncreate user 'approver'@'localhost' identified by 'foobar';\ncreate user 'crawler'@'localhost' identified by 'seekout';\nuse wiby;\ngrant select on accounts to 'approver'@'localhost';\ngrant select on reviewqueue to 'approver'@'localhost';\ngrant insert on indexqueue to 'approver'@'localhost';\ngrant delete on reviewqueue to 'approver'@'localhost';\ngrant update on reviewqueue to 'approver'@'localhost';\ngrant select on indexqueue to 'crawler'@'localhost';\ngrant insert on windex to 'crawler'@'localhost';\ngrant insert on indexqueue to 'crawler'@'localhost';\ngrant update on windex to 'crawler'@'localhost';\ngrant delete on indexqueue to 'crawler'@'localhost';\ngrant delete on windex to 'crawler'@'localhost';\ngrant select on windex to 'crawler'@'localhost';\ngrant insert on reviewqueue to 'crawler'@'localhost';\ngrant select on windex to 'guest'@'localhost';\ngrant insert on reviewqueue to 'guest'@'localhost';\ngrant insert on feedback to 'guest'@'localhost';\ngrant select on feedback to 'approver'@'localhost';\ngrant delete on feedback to 'approver'@'localhost';\ngrant insert on graveyard to 'approver'@'localhost';\ngrant update on graveyard to 'approver'@'localhost';\ngrant delete on graveyard to 'approver'@'localhost';\ngrant select on graveyard to 'approver'@'localhost';\ngrant update on accounts to 'approver'@'localhost';\ngrant insert on accounts to 'approver'@'localhost';\ngrant delete on accounts to 'approver'@'localhost';\nuse wibytemp;\ngrant select on titlecheck to 'crawler'@'localhost';\ngrant insert on titlecheck to 'crawler'@'localhost';\ngrant delete on titlecheck to 'crawler'@'localhost';\ngrant select on rejected to 'approver'@'localhost';\ngrant insert on rejected to 'approver'@'localhost';\ngrant delete on rejected to 'approver'@'localhost';\ngrant select on reserve_id to 'crawler'@'localhost';\ngrant insert on reserve_id to 'crawler'@'localhost';\ngrant delete on reserve_id to 'crawler'@'localhost';\nFLUSH PRIVILEGES;\n\nCopy the HTML files and PHP scripts to your web server\nCopy the contents of the the html directory into the nginx html directory (/var/www/html)\n\nConfigure nginx for Wiby\nIn /etc/nginx/, create a directory called 'phpcache', and another one called 'cache'.\n\nInstead of going through every detail, I will provide a template for you to try out as your default nginx config from inside /etc/nginx/sites-available/ of the source code.\n\n\nYou should learn nginx configuration on your own, this template is just to assist. \nIf you are using only the php version, comment all \"core app\" location entries to revert Wiby search to the php only version.\n\nMake sure ssl_certificate and ssl_certificate_key have the path for your SSL files instead of the example paths. If you don't want to use SSL, just remove the server {} configuration for SSL connections (on port 443). \nAlso the example file references php7.4-fpm.sock, so if you are using a different version remember to update that as well (such as php8.1-fpm.sock on Ubuntu 22).\nStart the Refresh Scheduler\nThis program (rs) will make sure all pages indexed are refreshed at least once per week (or sooner depending on how you assign updates to an individual website). \nYou may want to run this on startup, easiest way to set that is with a cron job (crontab -e). Run './rs -h' to get more parameters.\nStart the Crawler\nIt is best to run the crawler in a screen session so that you can monitor its output. You can have more than one crawler running as long as you keep them in separate directories, include a symlink to the same robots folder, and also set the correct parameters on each. \nTo view the parameters, type './cr -h'. Without any parameters set, you can only run one crawler (which is probably all you need anyway). If necessary, you can change the database connection from 'localhost' to a different IP from inside cr.c, then rebuild.\n\n\nNote that you may need to change the crawler's user-agent if you have issues indexing some websites. Pages that fail to index are noted inside of abandoned.txt.\n \n\nMake sure the robots folder exists. All robots.txt files are stored in the robots folder. They are downloaded once and then referenced from that folder on future updates. Clear this folder every few weeks to ensure robots.txt files get refreshed from time to time. \nYou can turn off checking for robots.txt files by commenting out the line calling the \"checkrobots\" function inside of cr.c.\n\n\nIf crawling through hyperlinks on a page, the following file types are accepted: html, htm, txt, php, asp. Links containing parameters are ignored. These limitations do not apply to pages directly submitted by people.\nStart the core server\nIf you are just starting out, '1core' or the php version is easiest to start with. Use 'core' if you want to scale computer resources as the index grows or if you have a lot of available CPU cores. Make sure to read the scaling section. \nYou can run the core server on startup with a cron job.\nSet Administrator Password for the Web Interface\nThere is no default web login, you will have to set this manually the first time:\nRename the /html/hash folder to something private.\n\nEdit html/private_folder_name/hashmake.php and change 'secretpassword' to your preferred admin password. \n\nAccess /private_folder_name/hashmake.php from your browser and copy down the hash.\n\nAfter you have copied it down, delete or remove hashmake.php from your web server folder so that the hash cannot be discovered.\n\nLogin to MySQL and create the account:\nuse wiby;\nINSERT INTO accounts (name,hash,level) VALUES('your_username','your_password_hash','admin');\n\nYou can now access /accounts/ from your browser, login to create and manage all accounts for administrators and guardians of the search engine.\n\n\nadmin - Can access all web forms for the search engine and use the /accounts/ page to create and delete accounts.\n\n\nguardian - The main role of a guardian is to gatekeep the index of the search engine. Can access all forms except for /readf/, and can only use the /accounts/ page to change their own password.\n\nControlling the Search Engine\n\nThere are several forms to control the search engine. There is no central form linking everything together, just a collection of different folders that you can rename if you want.\n/submit/ This public facing form allows users of the search engine to submit websites for indexing, provided they comply with your submission criteria, which you can modify on /submit/form.html.php.\n/accounts/\nThis is the account management page. Admins have options to create, lock, change account type, delete, and reset passwords. Guardians have the option to change their password.\n/review/ This is the most important form, intended for you to verify website submissions meet your criteria. Up to 10 pages are assigned to each guardian or admin that accesses the form. The pages will remain assigned to that account for up to 30 minutes. \nFrom here you can control how much, how deep, and how often the web crawler will access each submission. Here is an example of the available options for a website submission:\n\n\nurl_that_was_submitted\n\n[Worksafe]\n\t    [Surprise]\n            [Skip]\n            [Bury]\n\t    [Deny]\n\t    [Updatable]\n\t\t\n\t\t[Crawl: Depth \n\t\tPages \n\t\tType \n\t\tEnforce Rules\n\t\tRepeat]\n\n\nExplanation of the above options:\n\n\nWorksafe - Indicates if the website is safe for work. Set by the user who submitted the website, however you can change it based on your determination.\n\n\nSurprise - Checking this box will put it in the \"surprise me\" feature, where users get redirected to random websites when they click \"surprise me\". Note that this feature won't show NSFW websites even if they are set to surprise.\n\n\nSkip - Selecting this option will skip indexing the page and it will reappear on the review form after you submit the rest of the pages for crawling.\n\n\nBury - Selecting this will move the page to a grave yard (/grave/), a holding place with the same options as /review/ for websites that might have stopped working but that you suspect may come back online. The crawler will detect this automatically and send the page back into review. When you click on the link and see a 404, you can be assured the crawler sent it back to review after failing two update cycles. This also happens if the title of the page changes. The crawler will only do this for pages directly submitted by people. This curtesy is not given to websites that are automatically crawled but then fail to work later on. For those sites, after two failed update cycles, the page will be removed.\n\n\nDeny - Select this to drop the page from being indexed. If the page does not meet your submission criteria, this would be the option to remove it from the queue.\n\n\nUpdatable - The update cycle for the web crawler to return to the page. This only applies to pages submitted by people, pages found by link crawling always go on a 1 week update cycle.\n\n\n------------------- Crawl ------------------- \n\nThe options listed below control how the crawler indexes hyperlinks on the website. By default, the crawler does not index any hyperlinks, it will only index the page that is submitted.\n\n\nDepth  - How many layers of links to crawl through. You must set at least a depth of 1 if you want to crawl any hyperlinks. Setting a negative value = no limit. Be careful about that.\n\n\nPages - How many pages to crawl on each link layer (depth). They will be randomly selected. You must set at least 1 if you want to crawl any hyperlinks. Setting a negative value = no limit. Be careful about that.\n\n\nType - Indicates if you want to only crawl links local to the website, or links external to the website, or both.\n\n\nEnforce rules - This is a blunt tool that checks if pages have more than two scripts and/or css files. If the limit is exceded, the page will not be indexed. I don't use it and prefer to manually check based on more forgiving criteria.\n\n\nRepeat - While the crawler will always return to update each page in the index, it wont crawl through hyperlinks again unless you tell it to. Even so, it only crawls hyperlinks on the page at a depth of 1 when repeat is selected.\n/ban/\nYou can delete or ban individual URL's from the index with this form. Its pretty simple as I don't use it much. You can't delete an entire domain with it, for that you can build your own query in the MySQL console.\n/bulksubmit/\nAdmins/Guardians can import a list of URLs into the review queue with this form.\n/feedback/\nUsers can submit feedback for you with this form.\n/readf/\nWhere admin accounts can read feedback submitted by users.\n/grave/\nIt has the same features as /review/. Websites that you don't yet want to index but don't want to forget about are stored inside /grave/ by selecting 'bury' from inside /review/. The web crawler will (only for pages submitted directly by people), move 404'd pages or pages where the title has changed back to /review/ after two update cycles \nwhere the page does not return to normal. So after a few weeks you may notice dead pages appearing in /review/, you can decide to drop the page or to bury it where it will be moved to /grave/. The page might go back to normal at some point and you can check /grave/ to see if it resurrects. \n/insert/\nThis was the first form created back in late 2016 to populate the Wiby index and see if the search engine could even work as a proof of concept. It was meant to manually enter pages into the index as no crawler existed yet. \nIt is still useful if you want to manually index a page that refuses to permit the crawler to access it. In that case, set updatable to 0.\n/tags/\nIf you want to force a website to appear at the top rank for specific single word queries (like \"weather\"), you can force it by tagging the words to the target url.\n/json/\nThis is the JSON API developers can use to connect their services to the search engine. Instructions are located at that location.\nAdditional Notes\nIf you need to stop the web crawler in a situation where it was accidently queued to index an unlimited number of pages, first stop the crawler program, truncate the indexqueue table 'truncate indexqueue;', then restart the crawler.\n\nScaling the Search Engine\n\nYou can help ensure sub-second search queries as your index grows by building MySQL replica servers on a local network close to eachother, run the core application AND replication tracker (rt) on one or more replica servers and point your reverse proxy to use it. \nEdit the servers.csv file for rt to indicate all available replica servers. If you have a machine with a huge amount of resources and cores, entering multiple duplicate entries to the same sever inside servers.csv (e.g. one for each core) works also.\n\n\nThe core application checks the replication tracker (rt) output to determine if any replicas are online, it will initiate a connection on those replicas and task each one to search a different section of the index, \ndrastically speeding up search speeds especially for multi-word queries. By default, single-word queries will not initiate multiple connections across replicas. To enable that on single-word queries, comment out the IF statement \non line 373 and rebuild the core application.\n\n\nThe number of available replicas must divide evenly into the search results per page limit (lim), OR, the search results per page limit must divide evenly into the number of available replicas. If there \nis an excess of available replicas such that they do not divide evenly, those will remain in synch but will not be used for searches unless another replica fails. You can adjust the search results per page limit (lim) to a different value (default 12), \nand then rebuild to make excess available replicas divide evenly (if necessary).\n\n\nThe reverse proxy and replica servers can be connected through a VPN such as wireguard or openvpn, however the IPs for servers.csv should be the local IPs for the LAN \nthe replicas are all connected on. Here is a tutorial for setting up MySQL replicas.\n\nFull instructions below:\n\nOn the primary server add these lines to my.cnf under [mysqld] but only once you have a VPN to reach your replicas. Replace my.vpn.ip with your own.\n#setting up replication below\nbind-address = 127.0.0.1,my.vpn.ip\nserver-id = 1\nlog_bin = /var/log/mysql/mysql-bin.log\nbinlog_do_db = wiby\nbinlog_format = mixed\n\nIn MySQL on the primary server, create a user for replica access:\ncreate user 'slave_user'@'%' identified by 'd0gemuchw0w';\nGRANT REPLICATION SLAVE ON *.* TO 'slave_user'@'%';\nFLUSH PRIVILEGES;\n\nOn the replica server, ensure the following my.cnf configuration, set the server-id as a unique id for each replica, then restart mysql:\n[client]\ndefault-character-set=utf8mb4\n\n[mysql]\ndefault-character-set = utf8mb4\n\n[mysqld]\nmax_connections = 400\nft_min_word_len=2\nsql_mode = \"NO_BACKSLASH_ESCAPES\"\n#character-set-client-handshake = FALSE\ncharacter-set-server = utf8mb4\ncollation-server = utf8mb4_0900_ai_ci\nskip-character-set-client-handshake\ndefault-authentication-plugin=mysql_native_password\nwait_timeout = 800\n\n#memory use settings, you should adjust this based on your hardware\ninnodb_buffer_pool_size = 1342177280\ninnodb_buffer_pool_instances = 2\n\n#setting up replication below\nbind-address = 0.0.0.0\nserver-id = 2\nrelay_log_info_repository = TABLE\nrelay_log_recovery = ON\nsync_binlog=1\n\nMake sure only VPN and VLAN addresses can reach your replicas. The bind address of 0.0.0.0 can be replaced with '127.0.0.1,replica.vpn.ip' which is safer but also more crash prone if the VPN address is not available on startup.\n\n\nTo export the database to the replica server, on the primary server, stop the web crawler and hide any web forms that can accept new data, then open MySQL and do the following.\nUSE wiby;\nFLUSH TABLES WITH READ LOCK;\nSHOW MASTER STATUS;\n\n+------------------+----------+--------------+------------------+-------------------+\n| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |\n+------------------+----------+--------------+------------------+-------------------+\n| mysql-bin.000055 | 15871269 | wiby         |                  |                   |\n+------------------+----------+--------------+------------------+-------------------+\n\nKeep the above session window open (or run it in a screen session).\n\nCopy down the information from that table. In a separate session window, export the database:\nmysqldump -u root -p wiby > wiby.sql\n\nOnce you have exported the database and recorded what you need, you can unlock the tables, and resume as normal. On the session window displaying the master status:\nUNLOCK TABLES;\n\nYou can now close that window if you want.\n\n\nOn the replica server, login to MySQL and create the database:\nCREATE DATABASE wiby;\nEXIT;\n\nImport the database:\nmysql -u root -p wiby < wiby.sql\n\nLogin to MySQL and type the following but replace the primary_server_ip, MASTER_LOG_FILE, and MASTER_LOG_POS with yours from the table:\nCHANGE MASTER TO MASTER_HOST='primary_server_ip',MASTER_USER='slave_user', MASTER_PASSWORD='d0gemuchw0w', MASTER_LOG_FILE='mysql-bin.000055', MASTER_LOG_POS=15871269;\nSTART SLAVE;\n\nTo verify that the replica is syncronized, type the following on the replica in MySQL:\nSHOW SLAVE STATUS\\G\n\nMake sure that:\nSlave_IO_Running: Yes\nSlave_SQL_Running: Yes\n\nIn MySQL on the replica, create the accounts required for the replication tracker and core application:\nuse wiby;\ncreate user 'remote_guest'@'%' identified by 'd0gemuchw0w';\ngrant select on windex to 'remote_guest'@'%';\ncreate user 'guest'@'localhost' identified by 'qwer';\ngrant select on windex to 'guest'@'localhost';\nFLUSH PRIVILEGES;\n\n\nLoad Balancing\nYou should run the core application on one or more of your replicas and have nginx send traffic to it, this way you can reduce the burden on your VPS. The replication tracker (rt) must run on the same server \nand directory that the core application is running on (not required for 1core).\n\n\nAdd the replica server's VPN address/port to upstream remote_core {} from the default config for nginx (see the provided example template). You can use the VPS as a backup instead by adding 'backup' to its address (eg: server 127.0.0.1:8080 backup;)\n\n","length":24238,"excerpt":"The source code and instructions to create your own version of Wiby.","byline":null,"dir":null,"siteName":null,"lang":"en"},"finalizedMeta":{"title":"Build your own Search Engine","description":"The source code and instructions to create your own version of Wiby.","author":false,"creator":"","publisher":false,"date":"2022-11-07T05:32:19.794Z","topics":[]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"Build your own Search Engine","description":"The source code and instructions to create your own version of Wiby.","canonical":"https://wiby.me/about/guide.html","keywords":[],"image":false,"firstParagraph":"\nOverview\n\nInstallation\n\nControlling\n\nScaling\n\n"},"dublinCore":{},"opengraph":{"title":false,"description":false,"url":false,"site_name":false,"locale":false,"type":false,"typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":false},"twitter":{"site":false,"description":false,"card":false,"creator":false,"title":false,"image":false},"archivedData":{"link":"https://web.archive.org/web/20221102202830/https://wiby.me/about/guide.html","wayback":"https://web.archive.org/web/20221102202830/https://wiby.me/about/guide.html"}}}