{"initialLink":"https://www.techpolicy.press/considering-the-human-rights-impacts-of-llm-content-moderation/","sanitizedLink":"https://www.techpolicy.press/considering-the-human-rights-impacts-of-llm-content-moderation/","finalLink":"https://techpolicy.press/considering-the-human-rights-impacts-of-llm-content-moderation","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://techpolicy.press/considering-the-human-rights-impacts-of-llm-content-moderation\" itemprop=\"url\">Considering the Human Rights Impacts of LLM Content Moderation</a></contexter-box-head></contexter-box-head><contexter-byline class=\"p-author author\" slot=\"author\"><span class=\"p-name byline\" rel=\"author\" itemprop=\"author\">Justin Hendrix</span></contexter-byline><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2025-07-06T14:08:55.749Z\">7/6/2025</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>A conversation with ECNL&apos;s Marlena Wisniak on her report, Algorithmic Gatekeepers: The Human Rights Impacts of LLM Content Moderation.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a is=\"contexter-link\" href=\"https://techpolicy.press/considering-the-human-rights-impacts-of-llm-content-moderation\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"8378a6a6fd21aa0fb38a9c5ab7bc7e163ac32251","data":{"originalLink":"https://www.techpolicy.press/considering-the-human-rights-impacts-of-llm-content-moderation/","sanitizedLink":"https://www.techpolicy.press/considering-the-human-rights-impacts-of-llm-content-moderation/","canonical":"https://techpolicy.press/considering-the-human-rights-impacts-of-llm-content-moderation","htmlText":"<!DOCTYPE html><html lang=\"en\"><head><title>Considering the Human Rights Impacts of LLM Content Moderation | TechPolicy.Press</title><meta name=\"robots\" content=\"index,follow\"/><meta name=\"description\" content=\"A conversation with ECNL&#x27;s Marlena Wisniak on her report, Algorithmic Gatekeepers: The Human Rights Impacts of LLM Content Moderation.\"/><meta name=\"twitter:card\" content=\"summary_large_image\"/><meta name=\"twitter:site\" content=\"@TechPolicyPress\"/><meta name=\"twitter:creator\" content=\"@TechPolicyPress\"/><meta property=\"og:title\" content=\"Considering the Human Rights Impacts of LLM Content Moderation | TechPolicy.Press\"/><meta property=\"og:description\" content=\"A conversation with ECNL&#x27;s Marlena Wisniak on her report, Algorithmic Gatekeepers: The Human Rights Impacts of LLM Content Moderation.\"/><meta property=\"og:url\" content=\"https://techpolicy.press/considering-the-human-rights-impacts-of-llm-content-moderation\"/><meta property=\"og:type\" content=\"article\"/><meta property=\"article:published_time\" content=\"2025-07-06T14:08:55.749Z\"/><meta property=\"article:modified_time\" content=\"2025-07-06T18:49:48Z\"/><meta property=\"article:author\" content=\"http://techpolicy.press/author/justin-hendrix\"/><meta property=\"og:image\" content=\"https://cdn.sanity.io/images/3tzzh18d/production/ca8393b380961a234123ad924f89ed855aed0c1d-1200x675.png\"/><meta property=\"og:locale\" content=\"en_US\"/><meta property=\"og:site_name\" content=\"Tech Policy Press\"/><link rel=\"canonical\" href=\"https://techpolicy.press/considering-the-human-rights-impacts-of-llm-content-moderation\"/><script type=\"application/ld+json\">{\"@context\":\"https://schema.org\",\"@type\":\"Article\",\"datePublished\":\"2025-07-06T14:08:55.749Z\",\"description\":\"A conversation with ECNL&apos;s Marlena Wisniak on her report, Algorithmic Gatekeepers: The Human Rights Impacts of LLM Content Moderation.\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https://techpolicy.press/considering-the-human-rights-impacts-of-llm-content-moderation\"},\"headline\":\"Considering the Human Rights Impacts of LLM Content Moderation\",\"dateModified\":\"2025-07-06T18:49:48Z\",\"author\":[{\"@type\":\"Person\",\"name\":\"Justin Hendrix\",\"url\":\"https://techpolicy.press/author/justin-hendrix\"}],\"publisher\":{\"@type\":\"Organization\",\"name\":\"Tech Policy Press\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https://cdn.sanity.io/images/3tzzh18d/production/697d4cc6122b80fcb64b256d888010c242ce6beb-1200x675.png\"}},\"isAccessibleForFree\":true}</script><meta charSet=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width, initialScale=1.0\"/><meta name=\"google\" content=\"notranslate\"/><meta name=\"author\" content=\"Justin Hendrix\"/><meta name=\"publish_date\" content=\"2025-07-06T14:08:55.749Z\"/><link rel=\"alternate\" type=\"application/rss+xml\" title=\"RSS feed for The Sunday Show\" href=\"https://feeds.captivate.fm/techpolicypress/\"/><link rel=\"alternate\" type=\"application/rss+xml\" title=\"Tech Policy Press » Feed\" href=\"https://techpolicy.press/rss/feed.xml\"/><link rel=\"icon\" href=\"https://cdn.sanity.io/images/3tzzh18d/production/9a2224d300c1699fc1b87235aac36318e2c76cec-867x867.png\"/><meta name=\"next-head-count\" content=\"28\"/><link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin /><link rel=\"preload\" href=\"/_next/static/css/76b6c2647651fec1.css\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/76b6c2647651fec1.css\" data-n-g=\"\"/><link rel=\"preload\" href=\"/_next/static/css/907c8b40563e8791.css\" as=\"style\"/><link rel=\"stylesheet\" href=\"/_next/static/css/907c8b40563e8791.css\" data-n-p=\"\"/><noscript data-n-css=\"\"></noscript><script defer=\"\" nomodule=\"\" src=\"/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js\"></script><script src=\"/_next/static/chunks/webpack-3c216928ee00e5f9.js\" defer=\"\"></script><script src=\"/_next/static/chunks/framework-9620da855a94eb57.js\" defer=\"\"></script><script src=\"/_next/static/chunks/main-89688f241e62cd05.js\" defer=\"\"></script><script src=\"/_next/static/chunks/pages/_app-46afd193c62360b9.js\" defer=\"\"></script><script src=\"/_next/static/chunks/29107295-a3480e51fe70b9c7.js\" defer=\"\"></script><script src=\"/_next/static/chunks/219-0069b325e47ba8ea.js\" defer=\"\"></script><script src=\"/_next/static/chunks/798-ca308619095a183b.js\" defer=\"\"></script><script src=\"/_next/static/chunks/618-c554726c7f36ee82.js\" defer=\"\"></script><script src=\"/_next/static/chunks/pages/%5B...slug%5D-a2ca510dd599acd8.js\" defer=\"\"></script><script src=\"/_next/static/53BaSRGe9oJ7dm7CovmVM/_buildManifest.js\" defer=\"\"></script><script src=\"/_next/static/53BaSRGe9oJ7dm7CovmVM/_ssgManifest.js\" defer=\"\"></script><style id=\"jss-server-side\">.jss1 {\n  font-style: italic;\n}\n.jss3 {\n  color: #000 !important;\n  text-decoration: none!important;\n}\n.jss4 {\n  color: unset;\n  text-decoration: none;\n}\n.jss6 {\n  width: 100%;\n  display: block;\n  max-width: 20px;\n  max-height: 20px;\n}\n.jss9 {\n  font-family: 'Lexend', sans-serif;\n  font-weight: 500;\n  text-transform: uppercase;\n}\n.jss9:active, .jss9:focus, .jss9:hover {\n  text-decoration: underline;\n}\n.jss7 {\n  gap: 10px;\n  display: flex;\n  flex-direction: row;\n}\n@media (max-width:599.95px) {\n  .jss7 {\n    align-items: center;\n    flex-direction: column;\n  }\n}\n  .jss8 {\n    display: flex;\n    font-size: 16px;\n    font-family: 'Libre Baskerville', serif;\n    font-weight: 700;\n    line-height: 1.5;\n    flex-direction: column;\n  }\n  .jss8>a:hover {\n    text-decoration: underline;\n  }\n  .jss8:not(:last-child) {\n    border-bottom: 1px solid #E2D7BB77;\n    padding-bottom: 10px;\n  }</style><style data-href=\"https://fonts.googleapis.com/css?family=Roboto:400,400i,500,700,700i&display=swap\">@font-face{font-family:'Roboto';font-style:italic;font-weight:400;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFOKCnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmOClHrs6ljXfMMLoHQuAj-lQ.woff) format('woff')}@font-face{font-family:'Roboto';font-style:italic;font-weight:700;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFOKCnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmOClHrs6ljXfMMLmbXuAj-lQ.woff) format('woff')}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFOMCnqEu92Fr1ME7kSn66aGLdTylUAMQXC89YmC2DPNWubEbVmUiAw.woff) format('woff')}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFOMCnqEu92Fr1ME7kSn66aGLdTylUAMQXC89YmC2DPNWub2bVmUiAw.woff) format('woff')}@font-face{font-family:'Roboto';font-style:normal;font-weight:700;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFOMCnqEu92Fr1ME7kSn66aGLdTylUAMQXC89YmC2DPNWuYjalmUiAw.woff) format('woff')}@font-face{font-family:'Roboto';font-style:italic;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkC3kaSTbQWt4N.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Roboto';font-style:italic;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkAnkaSTbQWt4N.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Roboto';font-style:italic;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkCnkaSTbQWt4N.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Roboto';font-style:italic;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkBXkaSTbQWt4N.woff2) format('woff2');unicode-range:U+0370-0377,U+037A-037F,U+0384-038A,U+038C,U+038E-03A1,U+03A3-03FF}@font-face{font-family:'Roboto';font-style:italic;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkenkaSTbQWt4N.woff2) format('woff2');unicode-range:U+0302-0303,U+0305,U+0307-0308,U+0310,U+0312,U+0315,U+031A,U+0326-0327,U+032C,U+032F-0330,U+0332-0333,U+0338,U+033A,U+0346,U+034D,U+0391-03A1,U+03A3-03A9,U+03B1-03C9,U+03D1,U+03D5-03D6,U+03F0-03F1,U+03F4-03F5,U+2016-2017,U+2034-2038,U+203C,U+2040,U+2043,U+2047,U+2050,U+2057,U+205F,U+2070-2071,U+2074-208E,U+2090-209C,U+20D0-20DC,U+20E1,U+20E5-20EF,U+2100-2112,U+2114-2115,U+2117-2121,U+2123-214F,U+2190,U+2192,U+2194-21AE,U+21B0-21E5,U+21F1-21F2,U+21F4-2211,U+2213-2214,U+2216-22FF,U+2308-230B,U+2310,U+2319,U+231C-2321,U+2336-237A,U+237C,U+2395,U+239B-23B7,U+23D0,U+23DC-23E1,U+2474-2475,U+25AF,U+25B3,U+25B7,U+25BD,U+25C1,U+25CA,U+25CC,U+25FB,U+266D-266F,U+27C0-27FF,U+2900-2AFF,U+2B0E-2B11,U+2B30-2B4C,U+2BFE,U+3030,U+FF5B,U+FF5D,U+1D400-1D7FF,U+1EE00-1EEFF}@font-face{font-family:'Roboto';font-style:italic;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkaHkaSTbQWt4N.woff2) format('woff2');unicode-range:U+0001-000C,U+000E-001F,U+007F-009F,U+20DD-20E0,U+20E2-20E4,U+2150-218F,U+2190,U+2192,U+2194-2199,U+21AF,U+21E6-21F0,U+21F3,U+2218-2219,U+2299,U+22C4-22C6,U+2300-243F,U+2440-244A,U+2460-24FF,U+25A0-27BF,U+2800-28FF,U+2921-2922,U+2981,U+29BF,U+29EB,U+2B00-2BFF,U+4DC0-4DFF,U+FFF9-FFFB,U+10140-1018E,U+10190-1019C,U+101A0,U+101D0-101FD,U+102E0-102FB,U+10E60-10E7E,U+1D2C0-1D2D3,U+1D2E0-1D37F,U+1F000-1F0FF,U+1F100-1F1AD,U+1F1E6-1F1FF,U+1F30D-1F30F,U+1F315,U+1F31C,U+1F31E,U+1F320-1F32C,U+1F336,U+1F378,U+1F37D,U+1F382,U+1F393-1F39F,U+1F3A7-1F3A8,U+1F3AC-1F3AF,U+1F3C2,U+1F3C4-1F3C6,U+1F3CA-1F3CE,U+1F3D4-1F3E0,U+1F3ED,U+1F3F1-1F3F3,U+1F3F5-1F3F7,U+1F408,U+1F415,U+1F41F,U+1F426,U+1F43F,U+1F441-1F442,U+1F444,U+1F446-1F449,U+1F44C-1F44E,U+1F453,U+1F46A,U+1F47D,U+1F4A3,U+1F4B0,U+1F4B3,U+1F4B9,U+1F4BB,U+1F4BF,U+1F4C8-1F4CB,U+1F4D6,U+1F4DA,U+1F4DF,U+1F4E3-1F4E6,U+1F4EA-1F4ED,U+1F4F7,U+1F4F9-1F4FB,U+1F4FD-1F4FE,U+1F503,U+1F507-1F50B,U+1F50D,U+1F512-1F513,U+1F53E-1F54A,U+1F54F-1F5FA,U+1F610,U+1F650-1F67F,U+1F687,U+1F68D,U+1F691,U+1F694,U+1F698,U+1F6AD,U+1F6B2,U+1F6B9-1F6BA,U+1F6BC,U+1F6C6-1F6CF,U+1F6D3-1F6D7,U+1F6E0-1F6EA,U+1F6F0-1F6F3,U+1F6F7-1F6FC,U+1F700-1F7FF,U+1F800-1F80B,U+1F810-1F847,U+1F850-1F859,U+1F860-1F887,U+1F890-1F8AD,U+1F8B0-1F8BB,U+1F8C0-1F8C1,U+1F900-1F90B,U+1F93B,U+1F946,U+1F984,U+1F996,U+1F9E9,U+1FA00-1FA6F,U+1FA70-1FA7C,U+1FA80-1FA89,U+1FA8F-1FAC6,U+1FACE-1FADC,U+1FADF-1FAE9,U+1FAF0-1FAF8,U+1FB00-1FBFF}@font-face{font-family:'Roboto';font-style:italic;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkCXkaSTbQWt4N.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Roboto';font-style:italic;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkCHkaSTbQWt4N.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Roboto';font-style:italic;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkBnkaSTbQWg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Roboto';font-style:italic;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkC3kaSTbQWt4N.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Roboto';font-style:italic;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkAnkaSTbQWt4N.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Roboto';font-style:italic;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkCnkaSTbQWt4N.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Roboto';font-style:italic;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkBXkaSTbQWt4N.woff2) format('woff2');unicode-range:U+0370-0377,U+037A-037F,U+0384-038A,U+038C,U+038E-03A1,U+03A3-03FF}@font-face{font-family:'Roboto';font-style:italic;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkenkaSTbQWt4N.woff2) format('woff2');unicode-range:U+0302-0303,U+0305,U+0307-0308,U+0310,U+0312,U+0315,U+031A,U+0326-0327,U+032C,U+032F-0330,U+0332-0333,U+0338,U+033A,U+0346,U+034D,U+0391-03A1,U+03A3-03A9,U+03B1-03C9,U+03D1,U+03D5-03D6,U+03F0-03F1,U+03F4-03F5,U+2016-2017,U+2034-2038,U+203C,U+2040,U+2043,U+2047,U+2050,U+2057,U+205F,U+2070-2071,U+2074-208E,U+2090-209C,U+20D0-20DC,U+20E1,U+20E5-20EF,U+2100-2112,U+2114-2115,U+2117-2121,U+2123-214F,U+2190,U+2192,U+2194-21AE,U+21B0-21E5,U+21F1-21F2,U+21F4-2211,U+2213-2214,U+2216-22FF,U+2308-230B,U+2310,U+2319,U+231C-2321,U+2336-237A,U+237C,U+2395,U+239B-23B7,U+23D0,U+23DC-23E1,U+2474-2475,U+25AF,U+25B3,U+25B7,U+25BD,U+25C1,U+25CA,U+25CC,U+25FB,U+266D-266F,U+27C0-27FF,U+2900-2AFF,U+2B0E-2B11,U+2B30-2B4C,U+2BFE,U+3030,U+FF5B,U+FF5D,U+1D400-1D7FF,U+1EE00-1EEFF}@font-face{font-family:'Roboto';font-style:italic;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkaHkaSTbQWt4N.woff2) format('woff2');unicode-range:U+0001-000C,U+000E-001F,U+007F-009F,U+20DD-20E0,U+20E2-20E4,U+2150-218F,U+2190,U+2192,U+2194-2199,U+21AF,U+21E6-21F0,U+21F3,U+2218-2219,U+2299,U+22C4-22C6,U+2300-243F,U+2440-244A,U+2460-24FF,U+25A0-27BF,U+2800-28FF,U+2921-2922,U+2981,U+29BF,U+29EB,U+2B00-2BFF,U+4DC0-4DFF,U+FFF9-FFFB,U+10140-1018E,U+10190-1019C,U+101A0,U+101D0-101FD,U+102E0-102FB,U+10E60-10E7E,U+1D2C0-1D2D3,U+1D2E0-1D37F,U+1F000-1F0FF,U+1F100-1F1AD,U+1F1E6-1F1FF,U+1F30D-1F30F,U+1F315,U+1F31C,U+1F31E,U+1F320-1F32C,U+1F336,U+1F378,U+1F37D,U+1F382,U+1F393-1F39F,U+1F3A7-1F3A8,U+1F3AC-1F3AF,U+1F3C2,U+1F3C4-1F3C6,U+1F3CA-1F3CE,U+1F3D4-1F3E0,U+1F3ED,U+1F3F1-1F3F3,U+1F3F5-1F3F7,U+1F408,U+1F415,U+1F41F,U+1F426,U+1F43F,U+1F441-1F442,U+1F444,U+1F446-1F449,U+1F44C-1F44E,U+1F453,U+1F46A,U+1F47D,U+1F4A3,U+1F4B0,U+1F4B3,U+1F4B9,U+1F4BB,U+1F4BF,U+1F4C8-1F4CB,U+1F4D6,U+1F4DA,U+1F4DF,U+1F4E3-1F4E6,U+1F4EA-1F4ED,U+1F4F7,U+1F4F9-1F4FB,U+1F4FD-1F4FE,U+1F503,U+1F507-1F50B,U+1F50D,U+1F512-1F513,U+1F53E-1F54A,U+1F54F-1F5FA,U+1F610,U+1F650-1F67F,U+1F687,U+1F68D,U+1F691,U+1F694,U+1F698,U+1F6AD,U+1F6B2,U+1F6B9-1F6BA,U+1F6BC,U+1F6C6-1F6CF,U+1F6D3-1F6D7,U+1F6E0-1F6EA,U+1F6F0-1F6F3,U+1F6F7-1F6FC,U+1F700-1F7FF,U+1F800-1F80B,U+1F810-1F847,U+1F850-1F859,U+1F860-1F887,U+1F890-1F8AD,U+1F8B0-1F8BB,U+1F8C0-1F8C1,U+1F900-1F90B,U+1F93B,U+1F946,U+1F984,U+1F996,U+1F9E9,U+1FA00-1FA6F,U+1FA70-1FA7C,U+1FA80-1FA89,U+1FA8F-1FAC6,U+1FACE-1FADC,U+1FADF-1FAE9,U+1FAF0-1FAF8,U+1FB00-1FBFF}@font-face{font-family:'Roboto';font-style:italic;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkCXkaSTbQWt4N.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Roboto';font-style:italic;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkCHkaSTbQWt4N.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Roboto';font-style:italic;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO5CnqEu92Fr1Mu53ZEC9_Vu3r1gIhOszmkBnkaSTbQWg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3GUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3iUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3CUBHMdazTgWw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3-UBHMdazTgWw.woff2) format('woff2');unicode-range:U+0370-0377,U+037A-037F,U+0384-038A,U+038C,U+038E-03A1,U+03A3-03FF}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMawCUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0302-0303,U+0305,U+0307-0308,U+0310,U+0312,U+0315,U+031A,U+0326-0327,U+032C,U+032F-0330,U+0332-0333,U+0338,U+033A,U+0346,U+034D,U+0391-03A1,U+03A3-03A9,U+03B1-03C9,U+03D1,U+03D5-03D6,U+03F0-03F1,U+03F4-03F5,U+2016-2017,U+2034-2038,U+203C,U+2040,U+2043,U+2047,U+2050,U+2057,U+205F,U+2070-2071,U+2074-208E,U+2090-209C,U+20D0-20DC,U+20E1,U+20E5-20EF,U+2100-2112,U+2114-2115,U+2117-2121,U+2123-214F,U+2190,U+2192,U+2194-21AE,U+21B0-21E5,U+21F1-21F2,U+21F4-2211,U+2213-2214,U+2216-22FF,U+2308-230B,U+2310,U+2319,U+231C-2321,U+2336-237A,U+237C,U+2395,U+239B-23B7,U+23D0,U+23DC-23E1,U+2474-2475,U+25AF,U+25B3,U+25B7,U+25BD,U+25C1,U+25CA,U+25CC,U+25FB,U+266D-266F,U+27C0-27FF,U+2900-2AFF,U+2B0E-2B11,U+2B30-2B4C,U+2BFE,U+3030,U+FF5B,U+FF5D,U+1D400-1D7FF,U+1EE00-1EEFF}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMaxKUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0001-000C,U+000E-001F,U+007F-009F,U+20DD-20E0,U+20E2-20E4,U+2150-218F,U+2190,U+2192,U+2194-2199,U+21AF,U+21E6-21F0,U+21F3,U+2218-2219,U+2299,U+22C4-22C6,U+2300-243F,U+2440-244A,U+2460-24FF,U+25A0-27BF,U+2800-28FF,U+2921-2922,U+2981,U+29BF,U+29EB,U+2B00-2BFF,U+4DC0-4DFF,U+FFF9-FFFB,U+10140-1018E,U+10190-1019C,U+101A0,U+101D0-101FD,U+102E0-102FB,U+10E60-10E7E,U+1D2C0-1D2D3,U+1D2E0-1D37F,U+1F000-1F0FF,U+1F100-1F1AD,U+1F1E6-1F1FF,U+1F30D-1F30F,U+1F315,U+1F31C,U+1F31E,U+1F320-1F32C,U+1F336,U+1F378,U+1F37D,U+1F382,U+1F393-1F39F,U+1F3A7-1F3A8,U+1F3AC-1F3AF,U+1F3C2,U+1F3C4-1F3C6,U+1F3CA-1F3CE,U+1F3D4-1F3E0,U+1F3ED,U+1F3F1-1F3F3,U+1F3F5-1F3F7,U+1F408,U+1F415,U+1F41F,U+1F426,U+1F43F,U+1F441-1F442,U+1F444,U+1F446-1F449,U+1F44C-1F44E,U+1F453,U+1F46A,U+1F47D,U+1F4A3,U+1F4B0,U+1F4B3,U+1F4B9,U+1F4BB,U+1F4BF,U+1F4C8-1F4CB,U+1F4D6,U+1F4DA,U+1F4DF,U+1F4E3-1F4E6,U+1F4EA-1F4ED,U+1F4F7,U+1F4F9-1F4FB,U+1F4FD-1F4FE,U+1F503,U+1F507-1F50B,U+1F50D,U+1F512-1F513,U+1F53E-1F54A,U+1F54F-1F5FA,U+1F610,U+1F650-1F67F,U+1F687,U+1F68D,U+1F691,U+1F694,U+1F698,U+1F6AD,U+1F6B2,U+1F6B9-1F6BA,U+1F6BC,U+1F6C6-1F6CF,U+1F6D3-1F6D7,U+1F6E0-1F6EA,U+1F6F0-1F6F3,U+1F6F7-1F6FC,U+1F700-1F7FF,U+1F800-1F80B,U+1F810-1F847,U+1F850-1F859,U+1F860-1F887,U+1F890-1F8AD,U+1F8B0-1F8BB,U+1F8C0-1F8C1,U+1F900-1F90B,U+1F93B,U+1F946,U+1F984,U+1F996,U+1F9E9,U+1FA00-1FA6F,U+1FA70-1FA7C,U+1FA80-1FA89,U+1FA8F-1FAC6,U+1FACE-1FADC,U+1FADF-1FAE9,U+1FAF0-1FAF8,U+1FB00-1FBFF}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3OUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3KUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Roboto';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3yUBHMdazQ.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3GUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3iUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3CUBHMdazTgWw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3-UBHMdazTgWw.woff2) format('woff2');unicode-range:U+0370-0377,U+037A-037F,U+0384-038A,U+038C,U+038E-03A1,U+03A3-03FF}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMawCUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0302-0303,U+0305,U+0307-0308,U+0310,U+0312,U+0315,U+031A,U+0326-0327,U+032C,U+032F-0330,U+0332-0333,U+0338,U+033A,U+0346,U+034D,U+0391-03A1,U+03A3-03A9,U+03B1-03C9,U+03D1,U+03D5-03D6,U+03F0-03F1,U+03F4-03F5,U+2016-2017,U+2034-2038,U+203C,U+2040,U+2043,U+2047,U+2050,U+2057,U+205F,U+2070-2071,U+2074-208E,U+2090-209C,U+20D0-20DC,U+20E1,U+20E5-20EF,U+2100-2112,U+2114-2115,U+2117-2121,U+2123-214F,U+2190,U+2192,U+2194-21AE,U+21B0-21E5,U+21F1-21F2,U+21F4-2211,U+2213-2214,U+2216-22FF,U+2308-230B,U+2310,U+2319,U+231C-2321,U+2336-237A,U+237C,U+2395,U+239B-23B7,U+23D0,U+23DC-23E1,U+2474-2475,U+25AF,U+25B3,U+25B7,U+25BD,U+25C1,U+25CA,U+25CC,U+25FB,U+266D-266F,U+27C0-27FF,U+2900-2AFF,U+2B0E-2B11,U+2B30-2B4C,U+2BFE,U+3030,U+FF5B,U+FF5D,U+1D400-1D7FF,U+1EE00-1EEFF}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMaxKUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0001-000C,U+000E-001F,U+007F-009F,U+20DD-20E0,U+20E2-20E4,U+2150-218F,U+2190,U+2192,U+2194-2199,U+21AF,U+21E6-21F0,U+21F3,U+2218-2219,U+2299,U+22C4-22C6,U+2300-243F,U+2440-244A,U+2460-24FF,U+25A0-27BF,U+2800-28FF,U+2921-2922,U+2981,U+29BF,U+29EB,U+2B00-2BFF,U+4DC0-4DFF,U+FFF9-FFFB,U+10140-1018E,U+10190-1019C,U+101A0,U+101D0-101FD,U+102E0-102FB,U+10E60-10E7E,U+1D2C0-1D2D3,U+1D2E0-1D37F,U+1F000-1F0FF,U+1F100-1F1AD,U+1F1E6-1F1FF,U+1F30D-1F30F,U+1F315,U+1F31C,U+1F31E,U+1F320-1F32C,U+1F336,U+1F378,U+1F37D,U+1F382,U+1F393-1F39F,U+1F3A7-1F3A8,U+1F3AC-1F3AF,U+1F3C2,U+1F3C4-1F3C6,U+1F3CA-1F3CE,U+1F3D4-1F3E0,U+1F3ED,U+1F3F1-1F3F3,U+1F3F5-1F3F7,U+1F408,U+1F415,U+1F41F,U+1F426,U+1F43F,U+1F441-1F442,U+1F444,U+1F446-1F449,U+1F44C-1F44E,U+1F453,U+1F46A,U+1F47D,U+1F4A3,U+1F4B0,U+1F4B3,U+1F4B9,U+1F4BB,U+1F4BF,U+1F4C8-1F4CB,U+1F4D6,U+1F4DA,U+1F4DF,U+1F4E3-1F4E6,U+1F4EA-1F4ED,U+1F4F7,U+1F4F9-1F4FB,U+1F4FD-1F4FE,U+1F503,U+1F507-1F50B,U+1F50D,U+1F512-1F513,U+1F53E-1F54A,U+1F54F-1F5FA,U+1F610,U+1F650-1F67F,U+1F687,U+1F68D,U+1F691,U+1F694,U+1F698,U+1F6AD,U+1F6B2,U+1F6B9-1F6BA,U+1F6BC,U+1F6C6-1F6CF,U+1F6D3-1F6D7,U+1F6E0-1F6EA,U+1F6F0-1F6F3,U+1F6F7-1F6FC,U+1F700-1F7FF,U+1F800-1F80B,U+1F810-1F847,U+1F850-1F859,U+1F860-1F887,U+1F890-1F8AD,U+1F8B0-1F8BB,U+1F8C0-1F8C1,U+1F900-1F90B,U+1F93B,U+1F946,U+1F984,U+1F996,U+1F9E9,U+1FA00-1FA6F,U+1FA70-1FA7C,U+1FA80-1FA89,U+1FA8F-1FAC6,U+1FACE-1FADC,U+1FADF-1FAE9,U+1FAF0-1FAF8,U+1FB00-1FBFF}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3OUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3KUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Roboto';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3yUBHMdazQ.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Roboto';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3GUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Roboto';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3iUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Roboto';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3CUBHMdazTgWw.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Roboto';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3-UBHMdazTgWw.woff2) format('woff2');unicode-range:U+0370-0377,U+037A-037F,U+0384-038A,U+038C,U+038E-03A1,U+03A3-03FF}@font-face{font-family:'Roboto';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMawCUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0302-0303,U+0305,U+0307-0308,U+0310,U+0312,U+0315,U+031A,U+0326-0327,U+032C,U+032F-0330,U+0332-0333,U+0338,U+033A,U+0346,U+034D,U+0391-03A1,U+03A3-03A9,U+03B1-03C9,U+03D1,U+03D5-03D6,U+03F0-03F1,U+03F4-03F5,U+2016-2017,U+2034-2038,U+203C,U+2040,U+2043,U+2047,U+2050,U+2057,U+205F,U+2070-2071,U+2074-208E,U+2090-209C,U+20D0-20DC,U+20E1,U+20E5-20EF,U+2100-2112,U+2114-2115,U+2117-2121,U+2123-214F,U+2190,U+2192,U+2194-21AE,U+21B0-21E5,U+21F1-21F2,U+21F4-2211,U+2213-2214,U+2216-22FF,U+2308-230B,U+2310,U+2319,U+231C-2321,U+2336-237A,U+237C,U+2395,U+239B-23B7,U+23D0,U+23DC-23E1,U+2474-2475,U+25AF,U+25B3,U+25B7,U+25BD,U+25C1,U+25CA,U+25CC,U+25FB,U+266D-266F,U+27C0-27FF,U+2900-2AFF,U+2B0E-2B11,U+2B30-2B4C,U+2BFE,U+3030,U+FF5B,U+FF5D,U+1D400-1D7FF,U+1EE00-1EEFF}@font-face{font-family:'Roboto';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMaxKUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0001-000C,U+000E-001F,U+007F-009F,U+20DD-20E0,U+20E2-20E4,U+2150-218F,U+2190,U+2192,U+2194-2199,U+21AF,U+21E6-21F0,U+21F3,U+2218-2219,U+2299,U+22C4-22C6,U+2300-243F,U+2440-244A,U+2460-24FF,U+25A0-27BF,U+2800-28FF,U+2921-2922,U+2981,U+29BF,U+29EB,U+2B00-2BFF,U+4DC0-4DFF,U+FFF9-FFFB,U+10140-1018E,U+10190-1019C,U+101A0,U+101D0-101FD,U+102E0-102FB,U+10E60-10E7E,U+1D2C0-1D2D3,U+1D2E0-1D37F,U+1F000-1F0FF,U+1F100-1F1AD,U+1F1E6-1F1FF,U+1F30D-1F30F,U+1F315,U+1F31C,U+1F31E,U+1F320-1F32C,U+1F336,U+1F378,U+1F37D,U+1F382,U+1F393-1F39F,U+1F3A7-1F3A8,U+1F3AC-1F3AF,U+1F3C2,U+1F3C4-1F3C6,U+1F3CA-1F3CE,U+1F3D4-1F3E0,U+1F3ED,U+1F3F1-1F3F3,U+1F3F5-1F3F7,U+1F408,U+1F415,U+1F41F,U+1F426,U+1F43F,U+1F441-1F442,U+1F444,U+1F446-1F449,U+1F44C-1F44E,U+1F453,U+1F46A,U+1F47D,U+1F4A3,U+1F4B0,U+1F4B3,U+1F4B9,U+1F4BB,U+1F4BF,U+1F4C8-1F4CB,U+1F4D6,U+1F4DA,U+1F4DF,U+1F4E3-1F4E6,U+1F4EA-1F4ED,U+1F4F7,U+1F4F9-1F4FB,U+1F4FD-1F4FE,U+1F503,U+1F507-1F50B,U+1F50D,U+1F512-1F513,U+1F53E-1F54A,U+1F54F-1F5FA,U+1F610,U+1F650-1F67F,U+1F687,U+1F68D,U+1F691,U+1F694,U+1F698,U+1F6AD,U+1F6B2,U+1F6B9-1F6BA,U+1F6BC,U+1F6C6-1F6CF,U+1F6D3-1F6D7,U+1F6E0-1F6EA,U+1F6F0-1F6F3,U+1F6F7-1F6FC,U+1F700-1F7FF,U+1F800-1F80B,U+1F810-1F847,U+1F850-1F859,U+1F860-1F887,U+1F890-1F8AD,U+1F8B0-1F8BB,U+1F8C0-1F8C1,U+1F900-1F90B,U+1F93B,U+1F946,U+1F984,U+1F996,U+1F9E9,U+1FA00-1FA6F,U+1FA70-1FA7C,U+1FA80-1FA89,U+1FA8F-1FAC6,U+1FACE-1FADC,U+1FADF-1FAE9,U+1FAF0-1FAF8,U+1FB00-1FBFF}@font-face{font-family:'Roboto';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3OUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Roboto';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3KUBHMdazTgWw.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Roboto';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/roboto/v48/KFO7CnqEu92Fr1ME7kSn66aGLdTylUAMa3yUBHMdazQ.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-href=\"https://fonts.googleapis.com/css?family=Libre+Baskerville:700&display=swap\">@font-face{font-family:'Libre Baskerville';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/librebaskerville/v16/kmKiZrc3Hgbbcjq75U4uslyuy4kn0qviTgY3KcY.woff) format('woff')}@font-face{font-family:'Libre Baskerville';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/librebaskerville/v16/kmKiZrc3Hgbbcjq75U4uslyuy4kn0qviTgY5KcC-wLOjAUw.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Libre Baskerville';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/librebaskerville/v16/kmKiZrc3Hgbbcjq75U4uslyuy4kn0qviTgY3KcC-wLOj.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body><div id=\"__next\"><noscript><img src=\"https://sa.recoding.tech/noscript.gif\" alt=\"\" referrerPolicy=\"no-referrer-when-downgrade\"/></noscript><header class=\"jss2\"><style data-emotion=\"css owsmld\">.css-owsmld{padding:32px;margin-bottom:32px;margin-top:12px;box-shadow:0px 2px 1px -1px rgba(0, 0, 0, 0.2);}</style><div class=\"MuiBox-root css-owsmld\"><style data-emotion=\"css vemis6\">.css-vemis6{box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;width:100%;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;margin-top:-24px;width:calc(100% + 24px);margin-left:-24px;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}.css-vemis6>.MuiGrid-item{padding-top:24px;}.css-vemis6>.MuiGrid-item{padding-left:24px;}</style><div class=\"MuiGrid-root MuiGrid-container MuiGrid-spacing-xs-3 css-vemis6\"><style data-emotion=\"css 1y7gpk0\">.css-1y7gpk0{box-sizing:border-box;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-basis:83.333333%;-ms-flex-preferred-size:83.333333%;flex-basis:83.333333%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:83.333333%;}@media (min-width:600px){.css-1y7gpk0{-webkit-flex-basis:83.333333%;-ms-flex-preferred-size:83.333333%;flex-basis:83.333333%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:83.333333%;}}@media (min-width:900px){.css-1y7gpk0{-webkit-flex-basis:16.666667%;-ms-flex-preferred-size:16.666667%;flex-basis:16.666667%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:16.666667%;}}@media (min-width:1200px){.css-1y7gpk0{-webkit-flex-basis:16.666667%;-ms-flex-preferred-size:16.666667%;flex-basis:16.666667%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:16.666667%;}}@media (min-width:1536px){.css-1y7gpk0{-webkit-flex-basis:16.666667%;-ms-flex-preferred-size:16.666667%;flex-basis:16.666667%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:16.666667%;}}</style><div class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-10 MuiGrid-grid-md-2 css-1y7gpk0\"><style data-emotion=\"css tgyajb\">.css-tgyajb{-webkit-text-decoration:underline;text-decoration:underline;text-decoration-color:rgba(0, 0, 0, 0.4);}.css-tgyajb:hover{text-decoration-color:inherit;}</style><style data-emotion=\"css mctcqb\">.css-mctcqb{margin:0;font:inherit;color:#000;-webkit-text-decoration:underline;text-decoration:underline;text-decoration-color:rgba(0, 0, 0, 0.4);}.css-mctcqb:hover{text-decoration-color:inherit;}</style><a class=\"MuiTypography-root MuiTypography-inherit MuiLink-root MuiLink-underlineAlways jss4 css-mctcqb\" href=\"/\"><style data-emotion=\"css 19sf0qu\">.css-19sf0qu{max-width:170px;}</style><div class=\"MuiBox-root css-19sf0qu\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 741.46 299.57\"><path fill=\"#407569\" d=\"M283.82 176.65a23.6 23.6 0 0 0-16.11-11.04 25.91 25.91 0 0 0-17.48 3.01 23.63 23.63 0 0 0-6.88 5.81 22.48 22.48 0 0 0-4.97 17.02 25.77 25.77 0 0 0 15.75 20.49 24.1 24.1 0 0 0 9.18 1.84 23.78 23.78 0 0 0 18.05-8.47 23.75 23.75 0 0 0 4.95-9.3 24.42 24.42 0 0 0-2.49-19.37Z\"></path><path fill=\"#559482\" d=\"M224.98 226.38a21.73 21.73 0 0 0-15.45 5.02l-.34.29a21.7 21.7 0 0 0-7.17 14.6 24.05 24.05 0 0 0 4.25 16.07 22.06 22.06 0 0 0 15.38 9.35l.24.03a22.8 22.8 0 0 0 17.48-5.56 22.84 22.84 0 0 0 7.98-16.52v-.26a22.06 22.06 0 0 0-7.07-16.54 24 24 0 0 0-15.32-6.48Z\"></path><path fill=\"#e5dcc1\" d=\"M171.54 260.92a22.25 22.25 0 0 0-6.53-3.15 20.1 20.1 0 0 0-7.71-.58l-.2.02a18.4 18.4 0 0 0-7.58 2.74 20.62 20.62 0 0 0-5.67 5.07 22.94 22.94 0 0 0-4.77 14.75 20.33 20.33 0 0 0 7.35 15.02 21.38 21.38 0 0 0 7.49 3.98l.27.08a22.49 22.49 0 0 0 8.5.55c2.88-.31 5.68-1.3 8.64-3.07a19.91 19.91 0 0 0 6.28-6.02 20.28 20.28 0 0 0 2.82-16.43 22.61 22.61 0 0 0-8.89-12.95Z\"></path><path fill=\"#1d5d9e\" d=\"M98.01 250.35a16.99 16.99 0 0 0-3.47-1.13 16.55 16.55 0 0 0-3.51-.46 19.4 19.4 0 0 0-6.91 1.12 20.85 20.85 0 0 0-11.23 9.13 18.8 18.8 0 0 0-1.82 15.26 19.2 19.2 0 0 0 3.94 6.89 21.6 21.6 0 0 0 2.92 2.73l.19.14c1.09.78 2.29 1.47 3.49 2.02a19.51 19.51 0 0 0 15.92.35 18.85 18.85 0 0 0 10.39-11.38 21.01 21.01 0 0 0-.39-14.35 18.53 18.53 0 0 0-9.52-10.31Z\"></path><path fill=\"#eebaad\" d=\"m51.52 212.14-.09-.14a16.84 16.84 0 0 0-10.44-7.13 19.19 19.19 0 0 0-13.22 1.51 17.15 17.15 0 0 0-8.95 10.94 17.81 17.81 0 0 0 2.35 14.21 17.32 17.32 0 0 0 5.39 5.48 17.76 17.76 0 0 0 6.74 2.66 17.24 17.24 0 0 0 13.55-3.64 19.08 19.08 0 0 0 6.84-11.37c.45-2.24.49-4.42.12-6.54a17.4 17.4 0 0 0-2.3-5.98Z\"></path><path fill=\"#df1316\" d=\"M32.74 153.88c.09-3.74-1.42-7.4-4.41-10.63a17.67 17.67 0 0 0-10.98-5.37c-4.49-.46-8.77.93-12.01 3.87-3.32 2.89-5.21 7.17-5.34 12.04v.18c.13 4.87 2.02 9.15 5.29 12.02a15.3 15.3 0 0 0 12.05 3.9 17.78 17.78 0 0 0 11.02-5.42c2.95-3.19 4.46-6.85 4.37-10.59Z\"></path><path fill=\"#407569\" d=\"M29.25 98.46a15.9 15.9 0 0 0 10.94 1.26c3.57-.8 6.52-2.8 8.55-5.83.9-1.44 1.52-3.07 1.86-4.87.29-1.65.25-3.43-.12-5.25a15.86 15.86 0 0 0-5.68-9.48 13.96 13.96 0 0 0-11.01-2.95 14.37 14.37 0 0 0-5.43 2.16 14 14 0 0 0-4.39 4.44c-2.28 3.41-3 7.68-1.97 11.74a13.85 13.85 0 0 0 7.25 8.79Z\"></path><path fill=\"#df1316\" d=\"M86.23 51.68c1.5.52 3.08.82 4.6.77.76-.02 1.52-.12 2.26-.3.75-.15 1.5-.38 2.22-.71a12.1 12.1 0 0 0 6.12-6.71 14.41 14.41 0 0 0 .3-9.91 12.26 12.26 0 0 0-6.76-7.44 13.02 13.02 0 0 0-10.54.24c-.89.4-1.72.87-2.47 1.42a15 15 0 0 0-2.01 1.88 12.53 12.53 0 0 0-2.6 4.51 12.3 12.3 0 0 0 1.21 9.98 14.4 14.4 0 0 0 7.69 6.26Z\"></path><path fill=\"#1d5d9e\" d=\"M145.23 54.78c3.35 2.1 7 3.41 10.86 3.89l.38.04c1.15.12 2.27.18 3.36.18 2.61 0 5.05-.34 7.38-1.01 3.1-.88 6.12-2.33 8.97-4.3l.19-.13a30.69 30.69 0 0 0 11.99-17.56c1.97-7.86.55-16.2-3.9-22.89a28.02 28.02 0 0 0-8.86-8.53l-.16-.1A29.28 29.28 0 0 0 163.73.25 30.72 30.72 0 0 0 152.1 1l-.55.15a29.24 29.24 0 0 0-10.26 5.43 28.52 28.52 0 0 0-10.42 21.1 31.22 31.22 0 0 0 6.52 20.13 28.73 28.73 0 0 0 7.8 6.97Z\"></path><path fill=\"#eebaad\" d=\"m205.9 79.71.5.43a26.86 26.86 0 0 0 18.84 6.14 28.94 28.94 0 0 0 18.41-7.8 26.88 26.88 0 0 0 8.62-20.18v-.39a27.82 27.82 0 0 0-9.68-20.03 27.74 27.74 0 0 0-21.2-6.73l-.36.04a27.27 27.27 0 0 0-18.78 11.4 29.02 29.02 0 0 0-5.12 19.33 26.86 26.86 0 0 0 8.76 17.8Z\"></path><path fill=\"#df1316\" d=\"M287.91 111.41a25.63 25.63 0 0 0-5.37-10.09 25.25 25.25 0 0 0-29.03-7.02 27.38 27.38 0 0 0-13.99 12.88l-.07.15a26.5 26.5 0 0 0-2.69 8.7c-.37 3.07-.13 6.24.73 9.55a25.42 25.42 0 0 0 12.01 15.03 27.72 27.72 0 0 0 18.5 3.15 25.2 25.2 0 0 0 17.19-11.74 26.19 26.19 0 0 0 3.48-9.76c.52-3.48.26-7.17-.75-10.85Z\"></path><path fill=\"#407569\" d=\"M75.31 136.46a11.73 11.73 0 0 0 8 5.48c2.95.54 5.94.03 8.67-1.49a11.8 11.8 0 0 0 5.56-7c.38-1.48.49-2.95.33-4.33a12.76 12.76 0 0 0-7.82-10.17 11.92 11.92 0 0 0-4.55-.91 11.8 11.8 0 0 0-8.96 4.21 11.82 11.82 0 0 0-2.46 4.61 12.2 12.2 0 0 0 1.23 9.61Z\"></path><path fill=\"#559482\" d=\"M104.51 111.78a10.8 10.8 0 0 0 7.67-2.49l.17-.14c2.1-1.9 3.33-4.41 3.56-7.24.27-2.82-.48-5.65-2.11-7.98a10.94 10.94 0 0 0-7.63-4.64h-.12a11.32 11.32 0 0 0-8.67 2.74 11.36 11.36 0 0 0-3.96 8.2v.13a10.93 10.93 0 0 0 3.51 8.21 11.99 11.99 0 0 0 7.6 3.22Z\"></path><path fill=\"#e5dcc1\" d=\"M131.03 94.64a9.91 9.91 0 0 0 7.07 1.85h.1a9.4 9.4 0 0 0 3.76-1.37 10.02 10.02 0 0 0 2.81-2.52 11.38 11.38 0 0 0 2.36-7.32 10.04 10.04 0 0 0-3.65-7.45 10.5 10.5 0 0 0-3.72-1.97l-.14-.04a11.3 11.3 0 0 0-4.21-.27 10.63 10.63 0 0 0-7.41 4.51 10.07 10.07 0 0 0-1.4 8.15c.65 2.58 2.2 4.85 4.41 6.43Z\"></path><path fill=\"#1d5d9e\" d=\"M167.51 99.89c.54.24 1.12.43 1.72.56a9.37 9.37 0 0 0 5.17-.33 10.3 10.3 0 0 0 5.57-4.53 9.32 9.32 0 0 0-1.05-10.99c-.43-.49-.92-.95-1.45-1.36l-.09-.07a10.35 10.35 0 0 0-5.87-1.93 9.37 9.37 0 0 0-8.93 6.41c-.78 2.31-.7 4.84.2 7.12a9.18 9.18 0 0 0 4.73 5.12Z\"></path><path fill=\"#eebaad\" d=\"m190.59 118.85.04.07a8.36 8.36 0 0 0 5.18 3.54c2.21.54 4.54.27 6.56-.75a8.5 8.5 0 0 0 4.44-5.43 8.84 8.84 0 0 0-1.17-7.05 8.6 8.6 0 0 0-2.68-2.72 8.77 8.77 0 0 0-4.7-1.43c-1.94 0-3.81.66-5.37 1.91a9.4 9.4 0 0 0-3.39 5.64 8.9 8.9 0 0 0-.06 3.25 8.69 8.69 0 0 0 1.14 2.96Z\"></path><path fill=\"#df1316\" d=\"M199.9 147.76a7.5 7.5 0 0 0 2.19 5.28 8.85 8.85 0 0 0 5.45 2.67 7.6 7.6 0 0 0 5.96-1.92 8.12 8.12 0 0 0 2.65-5.98v-.08a8.03 8.03 0 0 0-2.63-5.96 7.61 7.61 0 0 0-5.99-1.94c-2.07.21-4 1.16-5.47 2.69a7.45 7.45 0 0 0-2.17 5.25Z\"></path><path fill=\"#407569\" d=\"M201.63 175.26a7.9 7.9 0 0 0-5.43-.63 6.82 6.82 0 0 0-4.24 2.89 6.84 6.84 0 0 0-.86 5.03 7.82 7.82 0 0 0 2.82 4.7 6.97 6.97 0 0 0 5.47 1.47 7.18 7.18 0 0 0 2.7-1.07 7.04 7.04 0 0 0 2.18-2.2 7.26 7.26 0 0 0 .98-5.82 6.9 6.9 0 0 0-3.6-4.36Z\"></path><path fill=\"#eebaad\" d=\"M173.36 198.47a6.3 6.3 0 0 0-2.29-.38c-.38 0-.75.06-1.12.15-.37.08-.74.19-1.1.35a6 6 0 0 0-3.04 3.33 7.2 7.2 0 0 0-.15 4.92 6.09 6.09 0 0 0 3.36 3.69c1.55.65 3.48.71 5.23-.12.44-.2.85-.43 1.23-.7.37-.28.7-.6 1-.93a6.1 6.1 0 0 0 .69-7.19 7.1 7.1 0 0 0-3.82-3.1Z\"></path><path fill=\"#1d5d9e\" d=\"M144.08 196.93a13.15 13.15 0 0 0-5.39-1.93l-.19-.02a13.26 13.26 0 0 0-5.33.41 15.28 15.28 0 0 0-4.45 2.13l-.09.07a15.23 15.23 0 0 0-5.95 8.71 14.3 14.3 0 0 0 1.93 11.36c1.12 1.7 2.6 3.13 4.39 4.23l.08.05a14.6 14.6 0 0 0 5.81 2.05c1.95.24 3.9.12 5.77-.37l.27-.08a14.15 14.15 0 0 0 10.26-13.16c.14-3.54-1-7.07-3.24-9.99a14.32 14.32 0 0 0-3.87-3.46Z\"></path><path fill=\"#eebaad\" d=\"m113.97 184.56-.25-.22a13.3 13.3 0 0 0-8.53-3.07c-.27 0-.55 0-.82.02-3.4.16-6.64 1.53-9.14 3.87a13.33 13.33 0 0 0-4.28 10.01v.2c.16 3.86 1.9 7.47 4.81 9.94a13.78 13.78 0 0 0 10.52 3.34l.18-.02a13.5 13.5 0 0 0 9.32-5.66 14.36 14.36 0 0 0 2.54-9.59 13.32 13.32 0 0 0-4.35-8.83Z\"></path><path fill=\"#df1316\" d=\"M73.28 168.83a12.75 12.75 0 0 0 2.66 5.01 12.52 12.52 0 0 0 14.4 3.48c3-1.23 5.47-3.5 6.94-6.39l.04-.07a13.05 13.05 0 0 0 1.33-4.32 11.97 11.97 0 0 0-2.65-9.11 12.58 12.58 0 0 0-3.67-3.09 13.82 13.82 0 0 0-9.18-1.57 12.5 12.5 0 0 0-10.26 10.67c-.26 1.73-.13 3.56.37 5.38Z\"></path><path fill=\"#1d5d9e\" d=\"M144.46 132.54a17.25 17.25 0 1 0 0 34.5 17.25 17.25 0 0 0 0-34.5Z\"></path><path fill=\"#231f20\" d=\"M361.96 107.57c10.32-1.47 16.95-3.39 16.95-11.35V25.45c0-3.98-1.47-5.01-4.86-5.01h-2.8c-12.53 0-22.41 4.28-24.62 9.58-1.33 3.39-7.22 2.8-6.49 0 1.48-4.72 2.65-10.32 3.54-18.58.29-3.24 1.77-2.95 4.57-2.65 8.7.88 13.12 1.18 42.31 1.18s37.45-.29 46.88-1.18c2.65-.3 5.16-.59 4.13 2.65a173.13 173.13 0 0 0-4.72 18.58c-.44 2.51-4.87 2.95-5.6 0-1.62-5.01-8.7-9.58-20.64-9.58h-3.54c-3.54 0-5.89 1.03-5.89 5.01v70.77c0 7.96 6.63 9.88 17.1 11.35 1.18.15.29 5.6-1.03 5.6h-54.4c-1.18 0-2.21-5.45-.88-5.6Zm132.4-1.91c-5.45 2.21-17.1 9.88-20.2 9.88-19.46 0-42.61-7.96-42.61-34.35 0-18.13 11.5-39.22 37.74-39.22 22.26 0 25.21 16.22 25.21 22.11 0 3.83-1.33 10.47-3.24 14.3l-40.54 1.18c3.1 16.51 16.22 23.59 28.31 23.59 1.33 0 7.67-.74 13.42-1.92 1.33-.29 2.65 4.13 1.91 4.42Zm-44.38-34.79 27.57-.88c1.47-9.29-4.13-17.4-13.71-17.4-10.32 0-13.71 7.81-13.86 18.28Zm48.96 10.02c0-17.69 11.5-38.92 40.99-38.92 8.99 0 20.35 2.8 19.02 7.52l-1.62 5.9c-3.54 14.15-11.21-2.8-25.36-2.8-9.73 0-13.71 8.99-13.71 19.17 0 19.61 15.18 31.4 24.77 31.4 1.33 0 6.49-.59 14.01-2.21 1.33-.29 2.8 4.28 2.06 4.57-5.31 2.65-16.07 10.02-19.17 10.02-17.69 0-40.99-8.99-40.99-34.65Z\"></path><path fill=\"#231f20\" d=\"M558.85 108.02c4.42-.59 10.02-2.65 10.02-11.2V32.98c0-15.92-2.21-18.72-10.47-19.61-1.33-.15-.59-4.42.59-4.87l29.48-6.19a1.94 1.94 0 0 1 2.21 2.65c-2.36 9.14-2.8 27.86-2.8 39.66 0 3.69 2.06 5.01 4.27 4.28 7.52-2.51 14.89-6.93 23.59-6.93 14.74 0 19.02 11.06 19.02 20.05v34.79c0 8.55 5.75 10.61 10.02 11.2 1.18.15.15 5.16-.88 5.16h-36.12c-.88 0-1.62-5.01-.74-5.16 5.9-.89 8.7-3.83 8.7-11.2V70.86c0-12.38-3.84-15.63-14.3-15.63-5.9 0-13.56 1.62-13.56 5.46v36.12c0 6.78 3.54 10.32 10.17 11.2 1.03.15.15 5.16-1.03 5.16h-37.15c-1.18 0-2.21-5.01-1.03-5.16Zm-163.41 83.72v20.05c0 10.32 7.52 12.09 16.22 13.27 1.03.15.29 5.75-.89 5.75h-48.06c-1.18 0-2.07-5.6-.88-5.75 7.96-1.03 11.94-4.42 11.94-13.27v-64.28c0-9.73-5.01-12.53-11.94-14.45-1.18-.29-.44-5.9.88-5.75 5.6.74 11.94.89 17.25.89 12.53 0 22.11-.89 29.48-.89 24.33 0 43.49 8.55 43.49 31.55 0 25.8-24.62 37.59-57.5 32.88Zm0-44.23v36.27c23.15 2.95 34.2-3.98 34.2-21.38 0-14.74-8.84-26.39-24.77-26.39-7.67 0-9.44 2.8-9.44 11.5Zm58.89 49.69c0-21.38 15.48-37.59 37.15-37.59 20.49 0 36.42 14.3 36.42 36.12s-15.19 37.45-37.15 37.45c-20.49 0-36.42-14.3-36.42-35.97Zm54.7 6.34c0-19.17-8.99-33.76-22.26-33.76-6.34 0-13.57 3.83-13.57 19.17 0 19.46 8.99 34.06 22.11 34.06 6.49 0 13.71-3.83 13.71-19.46Z\"></path><path fill=\"#231f20\" d=\"M527.66 225.65c4.42-.59 9.88-2.65 9.88-11.2v-63.1c0-16.36-2.21-19.46-11.21-20.34-1.33-.15-.59-4.42.59-4.57l30.22-6.49c1.18-.29 2.8.74 2.21 2.65-.88 4.72-2.8 14.74-2.8 34.65v57.2c0 8.55 5.31 10.61 9.73 11.2 1.18.15.15 5.16-1.03 5.16h-36.56c-1.18 0-2.21-5.01-1.03-5.16Z\"></path><path fill=\"#231f20\" d=\"M564.18 225.65c4.57-.59 10.17-2.65 10.17-11.2v-30.81c0-9.29-2.36-11.35-9.73-12.24-1.33-.15-.59-4.57.59-4.87l28.75-6.63c1.47-.15 2.21.74 1.77 2.65a116.38 116.38 0 0 0-2.36 23.15v28.75c0 8.55 5.45 10.61 9.88 11.2 1.03.15.15 5.16-1.03 5.16h-37c-1.18 0-2.21-5.01-1.03-5.16Zm5.6-90.23c0-4.72 8.55-12.53 12.68-12.53s12.68 7.81 12.68 12.53-8.55 12.53-12.68 12.53-12.68-7.67-12.68-12.53Z\"></path><path fill=\"#231f20\" d=\"M601.19 198.52c0-17.69 11.5-38.92 40.99-38.92 8.99 0 20.35 2.8 19.02 7.52l-1.62 5.9c-3.54 14.15-11.21-2.8-25.36-2.8-9.73 0-13.71 8.99-13.71 19.17 0 19.61 15.18 31.4 24.77 31.4 1.33 0 6.49-.59 14.01-2.21 1.33-.29 2.8 4.28 2.06 4.57-5.31 2.65-16.07 10.02-19.17 10.02-17.69 0-40.99-8.99-40.99-34.65Z\"></path><path fill=\"#231f20\" d=\"M740.22 168.3c-4.13 1.18-8.25 5.46-12.53 15.04l-28.45 63.99c-4.86 10.61-16.07 12.09-21.08 12.09-8.55 0-12.53-4.57-14.89-10.76-2.8-8.11.59-4.72 11.5-4.72 9.88 0 17.84-2.36 22.41-12.53l-21.23-48.06c-4.57-10.32-8.26-14.3-12.53-15.04-1.62-.29-1.62-6.04.15-6.04h33.17c2.21 0 2.21 4.87.29 6.04-3.69 2.21-4.86 5.46-.44 15.04l10.76 24.47 10.47-24.18c4.27-10.02 3.54-14.6-3.1-15.33-1.62-.15-1.62-6.04.15-6.04h25.06c2.07 0 1.92 5.6.29 6.04Z\"></path><path fill=\"#1d5d9e\" d=\"M360.99 292c0-1.92 3.38-4.96 5.01-4.96s5.02 3.03 5.02 4.96-3.38 4.96-5.02 4.96-5.01-3.03-5.01-4.96Zm29.1-11.08v7.93c0 4.08 2.97 4.78 6.41 5.25.41.06.12 2.27-.35 2.27h-19.01c-.47 0-.82-2.22-.35-2.27 3.15-.41 4.72-1.75 4.72-5.25v-25.42c0-3.85-1.98-4.96-4.72-5.72-.47-.12-.18-2.33.35-2.27 2.22.29 4.72.35 6.82.35 4.96 0 8.75-.35 11.66-.35 9.62 0 17.2 3.38 17.2 12.48 0 10.2-9.74 14.87-22.74 13Zm0-17.49v14.34c9.15 1.17 13.53-1.57 13.53-8.46 0-5.83-3.5-10.44-9.8-10.44-3.03 0-3.73 1.11-3.73 4.55Zm73.01 33.12c-4.32 1.17-9.74 1.4-13.7-3.67l-8.81-11.31c-2.62-3.44-5.13-3.79-8.98-3.85v11.14c0 4.08 2.97 4.78 6.41 5.25.41.06.12 2.27-.35 2.27h-19.01c-.47 0-.82-2.22-.35-2.27 3.15-.41 4.72-1.75 4.72-5.25v-25.42c0-3.85-1.98-4.96-4.72-5.72-.47-.12-.18-2.33.35-2.27 2.22.29 4.72.35 6.82.35 4.96 0 9.5-.35 12.42-.35 9.62 0 15.16 3.85 15.16 10.79 0 6.47-5.07 9.8-10.73 10.09v.29c2.68 0 4.72 1.11 7.46 4.49l8.16 10.38c2.1 2.57 4.14 3.27 5.13 3.27.47 0 .29 1.75 0 1.81Zm-18.95-28.63c0-6.12-3.56-9.04-8.46-9.04-3.27 0-4.08 1.34-4.08 4.55v11.31h1.46c8.57 0 11.08-1.46 11.08-6.82Zm21.74 26.06c3.32-.87 5.89-1.4 5.89-5.72v-25.48c0-3.5-2.33-4.43-4.78-5.13-.47-.12-.12-2.27.23-2.22 4.2.64 13.35.35 15.69.35 5.31 0 8.98-.23 14.05-.35 1.46 0 1.52.93 1.22 2.33l-1.34 5.6c-.18.87-1.69.87-1.92 0-.99-3.03-2.57-4.26-9.68-4.26-3.61 0-4.9 1.05-4.9 3.79v9.21c0 1.52.76 1.81 1.92 1.81h1.52c4.02 0 8.4-.64 10.5-2.33.64-.53 1.98-.41 1.4.87l-3.15 7.35c-.52 1.17-1.52 1.05-2.16.12-1.28-1.92-4.14-2.74-7-2.74h-1.11c-1.17 0-1.92.29-1.92 1.81v9.27c0 3.27 1.17 4.66 5.72 4.66 7.76 0 12.07-4.37 13.35-6.24.35-.47 2.22 0 2.22.87 0 2.51-2.68 9.04-6.53 8.92-3.97-.18-6.36-.35-11.08-.35-5.31 0-12.6-.18-17.78.35-.58.06-.82-2.39-.35-2.51Zm41.63-1.34.64-2.39c1.81-6.82 4.02 2.86 13.65 2.86 4.61 0 6.24-2.22 6.24-5.19 0-2.22-1.05-3.15-5.83-6.24l-4.2-2.92c-6.88-4.55-9.85-6.82-9.85-11.72 0-7.58 7.58-12.42 14.52-12.42 4.37 0 10.26 1.98 9.56 3.97l-.76 2.33c-2.1 6.36-3.38-2.04-11.37-2.04-3.61 0-5.6 1.75-5.6 4.66 0 2.74 1.63 4.08 5.48 6.53l4.96 3.32c6.59 4.26 9.27 6.82 9.27 11.31 0 7.52-7.23 12.6-15.05 12.6-4.72 0-12.42-1.92-11.66-4.66Zm33.18 0 .64-2.39c1.81-6.82 4.02 2.86 13.65 2.86 4.61 0 6.24-2.22 6.24-5.19 0-2.22-1.05-3.15-5.83-6.24l-4.2-2.92c-6.88-4.55-9.85-6.82-9.85-11.72 0-7.58 7.58-12.42 14.52-12.42 4.37 0 10.26 1.98 9.56 3.97l-.76 2.33c-2.1 6.36-3.38-2.04-11.37-2.04-3.61 0-5.6 1.75-5.6 4.66 0 2.74 1.63 4.08 5.48 6.53l4.96 3.32c6.59 4.26 9.27 6.82 9.27 11.31 0 7.52-7.23 12.6-15.05 12.6-4.72 0-12.42-1.92-11.66-4.66Z\"></path></svg></div><style data-emotion=\"css 1sm5mum\">.css-1sm5mum{margin:0;font-size:14px;line-height:1.4;font-family:'Libre Baskerville',serif;font-weight:400;display:none;}</style><p class=\"MuiTypography-root MuiTypography-body1 css-1sm5mum\">Home</p></a></div><style data-emotion=\"css 1cb5ytm\">.css-1cb5ytm{box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;width:100%;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;margin-top:-32px;width:calc(100% + 32px);margin-left:-32px;-webkit-flex-basis:calc(16.666667% + 32px);-ms-flex-preferred-size:calc(16.666667% + 32px);flex-basis:calc(16.666667% + 32px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(16.666667% + 32px);-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:end;-ms-flex-pack:end;-webkit-justify-content:flex-end;justify-content:flex-end;}.css-1cb5ytm>.MuiGrid-item{padding-top:32px;}.css-1cb5ytm>.MuiGrid-item{padding-left:32px;}@media (min-width:600px){.css-1cb5ytm{-webkit-flex-basis:calc(16.666667% + 32px);-ms-flex-preferred-size:calc(16.666667% + 32px);flex-basis:calc(16.666667% + 32px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(16.666667% + 32px);}}@media (min-width:900px){.css-1cb5ytm{-webkit-flex-basis:calc(83.333333% + 32px);-ms-flex-preferred-size:calc(83.333333% + 32px);flex-basis:calc(83.333333% + 32px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(83.333333% + 32px);}}@media (min-width:1200px){.css-1cb5ytm{-webkit-flex-basis:calc(83.333333% + 32px);-ms-flex-preferred-size:calc(83.333333% + 32px);flex-basis:calc(83.333333% + 32px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(83.333333% + 32px);}}@media (min-width:1536px){.css-1cb5ytm{-webkit-flex-basis:calc(83.333333% + 32px);-ms-flex-preferred-size:calc(83.333333% + 32px);flex-basis:calc(83.333333% + 32px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(83.333333% + 32px);}}</style><div class=\"MuiGrid-root MuiGrid-container MuiGrid-item MuiGrid-spacing-xs-4 MuiGrid-grid-xs-2 MuiGrid-grid-md-10 jss5 css-1cb5ytm\"><style data-emotion=\"css 1waopz6\">.css-1waopz6{box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;width:100%;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;margin-top:-48px;width:calc(100% + 48px);margin-left:-48px;-webkit-flex-basis:calc(100% + 48px);-ms-flex-preferred-size:calc(100% + 48px);flex-basis:calc(100% + 48px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(100% + 48px);-webkit-align-items:flex-end;-webkit-box-align:flex-end;-ms-flex-align:flex-end;align-items:flex-end;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}.css-1waopz6>.MuiGrid-item{padding-top:48px;}.css-1waopz6>.MuiGrid-item{padding-left:48px;}@media (min-width:600px){.css-1waopz6{-webkit-flex-basis:calc(100% + 48px);-ms-flex-preferred-size:calc(100% + 48px);flex-basis:calc(100% + 48px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(100% + 48px);}}@media (min-width:900px){.css-1waopz6{-webkit-flex-basis:calc(100% + 48px);-ms-flex-preferred-size:calc(100% + 48px);flex-basis:calc(100% + 48px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(100% + 48px);}}@media (min-width:1200px){.css-1waopz6{-webkit-flex-basis:calc(100% + 48px);-ms-flex-preferred-size:calc(100% + 48px);flex-basis:calc(100% + 48px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(100% + 48px);}}@media (min-width:1536px){.css-1waopz6{-webkit-flex-basis:calc(100% + 48px);-ms-flex-preferred-size:calc(100% + 48px);flex-basis:calc(100% + 48px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(100% + 48px);}}</style><div class=\"MuiGrid-root MuiGrid-container MuiGrid-item MuiGrid-spacing-xs-6 MuiGrid-grid-xs-12 css-1waopz6\"><style data-emotion=\"css iplmjk\">.css-iplmjk{box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;width:100%;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;margin-top:-8px;width:calc(100% + 8px);margin-left:-8px;-webkit-flex-basis:calc(75% + 8px);-ms-flex-preferred-size:calc(75% + 8px);flex-basis:calc(75% + 8px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(75% + 8px);margin-top:8px;-webkit-box-flex-wrap:nowrap;-webkit-flex-wrap:nowrap;-ms-flex-wrap:nowrap;flex-wrap:nowrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}.css-iplmjk>.MuiGrid-item{padding-top:8px;}.css-iplmjk>.MuiGrid-item{padding-left:8px;}@media (min-width:600px){.css-iplmjk{-webkit-flex-basis:calc(75% + 8px);-ms-flex-preferred-size:calc(75% + 8px);flex-basis:calc(75% + 8px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(75% + 8px);}}@media (min-width:900px){.css-iplmjk{-webkit-flex-basis:calc(75% + 8px);-ms-flex-preferred-size:calc(75% + 8px);flex-basis:calc(75% + 8px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(75% + 8px);}}@media (min-width:1200px){.css-iplmjk{-webkit-flex-basis:calc(75% + 8px);-ms-flex-preferred-size:calc(75% + 8px);flex-basis:calc(75% + 8px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(75% + 8px);}}@media (min-width:1536px){.css-iplmjk{-webkit-flex-basis:calc(75% + 8px);-ms-flex-preferred-size:calc(75% + 8px);flex-basis:calc(75% + 8px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(75% + 8px);}}</style><div class=\"MuiGrid-root MuiGrid-container MuiGrid-item MuiGrid-spacing-xs-1 MuiGrid-grid-xs-9 css-iplmjk\"><style data-emotion=\"css 1wxaqej\">.css-1wxaqej{box-sizing:border-box;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}</style><div class=\"MuiGrid-root MuiGrid-item css-1wxaqej\"><style data-emotion=\"css 1jk0jc8\">.css-1jk0jc8{font-family:'Libre Baskerville',serif;font-weight:500;font-size:0.875rem;line-height:1.75;text-transform:uppercase;min-width:64px;padding:6px 8px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;color:#000;color:#000;font-family:'Lexend',sans-serif;text-align:center;text-transform:none;font-size:1em;font-weight:500;margin-top:3px;padding-top:0px;text-transform:none;}.css-1jk0jc8:hover{-webkit-text-decoration:none;text-decoration:none;background-color:rgba(0, 0, 0, 0.04);}@media (hover: none){.css-1jk0jc8:hover{background-color:transparent;}}.css-1jk0jc8.Mui-disabled{color:rgba(0, 0, 0, 0.26);}.css-1jk0jc8:active,.css-1jk0jc8:focus,.css-1jk0jc8:hover{background-color:#f2f2f2;border-radius:0;}</style><style data-emotion=\"css 1h1zga1\">.css-1h1zga1{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;box-sizing:border-box;-webkit-tap-highlight-color:transparent;background-color:transparent;outline:0;border:0;margin:0;border-radius:0;padding:0;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;-moz-appearance:none;-webkit-appearance:none;-webkit-text-decoration:none;text-decoration:none;color:inherit;font-family:'Libre Baskerville',serif;font-weight:500;font-size:0.875rem;line-height:1.75;text-transform:uppercase;min-width:64px;padding:6px 8px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;color:#000;color:#000;font-family:'Lexend',sans-serif;text-align:center;text-transform:none;font-size:1em;font-weight:500;margin-top:3px;padding-top:0px;text-transform:none;}.css-1h1zga1::-moz-focus-inner{border-style:none;}.css-1h1zga1.Mui-disabled{pointer-events:none;cursor:default;}@media print{.css-1h1zga1{-webkit-print-color-adjust:exact;color-adjust:exact;}}.css-1h1zga1:hover{-webkit-text-decoration:none;text-decoration:none;background-color:rgba(0, 0, 0, 0.04);}@media (hover: none){.css-1h1zga1:hover{background-color:transparent;}}.css-1h1zga1.Mui-disabled{color:rgba(0, 0, 0, 0.26);}.css-1h1zga1:active,.css-1h1zga1:focus,.css-1h1zga1:hover{background-color:#f2f2f2;border-radius:0;}</style><button class=\"MuiButtonBase-root MuiButton-root MuiButton-text MuiButton-textPrimary MuiButton-sizeMedium MuiButton-textSizeMedium MuiButton-root MuiButton-text MuiButton-textPrimary MuiButton-sizeMedium MuiButton-textSizeMedium css-1h1zga1\" tabindex=\"0\" type=\"button\" id=\"topics-button\" aria-haspopup=\"true\">Topics<style data-emotion=\"css pt151d\">.css-pt151d{display:inherit;margin-right:-4px;margin-left:8px;}.css-pt151d>*:nth-of-type(1){font-size:20px;}</style><span class=\"MuiButton-endIcon MuiButton-iconSizeMedium css-pt151d\"><style data-emotion=\"css vubbuv\">.css-vubbuv{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:1em;height:1em;display:inline-block;fill:currentColor;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;-webkit-transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;font-size:1.5rem;}</style><svg class=\"MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-vubbuv\" focusable=\"false\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" data-testid=\"ArrowDropDownIcon\"><path d=\"m7 10 5 5 5-5z\"></path></svg></span></button><style data-emotion=\"css krqjsm\">.css-krqjsm{margin-top:32px;}.css-krqjsm ul{display:grid;grid-template-columns:repeat(4, 1fr);max-width:60vw!important;padding:24px;width:60vw;}</style><style data-emotion=\"css 1ahz2t2\">.css-1ahz2t2{margin-top:32px;}.css-1ahz2t2 ul{display:grid;grid-template-columns:repeat(4, 1fr);max-width:60vw!important;padding:24px;width:60vw;}</style></div><div class=\"MuiGrid-root MuiGrid-item css-1wxaqej\"><style data-emotion=\"css 1lsxddw\">.css-1lsxddw{margin:0;font-family:'Lexend',sans-serif;font-size:1em;font-weight:700;margin-bottom:20px;text-transform:uppercase;line-height:1.235;font-weight:500;margin-bottom:0px;padding:8px;text-transform:none;}.css-1lsxddw:active,.css-1lsxddw:focus,.css-1lsxddw:hover{background:#f2f2f2;}</style><div class=\"MuiTypography-root MuiTypography-h4 css-1lsxddw\"><a class=\"MuiTypography-root MuiTypography-inherit MuiLink-root MuiLink-underlineAlways jss3 css-mctcqb\" href=\"/tracker\">Policy Tracker</a></div></div><div class=\"MuiGrid-root MuiGrid-item css-1wxaqej\"><div class=\"MuiTypography-root MuiTypography-h4 css-1lsxddw\"><a class=\"MuiTypography-root MuiTypography-inherit MuiLink-root MuiLink-underlineAlways jss3 css-mctcqb\" href=\"/newsletter\">Newsletter</a></div></div><div class=\"MuiGrid-root MuiGrid-item css-1wxaqej\"><div class=\"MuiTypography-root MuiTypography-h4 css-1lsxddw\"><a class=\"MuiTypography-root MuiTypography-inherit MuiLink-root MuiLink-underlineAlways jss3 css-mctcqb\" href=\"/podcast\">Podcast</a></div></div><div class=\"MuiGrid-root MuiGrid-item css-1wxaqej\"><button class=\"MuiButtonBase-root MuiButton-root MuiButton-text MuiButton-textPrimary MuiButton-sizeMedium MuiButton-textSizeMedium MuiButton-root MuiButton-text MuiButton-textPrimary MuiButton-sizeMedium MuiButton-textSizeMedium css-1h1zga1\" tabindex=\"0\" type=\"button\" id=\"projects-button\" aria-haspopup=\"true\">Projects<span class=\"MuiButton-endIcon MuiButton-iconSizeMedium css-pt151d\"><svg class=\"MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-vubbuv\" focusable=\"false\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" data-testid=\"ArrowDropDownIcon\"><path d=\"m7 10 5 5 5-5z\"></path></svg></span></button><style data-emotion=\"css 182hug8\">.css-182hug8{margin-top:32px;padding:24px;}.css-182hug8 ul{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;max-width:30vw;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;width:100%;}</style><style data-emotion=\"css 26s0vz\">.css-26s0vz{margin-top:32px;padding:24px;}.css-26s0vz ul{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;max-width:30vw;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;width:100%;}</style></div><div class=\"MuiGrid-root MuiGrid-item css-1wxaqej\"><div class=\"MuiTypography-root MuiTypography-h4 css-1lsxddw\"><a class=\"MuiTypography-root MuiTypography-inherit MuiLink-root MuiLink-underlineAlways jss3 css-mctcqb\" href=\"/contributors\">Contributors</a></div></div><div class=\"MuiGrid-root MuiGrid-item css-1wxaqej\"><div class=\"MuiTypography-root MuiTypography-h4 css-1lsxddw\"><a class=\"MuiTypography-root MuiTypography-inherit MuiLink-root MuiLink-underlineAlways jss3 css-mctcqb\" href=\"/about-us\">About</a></div></div></div><style data-emotion=\"css scfx6e\">.css-scfx6e{box-sizing:border-box;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-basis:25%;-ms-flex-preferred-size:25%;flex-basis:25%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:25%;-webkit-box-flex:2;-webkit-flex-grow:2;-ms-flex-positive:2;flex-grow:2;}@media (min-width:600px){.css-scfx6e{-webkit-flex-basis:25%;-ms-flex-preferred-size:25%;flex-basis:25%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:25%;}}@media (min-width:900px){.css-scfx6e{-webkit-flex-basis:25%;-ms-flex-preferred-size:25%;flex-basis:25%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:25%;}}@media (min-width:1200px){.css-scfx6e{-webkit-flex-basis:25%;-ms-flex-preferred-size:25%;flex-basis:25%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:25%;}}@media (min-width:1536px){.css-scfx6e{-webkit-flex-basis:25%;-ms-flex-preferred-size:25%;flex-basis:25%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:25%;}}</style><div class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-3 css-scfx6e\"><style data-emotion=\"css r03b1s\">.css-r03b1s{text-align:center;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;font-size:1.5rem;padding:8px;border-radius:50%;overflow:visible;color:rgba(0, 0, 0, 0.54);-webkit-transition:background-color 150ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 150ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;width:40px;}.css-r03b1s:hover{background-color:rgba(0, 0, 0, 0.04);}@media (hover: none){.css-r03b1s:hover{background-color:transparent;}}.css-r03b1s.Mui-disabled{background-color:transparent;color:rgba(0, 0, 0, 0.26);}</style><style data-emotion=\"css 1jrrc4c\">.css-1jrrc4c{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;box-sizing:border-box;-webkit-tap-highlight-color:transparent;background-color:transparent;outline:0;border:0;margin:0;border-radius:0;padding:0;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;-moz-appearance:none;-webkit-appearance:none;-webkit-text-decoration:none;text-decoration:none;color:inherit;text-align:center;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;font-size:1.5rem;padding:8px;border-radius:50%;overflow:visible;color:rgba(0, 0, 0, 0.54);-webkit-transition:background-color 150ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 150ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;width:40px;}.css-1jrrc4c::-moz-focus-inner{border-style:none;}.css-1jrrc4c.Mui-disabled{pointer-events:none;cursor:default;}@media print{.css-1jrrc4c{-webkit-print-color-adjust:exact;color-adjust:exact;}}.css-1jrrc4c:hover{background-color:rgba(0, 0, 0, 0.04);}@media (hover: none){.css-1jrrc4c:hover{background-color:transparent;}}.css-1jrrc4c.Mui-disabled{background-color:transparent;color:rgba(0, 0, 0, 0.26);}</style><button class=\"MuiButtonBase-root MuiIconButton-root MuiIconButton-sizeMedium css-1jrrc4c\" tabindex=\"0\" type=\"search\" aria-label=\"search\"><style data-emotion=\"css o261au\">.css-o261au{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:1em;height:1em;display:inline-block;fill:currentColor;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;-webkit-transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;font-size:1.5rem;color:#000;}</style><svg class=\"MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-o261au\" focusable=\"false\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" data-testid=\"SearchIcon\"><path d=\"M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z\"></path></svg></button><style data-emotion=\"css 1mxq86i\">.css-1mxq86i{font-family:'Libre Baskerville',serif;font-weight:500;font-size:0.875rem;line-height:1.75;text-transform:uppercase;min-width:64px;padding:6px 16px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;background-color:#273649;box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);color:#000;font-family:'Lexend',sans-serif;text-align:center;text-transform:none;border-radius:4px;color:#FFF;background-color:#DF1316;font-family:\"Roboto\",sans-serif;font-size:13px;font-weight:500;position:relative;text-transform:uppercase;margin-left:24px;}.css-1mxq86i:hover{-webkit-text-decoration:none;text-decoration:none;box-shadow:0px 2px 4px -1px rgba(0,0,0,0.2),0px 4px 5px 0px rgba(0,0,0,0.14),0px 1px 10px 0px rgba(0,0,0,0.12);}@media (hover: none){.css-1mxq86i:hover{background-color:#273649;}}.css-1mxq86i:active{box-shadow:0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);}.css-1mxq86i.Mui-focusVisible{box-shadow:0px 3px 5px -1px rgba(0,0,0,0.2),0px 6px 10px 0px rgba(0,0,0,0.14),0px 1px 18px 0px rgba(0,0,0,0.12);}.css-1mxq86i.Mui-disabled{color:rgba(0, 0, 0, 0.26);box-shadow:none;background-color:rgba(0, 0, 0, 0.12);}</style><style data-emotion=\"css iojclc\">.css-iojclc{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;box-sizing:border-box;-webkit-tap-highlight-color:transparent;background-color:transparent;outline:0;border:0;margin:0;border-radius:0;padding:0;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;-moz-appearance:none;-webkit-appearance:none;-webkit-text-decoration:none;text-decoration:none;color:inherit;font-family:'Libre Baskerville',serif;font-weight:500;font-size:0.875rem;line-height:1.75;text-transform:uppercase;min-width:64px;padding:6px 16px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;background-color:#273649;box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);color:#000;font-family:'Lexend',sans-serif;text-align:center;text-transform:none;border-radius:4px;color:#FFF;background-color:#DF1316;font-family:\"Roboto\",sans-serif;font-size:13px;font-weight:500;position:relative;text-transform:uppercase;margin-left:24px;}.css-iojclc::-moz-focus-inner{border-style:none;}.css-iojclc.Mui-disabled{pointer-events:none;cursor:default;}@media print{.css-iojclc{-webkit-print-color-adjust:exact;color-adjust:exact;}}.css-iojclc:hover{-webkit-text-decoration:none;text-decoration:none;box-shadow:0px 2px 4px -1px rgba(0,0,0,0.2),0px 4px 5px 0px rgba(0,0,0,0.14),0px 1px 10px 0px rgba(0,0,0,0.12);}@media (hover: none){.css-iojclc:hover{background-color:#273649;}}.css-iojclc:active{box-shadow:0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);}.css-iojclc.Mui-focusVisible{box-shadow:0px 3px 5px -1px rgba(0,0,0,0.2),0px 6px 10px 0px rgba(0,0,0,0.14),0px 1px 18px 0px rgba(0,0,0,0.12);}.css-iojclc.Mui-disabled{color:rgba(0, 0, 0, 0.26);box-shadow:none;background-color:rgba(0, 0, 0, 0.12);}</style><a class=\"MuiButtonBase-root MuiButton-root MuiButton-contained MuiButton-containedTertiary MuiButton-sizeMedium MuiButton-containedSizeMedium MuiButton-root MuiButton-contained MuiButton-containedTertiary MuiButton-sizeMedium MuiButton-containedSizeMedium css-iojclc\" tabindex=\"0\" href=\"/donate\">Donate</a></div></div></div></div></div></header><main id=\"main\"><style data-emotion=\"css 62igne\">.css-62igne{margin-top:48px;margin-bottom:48px;}</style><div class=\"MuiBox-root css-62igne\"><style data-emotion=\"css 1qsxih2\">.css-1qsxih2{width:100%;margin-left:auto;box-sizing:border-box;margin-right:auto;display:block;padding-left:16px;padding-right:16px;}@media (min-width:600px){.css-1qsxih2{padding-left:24px;padding-right:24px;}}@media (min-width:1200px){.css-1qsxih2{max-width:1200px;}}</style><div class=\"MuiContainer-root MuiContainer-maxWidthLg css-1qsxih2\"><style data-emotion=\"css 1l5mznc\">.css-1l5mznc{box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;width:100%;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;margin-top:-64px;width:calc(100% + 64px);margin-left:-64px;}.css-1l5mznc>.MuiGrid-item{padding-top:64px;}.css-1l5mznc>.MuiGrid-item{padding-left:64px;}</style><div class=\"MuiGrid-root MuiGrid-container MuiGrid-spacing-xs-8 css-1l5mznc\"><style data-emotion=\"css 1uc8nzd\">.css-1uc8nzd{box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;width:100%;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;margin-top:-32px;width:calc(100% + 32px);margin-left:-32px;-webkit-flex-basis:calc(100% + 32px);-ms-flex-preferred-size:calc(100% + 32px);flex-basis:calc(100% + 32px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(100% + 32px);}.css-1uc8nzd>.MuiGrid-item{padding-top:32px;}.css-1uc8nzd>.MuiGrid-item{padding-left:32px;}@media (min-width:600px){.css-1uc8nzd{-webkit-flex-basis:calc(100% + 32px);-ms-flex-preferred-size:calc(100% + 32px);flex-basis:calc(100% + 32px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(100% + 32px);}}@media (min-width:900px){.css-1uc8nzd{-webkit-flex-basis:calc(66.666667% + 32px);-ms-flex-preferred-size:calc(66.666667% + 32px);flex-basis:calc(66.666667% + 32px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(66.666667% + 32px);}}@media (min-width:1200px){.css-1uc8nzd{-webkit-flex-basis:calc(66.666667% + 32px);-ms-flex-preferred-size:calc(66.666667% + 32px);flex-basis:calc(66.666667% + 32px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(66.666667% + 32px);}}@media (min-width:1536px){.css-1uc8nzd{-webkit-flex-basis:calc(66.666667% + 32px);-ms-flex-preferred-size:calc(66.666667% + 32px);flex-basis:calc(66.666667% + 32px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(66.666667% + 32px);}}</style><div class=\"MuiGrid-root MuiGrid-container MuiGrid-item MuiGrid-spacing-xs-4 MuiGrid-grid-xs-12 MuiGrid-grid-md-8 css-1uc8nzd\"><style data-emotion=\"css svts4y\">.css-svts4y{box-sizing:border-box;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;max-width:100%;}</style><div class=\"MuiGrid-root MuiGrid-item css-svts4y\"><style data-emotion=\"css wpk4sk\">.css-wpk4sk{margin-bottom:16px;color:#fff;background-color:#6c5c29;padding:4px 16px;border-radius:4px;display:inline-block;text-transform:uppercase;font-family:Lexend;font-size:12px;font-weight:500;line-height:1.75;}</style><div class=\"MuiBox-root css-wpk4sk\">Podcast</div><style data-emotion=\"css i2risw\">.css-i2risw{margin:0;font-family:'Libre Baskerville',serif;font-size:2em;font-weight:700;margin-bottom:16px;border-bottom:1px solid #8AA29D;padding-bottom:16px;}</style><h1 class=\"MuiTypography-root MuiTypography-h2_article css-i2risw\">Considering the Human Rights Impacts of LLM Content Moderation</h1><style data-emotion=\"css 1bs141q\">.css-1bs141q{margin:0;font-family:'Lexend',sans-serif;font-weight:300;line-height:1.4;font-size:0.875rem;color:#616161;font-size:12px;text-transform:uppercase;}</style><span class=\"MuiTypography-root MuiTypography-body2 css-1bs141q\">Justin Hendrix / </span><span class=\"MuiTypography-root MuiTypography-body2 css-1bs141q\">Jul 6, 2025</span><style data-emotion=\"css 3vr8u\">.css-3vr8u{margin:0;font-size:14px;line-height:1.4;font-family:'Libre Baskerville',serif;font-weight:400;}</style><div class=\"MuiTypography-root MuiTypography-body1 html-to-react-article css-3vr8u\"><p><a href=\"https://techpolicypress.captivate.fm/listen\" target=\"_blank\" rel=\"noopener\"><em>Audio of this conversation is available via your favorite podcast service.</em></a></p><iframe src=\"https://player.captivate.fm/episode/9498004b-9b44-4855-bf5f-6e6cb0321410/\" style=\"width:100%;height:200px;border:none\"></iframe><p>At Tech Policy Press, we’ve been <a href=\"https://www.techpolicy.press/syllabus-large-language-models-content-moderation-and-political-communication/\" target=\"_blank\" rel=\"noopener\">tracking</a> the emerging application of generative AI systems in content moderation. Recently, the <a href=\"https://ecnl.org/\" target=\"_blank\" rel=\"noopener\">European Center for Not-for-Profit Law</a> (ECNL) released a comprehensive report titled <a href=\"https://ecnl.org/sites/default/files/2025-04/ECNL_LLM_CM_Excecutive%20Summary_2025.pdf\" target=\"_blank\" rel=\"noopener\"><em>Algorithmic Gatekeepers: The Human Rights Impacts of LLM Content Moderation</em></a>, which looks at the opportunities and challenges of using generative AI in content moderation systems at scale. I spoke to its primary author, ECNL senior legal manager <strong>Marlena Wisniak</strong>.</p><p><em>What follows is a lightly edited transcript of the discussion.</em></p><div style=\"position:relative;width:100%\"><figure><img alt=\" \" loading=\"lazy\" width=\"1024\" height=\"576\" decoding=\"async\" data-nimg=\"1\" style=\"color:transparent;aspect-ratio:1.7777777777777777;width:100%;height:auto\" src=\"https://cdn.sanity.io/images/3tzzh18d/production/ca8393b380961a234123ad924f89ed855aed0c1d-1200x675.png\"/><figcaption><div class=\"MuiTypography-root MuiTypography-body1 html-to-react-caption css-3vr8u\" style=\"color:#7C7B7B\"><p>Game of Pixels x Toy Models by Elise Racine. <a href=\"https://betterimagesofai.org/images?artist=EliseRacine&amp;title=GameofPixelsxToyModels\">Better Images of AI</a>/CC by 4.0</p></div></figcaption></figure></div><p><strong>Justin Hendrix:</strong></p><p>Marlena, can you tell us a little bit about what the ECNL does?</p><p><strong>Marlena Wisniak:</strong></p><p>The Center, or ECNL as we call it, is a human rights and civil liberties organization for over 20 years. We&#x27;ve been mostly focusing on civic space, making sure that civil society organizations, but also grassroots orgs, activists, have a safe space to organize and do their work. We&#x27;re mostly lawyers, but I have to say we&#x27;re the fun lawyers. So we also do advocacy, research, and basically anything that we can do to protect and promote in enabling civic space. My team was founded in 2020, I believe, and has from the beginning focused mostly on AI, but it&#x27;s more broadly the digital team at ECNL.</p><p>So we look at how technologies, specifically emerging technologies, impact civic space and human rights. So our core human rights or civil liberties that we look at is typically privacy, freedom of expression, freedom of assembly, so rights to protest, for example, association, right to organize, and non-discrimination. And our key areas of focus substantively are typically surveillance, AI-driven surveillance and biometric surveillance, and broader and social media platforms that plays such a big role in civic space today. And that&#x27;s what I&#x27;ll be talking about today. But that&#x27;s, in a nutshell, my team and how it fits within the broader org.</p><p><strong>Justin Hendrix:</strong></p><p>Well, I&#x27;m excited to talk to you about this report that you have authored with help from a variety of different corners, but it&#x27;s called Algorithmic Gatekeepers, the Human Rights Impacts of LLM Content Moderation. So this is a topic that we&#x27;ve been trying to follow at Tech Policy Press fairly closely, because I feel like it is the sort of intersection of a lot of things that we care about around social media, content moderation, online safety, free expression, and then, of course, artificial intelligence. And LLMs, generative AI generally, the intersection of these two things, I think, is probably one of the most interesting and possibly under-covered or under-explored, at least to date, issues at the intersection of tech and democracy.</p><p>So I want to just start by asking you how you did this report. It&#x27;s quite a significant document. A lot of research obviously went into this, including gathering some new information, not just combining citations, as many people do when they produce a PDF white paper. What got you started on this and how did you go about it?</p><p><strong>Marlena Wisniak:</strong></p><p>Yes, you&#x27;re right. It was really a heavy lift. It was a research project that went on for about a year, and we collaborated with great folks including yourself. I remember, I think last year we had a call to hear your thoughts, and shout out, before I go deep into context, to Isabelle Anzabi, who was our fellow last summer, and really helped me just go through a lot of papers, mostly in computer science, and the Omidyar Network, who provided generous support for this project.</p><p>And so how it started, I accidentally fell into AI in 2016, but come from content moderation in the 2010s, &#x27;11, &#x27;12, so I&#x27;d say early days of content moderation. So automated content moderation has always been a big focus of mine. I was also at Twitter at some point, overseeing their legal department, sorry, content-governance and illegal department. And living in San Francisco right now, really, the big talk on the street is always LLMs, and GenAI more broadly.</p><p>And so I started hearing various use cases of LLMs for content moderation. Most of the talk, I will say, and focus of the research and civil society community is how to moderate LLM-generated content, so how to moderate ChatGPT, for example, or Claude. That is, of course, super important. And I wanted to look at it from another angle, which is how are LLMs used for content moderation? And this I will say, Justin, has been a little bit of a chicken-and-egg conversation, or reflection, because the way how LLMs can moderate content also impacts how LLM-driven content will be moderated, and how they&#x27;re moderated impacts how they moderate content. So it&#x27;s hard to separate those, one from another, but I did choose to have that focus. And it&#x27;s interesting because it&#x27;s very much a nerdy topic and yet has real-world implications, and it&#x27;s very hyped up.</p><p>So one of the things that I always love to do is go beyond the hype. I&#x27;m not a hype person. In fact, now I hate AI. It was fun working on AI in the 2016, &#x27;17, &#x27;18 years when nobody really cared about AI. Since ChatGPT was released, the only thing people ever want to talk about is AI. So now I want to scream. But all this to say that I did want to impact some of the real implications and post what our promises and what our, really, I&#x27;d say, not even realistic promises, but the types of impacts we want to see, because I do believe that it can helpful, and also, as this is an emerging space, prevent any possible harm. And I think folks listening to this podcast probably know that human motivation is extremely complicated. It&#x27;s horrible for workers. It doesn&#x27;t always, and always is an understatement, produce good outcomes.</p><p>And so machine learning came as a solution to that. It was expanded during COVID, and sort of came as this silver-bullet solution. There has been increasing research showing the limitations of that. And so now, LLM is the new white horse, is that an expression? Is the savior, and there&#x27;s a lot of hype. So that&#x27;s where I came from. It&#x27;s like, okay, let&#x27;s go deeper into this. Let&#x27;s review a lot of computer science papers where some of this more rigorous qualitative and quantitative work has done, translate it to policy folks, and bring a human-rights approach because that&#x27;s my background. I&#x27;m an international human rights lawyer.</p><p><strong>Justin Hendrix:</strong></p><p>So readers of Tech Policy Press have at least had multiple pieces that we&#x27;ve posted on the site about this issue, and often there&#x27;s been this sort of question about what is the promise potentially to offset the dangers to the labor force that currently engage in content moderation on behalf of platforms, but then also, of course, there are various perils that we can imagine as well. We&#x27;re going to get into those a little bit.</p><p>But I think one thing that distinguishes your report that I wanted to just start with perhaps is that you include a technical primer. You&#x27;ve got a kind of set of definitions and, I think usefully, some distinctions between what&#x27;s going on with LLMs for content moderation versus more standard machine-learning classifiers and recommendation mechanisms and other types of algorithmic models that have been used in content moderation for quite some time now. What do you think the listener needs to know about the technical aspect of this phenomenon, of the use of LLMs for content moderation at scale from our vantage right now in June of 2025?</p><p><strong>Marlena Wisniak:</strong></p><p>Yeah, I mean, I encourage folks to look at the technical primer, that is, the audience for that are mostly folks who aren&#x27;t very familiar with either industry jargon or technical terms. Leveling up, I think one thing that is really critical to consider when thinking about LLM-driven content moderation is that you have typically two layers. So one is the foundation model layer, or the LLMs. LLM are, I should have started, large-language models. And that&#x27;s pretty explicit. It&#x27;s large-language model, so people see it as God, or the sci-fi technology. It&#x27;s really not. It&#x27;s really big data sets.</p><p>And I think we often forget that. So if we think about traditional machine learning, the difference is that this is a larger set. And so there&#x27;s this implication, obviously, for privacy and other rights that we can explore later down the line. But one, considered, they&#x27;re very, very, very big, enormous data sets that require a lot of computing power. And so really, there&#x27;s only a handful of companies right now building these models.</p><p>But the ones that we looked at in more depth are Llama by Facebook, sorry, Meta, ChatGPT, OpenAI, Claude, Anthropic, Gemini, Google. And then right as I was finishing out the report, DeepSeek came out. So there&#x27;s emerging models as well, but it&#x27;s still a very small number of foundation models, given how much data and compute they use. And I often say they&#x27;re really not that technically complicated. They&#x27;re just bigger. And so from a concentration of power, that has a lot of importance.</p><p>And so then the platforms that I mentioned, they both develop and often use... they develop the LLMs and they use them for content moderation. But what happens for all the other ones like Discord or Reddit, or Slack? Although Slack might be bought by one of them. But anyways, there are so many other platforms. Typically, they do not build their own LLMs. They will either have a license with one of the foundation-model companies and then fine-tune them or they will use one of the open source, like DeepSeek or Llama or something they find on Hugging Face, and then fine-tune them for their purposes.</p><p>But what that means, so that&#x27;s one thing from a technical thing to understand how LLMs work and how they&#x27;re used in practice. And then another thing I will flag is that LLMs are a subset of generative AI. That&#x27;s a term we most commonly know. Generative AI today has, to some extent, become equivalent of ChatGPT. It&#x27;s much broader than ChatGPT, but LLMs are one subset of that, or foundation models as well.</p><p>And then maybe one last thing I&#x27;ll flag is multilingual language models are those that are trained on text data from dozens to sometimes hundreds of languages simultaneously. And so the idea is that they will be more capable of processing and generating inputs and outputs in multiple languages. And we can about whether that works or not. There&#x27;s fantastic research that has been done that inspired me a lot for my work.</p><p><strong>Justin Hendrix:</strong></p><p>Yeah, I&#x27;ll just say that the primer, I think what&#x27;s great about, again, this report is that it does detail at least some of the ambition technically that people have for using large-language models in content generation and the possibilities it opens up for sorts of things that have been very hard to do or difficult, certainly at scale, with today&#x27;s mechanisms, including, as you say, a big one, which is servicing many languages that previously social media firms might have decided are simply off limits for their consideration because it&#x27;s just not perhaps feasible or profitable for them to address certain markets or certain languages with the type of scale that perhaps people in those places might believe is necessary in order to do a good job. So that&#x27;s been a kind of consistent complaint from civil society for a long time.</p><p>But there are other things here that you point to that seem to fit in the category of promise, such as possibly more robust types of nudges or interventions in what people post, or after the fact what they post. I know I&#x27;ve talked to people about the idea that we can imagine content moderation with LLM starting before you ever post something, kind of prefacto content moderation, and possibly in interaction with the content moderation system as it were at that point, before you even pushed something live to your feed. But I don&#x27;t know. What else did you learn in terms of just studying these things technically about the possibilities they open up?</p><p><strong>Marlena Wisniak:</strong></p><p>Yeah, thanks are bringing it up. And the report does obviously highlight harms and also explores promises for each individual, right. So I&#x27;m with you. One of the key findings of this research was that probably the most promise of LLM-driven content moderation is not to remove content or even to moderate it through a ranking system or curating content, but really anything on procedural rights. So it&#x27;s before posting, like you said, nudges. Even though I will say that we also don&#x27;t want this Big Brother-type experience where we&#x27;re typing something and, oh dear Lord, the algorithm has found that this may be controversial. But yeah, there can be nudges, there can be suggestions for reformulating something that could be abusive, less invasive. They could even suggest other informations, for example, if someone is posting something clearly wrong about... like, factually incorrect about the Ukraine war, or elections, like civic integrity stuff, they could check out xyz.gov, right, elections.gov or something.</p><p>And also on the flip side, appeals and remedy. When a user posts content, they can immediately know that this has been flagged for... especially content that would be automatically removed, they can get an automated notice that&#x27;s more personalized that this content has been flagged for review or has been removed, and give more personalized information about how they can appeal that decision. So there&#x27;s all this kind of user interaction that I think is pretty cool and exciting.</p><p>There&#x27;s also the possibility for personalized content moderation. So let&#x27;s say one user really welcomes gore or sensitive content, borderline content that doesn&#x27;t violate either the platform&#x27;s policies or the law. They can adapt and adjust their own moderation, as opposed to someone who really doesn&#x27;t want to see anything remotely sensitive or related to some topics because of any trauma or just dislike or whatever. That can be helpful.</p><p>And I&#x27;ll give a shout out here to Discord, with whom we collaborated for a year on engaging external stakeholders as they develop ML-driven interventions to moderate abuse and harassment on their platform, specifically for teens. And so we really worked with a broad range of stakeholders, including youth and children, which is interesting, and understanding how they would like to see AI-driven innovation and intervention. So it wasn&#x27;t only LLM, it was also traditional machine learning.</p><p>But yeah, so I think, to sum up creative uses of LLM for moderating content, and by moderating I mean broadly, is something that I support much more than automated removal, which one of the conclusions of this report is that, at least today, it is still too risky, potentially harmful, and just doesn&#x27;t... like, ineffective to do that. There will be too many false positives and too many false negatives, both of those disproportionately falling on already marginalized groups who tend to be, as most folks, you probably know, disproportionately silenced by platform and also disproportionately targeted by violative content. So that&#x27;s one of the main findings of how we can use LLMs safely is more on the procedural side than actually the content.</p><p><strong>Justin Hendrix:</strong></p><div id=\"mlb2-5983225\" class=\"ml-form-embedContainer ml-subscribe-form ml-subscribe-form-5983225\"><div style=\"margin:32px 0 24px 0;border-top:1px solid #E2D7BB\"></div><div class=\"ml-form-align-center\"><div class=\"ml-form-embedWrapper embedForm\" style=\"max-width:100%\"><div class=\"ml-form-embedBody ml-form-embedBodyDefault row-form\" style=\"padding:25px 0 30px 0\"><div class=\"ml-form-embedContent\" style=\"margin-bottom:20px\"><h4 style=\"font-size:24px\">Our Content delivered to your inbox.</h4><div style=\"text-align:center;font-weight:400;line-height:22px;max-width:390px;margin-right:auto;margin-left:auto\">Join our newsletter on issues and ideas at the intersection of tech &amp; democracy</div></div><form class=\"ml-block-form\" action=\"https://static.mailerlite.com/webforms/submit/s8h5n5\" data-code=\"s8h5n5\" method=\"post\" target=\"_blank\"><div style=\"display:flex;justify-content:space-around\"><div class=\"jss7 MuiBox-root css-0\"><div class=\"ml-form-formContent\" style=\"width:221px;margin-bottom:0px\"><div class=\"ml-form-fieldRow ml-last-item\"><div class=\"ml-field-group ml-field-email ml-validate-email ml-validate-required\"><input aria-label=\"email\" aria-required=\"true\" type=\"email\" data-inputmask=\"\" name=\"fields[email]\" placeholder=\"Enter email address\" autoComplete=\"email\" style=\"padding:6px 10px !important\"/></div></div></div><input type=\"hidden\" name=\"ml-submit\" value=\"1\"/><div class=\"ml-form-embedSubmit\" style=\"width:153px;margin-bottom:0px\"><button type=\"submit\" class=\"primary\" style=\"font-family:Lexend, sans-serif!important;display:flex;justify-content:space-evenly;font-size:14px !important;line-height:normal !important;font-weight:400 !important;align-items:center;padding:5px 10px !important\">Subscribe <style data-emotion=\"css vubbuv\">.css-vubbuv{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:1em;height:1em;display:inline-block;fill:currentColor;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;-webkit-transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;font-size:1.5rem;}</style><svg class=\"MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-vubbuv\" focusable=\"false\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" data-testid=\"ChevronRightIcon\"><path d=\"M10 6 8.59 7.41 13.17 12l-4.58 4.59L10 18l6-6z\"></path></svg></button><button disabled=\"\" style=\"display:none\" type=\"button\" class=\"loading\"> <div class=\"ml-form-embedSubmitLoad\"></div> <span class=\"sr-only\">Loading...</span> </button></div></div></div><input type=\"hidden\" name=\"anticsrf\" value=\"true\"/></form></div><div class=\"ml-form-successBody row-success\" style=\"display:none\"><div class=\"ml-form-successContent\"><h4>Thank you!</h4><p style=\"text-align:center\">You have successfully joined our subscriber list.</p></div></div></div></div><div style=\"margin:24px 0 32px 0;border-top:1px solid #E2D7BB\"></div></div><p>We&#x27;ll see how platforms attempt to go about this. We&#x27;re already seeing some in the wild, examples of uses of LLMs. There&#x27;s been reporting even this week of Meta&#x27;s new community notes program, using LLMs in certain ways. So I think there&#x27;ll be probably as wide a variety of applications of LLMs as there have been of machine learning in content moderation, and we&#x27;ll just, I suppose, see how it goes. I mean, different platforms, who knows, maybe one or two will err towards creating a AI nanny state hyper-sanitized environment that people may recoil from, or regard as overly censorious or what have you. And yet it&#x27;s possible to imagine, as you say, lots of different types of interventions that people might regard as useful or helpful in whatever way.</p><p>But I want to pause on maybe thinking about the benefits and dig a little more into the potential human rights impacts, because that&#x27;s where, of course, you spend the bulk of this report, concerned with things like privacy, freedom of expression and information and opinion, questions around peaceful assembly and association, non-discrimination, participation. Take us through a couple of those things. When you think about the most significant potential human rights impacts of the deployment of large language model systems in content moderation at scale, what do you think is most prominent?</p><p><strong>Marlena Wisniak:</strong></p><p>So I&#x27;ll just highlight some of the most specific to LLMs. One thing to consider is that LLMs often exacerbate and accelerate already-existing harms done by traditional machine learning, and traditional ML accelerates and exacerbates harms committed by humans often. So I think it&#x27;s like the more scale, scale can be good for accuracy and speed obviously, and it also has, there&#x27;s another side to that coin.</p><p>So some of the things you&#x27;ll find the report are kind of an LLM-ified analysis of automated content moderation, but I will single out a few really new concerns related to LLMs. And so one of them is kind of a, coming back to the concentration of power issue that I mentioned before, any decision that is made at the foundation model level, unless it is proactively fine-tuned at the deployment level, that will trickle down across platforms.</p><p>So to give you an example, if Meta decides that pro-Palestinian content is considered violent or terrorist content, and there has been a lot of reporting to show that, then if another platform uses Llama without changing that specifically, any decision that Meta makes at the level of Llama will then trickle down to the other platforms. So that&#x27;s something to consider from a freedom of expression angle, is generalized censorship, if it&#x27;s false positive, and at the same time, content that should be removed will not be removed if the foundation model does not consider that as harmful. So that interaction is particularly important because we&#x27;ve never seen that before, to my knowledge at least, and the dynamics on content moderation.</p><p>Another big thing around freedom of information, for example, is hallucinations. So that is a very stereotypical GenAI problem. And for folks who don&#x27;t know, hallucinations are -- it&#x27;s content that is generated by the AI system and that is just made up and wrong. So the weird thing is that it does in a way that seems so confident and so right, and it is just nonsense. So it&#x27;ll make up academic papers, or it&#x27;ll make up news articles or any kind of facts.</p><p>So if platforms use this to moderate missing this information, that will just be inaccurate to begin with. And it&#x27;s hard sometimes to parse that when you have pretty convincing content. And even if, for example, human moderators would use LLMs or GenAI to help them moderate content, if they see this really elaborate article about how, whatever, Trump won the 2020 elections, for example, and they&#x27;re not familiar with it, that could form the base of their decisions, and it&#x27;s just plain wrong. So that&#x27;s the new harm and risk.</p><p>Another one that I found was super interesting, and this, I will say, was me... a lot of this paper was me trying to envision harms and then probe it with engineers or technical folks and also non-technical as well, to ask them, does this make sense? Could this be true? And for example, from freedom of peaceful assembly and protest, one thing to consider is that protests are, and contrarian views are protest definition anti-majority.</p><p>You have a minority express themselves. You go against powerful interests like governments or companies, or even just the status quo. And what does that mean? This data is not well-represented in the training datasets. Because machine learning, I often say is just steroids on stats. So if a view is predominant in the dataset, even if it&#x27;s completely wrong, that is the output, right? Machine learning never gives a real decision. It gives a prediction about what is statistically possible. So when you have protestors or journalists, investigative journalists, for example, or anybody bringing up new stuff, that will not actually show up in the dataset and therefore will not be moderated well. And let&#x27;s give platforms and those deploying content moderation system the highest benefit of the doubt. They probably wants to moderate it well, just the system will not function unless specifically fine-tuned because protests fall outside the curve of statistical data.</p><p>And that&#x27;s also the case for conflicts or exceptional circumstances, crises. These events are, by definition, exceptional, and therefore fall outside the statistical curve and are not moderated well. And that&#x27;s another key finding. For freedom of association, one thing that could be interesting here is that some organizations can be mislabeled. Actually, you know what, Justin, forget that one. It&#x27;s too long. I&#x27;ll skip it.</p><p>Two last pieces I&#x27;d like to highlight that we found were really interesting. One was on participation. So on one side, LLMs actually have the potential to support more participatory design by enabling customizable moderation. So like I said before, the users have personalized content moderation, or perhaps in the future use AI agents to moderate their own content as they want. On the flip side, in practice, affected communities are largely excluded from shaping content moderation systems. That&#x27;s not new. That has also happened in machine learning.</p><p>Generally, it&#x27;s mostly white tech bros in San Francisco or Silicon Valley, so especially marginalized groups and those in the global majority are excluded from that. The addition with LLMs is that they are typically &quot;improved&quot; through a method called reinforcement learning from human feedback, or red-teaming. So you have folks thinking about, in the case of red-teaming, what are potential bad, worst-case scenarios and testing it from an adversarial perspective.</p><p>Same for reinforcement learning. They go through these models and they &quot;fix&quot; them, they reteach them how to learn. The problem is that people who do that are usually Stanford graduates, those in Silicon Valley or other elitist institutions, and it&#x27;s very rare. I personally have never heard of folks from marginalized groups being invited to participate in these kinds of activities. I myself, for example, have been invited, but you have to be in a niche kind of AI group to do that. And everybody who I&#x27;ve spoken to who has done that, I personally have not. I&#x27;ll say that it&#x27;s a very homogeneous group. And so basically what that means is then yes, the LLMs will be improved... or, I&#x27;m sorry, let me rephrase. There are efforts to reduce inaccuracy and improve the performance of the models of the LLMs. However, the people who do that are typically very homogeneous. So it&#x27;s just like snowball effects.</p><p>And the last piece on remedy is that while there are potential promises to access remedy better, like I said before, most everyone use a notification, helping them identify remedy, appeals mechanisms, speeding up the appeals, there&#x27;s also fundamentally a lack of explainability and transparency, and that can create barriers to remedy. Another issue, like I said before, is that there are these two layers, the layer of foundation models. So the ChatGPT, the Claude, the Gemini, and then the social media platform that deploys it. And it&#x27;s hard to know where to appeal, how to appeal. The foundation models themselves don&#x27;t really know how a decision was made. Social media platforms know that even less. Do you appeal to the platform? Does the platform then appeal again to the third party LLM? Where does the user fall into this? So just accountability becomes fragmented, and there&#x27;s a lot of confusion and lack of clarity around how to go around that.</p><p><strong>Justin Hendrix:</strong></p><p>So I want to come to some of your recommendations, because you have both recommendations to LLM developers and deployers as well as to, of course, those who are potentially applying these things inside of social media companies. But your section on recommendations to policymakers is perhaps mercifully brief. You&#x27;ve only got a handful of recommendations there. It&#x27;s clear, I think just from the onus in the report on where the recommendations are, that you see it largely as something that private sector needs to sort out, how are they going to deploy these technologies or not.</p><p>But when it comes to policymakers, what are you telling them? I see you&#x27;re interested in making sure they&#x27;re refraining, on some level, from mandating the use of these things. I suppose it&#x27;s a possibility that somebody might come along and say, &quot;Oh, in fact, we demand that you use them.&quot; I was trying to think of a context for that, but then I found myself thinking about some of the laws we&#x27;ve seen put forward in the United States even, where there have been segments of those laws, I think in some of the must-carry laws, where there have been these ideas around transparency of moderation decisions. I feel like I&#x27;ve read amicus briefs where some of the people opposed to those laws would make arguments like, &quot;Well, it&#x27;s just simply not possible to give explainable rationale to every single user for every single content-moderation decision that&#x27;s made.&quot;</p><p>Well, presumably, LLMs would make that quite possible, and you could imagine a government coming along and saying, &quot;Somehow, in the interest of free expression, we would like to mandate, use artificial intelligence to explain any content moderation decision that you might take.&quot; You say that&#x27;s a bad idea along with other things, but what would you tell policymakers to be paying attention to here?</p><p><strong>Marlena Wisniak:</strong></p><p>Yeah, I mean, that&#x27;s a very astute... you observe that well. Most of the recommendations are to LLMs developers and employers. The reason for this is not that we only see the owners on the private sector, not on the public sector, is that for practical reasons, this part of the research was mostly on assessing the human rights impact. And the recommendations piece was really the last part, and we didn&#x27;t have that much capacity to go deep. So the second iteration of this report, hopefully we will continue. It will be really to zoom into the recommendations.</p><p>And I will say then, that on the policy making side, so a lot of our work at ECNL is on policy and legal advocacy. We&#x27;ve been working behind the scenes on the AI Act for the past five years, and right now, there&#x27;s conversations around the GPAI code in the EU on general-purpose AI. And the reason why I didn&#x27;t go deep here is that one, we don&#x27;t want a specific AI content moderation or LLM moderation law. We have the DSA in Europe. The U.S. I&#x27;ll just set aside, because right now there&#x27;s a lot going on. So we&#x27;re not calling for a specific LLM content moderation law.</p><p>And the EU already has the DSA and the AI. And I&#x27;d say a lot of the foundational aspects of LLMs to policymakers would be kind of basic AI around, like, data protection, human rights impact assessment, stakeholder engagement. So I added these big categories. I didn&#x27;t go really deep into them. It&#x27;s mostly how can we not fuck it up, to be honest. And I think content moderation, as you know, is a very fraught topic where even folks who are well-intentioned just don&#x27;t understand it enough. So sometimes you will have these claims that would let... platform should remove problematic content within an hour. And it&#x27;s like, okay, cool. In theory, that sounds great. What does that mean in practice? It means a lot of false positives. It means that disproportionately marginalized groups will be impacted, including sex workers, racialized groups, queer folks. And you&#x27;ll have to use automated content moderation, which has all the false positives and false negatives that haven&#x27;t reported on.</p><p>So the key recommendation we made to policymakers here are very foundational, I&#x27;d say. One, do not mandate LLM moderation exactly for the reason that you expressed before. Two, maintain human oversight. So if LLMs are used to moderate content, and especially to remove content, there still should be a legal requirements for platforms to integrate human-in-the-loop systems, meaning that humans will review whatever decision LLM does. Three, kind of broad transparency and accountability metrics and requirements that&#x27;s very DSA-esque... sorry, I should have said Digital Services Act. And a lot of that is really about transparency, like mandating disclosure of how LLM systems function and how they&#x27;re used. The reality, Justin, we don&#x27;t know. I had several off-the-record calls with platforms, off-the-record means I couldn&#x27;t publish the names. They also gave me very vague information. There&#x27;s some information that was published by them, and you&#x27;ll see it in the report.</p><p>ChatGPT said they use LLMs for content moderation. Meta says they&#x27;re beginning to play around with it, gemini or Google. But overall, we don&#x27;t know. I definitely don&#x27;t know when it&#x27;s used when I use social media. We don&#x27;t know accuracy rates, we don&#x27;t know how they&#x27;re used in appeals, or how they&#x27;re enforced. So really requiring platforms to notify users about LLM moderation actions. And again, that&#x27;s nothing new, I would say. That&#x27;s just using prior Santa Clara principles on transparency and accountability, or DSA or kind of like mainstream civil-society asks and implementing them for LLMs.</p><p>The two last ones that we highlighted, one is mandating human rights impact assessments. So hey, platforms, good news. I did one here, so you can use... My goal is that this will be a starting point for platforms to have basically an HRA handed to them on a silver platter, and then obviously use this as a starting point to look at how they implement LLMs on their platform. It&#x27;ll be specific to each platform.</p><p>But on that note, one thing that policymakers could do is make HRAs mandatory for LLM developers and platforms that deploy them, both before deployment and throughout the LLM lifecycle. So for example, the pilot with Discord was at the ideation phase, so design, product design. Before they even developed the product, they consulted with a lot of folks to see how an LLM or machine-learning-driven system could be helpful. Is it nudges? Is it for removing content? Do we not want that? Why? And then continue that throughout all the way through development and deployment and make it accessible for external stakeholders.</p><p>There&#x27;s so much expertise in the room, and I often say to platforms that, you know, trust and safety teams, policy teams, human rights teams tend to be underfunded. Just go to civil society that has such extensive knowledge, or journalists like yourself and academics and just drink their wisdom, because there&#x27;s a lot of stuff out there.</p><p><strong>Justin Hendrix:</strong></p><p>This report seems like a good jumping-off point for a lot of folks who might be interested in investigating or collecting artifacts of how the platforms are deploying these things or generally kind of trying to pay attention to these issues and trying to discern whether the introduction of LLMs in this context is on balance a good thing, a bad thing, or perhaps somehow neutral or indiscernible. With regard to the overall information integrity environment, what would you encourage people to do next? What would you encourage them to go and look at? What threads would you like to see the field pulled from here?</p><p>Marlena Wisniak:</p><p>Yeah. One core piece that we didn&#x27;t talk about much is the multilingual piece of it, so how this can work in different languages. And I&#x27;ll just give a shout out here to really cool efforts in the global majority to develop community-driven local smaller LLMs or data sets, like the Masakhane in Africa. There&#x27;s really cool community-driven initiatives that kind of go beyond the Silicon Valley profit-first massive monopolization and dynamics. So that is one thing, and I really encourage not only researchers, but also platforms to talk to them. A lot of the platforms that I spoke with, they didn&#x27;t even know these existed, and if I say that this report is an HRA handed on a silver platter, they have really cool data sets that would be really helpful, especially to smaller platforms, and they could just plug these into their own model.</p><p>So that&#x27;s one thing that I hope research and industry will move towards is more languages, more dialects, understanding that it&#x27;s not only a difference between English and other language. It really is a colonial imperialist dynamic where English or French or German or Spanish will be much better moderated, and languages that are close to these ones are better, and then obscure, poorly researched languages work very, very poorly. And you can read in the report the reasons are both because the data sets do not exist or they&#x27;re just bad quality, because there&#x27;s not enough investment. So I really would encourage platforms to invest more resources into that, more participation, and proactively include stakeholders.</p><p>The other area I would love to see is just open conversations between platform and civil society. And like I said, this is a nerdy topic, like LLMs and content moderation. It doesn&#x27;t roll off the tongue. So if folks with expertise in content moderation would like, hopefully this report can give them more context. And then I would love to see more evidence, new ideas. Like I said, some of these things were my own kind of &quot;How could this work?&quot; But through many conversation with folks, and I really would want to see more thinking, more assessment of impacts and more evidence as well.</p><p>Like this paper, it&#x27;s long, 70 pages. I didn&#x27;t mean to make it this long, but there&#x27;s a lot of stuff. And I&#x27;d say the last part is computer science papers move so fast, incredibly fast. It&#x27;s hard to keep up, and they&#x27;re very theoretical. So to the extent that there can be more collaboration between technical folks who come from the machine learning, AI, or computer science field, and policy and human rights, I think we&#x27;ll actually be able to build much better products and push policymakers to regulate this stuff better.</p><p><strong>Justin Hendrix:</strong></p><p>I appreciate you taking the time to speak to me about this, and I would encourage my readers to go and check out this full report, which is available on the ecnl.org website. I will include a link to it in the show notes. Thank you very much.</p><p><strong>Marlena Wisniak:</strong></p><p>Thanks, Justin.</p></div></div></div><style data-emotion=\"css 19egsyp\">.css-19egsyp{box-sizing:border-box;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:100%;}@media (min-width:600px){.css-19egsyp{-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:100%;}}@media (min-width:900px){.css-19egsyp{-webkit-flex-basis:33.333333%;-ms-flex-preferred-size:33.333333%;flex-basis:33.333333%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:33.333333%;}}@media (min-width:1200px){.css-19egsyp{-webkit-flex-basis:33.333333%;-ms-flex-preferred-size:33.333333%;flex-basis:33.333333%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:33.333333%;}}@media (min-width:1536px){.css-19egsyp{-webkit-flex-basis:33.333333%;-ms-flex-preferred-size:33.333333%;flex-basis:33.333333%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:33.333333%;}}</style><div class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-md-4 css-19egsyp\"><style data-emotion=\"css 8qb8m4\">.css-8qb8m4{margin-bottom:48px;}</style><div class=\"MuiBox-root css-8qb8m4\"><style data-emotion=\"css nnwgpb\">.css-nnwgpb{margin:0;font-family:'Lexend',sans-serif;font-size:1em;font-weight:700;margin-bottom:20px;text-transform:uppercase;line-height:1.235;border-bottom:1px solid #8AA29D;padding-bottom:16px;width:100%;}</style><h2 class=\"MuiTypography-root MuiTypography-h4 css-nnwgpb\">Authors</h2><style data-emotion=\"css 1rrerex\">.css-1rrerex{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-1rrerex>:not(style):not(style){margin:0;}.css-1rrerex>:not(style)~:not(style){margin-top:32px;}</style><div class=\"MuiStack-root css-1rrerex\"><style data-emotion=\"css 1k82yfd\">.css-1k82yfd{box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;width:100%;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;margin-top:-16px;width:calc(100% + 16px);margin-left:-16px;-webkit-flex-basis:calc(100% + 16px);-ms-flex-preferred-size:calc(100% + 16px);flex-basis:calc(100% + 16px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(100% + 16px);}.css-1k82yfd>.MuiGrid-item{padding-top:16px;}.css-1k82yfd>.MuiGrid-item{padding-left:16px;}@media (min-width:600px){.css-1k82yfd{-webkit-flex-basis:calc(100% + 16px);-ms-flex-preferred-size:calc(100% + 16px);flex-basis:calc(100% + 16px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(100% + 16px);}}@media (min-width:900px){.css-1k82yfd{-webkit-flex-basis:calc(100% + 16px);-ms-flex-preferred-size:calc(100% + 16px);flex-basis:calc(100% + 16px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(100% + 16px);}}@media (min-width:1200px){.css-1k82yfd{-webkit-flex-basis:calc(100% + 16px);-ms-flex-preferred-size:calc(100% + 16px);flex-basis:calc(100% + 16px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(100% + 16px);}}@media (min-width:1536px){.css-1k82yfd{-webkit-flex-basis:calc(100% + 16px);-ms-flex-preferred-size:calc(100% + 16px);flex-basis:calc(100% + 16px);-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:calc(100% + 16px);}}</style><div class=\"MuiGrid-root MuiGrid-container MuiGrid-item MuiGrid-spacing-xs-2 MuiGrid-grid-xs-12 css-1k82yfd\"><style data-emotion=\"css q4iyp1\">.css-q4iyp1{box-sizing:border-box;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-basis:25%;-ms-flex-preferred-size:25%;flex-basis:25%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:25%;padding-left:0!important;}@media (min-width:600px){.css-q4iyp1{-webkit-flex-basis:25%;-ms-flex-preferred-size:25%;flex-basis:25%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:25%;}}@media (min-width:900px){.css-q4iyp1{-webkit-flex-basis:25%;-ms-flex-preferred-size:25%;flex-basis:25%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:25%;}}@media (min-width:1200px){.css-q4iyp1{-webkit-flex-basis:25%;-ms-flex-preferred-size:25%;flex-basis:25%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:25%;}}@media (min-width:1536px){.css-q4iyp1{-webkit-flex-basis:25%;-ms-flex-preferred-size:25%;flex-basis:25%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:25%;}}</style><div class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-3 css-q4iyp1\"><img alt=\"\" loading=\"lazy\" width=\"80\" height=\"80\" decoding=\"async\" data-nimg=\"1\" style=\"color:transparent;border-radius:50px\" src=\"https://cdn.sanity.io/images/3tzzh18d/production/13e89d005fe677a1c18c1b8be04209d7849a7de8-748x748.jpg?fit=max&amp;auto=format\"/></div><style data-emotion=\"css 14ybvol\">.css-14ybvol{box-sizing:border-box;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-basis:75%;-ms-flex-preferred-size:75%;flex-basis:75%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:75%;}@media (min-width:600px){.css-14ybvol{-webkit-flex-basis:75%;-ms-flex-preferred-size:75%;flex-basis:75%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:75%;}}@media (min-width:900px){.css-14ybvol{-webkit-flex-basis:75%;-ms-flex-preferred-size:75%;flex-basis:75%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:75%;}}@media (min-width:1200px){.css-14ybvol{-webkit-flex-basis:75%;-ms-flex-preferred-size:75%;flex-basis:75%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:75%;}}@media (min-width:1536px){.css-14ybvol{-webkit-flex-basis:75%;-ms-flex-preferred-size:75%;flex-basis:75%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:75%;}}</style><div class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-9 css-14ybvol\"><style data-emotion=\"css 18fhwlr\">.css-18fhwlr{-webkit-text-decoration:underline;text-decoration:underline;text-decoration-color:rgba(0, 0, 0, 0.4);-webkit-text-decoration:none;text-decoration:none;margin-bottom:10px;display:block;}.css-18fhwlr:hover{text-decoration-color:inherit;}.css-18fhwlr:active,.css-18fhwlr:focus,.css-18fhwlr:hover{color:#000;-webkit-text-decoration:underline;text-decoration:underline;}</style><style data-emotion=\"css ch6i07\">.css-ch6i07{margin:0;font:inherit;color:#000;-webkit-text-decoration:underline;text-decoration:underline;text-decoration-color:rgba(0, 0, 0, 0.4);-webkit-text-decoration:none;text-decoration:none;margin-bottom:10px;display:block;}.css-ch6i07:hover{text-decoration-color:inherit;}.css-ch6i07:active,.css-ch6i07:focus,.css-ch6i07:hover{color:#000;-webkit-text-decoration:underline;text-decoration:underline;}</style><a class=\"MuiTypography-root MuiTypography-inherit MuiLink-root MuiLink-underlineAlways css-ch6i07\" href=\"/author/justin-hendrix\"><style data-emotion=\"css 1ytgyi9\">.css-1ytgyi9{margin:0;font-family:'Lexend',sans-serif;font-size:1em;font-weight:700;margin-bottom:20px;text-transform:uppercase;line-height:1.235;color:#000;font-weight:400;}</style><span class=\"MuiTypography-root MuiTypography-h4 css-1ytgyi9\">Justin Hendrix</span></a><style data-emotion=\"css 1qd0hdj\">.css-1qd0hdj{margin:0;font-family:'Lexend',sans-serif;font-weight:300;line-height:1.4;font-size:0.875rem;color:rgba(0,0,0,0.48);}</style><div class=\"MuiTypography-root MuiTypography-body2 css-1qd0hdj\">Justin Hendrix is CEO and Editor of Tech Policy Press, a nonprofit media venture concerned with the intersection of technology and democracy. Previously, he was Executive Director of NYC Media Lab. He spent over a decade at The Economist in roles including Vice President of Business Development &amp; In<!-- -->...</div></div></div></div></div><section><style data-emotion=\"css 1d3bbye\">.css-1d3bbye{box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;width:100%;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}</style><div class=\"MuiGrid-root MuiGrid-container css-1d3bbye\"><style data-emotion=\"css 8x8291\">.css-8x8291{box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;width:100%;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}</style><div class=\"MuiGrid-root MuiGrid-container MuiGrid-item css-8x8291\"><style data-emotion=\"css 15j76c0\">.css-15j76c0{box-sizing:border-box;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:100%;}@media (min-width:600px){.css-15j76c0{-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:100%;}}@media (min-width:900px){.css-15j76c0{-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:100%;}}@media (min-width:1200px){.css-15j76c0{-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:100%;}}@media (min-width:1536px){.css-15j76c0{-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:100%;}}</style><div class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 css-15j76c0\"><style data-emotion=\"css 1ht81qj\">.css-1ht81qj{margin:0;font-family:'Lexend',sans-serif;font-size:1em;font-weight:700;margin-bottom:20px;text-transform:uppercase;line-height:1.235;}</style><h2 class=\"MuiTypography-root MuiTypography-h4 css-1ht81qj\">Related</h2></div><style data-emotion=\"css iutjyj\">.css-iutjyj{margin-bottom:32px;padding-top:16px;border-top:1px solid #8AA29D;width:100%;}</style><div class=\"MuiBox-root css-iutjyj\"><style data-emotion=\"css yd8sa2\">.css-yd8sa2{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:16px;}</style><div class=\"MuiBox-root css-yd8sa2\"><div class=\"jss8\"><style data-emotion=\"css 8t01r4\">.css-8t01r4{-webkit-text-decoration:underline;text-decoration:underline;text-decoration-color:rgba(0, 0, 0, 0.4);-webkit-text-decoration:none;text-decoration:none;}.css-8t01r4:hover{text-decoration-color:inherit;}</style><style data-emotion=\"css 1l2w1fg\">.css-1l2w1fg{margin:0;font:inherit;color:#000;-webkit-text-decoration:underline;text-decoration:underline;text-decoration-color:rgba(0, 0, 0, 0.4);-webkit-text-decoration:none;text-decoration:none;}.css-1l2w1fg:hover{text-decoration-color:inherit;}</style><a class=\"MuiTypography-root MuiTypography-inherit MuiLink-root MuiLink-underlineAlways css-1l2w1fg\" href=\"/syllabus-large-language-models-content-moderation-and-political-communication\">Syllabus: Large Language Models, Content Moderation, and Political Communication</a><style data-emotion=\"css ssjgjc\">.css-ssjgjc{margin:0;font-family:'Lexend',sans-serif;font-weight:300;line-height:1.4;font-size:0.875rem;color:#a7a7a7;font-size:14px;font-weight:400;margin-top:8px;text-transform:uppercase;}</style><span class=\"MuiTypography-root MuiTypography-body2 css-ssjgjc\">September 26, 2024</span></div><div class=\"jss8\"><a class=\"MuiTypography-root MuiTypography-inherit MuiLink-root MuiLink-underlineAlways css-1l2w1fg\" href=\"/using-llms-to-moderate-content-are-they-ready-for-commercial-use\">Using LLMs to Moderate Content: Are They Ready for Commercial Use?</a><span class=\"MuiTypography-root MuiTypography-body2 css-ssjgjc\">April 3, 2024</span></div><div class=\"jss8\"><a class=\"MuiTypography-root MuiTypography-inherit MuiLink-root MuiLink-underlineAlways css-1l2w1fg\" href=\"/is-generative-ai-the-answer-for-the-failures-of-content-moderation\">Is Generative AI the Answer for the Failures of Content Moderation?</a><span class=\"MuiTypography-root MuiTypography-body2 css-ssjgjc\">April 3, 2024</span></div><div class=\"jss8\"><a class=\"MuiTypography-root MuiTypography-inherit MuiLink-root MuiLink-underlineAlways css-1l2w1fg\" href=\"/using-llms-for-policy-driven-content-classification\">Using LLMs for Policy-Driven Content Classification</a><span class=\"MuiTypography-root MuiTypography-body2 css-ssjgjc\">January 29, 2024</span></div><div class=\"jss8\"><a class=\"MuiTypography-root MuiTypography-inherit MuiLink-root MuiLink-underlineAlways css-1l2w1fg\" href=\"/an-advocates-guide-to-automated-content-moderation\">An Advocate’s Guide to Automated Content Moderation</a><span class=\"MuiTypography-root MuiTypography-body2 css-ssjgjc\">February 12, 2025</span></div></div></div></div></div></section><section><div class=\"MuiGrid-root MuiGrid-container css-1d3bbye\"><div class=\"MuiGrid-root MuiGrid-container MuiGrid-item css-8x8291\"><div class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 css-15j76c0\"><h2 class=\"MuiTypography-root MuiTypography-h4 css-1ht81qj\">Topics</h2></div><div class=\"MuiBox-root css-iutjyj\"><style data-emotion=\"css 1gjxtvh\">.css-1gjxtvh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}.css-1gjxtvh>:not(style):not(style){margin:0;}.css-1gjxtvh>:not(style)~:not(style){margin-left:8px;}</style><div class=\"MuiStack-root css-1gjxtvh\"></div></div></div></div></section></div></div></div></div></main><footer style=\"background-color:#3475BF;color:#FFF\"><style data-emotion=\"css uek6ck\">.css-uek6ck{padding-top:64px;padding-bottom:32px;}</style><div class=\"MuiBox-root css-uek6ck\"><style data-emotion=\"css 1qsxih2\">.css-1qsxih2{width:100%;margin-left:auto;box-sizing:border-box;margin-right:auto;display:block;padding-left:16px;padding-right:16px;}@media (min-width:600px){.css-1qsxih2{padding-left:24px;padding-right:24px;}}@media (min-width:1200px){.css-1qsxih2{max-width:1200px;}}</style><div class=\"MuiContainer-root MuiContainer-maxWidthLg css-1qsxih2\"><style data-emotion=\"css 1h77wgb\">.css-1h77wgb{box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;width:100%;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;margin-top:-24px;width:calc(100% + 24px);margin-left:-24px;}.css-1h77wgb>.MuiGrid-item{padding-top:24px;}.css-1h77wgb>.MuiGrid-item{padding-left:24px;}</style><div class=\"MuiGrid-root MuiGrid-container MuiGrid-spacing-xs-3 css-1h77wgb\"><style data-emotion=\"css 1t2vos\">.css-1t2vos{box-sizing:border-box;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:100%;}@media (min-width:600px){.css-1t2vos{-webkit-flex-basis:16.666667%;-ms-flex-preferred-size:16.666667%;flex-basis:16.666667%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:16.666667%;}}@media (min-width:900px){.css-1t2vos{-webkit-flex-basis:16.666667%;-ms-flex-preferred-size:16.666667%;flex-basis:16.666667%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:16.666667%;}}@media (min-width:1200px){.css-1t2vos{-webkit-flex-basis:16.666667%;-ms-flex-preferred-size:16.666667%;flex-basis:16.666667%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:16.666667%;}}@media (min-width:1536px){.css-1t2vos{-webkit-flex-basis:16.666667%;-ms-flex-preferred-size:16.666667%;flex-basis:16.666667%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:16.666667%;}}</style><div class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-sm-2 css-1t2vos\"><a href=\"/\"><style data-emotion=\"css 1gtfl7l\">.css-1gtfl7l{width:120px;}</style><div class=\"MuiBox-root css-1gtfl7l\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 741.5 299.6\"><g fill=\"#fff\"><path d=\"M284 177a24 24 0 0 0-16-11 26 26 0 0 0-18 3 24 24 0 0 0-7 5 22 22 0 0 0-5 17 26 26 0 0 0 16 21 24 24 0 0 0 9 2 24 24 0 0 0 18-9 24 24 0 0 0 5-9 24 24 0 0 0-2-19Zm-59 49a22 22 0 0 0-15 5l-1 1a22 22 0 0 0-7 14 24 24 0 0 0 4 16 22 22 0 0 0 16 10 23 23 0 0 0 17-6 23 23 0 0 0 8-16v-1a22 22 0 0 0-7-16 24 24 0 0 0-15-7Zm-53 35a22 22 0 0 0-7-3 20 20 0 0 0-8-1 18 18 0 0 0-7 3 21 21 0 0 0-6 5 23 23 0 0 0-5 15 20 20 0 0 0 7 15 21 21 0 0 0 8 4 22 22 0 0 0 9 0c3 0 5-1 8-3a20 20 0 0 0 7-6 20 20 0 0 0 2-16 23 23 0 0 0-8-13Zm-74-11a17 17 0 0 0-3-1 17 17 0 0 0-4 0 19 19 0 0 0-7 1 21 21 0 0 0-11 9 19 19 0 0 0-2 15 19 19 0 0 0 4 7 22 22 0 0 0 3 3l4 2a20 20 0 0 0 16 0 19 19 0 0 0 10-11 21 21 0 0 0 0-14 19 19 0 0 0-10-11Zm-46-38h-1a17 17 0 0 0-10-7 19 19 0 0 0-13 1 17 17 0 0 0-9 11 18 18 0 0 0 2 15 17 17 0 0 0 6 5 18 18 0 0 0 6 3 17 17 0 0 0 14-4 19 19 0 0 0 7-11v-7a17 17 0 0 0-2-6Zm-19-58c0-4-2-8-5-11a18 18 0 0 0-11-5c-4-1-8 1-12 4-3 3-5 7-5 12s2 9 5 12a15 15 0 0 0 12 4 18 18 0 0 0 11-6c3-3 5-6 5-10Zm-4-56a16 16 0 0 0 11 2c4-1 7-3 9-6a14 14 0 0 0 1-10 16 16 0 0 0-5-10 14 14 0 0 0-11-3 14 14 0 0 0-6 2 14 14 0 0 0-4 5c-2 3-3 8-2 12a14 14 0 0 0 7 8Zm57-46a13 13 0 0 0 7 0l2-1a12 12 0 0 0 6-6 14 14 0 0 0 1-10 12 12 0 0 0-7-8 13 13 0 0 0-11 1l-2 1a15 15 0 0 0-2 2 13 13 0 0 0-3 4c-1 4 0 7 2 10s4 6 7 7Zm59 3 11 4h4l7-1 9-4v-1a31 31 0 0 0 12-17c2-8 1-16-4-23a28 28 0 0 0-8-9h-1a29 29 0 0 0-11-4 31 31 0 0 0-12 1 29 29 0 0 0-11 6 29 29 0 0 0-10 21 31 31 0 0 0 6 20 29 29 0 0 0 8 7Zm61 25a27 27 0 0 0 19 6 29 29 0 0 0 19-8 27 27 0 0 0 8-20 28 28 0 0 0-9-20 28 28 0 0 0-22-7 27 27 0 0 0-19 12 29 29 0 0 0-5 19 27 27 0 0 0 9 18Zm82 31a26 26 0 0 0-5-10 25 25 0 0 0-29-7 27 27 0 0 0-14 13h-1a27 27 0 0 0-2 9v10a25 25 0 0 0 13 15 28 28 0 0 0 18 3 25 25 0 0 0 17-12 26 26 0 0 0 4-10l-1-11ZM75 136a12 12 0 0 0 8 6c3 0 6 0 9-2a12 12 0 0 0 6-7v-4a13 13 0 0 0-8-10 12 12 0 0 0-4-1 12 12 0 0 0-9 4 12 12 0 0 0-3 5 12 12 0 0 0 1 9Zm30-24a11 11 0 0 0 7-3c2-2 4-4 4-7l-2-8a11 11 0 0 0-8-5 11 11 0 0 0-9 3 11 11 0 0 0-4 8 11 11 0 0 0 4 9 12 12 0 0 0 8 3Zm26-17a10 10 0 0 0 7 1 9 9 0 0 0 4-1 10 10 0 0 0 3-2 11 11 0 0 0 2-8 10 10 0 0 0-4-7 11 11 0 0 0-3-2 11 11 0 0 0-5 0 11 11 0 0 0-7 4 10 10 0 0 0-1 8c0 3 2 5 4 7Zm37 5h1a9 9 0 0 0 5 0 10 10 0 0 0 6-4 9 9 0 0 0-1-11l-2-2a10 10 0 0 0-5-2 9 9 0 0 0-9 7v7a9 9 0 0 0 5 5Zm23 19a8 8 0 0 0 5 3h6a9 9 0 0 0 5-6 9 9 0 0 0-1-7 9 9 0 0 0-3-2 9 9 0 0 0-5-2l-5 2a9 9 0 0 0-3 6 9 9 0 0 0-1 3 9 9 0 0 0 2 3Zm9 29a8 8 0 0 0 2 5 9 9 0 0 0 6 3 8 8 0 0 0 5-2 8 8 0 0 0 3-6 8 8 0 0 0-2-6 8 8 0 0 0-6-2c-3 0-4 1-6 3a7 7 0 0 0-2 5Zm2 27a8 8 0 0 0-6 0 7 7 0 0 0-4 3 7 7 0 0 0-1 5 8 8 0 0 0 3 4 7 7 0 0 0 5 2 7 7 0 0 0 3-1 7 7 0 0 0 2-3 7 7 0 0 0 1-5 7 7 0 0 0-3-5Zm-29 23a6 6 0 0 0-2 0h-1l-1 1a6 6 0 0 0-3 3 7 7 0 0 0 0 5 6 6 0 0 0 3 4l5-1h1l1-1a6 6 0 0 0 1-7 7 7 0 0 0-4-4Zm-29-1a13 13 0 0 0-5-2 13 13 0 0 0-6 0 15 15 0 0 0-4 3 15 15 0 0 0-6 8 14 14 0 0 0 2 12l4 4a15 15 0 0 0 6 2h6a14 14 0 0 0 10-14c0-3-1-7-3-10a14 14 0 0 0-4-3Zm-30-12v-1a13 13 0 0 0-9-3h-1c-3 0-6 2-9 4a13 13 0 0 0-4 10c0 4 2 8 5 10a14 14 0 0 0 10 4 14 14 0 0 0 10-6 14 14 0 0 0 2-10 13 13 0 0 0-4-8Zm-41-16a13 13 0 0 0 3 5 13 13 0 0 0 14 3c3-1 6-3 7-6a13 13 0 0 0 2-4 12 12 0 0 0-3-10 13 13 0 0 0-4-3 14 14 0 0 0-9-1 13 13 0 0 0-10 10v6Zm71-36a17 17 0 1 0 0 34 17 17 0 0 0 0-34ZM362 108c10-2 17-4 17-12V25c0-4-2-5-5-5h-3c-12 0-22 5-24 10-2 3-8 3-7 0l4-19c0-3 1-3 4-2l43 1 46-1c3-1 6-1 5 2a173 173 0 0 0-5 19c-1 3-5 3-6 0-1-5-8-10-20-10h-4c-3 0-6 1-6 5v71c0 8 7 10 17 12 1 0 1 5-1 5h-54l-1-5Zm132-2c-5 2-17 10-20 10-19 0-42-8-42-35 0-18 11-39 37-39 23 0 26 16 26 22 0 4-2 11-4 14l-40 2c3 16 16 23 28 23l13-2c2 0 3 4 2 5Zm-44-35 28-1c1-9-5-17-14-17-10 0-14 7-14 18Zm49 10c0-18 11-39 41-39 9 0 20 3 19 7l-2 6c-3 15-11-2-25-2-10 0-14 9-14 19 0 19 15 31 25 31l14-2c1 0 3 4 2 5-5 2-16 10-19 10-18 0-41-9-41-35Z\"></path><path d=\"M559 108c4-1 10-3 10-11V33c0-16-2-19-11-20l1-5 29-6a2 2 0 0 1 3 3c-3 9-3 28-3 40 0 3 2 5 4 4 8-3 15-7 24-7 14 0 19 11 19 20v35c0 8 6 10 10 11l-1 5h-36l-1-5c6-1 9-4 9-11V71c0-13-4-16-15-16-5 0-13 2-13 6v36c0 7 3 10 10 11l-1 5h-37l-1-5Zm-164 84v20c0 10 8 12 17 13l-1 6h-48l-1-6c8-1 12-4 12-13v-64c0-10-5-13-12-15-1 0-1-6 1-6l17 1 29-1c25 0 44 9 44 32 0 26-25 37-58 33Zm0-44v36c24 3 35-4 35-22 0-14-9-26-25-26-8 0-10 3-10 12Zm59 49c0-21 16-37 37-37s37 14 37 36-15 37-37 37c-21 0-37-14-37-36Zm55 7c0-20-9-34-22-34-7 0-14 4-14 19 0 19 9 34 22 34 7 0 14-4 14-19Z\"></path><path d=\"M528 226c4-1 10-3 10-12v-63c0-16-3-19-12-20l1-5 30-6c1 0 3 1 2 3-1 4-2 14-2 34v57c0 9 5 11 9 12l-1 5h-36c-1 0-3-5-1-5Z\"></path><path d=\"M564 226c5-1 10-3 10-12v-30c0-10-2-12-9-13-2 0-1-4 0-4l29-7c1 0 2 1 2 3a116 116 0 0 0-3 23v28c0 9 6 11 10 12l-1 5h-37l-1-5Zm6-91c0-4 8-12 12-12s13 8 13 12-8 13-13 13-12-8-12-13Z\"></path><path d=\"M601 199c0-18 12-39 41-39 9 0 21 2 19 7l-1 6c-4 14-12-3-26-3-10 0-13 9-13 19 0 20 15 32 24 32l14-2c2-1 3 4 2 4-5 3-16 10-19 10-18 0-41-9-41-34ZM740 168c-4 1-8 6-12 15l-29 64c-5 11-16 12-21 12-8 0-12-4-15-10-3-8 1-5 12-5 10 0 18-2 22-13l-21-48c-5-10-8-14-13-15-1 0-1-6 1-6h33c2 0 2 5 0 6-4 3-5 6 0 15l10 25 11-24c4-10 3-15-3-16-2 0-2-6 0-6h25c2 0 2 6 0 6ZM361 292c0-2 3-5 5-5s5 3 5 5-3 5-5 5-5-3-5-5Zm29-11v8c0 4 3 5 6 5v2h-19v-2c3 0 5-2 5-5v-26c0-3-2-5-5-5v-3l7 1 12-1c9 0 17 4 17 13 0 10-10 15-23 13Zm0-18v15c9 1 14-2 14-9 0-6-4-10-10-10-3 0-4 1-4 4Zm73 34c-4 1-10 1-14-4l-8-11c-3-4-6-4-9-4v11c0 4 3 5 6 5v2h-19l-1-2c3 0 5-2 5-5v-26c0-3-2-5-5-5l1-3 6 1 13-1c10 0 15 4 15 11s-5 10-11 10v1c3 0 5 1 8 4l8 10c2 3 4 4 5 4v2Zm-19-29c0-6-3-9-8-9-4 0-4 1-4 4v12h1c9 0 11-2 11-7Zm22 26c3-1 6-1 6-6v-25c0-4-3-5-5-5v-3l16 1 14-1 1 3-1 5h-2c-1-3-3-4-10-4-3 0-5 1-5 4v9c0 2 1 2 2 2h2l10-2h2l-3 8h-3c-1-2-4-3-7-3h-1c-1 0-2 0-2 2v9c0 4 2 5 6 5 8 0 12-4 13-6 1-1 3 0 3 1 0 2-3 9-7 8h-29v-2Zm42-1v-3c2-7 4 3 14 3 4 0 6-2 6-5 0-2-1-3-6-6l-4-3c-7-5-10-7-10-12 0-8 8-12 15-12 4 0 10 2 9 4l-1 2c-2 6-3-2-11-2-3 0-5 2-5 5 0 2 1 4 5 6l5 3c7 5 9 7 9 12 0 7-7 12-15 12-5 0-12-2-11-4Zm33 0v-3c2-7 4 3 14 3 5 0 6-2 6-5 0-2-1-3-6-6l-4-3c-7-5-10-7-10-12 0-8 8-12 15-12 4 0 10 2 9 4v2c-2 6-4-2-12-2-3 0-5 2-5 5 0 2 1 4 5 6l5 3c7 5 9 7 9 12 0 7-7 12-15 12-4 0-12-2-11-4Z\"></path></g></svg></div><style data-emotion=\"css 1sm5mum\">.css-1sm5mum{margin:0;font-size:14px;line-height:1.4;font-family:'Libre Baskerville',serif;font-weight:400;display:none;}</style><span class=\"MuiTypography-root MuiTypography-body1 css-1sm5mum\">Home</span></a></div><style data-emotion=\"css h3o6is\">.css-h3o6is{box-sizing:border-box;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:100%;}@media (min-width:600px){.css-h3o6is{-webkit-flex-basis:33.333333%;-ms-flex-preferred-size:33.333333%;flex-basis:33.333333%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:33.333333%;}}@media (min-width:900px){.css-h3o6is{-webkit-flex-basis:33.333333%;-ms-flex-preferred-size:33.333333%;flex-basis:33.333333%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:33.333333%;}}@media (min-width:1200px){.css-h3o6is{-webkit-flex-basis:33.333333%;-ms-flex-preferred-size:33.333333%;flex-basis:33.333333%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:33.333333%;}}@media (min-width:1536px){.css-h3o6is{-webkit-flex-basis:33.333333%;-ms-flex-preferred-size:33.333333%;flex-basis:33.333333%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:33.333333%;}}</style><div class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-sm-4 css-h3o6is\"><style data-emotion=\"css 1x9sdfd\">.css-1x9sdfd{margin:0;font-family:'Lexend',sans-serif;font-weight:300;line-height:1.4;font-size:0.875rem;margin-bottom:16px;max-width:275px;}</style><div class=\"MuiTypography-root MuiTypography-body2 css-1x9sdfd\">A nonprofit media and community venture intended to provoke new ideas, debate and discussion at the intersection of technology and democracy.</div><a href=\"https://techpolicy.press/rss/feed.xml\"><style data-emotion=\"css aibun8\">.css-aibun8{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:1em;height:1em;display:inline-block;fill:currentColor;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;-webkit-transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;font-size:1.5rem;background-color:#FFF;border-radius:4px;fill:#3475BF;margin-right:8px;}</style><svg class=\"MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-aibun8\" focusable=\"false\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" data-testid=\"RssFeedIcon\"><circle cx=\"6.18\" cy=\"17.82\" r=\"2.18\"></circle><path d=\"M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z\"></path></svg><span class=\"MuiTypography-root MuiTypography-body1 css-1sm5mum\">TPP RSS feed</span></a><a href=\"mailto:newsletter@techpolicy.press\"><style data-emotion=\"css 1ct922h\">.css-1ct922h{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:1em;height:1em;display:inline-block;fill:currentColor;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;-webkit-transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;font-size:1.5rem;background-color:#FFF;border-radius:4px;fill:#3475BF;}</style><svg class=\"MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-1ct922h\" focusable=\"false\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" data-testid=\"EmailIcon\"><path d=\"M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4-8 5-8-5V6l8 5 8-5v2z\"></path></svg><span class=\"MuiTypography-root MuiTypography-body1 css-1sm5mum\">Email TPP</span></a></div><style data-emotion=\"css 1xgnvjv\">.css-1xgnvjv{box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;width:100%;margin:0;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:100%;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;height:150px;}.css-1xgnvjv>.MuiGrid-item{max-width:none;}@media (min-width:600px){.css-1xgnvjv{-webkit-flex-basis:50%;-ms-flex-preferred-size:50%;flex-basis:50%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:50%;}}@media (min-width:900px){.css-1xgnvjv{-webkit-flex-basis:50%;-ms-flex-preferred-size:50%;flex-basis:50%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:50%;}}@media (min-width:1200px){.css-1xgnvjv{-webkit-flex-basis:50%;-ms-flex-preferred-size:50%;flex-basis:50%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:50%;}}@media (min-width:1536px){.css-1xgnvjv{-webkit-flex-basis:50%;-ms-flex-preferred-size:50%;flex-basis:50%;-webkit-box-flex:0;-webkit-flex-grow:0;-ms-flex-positive:0;flex-grow:0;max-width:50%;}}</style><div class=\"MuiGrid-root MuiGrid-container MuiGrid-item MuiGrid-direction-xs-column MuiGrid-grid-xs-12 MuiGrid-grid-sm-6 css-1xgnvjv\"><style data-emotion=\"css kzxgfv\">.css-kzxgfv{box-sizing:border-box;margin:0;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;margin-top:8px;margin-bottom:8px;}</style><div class=\"MuiGrid-root MuiGrid-item css-kzxgfv\"><a style=\"text-decoration:none\" href=\"/about-us/\"><style data-emotion=\"css 1it5f8v\">.css-1it5f8v{margin:0;font-family:'Lexend',sans-serif;font-size:0.81em;margin-bottom:20px;text-transform:uppercase;font-weight:400;line-height:1.334;color:#FFF;font-size:14px;-webkit-text-decoration:none;text-decoration:none;text-transform:none;}.css-1it5f8v:hover,.css-1it5f8v:focus{-webkit-text-decoration:underline;text-decoration:underline;}</style><span class=\"MuiTypography-root MuiTypography-h5 css-1it5f8v\">About</span></a></div><div class=\"MuiGrid-root MuiGrid-item css-kzxgfv\"><a style=\"text-decoration:none\" href=\"/donate/\"><span class=\"MuiTypography-root MuiTypography-h5 css-1it5f8v\">Donate</span></a></div><div class=\"MuiGrid-root MuiGrid-item css-kzxgfv\"><a style=\"text-decoration:none\" href=\"/privacy/\"><span class=\"MuiTypography-root MuiTypography-h5 css-1it5f8v\">Privacy Policy</span></a></div><div class=\"MuiGrid-root MuiGrid-item css-kzxgfv\"><a style=\"text-decoration:none\" href=\"/fellowships/\"><span class=\"MuiTypography-root MuiTypography-h5 css-1it5f8v\">Fellows</span></a></div><div class=\"MuiGrid-root MuiGrid-item css-kzxgfv\"><a style=\"text-decoration:none\" href=\"/contributors/\"><span class=\"MuiTypography-root MuiTypography-h5 css-1it5f8v\">Contributors</span></a></div><div class=\"MuiGrid-root MuiGrid-item css-kzxgfv\"><a style=\"text-decoration:none\" href=\"/contributor-guidelines/\"><span class=\"MuiTypography-root MuiTypography-h5 css-1it5f8v\">Submissions</span></a></div><div class=\"MuiGrid-root MuiGrid-item css-kzxgfv\"><a style=\"text-decoration:none\" href=\"/search/\"><span class=\"MuiTypography-root MuiTypography-h5 css-1it5f8v\">Articles</span></a></div><div class=\"MuiGrid-root MuiGrid-item css-kzxgfv\"><a style=\"text-decoration:none\" href=\"/podcast/\"><span class=\"MuiTypography-root MuiTypography-h5 css-1it5f8v\">Podcast</span></a></div><div class=\"MuiGrid-root MuiGrid-item css-kzxgfv\"><a style=\"text-decoration:none\" href=\"/library/\"><span class=\"MuiTypography-root MuiTypography-h5 css-1it5f8v\">Research Library</span></a></div></div></div><style data-emotion=\"css 18p24us\">.css-18p24us{margin:0;font-family:'Lexend',sans-serif;font-size:0.81em;margin-bottom:20px;text-transform:uppercase;font-weight:400;line-height:1.334;font-size:14px;margin-top:32px;text-align:center;text-transform:none;}</style><div class=\"MuiTypography-root MuiTypography-h5 css-18p24us\">Tech Policy Press © 2025 — a 501(c)(3) organization</div></div></div></footer></div><script id=\"__NEXT_DATA__\" type=\"application/json\">{\"props\":{\"pageProps\":{\"page\":{\"_createdAt\":\"2025-07-06T14:11:43Z\",\"_id\":\"fa1278c3-386a-4f02-9d98-075a23572414\",\"_type\":\"post\",\"_updatedAt\":\"2025-07-06T18:49:48Z\",\"authors\":[{\"bio\":[{\"_key\":\"40e551f7591f\",\"_type\":\"block\",\"children\":[{\"_key\":\"40e551f7591f0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Justin Hendrix is CEO and Editor of Tech Policy Press, a nonprofit media venture concerned with the intersection of technology and democracy. Previously, he was Executive Director of NYC Media Lab. He spent over a decade at The Economist in roles including Vice President of Business Development \\u0026 Innovation. He is an adjunct professor at NYU Tandon School of Engineering and a lecturer at Cornell Tech, and serves on the editorial board at Just Security. Opinions expressed here are his own.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"name\":\"Justin Hendrix\",\"photo\":{\"_type\":\"image\",\"asset\":{\"_ref\":\"image-13e89d005fe677a1c18c1b8be04209d7849a7de8-748x748-jpg\",\"_type\":\"reference\"}},\"slug\":{\"current\":\"justin-hendrix\"}}],\"badge\":\"podcast\",\"body\":[{\"_key\":\"96edc30b6850\",\"_type\":\"block\",\"children\":[{\"_key\":\"ece5a6f69aa30\",\"_type\":\"span\",\"marks\":[\"f998f6c2044d\",\"em\"],\"text\":\"Audio of this conversation is available via your favorite podcast service.\"}],\"markDefs\":[{\"_key\":\"f998f6c2044d\",\"_type\":\"link\",\"href\":\"https://techpolicypress.captivate.fm/listen\"}],\"style\":\"normal\"},{\"_key\":\"ae6ea8298cef\",\"_type\":\"iframeEmbed\",\"embedType\":\"player.captivate.fm\",\"url\":\"https://player.captivate.fm/episode/9498004b-9b44-4855-bf5f-6e6cb0321410/\"},{\"_key\":\"224434906976\",\"_type\":\"block\",\"children\":[{\"_key\":\"40d1bdad8e8e0\",\"_type\":\"span\",\"marks\":[],\"text\":\"At Tech Policy Press, we’ve been \"},{\"_key\":\"4862d55e074e\",\"_type\":\"span\",\"marks\":[\"c4cb1e256df3\"],\"text\":\"tracking\"},{\"_key\":\"0c81cdd47636\",\"_type\":\"span\",\"marks\":[],\"text\":\" the emerging application of generative AI systems in content moderation. Recently, the \"},{\"_key\":\"ea9ca07e2ff3\",\"_type\":\"span\",\"marks\":[\"169c9f9bea03\"],\"text\":\"European Center for Not-for-Profit Law\"},{\"_key\":\"e148ec469292\",\"_type\":\"span\",\"marks\":[],\"text\":\" (ECNL) released a comprehensive report titled \"},{\"_key\":\"40d1bdad8e8e1\",\"_type\":\"span\",\"marks\":[\"c09da26d0e20\",\"em\"],\"text\":\"Algorithmic Gatekeepers: The Human Rights Impacts of LLM Content Moderation\"},{\"_key\":\"40d1bdad8e8e2\",\"_type\":\"span\",\"marks\":[],\"text\":\", which looks at the opportunities and challenges of using generative AI in content moderation systems at scale. I spoke to its primary author, ECNL senior legal manager \"},{\"_key\":\"40d1bdad8e8e5\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Marlena Wisniak\"},{\"_key\":\"40d1bdad8e8e6\",\"_type\":\"span\",\"marks\":[],\"text\":\".\"}],\"markDefs\":[{\"_key\":\"c09da26d0e20\",\"_type\":\"link\",\"href\":\"https://ecnl.org/sites/default/files/2025-04/ECNL_LLM_CM_Excecutive%20Summary_2025.pdf\"},{\"_key\":\"c4cb1e256df3\",\"_type\":\"link\",\"href\":\"https://www.techpolicy.press/syllabus-large-language-models-content-moderation-and-political-communication/\"},{\"_key\":\"169c9f9bea03\",\"_type\":\"link\",\"href\":\"https://ecnl.org/\"}],\"style\":\"normal\"},{\"_key\":\"ad836e31c123\",\"_type\":\"block\",\"children\":[{\"_key\":\"7bc11b42d887\",\"_type\":\"span\",\"marks\":[\"em\"],\"text\":\"What follows is a lightly edited transcript of the discussion.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"61e893b98ad8\",\"_type\":\"Image\",\"altText\":\"Game of Pixels x Toy Models by Elise Racine. Better Images of AI/CC by 4.0\",\"asset\":{\"_ref\":\"image-ca8393b380961a234123ad924f89ed855aed0c1d-1200x675-png\",\"_type\":\"reference\"},\"caption\":[{\"_key\":\"970ed1d56dcd\",\"_type\":\"block\",\"children\":[{\"_key\":\"2a75be071551\",\"_type\":\"span\",\"marks\":[],\"text\":\"Game of Pixels x Toy Models by Elise Racine. \"},{\"_key\":\"cf365b3e0541\",\"_type\":\"span\",\"marks\":[\"bf9bda61ea67\"],\"text\":\"Better Images of AI\"},{\"_key\":\"3d3644793b99\",\"_type\":\"span\",\"marks\":[],\"text\":\"/CC by 4.0\"}],\"markDefs\":[{\"_key\":\"bf9bda61ea67\",\"_type\":\"link\",\"href\":\"https://betterimagesofai.org/images?artist=EliseRacine\\u0026title=GameofPixelsxToyModels\"}],\"style\":\"normal\"}]},{\"_key\":\"b0cdc0cab631\",\"_type\":\"block\",\"children\":[{\"_key\":\"66a96a0badde0\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Justin Hendrix:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"b2eadfe5351c\",\"_type\":\"block\",\"children\":[{\"_key\":\"ce1170bec73f0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Marlena, can you tell us a little bit about what the ECNL does?\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"00846c364de1\",\"_type\":\"block\",\"children\":[{\"_key\":\"110849187aae0\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Marlena Wisniak:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"46a070cbd555\",\"_type\":\"block\",\"children\":[{\"_key\":\"0a7414428f9c0\",\"_type\":\"span\",\"marks\":[],\"text\":\"The Center, or ECNL as we call it, is a human rights and civil liberties organization for over 20 years. We've been mostly focusing on civic space, making sure that civil society organizations, but also grassroots orgs, activists, have a safe space to organize and do their work. We're mostly lawyers, but I have to say we're the fun lawyers. So we also do advocacy, research, and basically anything that we can do to protect and promote in enabling civic space. My team was founded in 2020, I believe, and has from the beginning focused mostly on AI, but it's more broadly the digital team at ECNL.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"0c2a495e6add\",\"_type\":\"block\",\"children\":[{\"_key\":\"1717dff0cd190\",\"_type\":\"span\",\"marks\":[],\"text\":\"So we look at how technologies, specifically emerging technologies, impact civic space and human rights. So our core human rights or civil liberties that we look at is typically privacy, freedom of expression, freedom of assembly, so rights to protest, for example, association, right to organize, and non-discrimination. And our key areas of focus substantively are typically surveillance, AI-driven surveillance and biometric surveillance, and broader and social media platforms that plays such a big role in civic space today. And that's what I'll be talking about today. But that's, in a nutshell, my team and how it fits within the broader org.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"7eeb12f41b3c\",\"_type\":\"block\",\"children\":[{\"_key\":\"564bd0780e970\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Justin Hendrix:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"bd7387e5444f\",\"_type\":\"block\",\"children\":[{\"_key\":\"d8c43e5786520\",\"_type\":\"span\",\"marks\":[],\"text\":\"Well, I'm excited to talk to you about this report that you have authored with help from a variety of different corners, but it's called Algorithmic Gatekeepers, the Human Rights Impacts of LLM Content Moderation. So this is a topic that we've been trying to follow at Tech Policy Press fairly closely, because I feel like it is the sort of intersection of a lot of things that we care about around social media, content moderation, online safety, free expression, and then, of course, artificial intelligence. And LLMs, generative AI generally, the intersection of these two things, I think, is probably one of the most interesting and possibly under-covered or under-explored, at least to date, issues at the intersection of tech and democracy.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"6591bba600a3\",\"_type\":\"block\",\"children\":[{\"_key\":\"982f094a14ed0\",\"_type\":\"span\",\"marks\":[],\"text\":\"So I want to just start by asking you how you did this report. It's quite a significant document. A lot of research obviously went into this, including gathering some new information, not just combining citations, as many people do when they produce a PDF white paper. What got you started on this and how did you go about it?\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"c0530ba7fef1\",\"_type\":\"block\",\"children\":[{\"_key\":\"b16f82c397b20\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Marlena Wisniak:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"2938e549f497\",\"_type\":\"block\",\"children\":[{\"_key\":\"986abfdf31050\",\"_type\":\"span\",\"marks\":[],\"text\":\"Yes, you're right. It was really a heavy lift. It was a research project that went on for about a year, and we collaborated with great folks including yourself. I remember, I think last year we had a call to hear your thoughts, and shout out, before I go deep into context, to Isabelle Anzabi, who was our fellow last summer, and really helped me just go through a lot of papers, mostly in computer science, and the Omidyar Network, who provided generous support for this project.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"d4947c46800f\",\"_type\":\"block\",\"children\":[{\"_key\":\"c03b98b0fc5a0\",\"_type\":\"span\",\"marks\":[],\"text\":\"And so how it started, I accidentally fell into AI in 2016, but come from content moderation in the 2010s, '11, '12, so I'd say early days of content moderation. So automated content moderation has always been a big focus of mine. I was also at Twitter at some point, overseeing their legal department, sorry, content-governance and illegal department. And living in San Francisco right now, really, the big talk on the street is always LLMs, and GenAI more broadly.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"4388a931cb1e\",\"_type\":\"block\",\"children\":[{\"_key\":\"99242ea806af0\",\"_type\":\"span\",\"marks\":[],\"text\":\"And so I started hearing various use cases of LLMs for content moderation. Most of the talk, I will say, and focus of the research and civil society community is how to moderate LLM-generated content, so how to moderate ChatGPT, for example, or Claude. That is, of course, super important. And I wanted to look at it from another angle, which is how are LLMs used for content moderation? And this I will say, Justin, has been a little bit of a chicken-and-egg conversation, or reflection, because the way how LLMs can moderate content also impacts how LLM-driven content will be moderated, and how they're moderated impacts how they moderate content. So it's hard to separate those, one from another, but I did choose to have that focus. And it's interesting because it's very much a nerdy topic and yet has real-world implications, and it's very hyped up.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"1a8bc824090c\",\"_type\":\"block\",\"children\":[{\"_key\":\"555e6fb75caf0\",\"_type\":\"span\",\"marks\":[],\"text\":\"So one of the things that I always love to do is go beyond the hype. I'm not a hype person. In fact, now I hate AI. It was fun working on AI in the 2016, '17, '18 years when nobody really cared about AI. Since ChatGPT was released, the only thing people ever want to talk about is AI. So now I want to scream. But all this to say that I did want to impact some of the real implications and post what our promises and what our, really, I'd say, not even realistic promises, but the types of impacts we want to see, because I do believe that it can helpful, and also, as this is an emerging space, prevent any possible harm. And I think folks listening to this podcast probably know that human motivation is extremely complicated. It's horrible for workers. It doesn't always, and always is an understatement, produce good outcomes.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"693584e31efa\",\"_type\":\"block\",\"children\":[{\"_key\":\"b93c59b230b90\",\"_type\":\"span\",\"marks\":[],\"text\":\"And so machine learning came as a solution to that. It was expanded during COVID, and sort of came as this silver-bullet solution. There has been increasing research showing the limitations of that. And so now, LLM is the new white horse, is that an expression? Is the savior, and there's a lot of hype. So that's where I came from. It's like, okay, let's go deeper into this. Let's review a lot of computer science papers where some of this more rigorous qualitative and quantitative work has done, translate it to policy folks, and bring a human-rights approach because that's my background. I'm an international human rights lawyer.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"4a7ef6524b18\",\"_type\":\"block\",\"children\":[{\"_key\":\"4c1cd88dd99f0\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Justin Hendrix:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"4de67a5c3d13\",\"_type\":\"block\",\"children\":[{\"_key\":\"f69b367a42750\",\"_type\":\"span\",\"marks\":[],\"text\":\"So readers of Tech Policy Press have at least had multiple pieces that we've posted on the site about this issue, and often there's been this sort of question about what is the promise potentially to offset the dangers to the labor force that currently engage in content moderation on behalf of platforms, but then also, of course, there are various perils that we can imagine as well. We're going to get into those a little bit.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"7cfb93705a07\",\"_type\":\"block\",\"children\":[{\"_key\":\"682f89a620db0\",\"_type\":\"span\",\"marks\":[],\"text\":\"But I think one thing that distinguishes your report that I wanted to just start with perhaps is that you include a technical primer. You've got a kind of set of definitions and, I think usefully, some distinctions between what's going on with LLMs for content moderation versus more standard machine-learning classifiers and recommendation mechanisms and other types of algorithmic models that have been used in content moderation for quite some time now. What do you think the listener needs to know about the technical aspect of this phenomenon, of the use of LLMs for content moderation at scale from our vantage right now in June of 2025?\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"e01852753689\",\"_type\":\"block\",\"children\":[{\"_key\":\"8f132e3c0fac0\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Marlena Wisniak:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"7769df965759\",\"_type\":\"block\",\"children\":[{\"_key\":\"ca7a7b3f6ac70\",\"_type\":\"span\",\"marks\":[],\"text\":\"Yeah, I mean, I encourage folks to look at the technical primer, that is, the audience for that are mostly folks who aren't very familiar with either industry jargon or technical terms. Leveling up, I think one thing that is really critical to consider when thinking about LLM-driven content moderation is that you have typically two layers. So one is the foundation model layer, or the LLMs. LLM are, I should have started, large-language models. And that's pretty explicit. It's large-language model, so people see it as God, or the sci-fi technology. It's really not. It's really big data sets.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"58d85fc476b8\",\"_type\":\"block\",\"children\":[{\"_key\":\"0e606fc26b450\",\"_type\":\"span\",\"marks\":[],\"text\":\"And I think we often forget that. So if we think about traditional machine learning, the difference is that this is a larger set. And so there's this implication, obviously, for privacy and other rights that we can explore later down the line. But one, considered, they're very, very, very big, enormous data sets that require a lot of computing power. And so really, there's only a handful of companies right now building these models.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"f005d09476dd\",\"_type\":\"block\",\"children\":[{\"_key\":\"582af689ad8d0\",\"_type\":\"span\",\"marks\":[],\"text\":\"But the ones that we looked at in more depth are Llama by Facebook, sorry, Meta, ChatGPT, OpenAI, Claude, Anthropic, Gemini, Google. And then right as I was finishing out the report, DeepSeek came out. So there's emerging models as well, but it's still a very small number of foundation models, given how much data and compute they use. And I often say they're really not that technically complicated. They're just bigger. And so from a concentration of power, that has a lot of importance.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a6e95068eb1c\",\"_type\":\"block\",\"children\":[{\"_key\":\"2a9446106d000\",\"_type\":\"span\",\"marks\":[],\"text\":\"And so then the platforms that I mentioned, they both develop and often use... they develop the LLMs and they use them for content moderation. But what happens for all the other ones like Discord or Reddit, or Slack? Although Slack might be bought by one of them. But anyways, there are so many other platforms. Typically, they do not build their own LLMs. They will either have a license with one of the foundation-model companies and then fine-tune them or they will use one of the open source, like DeepSeek or Llama or something they find on Hugging Face, and then fine-tune them for their purposes.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"2d6f7641f4ee\",\"_type\":\"block\",\"children\":[{\"_key\":\"516726855dfe0\",\"_type\":\"span\",\"marks\":[],\"text\":\"But what that means, so that's one thing from a technical thing to understand how LLMs work and how they're used in practice. And then another thing I will flag is that LLMs are a subset of generative AI. That's a term we most commonly know. Generative AI today has, to some extent, become equivalent of ChatGPT. It's much broader than ChatGPT, but LLMs are one subset of that, or foundation models as well.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"acd347b6544b\",\"_type\":\"block\",\"children\":[{\"_key\":\"cd75156fe7280\",\"_type\":\"span\",\"marks\":[],\"text\":\"And then maybe one last thing I'll flag is multilingual language models are those that are trained on text data from dozens to sometimes hundreds of languages simultaneously. And so the idea is that they will be more capable of processing and generating inputs and outputs in multiple languages. And we can about whether that works or not. There's fantastic research that has been done that inspired me a lot for my work.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"58768d26e262\",\"_type\":\"block\",\"children\":[{\"_key\":\"66ebf6e78ff90\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Justin Hendrix:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"f8cf0adc5a27\",\"_type\":\"block\",\"children\":[{\"_key\":\"defeee30c5190\",\"_type\":\"span\",\"marks\":[],\"text\":\"Yeah, I'll just say that the primer, I think what's great about, again, this report is that it does detail at least some of the ambition technically that people have for using large-language models in content generation and the possibilities it opens up for sorts of things that have been very hard to do or difficult, certainly at scale, with today's mechanisms, including, as you say, a big one, which is servicing many languages that previously social media firms might have decided are simply off limits for their consideration because it's just not perhaps feasible or profitable for them to address certain markets or certain languages with the type of scale that perhaps people in those places might believe is necessary in order to do a good job. So that's been a kind of consistent complaint from civil society for a long time.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"69d7645fc525\",\"_type\":\"block\",\"children\":[{\"_key\":\"28a0e5acc9aa0\",\"_type\":\"span\",\"marks\":[],\"text\":\"But there are other things here that you point to that seem to fit in the category of promise, such as possibly more robust types of nudges or interventions in what people post, or after the fact what they post. I know I've talked to people about the idea that we can imagine content moderation with LLM starting before you ever post something, kind of prefacto content moderation, and possibly in interaction with the content moderation system as it were at that point, before you even pushed something live to your feed. But I don't know. What else did you learn in terms of just studying these things technically about the possibilities they open up?\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"8f06df681811\",\"_type\":\"block\",\"children\":[{\"_key\":\"5af5ff82c6730\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Marlena Wisniak:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"d32af4415377\",\"_type\":\"block\",\"children\":[{\"_key\":\"fea14cf632f70\",\"_type\":\"span\",\"marks\":[],\"text\":\"Yeah, thanks are bringing it up. And the report does obviously highlight harms and also explores promises for each individual, right. So I'm with you. One of the key findings of this research was that probably the most promise of LLM-driven content moderation is not to remove content or even to moderate it through a ranking system or curating content, but really anything on procedural rights. So it's before posting, like you said, nudges. Even though I will say that we also don't want this Big Brother-type experience where we're typing something and, oh dear Lord, the algorithm has found that this may be controversial. But yeah, there can be nudges, there can be suggestions for reformulating something that could be abusive, less invasive. They could even suggest other informations, for example, if someone is posting something clearly wrong about... like, factually incorrect about the Ukraine war, or elections, like civic integrity stuff, they could check out xyz.gov, right, elections.gov or something.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"a1b0a246c3b6\",\"_type\":\"block\",\"children\":[{\"_key\":\"bfd0ce7df7970\",\"_type\":\"span\",\"marks\":[],\"text\":\"And also on the flip side, appeals and remedy. When a user posts content, they can immediately know that this has been flagged for... especially content that would be automatically removed, they can get an automated notice that's more personalized that this content has been flagged for review or has been removed, and give more personalized information about how they can appeal that decision. So there's all this kind of user interaction that I think is pretty cool and exciting.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"87a65e569b1d\",\"_type\":\"block\",\"children\":[{\"_key\":\"263dc951b58c0\",\"_type\":\"span\",\"marks\":[],\"text\":\"There's also the possibility for personalized content moderation. So let's say one user really welcomes gore or sensitive content, borderline content that doesn't violate either the platform's policies or the law. They can adapt and adjust their own moderation, as opposed to someone who really doesn't want to see anything remotely sensitive or related to some topics because of any trauma or just dislike or whatever. That can be helpful.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"dbfbfdf1f35c\",\"_type\":\"block\",\"children\":[{\"_key\":\"3310e6f1d0d60\",\"_type\":\"span\",\"marks\":[],\"text\":\"And I'll give a shout out here to Discord, with whom we collaborated for a year on engaging external stakeholders as they develop ML-driven interventions to moderate abuse and harassment on their platform, specifically for teens. And so we really worked with a broad range of stakeholders, including youth and children, which is interesting, and understanding how they would like to see AI-driven innovation and intervention. So it wasn't only LLM, it was also traditional machine learning.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"e53bdc1530b0\",\"_type\":\"block\",\"children\":[{\"_key\":\"c2ccfb410e1f0\",\"_type\":\"span\",\"marks\":[],\"text\":\"But yeah, so I think, to sum up creative uses of LLM for moderating content, and by moderating I mean broadly, is something that I support much more than automated removal, which one of the conclusions of this report is that, at least today, it is still too risky, potentially harmful, and just doesn't... like, ineffective to do that. There will be too many false positives and too many false negatives, both of those disproportionately falling on already marginalized groups who tend to be, as most folks, you probably know, disproportionately silenced by platform and also disproportionately targeted by violative content. So that's one of the main findings of how we can use LLMs safely is more on the procedural side than actually the content.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"105beb6e765c\",\"_type\":\"block\",\"children\":[{\"_key\":\"72664f2ba8c30\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Justin Hendrix:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"225138d1792a\",\"_type\":\"block\",\"children\":[{\"_key\":\"c45dc8f1431a0\",\"_type\":\"span\",\"marks\":[],\"text\":\"We'll see how platforms attempt to go about this. We're already seeing some in the wild, examples of uses of LLMs. There's been reporting even this week of Meta's new community notes program, using LLMs in certain ways. So I think there'll be probably as wide a variety of applications of LLMs as there have been of machine learning in content moderation, and we'll just, I suppose, see how it goes. I mean, different platforms, who knows, maybe one or two will err towards creating a AI nanny state hyper-sanitized environment that people may recoil from, or regard as overly censorious or what have you. And yet it's possible to imagine, as you say, lots of different types of interventions that people might regard as useful or helpful in whatever way.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"fe6138304605\",\"_type\":\"block\",\"children\":[{\"_key\":\"37f13450aa790\",\"_type\":\"span\",\"marks\":[],\"text\":\"But I want to pause on maybe thinking about the benefits and dig a little more into the potential human rights impacts, because that's where, of course, you spend the bulk of this report, concerned with things like privacy, freedom of expression and information and opinion, questions around peaceful assembly and association, non-discrimination, participation. Take us through a couple of those things. When you think about the most significant potential human rights impacts of the deployment of large language model systems in content moderation at scale, what do you think is most prominent?\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"7097f0de14d9\",\"_type\":\"block\",\"children\":[{\"_key\":\"20ecf2bdadbb0\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Marlena Wisniak:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"ac5c4cddfdb3\",\"_type\":\"block\",\"children\":[{\"_key\":\"a352cd32d9020\",\"_type\":\"span\",\"marks\":[],\"text\":\"So I'll just highlight some of the most specific to LLMs. One thing to consider is that LLMs often exacerbate and accelerate already-existing harms done by traditional machine learning, and traditional ML accelerates and exacerbates harms committed by humans often. So I think it's like the more scale, scale can be good for accuracy and speed obviously, and it also has, there's another side to that coin.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"365da3551a1a\",\"_type\":\"block\",\"children\":[{\"_key\":\"8a76258f09f00\",\"_type\":\"span\",\"marks\":[],\"text\":\"So some of the things you'll find the report are kind of an LLM-ified analysis of automated content moderation, but I will single out a few really new concerns related to LLMs. And so one of them is kind of a, coming back to the concentration of power issue that I mentioned before, any decision that is made at the foundation model level, unless it is proactively fine-tuned at the deployment level, that will trickle down across platforms.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"0af757583218\",\"_type\":\"block\",\"children\":[{\"_key\":\"ae4b2a41625d0\",\"_type\":\"span\",\"marks\":[],\"text\":\"So to give you an example, if Meta decides that pro-Palestinian content is considered violent or terrorist content, and there has been a lot of reporting to show that, then if another platform uses Llama without changing that specifically, any decision that Meta makes at the level of Llama will then trickle down to the other platforms. So that's something to consider from a freedom of expression angle, is generalized censorship, if it's false positive, and at the same time, content that should be removed will not be removed if the foundation model does not consider that as harmful. So that interaction is particularly important because we've never seen that before, to my knowledge at least, and the dynamics on content moderation.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"791287bbae88\",\"_type\":\"block\",\"children\":[{\"_key\":\"694baa720aa10\",\"_type\":\"span\",\"marks\":[],\"text\":\"Another big thing around freedom of information, for example, is hallucinations. So that is a very stereotypical GenAI problem. And for folks who don't know, hallucinations are -- it's content that is generated by the AI system and that is just made up and wrong. So the weird thing is that it does in a way that seems so confident and so right, and it is just nonsense. So it'll make up academic papers, or it'll make up news articles or any kind of facts.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"108a8ebd1edb\",\"_type\":\"block\",\"children\":[{\"_key\":\"dbddb85f53df0\",\"_type\":\"span\",\"marks\":[],\"text\":\"So if platforms use this to moderate missing this information, that will just be inaccurate to begin with. And it's hard sometimes to parse that when you have pretty convincing content. And even if, for example, human moderators would use LLMs or GenAI to help them moderate content, if they see this really elaborate article about how, whatever, Trump won the 2020 elections, for example, and they're not familiar with it, that could form the base of their decisions, and it's just plain wrong. So that's the new harm and risk.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"e695cec3d0aa\",\"_type\":\"block\",\"children\":[{\"_key\":\"1711f4c16e7e0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Another one that I found was super interesting, and this, I will say, was me... a lot of this paper was me trying to envision harms and then probe it with engineers or technical folks and also non-technical as well, to ask them, does this make sense? Could this be true? And for example, from freedom of peaceful assembly and protest, one thing to consider is that protests are, and contrarian views are protest definition anti-majority.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"4a2b1f2b5b39\",\"_type\":\"block\",\"children\":[{\"_key\":\"9e5c1e5376950\",\"_type\":\"span\",\"marks\":[],\"text\":\"You have a minority express themselves. You go against powerful interests like governments or companies, or even just the status quo. And what does that mean? This data is not well-represented in the training datasets. Because machine learning, I often say is just steroids on stats. So if a view is predominant in the dataset, even if it's completely wrong, that is the output, right? Machine learning never gives a real decision. It gives a prediction about what is statistically possible. So when you have protestors or journalists, investigative journalists, for example, or anybody bringing up new stuff, that will not actually show up in the dataset and therefore will not be moderated well. And let's give platforms and those deploying content moderation system the highest benefit of the doubt. They probably wants to moderate it well, just the system will not function unless specifically fine-tuned because protests fall outside the curve of statistical data.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"c9faddb86b37\",\"_type\":\"block\",\"children\":[{\"_key\":\"99177dd8d5f10\",\"_type\":\"span\",\"marks\":[],\"text\":\"And that's also the case for conflicts or exceptional circumstances, crises. These events are, by definition, exceptional, and therefore fall outside the statistical curve and are not moderated well. And that's another key finding. For freedom of association, one thing that could be interesting here is that some organizations can be mislabeled. Actually, you know what, Justin, forget that one. It's too long. I'll skip it.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"513765997c8b\",\"_type\":\"block\",\"children\":[{\"_key\":\"cc8b0bb581030\",\"_type\":\"span\",\"marks\":[],\"text\":\"Two last pieces I'd like to highlight that we found were really interesting. One was on participation. So on one side, LLMs actually have the potential to support more participatory design by enabling customizable moderation. So like I said before, the users have personalized content moderation, or perhaps in the future use AI agents to moderate their own content as they want. On the flip side, in practice, affected communities are largely excluded from shaping content moderation systems. That's not new. That has also happened in machine learning.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"c5a95b72d24d\",\"_type\":\"block\",\"children\":[{\"_key\":\"323efe342bfe0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Generally, it's mostly white tech bros in San Francisco or Silicon Valley, so especially marginalized groups and those in the global majority are excluded from that. The addition with LLMs is that they are typically \\\"improved\\\" through a method called reinforcement learning from human feedback, or red-teaming. So you have folks thinking about, in the case of red-teaming, what are potential bad, worst-case scenarios and testing it from an adversarial perspective.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"ebe1e192bfcf\",\"_type\":\"block\",\"children\":[{\"_key\":\"bf68a207f9430\",\"_type\":\"span\",\"marks\":[],\"text\":\"Same for reinforcement learning. They go through these models and they \\\"fix\\\" them, they reteach them how to learn. The problem is that people who do that are usually Stanford graduates, those in Silicon Valley or other elitist institutions, and it's very rare. I personally have never heard of folks from marginalized groups being invited to participate in these kinds of activities. I myself, for example, have been invited, but you have to be in a niche kind of AI group to do that. And everybody who I've spoken to who has done that, I personally have not. I'll say that it's a very homogeneous group. And so basically what that means is then yes, the LLMs will be improved... or, I'm sorry, let me rephrase. There are efforts to reduce inaccuracy and improve the performance of the models of the LLMs. However, the people who do that are typically very homogeneous. So it's just like snowball effects.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"ad06950a5191\",\"_type\":\"block\",\"children\":[{\"_key\":\"17285d63f1470\",\"_type\":\"span\",\"marks\":[],\"text\":\"And the last piece on remedy is that while there are potential promises to access remedy better, like I said before, most everyone use a notification, helping them identify remedy, appeals mechanisms, speeding up the appeals, there's also fundamentally a lack of explainability and transparency, and that can create barriers to remedy. Another issue, like I said before, is that there are these two layers, the layer of foundation models. So the ChatGPT, the Claude, the Gemini, and then the social media platform that deploys it. And it's hard to know where to appeal, how to appeal. The foundation models themselves don't really know how a decision was made. Social media platforms know that even less. Do you appeal to the platform? Does the platform then appeal again to the third party LLM? Where does the user fall into this? So just accountability becomes fragmented, and there's a lot of confusion and lack of clarity around how to go around that.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"8e216f117f93\",\"_type\":\"block\",\"children\":[{\"_key\":\"f29a8959229f0\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Justin Hendrix:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"0a166279c89f\",\"_type\":\"block\",\"children\":[{\"_key\":\"bac0b110902e0\",\"_type\":\"span\",\"marks\":[],\"text\":\"So I want to come to some of your recommendations, because you have both recommendations to LLM developers and deployers as well as to, of course, those who are potentially applying these things inside of social media companies. But your section on recommendations to policymakers is perhaps mercifully brief. You've only got a handful of recommendations there. It's clear, I think just from the onus in the report on where the recommendations are, that you see it largely as something that private sector needs to sort out, how are they going to deploy these technologies or not.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"195669b501fc\",\"_type\":\"block\",\"children\":[{\"_key\":\"4c10872cfd3d0\",\"_type\":\"span\",\"marks\":[],\"text\":\"But when it comes to policymakers, what are you telling them? I see you're interested in making sure they're refraining, on some level, from mandating the use of these things. I suppose it's a possibility that somebody might come along and say, \\\"Oh, in fact, we demand that you use them.\\\" I was trying to think of a context for that, but then I found myself thinking about some of the laws we've seen put forward in the United States even, where there have been segments of those laws, I think in some of the must-carry laws, where there have been these ideas around transparency of moderation decisions. I feel like I've read amicus briefs where some of the people opposed to those laws would make arguments like, \\\"Well, it's just simply not possible to give explainable rationale to every single user for every single content-moderation decision that's made.\\\"\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"1d78e894b7dc\",\"_type\":\"block\",\"children\":[{\"_key\":\"b1f4fb59a4ce0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Well, presumably, LLMs would make that quite possible, and you could imagine a government coming along and saying, \\\"Somehow, in the interest of free expression, we would like to mandate, use artificial intelligence to explain any content moderation decision that you might take.\\\" You say that's a bad idea along with other things, but what would you tell policymakers to be paying attention to here?\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"9910fd54069e\",\"_type\":\"block\",\"children\":[{\"_key\":\"49b3ff9cfd7e0\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Marlena Wisniak:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"bc6923abf3cd\",\"_type\":\"block\",\"children\":[{\"_key\":\"3321c2b0951d0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Yeah, I mean, that's a very astute... you observe that well. Most of the recommendations are to LLMs developers and employers. The reason for this is not that we only see the owners on the private sector, not on the public sector, is that for practical reasons, this part of the research was mostly on assessing the human rights impact. And the recommendations piece was really the last part, and we didn't have that much capacity to go deep. So the second iteration of this report, hopefully we will continue. It will be really to zoom into the recommendations.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"d0012f6fc051\",\"_type\":\"block\",\"children\":[{\"_key\":\"254ff94beaf00\",\"_type\":\"span\",\"marks\":[],\"text\":\"And I will say then, that on the policy making side, so a lot of our work at ECNL is on policy and legal advocacy. We've been working behind the scenes on the AI Act for the past five years, and right now, there's conversations around the GPAI code in the EU on general-purpose AI. And the reason why I didn't go deep here is that one, we don't want a specific AI content moderation or LLM moderation law. We have the DSA in Europe. The U.S. I'll just set aside, because right now there's a lot going on. So we're not calling for a specific LLM content moderation law.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"7637e229d998\",\"_type\":\"block\",\"children\":[{\"_key\":\"3aaccfd2a7be0\",\"_type\":\"span\",\"marks\":[],\"text\":\"And the EU already has the DSA and the AI. And I'd say a lot of the foundational aspects of LLMs to policymakers would be kind of basic AI around, like, data protection, human rights impact assessment, stakeholder engagement. So I added these big categories. I didn't go really deep into them. It's mostly how can we not fuck it up, to be honest. And I think content moderation, as you know, is a very fraught topic where even folks who are well-intentioned just don't understand it enough. So sometimes you will have these claims that would let... platform should remove problematic content within an hour. And it's like, okay, cool. In theory, that sounds great. What does that mean in practice? It means a lot of false positives. It means that disproportionately marginalized groups will be impacted, including sex workers, racialized groups, queer folks. And you'll have to use automated content moderation, which has all the false positives and false negatives that haven't reported on.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"b9926b68867f\",\"_type\":\"block\",\"children\":[{\"_key\":\"12dc92f3a53a0\",\"_type\":\"span\",\"marks\":[],\"text\":\"So the key recommendation we made to policymakers here are very foundational, I'd say. One, do not mandate LLM moderation exactly for the reason that you expressed before. Two, maintain human oversight. So if LLMs are used to moderate content, and especially to remove content, there still should be a legal requirements for platforms to integrate human-in-the-loop systems, meaning that humans will review whatever decision LLM does. Three, kind of broad transparency and accountability metrics and requirements that's very DSA-esque... sorry, I should have said Digital Services Act. And a lot of that is really about transparency, like mandating disclosure of how LLM systems function and how they're used. The reality, Justin, we don't know. I had several off-the-record calls with platforms, off-the-record means I couldn't publish the names. They also gave me very vague information. There's some information that was published by them, and you'll see it in the report.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"8f5e348e8cb6\",\"_type\":\"block\",\"children\":[{\"_key\":\"bb0c9278058d0\",\"_type\":\"span\",\"marks\":[],\"text\":\"ChatGPT said they use LLMs for content moderation. Meta says they're beginning to play around with it, gemini or Google. But overall, we don't know. I definitely don't know when it's used when I use social media. We don't know accuracy rates, we don't know how they're used in appeals, or how they're enforced. So really requiring platforms to notify users about LLM moderation actions. And again, that's nothing new, I would say. That's just using prior Santa Clara principles on transparency and accountability, or DSA or kind of like mainstream civil-society asks and implementing them for LLMs.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"231b4a2b0e1b\",\"_type\":\"block\",\"children\":[{\"_key\":\"9da4a06e0bec0\",\"_type\":\"span\",\"marks\":[],\"text\":\"The two last ones that we highlighted, one is mandating human rights impact assessments. So hey, platforms, good news. I did one here, so you can use... My goal is that this will be a starting point for platforms to have basically an HRA handed to them on a silver platter, and then obviously use this as a starting point to look at how they implement LLMs on their platform. It'll be specific to each platform.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"3500770406d3\",\"_type\":\"block\",\"children\":[{\"_key\":\"0d44b5f4cccd0\",\"_type\":\"span\",\"marks\":[],\"text\":\"But on that note, one thing that policymakers could do is make HRAs mandatory for LLM developers and platforms that deploy them, both before deployment and throughout the LLM lifecycle. So for example, the pilot with Discord was at the ideation phase, so design, product design. Before they even developed the product, they consulted with a lot of folks to see how an LLM or machine-learning-driven system could be helpful. Is it nudges? Is it for removing content? Do we not want that? Why? And then continue that throughout all the way through development and deployment and make it accessible for external stakeholders.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"53fdb1e962ab\",\"_type\":\"block\",\"children\":[{\"_key\":\"30fecaf73a040\",\"_type\":\"span\",\"marks\":[],\"text\":\"There's so much expertise in the room, and I often say to platforms that, you know, trust and safety teams, policy teams, human rights teams tend to be underfunded. Just go to civil society that has such extensive knowledge, or journalists like yourself and academics and just drink their wisdom, because there's a lot of stuff out there.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"0b8bdc7625ee\",\"_type\":\"block\",\"children\":[{\"_key\":\"081e14ead3380\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Justin Hendrix:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"de0373ef0e68\",\"_type\":\"block\",\"children\":[{\"_key\":\"7bf416f869350\",\"_type\":\"span\",\"marks\":[],\"text\":\"This report seems like a good jumping-off point for a lot of folks who might be interested in investigating or collecting artifacts of how the platforms are deploying these things or generally kind of trying to pay attention to these issues and trying to discern whether the introduction of LLMs in this context is on balance a good thing, a bad thing, or perhaps somehow neutral or indiscernible. With regard to the overall information integrity environment, what would you encourage people to do next? What would you encourage them to go and look at? What threads would you like to see the field pulled from here?\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"70f36a6eecfb\",\"_type\":\"block\",\"children\":[{\"_key\":\"157052b6ad9d0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Marlena Wisniak:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"6f8139ae61ff\",\"_type\":\"block\",\"children\":[{\"_key\":\"5ae7f65f5fe20\",\"_type\":\"span\",\"marks\":[],\"text\":\"Yeah. One core piece that we didn't talk about much is the multilingual piece of it, so how this can work in different languages. And I'll just give a shout out here to really cool efforts in the global majority to develop community-driven local smaller LLMs or data sets, like the Masakhane in Africa. There's really cool community-driven initiatives that kind of go beyond the Silicon Valley profit-first massive monopolization and dynamics. So that is one thing, and I really encourage not only researchers, but also platforms to talk to them. A lot of the platforms that I spoke with, they didn't even know these existed, and if I say that this report is an HRA handed on a silver platter, they have really cool data sets that would be really helpful, especially to smaller platforms, and they could just plug these into their own model.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"85d8f8922bc5\",\"_type\":\"block\",\"children\":[{\"_key\":\"58f84917f1930\",\"_type\":\"span\",\"marks\":[],\"text\":\"So that's one thing that I hope research and industry will move towards is more languages, more dialects, understanding that it's not only a difference between English and other language. It really is a colonial imperialist dynamic where English or French or German or Spanish will be much better moderated, and languages that are close to these ones are better, and then obscure, poorly researched languages work very, very poorly. And you can read in the report the reasons are both because the data sets do not exist or they're just bad quality, because there's not enough investment. So I really would encourage platforms to invest more resources into that, more participation, and proactively include stakeholders.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"7f481c088fbe\",\"_type\":\"block\",\"children\":[{\"_key\":\"bf020ae5a14d0\",\"_type\":\"span\",\"marks\":[],\"text\":\"The other area I would love to see is just open conversations between platform and civil society. And like I said, this is a nerdy topic, like LLMs and content moderation. It doesn't roll off the tongue. So if folks with expertise in content moderation would like, hopefully this report can give them more context. And then I would love to see more evidence, new ideas. Like I said, some of these things were my own kind of \\\"How could this work?\\\" But through many conversation with folks, and I really would want to see more thinking, more assessment of impacts and more evidence as well.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"130074a78581\",\"_type\":\"block\",\"children\":[{\"_key\":\"956162e285b90\",\"_type\":\"span\",\"marks\":[],\"text\":\"Like this paper, it's long, 70 pages. I didn't mean to make it this long, but there's a lot of stuff. And I'd say the last part is computer science papers move so fast, incredibly fast. It's hard to keep up, and they're very theoretical. So to the extent that there can be more collaboration between technical folks who come from the machine learning, AI, or computer science field, and policy and human rights, I think we'll actually be able to build much better products and push policymakers to regulate this stuff better.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"bb92c8f9a3b2\",\"_type\":\"block\",\"children\":[{\"_key\":\"d484897161fe0\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Justin Hendrix:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"30aec9af1230\",\"_type\":\"block\",\"children\":[{\"_key\":\"a6f1ac1fe5850\",\"_type\":\"span\",\"marks\":[],\"text\":\"I appreciate you taking the time to speak to me about this, and I would encourage my readers to go and check out this full report, which is available on the ecnl.org website. I will include a link to it in the show notes. Thank you very much.\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"c54a3e5e7b6e\",\"_type\":\"block\",\"children\":[{\"_key\":\"50cbead85f0f0\",\"_type\":\"span\",\"marks\":[\"strong\"],\"text\":\"Marlena Wisniak:\"}],\"markDefs\":[],\"style\":\"normal\"},{\"_key\":\"3ca5eb85797f\",\"_type\":\"block\",\"children\":[{\"_key\":\"4df3600915b50\",\"_type\":\"span\",\"marks\":[],\"text\":\"Thanks, Justin.\"}],\"markDefs\":[],\"style\":\"normal\"}],\"date\":\"2025-07-06T14:08:55.749Z\",\"disableNewsletterSignup\":null,\"featuredImage\":{\"_type\":\"image\",\"altText\":\"Game of Pixels x Toy Models by Elise Racine. Better Images of AI/CC by 4.0\",\"asset\":{\"_ref\":\"image-ca8393b380961a234123ad924f89ed855aed0c1d-1200x675-png\",\"_type\":\"reference\"},\"caption\":[{\"_key\":\"2f4e5858583f\",\"_type\":\"block\",\"children\":[{\"_key\":\"f698bb2a2fdb0\",\"_type\":\"span\",\"marks\":[],\"text\":\"Game of Pixels x Toy Models by Elise Racine. \"},{\"_key\":\"0347110ba1d4\",\"_type\":\"span\",\"marks\":[\"e49774355a89\"],\"text\":\"Better Images of AI\"},{\"_key\":\"0756ea3a81fd\",\"_type\":\"span\",\"marks\":[],\"text\":\"/CC by 4.0\"}],\"markDefs\":[{\"_key\":\"e49774355a89\",\"_type\":\"link\",\"href\":\"https://betterimagesofai.org/images?artist=EliseRacine\\u0026title=GameofPixelsxToyModels\"}],\"style\":\"normal\"}]},\"heroContent\":null,\"layout\":null,\"relatedArticles\":[{\"authors\":[{\"firstName\":\"Prithvi\",\"lastName\":\"Iyer\"},{\"firstName\":\"Justin\",\"lastName\":\"Hendrix\"}],\"badge\":null,\"date\":\"2024-09-26T15:21:01.835Z\",\"slug\":{\"_type\":\"slug\",\"current\":\"syllabus-large-language-models-content-moderation-and-political-communication\"},\"title\":\"Syllabus: Large Language Models, Content Moderation, and Political Communication\"},{\"authors\":[{\"firstName\":\"Alyssa\",\"lastName\":\"Boicel\"}],\"badge\":null,\"date\":\"2024-04-03T13:00:03.761Z\",\"slug\":{\"_type\":\"slug\",\"current\":\"using-llms-to-moderate-content-are-they-ready-for-commercial-use\"},\"title\":\"Using LLMs to Moderate Content: Are They Ready for Commercial Use?\"},{\"authors\":[{\"firstName\":\"Paul\",\"lastName\":\"Barrett\"},{\"firstName\":\"Justin\",\"lastName\":\"Hendrix\"}],\"badge\":null,\"date\":\"2024-04-03T13:00:14.584Z\",\"slug\":{\"_type\":\"slug\",\"current\":\"is-generative-ai-the-answer-for-the-failures-of-content-moderation\"},\"title\":\"Is Generative AI the Answer for the Failures of Content Moderation?\"},{\"authors\":[{\"firstName\":\"Dave\",\"lastName\":\"Willner\"},{\"firstName\":\"Samidh \",\"lastName\":\"Chakrabarti\"}],\"badge\":null,\"date\":\"2024-01-29T14:10:34.613Z\",\"slug\":{\"_type\":\"slug\",\"current\":\"using-llms-for-policy-driven-content-classification\"},\"title\":\"Using LLMs for Policy-Driven Content Classification\"},{\"authors\":[{\"firstName\":\"Dia\",\"lastName\":\"Kayyali\"}],\"badge\":null,\"date\":\"2025-02-12T23:39:00.000Z\",\"slug\":{\"_type\":\"slug\",\"current\":\"an-advocates-guide-to-automated-content-moderation\"},\"title\":\"An Advocate’s Guide to Automated Content Moderation\"}],\"relatedCommentary\":null,\"relatedTopics\":[{\"displayName\":\"Content Moderation\",\"name\":\"Content Moderation\",\"slug\":{\"current\":\"content-moderation\"},\"stackbit_model_type\":\"page\",\"type\":null},{\"displayName\":\"Privacy\",\"name\":\"Privacy\",\"slug\":{\"current\":\"privacy\"},\"stackbit_model_type\":\"page\",\"type\":null},{\"displayName\":\"Human Rights\",\"name\":\"Human Rights\",\"slug\":{\"current\":\"human-rights\"},\"stackbit_model_type\":\"page\",\"type\":null},{\"displayName\":\"Technology\",\"name\":\"Technology\",\"slug\":{\"_type\":\"slug\",\"current\":\"technology\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"Policy\",\"name\":\"Policy\",\"slug\":{\"current\":\"policy\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"Authoritarianism\",\"name\":\"Authoritarianism\",\"slug\":{\"current\":\"authoritarianism\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"authoritarianism\",\"name\":\"authoritarianism\",\"slug\":{\"current\":\"authoritarianism_tag\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"AI\",\"name\":\"AI\",\"slug\":{\"current\":\"ai\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"content moderation\",\"name\":\"content moderation\",\"slug\":{\"current\":\"content-moderation_tag\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"generative ai\",\"name\":\"generative ai\",\"slug\":{\"current\":\"generative-ai\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"privacy\",\"name\":\"privacy\",\"slug\":{\"current\":\"privacy_tag\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"ECNL\",\"name\":\"ECNL\",\"slug\":{\"_type\":\"slug\",\"current\":\"ecnl\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"Marlena Wisniak\",\"name\":\"Marlena Wisniak\",\"slug\":{\"_type\":\"slug\",\"current\":\"marlena-wisniak\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"large language models\",\"name\":\"large language models\",\"slug\":{\"current\":\"large-language-models\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"Content Moderation with Large Language Models\",\"name\":\"Content Moderation with Large Language Models\",\"slug\":{\"_type\":\"slug\",\"current\":\"content-moderation-with-large-language-models\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"Content Moderation with LLMs\",\"name\":\"Content Moderation with LLMs\",\"slug\":{\"_type\":\"slug\",\"current\":\"content-moderation-with-llms\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"Claude\",\"name\":\"Claude\",\"slug\":{\"_type\":\"slug\",\"current\":\"claude\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"machine learning\",\"name\":\"machine learning\",\"slug\":{\"current\":\"machine-learning\"},\"stackbit_model_type\":\"data\",\"type\":null},{\"displayName\":\"European Center for Not-for-Profit Law\",\"name\":\"European Center for Not-for-Profit Law\",\"slug\":{\"_type\":\"slug\",\"current\":\"european-center-for-not-for-profit-law\"},\"stackbit_model_type\":\"data\",\"type\":null}],\"sections\":null,\"seo\":{\"_type\":\"stackbit_page_meta\",\"description\":\"A conversation with ECNL's Marlena Wisniak on her report, Algorithmic Gatekeepers: The Human Rights Impacts of LLM Content Moderation.\",\"ogImage\":{\"_type\":\"image\",\"asset\":{\"_ref\":\"image-ca8393b380961a234123ad924f89ed855aed0c1d-1200x675-png\",\"_type\":\"reference\"},\"ogImageAlt\":\"Game of Pixels x Toy Models by Elise Racine. Better Images of AI/CC by 4.0\"},\"title\":\"Considering the Human Rights Impacts of LLM Content Moderation\"},\"sidebar_content\":null,\"slug\":{\"_type\":\"slug\",\"current\":\"considering-the-human-rights-impacts-of-llm-content-moderation\"},\"stackbit_url_path\":null,\"title\":\"Considering the Human Rights Impacts of LLM Content Moderation\",\"toc\":null,\"tocTitle\":null,\"trackerText\":null},\"articles\":null,\"_type\":\"post\",\"authors\":null,\"path\":\"considering-the-human-rights-impacts-of-llm-content-moderation\",\"data\":{\"config\":{\"_createdAt\":\"2023-08-31T13:31:01Z\",\"_id\":\"config\",\"_rev\":\"CIQMb8zr2hKNcFuuO1UfSq\",\"_type\":\"config\",\"_updatedAt\":\"2025-07-18T18:21:12Z\",\"favicon\":{\"_type\":\"image\",\"asset\":{\"_ref\":\"image-9a2224d300c1699fc1b87235aac36318e2c76cec-867x867-png\",\"_type\":\"reference\"}},\"footer\":{\"_type\":\"footer\",\"content\":\"A nonprofit media and community venture intended to provoke new ideas, debate and discussion at the intersection of technology and democracy.\",\"copyright\":\"Tech Policy Press © 2025 — a 501(c)(3) organization\",\"links\":[{\"_key\":\"383ecfdbb924\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"About\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/about-us\"},{\"_key\":\"8d3176182c87\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Donate\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/donate\"},{\"_key\":\"8a19f618e7ef\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Privacy Policy\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/privacy\"},{\"_key\":\"a97a9b1c3c23\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Fellows\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/fellowships\"},{\"_key\":\"2fdb180c1d51\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Contributors\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/contributors\"},{\"_key\":\"80b2e742bc12\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Submissions\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/contributor-guidelines\"},{\"_key\":\"5c1d6d4fbce4\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Articles\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/search\"},{\"_key\":\"03ab4085b977\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Podcast\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/podcast\"},{\"_key\":\"0d769a4d55d7\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Research Library\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/library\"}],\"stackbit_model_type\":\"object\",\"type\":\"footer\"},\"header\":{\"_type\":\"header\",\"projectsLinks\":[{\"_key\":\"919f7bde0223\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Research Library\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/library\"},{\"_key\":\"8904af9ec118\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Race, Ethnicity, Technology \\u0026 Elections\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/race-ethnicity-technology-elections\"},{\"_key\":\"aa5d73b183c7\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Fellowship Program\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/fellowships\"}],\"stackbit_model_type\":\"object\",\"topicsLinks\":[{\"_key\":\"f76613b555e6\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Accessibility\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/access\"},{\"_key\":\"42364a6e2d91\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Antitrust\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/antitrust\"},{\"_key\":\"15c088e49ba3\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Artificial Intelligence\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/artificial-intelligence\"},{\"_key\":\"12763b68685d\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Content Moderation\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/content-moderation\"},{\"_key\":\"3c972905edac\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Competitition\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/competition\"},{\"_key\":\"69b291dd4be4\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Cybersecurity\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/cybersecurity\"},{\"_key\":\"b7c10450718a\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Data Centers\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/data-centers/\"},{\"_key\":\"9397f4e99522\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Democracy\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/democracy\"},{\"_key\":\"06a9c2d7de8e\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Discrimination\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/discrimination\"},{\"_key\":\"dcb58f29e5b7\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Disinformation\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/disinformation\"},{\"_key\":\"541f65b05283\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Extremism\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/extremism\"},{\"_key\":\"3844982cc5a9\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Free Speech\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/free-speech\"},{\"_key\":\"831a6aecea0a\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Hate \\u0026 Harassment\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/hate-and-harassment\"},{\"_key\":\"d120d1dc6629\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Human Rights\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/human-rights\"},{\"_key\":\"d098b6cc2046\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Liability\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/liability\"},{\"_key\":\"480bfb08b718\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"News \\u0026 Journalism\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/news-and-journalism\"},{\"_key\":\"621f0e3fa109\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Online Safety\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/online-safety\"},{\"_key\":\"1e95d7b8594f\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Privacy\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/privacy\"},{\"_key\":\"3376210663fa\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Public Health\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/public-health\"},{\"_key\":\"ccbda3672586\",\"_type\":\"action\",\"icon_class\":\"dev\",\"label\":\"Transparency\",\"stackbit_model_type\":\"object\",\"style\":\"link\",\"type\":\"action\",\"url\":\"/category/transparency\"}],\"type\":\"header\"},\"path_prefix\":\"http://127.0.0.1:3000\",\"stackbit_file_path\":\"content/data/config.json\",\"stackbit_model_type\":\"data\",\"title\":\"TechPolicy.Press\"},\"topics\":[{\"_id\":\"category-internet-access\",\"displayName\":\"Accessibility\",\"slug\":{\"current\":\"access\"}},{\"_id\":\"category-antitrust\",\"displayName\":\"Antitrust\",\"slug\":{\"current\":\"antitrust\"}},{\"_id\":\"category-artificial-intelligence\",\"displayName\":\"Artificial Intelligence\",\"slug\":{\"current\":\"artificial-intelligence\"}},{\"_id\":\"tag-competition\",\"displayName\":\"Competition\",\"slug\":{\"current\":\"competition\"}},{\"_id\":\"category-content-moderation\",\"displayName\":\"Content Moderation\",\"slug\":{\"current\":\"content-moderation\"}},{\"_id\":\"category-cybersecurity\",\"displayName\":\"Cybersecurity\",\"slug\":{\"current\":\"cybersecurity\"}},{\"_id\":\"category-democracy\",\"displayName\":\"Democracy\",\"slug\":{\"current\":\"democracy\"}},{\"_id\":\"tag-discrimination\",\"displayName\":\"Discrimination\",\"slug\":{\"current\":\"discrimination\"}},{\"_id\":\"category-disinformation\",\"displayName\":\"Disinformation\",\"slug\":{\"current\":\"disinformation\"}},{\"_id\":\"category-extremism\",\"displayName\":\"Extremism\",\"slug\":{\"current\":\"extremism\"}},{\"_id\":\"category-speech\",\"displayName\":\"Free speech\",\"slug\":{\"current\":\"free-speech\"}},{\"_id\":\"tag-hate-and-harassment\",\"displayName\":\"Hate and harassment\",\"slug\":{\"current\":\"hate-and-harassment\"}},{\"_id\":\"tag-human-rights\",\"displayName\":\"Human Rights\",\"slug\":{\"current\":\"human-rights\"}},{\"_id\":\"category-section-230\",\"displayName\":\"Liability\",\"slug\":{\"current\":\"liability\"}},{\"_id\":\"category-journalism-and-news-media\",\"displayName\":\"News and Journalism\",\"slug\":{\"current\":\"news-and-journalism\"}},{\"_id\":\"tag-online-safety\",\"displayName\":\"Online safety\",\"slug\":{\"current\":\"online-safety\"}},{\"_id\":\"category-privacy\",\"displayName\":\"Privacy\",\"slug\":{\"current\":\"privacy\"}},{\"_id\":\"tag-public-health\",\"displayName\":\"Public health\",\"slug\":{\"current\":\"public-health\"}},{\"_id\":\"tag-transparency\",\"displayName\":\"Transparency\",\"slug\":{\"current\":\"transparency\"}}]}},\"__N_SSG\":true},\"page\":\"/[...slug]\",\"query\":{\"slug\":[\"considering-the-human-rights-impacts-of-llm-content-moderation\"]},\"buildId\":\"53BaSRGe9oJ7dm7CovmVM\",\"isFallback\":false,\"isExperimentalCompile\":false,\"gsp\":true,\"locale\":\"en\",\"locales\":[\"en\"],\"defaultLocale\":\"en\",\"scriptLoader\":[]}</script><script async=\"\" defer=\"\" src=\"https://scripts.simpleanalyticscdn.com/latest.js\"></script><noscript><img src=\"https://queue.simpleanalyticscdn.com/noscript.gif\" alt=\"\" referrerPolicy=\"no-referrer-when-downgrade\"/></noscript></body></html>","oembed":false,"readabilityObject":{"title":"Considering the Human Rights Impacts of LLM Content Moderation","content":"<div id=\"readability-page-1\" class=\"page\"><div><p><a href=\"https://techpolicypress.captivate.fm/listen\" target=\"_blank\" rel=\"noopener\"><em>Audio of this conversation is available via your favorite podcast service.</em></a></p><p>At Tech Policy Press, we’ve been <a href=\"https://www.techpolicy.press/syllabus-large-language-models-content-moderation-and-political-communication/\" target=\"_blank\" rel=\"noopener\">tracking</a> the emerging application of generative AI systems in content moderation. Recently, the <a href=\"https://ecnl.org/\" target=\"_blank\" rel=\"noopener\">European Center for Not-for-Profit Law</a> (ECNL) released a comprehensive report titled <a href=\"https://ecnl.org/sites/default/files/2025-04/ECNL_LLM_CM_Excecutive%20Summary_2025.pdf\" target=\"_blank\" rel=\"noopener\"><em>Algorithmic Gatekeepers: The Human Rights Impacts of LLM Content Moderation</em></a>, which looks at the opportunities and challenges of using generative AI in content moderation systems at scale.&nbsp;I spoke to its primary author, ECNL senior legal manager <strong>Marlena Wisniak</strong>.</p><p><em>What follows is a lightly edited transcript of the discussion.</em></p><p><strong>Justin Hendrix:</strong></p><p>Marlena, can you tell us a little bit about what the ECNL does?</p><p><strong>Marlena Wisniak:</strong></p><p>The Center, or ECNL as we call it, is a human rights and civil liberties organization for over 20 years. We've been mostly focusing on civic space, making sure that civil society organizations, but also grassroots orgs, activists, have a safe space to organize and do their work. We're mostly lawyers, but I have to say we're the fun lawyers. So we also do advocacy, research, and basically anything that we can do to protect and promote in enabling civic space. My team was founded in 2020, I believe, and has from the beginning focused mostly on AI, but it's more broadly the digital team at ECNL.</p><p>So we look at how technologies, specifically emerging technologies, impact civic space and human rights. So our core human rights or civil liberties that we look at is typically privacy, freedom of expression, freedom of assembly, so rights to protest, for example, association, right to organize, and non-discrimination. And our key areas of focus substantively are typically surveillance, AI-driven surveillance and biometric surveillance, and broader and social media platforms that plays such a big role in civic space today. And that's what I'll be talking about today. But that's, in a nutshell, my team and how it fits within the broader org.</p><p><strong>Justin Hendrix:</strong></p><p>Well, I'm excited to talk to you about this report that you have authored with help from a variety of different corners, but it's called Algorithmic Gatekeepers, the Human Rights Impacts of LLM Content Moderation. So this is a topic that we've been trying to follow at Tech Policy Press fairly closely, because I feel like it is the sort of intersection of a lot of things that we care about around social media, content moderation, online safety, free expression, and then, of course, artificial intelligence. And LLMs, generative AI generally, the intersection of these two things, I think, is probably one of the most interesting and possibly under-covered or under-explored, at least to date, issues at the intersection of tech and democracy.</p><p>So I want to just start by asking you how you did this report. It's quite a significant document. A lot of research obviously went into this, including gathering some new information, not just combining citations, as many people do when they produce a PDF white paper. What got you started on this and how did you go about it?</p><p><strong>Marlena Wisniak:</strong></p><p>Yes, you're right. It was really a heavy lift. It was a research project that went on for about a year, and we collaborated with great folks including yourself. I remember, I think last year we had a call to hear your thoughts, and shout out, before I go deep into context, to Isabelle Anzabi, who was our fellow last summer, and really helped me just go through a lot of papers, mostly in computer science, and the Omidyar Network, who provided generous support for this project.</p><p>And so how it started, I accidentally fell into AI in 2016, but come from content moderation in the 2010s, '11, '12, so I'd say early days of content moderation. So automated content moderation has always been a big focus of mine. I was also at Twitter at some point, overseeing their legal department, sorry, content-governance and illegal department. And living in San Francisco right now, really, the big talk on the street is always LLMs, and GenAI more broadly.</p><p>And so I started hearing various use cases of LLMs for content moderation. Most of the talk, I will say, and focus of the research and civil society community is how to moderate LLM-generated content, so how to moderate ChatGPT, for example, or Claude. That is, of course, super important. And I wanted to look at it from another angle, which is how are LLMs used for content moderation? And this I will say, Justin, has been a little bit of a chicken-and-egg conversation, or reflection, because the way how LLMs can moderate content also impacts how LLM-driven content will be moderated, and how they're moderated impacts how they moderate content. So it's hard to separate those, one from another, but I did choose to have that focus. And it's interesting because it's very much a nerdy topic and yet has real-world implications, and it's very hyped up.</p><p>So one of the things that I always love to do is go beyond the hype. I'm not a hype person. In fact, now I hate AI. It was fun working on AI in the 2016, '17, '18 years when nobody really cared about AI. Since ChatGPT was released, the only thing people ever want to talk about is AI. So now I want to scream. But all this to say that I did want to impact some of the real implications and post what our promises and what our, really, I'd say, not even realistic promises, but the types of impacts we want to see, because I do believe that it can helpful, and also, as this is an emerging space, prevent any possible harm. And I think folks listening to this podcast probably know that human motivation is extremely complicated. It's horrible for workers. It doesn't always, and always is an understatement, produce good outcomes.</p><p>And so machine learning came as a solution to that. It was expanded during COVID, and sort of came as this silver-bullet solution. There has been increasing research showing the limitations of that. And so now, LLM is the new white horse, is that an expression? Is the savior, and there's a lot of hype. So that's where I came from. It's like, okay, let's go deeper into this. Let's review a lot of computer science papers where some of this more rigorous qualitative and quantitative work has done, translate it to policy folks, and bring a human-rights approach because that's my background. I'm an international human rights lawyer.</p><p><strong>Justin Hendrix:</strong></p><p>So readers of Tech Policy Press have at least had multiple pieces that we've posted on the site about this issue, and often there's been this sort of question about what is the promise potentially to offset the dangers to the labor force that currently engage in content moderation on behalf of platforms, but then also, of course, there are various perils that we can imagine as well. We're going to get into those a little bit.</p><p>But I think one thing that distinguishes your report that I wanted to just start with perhaps is that you include a technical primer. You've got a kind of set of definitions and, I think usefully, some distinctions between what's going on with LLMs for content moderation versus more standard machine-learning classifiers and recommendation mechanisms and other types of algorithmic models that have been used in content moderation for quite some time now. What do you think the listener needs to know about the technical aspect of this phenomenon, of the use of LLMs for content moderation at scale from our vantage right now in June of 2025?</p><p><strong>Marlena Wisniak:</strong></p><p>Yeah, I mean, I encourage folks to look at the technical primer, that is, the audience for that are mostly folks who aren't very familiar with either industry jargon or technical terms. Leveling up, I think one thing that is really critical to consider when thinking about LLM-driven content moderation is that you have typically two layers. So one is the foundation model layer, or the LLMs. LLM are, I should have started, large-language models. And that's pretty explicit. It's large-language model, so people see it as God, or the sci-fi technology. It's really not. It's really big data sets.</p><p>And I think we often forget that. So if we think about traditional machine learning, the difference is that this is a larger set. And so there's this implication, obviously, for privacy and other rights that we can explore later down the line. But one, considered, they're very, very, very big, enormous data sets that require a lot of computing power. And so really, there's only a handful of companies right now building these models.</p><p>But the ones that we looked at in more depth are Llama by Facebook, sorry, Meta, ChatGPT, OpenAI, Claude, Anthropic, Gemini, Google. And then right as I was finishing out the report, DeepSeek came out. So there's emerging models as well, but it's still a very small number of foundation models, given how much data and compute they use. And I often say they're really not that technically complicated. They're just bigger. And so from a concentration of power, that has a lot of importance.</p><p>And so then the platforms that I mentioned, they both develop and often use... they develop the LLMs and they use them for content moderation. But what happens for all the other ones like Discord or Reddit, or Slack? Although Slack might be bought by one of them. But anyways, there are so many other platforms. Typically, they do not build their own LLMs. They will either have a license with one of the foundation-model companies and then fine-tune them or they will use one of the open source, like DeepSeek or Llama or something they find on Hugging Face, and then fine-tune them for their purposes.</p><p>But what that means, so that's one thing from a technical thing to understand how LLMs work and how they're used in practice. And then another thing I will flag is that LLMs are a subset of generative AI. That's a term we most commonly know. Generative AI today has, to some extent, become equivalent of ChatGPT. It's much broader than ChatGPT, but LLMs are one subset of that, or foundation models as well.</p><p>And then maybe one last thing I'll flag is multilingual language models are those that are trained on text data from dozens to sometimes hundreds of languages simultaneously. And so the idea is that they will be more capable of processing and generating inputs and outputs in multiple languages. And we can about whether that works or not. There's fantastic research that has been done that inspired me a lot for my work.</p><p><strong>Justin Hendrix:</strong></p><p>Yeah, I'll just say that the primer, I think what's great about, again, this report is that it does detail at least some of the ambition technically that people have for using large-language models in content generation and the possibilities it opens up for sorts of things that have been very hard to do or difficult, certainly at scale, with today's mechanisms, including, as you say, a big one, which is servicing many languages that previously social media firms might have decided are simply off limits for their consideration because it's just not perhaps feasible or profitable for them to address certain markets or certain languages with the type of scale that perhaps people in those places might believe is necessary in order to do a good job. So that's been a kind of consistent complaint from civil society for a long time.</p><p>But there are other things here that you point to that seem to fit in the category of promise, such as possibly more robust types of nudges or interventions in what people post, or after the fact what they post. I know I've talked to people about the idea that we can imagine content moderation with LLM starting before you ever post something, kind of prefacto content moderation, and possibly in interaction with the content moderation system as it were at that point, before you even pushed something live to your feed. But I don't know. What else did you learn in terms of just studying these things technically about the possibilities they open up?</p><p><strong>Marlena Wisniak:</strong></p><p>Yeah, thanks are bringing it up. And the report does obviously highlight harms and also explores promises for each individual, right. So I'm with you. One of the key findings of this research was that probably the most promise of LLM-driven content moderation is not to remove content or even to moderate it through a ranking system or curating content, but really anything on procedural rights. So it's before posting, like you said, nudges. Even though I will say that we also don't want this Big Brother-type experience where we're typing something and, oh dear Lord, the algorithm has found that this may be controversial. But yeah, there can be nudges, there can be suggestions for reformulating something that could be abusive, less invasive. They could even suggest other informations, for example, if someone is posting something clearly wrong about... like, factually incorrect about the Ukraine war, or elections, like civic integrity stuff, they could check out xyz.gov, right, elections.gov or something.</p><p>And also on the flip side, appeals and remedy. When a user posts content, they can immediately know that this has been flagged for... especially content that would be automatically removed, they can get an automated notice that's more personalized that this content has been flagged for review or has been removed, and give more personalized information about how they can appeal that decision. So there's all this kind of user interaction that I think is pretty cool and exciting.</p><p>There's also the possibility for personalized content moderation. So let's say one user really welcomes gore or sensitive content, borderline content that doesn't violate either the platform's policies or the law. They can adapt and adjust their own moderation, as opposed to someone who really doesn't want to see anything remotely sensitive or related to some topics because of any trauma or just dislike or whatever. That can be helpful.</p><p>And I'll give a shout out here to Discord, with whom we collaborated for a year on engaging external stakeholders as they develop ML-driven interventions to moderate abuse and harassment on their platform, specifically for teens. And so we really worked with a broad range of stakeholders, including youth and children, which is interesting, and understanding how they would like to see AI-driven innovation and intervention. So it wasn't only LLM, it was also traditional machine learning.</p><p>But yeah, so I think, to sum up creative uses of LLM for moderating content, and by moderating I mean broadly, is something that I support much more than automated removal, which one of the conclusions of this report is that, at least today, it is still too risky, potentially harmful, and just doesn't... like, ineffective to do that. There will be too many false positives and too many false negatives, both of those disproportionately falling on already marginalized groups who tend to be, as most folks, you probably know, disproportionately silenced by platform and also disproportionately targeted by violative content. So that's one of the main findings of how we can use LLMs safely is more on the procedural side than actually the content.</p><p><strong>Justin Hendrix:</strong></p><div id=\"mlb2-5983225\"><h4>Our Content delivered to your inbox.</h4><p>Join our newsletter on issues and ideas at the intersection of tech &amp; democracy</p></div><p>We'll see how platforms attempt to go about this. We're already seeing some in the wild, examples of uses of LLMs. There's been reporting even this week of Meta's new community notes program, using LLMs in certain ways. So I think there'll be probably as wide a variety of applications of LLMs as there have been of machine learning in content moderation, and we'll just, I suppose, see how it goes. I mean, different platforms, who knows, maybe one or two will err towards creating a AI nanny state hyper-sanitized environment that people may recoil from, or regard as overly censorious or what have you. And yet it's possible to imagine, as you say, lots of different types of interventions that people might regard as useful or helpful in whatever way.</p><p>But I want to pause on maybe thinking about the benefits and dig a little more into the potential human rights impacts, because that's where, of course, you spend the bulk of this report, concerned with things like privacy, freedom of expression and information and opinion, questions around peaceful assembly and association, non-discrimination, participation. Take us through a couple of those things. When you think about the most significant potential human rights impacts of the deployment of large language model systems in content moderation at scale, what do you think is most prominent?</p><p><strong>Marlena Wisniak:</strong></p><p>So I'll just highlight some of the most specific to LLMs. One thing to consider is that LLMs often exacerbate and accelerate already-existing harms done by traditional machine learning, and traditional ML accelerates and exacerbates harms committed by humans often. So I think it's like the more scale, scale can be good for accuracy and speed obviously, and it also has, there's another side to that coin.</p><p>So some of the things you'll find the report are kind of an LLM-ified analysis of automated content moderation, but I will single out a few really new concerns related to LLMs. And so one of them is kind of a, coming back to the concentration of power issue that I mentioned before, any decision that is made at the foundation model level, unless it is proactively fine-tuned at the deployment level, that will trickle down across platforms.</p><p>So to give you an example, if Meta decides that pro-Palestinian content is considered violent or terrorist content, and there has been a lot of reporting to show that, then if another platform uses Llama without changing that specifically, any decision that Meta makes at the level of Llama will then trickle down to the other platforms. So that's something to consider from a freedom of expression angle, is generalized censorship, if it's false positive, and at the same time, content that should be removed will not be removed if the foundation model does not consider that as harmful. So that interaction is particularly important because we've never seen that before, to my knowledge at least, and the dynamics on content moderation.</p><p>Another big thing around freedom of information, for example, is hallucinations. So that is a very stereotypical GenAI problem. And for folks who don't know, hallucinations are -- it's content that is generated by the AI system and that is just made up and wrong. So the weird thing is that it does in a way that seems so confident and so right, and it is just nonsense. So it'll make up academic papers, or it'll make up news articles or any kind of facts.</p><p>So if platforms use this to moderate missing this information, that will just be inaccurate to begin with. And it's hard sometimes to parse that when you have pretty convincing content. And even if, for example, human moderators would use LLMs or GenAI to help them moderate content, if they see this really elaborate article about how, whatever, Trump won the 2020 elections, for example, and they're not familiar with it, that could form the base of their decisions, and it's just plain wrong. So that's the new harm and risk.</p><p>Another one that I found was super interesting, and this, I will say, was me... a lot of this paper was me trying to envision harms and then probe it with engineers or technical folks and also non-technical as well, to ask them, does this make sense? Could this be true? And for example, from freedom of peaceful assembly and protest, one thing to consider is that protests are, and contrarian views are protest definition anti-majority.</p><p>You have a minority express themselves. You go against powerful interests like governments or companies, or even just the status quo. And what does that mean? This data is not well-represented in the training datasets. Because machine learning, I often say is just steroids on stats. So if a view is predominant in the dataset, even if it's completely wrong, that is the output, right? Machine learning never gives a real decision. It gives a prediction about what is statistically possible. So when you have protestors or journalists, investigative journalists, for example, or anybody bringing up new stuff, that will not actually show up in the dataset and therefore will not be moderated well. And let's give platforms and those deploying content moderation system the highest benefit of the doubt. They probably wants to moderate it well, just the system will not function unless specifically fine-tuned because protests fall outside the curve of statistical data.</p><p>And that's also the case for conflicts or exceptional circumstances, crises. These events are, by definition, exceptional, and therefore fall outside the statistical curve and are not moderated well. And that's another key finding. For freedom of association, one thing that could be interesting here is that some organizations can be mislabeled. Actually, you know what, Justin, forget that one. It's too long. I'll skip it.</p><p>Two last pieces I'd like to highlight that we found were really interesting. One was on participation. So on one side, LLMs actually have the potential to support more participatory design by enabling customizable moderation. So like I said before, the users have personalized content moderation, or perhaps in the future use AI agents to moderate their own content as they want. On the flip side, in practice, affected communities are largely excluded from shaping content moderation systems. That's not new. That has also happened in machine learning.</p><p>Generally, it's mostly white tech bros in San Francisco or Silicon Valley, so especially marginalized groups and those in the global majority are excluded from that. The addition with LLMs is that they are typically \"improved\" through a method called reinforcement learning from human feedback, or red-teaming. So you have folks thinking about, in the case of red-teaming, what are potential bad, worst-case scenarios and testing it from an adversarial perspective.</p><p>Same for reinforcement learning. They go through these models and they \"fix\" them, they reteach them how to learn. The problem is that people who do that are usually Stanford graduates, those in Silicon Valley or other elitist institutions, and it's very rare. I personally have never heard of folks from marginalized groups being invited to participate in these kinds of activities. I myself, for example, have been invited, but you have to be in a niche kind of AI group to do that. And everybody who I've spoken to who has done that, I personally have not. I'll say that it's a very homogeneous group. And so basically what that means is then yes, the LLMs will be improved... or, I'm sorry, let me rephrase. There are efforts to reduce inaccuracy and improve the performance of the models of the LLMs. However, the people who do that are typically very homogeneous. So it's just like snowball effects.</p><p>And the last piece on remedy is that while there are potential promises to access remedy better, like I said before, most everyone use a notification, helping them identify remedy, appeals mechanisms, speeding up the appeals, there's also fundamentally a lack of explainability and transparency, and that can create barriers to remedy. Another issue, like I said before, is that there are these two layers, the layer of foundation models. So the ChatGPT, the Claude, the Gemini, and then the social media platform that deploys it. And it's hard to know where to appeal, how to appeal. The foundation models themselves don't really know how a decision was made. Social media platforms know that even less. Do you appeal to the platform? Does the platform then appeal again to the third party LLM? Where does the user fall into this? So just accountability becomes fragmented, and there's a lot of confusion and lack of clarity around how to go around that.</p><p><strong>Justin Hendrix:</strong></p><p>So I want to come to some of your recommendations, because you have both recommendations to LLM developers and deployers as well as to, of course, those who are potentially applying these things inside of social media companies. But your section on recommendations to policymakers is perhaps mercifully brief. You've only got a handful of recommendations there. It's clear, I think just from the onus in the report on where the recommendations are, that you see it largely as something that private sector needs to sort out, how are they going to deploy these technologies or not.</p><p>But when it comes to policymakers, what are you telling them? I see you're interested in making sure they're refraining, on some level, from mandating the use of these things. I suppose it's a possibility that somebody might come along and say, \"Oh, in fact, we demand that you use them.\" I was trying to think of a context for that, but then I found myself thinking about some of the laws we've seen put forward in the United States even, where there have been segments of those laws, I think in some of the must-carry laws, where there have been these ideas around transparency of moderation decisions. I feel like I've read amicus briefs where some of the people opposed to those laws would make arguments like, \"Well, it's just simply not possible to give explainable rationale to every single user for every single content-moderation decision that's made.\"</p><p>Well, presumably, LLMs would make that quite possible, and you could imagine a government coming along and saying, \"Somehow, in the interest of free expression, we would like to mandate, use artificial intelligence to explain any content moderation decision that you might take.\" You say that's a bad idea along with other things, but what would you tell policymakers to be paying attention to here?</p><p><strong>Marlena Wisniak:</strong></p><p>Yeah, I mean, that's a very astute... you observe that well. Most of the recommendations are to LLMs developers and employers. The reason for this is not that we only see the owners on the private sector, not on the public sector, is that for practical reasons, this part of the research was mostly on assessing the human rights impact. And the recommendations piece was really the last part, and we didn't have that much capacity to go deep. So the second iteration of this report, hopefully we will continue. It will be really to zoom into the recommendations.</p><p>And I will say then, that on the policy making side, so a lot of our work at ECNL is on policy and legal advocacy. We've been working behind the scenes on the AI Act for the past five years, and right now, there's conversations around the GPAI code in the EU on general-purpose AI. And the reason why I didn't go deep here is that one, we don't want a specific AI content moderation or LLM moderation law. We have the DSA in Europe. The U.S. I'll just set aside, because right now there's a lot going on. So we're not calling for a specific LLM content moderation law.</p><p>And the EU already has the DSA and the AI. And I'd say a lot of the foundational aspects of LLMs to policymakers would be kind of basic AI around, like, data protection, human rights impact assessment, stakeholder engagement. So I added these big categories. I didn't go really deep into them. It's mostly how can we not fuck it up, to be honest. And I think content moderation, as you know, is a very fraught topic where even folks who are well-intentioned just don't understand it enough. So sometimes you will have these claims that would let... platform should remove problematic content within an hour. And it's like, okay, cool. In theory, that sounds great. What does that mean in practice? It means a lot of false positives. It means that disproportionately marginalized groups will be impacted, including sex workers, racialized groups, queer folks. And you'll have to use automated content moderation, which has all the false positives and false negatives that haven't reported on.</p><p>So the key recommendation we made to policymakers here are very foundational, I'd say. One, do not mandate LLM moderation exactly for the reason that you expressed before. Two, maintain human oversight. So if LLMs are used to moderate content, and especially to remove content, there still should be a legal requirements for platforms to integrate human-in-the-loop systems, meaning that humans will review whatever decision LLM does. Three, kind of broad transparency and accountability metrics and requirements that's very DSA-esque... sorry, I should have said Digital Services Act. And a lot of that is really about transparency, like mandating disclosure of how LLM systems function and how they're used. The reality, Justin, we don't know. I had several off-the-record calls with platforms, off-the-record means I couldn't publish the names. They also gave me very vague information. There's some information that was published by them, and you'll see it in the report.</p><p>ChatGPT said they use LLMs for content moderation. Meta says they're beginning to play around with it, gemini or Google. But overall, we don't know. I definitely don't know when it's used when I use social media. We don't know accuracy rates, we don't know how they're used in appeals, or how they're enforced. So really requiring platforms to notify users about LLM moderation actions. And again, that's nothing new, I would say. That's just using prior Santa Clara principles on transparency and accountability, or DSA or kind of like mainstream civil-society asks and implementing them for LLMs.</p><p>The two last ones that we highlighted, one is mandating human rights impact assessments. So hey, platforms, good news. I did one here, so you can use... My goal is that this will be a starting point for platforms to have basically an HRA handed to them on a silver platter, and then obviously use this as a starting point to look at how they implement LLMs on their platform. It'll be specific to each platform.</p><p>But on that note, one thing that policymakers could do is make HRAs mandatory for LLM developers and platforms that deploy them, both before deployment and throughout the LLM lifecycle. So for example, the pilot with Discord was at the ideation phase, so design, product design. Before they even developed the product, they consulted with a lot of folks to see how an LLM or machine-learning-driven system could be helpful. Is it nudges? Is it for removing content? Do we not want that? Why? And then continue that throughout all the way through development and deployment and make it accessible for external stakeholders.</p><p>There's so much expertise in the room, and I often say to platforms that, you know, trust and safety teams, policy teams, human rights teams tend to be underfunded. Just go to civil society that has such extensive knowledge, or journalists like yourself and academics and just drink their wisdom, because there's a lot of stuff out there.</p><p><strong>Justin Hendrix:</strong></p><p>This report seems like a good jumping-off point for a lot of folks who might be interested in investigating or collecting artifacts of how the platforms are deploying these things or generally kind of trying to pay attention to these issues and trying to discern whether the introduction of LLMs in this context is on balance a good thing, a bad thing, or perhaps somehow neutral or indiscernible. With regard to the overall information integrity environment, what would you encourage people to do next? What would you encourage them to go and look at? What threads would you like to see the field pulled from here?</p><p>Marlena Wisniak:</p><p>Yeah. One core piece that we didn't talk about much is the multilingual piece of it, so how this can work in different languages. And I'll just give a shout out here to really cool efforts in the global majority to develop community-driven local smaller LLMs or data sets, like the Masakhane in Africa. There's really cool community-driven initiatives that kind of go beyond the Silicon Valley profit-first massive monopolization and dynamics. So that is one thing, and I really encourage not only researchers, but also platforms to talk to them. A lot of the platforms that I spoke with, they didn't even know these existed, and if I say that this report is an HRA handed on a silver platter, they have really cool data sets that would be really helpful, especially to smaller platforms, and they could just plug these into their own model.</p><p>So that's one thing that I hope research and industry will move towards is more languages, more dialects, understanding that it's not only a difference between English and other language. It really is a colonial imperialist dynamic where English or French or German or Spanish will be much better moderated, and languages that are close to these ones are better, and then obscure, poorly researched languages work very, very poorly. And you can read in the report the reasons are both because the data sets do not exist or they're just bad quality, because there's not enough investment. So I really would encourage platforms to invest more resources into that, more participation, and proactively include stakeholders.</p><p>The other area I would love to see is just open conversations between platform and civil society. And like I said, this is a nerdy topic, like LLMs and content moderation. It doesn't roll off the tongue. So if folks with expertise in content moderation would like, hopefully this report can give them more context. And then I would love to see more evidence, new ideas. Like I said, some of these things were my own kind of \"How could this work?\" But through many conversation with folks, and I really would want to see more thinking, more assessment of impacts and more evidence as well.</p><p>Like this paper, it's long, 70 pages. I didn't mean to make it this long, but there's a lot of stuff. And I'd say the last part is computer science papers move so fast, incredibly fast. It's hard to keep up, and they're very theoretical. So to the extent that there can be more collaboration between technical folks who come from the machine learning, AI, or computer science field, and policy and human rights, I think we'll actually be able to build much better products and push policymakers to regulate this stuff better.</p><p><strong>Justin Hendrix:</strong></p><p>I appreciate you taking the time to speak to me about this, and I would encourage my readers to go and check out this full report, which is available on the ecnl.org website. I will include a link to it in the show notes. Thank you very much.</p><p><strong>Marlena Wisniak:</strong></p><p>Thanks, Justin.</p></div></div>","textContent":"Audio of this conversation is available via your favorite podcast service.At Tech Policy Press, we’ve been tracking the emerging application of generative AI systems in content moderation. Recently, the European Center for Not-for-Profit Law (ECNL) released a comprehensive report titled Algorithmic Gatekeepers: The Human Rights Impacts of LLM Content Moderation, which looks at the opportunities and challenges of using generative AI in content moderation systems at scale. I spoke to its primary author, ECNL senior legal manager Marlena Wisniak.What follows is a lightly edited transcript of the discussion.Justin Hendrix:Marlena, can you tell us a little bit about what the ECNL does?Marlena Wisniak:The Center, or ECNL as we call it, is a human rights and civil liberties organization for over 20 years. We've been mostly focusing on civic space, making sure that civil society organizations, but also grassroots orgs, activists, have a safe space to organize and do their work. We're mostly lawyers, but I have to say we're the fun lawyers. So we also do advocacy, research, and basically anything that we can do to protect and promote in enabling civic space. My team was founded in 2020, I believe, and has from the beginning focused mostly on AI, but it's more broadly the digital team at ECNL.So we look at how technologies, specifically emerging technologies, impact civic space and human rights. So our core human rights or civil liberties that we look at is typically privacy, freedom of expression, freedom of assembly, so rights to protest, for example, association, right to organize, and non-discrimination. And our key areas of focus substantively are typically surveillance, AI-driven surveillance and biometric surveillance, and broader and social media platforms that plays such a big role in civic space today. And that's what I'll be talking about today. But that's, in a nutshell, my team and how it fits within the broader org.Justin Hendrix:Well, I'm excited to talk to you about this report that you have authored with help from a variety of different corners, but it's called Algorithmic Gatekeepers, the Human Rights Impacts of LLM Content Moderation. So this is a topic that we've been trying to follow at Tech Policy Press fairly closely, because I feel like it is the sort of intersection of a lot of things that we care about around social media, content moderation, online safety, free expression, and then, of course, artificial intelligence. And LLMs, generative AI generally, the intersection of these two things, I think, is probably one of the most interesting and possibly under-covered or under-explored, at least to date, issues at the intersection of tech and democracy.So I want to just start by asking you how you did this report. It's quite a significant document. A lot of research obviously went into this, including gathering some new information, not just combining citations, as many people do when they produce a PDF white paper. What got you started on this and how did you go about it?Marlena Wisniak:Yes, you're right. It was really a heavy lift. It was a research project that went on for about a year, and we collaborated with great folks including yourself. I remember, I think last year we had a call to hear your thoughts, and shout out, before I go deep into context, to Isabelle Anzabi, who was our fellow last summer, and really helped me just go through a lot of papers, mostly in computer science, and the Omidyar Network, who provided generous support for this project.And so how it started, I accidentally fell into AI in 2016, but come from content moderation in the 2010s, '11, '12, so I'd say early days of content moderation. So automated content moderation has always been a big focus of mine. I was also at Twitter at some point, overseeing their legal department, sorry, content-governance and illegal department. And living in San Francisco right now, really, the big talk on the street is always LLMs, and GenAI more broadly.And so I started hearing various use cases of LLMs for content moderation. Most of the talk, I will say, and focus of the research and civil society community is how to moderate LLM-generated content, so how to moderate ChatGPT, for example, or Claude. That is, of course, super important. And I wanted to look at it from another angle, which is how are LLMs used for content moderation? And this I will say, Justin, has been a little bit of a chicken-and-egg conversation, or reflection, because the way how LLMs can moderate content also impacts how LLM-driven content will be moderated, and how they're moderated impacts how they moderate content. So it's hard to separate those, one from another, but I did choose to have that focus. And it's interesting because it's very much a nerdy topic and yet has real-world implications, and it's very hyped up.So one of the things that I always love to do is go beyond the hype. I'm not a hype person. In fact, now I hate AI. It was fun working on AI in the 2016, '17, '18 years when nobody really cared about AI. Since ChatGPT was released, the only thing people ever want to talk about is AI. So now I want to scream. But all this to say that I did want to impact some of the real implications and post what our promises and what our, really, I'd say, not even realistic promises, but the types of impacts we want to see, because I do believe that it can helpful, and also, as this is an emerging space, prevent any possible harm. And I think folks listening to this podcast probably know that human motivation is extremely complicated. It's horrible for workers. It doesn't always, and always is an understatement, produce good outcomes.And so machine learning came as a solution to that. It was expanded during COVID, and sort of came as this silver-bullet solution. There has been increasing research showing the limitations of that. And so now, LLM is the new white horse, is that an expression? Is the savior, and there's a lot of hype. So that's where I came from. It's like, okay, let's go deeper into this. Let's review a lot of computer science papers where some of this more rigorous qualitative and quantitative work has done, translate it to policy folks, and bring a human-rights approach because that's my background. I'm an international human rights lawyer.Justin Hendrix:So readers of Tech Policy Press have at least had multiple pieces that we've posted on the site about this issue, and often there's been this sort of question about what is the promise potentially to offset the dangers to the labor force that currently engage in content moderation on behalf of platforms, but then also, of course, there are various perils that we can imagine as well. We're going to get into those a little bit.But I think one thing that distinguishes your report that I wanted to just start with perhaps is that you include a technical primer. You've got a kind of set of definitions and, I think usefully, some distinctions between what's going on with LLMs for content moderation versus more standard machine-learning classifiers and recommendation mechanisms and other types of algorithmic models that have been used in content moderation for quite some time now. What do you think the listener needs to know about the technical aspect of this phenomenon, of the use of LLMs for content moderation at scale from our vantage right now in June of 2025?Marlena Wisniak:Yeah, I mean, I encourage folks to look at the technical primer, that is, the audience for that are mostly folks who aren't very familiar with either industry jargon or technical terms. Leveling up, I think one thing that is really critical to consider when thinking about LLM-driven content moderation is that you have typically two layers. So one is the foundation model layer, or the LLMs. LLM are, I should have started, large-language models. And that's pretty explicit. It's large-language model, so people see it as God, or the sci-fi technology. It's really not. It's really big data sets.And I think we often forget that. So if we think about traditional machine learning, the difference is that this is a larger set. And so there's this implication, obviously, for privacy and other rights that we can explore later down the line. But one, considered, they're very, very, very big, enormous data sets that require a lot of computing power. And so really, there's only a handful of companies right now building these models.But the ones that we looked at in more depth are Llama by Facebook, sorry, Meta, ChatGPT, OpenAI, Claude, Anthropic, Gemini, Google. And then right as I was finishing out the report, DeepSeek came out. So there's emerging models as well, but it's still a very small number of foundation models, given how much data and compute they use. And I often say they're really not that technically complicated. They're just bigger. And so from a concentration of power, that has a lot of importance.And so then the platforms that I mentioned, they both develop and often use... they develop the LLMs and they use them for content moderation. But what happens for all the other ones like Discord or Reddit, or Slack? Although Slack might be bought by one of them. But anyways, there are so many other platforms. Typically, they do not build their own LLMs. They will either have a license with one of the foundation-model companies and then fine-tune them or they will use one of the open source, like DeepSeek or Llama or something they find on Hugging Face, and then fine-tune them for their purposes.But what that means, so that's one thing from a technical thing to understand how LLMs work and how they're used in practice. And then another thing I will flag is that LLMs are a subset of generative AI. That's a term we most commonly know. Generative AI today has, to some extent, become equivalent of ChatGPT. It's much broader than ChatGPT, but LLMs are one subset of that, or foundation models as well.And then maybe one last thing I'll flag is multilingual language models are those that are trained on text data from dozens to sometimes hundreds of languages simultaneously. And so the idea is that they will be more capable of processing and generating inputs and outputs in multiple languages. And we can about whether that works or not. There's fantastic research that has been done that inspired me a lot for my work.Justin Hendrix:Yeah, I'll just say that the primer, I think what's great about, again, this report is that it does detail at least some of the ambition technically that people have for using large-language models in content generation and the possibilities it opens up for sorts of things that have been very hard to do or difficult, certainly at scale, with today's mechanisms, including, as you say, a big one, which is servicing many languages that previously social media firms might have decided are simply off limits for their consideration because it's just not perhaps feasible or profitable for them to address certain markets or certain languages with the type of scale that perhaps people in those places might believe is necessary in order to do a good job. So that's been a kind of consistent complaint from civil society for a long time.But there are other things here that you point to that seem to fit in the category of promise, such as possibly more robust types of nudges or interventions in what people post, or after the fact what they post. I know I've talked to people about the idea that we can imagine content moderation with LLM starting before you ever post something, kind of prefacto content moderation, and possibly in interaction with the content moderation system as it were at that point, before you even pushed something live to your feed. But I don't know. What else did you learn in terms of just studying these things technically about the possibilities they open up?Marlena Wisniak:Yeah, thanks are bringing it up. And the report does obviously highlight harms and also explores promises for each individual, right. So I'm with you. One of the key findings of this research was that probably the most promise of LLM-driven content moderation is not to remove content or even to moderate it through a ranking system or curating content, but really anything on procedural rights. So it's before posting, like you said, nudges. Even though I will say that we also don't want this Big Brother-type experience where we're typing something and, oh dear Lord, the algorithm has found that this may be controversial. But yeah, there can be nudges, there can be suggestions for reformulating something that could be abusive, less invasive. They could even suggest other informations, for example, if someone is posting something clearly wrong about... like, factually incorrect about the Ukraine war, or elections, like civic integrity stuff, they could check out xyz.gov, right, elections.gov or something.And also on the flip side, appeals and remedy. When a user posts content, they can immediately know that this has been flagged for... especially content that would be automatically removed, they can get an automated notice that's more personalized that this content has been flagged for review or has been removed, and give more personalized information about how they can appeal that decision. So there's all this kind of user interaction that I think is pretty cool and exciting.There's also the possibility for personalized content moderation. So let's say one user really welcomes gore or sensitive content, borderline content that doesn't violate either the platform's policies or the law. They can adapt and adjust their own moderation, as opposed to someone who really doesn't want to see anything remotely sensitive or related to some topics because of any trauma or just dislike or whatever. That can be helpful.And I'll give a shout out here to Discord, with whom we collaborated for a year on engaging external stakeholders as they develop ML-driven interventions to moderate abuse and harassment on their platform, specifically for teens. And so we really worked with a broad range of stakeholders, including youth and children, which is interesting, and understanding how they would like to see AI-driven innovation and intervention. So it wasn't only LLM, it was also traditional machine learning.But yeah, so I think, to sum up creative uses of LLM for moderating content, and by moderating I mean broadly, is something that I support much more than automated removal, which one of the conclusions of this report is that, at least today, it is still too risky, potentially harmful, and just doesn't... like, ineffective to do that. There will be too many false positives and too many false negatives, both of those disproportionately falling on already marginalized groups who tend to be, as most folks, you probably know, disproportionately silenced by platform and also disproportionately targeted by violative content. So that's one of the main findings of how we can use LLMs safely is more on the procedural side than actually the content.Justin Hendrix:Our Content delivered to your inbox.Join our newsletter on issues and ideas at the intersection of tech & democracyWe'll see how platforms attempt to go about this. We're already seeing some in the wild, examples of uses of LLMs. There's been reporting even this week of Meta's new community notes program, using LLMs in certain ways. So I think there'll be probably as wide a variety of applications of LLMs as there have been of machine learning in content moderation, and we'll just, I suppose, see how it goes. I mean, different platforms, who knows, maybe one or two will err towards creating a AI nanny state hyper-sanitized environment that people may recoil from, or regard as overly censorious or what have you. And yet it's possible to imagine, as you say, lots of different types of interventions that people might regard as useful or helpful in whatever way.But I want to pause on maybe thinking about the benefits and dig a little more into the potential human rights impacts, because that's where, of course, you spend the bulk of this report, concerned with things like privacy, freedom of expression and information and opinion, questions around peaceful assembly and association, non-discrimination, participation. Take us through a couple of those things. When you think about the most significant potential human rights impacts of the deployment of large language model systems in content moderation at scale, what do you think is most prominent?Marlena Wisniak:So I'll just highlight some of the most specific to LLMs. One thing to consider is that LLMs often exacerbate and accelerate already-existing harms done by traditional machine learning, and traditional ML accelerates and exacerbates harms committed by humans often. So I think it's like the more scale, scale can be good for accuracy and speed obviously, and it also has, there's another side to that coin.So some of the things you'll find the report are kind of an LLM-ified analysis of automated content moderation, but I will single out a few really new concerns related to LLMs. And so one of them is kind of a, coming back to the concentration of power issue that I mentioned before, any decision that is made at the foundation model level, unless it is proactively fine-tuned at the deployment level, that will trickle down across platforms.So to give you an example, if Meta decides that pro-Palestinian content is considered violent or terrorist content, and there has been a lot of reporting to show that, then if another platform uses Llama without changing that specifically, any decision that Meta makes at the level of Llama will then trickle down to the other platforms. So that's something to consider from a freedom of expression angle, is generalized censorship, if it's false positive, and at the same time, content that should be removed will not be removed if the foundation model does not consider that as harmful. So that interaction is particularly important because we've never seen that before, to my knowledge at least, and the dynamics on content moderation.Another big thing around freedom of information, for example, is hallucinations. So that is a very stereotypical GenAI problem. And for folks who don't know, hallucinations are -- it's content that is generated by the AI system and that is just made up and wrong. So the weird thing is that it does in a way that seems so confident and so right, and it is just nonsense. So it'll make up academic papers, or it'll make up news articles or any kind of facts.So if platforms use this to moderate missing this information, that will just be inaccurate to begin with. And it's hard sometimes to parse that when you have pretty convincing content. And even if, for example, human moderators would use LLMs or GenAI to help them moderate content, if they see this really elaborate article about how, whatever, Trump won the 2020 elections, for example, and they're not familiar with it, that could form the base of their decisions, and it's just plain wrong. So that's the new harm and risk.Another one that I found was super interesting, and this, I will say, was me... a lot of this paper was me trying to envision harms and then probe it with engineers or technical folks and also non-technical as well, to ask them, does this make sense? Could this be true? And for example, from freedom of peaceful assembly and protest, one thing to consider is that protests are, and contrarian views are protest definition anti-majority.You have a minority express themselves. You go against powerful interests like governments or companies, or even just the status quo. And what does that mean? This data is not well-represented in the training datasets. Because machine learning, I often say is just steroids on stats. So if a view is predominant in the dataset, even if it's completely wrong, that is the output, right? Machine learning never gives a real decision. It gives a prediction about what is statistically possible. So when you have protestors or journalists, investigative journalists, for example, or anybody bringing up new stuff, that will not actually show up in the dataset and therefore will not be moderated well. And let's give platforms and those deploying content moderation system the highest benefit of the doubt. They probably wants to moderate it well, just the system will not function unless specifically fine-tuned because protests fall outside the curve of statistical data.And that's also the case for conflicts or exceptional circumstances, crises. These events are, by definition, exceptional, and therefore fall outside the statistical curve and are not moderated well. And that's another key finding. For freedom of association, one thing that could be interesting here is that some organizations can be mislabeled. Actually, you know what, Justin, forget that one. It's too long. I'll skip it.Two last pieces I'd like to highlight that we found were really interesting. One was on participation. So on one side, LLMs actually have the potential to support more participatory design by enabling customizable moderation. So like I said before, the users have personalized content moderation, or perhaps in the future use AI agents to moderate their own content as they want. On the flip side, in practice, affected communities are largely excluded from shaping content moderation systems. That's not new. That has also happened in machine learning.Generally, it's mostly white tech bros in San Francisco or Silicon Valley, so especially marginalized groups and those in the global majority are excluded from that. The addition with LLMs is that they are typically \"improved\" through a method called reinforcement learning from human feedback, or red-teaming. So you have folks thinking about, in the case of red-teaming, what are potential bad, worst-case scenarios and testing it from an adversarial perspective.Same for reinforcement learning. They go through these models and they \"fix\" them, they reteach them how to learn. The problem is that people who do that are usually Stanford graduates, those in Silicon Valley or other elitist institutions, and it's very rare. I personally have never heard of folks from marginalized groups being invited to participate in these kinds of activities. I myself, for example, have been invited, but you have to be in a niche kind of AI group to do that. And everybody who I've spoken to who has done that, I personally have not. I'll say that it's a very homogeneous group. And so basically what that means is then yes, the LLMs will be improved... or, I'm sorry, let me rephrase. There are efforts to reduce inaccuracy and improve the performance of the models of the LLMs. However, the people who do that are typically very homogeneous. So it's just like snowball effects.And the last piece on remedy is that while there are potential promises to access remedy better, like I said before, most everyone use a notification, helping them identify remedy, appeals mechanisms, speeding up the appeals, there's also fundamentally a lack of explainability and transparency, and that can create barriers to remedy. Another issue, like I said before, is that there are these two layers, the layer of foundation models. So the ChatGPT, the Claude, the Gemini, and then the social media platform that deploys it. And it's hard to know where to appeal, how to appeal. The foundation models themselves don't really know how a decision was made. Social media platforms know that even less. Do you appeal to the platform? Does the platform then appeal again to the third party LLM? Where does the user fall into this? So just accountability becomes fragmented, and there's a lot of confusion and lack of clarity around how to go around that.Justin Hendrix:So I want to come to some of your recommendations, because you have both recommendations to LLM developers and deployers as well as to, of course, those who are potentially applying these things inside of social media companies. But your section on recommendations to policymakers is perhaps mercifully brief. You've only got a handful of recommendations there. It's clear, I think just from the onus in the report on where the recommendations are, that you see it largely as something that private sector needs to sort out, how are they going to deploy these technologies or not.But when it comes to policymakers, what are you telling them? I see you're interested in making sure they're refraining, on some level, from mandating the use of these things. I suppose it's a possibility that somebody might come along and say, \"Oh, in fact, we demand that you use them.\" I was trying to think of a context for that, but then I found myself thinking about some of the laws we've seen put forward in the United States even, where there have been segments of those laws, I think in some of the must-carry laws, where there have been these ideas around transparency of moderation decisions. I feel like I've read amicus briefs where some of the people opposed to those laws would make arguments like, \"Well, it's just simply not possible to give explainable rationale to every single user for every single content-moderation decision that's made.\"Well, presumably, LLMs would make that quite possible, and you could imagine a government coming along and saying, \"Somehow, in the interest of free expression, we would like to mandate, use artificial intelligence to explain any content moderation decision that you might take.\" You say that's a bad idea along with other things, but what would you tell policymakers to be paying attention to here?Marlena Wisniak:Yeah, I mean, that's a very astute... you observe that well. Most of the recommendations are to LLMs developers and employers. The reason for this is not that we only see the owners on the private sector, not on the public sector, is that for practical reasons, this part of the research was mostly on assessing the human rights impact. And the recommendations piece was really the last part, and we didn't have that much capacity to go deep. So the second iteration of this report, hopefully we will continue. It will be really to zoom into the recommendations.And I will say then, that on the policy making side, so a lot of our work at ECNL is on policy and legal advocacy. We've been working behind the scenes on the AI Act for the past five years, and right now, there's conversations around the GPAI code in the EU on general-purpose AI. And the reason why I didn't go deep here is that one, we don't want a specific AI content moderation or LLM moderation law. We have the DSA in Europe. The U.S. I'll just set aside, because right now there's a lot going on. So we're not calling for a specific LLM content moderation law.And the EU already has the DSA and the AI. And I'd say a lot of the foundational aspects of LLMs to policymakers would be kind of basic AI around, like, data protection, human rights impact assessment, stakeholder engagement. So I added these big categories. I didn't go really deep into them. It's mostly how can we not fuck it up, to be honest. And I think content moderation, as you know, is a very fraught topic where even folks who are well-intentioned just don't understand it enough. So sometimes you will have these claims that would let... platform should remove problematic content within an hour. And it's like, okay, cool. In theory, that sounds great. What does that mean in practice? It means a lot of false positives. It means that disproportionately marginalized groups will be impacted, including sex workers, racialized groups, queer folks. And you'll have to use automated content moderation, which has all the false positives and false negatives that haven't reported on.So the key recommendation we made to policymakers here are very foundational, I'd say. One, do not mandate LLM moderation exactly for the reason that you expressed before. Two, maintain human oversight. So if LLMs are used to moderate content, and especially to remove content, there still should be a legal requirements for platforms to integrate human-in-the-loop systems, meaning that humans will review whatever decision LLM does. Three, kind of broad transparency and accountability metrics and requirements that's very DSA-esque... sorry, I should have said Digital Services Act. And a lot of that is really about transparency, like mandating disclosure of how LLM systems function and how they're used. The reality, Justin, we don't know. I had several off-the-record calls with platforms, off-the-record means I couldn't publish the names. They also gave me very vague information. There's some information that was published by them, and you'll see it in the report.ChatGPT said they use LLMs for content moderation. Meta says they're beginning to play around with it, gemini or Google. But overall, we don't know. I definitely don't know when it's used when I use social media. We don't know accuracy rates, we don't know how they're used in appeals, or how they're enforced. So really requiring platforms to notify users about LLM moderation actions. And again, that's nothing new, I would say. That's just using prior Santa Clara principles on transparency and accountability, or DSA or kind of like mainstream civil-society asks and implementing them for LLMs.The two last ones that we highlighted, one is mandating human rights impact assessments. So hey, platforms, good news. I did one here, so you can use... My goal is that this will be a starting point for platforms to have basically an HRA handed to them on a silver platter, and then obviously use this as a starting point to look at how they implement LLMs on their platform. It'll be specific to each platform.But on that note, one thing that policymakers could do is make HRAs mandatory for LLM developers and platforms that deploy them, both before deployment and throughout the LLM lifecycle. So for example, the pilot with Discord was at the ideation phase, so design, product design. Before they even developed the product, they consulted with a lot of folks to see how an LLM or machine-learning-driven system could be helpful. Is it nudges? Is it for removing content? Do we not want that? Why? And then continue that throughout all the way through development and deployment and make it accessible for external stakeholders.There's so much expertise in the room, and I often say to platforms that, you know, trust and safety teams, policy teams, human rights teams tend to be underfunded. Just go to civil society that has such extensive knowledge, or journalists like yourself and academics and just drink their wisdom, because there's a lot of stuff out there.Justin Hendrix:This report seems like a good jumping-off point for a lot of folks who might be interested in investigating or collecting artifacts of how the platforms are deploying these things or generally kind of trying to pay attention to these issues and trying to discern whether the introduction of LLMs in this context is on balance a good thing, a bad thing, or perhaps somehow neutral or indiscernible. With regard to the overall information integrity environment, what would you encourage people to do next? What would you encourage them to go and look at? What threads would you like to see the field pulled from here?Marlena Wisniak:Yeah. One core piece that we didn't talk about much is the multilingual piece of it, so how this can work in different languages. And I'll just give a shout out here to really cool efforts in the global majority to develop community-driven local smaller LLMs or data sets, like the Masakhane in Africa. There's really cool community-driven initiatives that kind of go beyond the Silicon Valley profit-first massive monopolization and dynamics. So that is one thing, and I really encourage not only researchers, but also platforms to talk to them. A lot of the platforms that I spoke with, they didn't even know these existed, and if I say that this report is an HRA handed on a silver platter, they have really cool data sets that would be really helpful, especially to smaller platforms, and they could just plug these into their own model.So that's one thing that I hope research and industry will move towards is more languages, more dialects, understanding that it's not only a difference between English and other language. It really is a colonial imperialist dynamic where English or French or German or Spanish will be much better moderated, and languages that are close to these ones are better, and then obscure, poorly researched languages work very, very poorly. And you can read in the report the reasons are both because the data sets do not exist or they're just bad quality, because there's not enough investment. So I really would encourage platforms to invest more resources into that, more participation, and proactively include stakeholders.The other area I would love to see is just open conversations between platform and civil society. And like I said, this is a nerdy topic, like LLMs and content moderation. It doesn't roll off the tongue. So if folks with expertise in content moderation would like, hopefully this report can give them more context. And then I would love to see more evidence, new ideas. Like I said, some of these things were my own kind of \"How could this work?\" But through many conversation with folks, and I really would want to see more thinking, more assessment of impacts and more evidence as well.Like this paper, it's long, 70 pages. I didn't mean to make it this long, but there's a lot of stuff. And I'd say the last part is computer science papers move so fast, incredibly fast. It's hard to keep up, and they're very theoretical. So to the extent that there can be more collaboration between technical folks who come from the machine learning, AI, or computer science field, and policy and human rights, I think we'll actually be able to build much better products and push policymakers to regulate this stuff better.Justin Hendrix:I appreciate you taking the time to speak to me about this, and I would encourage my readers to go and check out this full report, which is available on the ecnl.org website. I will include a link to it in the show notes. Thank you very much.Marlena Wisniak:Thanks, Justin.","length":34393,"excerpt":"A conversation with ECNL's Marlena Wisniak on her report, Algorithmic Gatekeepers: The Human Rights Impacts of LLM Content Moderation.","byline":"Justin Hendrix","dir":null,"siteName":"Tech Policy Press","lang":"en"},"finalizedMeta":{"title":"Considering the Human Rights Impacts of LLM Content Moderation","description":"A conversation with ECNL&apos;s Marlena Wisniak on her report, Algorithmic Gatekeepers: The Human Rights Impacts of LLM Content Moderation.","author":"Justin Hendrix","creator":"Justin Hendrix","publisher":"Tech Policy Press","date":"2025-07-06T18:49:48Z","topics":[]},"jsonLd":{"@type":"Article","headline":"Considering the Human Rights Impacts of LLM Content Moderation","description":"A conversation with ECNL&apos;s Marlena Wisniak on her report, Algorithmic Gatekeepers: The Human Rights Impacts of LLM Content Moderation.","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"https://techpolicy.press/considering-the-human-rights-impacts-of-llm-content-moderation"},"datePublished":"2025-07-06T14:08:55.749Z","dateModified":"2025-07-06T18:49:48Z","isAccessibleForFree":true,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":[{"@type":"Person","name":"Justin Hendrix","url":"https://techpolicy.press/author/justin-hendrix"}],"publisher":{"@type":"Organization","name":"Tech Policy Press","logo":{"@type":"ImageObject","url":"https://cdn.sanity.io/images/3tzzh18d/production/697d4cc6122b80fcb64b256d888010c242ce6beb-1200x675.png"}},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"@context":"https://schema.org"},"twitterObj":false,"status":200,"metadata":{"author":"Justin Hendrix","title":"Considering the Human Rights Impacts of LLM Content Moderation | TechPolicy.Press","description":"A conversation with ECNL's Marlena Wisniak on her report, Algorithmic Gatekeepers: The Human Rights Impacts of LLM Content Moderation.","canonical":"https://techpolicy.press/considering-the-human-rights-impacts-of-llm-content-moderation","keywords":[],"image":"https://sa.recoding.tech/noscript.gif","firstParagraph":"Home"},"dublinCore":{},"opengraph":{"title":"Considering the Human Rights Impacts of LLM Content Moderation | TechPolicy.Press","description":"A conversation with ECNL's Marlena Wisniak on her report, Algorithmic Gatekeepers: The Human Rights Impacts of LLM Content Moderation.","url":"https://techpolicy.press/considering-the-human-rights-impacts-of-llm-content-moderation","site_name":"Tech Policy Press","locale":"en_US","type":"article","typeObject":{"published_time":"2025-07-06T14:08:55.749Z","modified_time":"2025-07-06T18:49:48Z","author":"http://techpolicy.press/author/justin-hendrix","publisher":false,"section":false,"tag":[]},"image":"https://cdn.sanity.io/images/3tzzh18d/production/ca8393b380961a234123ad924f89ed855aed0c1d-1200x675.png"},"twitter":{"site":"@TechPolicyPress","description":false,"card":"summary_large_image","creator":"@TechPolicyPress","title":false,"image":false},"archivedData":{"link":false,"wayback":false}}}