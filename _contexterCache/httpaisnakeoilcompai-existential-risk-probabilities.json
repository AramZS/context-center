{"initialLink":"http://aisnakeoil.com/p/ai-existential-risk-probabilities","sanitizedLink":"http://aisnakeoil.com/p/ai-existential-risk-probabilities","finalLink":"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities\" itemprop=\"url\">AI existential risk probabilities are too unreliable to inform policy</a></contexter-box-head></contexter-box-head><contexter-byline class=\"p-author author\" slot=\"author\"><span class=\"p-name byline\" rel=\"author\" itemprop=\"author\">Arvind Narayanan</span></contexter-byline><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2024-07-26T11:29:25.000Z\">7/26/2024</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>How speculation gets laundered through pseudo-quantification</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a is=\"contexter-link\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"45b54086913f9b0f28c79679cf7926dd73c89d22","data":{"originalLink":"http://aisnakeoil.com/p/ai-existential-risk-probabilities","sanitizedLink":"http://aisnakeoil.com/p/ai-existential-risk-probabilities","canonical":"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities","htmlText":"<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n        <meta charset=\"utf-8\" />\n        <meta name=\"norton-safeweb-site-verification\" content=\"24usqpep0ejc5w6hod3dulxwciwp0djs6c6ufp96av3t4whuxovj72wfkdjxu82yacb7430qjm8adbd5ezlt4592dq4zrvadcn9j9n-0btgdzpiojfzno16-fnsnu7xd\" />\n        \n        <link rel=\"preconnect\" href=\"https://substackcdn.com\" />\n        \n\n        \n\n        \n        <link rel=\"preload\" as=\"style\" href=\"https://substackcdn.com/bundle/theme/main.548d4784e25e1f055dfd.css\" />\n        \n        <link rel=\"preload\" as=\"style\" href=\"https://substackcdn.com/bundle/theme/color_links.33eb7c3bd37d78bc2dc3.css\" />\n        \n        \n        \n        <link rel=\"preload\" as=\"font\" href=\"https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwr7I_FMl_E.woff2\" crossorigin />\n        \n\n        \n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/entry-73201f3b.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/index-2caf47c8.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/responsive_img-51b02764.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/FlexBox-9653b903.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/free_email_form-a84f1e43.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/Modal-507c84e7.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/createComponent-f90fb756.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/ElevatedTheme-4e706070.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/app_install_modal-6cf7eb60.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/ProfileHoverCard-8ea7be39.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/HoverCard-9fe4e916.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/Menu-b8c38d19.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/UserBadge-d10c54cf.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/Tooltip-d0af0cf2.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/Avatar-21e4510c.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/Select-1e64850c.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/IntroPopup-a01e8e94.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/SectionPageContainer-cc32eba2.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/NavbarUserWidget-63813d75.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/homepage_hooks-1b33585e.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/sortBy-fe5c4c56.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/Progress-3aee957a.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/recentSurfaces-fa9efc75.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/user_indicator-e7037a0a.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/ShareableImageModal-6a735338.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/Switch-6bb7e637.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/Button-8514f63d.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/PubAccentTheme-e486b485.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/overflow_menu-db9a534e.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/newsletter_item_list-03468b3d.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/CreditCardIcon-d7e0cff2.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/ProfileSetupToast-b1d1db47.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/CommunityPostView-26824100.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/Attachments-5958cf02.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/CommentBody-984c6bbf.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/AlertDialog-9ade601d.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/uniq-b886ec82.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/Popover-6a0c224a.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/ChatPage-3c675abd.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/ChatAppUpsell-5a642ab8.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/post-23d0d6c8.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/mention-5272cfb1.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/ImageViewerModal-3811bdd5.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/Recipe-796f9114.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/recommend_linked_publication_modal-f03dfb3f.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/AuditionPlayer-5637d633.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/SectionLogo-44ead5a1.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/TabBar-7430851f.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/comments_page-83ec46a1.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/RewardBox-5297e7a9.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/Field-6e53593d.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/Radio-736388f9.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/Logo-8a4d9999.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/FilePicker-011ef325.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/setup_all_podcasts-9189be33.css\" />\n            \n                <link type=\"text/css\" rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/assets/CookieConsentFooter-0d9aca19.css\" />\n            \n        \n\n        \n        \n        \n        \n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0, viewport-fit=cover\" />\n        <meta name=\"author\" content=\"Arvind Narayanan\" />\n        <meta property=\"og:url\" content=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities\" />\n        <title data-preact-helmet>AI existential risk probabilities are too unreliable to inform policy</title>\n        <meta data-preact-helmet name=\"theme-color\" content=\"#ffffff\"><meta data-preact-helmet property=\"og:type\" content=\"article\"><meta data-preact-helmet property=\"og:title\" content=\"AI existential risk probabilities are too unreliable to inform policy\"><meta data-preact-helmet name=\"twitter:title\" content=\"AI existential risk probabilities are too unreliable to inform policy\"><meta data-preact-helmet name=\"description\" content=\"How speculation gets laundered through pseudo-quantification\"><meta data-preact-helmet property=\"og:description\" content=\"How speculation gets laundered through pseudo-quantification\"><meta data-preact-helmet name=\"twitter:description\" content=\"How speculation gets laundered through pseudo-quantification\"><meta data-preact-helmet property=\"og:image\" content=\"https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6418811e-f1c2-4127-b6dd-f243c4f458ef_1398x918.png\"><meta data-preact-helmet name=\"twitter:image\" content=\"https://substackcdn.com/image/fetch/f_auto,q_auto:best,fl_progressive:steep/https%3A%2F%2Faisnakeoil.substack.com%2Fapi%2Fv1%2Fpost_preview%2F147019742%2Ftwitter.jpg%3Fversion%3D4\"><meta data-preact-helmet name=\"twitter:card\" content=\"summary_large_image\">\n        <link rel=\"canonical\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities\" />\n\n        \n\n        \n\n        \n            \n                <link rel=\"shortcut icon\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Ffavicon.ico\">\n            \n        \n            \n                <link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Ffavicon-16x16.png\">\n            \n        \n            \n                <link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Ffavicon-32x32.png\">\n            \n        \n            \n                <link rel=\"icon\" type=\"image/png\" sizes=\"48x48\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Ffavicon-48x48.png\">\n            \n        \n            \n                <link rel=\"apple-touch-icon\" sizes=\"57x57\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Fapple-touch-icon-57x57.png\">\n            \n        \n            \n                <link rel=\"apple-touch-icon\" sizes=\"60x60\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Fapple-touch-icon-60x60.png\">\n            \n        \n            \n                <link rel=\"apple-touch-icon\" sizes=\"72x72\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Fapple-touch-icon-72x72.png\">\n            \n        \n            \n                <link rel=\"apple-touch-icon\" sizes=\"76x76\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Fapple-touch-icon-76x76.png\">\n            \n        \n            \n                <link rel=\"apple-touch-icon\" sizes=\"114x114\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Fapple-touch-icon-114x114.png\">\n            \n        \n            \n                <link rel=\"apple-touch-icon\" sizes=\"120x120\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Fapple-touch-icon-120x120.png\">\n            \n        \n            \n                <link rel=\"apple-touch-icon\" sizes=\"144x144\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Fapple-touch-icon-144x144.png\">\n            \n        \n            \n                <link rel=\"apple-touch-icon\" sizes=\"152x152\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Fapple-touch-icon-152x152.png\">\n            \n        \n            \n                <link rel=\"apple-touch-icon\" sizes=\"167x167\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Fapple-touch-icon-167x167.png\">\n            \n        \n            \n                <link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Fapple-touch-icon-180x180.png\">\n            \n        \n            \n                <link rel=\"apple-touch-icon\" sizes=\"1024x1024\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9e2879b-458a-437a-8501-809e603287e5%2Fapple-touch-icon-1024x1024.png\">\n            \n        \n            \n        \n            \n        \n            \n        \n\n        \n\n        \n            <link rel=\"alternate\" type=\"application/rss+xml\" href=\"/feed\" title=\"AI Snake Oil\"/>\n        \n\n        \n        \n          <style>\n            @font-face{font-family:'Lora';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QI8MX1D_JOuMw_hLdO6T2wV9KnW-MoFoqJ2nPWc3ZyhTjcV.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Lora';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QI8MX1D_JOuMw_hLdO6T2wV9KnW-MoFoqt2nPWc3ZyhTjcV.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Lora';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QI8MX1D_JOuMw_hLdO6T2wV9KnW-MoFoqB2nPWc3ZyhTjcV.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Lora';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QI8MX1D_JOuMw_hLdO6T2wV9KnW-MoFoqF2nPWc3ZyhTjcV.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Lora';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QI8MX1D_JOuMw_hLdO6T2wV9KnW-MoFoq92nPWc3ZyhTg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwf7I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMw77I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwX7I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwT7I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Lora';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwr7I_FMl_E.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Lora';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwf7I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Lora';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMw77I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Lora';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwX7I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Lora';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwT7I_FMl_GW8g.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Lora';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/lora/v32/0QIvMX1D_JOuMwr7I_FMl_E.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}\n            \n          </style>\n        \n\n        <style>:root{--color_theme_bg_pop:#fd6752;--background_pop:#fd6752;--color_theme_bg_web:#ffffff;--cover_bg_color:#ffffff;--background_pop_darken:#fd5139;--print_on_pop:#ffffff;--color_theme_bg_pop_darken:#fd5139;--color_theme_print_on_pop:#ffffff;--border_subtle:rgba(204, 204, 204, 0.5);--background_subtle:rgba(255, 232, 229, 0.4);--print_pop:#fd6752;--color_theme_accent:#fd6752;--cover_print_primary:#363737;--cover_print_secondary:#757575;--cover_print_tertiary:#b6b6b6;--cover_border_color:#fd6752;--font_family_headings_preset:'SF Pro Display', -apple-system, system-ui, BlinkMacSystemFont, 'Inter', 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol';--font_weight_headings_preset:900;--font_family_body_preset:Lora,sans-serif;--font_weight_body_preset:400;--font_size_body_offset:1px;--font_preset_heading:heavy_sans;--font_preset_body:fancy_serif;--home_hero:newspaper;--home_posts:custom;--web_bg_color:#ffffff;--background_contrast_1:#f0f0f0;--color_theme_bg_contrast_1:#f0f0f0;--background_contrast_2:#dddddd;--color_theme_bg_contrast_2:#dddddd;--background_contrast_3:#b7b7b7;--color_theme_bg_contrast_3:#b7b7b7;--background_contrast_4:#929292;--color_theme_bg_contrast_4:#929292;--background_contrast_5:#515151;--color_theme_bg_contrast_5:#515151;--color_theme_detail:#e6e6e6;--background_contrast_pop:rgba(253, 103, 82, 0.4);--color_theme_bg_contrast_pop:rgba(253, 103, 82, 0.4);--input_background:#ffffff;--cover_input_background:#ffffff;--tooltip_background:#191919;--web_bg_color_h:0;--web_bg_color_s:0%;--web_bg_color_l:100%;--print_on_web_bg_color:#363737;--print_secondary_on_web_bg_color:#868787;--selected_comment_background_color:#fdf9f3;--background_pop_rgb:253, 103, 82;--background_pop_rgb_pc:253 103 82;--color_theme_bg_pop_rgb:253, 103, 82;--color_theme_bg_pop_rgb_pc:253 103 82;--color_theme_accent_rgb:253, 103, 82;--color_theme_accent_rgb_pc:253 103 82;}</style>\n\n        \n            <link rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/theme/main.548d4784e25e1f055dfd.css\" />\n        \n            <link rel=\"stylesheet\" href=\"https://substackcdn.com/bundle/theme/color_links.33eb7c3bd37d78bc2dc3.css\" />\n        \n\n        <style></style>\n\n        \n\n        \n\n        \n    </head>\n\n    <body class=\"\">\n        \n\n        \n\n        <div id=\"entry\">\n            <div id=\"main\" class=\"main typography use-theme-bg\"><div data-testid=\"navbar\" class=\"main-menu animated with-wordmark\"><div class=\"main-menu-content\"><div class=\"topbar\"><div class=\"topbar-content\"><div class=\"navbar-logo-container\"><a href=\"/\" native><img src=\"https://substackcdn.com/image/fetch/w_96,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png\" class=\"navbar-logo\" /></a></div><h1 class=\"navbar-title left-align loading\"><a href=\"/\" native class=\"navbar-title-link\"><img src=\"https://substackcdn.com/image/fetch/e_trim:10:white/e_trim:10:transparent/h_72,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74fcca0d-74fc-4658-8b78-894d92a24750_1344x256.png\" alt=\"AI Snake Oil\" class=\"navbar-logo-wordmark\" /></a></h1><div class=\"navbar-buttons\"><div class=\"pencraft pc-display-contents pc-reset _pubTheme_ipix0_1\"><div class=\"pencraft pc-display-flex pc-gap-4 pc-reset\"><span><button tabIndex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_md_1oht6_127 _priority_tertiary_1oht6_69\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-search \"><circle cx=\"11\" cy=\"11\" r=\"8\"></circle><path d=\"m21 21-4.3-4.3\"></path></svg></button></span><button tabIndex=\"0\" type=\"button\" aria-label=\"Share Publication\" id=\"trigger4153\" aria-expanded=\"false\" aria-haspopup=\"dialog\" aria-controls=\"dialog4154\" ariaLabel=\"View more\" class=\"pencraft pc-reset pencraft _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_md_1oht6_127 _priority_tertiary_1oht6_69\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-share \"><path d=\"M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8\"></path><polyline points=\"16 6 12 2 8 6\"></polyline><line x1=\"12\" x2=\"12\" y1=\"2\" y2=\"15\"></line></svg></button></div><button tabIndex=\"0\" type=\"button\" data-testid=\"noncontributor-cta-button\" class=\"pencraft pc-reset pencraft _buttonBase_1oht6_1 _button_1oht6_1 _buttonNew_1oht6_83 _button2_1oht6_117 _priority_primary_1oht6_57 _size_md_1oht6_127\">Subscribe</button><button tabIndex=\"0\" type=\"button\" native data-href=\"https://substack.com/sign-in?redirect=%2Fp%2Fai-existential-risk-probabilities&amp;for_pub=aisnakeoil\" class=\"pencraft pc-reset pencraft _buttonBase_1oht6_1 _button_1oht6_1 _buttonNew_1oht6_83 _button2_1oht6_117 _priority_tertiary_1oht6_69 _size_md_1oht6_127\">Sign in</button></div></div></div></div></div><div class=\"topbar-spacer\"></div></div><div><script type=\"application/ld+json\">{\"@context\":\"https://schema.org\",\"@type\":\"NewsArticle\",\"url\":\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities\",\"mainEntityOfPage\":\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities\",\"headline\":\"AI existential risk probabilities are too unreliable to inform policy\",\"description\":\"How speculation gets laundered through pseudo-quantification\",\"image\":[{\"@type\":\"ImageObject\",\"url\":\"https://substack-post-media.s3.amazonaws.com/public/images/6418811e-f1c2-4127-b6dd-f243c4f458ef_1398x918.png\"}],\"datePublished\":\"2024-07-26T11:29:25+00:00\",\"dateModified\":\"2024-07-26T11:29:25+00:00\",\"isAccessibleForFree\":true,\"author\":[{\"@type\":\"Person\",\"name\":\"Arvind Narayanan\",\"url\":\"https://substack.com/@arvindnarayanan\",\"description\":null,\"identifier\":\"user:19265909\",\"image\":{\"@type\":\"ImageObject\",\"contentUrl\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F149ffaff-ad45-4b76-9667-bbd39d3a3d23_1795x1795.jpeg\",\"thumbnailUrl\":\"https://substackcdn.com/image/fetch/w_128,h_128,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F149ffaff-ad45-4b76-9667-bbd39d3a3d23_1795x1795.jpeg\"}},{\"@type\":\"Person\",\"name\":\"Sayash Kapoor\",\"url\":\"https://substack.com/@sayash\",\"description\":\"CS PhD candidate at Princeton. I study the societal impact of AI. Currently writing a book on AI Snake Oil: http://aisnakeoil.com \",\"identifier\":\"user:891603\",\"image\":{\"@type\":\"ImageObject\",\"contentUrl\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f87ce8-8dbc-468f-8f8b-9fbf430e323c_976x974.png\",\"thumbnailUrl\":\"https://substackcdn.com/image/fetch/w_128,h_128,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f87ce8-8dbc-468f-8f8b-9fbf430e323c_976x974.png\"}}],\"publisher\":{\"@type\":\"Organization\",\"name\":\"AI Snake Oil\",\"url\":\"https://www.aisnakeoil.com\",\"description\":\"What Artificial Intelligence Can Do, What It Can\\u2019t, and How to Tell the Difference\",\"interactionStatistic\":{\"@type\":\"InteractionCounter\",\"name\":\"Subscribers\",\"interactionType\":\"https://schema.org/SubscribeAction\",\"userInteractionCount\":10000},\"identifier\":\"pub:1008003\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png\",\"contentUrl\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png\",\"thumbnailUrl\":\"https://substackcdn.com/image/fetch/w_128,h_128,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png\"},\"image\":{\"@type\":\"ImageObject\",\"url\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png\",\"contentUrl\":\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png\",\"thumbnailUrl\":\"https://substackcdn.com/image/fetch/w_128,h_128,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png\"}}}</script><div class=\"single-post-container\"><div class=\"container\"><div class=\"single-post\"><article class=\"typography newsletter-post post\"><div inert role=\"dialog\" class=\"modal typography out gone share-dialog popup\"><div class=\"modal-table\"><div class=\"modal-row\"><div class=\"modal-cell modal-content no-fullscreen\"><div class=\"container\"><button tabIndex=\"0\" type=\"button\" data-testid=\"close-modal\" class=\"pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_md_1oht6_127 _priority_tertiary_1oht6_69\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"secondary\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-x \"><path d=\"M18 6 6 18\"></path><path d=\"m6 6 12 12\"></path></svg></button><div class=\"share-dialog-title\">Share this post</div><div class=\"pencraft pc-display-flex pc-flexDirection-column pc-gap-32 pc-paddingLeft-24 pc-paddingRight-24 pc-paddingTop-32 pc-paddingBottom-48 pc-reset\"><div class=\"pencraft pc-display-flex pc-padding-8 pc-reset _border-detail_17s6c_25 pc-borderRadius-md social-preview-box post\"><div class=\"social-image-box\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_120,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6418811e-f1c2-4127-b6dd-f243c4f458ef_1398x918.png\" /><img src=\"https://substackcdn.com/image/fetch/w_120,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6418811e-f1c2-4127-b6dd-f243c4f458ef_1398x918.png\" sizes=\"100vw\" alt width=\"120\" loading=\"lazy\" class=\"_img_16u6n_1 social-image pencraft pc-reset\" /></picture></div><div class=\"pencraft pc-display-flex pc-flexDirection-column pc-paddingTop-8 pc-paddingBottom-8 pc-paddingLeft-12 pc-reset\"><h4 class=\"pencraft pc-reset _line-height-24_3axfk_98 _font-display_3axfk_118 _size-20_3axfk_70 _weight-bold_3axfk_168 _reset_3axfk_1\">AI existential risk probabilities are too unreliable to inform policy</h4><div class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">www.aisnakeoil.com</div></div></div><div class=\"pencraft pc-display-flex pc-gap-8 pc-justifyContent-space-between pc-reset share-dialog-buttons-wrapper\"><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"20\" height=\"16\" viewBox=\"0 0 20 16\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M12.1303 0.000379039C10.9833 -0.00959082 9.87819 0.431464 9.05309 1.22855L9.04556 1.23593L7.79145 2.48303C7.50587 2.767 7.50453 3.22877 7.78844 3.51441C8.07235 3.80004 8.53401 3.80139 8.81959 3.51741L10.0698 2.27423C10.6194 1.74503 11.3546 1.45229 12.1177 1.45892C12.8824 1.46556 13.6139 1.77236 14.1546 2.31323C14.6954 2.8541 15.0021 3.58577 15.0087 4.35065C15.0154 5.11353 14.7229 5.84857 14.1943 6.39829L12.0116 8.58145L12.0115 8.58155C11.7159 8.87739 11.36 9.10617 10.9682 9.25237C10.5764 9.39857 10.1577 9.45878 9.74051 9.42889C9.32337 9.39901 8.91752 9.27975 8.55051 9.07918C8.1835 8.87862 7.8639 8.60146 7.6134 8.26649C7.3722 7.94396 6.91526 7.87807 6.5928 8.11933C6.27034 8.36059 6.20447 8.81763 6.44567 9.14016C6.82142 9.64261 7.30082 10.0584 7.85134 10.3592C8.40186 10.66 9.01062 10.8389 9.63634 10.8838C10.2621 10.9286 10.8901 10.8383 11.4779 10.619C12.0656 10.3997 12.5994 10.0565 13.0429 9.61274L15.2302 7.42494L15.2391 7.4159C16.036 6.59062 16.4769 5.48529 16.467 4.33797C16.457 3.19066 15.9969 2.09316 15.1858 1.28185C14.3746 0.470545 13.2774 0.0103489 12.1303 0.000379039ZM7.29806 5.11625C6.67234 5.07142 6.0443 5.16173 5.45654 5.38103C4.86882 5.60031 4.33502 5.94355 3.89153 6.38727L1.70423 8.57506L1.69534 8.5841C0.898438 9.40939 0.457483 10.5147 0.467451 11.662C0.477418 12.8094 0.937512 13.9069 1.74864 14.7182C2.55976 15.5295 3.65701 15.9897 4.80407 15.9996C5.95113 16.0096 7.05622 15.5685 7.88132 14.7715L7.89035 14.7626L9.13717 13.5155C9.42192 13.2307 9.42192 12.7689 9.13717 12.4841C8.85243 12.1993 8.39077 12.1993 8.10602 12.4841L6.86392 13.7265C6.31432 14.2552 5.57945 14.5477 4.81675 14.5411C4.05204 14.5344 3.32054 14.2276 2.77979 13.6868C2.23904 13.1459 1.93231 12.4142 1.92566 11.6494C1.91904 10.8865 2.21146 10.1514 2.74011 9.60172L4.92287 7.41846C5.21854 7.12262 5.57437 6.89384 5.96621 6.74763C6.35805 6.60143 6.77674 6.54123 7.19389 6.57111C7.61104 6.601 8.01688 6.72026 8.38389 6.92082C8.75091 7.12138 9.0705 7.39855 9.32101 7.73352C9.56221 8.05605 10.0191 8.12194 10.3416 7.88068C10.6641 7.63942 10.7299 7.18238 10.4887 6.85985C10.113 6.3574 9.63359 5.94165 9.08307 5.64081C8.53255 5.33997 7.92378 5.16107 7.29806 5.11625Z\"></path></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Copy link</div></button><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"16\" height=\"17\" viewBox=\"0 0 16 17\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M10.6543 1.38723C10.3533 0.960814 9.95383 0.61341 9.48976 0.374567C9.02902 0.137956 8.51908 0.0130716 8.00115 0.0100098C7.86087 0.0101844 7.72354 0.0502687 7.60519 0.125581C7.48684 0.200893 7.39237 0.308324 7.3328 0.435326L5.00368 5.67077H3.029C2.72335 5.66964 2.42059 5.73003 2.13876 5.84833C1.85692 5.96663 1.60177 6.14043 1.38849 6.35938C1.16707 6.57502 0.991841 6.83346 0.873459 7.11897C0.755078 7.40447 0.696022 7.71108 0.699885 8.02014V13.691C0.699885 14.3087 0.945273 14.9012 1.38207 15.338C1.81886 15.7747 2.41128 16.0201 3.029 16.0201H13.348C13.8951 16.021 14.425 15.8283 14.8438 15.4762C15.2626 15.1241 15.5434 14.6352 15.6366 14.0961L16.6493 8.4252C16.7252 8.09192 16.7252 7.74582 16.6493 7.41254C16.566 7.08205 16.4104 6.7742 16.1936 6.51128C15.9746 6.25 15.7017 6.03926 15.3936 5.89355C15.0762 5.7467 14.7306 5.67068 14.3809 5.67077H10.5328L11.0391 4.37457C11.2397 3.88784 11.3162 3.35894 11.2619 2.83533C11.1853 2.30894 10.9763 1.81065 10.6543 1.38723ZM4.75052 14.5518H3.029C2.91049 14.5525 2.79303 14.5296 2.68349 14.4844C2.57394 14.4392 2.47452 14.3726 2.39102 14.2885C2.23609 14.1199 2.14945 13.8997 2.14799 13.6708V8.02014C2.14913 7.901 2.17389 7.78328 2.22082 7.67377C2.26775 7.56427 2.33592 7.46515 2.4214 7.38216C2.50369 7.29576 2.60267 7.22698 2.71233 7.17998C2.822 7.13298 2.94007 7.10874 3.05938 7.10874H4.7809L4.75052 14.5518ZM10.6746 7.05811H14.3809C14.5145 7.05821 14.6462 7.08942 14.7657 7.14925C14.8875 7.20532 14.9948 7.28845 15.0796 7.39229C15.1675 7.49052 15.2301 7.60871 15.2619 7.73659C15.2922 7.8665 15.2922 8.00162 15.2619 8.13153L14.2493 13.8024C14.2087 14.017 14.094 14.2106 13.9252 14.3492C13.7619 14.4812 13.558 14.5528 13.348 14.5518H6.19862V6.45052L8.43659 1.38723H8.52773C8.9042 1.50037 9.23304 1.73413 9.4636 2.05252C9.69416 2.37092 9.81365 2.75627 9.80368 3.14925C9.8181 3.39741 9.78015 3.64583 9.69229 3.87836L9.23659 5.04292C9.15397 5.273 9.12623 5.51921 9.15558 5.76191C9.1877 6.00427 9.27425 6.23623 9.40875 6.44039C9.5535 6.6376 9.74028 6.80017 9.95558 6.91634C10.1774 7.03206 10.4244 7.0912 10.6746 7.08849V7.05811Z\"></path></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Facebook</div></button><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"21\" height=\"16\" viewBox=\"0 0 21 16\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M2.22192 2.20503C2.36754 1.77115 2.78269 1.45455 3.26639 1.45455H17.9332C18.4169 1.45455 18.8321 1.77118 18.9777 2.2051L10.5999 8.02107L2.22192 2.20503ZM2.16639 3.94198V13.4545C2.16639 14.0529 2.66307 14.5455 3.26639 14.5455H17.9332C18.5365 14.5455 19.0332 14.0529 19.0332 13.4545V3.94206L11.0204 9.50462C10.7679 9.67991 10.4318 9.67991 10.1793 9.50462L2.16639 3.94198ZM20.4999 2.55809V13.4545C20.4999 14.8562 19.3465 16 17.9332 16H3.26639C1.85304 16 0.699707 14.8562 0.699707 13.4545V2.54545C0.699707 1.14379 1.85304 0 3.26639 0H17.9332C19.3407 0 20.4904 1.13441 20.4998 2.52818C20.5 2.53816 20.5001 2.54813 20.4999 2.55809Z\"></path></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Email</div></button><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M6.785 1.92766C5.45134 1.57031 4.08049 2.36176 3.72314 3.69543L0.444815 15.9303C0.0874636 17.264 0.878901 18.6348 2.21255 18.9922L5.37495 19.8396V7.66664C5.37495 6.40099 6.40096 5.37498 7.66661 5.37498H19.4723C19.3299 5.30548 19.1788 5.24858 19.0201 5.20604L6.785 1.92766Z\" stroke=\"none\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M8.44161 7.4C7.86632 7.4 7.39995 7.86637 7.39995 8.44167V22.1081C7.39995 22.6834 7.86631 23.1498 8.4416 23.1498L22.1083 23.15C22.6836 23.15 23.1499 22.6836 23.1499 22.1083V8.44167C23.1499 7.86637 22.6836 7.4 22.1083 7.4H8.44161ZM10.3999 9.65C9.84766 9.65 9.39995 10.0977 9.39995 10.65C9.39995 11.2023 9.84766 11.65 10.3999 11.65H18.3999C18.9522 11.65 19.3999 11.2023 19.3999 10.65C19.3999 10.0977 18.9522 9.65 18.3999 9.65H10.3999ZM10.3999 14.15C9.84766 14.15 9.39995 14.5977 9.39995 15.15C9.39995 15.7023 9.84766 16.15 10.3999 16.15H15.3999C15.9522 16.15 16.3999 15.7023 16.3999 15.15C16.3999 14.5977 15.9522 14.15 15.3999 14.15H10.3999Z\" stroke=\"none\"></path></g></svg></div><div class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Note</div></button><button tabIndex=\"0\" id=\"trigger4155\" aria-expanded=\"false\" aria-haspopup=\"dialog\" aria-controls=\"dialog4156\" ariaLabel=\"View more\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"100\" height=\"100\" viewBox=\"0 0 100 100\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><circle cx=\"23\" cy=\"50\" r=\"9\"></circle><circle cx=\"50\" cy=\"50\" r=\"9\"></circle><circle cx=\"77\" cy=\"50\" r=\"9\"></circle></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Other</div></button></div></div></div></div></div></div></div><div class=\"post-header\"><h1 class=\"post-title unpublished\">AI existential risk probabilities are too unreliable to inform policy</h1><h3 class=\"subtitle\">How speculation gets laundered through pseudo-quantification</h3><div class=\"pencraft pc-display-flex pc-flexDirection-column pc-paddingBottom-16 pc-reset\"><div class=\"pencraft pc-display-flex pc-flexDirection-column pc-paddingTop-16 pc-paddingBottom-16 pc-reset\"><div class=\"pencraft pc-display-flex pc-gap-12 pc-alignItems-center pc-reset byline-wrapper\"><div class=\"pencraft pc-display-flex pc-reset\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-alignItems-center _flexGrow_17s6c_230 pc-reset facepile\"><div class=\"pencraft pc-display-flex pc-alignItems-center pc-reset\"><div class=\"pencraft pc-display-flex pc-alignItems-center pc-reset _faces_deldb_7 _size-40_deldb_23\"><div class=\"profile-hover-card-target _profileHoverCardTarget_c9bh7_50\"><a href=\"https://substack.com/profile/19265909-arvind-narayanan\" target=\"_blank\" rel=\"noopener\" class=\"pencraft pc-display-flex _flexAuto_17s6c_233 pc-reset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_80,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F149ffaff-ad45-4b76-9667-bbd39d3a3d23_1795x1795.jpeg\" /><img src=\"https://substackcdn.com/image/fetch/w_80,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F149ffaff-ad45-4b76-9667-bbd39d3a3d23_1795x1795.jpeg\" sizes=\"100vw\" alt width=\"80\" style=\"z-index: 2;\" class=\"_img_16u6n_1 facepile-face _face_deldb_7 _size-40_deldb_23 _first_deldb_75 pencraft pc-reset\" /></picture></a></div><div class=\"profile-hover-card-target _profileHoverCardTarget_c9bh7_50\"><a href=\"https://substack.com/profile/891603-sayash-kapoor\" target=\"_blank\" rel=\"noopener\" class=\"pencraft pc-display-flex _flexAuto_17s6c_233 pc-reset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_80,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f87ce8-8dbc-468f-8f8b-9fbf430e323c_976x974.png\" /><img src=\"https://substackcdn.com/image/fetch/w_80,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f87ce8-8dbc-468f-8f8b-9fbf430e323c_976x974.png\" sizes=\"100vw\" alt width=\"80\" style=\"z-index: 1;\" class=\"_img_16u6n_1 facepile-face _face_deldb_7 _size-40_deldb_23 _last_deldb_79 pencraft pc-reset\" /></picture></a></div></div></div></div></div><div class=\"pencraft pc-display-flex pc-flexDirection-column pc-reset\"><div class=\"pencraft pc-reset _color-pub-primary-text_3axfk_204 _line-height-20_3axfk_95 _font-meta_3axfk_131 _size-11_3axfk_35 _weight-medium_3axfk_162 _transform-uppercase_3axfk_242 _reset_3axfk_1 _meta_3axfk_442\"><div class=\"profile-hover-card-target _profileHoverCardTarget_c9bh7_50\"><a href=\"https://substack.com/@arvindnarayanan\" class=\"pencraft pc-reset _decoration-hover-underline_3axfk_298 _reset_3axfk_1\">Arvind Narayanan</a></div> and <div class=\"profile-hover-card-target _profileHoverCardTarget_c9bh7_50\"><a href=\"https://substack.com/@sayash\" class=\"pencraft pc-reset _decoration-hover-underline_3axfk_298 _reset_3axfk_1\">Sayash Kapoor</a></div></div><div class=\"pencraft pc-display-flex pc-gap-4 pc-reset\"><div class=\"pencraft pc-reset _color-pub-secondary-text_3axfk_207 _line-height-20_3axfk_95 _font-meta_3axfk_131 _size-11_3axfk_35 _weight-medium_3axfk_162 _transform-uppercase_3axfk_242 _reset_3axfk_1 _meta_3axfk_442\">Jul 26, 2024</div></div></div></div></div><div class=\"pencraft pc-display-flex pc-gap-16 pc-paddingTop-16 pc-paddingBottom-16 pc-justifyContent-space-between pc-alignItems-center _flexGrow_17s6c_230 pc-reset _border-top-detail-themed_17s6c_47 _border-bottom-detail-themed_17s6c_50 post-ufi\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"like-button-container post-ufi-button style-button\"><a role=\"button\" class=\"post-ufi-button style-button has-label with-border\"><svg role=\"img\" style=\"height: 20px; width: 20px;\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"#000000\" stroke-width=\"2\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\" class=\"icon\"><g><title></title><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-heart \"><path d=\"M19 14c1.49-1.46 3-3.21 3-5.5A5.5 5.5 0 0 0 16.5 3c-1.76 0-3 .5-4.5 2-1.5-1.5-2.74-2-4.5-2A5.5 5.5 0 0 0 2 8.5c0 2.3 1.5 4.05 3 5.5l7 7Z\"></path></svg></g></svg><div class=\"label\">100</div></a><div inert role=\"dialog\" class=\"modal typography out gone share-dialog popup\"><div class=\"modal-table\"><div class=\"modal-row\"><div class=\"modal-cell modal-content no-fullscreen\"><div class=\"container\"><button tabIndex=\"0\" type=\"button\" data-testid=\"close-modal\" class=\"pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_md_1oht6_127 _priority_tertiary_1oht6_69\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"secondary\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-x \"><path d=\"M18 6 6 18\"></path><path d=\"m6 6 12 12\"></path></svg></button><div class=\"share-dialog-title\">Share this post</div><div class=\"pencraft pc-display-flex pc-flexDirection-column pc-gap-32 pc-paddingLeft-24 pc-paddingRight-24 pc-paddingTop-32 pc-paddingBottom-48 pc-reset\"><div class=\"pencraft pc-display-flex pc-padding-8 pc-reset _border-detail_17s6c_25 pc-borderRadius-md social-preview-box post\"><div class=\"social-image-box\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_120,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6418811e-f1c2-4127-b6dd-f243c4f458ef_1398x918.png\" /><img src=\"https://substackcdn.com/image/fetch/w_120,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6418811e-f1c2-4127-b6dd-f243c4f458ef_1398x918.png\" sizes=\"100vw\" alt width=\"120\" loading=\"lazy\" class=\"_img_16u6n_1 social-image pencraft pc-reset\" /></picture></div><div class=\"pencraft pc-display-flex pc-flexDirection-column pc-paddingTop-8 pc-paddingBottom-8 pc-paddingLeft-12 pc-reset\"><h4 class=\"pencraft pc-reset _line-height-24_3axfk_98 _font-display_3axfk_118 _size-20_3axfk_70 _weight-bold_3axfk_168 _reset_3axfk_1\">AI existential risk probabilities are too unreliable to inform policy</h4><div class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">www.aisnakeoil.com</div></div></div><div class=\"pencraft pc-display-flex pc-gap-8 pc-justifyContent-space-between pc-reset share-dialog-buttons-wrapper\"><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"20\" height=\"16\" viewBox=\"0 0 20 16\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M12.1303 0.000379039C10.9833 -0.00959082 9.87819 0.431464 9.05309 1.22855L9.04556 1.23593L7.79145 2.48303C7.50587 2.767 7.50453 3.22877 7.78844 3.51441C8.07235 3.80004 8.53401 3.80139 8.81959 3.51741L10.0698 2.27423C10.6194 1.74503 11.3546 1.45229 12.1177 1.45892C12.8824 1.46556 13.6139 1.77236 14.1546 2.31323C14.6954 2.8541 15.0021 3.58577 15.0087 4.35065C15.0154 5.11353 14.7229 5.84857 14.1943 6.39829L12.0116 8.58145L12.0115 8.58155C11.7159 8.87739 11.36 9.10617 10.9682 9.25237C10.5764 9.39857 10.1577 9.45878 9.74051 9.42889C9.32337 9.39901 8.91752 9.27975 8.55051 9.07918C8.1835 8.87862 7.8639 8.60146 7.6134 8.26649C7.3722 7.94396 6.91526 7.87807 6.5928 8.11933C6.27034 8.36059 6.20447 8.81763 6.44567 9.14016C6.82142 9.64261 7.30082 10.0584 7.85134 10.3592C8.40186 10.66 9.01062 10.8389 9.63634 10.8838C10.2621 10.9286 10.8901 10.8383 11.4779 10.619C12.0656 10.3997 12.5994 10.0565 13.0429 9.61274L15.2302 7.42494L15.2391 7.4159C16.036 6.59062 16.4769 5.48529 16.467 4.33797C16.457 3.19066 15.9969 2.09316 15.1858 1.28185C14.3746 0.470545 13.2774 0.0103489 12.1303 0.000379039ZM7.29806 5.11625C6.67234 5.07142 6.0443 5.16173 5.45654 5.38103C4.86882 5.60031 4.33502 5.94355 3.89153 6.38727L1.70423 8.57506L1.69534 8.5841C0.898438 9.40939 0.457483 10.5147 0.467451 11.662C0.477418 12.8094 0.937512 13.9069 1.74864 14.7182C2.55976 15.5295 3.65701 15.9897 4.80407 15.9996C5.95113 16.0096 7.05622 15.5685 7.88132 14.7715L7.89035 14.7626L9.13717 13.5155C9.42192 13.2307 9.42192 12.7689 9.13717 12.4841C8.85243 12.1993 8.39077 12.1993 8.10602 12.4841L6.86392 13.7265C6.31432 14.2552 5.57945 14.5477 4.81675 14.5411C4.05204 14.5344 3.32054 14.2276 2.77979 13.6868C2.23904 13.1459 1.93231 12.4142 1.92566 11.6494C1.91904 10.8865 2.21146 10.1514 2.74011 9.60172L4.92287 7.41846C5.21854 7.12262 5.57437 6.89384 5.96621 6.74763C6.35805 6.60143 6.77674 6.54123 7.19389 6.57111C7.61104 6.601 8.01688 6.72026 8.38389 6.92082C8.75091 7.12138 9.0705 7.39855 9.32101 7.73352C9.56221 8.05605 10.0191 8.12194 10.3416 7.88068C10.6641 7.63942 10.7299 7.18238 10.4887 6.85985C10.113 6.3574 9.63359 5.94165 9.08307 5.64081C8.53255 5.33997 7.92378 5.16107 7.29806 5.11625Z\"></path></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Copy link</div></button><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"16\" height=\"17\" viewBox=\"0 0 16 17\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M10.6543 1.38723C10.3533 0.960814 9.95383 0.61341 9.48976 0.374567C9.02902 0.137956 8.51908 0.0130716 8.00115 0.0100098C7.86087 0.0101844 7.72354 0.0502687 7.60519 0.125581C7.48684 0.200893 7.39237 0.308324 7.3328 0.435326L5.00368 5.67077H3.029C2.72335 5.66964 2.42059 5.73003 2.13876 5.84833C1.85692 5.96663 1.60177 6.14043 1.38849 6.35938C1.16707 6.57502 0.991841 6.83346 0.873459 7.11897C0.755078 7.40447 0.696022 7.71108 0.699885 8.02014V13.691C0.699885 14.3087 0.945273 14.9012 1.38207 15.338C1.81886 15.7747 2.41128 16.0201 3.029 16.0201H13.348C13.8951 16.021 14.425 15.8283 14.8438 15.4762C15.2626 15.1241 15.5434 14.6352 15.6366 14.0961L16.6493 8.4252C16.7252 8.09192 16.7252 7.74582 16.6493 7.41254C16.566 7.08205 16.4104 6.7742 16.1936 6.51128C15.9746 6.25 15.7017 6.03926 15.3936 5.89355C15.0762 5.7467 14.7306 5.67068 14.3809 5.67077H10.5328L11.0391 4.37457C11.2397 3.88784 11.3162 3.35894 11.2619 2.83533C11.1853 2.30894 10.9763 1.81065 10.6543 1.38723ZM4.75052 14.5518H3.029C2.91049 14.5525 2.79303 14.5296 2.68349 14.4844C2.57394 14.4392 2.47452 14.3726 2.39102 14.2885C2.23609 14.1199 2.14945 13.8997 2.14799 13.6708V8.02014C2.14913 7.901 2.17389 7.78328 2.22082 7.67377C2.26775 7.56427 2.33592 7.46515 2.4214 7.38216C2.50369 7.29576 2.60267 7.22698 2.71233 7.17998C2.822 7.13298 2.94007 7.10874 3.05938 7.10874H4.7809L4.75052 14.5518ZM10.6746 7.05811H14.3809C14.5145 7.05821 14.6462 7.08942 14.7657 7.14925C14.8875 7.20532 14.9948 7.28845 15.0796 7.39229C15.1675 7.49052 15.2301 7.60871 15.2619 7.73659C15.2922 7.8665 15.2922 8.00162 15.2619 8.13153L14.2493 13.8024C14.2087 14.017 14.094 14.2106 13.9252 14.3492C13.7619 14.4812 13.558 14.5528 13.348 14.5518H6.19862V6.45052L8.43659 1.38723H8.52773C8.9042 1.50037 9.23304 1.73413 9.4636 2.05252C9.69416 2.37092 9.81365 2.75627 9.80368 3.14925C9.8181 3.39741 9.78015 3.64583 9.69229 3.87836L9.23659 5.04292C9.15397 5.273 9.12623 5.51921 9.15558 5.76191C9.1877 6.00427 9.27425 6.23623 9.40875 6.44039C9.5535 6.6376 9.74028 6.80017 9.95558 6.91634C10.1774 7.03206 10.4244 7.0912 10.6746 7.08849V7.05811Z\"></path></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Facebook</div></button><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"21\" height=\"16\" viewBox=\"0 0 21 16\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M2.22192 2.20503C2.36754 1.77115 2.78269 1.45455 3.26639 1.45455H17.9332C18.4169 1.45455 18.8321 1.77118 18.9777 2.2051L10.5999 8.02107L2.22192 2.20503ZM2.16639 3.94198V13.4545C2.16639 14.0529 2.66307 14.5455 3.26639 14.5455H17.9332C18.5365 14.5455 19.0332 14.0529 19.0332 13.4545V3.94206L11.0204 9.50462C10.7679 9.67991 10.4318 9.67991 10.1793 9.50462L2.16639 3.94198ZM20.4999 2.55809V13.4545C20.4999 14.8562 19.3465 16 17.9332 16H3.26639C1.85304 16 0.699707 14.8562 0.699707 13.4545V2.54545C0.699707 1.14379 1.85304 0 3.26639 0H17.9332C19.3407 0 20.4904 1.13441 20.4998 2.52818C20.5 2.53816 20.5001 2.54813 20.4999 2.55809Z\"></path></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Email</div></button><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M6.785 1.92766C5.45134 1.57031 4.08049 2.36176 3.72314 3.69543L0.444815 15.9303C0.0874636 17.264 0.878901 18.6348 2.21255 18.9922L5.37495 19.8396V7.66664C5.37495 6.40099 6.40096 5.37498 7.66661 5.37498H19.4723C19.3299 5.30548 19.1788 5.24858 19.0201 5.20604L6.785 1.92766Z\" stroke=\"none\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M8.44161 7.4C7.86632 7.4 7.39995 7.86637 7.39995 8.44167V22.1081C7.39995 22.6834 7.86631 23.1498 8.4416 23.1498L22.1083 23.15C22.6836 23.15 23.1499 22.6836 23.1499 22.1083V8.44167C23.1499 7.86637 22.6836 7.4 22.1083 7.4H8.44161ZM10.3999 9.65C9.84766 9.65 9.39995 10.0977 9.39995 10.65C9.39995 11.2023 9.84766 11.65 10.3999 11.65H18.3999C18.9522 11.65 19.3999 11.2023 19.3999 10.65C19.3999 10.0977 18.9522 9.65 18.3999 9.65H10.3999ZM10.3999 14.15C9.84766 14.15 9.39995 14.5977 9.39995 15.15C9.39995 15.7023 9.84766 16.15 10.3999 16.15H15.3999C15.9522 16.15 16.3999 15.7023 16.3999 15.15C16.3999 14.5977 15.9522 14.15 15.3999 14.15H10.3999Z\" stroke=\"none\"></path></g></svg></div><div class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Note</div></button><button tabIndex=\"0\" id=\"trigger4157\" aria-expanded=\"false\" aria-haspopup=\"dialog\" aria-controls=\"dialog4158\" ariaLabel=\"View more\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"100\" height=\"100\" viewBox=\"0 0 100 100\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><circle cx=\"23\" cy=\"50\" r=\"9\"></circle><circle cx=\"50\" cy=\"50\" r=\"9\"></circle><circle cx=\"77\" cy=\"50\" r=\"9\"></circle></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Other</div></button></div></div></div></div></div></div></div></div><a role=\"button\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities/comments\" class=\"post-ufi-button style-button post-ufi-comment-button has-label with-border\"><svg role=\"img\" style=\"height: 20px; width: 20px;\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"#000000\" stroke-width=\"2\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\" class=\"icon\"><g><title></title><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-message-circle \"><path d=\"M7.9 20A9 9 0 1 0 4 16.1L2 22Z\"></path></svg></g></svg><div class=\"label\">37</div></a></div><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><a role=\"button\" href=\"javascript:void(0)\" class=\"post-ufi-button style-button no-icon has-label with-border\"><div class=\"label\">Share</div></a></div></div></div></div><div class=\"visibility-check\"></div><div><div class=\"available-content\"><div dir=\"auto\" class=\"body markup\"><p>How seriously should governments take the threat of existential risk from AI, given the lack of consensus among researchers? On the one hand, existential risks (x-risks) are necessarily somewhat speculative: by the time there is concrete evidence, it may be too late. On the other hand, governments must prioritize — after all, they don’t worry too much about x-risk from alien invasions.</p><p>This is the first in a series of essays laying out an evidence-based approach for policymakers concerned about AI x-risk, an approach that stays grounded in reality while acknowledging that there are “unknown unknowns”. </p><p>In this first essay, we look at one type of evidence: probability estimates. The AI safety community relies heavily on forecasting the probability of human extinction due to AI (in a given timeframe) in order to inform decision making and policy. An estimate of 10% over a few decades, for example, would obviously be high enough for the issue to be a top priority for society. </p><p>Our central claim is that AI x-risk forecasts are far too unreliable to be useful for policy, and in fact highly misleading.</p><div class=\"subscription-widget-wrap\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p>Subscribe to receive future essays in this series.</p></div><div data-component-name=\"SubscribeWidget\" class=\"subscribe-widget\"><div class=\"pencraft pc-display-flex pc-justifyContent-center pc-reset\"><div class=\"_container_11q5m_1\"><form action=\"/api/v1/free?nojs=true\" method=\"post\" noValidate class=\"form _form_11q5m_6\"><input type=\"hidden\" name=\"first_url\" value /><input type=\"hidden\" name=\"first_referrer\" value /><input type=\"hidden\" name=\"current_url\" /><input type=\"hidden\" name=\"current_referrer\" /><input type=\"hidden\" name=\"referral_code\" /><input type=\"hidden\" name=\"source\" value=\"subscribe-widget-preamble\" /><input type=\"hidden\" name=\"referring_pub_id\" /><input type=\"hidden\" name=\"additional_referring_pub_ids\" /><div class=\"_sideBySideWrap_11q5m_10\"><div class=\"_emailInputWrapper_11q5m_57\"><input type=\"email\" name=\"email\" placeholder=\"Type your email...\" class=\"pencraft _emailInput_11q5m_23\" /></div><button tabIndex=\"0\" type=\"submit\" class=\"button rightButton primary subscribe-btn _button_11q5m_76\"><span class=\"button-text \">Subscribe</span></button></div><div id=\"error-container\"></div></form></div></div></div></div></div><h3 class=\"header-anchor-post\"><strong>Look behind the curtain</strong><div class=\"pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent\"><div class=\"pencraft pc-display-contents pc-reset _pubTheme_ipix0_1\"><div id=\"§look-behind-the-curtain\" class=\"pencraft pc-reset header-anchor offset-top\"></div><button tabIndex=\"0\" type=\"button\" data-href=\"https://www.aisnakeoil.com/i/147019742/look-behind-the-curtain\" class=\"pencraft pc-reset pencraft _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_sm_1oht6_119 _priority_secondary_1oht6_63\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-link \"><path d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"></path><path d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"></path></svg></button></div></div></h3><p><span>If the two of us predicted an 80% probability of aliens landing on earth in the next ten years, would you take this possibility seriously? Of course not. You would ask to see our evidence. As obvious as this may seem, it seems to have been forgotten in the AI x-risk debate that probabilities carry no authority by themselves. Probabilities are </span><em>usually</em><span> derived from some grounded method, so we have a strong cognitive bias to view quantified risk estimates as more valid than qualitative ones. But it is possible for probabilities to be nothing more than guesses. Keep this in mind throughout this essay (and more broadly in the AI x-risk debate).</span></p><p><span>If we predicted odds for the Kentucky Derby, we don’t have to give you a reason — you can take it or leave it. But if a policymaker takes actions based on probabilities put forth by a forecaster, they had better be able to explain those probabilities to the public (and that explanation must in turn come from the forecaster). </span><a href=\"https://plato.stanford.edu/entries/justification-public/\" rel>Justification</a><span> is essential to legitimacy of government and the exercise of power. A core principle of liberal democracy is that the state should not limit people's freedom based on controversial beliefs that reasonable people can reject. </span></p><p>Explanation is especially important when the policies being considered are costly, and even more so when those costs are unevenly distributed among stakeholders. A good example is restricting open releases of AI models. Can governments convince people and companies who stand to benefit from open models that they should make this sacrifice because of a speculative future risk?</p><p>The main aim of this essay is analyzing whether there is any justification for any of the specific x-risk probability estimates that have been cited in the policy debate. We have no objection to AI x-risk forecasting as an academic activity, and forecasts may be helpful to companies and other private decision makers. We only question its use in the context of public policy.</p><p>There are basically only three known ways by which a forecaster can try to convince a skeptic: inductive, deductive, and subjective probability estimation. We consider each of these in the following sections. All three require both parties to agree on some basic assumptions about the world (which cannot themselves be proven). The three approaches differ in terms of the empirical and logical ways in which the probability estimate follows from that set of assumptions.</p><h3 class=\"header-anchor-post\"><strong>Inductive probability estimation is unreliable due to the lack of a reference class</strong><div class=\"pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent\"><div class=\"pencraft pc-display-contents pc-reset _pubTheme_ipix0_1\"><div id=\"§inductive-probability-estimation-is-unreliable-due-to-the-lack-of-a-reference-class\" class=\"pencraft pc-reset header-anchor offset-top\"></div><button tabIndex=\"0\" type=\"button\" data-href=\"https://www.aisnakeoil.com/i/147019742/inductive-probability-estimation-is-unreliable-due-to-the-lack-of-a-reference-class\" class=\"pencraft pc-reset pencraft _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_sm_1oht6_119 _priority_secondary_1oht6_63\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-link \"><path d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"></path><path d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"></path></svg></button></div></div></h3><p>Most risk estimates are inductive: they are based on past observations. For example, insurers base their predictions of an individual’s car accident risk on data from past accidents about similar drivers. The set of observations used for probability estimation is called a reference class. A suitable reference class for car insurance might be the set of drivers who live in the same city. If the analyst has more information about the individual, such as their age or the type of car they drive, the reference class can be further refined. </p><p>For existential risk from AI, there is no reference class, as it is an event like no other. To be clear, this is a matter of degree, not kind. There is never a clear “correct” reference class to use, and the choice of a reference class in practice comes down to the analyst’s intuition. </p><p>The accuracy of the forecasts depends on the degree of similarity between the process that generates the event being forecast and the process that generated the events in the reference class, which can be seen as a spectrum. For predicting the outcome of a physical system such as a coin toss, past experience is a highly reliable guide. Next, for car accidents, risk estimates might vary by, say, 20% based on the past dataset used — good enough for insurance companies. </p><div class=\"captioned-image-container\"><figure><a class=\"image-link is-viewable-img image2\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png\" data-component-name=\"Image2ToDOM\" rel><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 1456w\" sizes=\"100vw\" /><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png\" width=\"1238\" height=\"334\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:334,&quot;width&quot;:1238,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:41682,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\" class=\"sizing-normal\" alt srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 1456w\" sizes=\"100vw\" loading=\"lazy\" /></picture><div class=\"image-link-expand\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 \"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></a></figure></div><p><span>Further along the spectrum are geopolitical events, where the choice of reference class gets even fuzzier. Forecasting expert Philip Tetlock </span><a href=\"https://www.jstor.org/stable/j.ctt1pk86s8.5\" rel>explains</a><span>: “Grexit may have looked sui generis, because no country had exited the Eurozone as of 2015, but it could also be viewed as just another instance of a broad comparison class, such as negotiation failures, or of a narrower class, such as a nation-states withdrawing from international agreements or, narrower still, of forced currency conversions.” He goes on to defend the idea that even seeming </span><a href=\"https://en.wikipedia.org/wiki/Black_swan_theory\" rel>Black Swan events</a><span> like the collapse of the USSR or the Arab Spring can be modeled as members of reference classes, and that inductive reasoning is useful even for this kind of event.</span></p><p><span>In Tetlock’s spectrum, these events represent the “peak” of uniqueness. When it comes to geopolitical events, that might be true. But even those events are far less unique than extinction from AI. Just look at the </span><a href=\"https://www.openphilanthropy.org/wp-content/uploads/2023.05.22-AI-Reference-Classes-Zachary-Freitas-Groff.pdf\" rel>attempts</a><span> to find reference classes for AI x-risk: animal extinction (as a reference class for human extinction), past global transformations such as the industrial revolution (as a reference class for socioeconomic transformation from AI), or accidents causing mass deaths (as a reference class for accidents causing global catastrophe). Let’s get real. None of those tell us anything about the possibility of developing superintelligent AI or losing control over such AI, which are the central sources of uncertainty for AI x-risk forecasting. </span></p><p>To summarize, human extinction due to AI is an outcome so far removed from anything that has happened in the past that we cannot use inductive methods to “predict” the odds. Of course, we can get qualitative insights from past technical breakthroughs as well as past catastrophic events, but AI risk is sufficiently different that quantitative estimates lack the kind of justification needed for legitimacy in policymaking.</p><h3 class=\"header-anchor-post\"><strong>Deductive probability estimation is unreliable due to the lack of theory</strong><div class=\"pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent\"><div class=\"pencraft pc-display-contents pc-reset _pubTheme_ipix0_1\"><div id=\"§deductive-probability-estimation-is-unreliable-due-to-the-lack-of-theory\" class=\"pencraft pc-reset header-anchor offset-top\"></div><button tabIndex=\"0\" type=\"button\" data-href=\"https://www.aisnakeoil.com/i/147019742/deductive-probability-estimation-is-unreliable-due-to-the-lack-of-theory\" class=\"pencraft pc-reset pencraft _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_sm_1oht6_119 _priority_secondary_1oht6_63\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-link \"><path d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"></path><path d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"></path></svg></button></div></div></h3><p><span>In Conan Doyle’s </span><em>The Adventure of the Six Napoleons</em><span> </span><em>—</em><span> spoiler alert! </span><em>— </em><span>Sherlock Holmes announces before embarking on a stakeout that the probability of catching the suspect is exactly two-thirds. This seems bewildering — how can anything related to human behavior be ascribed a mathematically precise probability?</span></p><p><span>It turns out that Holmes has deduced the underlying series of events that gave rise to the suspect’s seemingly erratic observed behavior: the suspect is methodically searching for a jewel that is known to be hidden inside one of six busts of Napoleon owned by different people in and around London. The details aren’t too important, but the key is that neither the suspect nor the detectives know </span><em>which</em><span> of the six busts it is in, and everything else about the suspect’s behavior is (assumed to be) entirely predictable. Hence the precisely quantifiable uncertainty.</span></p><p>The point is that if we have a model of the world that we can rely upon, we can estimate risk through logical deduction, even without relying on past observations. Of course, outside of fictional scenarios, the world isn’t so neat, especially when we want to project far into the future.</p><p>When it comes to x-risk, there is an interesting exception to the general rule that we don’t have deductive models — asteroid impact. A combination of inductive and deductive risk estimation does allow us to estimate the probability of x-risk, only because we’re talking about a purely physical system. Let’s take a minute to review how this works, because it’s important to recognize that the methods are not generalizable to other types of x-risk. </p><p><span>The key is being able to </span><a href=\"https://www.nature.com/articles/367033a0\" rel>model</a><span> the relationship between the size of the asteroid (more precisely, the energy of impact) and the frequency of impact. Since we have observed thousands of small impacts, we can extrapolate to infer the frequency of large impacts that have never been directly observed. We can also estimate the threshold that would cause global catastrophe.</span><span class=\"footnote-hovercard-target\"><a class=\"footnote-anchor\" data-component-name=\"FootnoteAnchorToDOM\" id=\"footnote-anchor-1-147019742\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities#footnote-1-147019742\" target=\"_self\" rel>1</a></span></p><div class=\"captioned-image-container\"><figure><a class=\"image-link is-viewable-img image2\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png\" data-component-name=\"Image2ToDOM\" rel><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 1456w\" sizes=\"100vw\" /><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png\" width=\"1456\" height=\"559\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:559,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1119384,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\" class=\"sizing-normal\" alt srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 1456w\" sizes=\"100vw\" loading=\"lazy\" /></picture><div class=\"image-link-expand\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 \"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></a><figcaption class=\"image-caption\"><span>Figure: data on small asteroid impacts (illustrated on the </span><a href=\"https://commons.wikimedia.org/wiki/File:Bolide_events_1994-2013.jpg\" rel>left</a><span>) can be extrapolated to extinction-level impacts (right).</span></figcaption></figure></div><p><span>With AI, the unknowns relate to </span><a href=\"https://www.aisnakeoil.com/p/ai-scaling-myths\" rel>technological progress</a><span> and governance rather than a physical system, so it isn’t clear how to model it mathematically. Still, people have tried. For example, in order to predict the computational requirements of a hypothetical AGI, </span><a href=\"https://arxiv.org/pdf/2306.02519\" rel>several</a><span> </span><a href=\"https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines\" rel>works</a><span> assume that an AI system would require roughly as many computations as the human brain, and further make assumptions about the number of computations required by the human brain. These assumptions are far more tenuous than those involved in asteroid modeling, and none of this even addresses the loss-of-control question.</span></p><h3 class=\"header-anchor-post\"><strong>Subjective probabilities are feelings dressed up as numbers</strong><div class=\"pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent\"><div class=\"pencraft pc-display-contents pc-reset _pubTheme_ipix0_1\"><div id=\"§subjective-probabilities-are-feelings-dressed-up-as-numbers\" class=\"pencraft pc-reset header-anchor offset-top\"></div><button tabIndex=\"0\" type=\"button\" data-href=\"https://www.aisnakeoil.com/i/147019742/subjective-probabilities-are-feelings-dressed-up-as-numbers\" class=\"pencraft pc-reset pencraft _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_sm_1oht6_119 _priority_secondary_1oht6_63\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-link \"><path d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"></path><path d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"></path></svg></button></div></div></h3><p>Without the reference classes or grounded theories, forecasts are necessarily “subjective probabilities”, that is, guesses based on the forecaster’s judgment. Unsurprisingly, these vary by orders of magnitude.</p><p><span>Subjective probability estimation does not get around the need for having either an inductive or a deductive basis for probability estimates. It merely avoids the need for the forecaster to </span><em>explain</em><span> their estimate. Explanation can be hard due to humans’ limited ability to explain our intuitive reasoning, whether inductive, deductive, or a combination thereof. Essentially, it allows the forecaster to say: “even though I haven’t shown my methods, you can trust this estimate because of my track record” (we explain in the next section why even this breaks down for AI x-risk forecasting). But ultimately, lacking either an inductive or a deductive basis, all that forecasters can do is to make up a number, and those made-up numbers are all over the place.</span></p><div class=\"captioned-image-container\"><figure><a class=\"image-link is-viewable-img image2\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png\" data-component-name=\"Image2ToDOM\" rel><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 1456w\" sizes=\"100vw\" /><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png\" width=\"1272\" height=\"896\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:896,&quot;width&quot;:1272,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:172573,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\" class=\"sizing-normal\" alt srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 1456w\" sizes=\"100vw\" loading=\"lazy\" /></picture><div class=\"image-link-expand\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 \"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></a></figure></div><p><span>Consider the </span><a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf\" rel>Existential Risk Persuasion Tournament</a><span> (XPT) conducted by the Forecasting Research Institute in late 2022, which we think is the most elaborate and well-executed x-risk forecasting exercise conducted to date. It involved various groups of forecasters, including AI experts and forecasting experts (“superforecasters” in the figure). For AI experts, the high end (75th percentile) of estimates for AI extinction risk by 2100 is 12%, the median estimate is 3%, and the low end (25th percentile) is 0.25%. For forecasting experts, even the high end (75th percentile) is only 1%, the median is a mere 0.38%, and the low end (25th percentile) is visually indistinguishable from zero on the graph. In other words, the 75th percentile AI expert forecast and the 25th percentile superforecaster forecast differ by at least a factor of 100.</span></p><p>All of these estimates are from people who have deep expertise on the topic and participated in a months-long tournament where they tried to persuade each other! If this range of forecasts here isn’t extreme enough, keep in mind that this whole exercise was conducted by one group at one point in time. We might get different numbers if the tournament were repeated today, if the questions were framed differently, etc.</p><p>What’s most telling is to look at the rationales that forecasters provided, which are extensively detailed in the report. They aren’t using quantitative models, especially when thinking about the likelihood of bad outcomes conditional on developing powerful AI. For the most part, forecasters are engaging in the same kind of speculation that everyday people do when they discuss superintelligent AI. Maybe AI will take over critical systems through superhuman persuasion of system operators. Maybe AI will seek to lower global temperatures because it helps computers run faster, and accidentally wipe out humanity. Or maybe AI will seek resources in space rather than Earth, so we don’t need to be as worried. There’s nothing wrong with such speculation. But we should be clear that when it comes to AI x-risk, forecasters aren’t drawing on any special knowledge, evidence, or models that make their hunches more credible than yours or ours or anyone else’s.  </p><p><span>The term </span><a href=\"https://www.amazon.com/Superforecasting-Science-Prediction-Philip-Tetlock/dp/0804136718\" rel>superforecasting</a><span> comes from Philip Tetlock’s 20 year study of forecasting (he was also one of the organizers of the XPT). Superforecasters tend to be trained in methods to improve forecasts such as by integrating diverse information and by minimizing psychological biases. These methods have been shown to be effective in domains such as geopolitics. But no amount of training will lead to good forecasts if there isn’t much useful evidence to draw from.</span></p><p><span>Even if forecasters had credible quantitative models (they don’t), they must account for “unknown unknowns”, that is, the possibility that the model itself might be wrong. As noted x-risk philosopher Nick Bostrom </span><a href=\"https://existential-risk.com/concept.pdf\" rel>explains</a><span>: “The uncertainty and error-proneness of our first-order assessments of risk is itself something we must factor into our all-things-considered probability assignments. This factor often dominates in low-probability, high-consequence risks — especially those involving poorly understood natural phenomena, complex social dynamics, or new technology, or that are difficult to assess for other reasons.” </span></p><p><span>This is a reasonable perspective, and AI x-risk forecasters do worry a lot about uncertainty in risk assessment. But one consequence of this is that for those who follow this principle, forecasts are </span><em>guaranteed</em><span> to be guesses rather than the output of a model — after all, no model can be used to estimate the probability that the model itself is wrong, or what the risk would be if the model were wrong.</span></p><h3 class=\"header-anchor-post\"><strong>Forecast skill cannot be measured when it comes to unique or rare events</strong><div class=\"pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent\"><div class=\"pencraft pc-display-contents pc-reset _pubTheme_ipix0_1\"><div id=\"§forecast-skill-cannot-be-measured-when-it-comes-to-unique-or-rare-events\" class=\"pencraft pc-reset header-anchor offset-top\"></div><button tabIndex=\"0\" type=\"button\" data-href=\"https://www.aisnakeoil.com/i/147019742/forecast-skill-cannot-be-measured-when-it-comes-to-unique-or-rare-events\" class=\"pencraft pc-reset pencraft _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_sm_1oht6_119 _priority_secondary_1oht6_63\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-link \"><path d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"></path><path d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"></path></svg></button></div></div></h3><p><span>To recap, subjective AI-risk forecasts vary by orders of magnitude. But if we can measure forecasters’ track records, maybe we can use that to figure out which forecasters to trust. In contrast to the previous two approaches for justifying risk estimates (inductive and deductive), the forecaster doesn’t have to explain their estimate, but instead justifies it based on their demonstrated skill at predicting </span><em>other</em><span> outcomes in the past. </span></p><p><span>This has proved to be invaluable in the domain of geopolitical events, and the forecasting community spends a lot of effort on skill measurement. Many ways to evaluate forecasting skill exist, such as calibration, the Brier score, the logarithmic score, or the Peer score used on the forecasting competition website </span><a href=\"https://www.metaculus.com/\" rel>Metaculus</a><span>. </span></p><p>But regardless of which method is used, when it comes to existential risk, there are many barriers to assessing forecast skill for subjective probabilities: the lack of a reference class, the low base rate, and the long time horizon. Let’s look at each of these in turn.</p><p>Just as the reference class problem plagues the forecaster, it also affects the evaluator. Let’s return to the alien landing example. Consider a forecaster who has proved highly accurate at calling elections. Suppose this forecaster announces, without any evidence, that aliens will land on Earth within a year. Despite the forecaster’s demonstrated skill, this would not cause us to update our beliefs about an alien landing, because it is too dissimilar to election forecasting and we do not expect the forecaster’s skill to generalize. Similarly, AI x-risk is so dissimilar to any past events that have been forecast that there is no evidence of any forecaster’s skill at estimating AI x-risk. </p><p><span>Even if we somehow do away with the reference class problem, other problems remain — notably, the fact that extinction risks are “tail risks”, or risks that result from rare events. Suppose forecaster A says the probability of AI x-risk is 1%, and forecaster B says it is 1 in a million. Which forecast should we have more confidence in? We could look at their track records. Say we find that forecaster A (who has assigned a 1% probability to AI x-risk) has a better track record. It still doesn’t mean we should have more confidence in A’s forecast, because </span><em>skill evaluations are insensitive to overestimation of tail risks</em><span>. In other words, it could be that A scores higher overall because A is slightly better calibrated than B when it comes to everyday events that have a substantial probability of occurring, but tends to massively overestimate tail risks that occur rarely (for example, those with a probability of 1 in a million) by orders of magnitude. No scoring rule adequately penalizes this type of miscalibration. </span></p><p><span>Here’s a thought experiment to show why this is true. Suppose two forecasters F and G forecast two different sets of events, and the “true” probabilities of events in both sets are uniformly distributed between 0 and 1. We assume, highly optimistically, that both F and G know the true probability P[</span><em>e</em><span>] for every event </span><em>e</em><span> that they forecast. F always outputs P[</span><em>e</em><span>], but G is slightly conservative, never predicting a value less than 1%. That is, G outputs P[</span><em>e</em><span>] if P[</span><em>e</em><span>] &gt;= 1%, otherwise outputs 1%.</span></p><p><span>By construction, F is the better forecaster. But would this be evident from their track records? In other words, how many forecasts from each would we have to evaluate so that there’s a 95% chance that F outscores G? With the logarithmic scoring rule, it turns out to be on the order of a hundred million. With the Brier score, it is on the order of a trillion.</span><span class=\"footnote-hovercard-target\"><a class=\"footnote-anchor\" data-component-name=\"FootnoteAnchorToDOM\" id=\"footnote-anchor-2-147019742\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities#footnote-2-147019742\" target=\"_self\" rel>2</a></span><span>  We can quibble with the assumptions here but the point is that if a forecaster systematically overestimates tail risks, it is simply empirically undetectable.</span></p><p><span>The final barrier to assessing forecaster skill at predicting x-risk is that long-term forecasts take too long to evaluate (and extinction forecasts are of course impossible to evaluate). This can potentially be overcome. Researchers have developed a method called </span><a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3954498\" rel>reciprocal scoring</a><span> — where forecasters are rewarded based on how well they predict each others’ forecasts — and validate it in some real-world settings, such as predicting the effect of Covid-19 policies. In these settings, reciprocal scoring yielded forecasts that are as good as traditional scoring methods. Fair enough. But reciprocal scoring is not a way around the reference class problem or the tail risk problem.</span></p><div class=\"captioned-image-container\"><figure><a class=\"image-link image2\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png\" data-component-name=\"Image2ToDOM\" rel><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 1456w\" sizes=\"100vw\" /><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png\" width=\"1456\" height=\"337\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:337,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:87385,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\" class=\"sizing-normal\" alt srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 1456w\" sizes=\"100vw\" loading=\"lazy\" /></picture><div></div></div></a><figcaption class=\"image-caption\"><em>Summary of our argument so far, showing why none of the three forecasting methods can yield credible estimates of AI x-risk.</em></figcaption></figure></div><h3 class=\"header-anchor-post\"><strong>There are many reasons why risk estimates may be systematically inflated</strong><div class=\"pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent\"><div class=\"pencraft pc-display-contents pc-reset _pubTheme_ipix0_1\"><div id=\"§there-are-many-reasons-why-risk-estimates-may-be-systematically-inflated\" class=\"pencraft pc-reset header-anchor offset-top\"></div><button tabIndex=\"0\" type=\"button\" data-href=\"https://www.aisnakeoil.com/i/147019742/there-are-many-reasons-why-risk-estimates-may-be-systematically-inflated\" class=\"pencraft pc-reset pencraft _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_sm_1oht6_119 _priority_secondary_1oht6_63\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-link \"><path d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"></path><path d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"></path></svg></button></div></div></h3><p>To recap, inductive and deductive methods don’t work, subjective forecasts are all over the place, and there’s no way to tell which forecasts are more trustworthy.</p><p><span>So in an attempt to derive more reliable estimates that could potentially inform policy, some researchers have turned to forecast aggregation methods that combine the predictions of multiple forecasters. A notable effort is the </span><a href=\"https://wiki.aiimpacts.org/doku.php?id=ai_timelines:predictions_of_human-level_ai_timelines:ai_timeline_surveys:2022_expert_survey_on_progress_in_ai\" rel>AI Impacts Survey on Progress in AI</a><span>, but it has been criticized for serious methodological limitations including </span><a href=\"https://aiguide.substack.com/p/do-half-of-ai-researchers-believe\" rel>non</a><span>-</span><a href=\"https://www.scientificamerican.com/article/ai-survey-exaggerates-apocalyptic-risks/\" rel>response</a><span> </span><a href=\"https://spectrum.ieee.org/ai-existential-risk-survey\" rel>bias</a><span>. More importantly, it is unclear why aggregation should improve forecast accuracy: after all, most forecasters might share the same biases (and again, none of them have any basis for a reliable forecast).</span></p><p><span>There are many reasons why forecasters might systematically overestimate AI x-risk.</span><span class=\"footnote-hovercard-target\"><a class=\"footnote-anchor\" data-component-name=\"FootnoteAnchorToDOM\" id=\"footnote-anchor-3-147019742\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities#footnote-3-147019742\" target=\"_self\" rel>3</a></span><span> The first is selection bias. Take AI researchers: the belief that AI can change the world is one of the main motivations for becoming an AI researcher. And once someone enters this community, they are in an environment where that message is constantly reinforced. And if one believes that this technology is terrifyingly powerful, it is perfectly rational to think there is a serious chance that its world-altering effects will be negative rather than positive.</span></p><p><span>And in the AI safety subcommunity, which is a bit insular, the </span><a href=\"https://www.washingtonpost.com/technology/2023/07/05/ai-apocalypse-college-students/\" rel>echo chamber</a><span> can be deafening. Claiming to have a high </span><em>p(doom)</em><span> (one’s estimate of the probability of AI doom) seems to have become a way to signal one’s identity and commitment to the cause.</span></p><p><span>There is a slightly different selection bias at play when it comes to forecasting experts. The forecasting community has a strong overlap with effective altruism and concerns about existential risk, especially AI risk. This doesn’t mean that individual forecasters are biased. But having a high </span><em>p(doom)</em><span> might make someone more inclined to take up forecasting as an activity. So the community as a whole is likely biased toward people with x-risk worries. </span></p><p><span>Forecasters are good at updating their beliefs in response to evidence, but the problem is that unlike, say, asteroid impact risk, there is little evidence that can change one’s beliefs one way or another when it comes to AI x-risk, so we suspect that forecasts are strongly influenced by the priors with which people enter the community. The </span><a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf\" rel>XPT report</a><span> notes that “Few minds were changed during the XPT, even among the most active participants, and despite monetary incentives for persuading others.” In a </span><a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/65ef1ee52e64b52f145ebb49/1710169832137/AIcollaboration.pdf\" rel>follow-up study</a><span>, they found that many of the disagreements were due to fundamental worldview differences that go beyond AI.</span></p><p>To reemphasize, our points about bias are specific to AI x-risk. If there were a community of election forecasters who were systematically biased (say, toward incumbents), this would become obvious after a few elections when comparing predictions with reality. But with AI x-risk, as we showed in the previous section, skill evaluation is insensitive to overestimation of tail risks.</p><p><span>Interestingly, skill evaluation is extremely sensitive to </span><em>underestimation</em><span> of tail risks: if you assign a probability of 0 for a rare event that actually ends up occurring, you incur an </span><em>infinite </em><span>penalty under the logarithmic scoring rule, from which you can never recover regardless of how well you predicted other events. This is considered one of the main benefits of the logarithmic score and is the reason it is adopted by Metaculus.</span></p><p><span>Now consider a forecaster who doesn’t have a precise estimate — and surely no forecaster has a precise estimate for something with so many axes of uncertainty as AI x-risk. Given the asymmetric penalties, the rational thing to do is to go with the higher end of their range of estimates.</span><span class=\"footnote-hovercard-target\"><a class=\"footnote-anchor\" data-component-name=\"FootnoteAnchorToDOM\" id=\"footnote-anchor-4-147019742\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities#footnote-4-147019742\" target=\"_self\" rel>4</a></span></p><p>In any case, it’s not clear what forecasters actually report when their estimates are highly uncertain. Maybe they don’t respond to the incentives of the scoring function. After all, long-term forecasts won’t be resolved anytime soon. And recall that in the case of the XPT, the incentive is actually to predict each others’ forecasts to get around the problem of long time horizons. The reciprocal scoring paper argues that this will incentivize forecasters to submit their true, high-effort estimates, and considers various objections to this claim. Their defense of the method rests on two key assumptions: that by exerting more effort forecasters can get closer to the true estimate, and that they have no better way to predict what other forecasters will do.</p><p><span>What if these assumptions are not satisfied? As we have argued throughout this post, with AI x-risk, we shouldn’t expect evidence to change forecasters’ prior beliefs, so the first assumption is dubious. And now that one iteration of the XPT has concluded, the published median estimates from that tournament serve as a powerful anchor (a “</span><a href=\"https://en.wikipedia.org/wiki/Focal_point_(game_theory)\" rel>focal point</a><span>” in game theory). It is possible that in the future, forecasters with reciprocal scoring incentives will use existing median forecasts as a starting point, only making minor adjustments to account for new information that has become available since the last tournament. The range of estimates might narrow as existing estimates serve as anchors for future estimates. All that is a roundabout way to say: the less actual evidence there is to draw upon, the more the risk of groupthink.</span></p><h3 class=\"header-anchor-post\"><strong>Beware Pascal’s wager: the dangers of utility maximization </strong><div class=\"pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent\"><div class=\"pencraft pc-display-contents pc-reset _pubTheme_ipix0_1\"><div id=\"§beware-pascals-wager-the-dangers-of-utility-maximization\" class=\"pencraft pc-reset header-anchor offset-top\"></div><button tabIndex=\"0\" type=\"button\" data-href=\"https://www.aisnakeoil.com/i/147019742/beware-pascals-wager-the-dangers-of-utility-maximization\" class=\"pencraft pc-reset pencraft _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_sm_1oht6_119 _priority_secondary_1oht6_63\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-link \"><path d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"></path><path d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"></path></svg></button></div></div></h3><p><span>For what it’s worth, here are the median estimates from the </span><a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf\" rel>XPT</a><span> of both extinction risk and sub-extinction catastrophic risks from AI:</span></p><div class=\"captioned-image-container\"><figure><a class=\"image-link is-viewable-img image2\" target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png\" data-component-name=\"Image2ToDOM\" rel><div class=\"image2-inset\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 1456w\" sizes=\"100vw\" /><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png\" width=\"1456\" height=\"473\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:473,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:442848,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\" class=\"sizing-normal\" alt srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 1456w\" sizes=\"100vw\" loading=\"lazy\" /></picture><div class=\"image-link-expand\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-maximize2 \"><polyline points=\"15 3 21 3 21 9\"></polyline><polyline points=\"9 21 3 21 3 15\"></polyline><line x1=\"21\" x2=\"14\" y1=\"3\" y2=\"10\"></line><line x1=\"3\" x2=\"10\" y1=\"21\" y2=\"14\"></line></svg></div></div></a></figure></div><p>To reiterate, our view is that we shouldn’t take any of these numbers too seriously. They are a reflection of how much different samples of participants fret about AI than anything else.</p><p>As before, the estimates from forecasting experts (superforecasters) and AI experts differ by an order of magnitude or more. To the extent that we put any stock into these estimates, it should be the forecasting experts’ rather than the AI experts’ estimates. One important insight from past research is that domain experts perform worse than forecasting experts who have training in integrating diverse information and by minimizing psychological biases. Still, as we said above, even their forecasts may be vast overestimates, and we just can’t know for sure.</p><p>So what’s the big deal? So what if policymakers believe the risk over a certain timeframe is 1% instead of 0.01%? It seems pretty low in either case!</p><p>It depends on what they do with those probabilities. Most often, these estimates are merely a way to signal the fact that some group of experts thinks the risk is significant. If that’s all they are, so be it. But it’s not clear that all this elaborate effort at quantification is even helpful for this signaling purpose, given that different people interpret the same numbers wildly differently. </p><p><span>For example, Federal Trade Commission chair Lina Khan said her views on the matter were techno-optimistic since her </span><em>p(doom)</em><span> was </span><a href=\"https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html\" rel>only 15%</a><span>, which left experts bewildered. (For what it’s worth, that number is about a thousandfold higher than what we would be comfortable labeling techno-optimist.) It takes a lot of quantitative training to be able to mentally process very small or very large numbers correctly in decision making, and not simply bucket them into categories like “insignificantly small”. Most people are not trained this way. </span></p><p><span>In short, what seems to be happening is that experts’ vague intuitions and fears are being translated into pseudo-precise numbers, and then translated back into vague intuitions and fears by policymakers. Let’s just cut the charade of quantification! The Center for AI Safety’s </span><a href=\"https://www.safe.ai/work/statement-on-ai-risk\" rel>Statement on AI Risk</a><span> was admirably blunt in this regard (of course, we </span><a href=\"https://www.aisnakeoil.com/p/is-avoiding-extinction-from-ai-really\" rel>strongly disagree with its substance</a><span>).</span></p><p>A principled, quantitative way to use probabilities in decision making is utility maximization through cost-benefit analysis. The idea is simple: if we consider an outcome to have a subjective value, or utility, of U (which can be positive or negative), and it has, say, a 10% probability of occurring, we can act as if it is certain to occur and has a value of 0.1 * U. We can then add up the costs and benefits for each option available to us, and choose the one that maximizes costs minus benefits (the “expected utility”).</p><p><span>This is where things get really problematic. First, some people might consider extinction to have an unfathomably large negative value, because it precludes the existence of all the human lives, physical or </span><a href=\"https://www.vox.com/future-perfect/23298870/effective-altruism-longtermism-will-macaskill-future\" rel>simulated</a><span>, that might ever be born in the future. The logical conclusion is that x-risk should be everyone’s top priority all the time! It is reminiscent of </span><a href=\"https://en.wikipedia.org/wiki/Pascal%27s_wager\" rel>Pascal’s wager</a><span>, the argument that it is rational believe in God because even if there is an infinitesimally small chance that God exists, the cost of non-belief is infinite (an eternity in hell as opposed to eternal happiness), and hence so is the expected utility. Fortunately, policymakers don’t give too much credence to decision making frameworks involving infinities. But the idea has taken a powerful hold of the AI safety community and drives some people’s conviction that AI x-risk should be society’s top priority.</span></p><p>Even if we limit ourselves to catastrophic but not existential risks, we are talking about billions of lives on the line, so the expected cost of even a 1% risk is so high that the policy implications are drastic — governments should increase spending on AI x-risk mitigation by orders of magnitude and consider draconian measures such as stopping AI development. This is why it is so vital to understand that these estimates are not backed by any methodology. It would be incredibly unwise to make world-changing policy decisions based on so little evidence.</p><div class=\"subscription-widget-wrap\"><div class=\"subscription-widget show-subscribe\"><div class=\"preamble\"><p>You’re reading AI Snake Oil, a blog about our <a href=\"https://www.aisnakeoil.com/p/ai-snake-oil-is-now-available-to\">book</a>. Subscribe to get new posts.</p></div><div data-component-name=\"SubscribeWidget\" class=\"subscribe-widget\"><div class=\"pencraft pc-display-flex pc-justifyContent-center pc-reset\"><div class=\"_container_11q5m_1\"><form action=\"/api/v1/free?nojs=true\" method=\"post\" noValidate class=\"form _form_11q5m_6\"><input type=\"hidden\" name=\"first_url\" value /><input type=\"hidden\" name=\"first_referrer\" value /><input type=\"hidden\" name=\"current_url\" /><input type=\"hidden\" name=\"current_referrer\" /><input type=\"hidden\" name=\"referral_code\" /><input type=\"hidden\" name=\"source\" value=\"subscribe-widget-preamble\" /><input type=\"hidden\" name=\"referring_pub_id\" /><input type=\"hidden\" name=\"additional_referring_pub_ids\" /><div class=\"_sideBySideWrap_11q5m_10\"><div class=\"_emailInputWrapper_11q5m_57\"><input type=\"email\" name=\"email\" placeholder=\"Type your email...\" class=\"pencraft _emailInput_11q5m_23\" /></div><button tabIndex=\"0\" type=\"submit\" class=\"button rightButton primary subscribe-btn _button_11q5m_76\"><span class=\"button-text \">Subscribe</span></button></div><div id=\"error-container\"></div></form></div></div></div></div></div><h3 class=\"header-anchor-post\"><strong>Forecasts of milestones suffer from outcome ambiguity</strong><div class=\"pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent\"><div class=\"pencraft pc-display-contents pc-reset _pubTheme_ipix0_1\"><div id=\"§forecasts-of-milestones-suffer-from-outcome-ambiguity\" class=\"pencraft pc-reset header-anchor offset-top\"></div><button tabIndex=\"0\" type=\"button\" data-href=\"https://www.aisnakeoil.com/i/147019742/forecasts-of-milestones-suffer-from-outcome-ambiguity\" class=\"pencraft pc-reset pencraft _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_sm_1oht6_119 _priority_secondary_1oht6_63\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-link \"><path d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"></path><path d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"></path></svg></button></div></div></h3><p>Is there a role for forecasting in AI policy? We think yes — just not forecasting existential risk. Forecasting AI milestones, such as performance on certain capability benchmarks or economic impacts, is more achievable and meaningful. If a forecaster has demonstrated skill in predicting when various AI milestones would be reached, it does give us evidence that they will do well in the future. We are no longer talking about unique or rare events. And when considering lower-stakes policy interventions — preparing for potential economic disruption rather than staving off killer robots — it is less critical that forecasts be justified to the satisfaction of every reasonable person. </p><p><span>The forecasting community devotes a lot of energy to milestone forecasting. On Metaculus, the question “Will there be Human-machine intelligence parity before 2040?” has an aggregate prediction of </span><a href=\"https://www.metaculus.com/questions/384/humanmachine-intelligence-parity-by-2040/\" rel>96%</a><span> based on over 1,300 forecasters. That’s remarkable! If we agreed with this forecast, we would be in favor of the position that managing the safe transition to AGI should be a global priority. Why don’t we?</span></p><p>The answer is in the fine print. There is no consensus on the definition of a fuzzy concept such as AGI. Even if we fix a definition, determining whether it has been achieved can be hard or impossible. For effective forecasting, it is extremely important to avoid ambiguous outcomes. The way the forecasting community gets around this is by defining it in terms of relatively narrow skills, such as exam performance. </p><p><span>The Metaculus intelligence parity question is defined in terms of the performance on graduate exams in math, physics, and computer science. Based on this definition, we do agree with the forecast of 96%. But we think the definition is so watered down that it doesn’t mean much for policy. Forget existential risk — as we’ve written before, AI performance on exams has so little </span><a href=\"https://www.aisnakeoil.com/p/gpt-4-and-professional-benchmarks\" rel>construct validity</a><span> that it doesn’t even let us predict whether AI will replace workers.</span></p><p><span>Other benchmarks </span><a href=\"https://www.aisnakeoil.com/p/new-paper-ai-agents-that-matter\" rel>aren’t much better</a><span>. In short, forecasting AI capability timelines is tricky because of the huge gap between benchmarks and real-world implications. Fortunately, </span><a href=\"https://www.openphilanthropy.org/rfp-llm-benchmarks/\" rel>better benchmarks</a><span> reflecting consequential real-world tasks are being developed. In addition to benchmarks, we need naturalistic evaluation, even if it is more costly. One type of naturalistic evaluation is to measure how people perform their jobs differently with AI assistance. Directly forecasting economic, social, or political impacts — such as labor market transformation or AI-related spending by militaries — could be even more useful, although harder to unambiguously define and measure.</span></p><h3 class=\"header-anchor-post\"><strong>Concluding thoughts</strong><div class=\"pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent\"><div class=\"pencraft pc-display-contents pc-reset _pubTheme_ipix0_1\"><div id=\"§concluding-thoughts\" class=\"pencraft pc-reset header-anchor offset-top\"></div><button tabIndex=\"0\" type=\"button\" data-href=\"https://www.aisnakeoil.com/i/147019742/concluding-thoughts\" class=\"pencraft pc-reset pencraft _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_sm_1oht6_119 _priority_secondary_1oht6_63\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-link \"><path d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"></path><path d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"></path></svg></button></div></div></h3><p><span>The responsibility for avoiding misuses of probability in policy lies with policymakers. We are not calling for forecasters to stop publishing forecasts in order to “protect” policymakers from being misled. That said, we think forecasts should be accompanied by a clear explanation of the process used and evidence considered. This would allow policymakers to make informed decisions about whether the justification presented meets the threshold that they are comfortable with. The XPT is a good example of transparency, as is </span><a href=\"https://arxiv.org/abs/2306.02519\" rel>this paper</a><span> (though it is not about x-risk). On the other hand, simply surveying a bunch of researchers and presenting aggregate numbers is misinformative and should be ignored by policymakers.</span></p><p><span>So what should governments do about AI x-risk? Our view isn’t that they should do nothing. But they should reject the kind of policies that might seem compelling if we view x-risk as urgent and serious, notably: restricting AI development. As we’ll argue in a future essay in this series, not only are such policies unnecessary, they are likely to </span><em>increase</em><span> x-risk. Instead, governments should adopt policies that are compatible with a range of possible estimates of AI risk, and are on balance helpful even if the risk is negligible. Fortunately, such policies exist. Governments should also change policymaking </span><em>processes</em><span> so that they are more responsive to new evidence. More on all that soon.</span></p><h4 class=\"header-anchor-post\"><strong>Further reading</strong><div class=\"pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent\"><div class=\"pencraft pc-display-contents pc-reset _pubTheme_ipix0_1\"><div id=\"§further-reading\" class=\"pencraft pc-reset header-anchor offset-top\"></div><button tabIndex=\"0\" type=\"button\" data-href=\"https://www.aisnakeoil.com/i/147019742/further-reading\" class=\"pencraft pc-reset pencraft _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_sm_1oht6_119 _priority_secondary_1oht6_63\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"18\" height=\"18\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-link \"><path d=\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"></path><path d=\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"></path></svg></button></div></div></h4><ul><li><p><span>The XPT report is titled </span><a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf\" rel>Forecasting Existential Risks: Evidence from a Long-Run Forecasting Tournament</a><span>.</span></p></li><li><p><span>Tetlock and Gardner’s book </span><a href=\"https://www.amazon.com/Superforecasting-Science-Prediction-Philip-Tetlock/dp/0804136718\" rel>Superforecasting</a><span> summarizes research by Tetlock, Barbara Mellers, and others.</span></p></li><li><p><span>Scott Alexander refutes the claim that there is something wrong in principle with ascribing </span><a href=\"https://www.astralcodexten.com/p/in-continued-defense-of-non-frequentist\" rel>probabilities to unique events</a><span> (we largely agree). Our argument differs in two key ways from the position he addresses. We aren’t talking about forecasting in general; just its application to policymaking. And we don’t object to it in principle. Our argument is empirical: AI x-risk forecasts are extremely unreliable and lack justification. Theoretically this could change in the future, though we aren’t holding our breath.</span></p></li><li><p><span>A </span><a href=\"https://www.jstor.org/stable/45094450?seq=1\" rel>paper</a><span> by Friedman and Zuckhauser explains why probabilities aren’t the whole story: two forecasts that have the same risk estimate might have very different implications for policymakers. In our view, AI x-risk forecasts fare poorly on two of the three dimensions of confidence: a sound basis in evidence and a narrow range of reasonable opinion.</span></p></li><li><p><span>A paper by our Princeton CITP colleagues led by Shazeda Ahmed explains the </span><a href=\"https://firstmonday.org/ojs/index.php/fm/article/view/13626/11596\" rel>epistemic culture of AI safety</a><span>, which consists of “cohesive, interwoven social structures of knowledge-production and community-building”. It helps understand why practices such as forecasting have become pillars of how the AI safety community forms its beliefs, as opposed to the broader scientific community that centers practices such as peer review. Of course, we shouldn’t reject a view just because it doesn’t conform to scientific orthodoxy. But at the same time, we shouldn’t give any deference to the self-styled AI safety community’s views on AI safety. It is important to understand that the median member of the AI safety community holds one particular stance on AI safety — a stance that is highly contested and in our view rather alarmist.</span></p></li><li><p><span>We have written extensively about </span><a href=\"https://www.aisnakeoil.com/t/ai-safety\" rel>evidence-based AI safety</a><span>. Our best-known work includes the essay </span><a href=\"https://www.aisnakeoil.com/p/ai-safety-is-not-a-model-property\" rel>AI safety is not a model property</a><span> and the paper titled </span><a href=\"https://crfm.stanford.edu/open-fms/\" rel>On the Societal Impact of Open Foundation Models</a><span> which was the result of a large collaboration.</span></p></li></ul><p><strong>Acknowledgements.</strong><span> We are grateful to Benjamin Edelman, Ezra Karger, Matt Salganik, and Ollie Stephenson for feedback on a draft. This series of essays is based on an upcoming paper that benefited from feedback from many people, including Seth Lazar and members of the MINT lab at Australian National University, students in the Limits to Prediction course at Princeton, Shazeda Ahmed, and Zachary Siegel.</span></p><div class=\"footnote\" data-component-name=\"FootnoteToDOM\"><a id=\"footnote-1-147019742\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities#footnote-anchor-1-147019742\" class=\"footnote-number\" contenteditable=\"false\" target=\"_self\" rel>1</a><div class=\"footnote-content\"><p>Specifically, by using data from nuclear weapons tests and making a few physics-informed assumptions, we can calculate what it would take to kick up enough dust to darken the skies for a prolonged period and lead to a collapse of global agriculture.</p></div></div><div class=\"footnote\" data-component-name=\"FootnoteToDOM\"><a id=\"footnote-2-147019742\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities#footnote-anchor-2-147019742\" class=\"footnote-number\" contenteditable=\"false\" target=\"_self\" rel>2</a><div class=\"footnote-content\"><p>Suppose G, the conservative forecaster, has a floor of ϵ for their forecasts (in our example, 0.01). There is only an ϵ fraction of events where the difference between the two forecasters is even relevant. Even limiting ourselves to the set of events with a true probability less than ϵ, some calculation shows that F’s expected log score is better than G’s by a tiny amount — O(ϵ), and with the Brier score, O(ϵ^2). So when looking at all events, the differences in expected scores are O(ϵ^2) and O(ϵ^3) respectively. Meanwhile, the variance in their scores comes out to about 0.1 for both scoring functions. To be able to confidently assert that the means of the two forecaster’s scores are different, we need the number of events N to be large enough that the standard deviation of the difference in mean scores, which is sqrt(0.1 / N), to be much less than the expected difference. So we need N ~ O(1/ϵ^4) for the log score and N ~ O(1/ϵ^6) for the Brier score.</p></div></div><div class=\"footnote\" data-component-name=\"FootnoteToDOM\"><a id=\"footnote-3-147019742\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities#footnote-anchor-3-147019742\" class=\"footnote-number\" contenteditable=\"false\" target=\"_self\" rel>3</a><div class=\"footnote-content\"><p><span>Of course, there are also reasons why forecasters might underestimate the risk. We find those reasons less persuasive, but more importantly, we consider them out of scope for this discussion. The reason is simple: the advocacy for governments to implement freedom-restricting policies is being justified by </span><em>high</em><span> x-risk estimates. The burden of evidence is thus asymmetric. Those who call for such policies must justify their probability estimates, including responding to concerns about upward biases. If, in some strange future world, some people advocate for restrictive policies based on </span><em>low</em><span> x-risk estimates, it will become important to subject potential </span><em>underestimation</em><span> to the same scrutiny.</span></p></div></div><div class=\"footnote\" data-component-name=\"FootnoteToDOM\"><a id=\"footnote-4-147019742\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities#footnote-anchor-4-147019742\" class=\"footnote-number\" contenteditable=\"false\" target=\"_self\" rel>4</a><div class=\"footnote-content\"><p>In the theory of forecasting, the logarithmic score and other so-called proper scoring rules are mathematically guaranteed to elicit the forecaster’s &quot;true belief&quot;. But the definition of a proper scoring rule assumes that the true belief is a single value, whereas it tends to be a wide range. In such a scenario, the forecaster is incentivized to report the mean of their distribution, but what we intuitively mean by the “true belief” might better correspond to the median.</p><p></p></div></div></div></div><div class=\"visibility-check\"></div><div class=\"post-footer\"><div class=\"pencraft pc-display-flex pc-gap-16 pc-paddingTop-16 pc-paddingBottom-16 pc-justifyContent-space-between pc-alignItems-center _flexGrow_17s6c_230 pc-reset _border-top-detail-themed_17s6c_47 _border-bottom-detail-themed_17s6c_50 post-ufi\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><div class=\"like-button-container post-ufi-button style-button\"><a role=\"button\" class=\"post-ufi-button style-button has-label with-border\"><svg role=\"img\" style=\"height: 20px; width: 20px;\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"#000000\" stroke-width=\"2\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\" class=\"icon\"><g><title></title><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-heart \"><path d=\"M19 14c1.49-1.46 3-3.21 3-5.5A5.5 5.5 0 0 0 16.5 3c-1.76 0-3 .5-4.5 2-1.5-1.5-2.74-2-4.5-2A5.5 5.5 0 0 0 2 8.5c0 2.3 1.5 4.05 3 5.5l7 7Z\"></path></svg></g></svg><div class=\"label\">100</div></a><div inert role=\"dialog\" class=\"modal typography out gone share-dialog popup\"><div class=\"modal-table\"><div class=\"modal-row\"><div class=\"modal-cell modal-content no-fullscreen\"><div class=\"container\"><button tabIndex=\"0\" type=\"button\" data-testid=\"close-modal\" class=\"pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_md_1oht6_127 _priority_tertiary_1oht6_69\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"secondary\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-x \"><path d=\"M18 6 6 18\"></path><path d=\"m6 6 12 12\"></path></svg></button><div class=\"share-dialog-title\">Share this post</div><div class=\"pencraft pc-display-flex pc-flexDirection-column pc-gap-32 pc-paddingLeft-24 pc-paddingRight-24 pc-paddingTop-32 pc-paddingBottom-48 pc-reset\"><div class=\"pencraft pc-display-flex pc-padding-8 pc-reset _border-detail_17s6c_25 pc-borderRadius-md social-preview-box post\"><div class=\"social-image-box\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_120,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6418811e-f1c2-4127-b6dd-f243c4f458ef_1398x918.png\" /><img src=\"https://substackcdn.com/image/fetch/w_120,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6418811e-f1c2-4127-b6dd-f243c4f458ef_1398x918.png\" sizes=\"100vw\" alt width=\"120\" loading=\"lazy\" class=\"_img_16u6n_1 social-image pencraft pc-reset\" /></picture></div><div class=\"pencraft pc-display-flex pc-flexDirection-column pc-paddingTop-8 pc-paddingBottom-8 pc-paddingLeft-12 pc-reset\"><h4 class=\"pencraft pc-reset _line-height-24_3axfk_98 _font-display_3axfk_118 _size-20_3axfk_70 _weight-bold_3axfk_168 _reset_3axfk_1\">AI existential risk probabilities are too unreliable to inform policy</h4><div class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">www.aisnakeoil.com</div></div></div><div class=\"pencraft pc-display-flex pc-gap-8 pc-justifyContent-space-between pc-reset share-dialog-buttons-wrapper\"><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"20\" height=\"16\" viewBox=\"0 0 20 16\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M12.1303 0.000379039C10.9833 -0.00959082 9.87819 0.431464 9.05309 1.22855L9.04556 1.23593L7.79145 2.48303C7.50587 2.767 7.50453 3.22877 7.78844 3.51441C8.07235 3.80004 8.53401 3.80139 8.81959 3.51741L10.0698 2.27423C10.6194 1.74503 11.3546 1.45229 12.1177 1.45892C12.8824 1.46556 13.6139 1.77236 14.1546 2.31323C14.6954 2.8541 15.0021 3.58577 15.0087 4.35065C15.0154 5.11353 14.7229 5.84857 14.1943 6.39829L12.0116 8.58145L12.0115 8.58155C11.7159 8.87739 11.36 9.10617 10.9682 9.25237C10.5764 9.39857 10.1577 9.45878 9.74051 9.42889C9.32337 9.39901 8.91752 9.27975 8.55051 9.07918C8.1835 8.87862 7.8639 8.60146 7.6134 8.26649C7.3722 7.94396 6.91526 7.87807 6.5928 8.11933C6.27034 8.36059 6.20447 8.81763 6.44567 9.14016C6.82142 9.64261 7.30082 10.0584 7.85134 10.3592C8.40186 10.66 9.01062 10.8389 9.63634 10.8838C10.2621 10.9286 10.8901 10.8383 11.4779 10.619C12.0656 10.3997 12.5994 10.0565 13.0429 9.61274L15.2302 7.42494L15.2391 7.4159C16.036 6.59062 16.4769 5.48529 16.467 4.33797C16.457 3.19066 15.9969 2.09316 15.1858 1.28185C14.3746 0.470545 13.2774 0.0103489 12.1303 0.000379039ZM7.29806 5.11625C6.67234 5.07142 6.0443 5.16173 5.45654 5.38103C4.86882 5.60031 4.33502 5.94355 3.89153 6.38727L1.70423 8.57506L1.69534 8.5841C0.898438 9.40939 0.457483 10.5147 0.467451 11.662C0.477418 12.8094 0.937512 13.9069 1.74864 14.7182C2.55976 15.5295 3.65701 15.9897 4.80407 15.9996C5.95113 16.0096 7.05622 15.5685 7.88132 14.7715L7.89035 14.7626L9.13717 13.5155C9.42192 13.2307 9.42192 12.7689 9.13717 12.4841C8.85243 12.1993 8.39077 12.1993 8.10602 12.4841L6.86392 13.7265C6.31432 14.2552 5.57945 14.5477 4.81675 14.5411C4.05204 14.5344 3.32054 14.2276 2.77979 13.6868C2.23904 13.1459 1.93231 12.4142 1.92566 11.6494C1.91904 10.8865 2.21146 10.1514 2.74011 9.60172L4.92287 7.41846C5.21854 7.12262 5.57437 6.89384 5.96621 6.74763C6.35805 6.60143 6.77674 6.54123 7.19389 6.57111C7.61104 6.601 8.01688 6.72026 8.38389 6.92082C8.75091 7.12138 9.0705 7.39855 9.32101 7.73352C9.56221 8.05605 10.0191 8.12194 10.3416 7.88068C10.6641 7.63942 10.7299 7.18238 10.4887 6.85985C10.113 6.3574 9.63359 5.94165 9.08307 5.64081C8.53255 5.33997 7.92378 5.16107 7.29806 5.11625Z\"></path></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Copy link</div></button><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"16\" height=\"17\" viewBox=\"0 0 16 17\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M10.6543 1.38723C10.3533 0.960814 9.95383 0.61341 9.48976 0.374567C9.02902 0.137956 8.51908 0.0130716 8.00115 0.0100098C7.86087 0.0101844 7.72354 0.0502687 7.60519 0.125581C7.48684 0.200893 7.39237 0.308324 7.3328 0.435326L5.00368 5.67077H3.029C2.72335 5.66964 2.42059 5.73003 2.13876 5.84833C1.85692 5.96663 1.60177 6.14043 1.38849 6.35938C1.16707 6.57502 0.991841 6.83346 0.873459 7.11897C0.755078 7.40447 0.696022 7.71108 0.699885 8.02014V13.691C0.699885 14.3087 0.945273 14.9012 1.38207 15.338C1.81886 15.7747 2.41128 16.0201 3.029 16.0201H13.348C13.8951 16.021 14.425 15.8283 14.8438 15.4762C15.2626 15.1241 15.5434 14.6352 15.6366 14.0961L16.6493 8.4252C16.7252 8.09192 16.7252 7.74582 16.6493 7.41254C16.566 7.08205 16.4104 6.7742 16.1936 6.51128C15.9746 6.25 15.7017 6.03926 15.3936 5.89355C15.0762 5.7467 14.7306 5.67068 14.3809 5.67077H10.5328L11.0391 4.37457C11.2397 3.88784 11.3162 3.35894 11.2619 2.83533C11.1853 2.30894 10.9763 1.81065 10.6543 1.38723ZM4.75052 14.5518H3.029C2.91049 14.5525 2.79303 14.5296 2.68349 14.4844C2.57394 14.4392 2.47452 14.3726 2.39102 14.2885C2.23609 14.1199 2.14945 13.8997 2.14799 13.6708V8.02014C2.14913 7.901 2.17389 7.78328 2.22082 7.67377C2.26775 7.56427 2.33592 7.46515 2.4214 7.38216C2.50369 7.29576 2.60267 7.22698 2.71233 7.17998C2.822 7.13298 2.94007 7.10874 3.05938 7.10874H4.7809L4.75052 14.5518ZM10.6746 7.05811H14.3809C14.5145 7.05821 14.6462 7.08942 14.7657 7.14925C14.8875 7.20532 14.9948 7.28845 15.0796 7.39229C15.1675 7.49052 15.2301 7.60871 15.2619 7.73659C15.2922 7.8665 15.2922 8.00162 15.2619 8.13153L14.2493 13.8024C14.2087 14.017 14.094 14.2106 13.9252 14.3492C13.7619 14.4812 13.558 14.5528 13.348 14.5518H6.19862V6.45052L8.43659 1.38723H8.52773C8.9042 1.50037 9.23304 1.73413 9.4636 2.05252C9.69416 2.37092 9.81365 2.75627 9.80368 3.14925C9.8181 3.39741 9.78015 3.64583 9.69229 3.87836L9.23659 5.04292C9.15397 5.273 9.12623 5.51921 9.15558 5.76191C9.1877 6.00427 9.27425 6.23623 9.40875 6.44039C9.5535 6.6376 9.74028 6.80017 9.95558 6.91634C10.1774 7.03206 10.4244 7.0912 10.6746 7.08849V7.05811Z\"></path></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Facebook</div></button><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"21\" height=\"16\" viewBox=\"0 0 21 16\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M2.22192 2.20503C2.36754 1.77115 2.78269 1.45455 3.26639 1.45455H17.9332C18.4169 1.45455 18.8321 1.77118 18.9777 2.2051L10.5999 8.02107L2.22192 2.20503ZM2.16639 3.94198V13.4545C2.16639 14.0529 2.66307 14.5455 3.26639 14.5455H17.9332C18.5365 14.5455 19.0332 14.0529 19.0332 13.4545V3.94206L11.0204 9.50462C10.7679 9.67991 10.4318 9.67991 10.1793 9.50462L2.16639 3.94198ZM20.4999 2.55809V13.4545C20.4999 14.8562 19.3465 16 17.9332 16H3.26639C1.85304 16 0.699707 14.8562 0.699707 13.4545V2.54545C0.699707 1.14379 1.85304 0 3.26639 0H17.9332C19.3407 0 20.4904 1.13441 20.4998 2.52818C20.5 2.53816 20.5001 2.54813 20.4999 2.55809Z\"></path></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Email</div></button><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M6.785 1.92766C5.45134 1.57031 4.08049 2.36176 3.72314 3.69543L0.444815 15.9303C0.0874636 17.264 0.878901 18.6348 2.21255 18.9922L5.37495 19.8396V7.66664C5.37495 6.40099 6.40096 5.37498 7.66661 5.37498H19.4723C19.3299 5.30548 19.1788 5.24858 19.0201 5.20604L6.785 1.92766Z\" stroke=\"none\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M8.44161 7.4C7.86632 7.4 7.39995 7.86637 7.39995 8.44167V22.1081C7.39995 22.6834 7.86631 23.1498 8.4416 23.1498L22.1083 23.15C22.6836 23.15 23.1499 22.6836 23.1499 22.1083V8.44167C23.1499 7.86637 22.6836 7.4 22.1083 7.4H8.44161ZM10.3999 9.65C9.84766 9.65 9.39995 10.0977 9.39995 10.65C9.39995 11.2023 9.84766 11.65 10.3999 11.65H18.3999C18.9522 11.65 19.3999 11.2023 19.3999 10.65C19.3999 10.0977 18.9522 9.65 18.3999 9.65H10.3999ZM10.3999 14.15C9.84766 14.15 9.39995 14.5977 9.39995 15.15C9.39995 15.7023 9.84766 16.15 10.3999 16.15H15.3999C15.9522 16.15 16.3999 15.7023 16.3999 15.15C16.3999 14.5977 15.9522 14.15 15.3999 14.15H10.3999Z\" stroke=\"none\"></path></g></svg></div><div class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Note</div></button><button tabIndex=\"0\" id=\"trigger4161\" aria-expanded=\"false\" aria-haspopup=\"dialog\" aria-controls=\"dialog4162\" ariaLabel=\"View more\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"100\" height=\"100\" viewBox=\"0 0 100 100\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><circle cx=\"23\" cy=\"50\" r=\"9\"></circle><circle cx=\"50\" cy=\"50\" r=\"9\"></circle><circle cx=\"77\" cy=\"50\" r=\"9\"></circle></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Other</div></button></div></div></div></div></div></div></div></div><a role=\"button\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities/comments\" class=\"post-ufi-button style-button post-ufi-comment-button has-label with-border\"><svg role=\"img\" style=\"height: 20px; width: 20px;\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"#000000\" stroke-width=\"2\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\" class=\"icon\"><g><title></title><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-message-circle \"><path d=\"M7.9 20A9 9 0 1 0 4 16.1L2 22Z\"></path></svg></g></svg><div class=\"label\">37</div></a></div><div class=\"pencraft pc-display-flex pc-gap-8 pc-reset\"><a role=\"button\" href=\"javascript:void(0)\" class=\"post-ufi-button style-button no-icon has-label with-border\"><div class=\"label\">Share</div></a></div></div></div><div class=\"pencraft pc-display-contents pc-reset _pubTheme_ipix0_1\"><div class=\"pencraft pc-display-flex pc-paddingTop-16 pc-paddingBottom-16 pc-justifyContent-space-between pc-reset\"><button tabIndex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft _buttonBase_1oht6_1 _button_1oht6_1 _buttonNew_1oht6_83 _button2_1oht6_117 _priority_secondary_1oht6_63 _size_md_1oht6_127\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-arrow-left \"><path d=\"m12 19-7-7 7-7\"></path><path d=\"M19 12H5\"></path></svg>Previous</button><button tabIndex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft _buttonBase_1oht6_1 _button_1oht6_1 _buttonNew_1oht6_83 _button2_1oht6_117 _priority_secondary_1oht6_63 _size_md_1oht6_127\">Next<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-arrow-right \"><path d=\"M5 12h14\"></path><path d=\"m12 5 7 7-7 7\"></path></svg></button></div></div></div></article></div></div><div class=\"single-post-section comments-section\"><div class=\"container\"><div class=\"comments-section-title\">37 Comments</div><div class=\"visibility-check\"></div><div class=\"pencraft pc-display-contents pc-reset _pubTheme_ipix0_1\"><div data-test-id=\"comment-input\" class=\"pencraft pc-display-flex _flexGrow_17s6c_230 pc-reset comment-input-wrap\"><form method=\"post\" noValidate class=\"form comment-input\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_64,h_64,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Flogged-out.png\" /><img src=\"https://substackcdn.com/image/fetch/w_64,h_64,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Flogged-out.png\" sizes=\"100vw\" alt width=\"64\" height=\"64\" style=\"width: 32px; height: 32px;\" class=\"_img_16u6n_1 _avatar_u4hgo_1 _object-fit-cover_16u6n_5 pencraft pc-reset\" /></picture><div class=\"pencraft pc-display-flex pc-flexDirection-column _flexGrow_17s6c_230 pc-reset comment-input-right\"><textarea data-gramm=\"false\" data-gramm_editor=\"false\" data-enable-grammarly=\"false\" name=\"body\" placeholder=\"Write a comment...\"></textarea><div id=\"error-container\"></div><div class=\"pencraft pc-display-flex pc-paddingTop-8 pc-justifyContent-space-between pc-alignItems-center pc-reset\"></div></div></form></div></div><div class=\"comment-list post-page-root-comment-list\"><div class=\"comment-list-items\"><div class=\"comment\"><div id=\"comment-63384762\" class=\"comment-anchor\"></div><div id=\"comment-63384762-reply\" class=\"comment-anchor\"></div><table class=\"comment-content\"><tr><td class=\"comment-head\"><div class=\"profile-hover-card-target _profileHoverCardTarget_c9bh7_50\"><div class=\"user-head\"><a href=\"https://substack.com/profile/10448526-aryeh-l-englander\"><div class=\"profile-img-wrap\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_66,h_66,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50a9f95a-009b-4da0-a1a4-2b18366ab848_241x241.jpeg\" /><img src=\"https://substackcdn.com/image/fetch/w_66,h_66,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50a9f95a-009b-4da0-a1a4-2b18366ab848_241x241.jpeg\" sizes=\"100vw\" alt width=\"66\" height=\"66\" class=\"_img_16u6n_1 pencraft pc-reset\" /></picture></div></a></div></div></td><td class=\"comment-rest\"><div class=\"comment-meta\"><span class=\"commenter-name\"><div class=\"pencraft pc-display-flex pc-gap-4 pc-alignItems-center pc-reset pc-display-inline-flex\"><div class=\"profile-hover-card-target _profileHoverCardTarget_c9bh7_50\"><a href=\"https://substack.com/profile/10448526-aryeh-l-englander\"><div class=\"pencraft pc-reset _color-pub-primary-text_3axfk_204 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-semibold_3axfk_165 _reset_3axfk_1\">Aryeh L. Englander</div></a></div></div></span><div class=\"_publicationHoverCardTarget_c9bh7_51\"><a href=\"https://aryehlenglander.substack.com?utm_source=substack&amp;utm_medium=web&amp;utm_content=comment_metadata\" rel=\"nofollow\" target=\"_blank\" class=\"commenter-publication\"><span>Aryeh L. Englander </span></a></div><a href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities/comment/63384762\" rel=\"nofollow\" native class=\"comment-timestamp\">Jul 26</a></div><div class=\"comment-body\"><p><span>Doesn't uncertainty cuts both ways? Sure, I can get on board with saying that forecasts are unreliable, there are no really good reference classes to use, etc. But doesn't that also mean you can't confidently state that &quot;not only are such policies unnecessary, they are likely to increase x-risk&quot;? And if you can't confidently state one way or the other, then it's not at all clear to me that the correct approach is to not restrict AI development. (It's also not at all clear to me that the correct approach is the opposite, of course.) So, sure, I am happy to get on board with, &quot;governments should adopt policies that are compatible with a range of possible estimates of AI risk, and are on balance helpful even if the risk is negligible.&quot; But shouldn't we also make sure that the policies are on balance helpful even if the risk is high?</span></p><div role=\"button\" class=\"show-all-toggle\"><div class=\"show-all-toggle-label\">Expand full comment</div></div></div><div class=\"pencraft pc-display-flex pc-gap-16 pc-paddingTop-8 pc-justifyContent-flex-start pc-alignItems-center pc-reset comment-actions _withShareButton_mhaex_5\"><span class=\"pencraft pc-reset _decoration-hover-underline_3axfk_298 _reset_3axfk_1\"><a class=\"pencraft pc-reset _link_mhaex_1 _link_1ixw5_1\"><div class=\"pencraft pc-display-flex pc-gap-6 pc-alignItems-center pc-reset\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"var(--color-secondary-themed)\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-message-circle \"><path d=\"M7.9 20A9 9 0 1 0 4 16.1L2 22Z\"></path></svg><div class=\"pencraft pc-reset _color-pub-secondary-text_3axfk_207 _line-height-20_3axfk_95 _font-meta_3axfk_131 _size-11_3axfk_35 _weight-medium_3axfk_162 _transform-uppercase_3axfk_242 _reset_3axfk_1 _meta_3axfk_442\">Reply</div></div></a></span><span class=\"pencraft pc-reset _decoration-hover-underline_3axfk_298 _reset_3axfk_1\"><a class=\"pencraft pc-reset _link_mhaex_1 _link_1ixw5_1\"><div class=\"pencraft pc-display-flex pc-gap-6 pc-alignItems-center pc-reset\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"var(--color-secondary-themed)\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-share \"><path d=\"M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8\"></path><polyline points=\"16 6 12 2 8 6\"></polyline><line x1=\"12\" x2=\"12\" y1=\"2\" y2=\"15\"></line></svg><div class=\"pencraft pc-reset _color-pub-secondary-text_3axfk_207 _line-height-20_3axfk_95 _font-meta_3axfk_131 _size-11_3axfk_35 _weight-medium_3axfk_162 _transform-uppercase_3axfk_242 _reset_3axfk_1 _meta_3axfk_442\">Share</div></div></a></span><button tabIndex=\"0\" type=\"button\" id=\"trigger4165\" aria-expanded=\"false\" aria-haspopup=\"dialog\" aria-controls=\"dialog4166\" ariaLabel=\"View more\" class=\"pencraft pc-reset pencraft _iconButton_1oht6_145 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonOldColors_1oht6_56 _priority_secondary-theme_1oht6_256 _fill_empty_1oht6_317\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-more-horizontal \"><circle cx=\"12\" cy=\"12\" r=\"1\"></circle><circle cx=\"19\" cy=\"12\" r=\"1\"></circle><circle cx=\"5\" cy=\"12\" r=\"1\"></circle></svg></button></div></td></tr></table><div class=\"more-replies-container\"><a href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities/comment/63384762\" class=\"more-replies\">2 replies</a></div><div class=\"comment-list-collapser\"><div class=\"comment-list-collapser-line\"></div></div></div><div class=\"comment\"><div id=\"comment-63417221\" class=\"comment-anchor\"></div><div id=\"comment-63417221-reply\" class=\"comment-anchor\"></div><table class=\"comment-content\"><tr><td class=\"comment-head\"><div class=\"profile-hover-card-target _profileHoverCardTarget_c9bh7_50\"><div class=\"user-head\"><a href=\"https://substack.com/profile/18241816-malcolm-sharpe\"><div class=\"profile-img-wrap\"><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_66,h_66,c_fill,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad2b8fee-820f-4544-95b7-afc7a0c9cc12_144x144.png\" /><img src=\"https://substackcdn.com/image/fetch/w_66,h_66,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad2b8fee-820f-4544-95b7-afc7a0c9cc12_144x144.png\" sizes=\"100vw\" alt width=\"66\" height=\"66\" class=\"_img_16u6n_1 pencraft pc-reset\" /></picture></div></a></div></div></td><td class=\"comment-rest\"><div class=\"comment-meta\"><span class=\"commenter-name\"><div class=\"pencraft pc-display-flex pc-gap-4 pc-alignItems-center pc-reset pc-display-inline-flex\"><div class=\"profile-hover-card-target _profileHoverCardTarget_c9bh7_50\"><a href=\"https://substack.com/profile/18241816-malcolm-sharpe\"><div class=\"pencraft pc-reset _color-pub-primary-text_3axfk_204 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-semibold_3axfk_165 _reset_3axfk_1\">Malcolm Sharpe</div></a></div></div></span><a href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities/comment/63417221\" rel=\"nofollow\" native class=\"comment-timestamp\">Jul 26</a><span class=\"highlight\"><svg role=\"img\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"#000000\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M20.4134 4.58753C19.9121 4.08434 19.3164 3.68508 18.6606 3.41266C18.0047 3.14023 17.3015 3 16.5914 3C15.8812 3 15.178 3.14023 14.5222 3.41266C13.8663 3.68508 13.2707 4.08434 12.7694 4.58753L12 5.36717L11.2306 4.58753C10.7293 4.08434 10.1337 3.68508 9.47781 3.41266C8.82195 3.14023 8.11878 3 7.40863 3C6.69847 3 5.9953 3.14023 5.33944 3.41266C4.68358 3.68508 4.08793 4.08434 3.58665 4.58753C1.46832 6.70656 1.33842 10.2849 4.00631 13.0037L12 21L19.9937 13.0037C22.6616 10.2849 22.5317 6.70656 20.4134 4.58753Z\"></path></g></svg><span class=\"highlight-text\">Liked by Arvind Narayanan</span></span></div><div class=\"comment-body\"><p><span>This was clarifying, and there's another tricky issue that I'm curious to know your thoughts on, which is that policy-making requires a causal estimate of the impact of the proposed intervention, and it is unclear how &quot;P(doom)&quot; handles causality.</span></p><p><span>For the asteroid example, the causality issue is simple enough, since asteroid impacts are a natural phenomenon, so we can ignore human activity when making the estimate. But if you were to want an estimate of asteroid extinction risk that _includes_ human activity, the probability decreases: after all, if we did find a large asteroid on a collision course for Earth, we'd probably try to divert it, and there's a non-negligible chance that we'd succeed. But even if we thought that we'd certainly succeed at diverting the asteroid, it'd be incorrect to say &quot;we don't need to mitigate asteroid extinction because the probability is ~0%&quot;, because choosing not to mitigate would raise the probability. So excluding human activity is clearly the right choice.</span></p><p><span>With AI x-risk though, if we exclude human activity, there is no risk, because AI is only developed as a result of human activity. It seems like forecasters implicitly try to handle this by drawing a distinction between &quot;AI capabilities&quot; and &quot;AI safety&quot;, then imagining hypothetically increasing capabilities without increasing safety. But this hypothetical is hopelessly unrealistic: companies try to increase the controllability and reliability of their AI systems as a normal part of product development.</span></p><p><span>Even in climate change, where the risks are caused by human activity, a reasonably clean separation between business-as-usual and mitigation is possible. In the absence of any incentives for mitigation, your own CO2 emissions are irrelevant. So while it may be very hard to determine which climate-mitigation actions are net beneficial, at least we have a well-defined no-mitigation baseline to compare against.</span></p><p><span>With AI, unlike with climate, it seems hopeless to try to find a well-defined no-mitigation baseline, because, as mentioned before, having an AI system do what you want is also a key aspect of being a good product. Surely this makes the probabilistic approach to AI x-risk entirely useless.</span></p><div role=\"button\" class=\"show-all-toggle\"><div class=\"show-all-toggle-label\">Expand full comment</div></div></div><div class=\"pencraft pc-display-flex pc-gap-16 pc-paddingTop-8 pc-justifyContent-flex-start pc-alignItems-center pc-reset comment-actions _withShareButton_mhaex_5\"><span class=\"pencraft pc-reset _decoration-hover-underline_3axfk_298 _reset_3axfk_1\"><a class=\"pencraft pc-reset _link_mhaex_1 _link_1ixw5_1\"><div class=\"pencraft pc-display-flex pc-gap-6 pc-alignItems-center pc-reset\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"var(--color-secondary-themed)\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-message-circle \"><path d=\"M7.9 20A9 9 0 1 0 4 16.1L2 22Z\"></path></svg><div class=\"pencraft pc-reset _color-pub-secondary-text_3axfk_207 _line-height-20_3axfk_95 _font-meta_3axfk_131 _size-11_3axfk_35 _weight-medium_3axfk_162 _transform-uppercase_3axfk_242 _reset_3axfk_1 _meta_3axfk_442\">Reply</div></div></a></span><span class=\"pencraft pc-reset _decoration-hover-underline_3axfk_298 _reset_3axfk_1\"><a class=\"pencraft pc-reset _link_mhaex_1 _link_1ixw5_1\"><div class=\"pencraft pc-display-flex pc-gap-6 pc-alignItems-center pc-reset\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"var(--color-secondary-themed)\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-share \"><path d=\"M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8\"></path><polyline points=\"16 6 12 2 8 6\"></polyline><line x1=\"12\" x2=\"12\" y1=\"2\" y2=\"15\"></line></svg><div class=\"pencraft pc-reset _color-pub-secondary-text_3axfk_207 _line-height-20_3axfk_95 _font-meta_3axfk_131 _size-11_3axfk_35 _weight-medium_3axfk_162 _transform-uppercase_3axfk_242 _reset_3axfk_1 _meta_3axfk_442\">Share</div></div></a></span><button tabIndex=\"0\" type=\"button\" id=\"trigger4167\" aria-expanded=\"false\" aria-haspopup=\"dialog\" aria-controls=\"dialog4168\" ariaLabel=\"View more\" class=\"pencraft pc-reset pencraft _iconButton_1oht6_145 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonOldColors_1oht6_56 _priority_secondary-theme_1oht6_256 _fill_empty_1oht6_317\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-more-horizontal \"><circle cx=\"12\" cy=\"12\" r=\"1\"></circle><circle cx=\"19\" cy=\"12\" r=\"1\"></circle><circle cx=\"5\" cy=\"12\" r=\"1\"></circle></svg></button></div></td></tr></table><div class=\"more-replies-container\"><a href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities/comment/63417221\" class=\"more-replies\">3 replies</a></div><div class=\"comment-list-collapser\"><div class=\"comment-list-collapser-line\"></div></div></div></div></div><a href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities/comments\" class=\"more-comments\">35 more comments...</a></div></div><div class=\"single-post-section\"><div class=\"container\"><div class=\"visibility-check\"></div><div class=\"pencraft pc-display-contents pc-reset _pubTheme_ipix0_1\"><div class=\"pencraft pc-paddingTop-24 pc-paddingBottom-24 pc-reset\"><div class=\"portable-archive empty-list\"><div class=\"pencraft pc-display-flex pc-paddingLeft-8 pc-paddingRight-8 pc-paddingBottom-16 pc-justifyContent-space-between pc-alignItems-center pc-reset\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-padding-4 pc-position-relative pc-reset _bg-secondary_17s6c_172 pc-borderRadius-full _segments_15y0g_56\"><button tabIndex=\"0\" type=\"button\" class=\"pencraft pc-display-flex pc-height-32 pc-paddingLeft-12 pc-paddingRight-12 pc-justifyContent-center pc-alignItems-center pc-cursor-pointer pc-reset _bg-unset_17s6c_166 _border-unset_17s6c_21 _sizing-border-box_17s6c_274 pencraft _segment_15y0g_56 _active_15y0g_26\"><div class=\"pencraft pc-reset _line-height-20_3axfk_95 _font-text_3axfk_121 _size-14_3axfk_50 _weight-semibold_3axfk_165 _reset_3axfk_1 _segmentText_15y0g_66 _active_15y0g_26\">Top</div></button><button tabIndex=\"0\" type=\"button\" class=\"pencraft pc-display-flex pc-height-32 pc-paddingLeft-12 pc-paddingRight-12 pc-justifyContent-center pc-alignItems-center pc-cursor-pointer pc-reset _bg-unset_17s6c_166 _border-unset_17s6c_21 _sizing-border-box_17s6c_274 pencraft _segment_15y0g_56\"><div class=\"pencraft pc-reset _line-height-20_3axfk_95 _font-text_3axfk_121 _size-14_3axfk_50 _weight-semibold_3axfk_165 _reset_3axfk_1 _segmentText_15y0g_66\">Latest</div></button><button tabIndex=\"0\" type=\"button\" class=\"pencraft pc-display-flex pc-height-32 pc-paddingLeft-12 pc-paddingRight-12 pc-justifyContent-center pc-alignItems-center pc-cursor-pointer pc-reset _bg-unset_17s6c_166 _border-unset_17s6c_21 _sizing-border-box_17s6c_274 pencraft _segment_15y0g_56\"><div class=\"pencraft pc-reset _line-height-20_3axfk_95 _font-text_3axfk_121 _size-14_3axfk_50 _weight-semibold_3axfk_165 _reset_3axfk_1 _segmentText_15y0g_66\">Discussions</div></button><div class=\"pencraft pc-display-flex pc-height-32 pc-boxShadow-sm pc-position-absolute pc-reset _bg-primary_17s6c_169 pc-borderRadius-full _segmentPill_15y0g_81\"></div></div><button tabIndex=\"0\" type=\"button\" class=\"pencraft pc-reset pencraft _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_md_1oht6_127 _priority_tertiary_1oht6_69\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-search \"><circle cx=\"11\" cy=\"11\" r=\"8\"></circle><path d=\"m21 21-4.3-4.3\"></path></svg></button></div><div class=\"portable-archive-list\"><p class=\"portable-archive-empty\">No posts</p></div></div></div></div></div></div><div class=\"visibility-check\"></div><div class=\"subscribe-footer\"><div class=\"container\"><p>Ready for more?</p><div class=\"pencraft pc-display-flex pc-justifyContent-center pc-reset\"><div><div class=\"_container_11q5m_1\"><form action=\"/api/v1/free?nojs=true\" method=\"post\" noValidate class=\"form _form_11q5m_6\"><input type=\"hidden\" name=\"first_url\" value /><input type=\"hidden\" name=\"first_referrer\" value /><input type=\"hidden\" name=\"current_url\" /><input type=\"hidden\" name=\"current_referrer\" /><input type=\"hidden\" name=\"referral_code\" /><input type=\"hidden\" name=\"source\" value=\"subscribe_footer\" /><input type=\"hidden\" name=\"referring_pub_id\" /><input type=\"hidden\" name=\"additional_referring_pub_ids\" /><div class=\"_sideBySideWrap_11q5m_10\"><div class=\"_emailInputWrapper_11q5m_57\"><input type=\"email\" name=\"email\" placeholder=\"Type your email...\" class=\"pencraft _emailInput_11q5m_23 _emailInputOnAccentBackground_11q5m_49\" /></div><button tabIndex=\"0\" type=\"submit\" class=\"button rightButton primary subscribe-btn _button_11q5m_76 _buttonOnAccentBackground_11q5m_89\"><span class=\"button-text \">Subscribe</span></button></div><div id=\"error-container\"></div></form></div></div></div></div></div></div></div><div class=\"footer-wrap publication-footer\"><div class=\"visibility-check\"></div><div class=\"footer themed-background\"><div class=\"container\"><div class=\"footer-blurbs\"><div class=\"footer-copyright-blurb\">© 2024 Sayash Kapoor and Arvind Narayanan</div><div class=\"footer-terms-blurb\"><a href=\"https://substack.com/privacy\" target=\"_blank\" rel=\"noopener noreferrer\">Privacy</a><span> ∙ </span><a href=\"https://substack.com/tos\" target=\"_blank\" rel=\"noopener noreferrer\">Terms</a><span> ∙ </span><a href=\"https://substack.com/ccpa#personal-data-collected\" target=\"_blank\" rel=\"noopener noreferrer\">Collection notice</a></div></div><div class=\"footer-buttons\"><a native href=\"https://substack.com/signup?utm_source=substack&amp;utm_medium=web&amp;utm_content=footer\" class=\"footer-substack-cta start-publishing\"><svg role=\"img\" width=\"1000\" height=\"1000\" viewBox=\"0 0 1000 1000\" fill=\"#FF6719\" stroke-width=\"1.8\" stroke=\"none\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M764.166 348.371H236.319V419.402H764.166V348.371Z\"></path><path d=\"M236.319 483.752V813.999L500.231 666.512L764.19 813.999V483.752H236.319Z\"></path><path d=\"M764.166 213H236.319V284.019H764.166V213Z\"></path></g></svg> Start Writing</a><a native href=\"https://substack.com/app/app-store-redirect?utm_campaign=app-marketing&amp;utm_content=web-footer-button\" class=\"footer-substack-cta get-the-app no-icon\">Get the app</a></div><div translated class=\"pencraft pc-reset _reset_3axfk_1 footer-slogan-blurb\"><a href=\"https://substack.com\" native>Substack</a> is the home for great culture</div></div></div></div></div><div inert role=\"dialog\" class=\"modal typography out gone share-dialog popup\"><div class=\"modal-table\"><div class=\"modal-row\"><div class=\"modal-cell modal-content no-fullscreen\"><div class=\"container\"><button tabIndex=\"0\" type=\"button\" data-testid=\"close-modal\" class=\"pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin _iconButton2_1oht6_625 _iconButtonBase_1oht6_145 _buttonBase_1oht6_1 _buttonNew_1oht6_83 _size_md_1oht6_127 _priority_tertiary_1oht6_69\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"20\" height=\"20\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"secondary\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-x \"><path d=\"M18 6 6 18\"></path><path d=\"m6 6 12 12\"></path></svg></button><div class=\"share-dialog-title\">Share</div><div class=\"pencraft pc-display-flex pc-flexDirection-column pc-gap-32 pc-paddingLeft-24 pc-paddingRight-24 pc-paddingTop-32 pc-paddingBottom-48 pc-reset\"><div class=\"pencraft pc-display-flex pc-gap-8 pc-justifyContent-space-between pc-reset share-dialog-buttons-wrapper\"><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"20\" height=\"16\" viewBox=\"0 0 20 16\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M12.1303 0.000379039C10.9833 -0.00959082 9.87819 0.431464 9.05309 1.22855L9.04556 1.23593L7.79145 2.48303C7.50587 2.767 7.50453 3.22877 7.78844 3.51441C8.07235 3.80004 8.53401 3.80139 8.81959 3.51741L10.0698 2.27423C10.6194 1.74503 11.3546 1.45229 12.1177 1.45892C12.8824 1.46556 13.6139 1.77236 14.1546 2.31323C14.6954 2.8541 15.0021 3.58577 15.0087 4.35065C15.0154 5.11353 14.7229 5.84857 14.1943 6.39829L12.0116 8.58145L12.0115 8.58155C11.7159 8.87739 11.36 9.10617 10.9682 9.25237C10.5764 9.39857 10.1577 9.45878 9.74051 9.42889C9.32337 9.39901 8.91752 9.27975 8.55051 9.07918C8.1835 8.87862 7.8639 8.60146 7.6134 8.26649C7.3722 7.94396 6.91526 7.87807 6.5928 8.11933C6.27034 8.36059 6.20447 8.81763 6.44567 9.14016C6.82142 9.64261 7.30082 10.0584 7.85134 10.3592C8.40186 10.66 9.01062 10.8389 9.63634 10.8838C10.2621 10.9286 10.8901 10.8383 11.4779 10.619C12.0656 10.3997 12.5994 10.0565 13.0429 9.61274L15.2302 7.42494L15.2391 7.4159C16.036 6.59062 16.4769 5.48529 16.467 4.33797C16.457 3.19066 15.9969 2.09316 15.1858 1.28185C14.3746 0.470545 13.2774 0.0103489 12.1303 0.000379039ZM7.29806 5.11625C6.67234 5.07142 6.0443 5.16173 5.45654 5.38103C4.86882 5.60031 4.33502 5.94355 3.89153 6.38727L1.70423 8.57506L1.69534 8.5841C0.898438 9.40939 0.457483 10.5147 0.467451 11.662C0.477418 12.8094 0.937512 13.9069 1.74864 14.7182C2.55976 15.5295 3.65701 15.9897 4.80407 15.9996C5.95113 16.0096 7.05622 15.5685 7.88132 14.7715L7.89035 14.7626L9.13717 13.5155C9.42192 13.2307 9.42192 12.7689 9.13717 12.4841C8.85243 12.1993 8.39077 12.1993 8.10602 12.4841L6.86392 13.7265C6.31432 14.2552 5.57945 14.5477 4.81675 14.5411C4.05204 14.5344 3.32054 14.2276 2.77979 13.6868C2.23904 13.1459 1.93231 12.4142 1.92566 11.6494C1.91904 10.8865 2.21146 10.1514 2.74011 9.60172L4.92287 7.41846C5.21854 7.12262 5.57437 6.89384 5.96621 6.74763C6.35805 6.60143 6.77674 6.54123 7.19389 6.57111C7.61104 6.601 8.01688 6.72026 8.38389 6.92082C8.75091 7.12138 9.0705 7.39855 9.32101 7.73352C9.56221 8.05605 10.0191 8.12194 10.3416 7.88068C10.6641 7.63942 10.7299 7.18238 10.4887 6.85985C10.113 6.3574 9.63359 5.94165 9.08307 5.64081C8.53255 5.33997 7.92378 5.16107 7.29806 5.11625Z\"></path></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Copy link</div></button><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"16\" height=\"17\" viewBox=\"0 0 16 17\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M10.6543 1.38723C10.3533 0.960814 9.95383 0.61341 9.48976 0.374567C9.02902 0.137956 8.51908 0.0130716 8.00115 0.0100098C7.86087 0.0101844 7.72354 0.0502687 7.60519 0.125581C7.48684 0.200893 7.39237 0.308324 7.3328 0.435326L5.00368 5.67077H3.029C2.72335 5.66964 2.42059 5.73003 2.13876 5.84833C1.85692 5.96663 1.60177 6.14043 1.38849 6.35938C1.16707 6.57502 0.991841 6.83346 0.873459 7.11897C0.755078 7.40447 0.696022 7.71108 0.699885 8.02014V13.691C0.699885 14.3087 0.945273 14.9012 1.38207 15.338C1.81886 15.7747 2.41128 16.0201 3.029 16.0201H13.348C13.8951 16.021 14.425 15.8283 14.8438 15.4762C15.2626 15.1241 15.5434 14.6352 15.6366 14.0961L16.6493 8.4252C16.7252 8.09192 16.7252 7.74582 16.6493 7.41254C16.566 7.08205 16.4104 6.7742 16.1936 6.51128C15.9746 6.25 15.7017 6.03926 15.3936 5.89355C15.0762 5.7467 14.7306 5.67068 14.3809 5.67077H10.5328L11.0391 4.37457C11.2397 3.88784 11.3162 3.35894 11.2619 2.83533C11.1853 2.30894 10.9763 1.81065 10.6543 1.38723ZM4.75052 14.5518H3.029C2.91049 14.5525 2.79303 14.5296 2.68349 14.4844C2.57394 14.4392 2.47452 14.3726 2.39102 14.2885C2.23609 14.1199 2.14945 13.8997 2.14799 13.6708V8.02014C2.14913 7.901 2.17389 7.78328 2.22082 7.67377C2.26775 7.56427 2.33592 7.46515 2.4214 7.38216C2.50369 7.29576 2.60267 7.22698 2.71233 7.17998C2.822 7.13298 2.94007 7.10874 3.05938 7.10874H4.7809L4.75052 14.5518ZM10.6746 7.05811H14.3809C14.5145 7.05821 14.6462 7.08942 14.7657 7.14925C14.8875 7.20532 14.9948 7.28845 15.0796 7.39229C15.1675 7.49052 15.2301 7.60871 15.2619 7.73659C15.2922 7.8665 15.2922 8.00162 15.2619 8.13153L14.2493 13.8024C14.2087 14.017 14.094 14.2106 13.9252 14.3492C13.7619 14.4812 13.558 14.5528 13.348 14.5518H6.19862V6.45052L8.43659 1.38723H8.52773C8.9042 1.50037 9.23304 1.73413 9.4636 2.05252C9.69416 2.37092 9.81365 2.75627 9.80368 3.14925C9.8181 3.39741 9.78015 3.64583 9.69229 3.87836L9.23659 5.04292C9.15397 5.273 9.12623 5.51921 9.15558 5.76191C9.1877 6.00427 9.27425 6.23623 9.40875 6.44039C9.5535 6.6376 9.74028 6.80017 9.95558 6.91634C10.1774 7.03206 10.4244 7.0912 10.6746 7.08849V7.05811Z\"></path></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Facebook</div></button><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"21\" height=\"16\" viewBox=\"0 0 21 16\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M2.22192 2.20503C2.36754 1.77115 2.78269 1.45455 3.26639 1.45455H17.9332C18.4169 1.45455 18.8321 1.77118 18.9777 2.2051L10.5999 8.02107L2.22192 2.20503ZM2.16639 3.94198V13.4545C2.16639 14.0529 2.66307 14.5455 3.26639 14.5455H17.9332C18.5365 14.5455 19.0332 14.0529 19.0332 13.4545V3.94206L11.0204 9.50462C10.7679 9.67991 10.4318 9.67991 10.1793 9.50462L2.16639 3.94198ZM20.4999 2.55809V13.4545C20.4999 14.8562 19.3465 16 17.9332 16H3.26639C1.85304 16 0.699707 14.8562 0.699707 13.4545V2.54545C0.699707 1.14379 1.85304 0 3.26639 0H17.9332C19.3407 0 20.4904 1.13441 20.4998 2.52818C20.5 2.53816 20.5001 2.54813 20.4999 2.55809Z\"></path></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Email</div></button><button tabIndex=\"0\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><path d=\"M6.785 1.92766C5.45134 1.57031 4.08049 2.36176 3.72314 3.69543L0.444815 15.9303C0.0874636 17.264 0.878901 18.6348 2.21255 18.9922L5.37495 19.8396V7.66664C5.37495 6.40099 6.40096 5.37498 7.66661 5.37498H19.4723C19.3299 5.30548 19.1788 5.24858 19.0201 5.20604L6.785 1.92766Z\" stroke=\"none\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M8.44161 7.4C7.86632 7.4 7.39995 7.86637 7.39995 8.44167V22.1081C7.39995 22.6834 7.86631 23.1498 8.4416 23.1498L22.1083 23.15C22.6836 23.15 23.1499 22.6836 23.1499 22.1083V8.44167C23.1499 7.86637 22.6836 7.4 22.1083 7.4H8.44161ZM10.3999 9.65C9.84766 9.65 9.39995 10.0977 9.39995 10.65C9.39995 11.2023 9.84766 11.65 10.3999 11.65H18.3999C18.9522 11.65 19.3999 11.2023 19.3999 10.65C19.3999 10.0977 18.9522 9.65 18.3999 9.65H10.3999ZM10.3999 14.15C9.84766 14.15 9.39995 14.5977 9.39995 15.15C9.39995 15.7023 9.84766 16.15 10.3999 16.15H15.3999C15.9522 16.15 16.3999 15.7023 16.3999 15.15C16.3999 14.5977 15.9522 14.15 15.3999 14.15H10.3999Z\" stroke=\"none\"></path></g></svg></div><div class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Note</div></button><button tabIndex=\"0\" id=\"trigger4169\" aria-expanded=\"false\" aria-haspopup=\"dialog\" aria-controls=\"dialog4170\" ariaLabel=\"View more\" type=\"button\" class=\"button share-action\"><div class=\"action-icon\"><svg role=\"img\" width=\"100\" height=\"100\" viewBox=\"0 0 100 100\" fill=\"none\" stroke-width=\"1.8\" stroke=\"#000\" xmlns=\"http://www.w3.org/2000/svg\"><g><title></title><circle cx=\"23\" cy=\"50\" r=\"9\"></circle><circle cx=\"50\" cy=\"50\" r=\"9\"></circle><circle cx=\"77\" cy=\"50\" r=\"9\"></circle></g></svg></div><div translated class=\"pencraft pc-reset _color-secondary_3axfk_186 _line-height-20_3axfk_95 _font-text_3axfk_121 _size-13_3axfk_45 _weight-regular_3axfk_159 _reset_3axfk_1\">Other</div></button></div></div></div></div></div></div></div><div style=\"left: auto; right: 16px; bottom: 16px; z-index: 999; transform: translateY(0px);\" class=\"pencraft pc-position-fixed pc-reset _sizing-border-box_17s6c_274\"></div><div></div>\n        </div>\n\n        \n            <script src=\"https://js.sentry-cdn.com/6c2ff3e3828e4017b7faf7b63e24cdf8.min.js\" crossorigin=\"anonymous\"></script>\n            <script>\n                window.Sentry && window.Sentry.onLoad(function() {\n                    window.Sentry.init({\n                        environment: window._preloads.sentry_environment,\n                        dsn: window._preloads.sentry_dsn,\n                    })\n                })\n            </script>\n        \n\n\n        \n        \n        \n        <script>window._preloads        = JSON.parse(\"{\\\"isEU\\\":false,\\\"language\\\":\\\"en\\\",\\\"country\\\":\\\"US\\\",\\\"base_url\\\":\\\"https://www.aisnakeoil.com\\\",\\\"stripe_publishable_key\\\":\\\"pk_live_vNnuGHOFnt4mM7V9PuCAAPJz\\\",\\\"captcha_site_key\\\":\\\"6LdYbsYZAAAAAIFIRh8X_16GoFRLIReh-e-q6qSa\\\",\\\"pub\\\":{\\\"apple_pay_disabled\\\":false,\\\"apex_domain\\\":null,\\\"author_id\\\":891603,\\\"byline_images_enabled\\\":true,\\\"bylines_enabled\\\":true,\\\"chartable_token\\\":null,\\\"community_enabled\\\":true,\\\"copyright\\\":\\\"Sayash Kapoor and Arvind Narayanan\\\",\\\"cover_photo_url\\\":\\\"https://substack-post-media.s3.amazonaws.com/public/images/ef9f061f-abb9-4ced-8e12-009e0105f601_600x600.png\\\",\\\"created_at\\\":\\\"2022-07-19T16:17:19.439Z\\\",\\\"custom_domain_optional\\\":false,\\\"custom_domain\\\":\\\"www.aisnakeoil.com\\\",\\\"custom_publication_theme_id\\\":null,\\\"default_comment_sort\\\":\\\"best_first\\\",\\\"default_coupon\\\":null,\\\"default_group_coupon\\\":null,\\\"default_show_guest_bios\\\":true,\\\"email_banner_url\\\":null,\\\"email_from_name\\\":\\\"Sayash and Arvind from AI Snake Oil\\\",\\\"email_from\\\":null,\\\"embed_tracking_disabled\\\":false,\\\"explicit\\\":false,\\\"expose_paywall_content_to_search_engines\\\":true,\\\"fb_pixel_id\\\":null,\\\"fb_site_verification_token\\\":null,\\\"flagged_as_spam\\\":false,\\\"founding_subscription_benefits\\\":null,\\\"free_subscription_benefits\\\":null,\\\"ga_pixel_id\\\":null,\\\"google_site_verification_token\\\":null,\\\"google_tag_manager_token\\\":null,\\\"hero_image\\\":null,\\\"hero_text\\\":\\\"What Artificial Intelligence Can Do, What It Can\\u2019t, and How to Tell the Difference\\\",\\\"hide_intro_subtitle\\\":null,\\\"hide_intro_title\\\":null,\\\"hide_podcast_feed_link\\\":false,\\\"homepage_type\\\":\\\"newspaper\\\",\\\"id\\\":1008003,\\\"image_thumbnails_always_enabled\\\":false,\\\"invite_only\\\":false,\\\"language\\\":\\\"en\\\",\\\"logo_url_wide\\\":\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74fcca0d-74fc-4658-8b78-894d92a24750_1344x256.png\\\",\\\"logo_url\\\":\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png\\\",\\\"minimum_group_size\\\":2,\\\"moderation_enabled\\\":true,\\\"name\\\":\\\"AI Snake Oil\\\",\\\"paid_subscription_benefits\\\":null,\\\"parsely_pixel_id\\\":null,\\\"payments_state\\\":\\\"disabled\\\",\\\"paywall_free_trial_enabled\\\":false,\\\"podcast_art_url\\\":null,\\\"paid_podcast_episode_art_url\\\":null,\\\"podcast_byline\\\":null,\\\"podcast_description\\\":null,\\\"podcast_enabled\\\":false,\\\"podcast_feed_url\\\":null,\\\"podcast_title\\\":null,\\\"post_preview_limit\\\":null,\\\"require_clickthrough\\\":false,\\\"rss_feed_url\\\":null,\\\"rss_website_url\\\":null,\\\"show_pub_podcast_tab\\\":false,\\\"show_recs_on_homepage\\\":true,\\\"subdomain\\\":\\\"aisnakeoil\\\",\\\"subscriber_invites\\\":0,\\\"support_email\\\":null,\\\"theme_var_background_pop\\\":\\\"#B599F1\\\",\\\"theme_var_color_links\\\":true,\\\"theme_var_cover_bg_color\\\":null,\\\"trial_end_override\\\":null,\\\"twitter_pixel_id\\\":null,\\\"type\\\":\\\"newsletter\\\",\\\"post_reaction_faces_enabled\\\":true,\\\"is_personal_mode\\\":false,\\\"plans\\\":null,\\\"stripe_user_id\\\":null,\\\"stripe_country\\\":null,\\\"stripe_publishable_key\\\":null,\\\"automatic_tax_enabled\\\":null,\\\"author_name\\\":\\\"Sayash Kapoor\\\",\\\"author_handle\\\":\\\"sayash\\\",\\\"author_photo_url\\\":\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f87ce8-8dbc-468f-8f8b-9fbf430e323c_976x974.png\\\",\\\"author_bio\\\":\\\"CS PhD candidate at Princeton. I study the societal impact of AI. Currently writing a book on AI Snake Oil: http://aisnakeoil.com \\\",\\\"has_custom_tos\\\":false,\\\"has_custom_privacy\\\":false,\\\"theme\\\":{\\\"background_pop_color\\\":\\\"#fd6752\\\",\\\"web_bg_color\\\":\\\"#ffffff\\\",\\\"cover_bg_color\\\":\\\"#ffffff\\\",\\\"publication_id\\\":1008003,\\\"color_links\\\":null,\\\"font_preset_heading\\\":\\\"heavy_sans\\\",\\\"font_preset_body\\\":\\\"fancy_serif\\\",\\\"font_family_headings\\\":null,\\\"font_family_body\\\":null,\\\"font_family_ui\\\":null,\\\"font_size_body_desktop\\\":null,\\\"print_secondary\\\":null,\\\"custom_css_web\\\":null,\\\"custom_css_email\\\":null,\\\"home_hero\\\":\\\"newspaper\\\",\\\"home_posts\\\":\\\"custom\\\",\\\"home_show_top_posts\\\":false,\\\"hide_images_from_list\\\":false,\\\"home_hero_alignment\\\":\\\"left\\\",\\\"home_hero_show_podcast_links\\\":true},\\\"threads_v2_settings\\\":null,\\\"default_group_coupon_percent_off\\\":null,\\\"pause_return_date\\\":null,\\\"has_posts\\\":true,\\\"has_recommendations\\\":false,\\\"first_post_date\\\":\\\"2022-08-25T15:33:52.616Z\\\",\\\"has_podcast\\\":false,\\\"has_free_podcast\\\":false,\\\"has_subscriber_only_podcast\\\":false,\\\"has_community_content\\\":true,\\\"twitter_share_on_publish_opt_in\\\":false,\\\"twitter_permissions\\\":\\\"read\\\",\\\"rankingDetail\\\":\\\"Launched 2 years ago\\\",\\\"rankingDetailFreeIncluded\\\":\\\"Tens of thousands of subscribers\\\",\\\"rankingDetailOrderOfMagnitude\\\":0,\\\"rankingDetailFreeIncludedOrderOfMagnitude\\\":10000,\\\"rankingDetailFreeSubscriberCount\\\":\\\"Over 35,000 subscribers\\\",\\\"rankingDetailByLanguage\\\":{\\\"de\\\":{\\\"rankingDetail\\\":\\\"Vor vor 2 Jahren gelauncht\\\",\\\"rankingDetailFreeIncluded\\\":\\\"Zehntausende von Abonnenten\\\",\\\"rankingDetailOrderOfMagnitude\\\":0,\\\"rankingDetailFreeIncludedOrderOfMagnitude\\\":10000,\\\"rankingDetailFreeSubscriberCount\\\":\\\"\\u00DCber 35,000 Abonnenten\\\",\\\"freeSubscriberCount\\\":\\\"35,000\\\",\\\"freeSubscriberCountOrderOfMagnitude\\\":\\\"35K+\\\"},\\\"es\\\":{\\\"rankingDetail\\\":\\\"Lanzado hace 2 a\\u00F1os\\\",\\\"rankingDetailFreeIncluded\\\":\\\"Decenas de miles de suscriptores\\\",\\\"rankingDetailOrderOfMagnitude\\\":0,\\\"rankingDetailFreeIncludedOrderOfMagnitude\\\":10000,\\\"rankingDetailFreeSubscriberCount\\\":\\\"M\\u00E1s de 35,000 suscriptores\\\",\\\"freeSubscriberCount\\\":\\\"35,000\\\",\\\"freeSubscriberCountOrderOfMagnitude\\\":\\\"35K+\\\"},\\\"fr\\\":{\\\"rankingDetail\\\":\\\"Lanc\\u00E9 il y a 2 ann\\u00E9es\\\",\\\"rankingDetailFreeIncluded\\\":\\\"Des dizaines de milliers d'abonn\\u00E9s\\\",\\\"rankingDetailOrderOfMagnitude\\\":0,\\\"rankingDetailFreeIncludedOrderOfMagnitude\\\":10000,\\\"rankingDetailFreeSubscriberCount\\\":\\\"Plus de 35,000 abonn\\u00E9s\\\",\\\"freeSubscriberCount\\\":\\\"35,000\\\",\\\"freeSubscriberCountOrderOfMagnitude\\\":\\\"35K+\\\"},\\\"pt\\\":{\\\"rankingDetail\\\":\\\"Lan\\u00E7ado 2 anos\\\",\\\"rankingDetailFreeIncluded\\\":\\\"Dezenas de milhares de subscritores\\\",\\\"rankingDetailOrderOfMagnitude\\\":0,\\\"rankingDetailFreeIncludedOrderOfMagnitude\\\":10000,\\\"rankingDetailFreeSubscriberCount\\\":\\\"Mais de 35,000 subscritores\\\",\\\"freeSubscriberCount\\\":\\\"35,000\\\",\\\"freeSubscriberCountOrderOfMagnitude\\\":\\\"35K+\\\"},\\\"pt-br\\\":{\\\"rankingDetail\\\":\\\"Lan\\u00E7ado 2 anos\\\",\\\"rankingDetailFreeIncluded\\\":\\\"Dezenas de milhares de assinantes\\\",\\\"rankingDetailOrderOfMagnitude\\\":0,\\\"rankingDetailFreeIncludedOrderOfMagnitude\\\":10000,\\\"rankingDetailFreeSubscriberCount\\\":\\\"Mais de 35,000 assinantes\\\",\\\"freeSubscriberCount\\\":\\\"35,000\\\",\\\"freeSubscriberCountOrderOfMagnitude\\\":\\\"35K+\\\"},\\\"it\\\":{\\\"rankingDetail\\\":\\\"Lanciato 2 anni\\\",\\\"rankingDetailFreeIncluded\\\":\\\"Decine di migliaia di abbonati\\\",\\\"rankingDetailOrderOfMagnitude\\\":0,\\\"rankingDetailFreeIncludedOrderOfMagnitude\\\":10000,\\\"rankingDetailFreeSubscriberCount\\\":\\\"Oltre 35,000 abbonati\\\",\\\"freeSubscriberCount\\\":\\\"35,000\\\",\\\"freeSubscriberCountOrderOfMagnitude\\\":\\\"35K+\\\"},\\\"en\\\":{\\\"rankingDetail\\\":\\\"Launched 2 years ago\\\",\\\"rankingDetailFreeIncluded\\\":\\\"Tens of thousands of subscribers\\\",\\\"rankingDetailOrderOfMagnitude\\\":0,\\\"rankingDetailFreeIncludedOrderOfMagnitude\\\":10000,\\\"rankingDetailFreeSubscriberCount\\\":\\\"Over 35,000 subscribers\\\",\\\"freeSubscriberCount\\\":\\\"35,000\\\",\\\"freeSubscriberCountOrderOfMagnitude\\\":\\\"35K+\\\"}},\\\"freeSubscriberCount\\\":\\\"35,000\\\",\\\"freeSubscriberCountOrderOfMagnitude\\\":\\\"35K+\\\",\\\"author_bestseller_tier\\\":0,\\\"disable_monthly_subscriptions\\\":false,\\\"disable_annual_subscriptions\\\":false,\\\"hide_post_restacks\\\":false,\\\"notes_feed_enabled\\\":false,\\\"no_follow\\\":false,\\\"paywall_chat\\\":\\\"free\\\",\\\"sections\\\":[],\\\"multipub_migration\\\":null,\\\"navigationBarItems\\\":[{\\\"id\\\":\\\"e2e0ea54-d6ba-49f4-af06-2ed2a7b33522\\\",\\\"publication_id\\\":1008003,\\\"sibling_rank\\\":0,\\\"link_title\\\":null,\\\"link_url\\\":null,\\\"section_id\\\":null,\\\"post_id\\\":null,\\\"is_hidden\\\":true,\\\"standard_key\\\":\\\"archive\\\",\\\"post_tag_id\\\":null,\\\"section\\\":null,\\\"postTag\\\":null,\\\"post\\\":null},{\\\"id\\\":\\\"d133957a-b2ee-418b-92e2-dacbce2cc55b\\\",\\\"publication_id\\\":1008003,\\\"sibling_rank\\\":0,\\\"link_title\\\":null,\\\"link_url\\\":null,\\\"section_id\\\":null,\\\"post_id\\\":null,\\\"is_hidden\\\":true,\\\"standard_key\\\":\\\"about\\\",\\\"post_tag_id\\\":null,\\\"section\\\":null,\\\"postTag\\\":null,\\\"post\\\":null},{\\\"id\\\":\\\"14647d34-f5fa-4c12-8794-14b8eb8aeb63\\\",\\\"publication_id\\\":1008003,\\\"sibling_rank\\\":2,\\\"link_title\\\":\\\"Start here\\\",\\\"link_url\\\":\\\"\\\",\\\"section_id\\\":null,\\\"post_id\\\":146768501,\\\"is_hidden\\\":null,\\\"standard_key\\\":null,\\\"post_tag_id\\\":null,\\\"section\\\":null,\\\"postTag\\\":null,\\\"post\\\":{\\\"id\\\":146768501,\\\"publication_id\\\":1008003,\\\"is_published\\\":true,\\\"title\\\":\\\"The AI Snake Oil newsletter at a glance\\\",\\\"body\\\":\\\"s3://substack-content/post/146768501/2024-07-25T20-15-27-377Z/19265909/b6d7c91aa4f13ed31dffe8b6f3f03763bf452dba\\\",\\\"slug\\\":\\\"the-ai-snake-oil-newsletter-at-a\\\",\\\"post_date\\\":\\\"2024-07-19T15:51:19.175Z\\\",\\\"draft_title\\\":\\\"The AI Snake Oil newsletter at a glance\\\",\\\"draft_body\\\":\\\"s3://substack-content/post/146768501/2024-07-25T20-15-27-377Z/19265909/b6d7c91aa4f13ed31dffe8b6f3f03763bf452dba\\\",\\\"draft_updated_at\\\":\\\"2024-07-25T20:15:27.508Z\\\",\\\"subtitle\\\":\\\"\\\",\\\"draft_subtitle\\\":\\\"\\\",\\\"email_sent_at\\\":null,\\\"audience\\\":\\\"everyone\\\",\\\"type\\\":\\\"page\\\",\\\"podcast_url\\\":\\\"\\\",\\\"draft_podcast_url\\\":\\\"\\\",\\\"podcast_duration\\\":null,\\\"draft_podcast_duration\\\":null,\\\"podcast_art_url\\\":null,\\\"podcast_description\\\":null,\\\"podcast_subtitle\\\":null,\\\"explicit\\\":null,\\\"podcast_content\\\":null,\\\"podcast_guid\\\":null,\\\"social_title\\\":null,\\\"description\\\":null,\\\"cover_image\\\":null,\\\"imported_podcast_url\\\":null,\\\"imported_podcast_art_url\\\":null,\\\"uuid\\\":\\\"82184f72-3953-4474-9bdd-43684db0e64c\\\",\\\"write_comment_permissions\\\":\\\"everyone\\\",\\\"should_send_email\\\":false,\\\"default_comment_sort\\\":null,\\\"search_engine_title\\\":null,\\\"search_engine_description\\\":null,\\\"updated_at\\\":\\\"2024-07-25T20:15:29.802Z\\\",\\\"canonical_url\\\":null,\\\"subscriber_set_id\\\":null,\\\"section_id\\\":null,\\\"section_chosen\\\":false,\\\"draft_section_id\\\":null,\\\"show_guest_bios\\\":true,\\\"reply_to_post_id\\\":null,\\\"should_send_free_preview\\\":false,\\\"word_count\\\":2632,\\\"video_upload_id\\\":null,\\\"draft_video_upload_id\\\":null,\\\"draft_created_at\\\":\\\"2024-07-18T20:44:44.744Z\\\",\\\"podcast_upload_id\\\":null,\\\"draft_podcast_upload_id\\\":null,\\\"voiceover_upload_id\\\":null,\\\"draft_voiceover_upload_id\\\":null,\\\"free_unlock_required\\\":false,\\\"podcast_preview_upload_id\\\":null,\\\"draft_podcast_preview_upload_id\\\":null,\\\"legacy_podcast_file_size\\\":null,\\\"syndicate_voiceover_to_rss\\\":false,\\\"audience_before_archived\\\":null,\\\"should_send_stats_email\\\":true,\\\"exempt_from_archive_paywall\\\":false,\\\"has_explicit_paywall\\\":false,\\\"inbox_sent_at\\\":null,\\\"editor_v2\\\":false,\\\"teaser_post_eligible\\\":true,\\\"has_dismissed_tk_warning\\\":false}},{\\\"id\\\":\\\"2204fe8a-2f39-4505-b6ba-fcccad8668de\\\",\\\"publication_id\\\":1008003,\\\"sibling_rank\\\":2,\\\"link_title\\\":\\\"Get the book\\\",\\\"link_url\\\":\\\"\\\",\\\"section_id\\\":null,\\\"post_id\\\":143434798,\\\"is_hidden\\\":null,\\\"standard_key\\\":null,\\\"post_tag_id\\\":null,\\\"section\\\":null,\\\"postTag\\\":null,\\\"post\\\":{\\\"id\\\":143434798,\\\"publication_id\\\":1008003,\\\"is_published\\\":true,\\\"title\\\":\\\"AI Snake Oil is now available to preorder\\\",\\\"body\\\":\\\"s3://substack-content/post/143434798/2024-08-06T19-54-49-786Z/891603/e0df424a4686aafbeee9805cef916bc26335fbc7\\\",\\\"slug\\\":\\\"ai-snake-oil-is-now-available-to\\\",\\\"post_date\\\":\\\"2024-04-10T12:55:34.425Z\\\",\\\"draft_title\\\":\\\"AI Snake Oil is now available to preorder\\\",\\\"draft_body\\\":\\\"s3://substack-content/post/143434798/2024-08-06T19-54-49-786Z/891603/e0df424a4686aafbeee9805cef916bc26335fbc7\\\",\\\"draft_updated_at\\\":\\\"2024-08-06T19:54:49.848Z\\\",\\\"subtitle\\\":\\\"What artificial intelligence can do, what it can't, and how to tell the difference\\\",\\\"draft_subtitle\\\":\\\"What artificial intelligence can do, what it can't, and how to tell the difference\\\",\\\"email_sent_at\\\":\\\"2024-04-10T12:55:34.716Z\\\",\\\"audience\\\":\\\"everyone\\\",\\\"type\\\":\\\"newsletter\\\",\\\"podcast_url\\\":\\\"\\\",\\\"draft_podcast_url\\\":\\\"\\\",\\\"podcast_duration\\\":null,\\\"draft_podcast_duration\\\":null,\\\"podcast_art_url\\\":null,\\\"podcast_description\\\":null,\\\"podcast_subtitle\\\":null,\\\"explicit\\\":null,\\\"podcast_content\\\":null,\\\"podcast_guid\\\":null,\\\"social_title\\\":\\\"AI Snake Oil is now available to preorder\\\",\\\"description\\\":\\\"What artificial intelligence can do, what it can't, and how to tell the difference\\\",\\\"cover_image\\\":\\\"https://substack-post-media.s3.amazonaws.com/public/images/5457f1d0-6b19-48cb-a63f-01f32fa6c92c_4480x2520.png\\\",\\\"imported_podcast_url\\\":null,\\\"imported_podcast_art_url\\\":null,\\\"uuid\\\":\\\"47da2f3e-20dd-4386-ab50-6f3f40dde519\\\",\\\"write_comment_permissions\\\":\\\"none\\\",\\\"should_send_email\\\":false,\\\"default_comment_sort\\\":null,\\\"search_engine_title\\\":null,\\\"search_engine_description\\\":null,\\\"updated_at\\\":\\\"2024-08-06T19:54:51.669Z\\\",\\\"canonical_url\\\":null,\\\"subscriber_set_id\\\":null,\\\"section_id\\\":null,\\\"section_chosen\\\":false,\\\"draft_section_id\\\":null,\\\"show_guest_bios\\\":true,\\\"reply_to_post_id\\\":null,\\\"should_send_free_preview\\\":false,\\\"word_count\\\":628,\\\"video_upload_id\\\":null,\\\"draft_video_upload_id\\\":null,\\\"draft_created_at\\\":\\\"2024-04-10T00:37:45.800Z\\\",\\\"podcast_upload_id\\\":null,\\\"draft_podcast_upload_id\\\":null,\\\"voiceover_upload_id\\\":null,\\\"draft_voiceover_upload_id\\\":null,\\\"free_unlock_required\\\":false,\\\"podcast_preview_upload_id\\\":null,\\\"draft_podcast_preview_upload_id\\\":null,\\\"legacy_podcast_file_size\\\":null,\\\"syndicate_voiceover_to_rss\\\":false,\\\"audience_before_archived\\\":null,\\\"should_send_stats_email\\\":true,\\\"exempt_from_archive_paywall\\\":false,\\\"has_explicit_paywall\\\":false,\\\"inbox_sent_at\\\":\\\"2024-04-10T12:55:34.716Z\\\",\\\"editor_v2\\\":false,\\\"teaser_post_eligible\\\":true,\\\"has_dismissed_tk_warning\\\":false}},{\\\"id\\\":\\\"957c39f5-3cfd-45ab-9db6-822171c9e295\\\",\\\"publication_id\\\":1008003,\\\"sibling_rank\\\":3,\\\"link_title\\\":\\\"About us\\\",\\\"link_url\\\":\\\"/about\\\",\\\"section_id\\\":null,\\\"post_id\\\":null,\\\"is_hidden\\\":null,\\\"standard_key\\\":null,\\\"post_tag_id\\\":null,\\\"section\\\":null,\\\"postTag\\\":null,\\\"post\\\":null}],\\\"contributors\\\":[{\\\"name\\\":\\\"Sayash Kapoor\\\",\\\"handle\\\":\\\"sayash\\\",\\\"role\\\":\\\"admin\\\",\\\"owner\\\":true,\\\"user_id\\\":891603,\\\"photo_url\\\":\\\"https://substack-post-media.s3.amazonaws.com/public/images/30f87ce8-8dbc-468f-8f8b-9fbf430e323c_976x974.png\\\",\\\"bio\\\":\\\"CS PhD candidate at Princeton. I study the societal impact of AI. Currently writing a book on AI Snake Oil: http://aisnakeoil.com \\\"},{\\\"name\\\":\\\"Arvind Narayanan\\\",\\\"handle\\\":\\\"arvindnarayanan\\\",\\\"role\\\":\\\"admin\\\",\\\"owner\\\":false,\\\"user_id\\\":19265909,\\\"photo_url\\\":\\\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/149ffaff-ad45-4b76-9667-bbd39d3a3d23_1795x1795.jpeg\\\",\\\"bio\\\":null}],\\\"threads_v2_enabled\\\":false,\\\"viralGiftsConfig\\\":null,\\\"tier\\\":2,\\\"no_index\\\":false,\\\"can_set_google_site_verification\\\":true,\\\"can_have_sitemap\\\":true,\\\"founding_plan_name_english\\\":\\\"Founding Member\\\",\\\"draft_plans\\\":null,\\\"base_url\\\":\\\"https://www.aisnakeoil.com\\\",\\\"hostname\\\":\\\"www.aisnakeoil.com\\\",\\\"is_on_substack\\\":false,\\\"spotify_podcast_settings\\\":null,\\\"podcastPalette\\\":{\\\"Vibrant\\\":{\\\"rgb\\\":[244.1949152542373,162.0762711864407,10.805084745762711],\\\"population\\\":0},\\\"DarkVibrant\\\":{\\\"rgb\\\":[126.9813559322034,84.27966101694916,5.618644067796606],\\\"population\\\":0},\\\"LightVibrant\\\":{\\\"rgb\\\":[250,212,142],\\\"population\\\":31},\\\"Muted\\\":{\\\"rgb\\\":[92,158,148],\\\"population\\\":4},\\\"DarkMuted\\\":{\\\"rgb\\\":[146.51694915254237,97.24576271186443,6.483050847457638],\\\"population\\\":0},\\\"LightMuted\\\":{\\\"rgb\\\":[164,204,196],\\\"population\\\":3}},\\\"pageThemes\\\":{\\\"podcast\\\":null},\\\"live_subscriber_counts\\\":false,\\\"subscribeCardVersionHash\\\":\\\"37e524f4f6d1771663f076c779d86d31\\\"},\\\"confirmedLogin\\\":false,\\\"hide_intro_popup\\\":true,\\\"block_auto_login\\\":true,\\\"domainInfo\\\":{\\\"isSubstack\\\":false,\\\"customDomain\\\":\\\"www.aisnakeoil.com\\\"},\\\"experimentFeatures\\\":{},\\\"experimentExposures\\\":{},\\\"siteConfigs\\\":{\\\"score_upsell_email\\\":\\\"control\\\",\\\"first_chat_email_enabled\\\":true,\\\"notes_video_max_duration_minutes\\\":5,\\\"reader-onboarding-promoted-pub\\\":737237,\\\"pub_creation_captcha_behavior\\\":\\\"risky_pubs\\\",\\\"new_commenter_approval\\\":false,\\\"pub_update_opennode_api_key\\\":false,\\\"note_velocity_insertion_min_ratio\\\":null,\\\"embedding_search_using_quantized_field\\\":false,\\\"zendesk_automation_cancellations\\\":false,\\\"hide_book_a_meeting_button\\\":false,\\\"note_velocity_insertion_max_ratio\\\":null,\\\"mfa_action_box_enabled\\\":false,\\\"publication_max_bylines\\\":35,\\\"no_contest_charge_disputes\\\":false,\\\"new_subscription_management\\\":false,\\\"publication_tabs_reorder\\\":false,\\\"comp_expiry_email_new_copy\\\":\\\"NONE\\\",\\\"free_unlock_required\\\":false,\\\"traffic_rule_check_enabled\\\":false,\\\"amp_emails_enabled\\\":false,\\\"enable_post_summarization\\\":false,\\\"image_deep_link_enabled\\\":false,\\\"bitcoin_enabled\\\":false,\\\"show_entire_square_image\\\":false,\\\"hide_subscriber_count\\\":false,\\\"publication_author_display_override\\\":\\\"\\\",\\\"generate_pdf_tax_report\\\":false,\\\"live_stream_email_all_subscribers\\\":true,\\\"show_generic_post_importer\\\":false,\\\"enable_pledges_modal\\\":true,\\\"include_pdf_invoice\\\":false,\\\"app_upsell_after_posting_notes\\\":\\\"experiment\\\",\\\"platform_searcher_enabled\\\":false,\\\"upload_to_youtube\\\":false,\\\"day_two_unread_top_post\\\":\\\"experiment\\\",\\\"meetings_v1\\\":false,\\\"custom_target_origin\\\":\\\"\\\",\\\"exempt_from_gtm_filter\\\":false,\\\"group_sections_and_podcasts_in_menu\\\":false,\\\"boost_optin_modal_enabled\\\":true,\\\"transcriptions_use_substrate\\\":false,\\\"trending_topics_live_mode\\\":false,\\\"post_blogspot_importer\\\":false,\\\"pub_tts_override\\\":\\\"default\\\",\\\"disable_monthly_subscriptions\\\":false,\\\"skip_welcome_email\\\":false,\\\"chat_reader_thread_notification_default\\\":false,\\\"scheduled_pinned_posts\\\":false,\\\"disable_redirect_outbound_utm_params\\\":false,\\\"reader_gift_referrals_enabled\\\":true,\\\"enable_bestseller_survey_modal\\\":false,\\\"dont_show_guest_byline\\\":false,\\\"like_comments_enabled\\\":true,\\\"extended_podcast_episode_metadata\\\":false,\\\"force_live_feed_tabs\\\":\\\"\\\",\\\"enable_author_note_email_toggle\\\":false,\\\"meetings_embed_publication_name\\\":false,\\\"no_auto_renewal_notice\\\":false,\\\"people_you_may_know_algorithm\\\":\\\"experiment\\\",\\\"welcome_screen_blurb_override\\\":\\\"\\\",\\\"post_recipients_batch_limit\\\":1000,\\\"like_posts_enabled\\\":true,\\\"notes_publication_mentions_enabled\\\":false,\\\"twitter_player_card_enabled\\\":true,\\\"show_app_upsell_subscribe_flow\\\":true,\\\"feed_promoted_user\\\":false,\\\"stripe_link_in_payment_element\\\":\\\"control\\\",\\\"section_specific_csv_imports_enabled\\\":false,\\\"improved_tts_voiceover\\\":false,\\\"bypass_profile_substack_logo_detection\\\":false,\\\"use_preloaded_player_sources\\\":false,\\\"generate_twitter_card_with_lamda\\\":false,\\\"list_pruning_enabled\\\":false,\\\"facebook_connect\\\":false,\\\"opt_in_to_sections_during_subscribe\\\":false,\\\"underlined_colored_links\\\":false,\\\"tk_reminders_enabled\\\":true,\\\"max_image_upload_mb\\\":32,\\\"modal_rec_variant_user\\\":\\\"control\\\",\\\"extract_stripe_receipt_url\\\":false,\\\"live_stream_notifications_disbled\\\":false,\\\"enable_android_dms_writer_beta\\\":false,\\\"enable_chat_inbox_pins\\\":false,\\\"threads_suggested_ios_version\\\":null,\\\"pledges_disabled\\\":false,\\\"threads_minimum_ios_version\\\":812,\\\"hide_podcast_email_setup_link\\\":false,\\\"subscribe_captcha_behavior\\\":\\\"default\\\",\\\"publication_ban_sample_rate\\\":0,\\\"allow_moderation_sampling_mode\\\":false,\\\"mobile_media_tab\\\":true,\\\"live_stream_ended_zync_delay\\\":10,\\\"continue_support_cta_in_newsletter_emails\\\":false,\\\"bloomberg_syndication_enabled\\\":false,\\\"custom_publication_theme\\\":false,\\\"lists_enabled\\\":false,\\\"social_context_in_pymk_feed_module\\\":\\\"experiment\\\",\\\"generated_database_maintenance_mode\\\":false,\\\"allow_document_freeze\\\":false,\\\"auto_login_email_ios\\\":\\\"experiment\\\",\\\"podcast_main_feed_is_firehose\\\":false,\\\"no_embed_redirect\\\":false,\\\"translate_mobile_app\\\":false,\\\"spotify_open_access_sandbox_mode\\\":false,\\\"fullstory_enabled\\\":false,\\\"chat_reply_poll_interval\\\":3,\\\"mobile_feed_tabs\\\":false,\\\"enable_reader_marketing_page\\\":false,\\\"force_pub_links_to_use_subdomain\\\":false,\\\"email_existing_users_on_import\\\":false,\\\"always_show_cookie_banner\\\":false,\\\"hide_media_download_option\\\":false,\\\"use_stripe_link\\\":\\\"control\\\",\\\"twitter_figures_enabled\\\":false,\\\"hide_post_restacks\\\":false,\\\"feed_item_source_debug_mode\\\":false,\\\"feed_pushes_for_all_v2\\\":\\\"experiment\\\",\\\"enable_publication_personal_mode_switcher\\\":false,\\\"daily_promoted_notes_enabled\\\":true,\\\"publication_homepage_title_display_override\\\":\\\"\\\",\\\"pub_banned_word_list\\\":\\\"\\\",\\\"post_preview_highlight_byline\\\":false,\\\"4k_video\\\":false,\\\"livekit_egress_custom_base_url\\\":\\\"\\\",\\\"bypass_unlock_token_limit\\\":false,\\\"notifications_disabled\\\":\\\"\\\",\\\"cross_post_notification_threshold\\\":1000,\\\"facebook_connect_prod_app\\\":true,\\\"use_slim_firehose_events\\\":false,\\\"messages_inbox_page_size\\\":50,\\\"minimum_android_version\\\":756,\\\"feed_main_disabled\\\":false,\\\"use_mobile_app_post_unlock_flow\\\":true,\\\"live_streams_enabled\\\":true,\\\"use_og_image_as_twitter_image_for_post_previews\\\":false,\\\"always_use_podcast_channel_art_as_episode_art_in_rss\\\":false,\\\"cookie_preference_middleware_enabled\\\":false,\\\"android_global_player_v2_enabled\\\":false,\\\"seo_tier_override\\\":\\\"NONE\\\",\\\"send_as_email_reply_bar\\\":\\\"control\\\",\\\"no_follow_links\\\":false,\\\"publisher_api_enabled\\\":false,\\\"zendesk_support_priority\\\":\\\"default\\\",\\\"enable_subscriber_referrals_awards\\\":true,\\\"web_feed_tabs\\\":false,\\\"photo_dna_enabled\\\":false,\\\"android_live_stream_creation_enabled\\\":false,\\\"enable_prepublish_clip_share_flow\\\":true,\\\"post_management_search_engine\\\":\\\"elasticsearch\\\",\\\"reengagement_push_notification\\\":\\\"experiment\\\",\\\"onboarding_retention_shuffle\\\":\\\"experiment\\\",\\\"web_reader_podcasts_tab\\\":false,\\\"use_gen_table_based_category_tabs\\\":false,\\\"live_stream_creation_enabled\\\":false,\\\"onboarding_pymk\\\":\\\"treatment\\\",\\\"enable_web_typing_indicators\\\":false,\\\"web_vitals_sample_rate\\\":0,\\\"skip_twitter_step_in_writer_onboarding\\\":true,\\\"temporal_zync_post_editor\\\":false,\\\"batch_push_notifications_chat\\\":false,\\\"enabled_category_tab_ids\\\":\\\"109,13645,15417,49715,11\\\",\\\"ai_image_generation_enabled\\\":true,\\\"disable_personal_substack_initialization\\\":false,\\\"section_specific_welcome_pages\\\":false,\\\"local_payment_methods\\\":\\\"control\\\",\\\"enable_personal_subscriber_notifications\\\":true,\\\"posts_in_rss_feed\\\":20,\\\"post_rec_endpoint\\\":\\\"\\\",\\\"publisher_dashboard_section_selector\\\":false,\\\"reader_surveys_platform_question_order\\\":\\\"36,1,4,2,3,5,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35\\\",\\\"app_install_drawer_2\\\":\\\"control\\\",\\\"login_guard_app_link_in_email\\\":true,\\\"modal_rec_variant_content\\\":\\\"control\\\",\\\"monthly_sub_is_one_off\\\":false,\\\"unread_notes_activity_digest\\\":\\\"control\\\",\\\"display_cookie_settings\\\":false,\\\"welcome_page_query_params\\\":false,\\\"enable_free_podcast_urls\\\":false,\\\"phone_verification_banned_countries\\\":\\\"\\\",\\\"use_microlink_for_instagram_embeds\\\":false,\\\"blocked_email_domains\\\":\\\"pelagius.net,teeuk.com,laafd.com,vjuum.com,txcct.com,rteet.com,dpptd.com\\\",\\\"use_whisper\\\":false,\\\"ios_post_stats_for_admins\\\":false,\\\"section_specific_preambles\\\":false,\\\"show_menu_on_posts\\\":false,\\\"app_upsell_follow_prompt\\\":\\\"control\\\",\\\"free_signup_confirmation_behavior\\\":\\\"with_email_validation\\\",\\\"ios_writer_stats_public_launch_v2\\\":false,\\\"phone_verification_fallback_to_twilio\\\":false,\\\"enable_android_post_stats\\\":false,\\\"ios_chat_revamp_enabled\\\":false,\\\"app_mode\\\":false,\\\"show_phone_banner\\\":false,\\\"main_aurora_reads_percent\\\":0,\\\"ios_global_player_v2_enabled\\\":false,\\\"enable_decagon_chat\\\":true,\\\"first_month_upsell\\\":\\\"control\\\",\\\"twitter_connect_flows_enabled\\\":false,\\\"enable_user_contacts\\\":true,\\\"gift_post_unlocks_recipient_2\\\":\\\"experiment\\\",\\\"live_stream_invite_ttl_seconds\\\":300,\\\"pin_primary_publication_chat_ios_v2\\\":\\\"control\\\",\\\"welcome_page_update_desktop_visuals_limited\\\":\\\"experiment\\\",\\\"rss_verification_code\\\":\\\"\\\",\\\"notification_post_emails\\\":\\\"experiment\\\",\\\"enable_crm_publisher_agreement_agree_wall\\\":\\\"personal_substacks\\\",\\\"export_hooks_enabled\\\":false,\\\"audio_encoding_bitrate\\\":null,\\\"bestseller_pub_override\\\":false,\\\"extra_seats_coupon_type\\\":false,\\\"post_subdomain_universal_links\\\":false,\\\"post_import_max_file_size\\\":26214400,\\\"minimum_ios_version\\\":2200,\\\"disable_annual_subscriptions\\\":false,\\\"enable_bestseller_survey_modal_override\\\":false,\\\"android_live_streams_enabled\\\":true,\\\"enable_android_dms\\\":false,\\\"non_mfa_password_login_guard\\\":\\\"block_suspicious\\\",\\\"recipes_enabled\\\":false,\\\"disable_deletion\\\":false,\\\"default_smart_delivery_on\\\":\\\"experiment\\\",\\\"clip_focused_video_upload_flow\\\":false,\\\"sort_inbox_with_pins\\\":false,\\\"unitary_image_moderation_percent\\\":100,\\\"publisher_banner\\\":\\\"\\\",\\\"more_like_this_recs_enabled\\\":false,\\\"live_event_mixin\\\":\\\"\\\",\\\"review_incoming_email\\\":\\\"default\\\",\\\"feed_pushes_for_all\\\":\\\"control\\\",\\\"ios_chat_uikit\\\":false,\\\"daily_promoted_note_intelligent_delivery\\\":\\\"control\\\",\\\"infer_categories_for_new_users\\\":false,\\\"post_podcast_import_batch_size\\\":0,\\\"load_test_unichat\\\":false},\\\"publicationSettings\\\":{\\\"block_ai_crawlers\\\":false,\\\"credit_token_enabled\\\":false,\\\"custom_tos_and_privacy\\\":false,\\\"did_identity\\\":null,\\\"disable_optimistic_bank_payments\\\":false,\\\"display_welcome_page_details\\\":true,\\\"enable_meetings\\\":false,\\\"payment_pledges_enabled\\\":false,\\\"enable_post_page_conversion\\\":true,\\\"enable_prev_next_nav\\\":true,\\\"enable_restacking\\\":true,\\\"google_analytics_4_token\\\":null,\\\"group_sections_and_podcasts_in_menu_enabled\\\":false,\\\"medium_length_description\\\":\\\"\\\",\\\"notes_feed_enabled\\\":false,\\\"paywall_unlock_tokens\\\":false,\\\"post_preview_crop_gravity\\\":\\\"center\\\",\\\"reader_referrals_enabled\\\":false,\\\"reader_referrals_leaderboard_enabled\\\":false,\\\"seen_coming_soon_explainer\\\":false,\\\"seen_google_analytics_migration_modal\\\":false,\\\"local_currency_modal_seen\\\":false,\\\"local_payment_methods_modal_seen\\\":false,\\\"twitter_pixel_signup_event_id\\\":null,\\\"twitter_pixel_subscribe_event_id\\\":null,\\\"use_local_currency\\\":false,\\\"use_local_payment_methods\\\":true,\\\"welcome_page_opt_out_text\\\":\\\"No thanks\\\",\\\"cookie_settings\\\":\\\"\\\"},\\\"publicationUserSettings\\\":null,\\\"userSettings\\\":{\\\"user_id\\\":null,\\\"activity_likes_enabled\\\":true,\\\"hasDismissedSectionToNewsletterRename\\\":false,\\\"is_guest_post_enabled\\\":true,\\\"feed_web_nux_seen_at\\\":null,\\\"has_seen_select_to_restack_tooltip_nux\\\":false,\\\"invite_friends_nux_dismissed_at\\\":null,\\\"suggestions_feed_item_last_shown_at\\\":null,\\\"has_seen_select_to_restack_modal\\\":false,\\\"last_home_tab\\\":null,\\\"last_notification_alert_shown_at\\\":null,\\\"disable_reply_hiding\\\":false,\\\"newest_seen_chat_item_published_at\\\":null,\\\"explicitContentEnabled\\\":false,\\\"contactMatchingEnabled\\\":false,\\\"messageRequestLevel\\\":\\\"everyone\\\",\\\"liveStreamAcceptableInviteLevel\\\":\\\"everyone\\\",\\\"creditTokensTreatmentExposed\\\":false,\\\"appBadgeIncludesChat\\\":false,\\\"autoPlayVideo\\\":true,\\\"smart_delivery_enabled\\\":false,\\\"chatbotTermsLastAcceptedAt\\\":null,\\\"has_seen_notes_post_app_upsell\\\":false},\\\"subscriberCountDetails\\\":\\\"tens of thousands of subscribers\\\",\\\"mux_env_key\\\":\\\"u42pci814i6011qg3segrcpp9\\\",\\\"sentry_environment\\\":\\\"production\\\",\\\"launchWelcomePage\\\":false,\\\"noIndex\\\":false,\\\"post\\\":{\\\"id\\\":147019742,\\\"editor_v2\\\":false,\\\"publication_id\\\":1008003,\\\"type\\\":\\\"newsletter\\\",\\\"title\\\":\\\"AI existential risk probabilities are too unreliable to inform policy\\\",\\\"social_title\\\":null,\\\"section_id\\\":null,\\\"search_engine_title\\\":null,\\\"search_engine_description\\\":null,\\\"subtitle\\\":\\\"How speculation gets laundered through pseudo-quantification\\\",\\\"slug\\\":\\\"ai-existential-risk-probabilities\\\",\\\"post_date\\\":\\\"2024-07-26T11:29:25.124Z\\\",\\\"podcast_url\\\":\\\"\\\",\\\"podcast_art_url\\\":null,\\\"podcast_duration\\\":null,\\\"video_upload_id\\\":null,\\\"podcast_upload_id\\\":null,\\\"podcast_preview_upload_id\\\":null,\\\"audience\\\":\\\"everyone\\\",\\\"should_send_free_preview\\\":false,\\\"write_comment_permissions\\\":\\\"everyone\\\",\\\"show_guest_bios\\\":true,\\\"free_unlock_required\\\":false,\\\"default_comment_sort\\\":null,\\\"canonical_url\\\":\\\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities\\\",\\\"audience_before_archived\\\":null,\\\"exempt_from_archive_paywall\\\":false,\\\"teaser_post_eligible\\\":true,\\\"restacks\\\":14,\\\"reactions\\\":{\\\"\\u2764\\\":100},\\\"top_exclusions\\\":[],\\\"pins\\\":[],\\\"section_pins\\\":[],\\\"previous_post_slug\\\":\\\"new-paper-ai-agents-that-matter\\\",\\\"next_post_slug\\\":\\\"ai-companies-are-pivoting-from-creating\\\",\\\"cover_image\\\":\\\"https://substack-post-media.s3.amazonaws.com/public/images/6418811e-f1c2-4127-b6dd-f243c4f458ef_1398x918.png\\\",\\\"cover_image_is_square\\\":false,\\\"cover_image_is_explicit\\\":false,\\\"videoUpload\\\":null,\\\"podcastFields\\\":{\\\"post_id\\\":147019742,\\\"podcast_episode_number\\\":null,\\\"podcast_season_number\\\":null,\\\"podcast_episode_type\\\":null,\\\"should_syndicate_to_other_feed\\\":null,\\\"syndicate_to_section_id\\\":null,\\\"hide_from_feed\\\":false,\\\"free_podcast_url\\\":null,\\\"free_podcast_duration\\\":null},\\\"podcastUpload\\\":null,\\\"podcastPreviewUpload\\\":null,\\\"voiceover_upload_id\\\":null,\\\"voiceoverUpload\\\":null,\\\"has_voiceover\\\":false,\\\"description\\\":\\\"How speculation gets laundered through pseudo-quantification\\\",\\\"body_html\\\":\\\"<p>How seriously should governments take the threat of existential risk from AI, given the lack of consensus among researchers? On the one hand, existential risks (x-risks) are necessarily somewhat speculative: by the time there is concrete evidence, it may be too late. On the other hand, governments must prioritize \\u2014 after all, they don\\u2019t worry too much about x-risk from alien invasions.</p><p>This is the first in a series of essays laying out an evidence-based approach for policymakers concerned about AI x-risk, an approach that stays grounded in reality while acknowledging that there are \\u201Cunknown unknowns\\u201D.&nbsp;</p><p>In this first essay, we look at one type of evidence: probability estimates. The AI safety community relies heavily on forecasting the probability of human extinction due to AI (in a given timeframe) in order to inform decision making and policy. An estimate of 10% over a few decades, for example, would obviously be high enough for the issue to be a top priority for society.&nbsp;</p><p>Our central claim is that AI x-risk forecasts are far too unreliable to be useful for policy, and in fact highly misleading.</p><div class=\\\\\\\"subscription-widget-wrap-editor\\\\\\\" data-attrs=\\\\\\\"{&quot;url&quot;:&quot;https://www.aisnakeoil.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\\\\\\\" data-component-name=\\\\\\\"SubscribeWidgetToDOM\\\\\\\"><div class=\\\\\\\"subscription-widget show-subscribe\\\\\\\"><div class=\\\\\\\"preamble\\\\\\\"><p class=\\\\\\\"cta-caption\\\\\\\">Subscribe to receive future essays in this series.</p></div><form class=\\\\\\\"subscription-widget-subscribe\\\\\\\"><input type=\\\\\\\"email\\\\\\\" class=\\\\\\\"email-input\\\\\\\" name=\\\\\\\"email\\\\\\\" placeholder=\\\\\\\"Type your email\\u2026\\\\\\\" tabindex=\\\\\\\"-1\\\\\\\"><input type=\\\\\\\"submit\\\\\\\" class=\\\\\\\"button primary\\\\\\\" value=\\\\\\\"Subscribe\\\\\\\"><div class=\\\\\\\"fake-input-wrapper\\\\\\\"><div class=\\\\\\\"fake-input\\\\\\\"></div><div class=\\\\\\\"fake-button\\\\\\\"></div></div></form></div></div><h3><strong>Look behind the curtain</strong></h3><p>If the two of us predicted an 80% probability of aliens landing on earth in the next ten years, would you take this possibility seriously? Of course not. You would ask to see our evidence. As obvious as this may seem, it seems to have been forgotten in the AI x-risk debate that probabilities carry no authority by themselves. Probabilities are <em>usually</em> derived from some grounded method, so we have a strong cognitive bias to view quantified risk estimates as more valid than qualitative ones. But it is possible for probabilities to be nothing more than guesses. Keep this in mind throughout this essay (and more broadly in the AI x-risk debate).</p><p>If we predicted odds for the Kentucky Derby, we don\\u2019t have to give you a reason \\u2014 you can take it or leave it. But if a policymaker takes actions based on probabilities put forth by a forecaster, they had better be able to explain those probabilities to the public (and that explanation must in turn come from the forecaster). <a href=\\\\\\\"https://plato.stanford.edu/entries/justification-public/\\\\\\\">Justification</a> is essential to legitimacy of government and the exercise of power. A core principle of liberal democracy is that the state should not limit people's freedom based on controversial beliefs that reasonable people can reject.&nbsp;</p><p>Explanation is especially important when the policies being considered are costly, and even more so when those costs are unevenly distributed among stakeholders. A good example is restricting open releases of AI models. Can governments convince people and companies who stand to benefit from open models that they should make this sacrifice because of a speculative future risk?</p><p>The main aim of this essay is analyzing whether there is any justification for any of the specific x-risk probability estimates that have been cited in the policy debate. We have no objection to AI x-risk forecasting as an academic activity, and forecasts may be helpful to companies and other private decision makers. We only question its use in the context of public policy.</p><p>There are basically only three known ways by which a forecaster can try to convince a skeptic: inductive, deductive, and subjective probability estimation. We consider each of these in the following sections. All three require both parties to agree on some basic assumptions about the world (which cannot themselves be proven). The three approaches differ in terms of the empirical and logical ways in which the probability estimate follows from that set of assumptions.</p><h3><strong>Inductive probability estimation is unreliable due to the lack of a reference class</strong></h3><p>Most risk estimates are inductive: they are based on past observations. For example, insurers base their predictions of an individual\\u2019s car accident risk on data from past accidents about similar drivers. The set of observations used for probability estimation is called a reference class. A suitable reference class for car insurance might be the set of drivers who live in the same city. If the analyst has more information about the individual, such as their age or the type of car they drive, the reference class can be further refined.&nbsp;</p><p>For existential risk from AI, there is no reference class, as it is an event like no other. To be clear, this is a matter of degree, not kind. There is never a clear \\u201Ccorrect\\u201D reference class to use, and the choice of a reference class in practice comes down to the analyst\\u2019s intuition.&nbsp;</p><p>The accuracy of the forecasts depends on the degree of similarity between the process that generates the event being forecast and the process that generated the events in the reference class, which can be seen as a spectrum. For predicting the outcome of a physical system such as a coin toss, past experience is a highly reliable guide. Next, for car accidents, risk estimates might vary by, say, 20% based on the past dataset used \\u2014 good enough for insurance companies.&nbsp;</p><div class=\\\\\\\"captioned-image-container\\\\\\\"><figure><a class=\\\\\\\"image-link is-viewable-img image2\\\\\\\" target=\\\\\\\"_blank\\\\\\\" href=\\\\\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png\\\\\\\" data-component-name=\\\\\\\"Image2ToDOM\\\\\\\"><div class=\\\\\\\"image2-inset\\\\\\\"><picture><source type=\\\\\\\"image/webp\\\\\\\" srcset=\\\\\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 1456w\\\\\\\" sizes=\\\\\\\"100vw\\\\\\\"><img src=\\\\\\\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png\\\\\\\" width=\\\\\\\"1238\\\\\\\" height=\\\\\\\"334\\\\\\\" data-attrs=\\\\\\\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:334,&quot;width&quot;:1238,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:41682,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\\\\\\\" class=\\\\\\\"sizing-normal\\\\\\\" alt=\\\\\\\"\\\\\\\" srcset=\\\\\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 1456w\\\\\\\" sizes=\\\\\\\"100vw\\\\\\\" loading=\\\\\\\"lazy\\\\\\\"></picture><div class=\\\\\\\"image-link-expand\\\\\\\"><svg xmlns=\\\\\\\"http://www.w3.org/2000/svg\\\\\\\" width=\\\\\\\"20\\\\\\\" height=\\\\\\\"20\\\\\\\" viewBox=\\\\\\\"0 0 24 24\\\\\\\" fill=\\\\\\\"none\\\\\\\" stroke=\\\\\\\"currentColor\\\\\\\" stroke-width=\\\\\\\"2\\\\\\\" stroke-linecap=\\\\\\\"round\\\\\\\" stroke-linejoin=\\\\\\\"round\\\\\\\" class=\\\\\\\"lucide lucide-maximize2 \\\\\\\"><polyline points=\\\\\\\"15 3 21 3 21 9\\\\\\\"></polyline><polyline points=\\\\\\\"9 21 3 21 3 15\\\\\\\"></polyline><line x1=\\\\\\\"21\\\\\\\" x2=\\\\\\\"14\\\\\\\" y1=\\\\\\\"3\\\\\\\" y2=\\\\\\\"10\\\\\\\"></line><line x1=\\\\\\\"3\\\\\\\" x2=\\\\\\\"10\\\\\\\" y1=\\\\\\\"21\\\\\\\" y2=\\\\\\\"14\\\\\\\"></line></svg></div></div></a></figure></div><p>Further along the spectrum are geopolitical events, where the choice of reference class gets even fuzzier. Forecasting expert Philip Tetlock <a href=\\\\\\\"https://www.jstor.org/stable/j.ctt1pk86s8.5\\\\\\\">explains</a>: \\u201CGrexit may have looked sui generis, because no country had exited the Eurozone as of 2015, but it could also be viewed as just another instance of a broad comparison class, such as negotiation failures, or of a narrower class, such as a nation-states withdrawing from international agreements or, narrower still, of forced currency conversions.\\u201D He goes on to defend the idea that even seeming <a href=\\\\\\\"https://en.wikipedia.org/wiki/Black_swan_theory\\\\\\\">Black Swan events</a> like the collapse of the USSR or the Arab Spring can be modeled as members of reference classes, and that inductive reasoning is useful even for this kind of event.</p><p>In Tetlock\\u2019s spectrum, these events represent the \\u201Cpeak\\u201D of uniqueness. When it comes to geopolitical events, that might be true. But even those events are far less unique than extinction from AI. Just look at the <a href=\\\\\\\"https://www.openphilanthropy.org/wp-content/uploads/2023.05.22-AI-Reference-Classes-Zachary-Freitas-Groff.pdf\\\\\\\">attempts</a> to find reference classes for AI x-risk: animal extinction (as a reference class for human extinction), past global transformations such as the industrial revolution (as a reference class for socioeconomic transformation from AI), or accidents causing mass deaths (as a reference class for accidents causing global catastrophe). Let\\u2019s get real. None of those tell us anything about the possibility of developing superintelligent AI or losing control over such AI, which are the central sources of uncertainty for AI x-risk forecasting.&nbsp;</p><p>To summarize, human extinction due to AI is an outcome so far removed from anything that has happened in the past that we cannot use inductive methods to \\u201Cpredict\\u201D the odds. Of course, we can get qualitative insights from past technical breakthroughs as well as past catastrophic events, but AI risk is sufficiently different that quantitative estimates lack the kind of justification needed for legitimacy in policymaking.</p><h3><strong>Deductive probability estimation is unreliable due to the lack of theory</strong></h3><p>In Conan Doyle\\u2019s <em>The Adventure of the Six Napoleons</em> <em>\\u2014</em> spoiler alert! <em>\\u2014 </em>Sherlock Holmes announces before embarking on a stakeout that the probability of catching the suspect is exactly two-thirds. This seems bewildering \\u2014 how can anything related to human behavior be ascribed a mathematically precise probability?</p><p>It turns out that Holmes has deduced the underlying series of events that gave rise to the suspect\\u2019s seemingly erratic observed behavior: the suspect is methodically searching for a jewel that is known to be hidden inside one of six busts of Napoleon owned by different people in and around London. The details aren\\u2019t too important, but the key is that neither the suspect nor the detectives know <em>which</em> of the six busts it is in, and everything else about the suspect\\u2019s behavior is (assumed to be) entirely predictable. Hence the precisely quantifiable uncertainty.</p><p>The point is that if we have a model of the world that we can rely upon, we can estimate risk through logical deduction, even without relying on past observations. Of course, outside of fictional scenarios, the world isn\\u2019t so neat, especially when we want to project far into the future.</p><p>When it comes to x-risk, there is an interesting exception to the general rule that we don\\u2019t have deductive models \\u2014 asteroid impact. A combination of inductive and deductive risk estimation does allow us to estimate the probability of x-risk, only because we\\u2019re talking about a purely physical system. Let\\u2019s take a minute to review how this works, because it\\u2019s important to recognize that the methods are not generalizable to other types of x-risk.&nbsp;</p><p>The key is being able to <a href=\\\\\\\"https://www.nature.com/articles/367033a0\\\\\\\">model</a> the relationship between the size of the asteroid (more precisely, the energy of impact) and the frequency of impact. Since we have observed thousands of small impacts, we can extrapolate to infer the frequency of large impacts that have never been directly observed. We can also estimate the threshold that would cause global catastrophe.<a class=\\\\\\\"footnote-anchor\\\\\\\" data-component-name=\\\\\\\"FootnoteAnchorToDOM\\\\\\\" id=\\\\\\\"footnote-anchor-1\\\\\\\" href=\\\\\\\"#footnote-1\\\\\\\" target=\\\\\\\"_self\\\\\\\">1</a></p><div class=\\\\\\\"captioned-image-container\\\\\\\"><figure><a class=\\\\\\\"image-link is-viewable-img image2\\\\\\\" target=\\\\\\\"_blank\\\\\\\" href=\\\\\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png\\\\\\\" data-component-name=\\\\\\\"Image2ToDOM\\\\\\\"><div class=\\\\\\\"image2-inset\\\\\\\"><picture><source type=\\\\\\\"image/webp\\\\\\\" srcset=\\\\\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 1456w\\\\\\\" sizes=\\\\\\\"100vw\\\\\\\"><img src=\\\\\\\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png\\\\\\\" width=\\\\\\\"1456\\\\\\\" height=\\\\\\\"559\\\\\\\" data-attrs=\\\\\\\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:559,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1119384,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\\\\\\\" class=\\\\\\\"sizing-normal\\\\\\\" alt=\\\\\\\"\\\\\\\" srcset=\\\\\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 1456w\\\\\\\" sizes=\\\\\\\"100vw\\\\\\\" loading=\\\\\\\"lazy\\\\\\\"></picture><div class=\\\\\\\"image-link-expand\\\\\\\"><svg xmlns=\\\\\\\"http://www.w3.org/2000/svg\\\\\\\" width=\\\\\\\"20\\\\\\\" height=\\\\\\\"20\\\\\\\" viewBox=\\\\\\\"0 0 24 24\\\\\\\" fill=\\\\\\\"none\\\\\\\" stroke=\\\\\\\"currentColor\\\\\\\" stroke-width=\\\\\\\"2\\\\\\\" stroke-linecap=\\\\\\\"round\\\\\\\" stroke-linejoin=\\\\\\\"round\\\\\\\" class=\\\\\\\"lucide lucide-maximize2 \\\\\\\"><polyline points=\\\\\\\"15 3 21 3 21 9\\\\\\\"></polyline><polyline points=\\\\\\\"9 21 3 21 3 15\\\\\\\"></polyline><line x1=\\\\\\\"21\\\\\\\" x2=\\\\\\\"14\\\\\\\" y1=\\\\\\\"3\\\\\\\" y2=\\\\\\\"10\\\\\\\"></line><line x1=\\\\\\\"3\\\\\\\" x2=\\\\\\\"10\\\\\\\" y1=\\\\\\\"21\\\\\\\" y2=\\\\\\\"14\\\\\\\"></line></svg></div></div></a><figcaption class=\\\\\\\"image-caption\\\\\\\">Figure: data on small asteroid impacts (illustrated on the <a href=\\\\\\\"https://commons.wikimedia.org/wiki/File:Bolide_events_1994-2013.jpg\\\\\\\">left</a>) can be extrapolated to extinction-level impacts (right).</figcaption></figure></div><p>With AI, the unknowns relate to <a href=\\\\\\\"https://www.aisnakeoil.com/p/ai-scaling-myths\\\\\\\">technological progress</a> and governance rather than a physical system, so it isn\\u2019t clear how to model it mathematically. Still, people have tried. For example, in order to predict the computational requirements of a hypothetical AGI, <a href=\\\\\\\"https://arxiv.org/pdf/2306.02519\\\\\\\">several</a> <a href=\\\\\\\"https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines\\\\\\\">works</a> assume that an AI system would require roughly as many computations as the human brain, and further make assumptions about the number of computations required by the human brain. These assumptions are far more tenuous than those involved in asteroid modeling, and none of this even addresses the loss-of-control question.</p><h3><strong>Subjective probabilities are feelings dressed up as numbers</strong></h3><p>Without the reference classes or grounded theories, forecasts are necessarily \\u201Csubjective probabilities\\u201D, that is, guesses based on the forecaster\\u2019s judgment. Unsurprisingly, these vary by orders of magnitude.</p><p>Subjective probability estimation does not get around the need for having either an inductive or a deductive basis for probability estimates. It merely avoids the need for the forecaster to <em>explain</em> their estimate. Explanation can be hard due to humans\\u2019 limited ability to explain our intuitive reasoning, whether inductive, deductive, or a combination thereof. Essentially, it allows the forecaster to say: \\u201Ceven though I haven\\u2019t shown my methods, you can trust this estimate because of my track record\\u201D (we explain in the next section why even this breaks down for AI x-risk forecasting). But ultimately, lacking either an inductive or a deductive basis, all that forecasters can do is to make up a number, and those made-up numbers are all over the place.</p><div class=\\\\\\\"captioned-image-container\\\\\\\"><figure><a class=\\\\\\\"image-link is-viewable-img image2\\\\\\\" target=\\\\\\\"_blank\\\\\\\" href=\\\\\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png\\\\\\\" data-component-name=\\\\\\\"Image2ToDOM\\\\\\\"><div class=\\\\\\\"image2-inset\\\\\\\"><picture><source type=\\\\\\\"image/webp\\\\\\\" srcset=\\\\\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 1456w\\\\\\\" sizes=\\\\\\\"100vw\\\\\\\"><img src=\\\\\\\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png\\\\\\\" width=\\\\\\\"1272\\\\\\\" height=\\\\\\\"896\\\\\\\" data-attrs=\\\\\\\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:896,&quot;width&quot;:1272,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:172573,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\\\\\\\" class=\\\\\\\"sizing-normal\\\\\\\" alt=\\\\\\\"\\\\\\\" srcset=\\\\\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 1456w\\\\\\\" sizes=\\\\\\\"100vw\\\\\\\" loading=\\\\\\\"lazy\\\\\\\"></picture><div class=\\\\\\\"image-link-expand\\\\\\\"><svg xmlns=\\\\\\\"http://www.w3.org/2000/svg\\\\\\\" width=\\\\\\\"20\\\\\\\" height=\\\\\\\"20\\\\\\\" viewBox=\\\\\\\"0 0 24 24\\\\\\\" fill=\\\\\\\"none\\\\\\\" stroke=\\\\\\\"currentColor\\\\\\\" stroke-width=\\\\\\\"2\\\\\\\" stroke-linecap=\\\\\\\"round\\\\\\\" stroke-linejoin=\\\\\\\"round\\\\\\\" class=\\\\\\\"lucide lucide-maximize2 \\\\\\\"><polyline points=\\\\\\\"15 3 21 3 21 9\\\\\\\"></polyline><polyline points=\\\\\\\"9 21 3 21 3 15\\\\\\\"></polyline><line x1=\\\\\\\"21\\\\\\\" x2=\\\\\\\"14\\\\\\\" y1=\\\\\\\"3\\\\\\\" y2=\\\\\\\"10\\\\\\\"></line><line x1=\\\\\\\"3\\\\\\\" x2=\\\\\\\"10\\\\\\\" y1=\\\\\\\"21\\\\\\\" y2=\\\\\\\"14\\\\\\\"></line></svg></div></div></a></figure></div><p>Consider the <a href=\\\\\\\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf\\\\\\\">Existential Risk Persuasion Tournament</a> (XPT) conducted by the Forecasting Research Institute in late 2022, which we think is the most elaborate and well-executed x-risk forecasting exercise conducted to date. It involved various groups of forecasters, including AI experts and forecasting experts (\\u201Csuperforecasters\\u201D in the figure). For AI experts, the high end (75th percentile) of estimates for AI extinction risk by 2100 is 12%, the median estimate is 3%, and the low end (25th percentile) is 0.25%. For forecasting experts, even the high end (75th percentile) is only 1%, the median is a mere 0.38%, and the low end (25th percentile) is visually indistinguishable from zero on the graph. In other words, the 75th percentile AI expert forecast and the 25th percentile superforecaster forecast differ by at least a factor of 100.</p><p>All of these estimates are from people who have deep expertise on the topic and participated in a months-long tournament where they tried to persuade each other! If this range of forecasts here isn\\u2019t extreme enough, keep in mind that this whole exercise was conducted by one group at one point in time. We might get different numbers if the tournament were repeated today, if the questions were framed differently, etc.</p><p>What\\u2019s most telling is to look at the rationales that forecasters provided, which are extensively detailed in the report. They aren\\u2019t using quantitative models, especially when thinking about the likelihood of bad outcomes conditional on developing powerful AI. For the most part, forecasters are engaging in the same kind of speculation that everyday people do when they discuss superintelligent AI. Maybe AI will take over critical systems through superhuman persuasion of system operators. Maybe AI will seek to lower global temperatures because it helps computers run faster, and accidentally wipe out humanity. Or maybe AI will seek resources in space rather than Earth, so we don\\u2019t need to be as worried. There\\u2019s nothing wrong with such speculation. But we should be clear that when it comes to AI x-risk, forecasters aren\\u2019t drawing on any special knowledge, evidence, or models that make their hunches more credible than yours or ours or anyone else\\u2019s.&nbsp;&nbsp;</p><p>The term <a href=\\\\\\\"https://www.amazon.com/Superforecasting-Science-Prediction-Philip-Tetlock/dp/0804136718\\\\\\\">superforecasting</a> comes from Philip Tetlock\\u2019s 20 year study of forecasting (he was also one of the organizers of the XPT). Superforecasters tend to be trained in methods to improve forecasts such as by integrating diverse information and by minimizing psychological biases. These methods have been shown to be effective in domains such as geopolitics. But no amount of training will lead to good forecasts if there isn\\u2019t much useful evidence to draw from.</p><p>Even if forecasters had credible quantitative models (they don\\u2019t), they must account for \\u201Cunknown unknowns\\u201D, that is, the possibility that the model itself might be wrong. As noted x-risk philosopher Nick Bostrom <a href=\\\\\\\"https://existential-risk.com/concept.pdf\\\\\\\">explains</a>: \\u201CThe uncertainty and error-proneness of our first-order assessments of risk is itself something we must factor into our all-things-considered probability assignments. This factor often dominates in low-probability, high-consequence risks \\u2014 especially those involving poorly understood natural phenomena, complex social dynamics, or new technology, or that are difficult to assess for other reasons.\\u201D&nbsp;</p><p>This is a reasonable perspective, and AI x-risk forecasters do worry a lot about uncertainty in risk assessment. But one consequence of this is that for those who follow this principle, forecasts are <em>guaranteed</em> to be guesses rather than the output of a model \\u2014 after all, no model can be used to estimate the probability that the model itself is wrong, or what the risk would be if the model were wrong.</p><h3><strong>Forecast skill cannot be measured when it comes to unique or rare events</strong></h3><p>To recap, subjective AI-risk forecasts vary by orders of magnitude. But if we can measure forecasters\\u2019 track records, maybe we can use that to figure out which forecasters to trust. In contrast to the previous two approaches for justifying risk estimates (inductive and deductive), the forecaster doesn\\u2019t have to explain their estimate, but instead justifies it based on their demonstrated skill at predicting <em>other</em> outcomes in the past.&nbsp;</p><p>This has proved to be invaluable in the domain of geopolitical events, and the forecasting community spends a lot of effort on skill measurement. Many ways to evaluate forecasting skill exist, such as calibration, the Brier score, the logarithmic score, or the Peer score used on the forecasting competition website <a href=\\\\\\\"https://www.metaculus.com/\\\\\\\">Metaculus</a>.&nbsp;</p><p>But regardless of which method is used, when it comes to existential risk, there are many barriers to assessing forecast skill for subjective probabilities: the lack of a reference class, the low base rate, and the long time horizon. Let\\u2019s look at each of these in turn.</p><p>Just as the reference class problem plagues the forecaster, it also affects the evaluator. Let\\u2019s return to the alien landing example. Consider a forecaster who has proved highly accurate at calling elections. Suppose this forecaster announces, without any evidence, that aliens will land on Earth within a year. Despite the forecaster\\u2019s demonstrated skill, this would not cause us to update our beliefs about an alien landing, because it is too dissimilar to election forecasting and we do not expect the forecaster\\u2019s skill to generalize. Similarly, AI x-risk is so dissimilar to any past events that have been forecast that there is no evidence of any forecaster\\u2019s skill at estimating AI x-risk.&nbsp;</p><p>Even if we somehow do away with the reference class problem, other problems remain \\u2014 notably, the fact that extinction risks are \\u201Ctail risks\\u201D, or risks that result from rare events. Suppose forecaster A says the probability of AI x-risk is 1%, and forecaster B says it is 1 in a million. Which forecast should we have more confidence in? We could look at their track records. Say we find that forecaster A (who has assigned a 1% probability to AI x-risk) has a better track record. It still doesn\\u2019t mean we should have more confidence in A\\u2019s forecast, because <em>skill evaluations are insensitive to overestimation of tail risks</em>. In other words, it could be that A scores higher overall because A is slightly better calibrated than B when it comes to everyday events that have a substantial probability of occurring, but tends to massively overestimate tail risks that occur rarely (for example, those with a probability of 1 in a million) by orders of magnitude. No scoring rule adequately penalizes this type of miscalibration.&nbsp;</p><p>Here\\u2019s a thought experiment to show why this is true. Suppose two forecasters F and G forecast two different sets of events, and the \\u201Ctrue\\u201D probabilities of events in both sets are uniformly distributed between 0 and 1. We assume, highly optimistically, that both F and G know the true probability P[<em>e</em>] for every event <em>e</em> that they forecast. F always outputs P[<em>e</em>], but G is slightly conservative, never predicting a value less than 1%. That is, G outputs P[<em>e</em>] if P[<em>e</em>] &gt;= 1%, otherwise outputs 1%.</p><p>By construction, F is the better forecaster. But would this be evident from their track records? In other words, how many forecasts from each would we have to evaluate so that there\\u2019s a 95% chance that F outscores G? With the logarithmic scoring rule, it turns out to be on the order of a hundred million. With the Brier score, it is on the order of a trillion.<a class=\\\\\\\"footnote-anchor\\\\\\\" data-component-name=\\\\\\\"FootnoteAnchorToDOM\\\\\\\" id=\\\\\\\"footnote-anchor-2\\\\\\\" href=\\\\\\\"#footnote-2\\\\\\\" target=\\\\\\\"_self\\\\\\\">2</a>&nbsp; We can quibble with the assumptions here but the point is that if a forecaster systematically overestimates tail risks, it is simply empirically undetectable.</p><p>The final barrier to assessing forecaster skill at predicting x-risk is that long-term forecasts take too long to evaluate (and extinction forecasts are of course impossible to evaluate). This can potentially be overcome. Researchers have developed a method called <a href=\\\\\\\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3954498\\\\\\\">reciprocal scoring</a> \\u2014 where forecasters are rewarded based on how well they predict each others\\u2019 forecasts \\u2014 and validate it in some real-world settings, such as predicting the effect of Covid-19 policies. In these settings, reciprocal scoring yielded forecasts that are as good as traditional scoring methods. Fair enough. But reciprocal scoring is not a way around the reference class problem or the tail risk problem.</p><div class=\\\\\\\"captioned-image-container\\\\\\\"><figure><a class=\\\\\\\"image-link image2\\\\\\\" target=\\\\\\\"_blank\\\\\\\" href=\\\\\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png\\\\\\\" data-component-name=\\\\\\\"Image2ToDOM\\\\\\\"><div class=\\\\\\\"image2-inset\\\\\\\"><picture><source type=\\\\\\\"image/webp\\\\\\\" srcset=\\\\\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 1456w\\\\\\\" sizes=\\\\\\\"100vw\\\\\\\"><img src=\\\\\\\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png\\\\\\\" width=\\\\\\\"1456\\\\\\\" height=\\\\\\\"337\\\\\\\" data-attrs=\\\\\\\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:337,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:87385,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\\\\\\\" class=\\\\\\\"sizing-normal\\\\\\\" alt=\\\\\\\"\\\\\\\" srcset=\\\\\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 1456w\\\\\\\" sizes=\\\\\\\"100vw\\\\\\\" loading=\\\\\\\"lazy\\\\\\\"></picture><div></div></div></a><figcaption class=\\\\\\\"image-caption\\\\\\\"><em>Summary of our argument so far, showing why none of the three forecasting methods can yield credible estimates of AI x-risk.</em></figcaption></figure></div><h3><strong>There are many reasons why risk estimates may be systematically inflated</strong></h3><p>To recap, inductive and deductive methods don\\u2019t work, subjective forecasts are all over the place, and there\\u2019s no way to tell which forecasts are more trustworthy.</p><p>So in an attempt to derive more reliable estimates that could potentially inform policy, some researchers have turned to forecast aggregation methods that combine the predictions of multiple forecasters. A notable effort is the <a href=\\\\\\\"https://wiki.aiimpacts.org/doku.php?id=ai_timelines:predictions_of_human-level_ai_timelines:ai_timeline_surveys:2022_expert_survey_on_progress_in_ai\\\\\\\">AI Impacts Survey on Progress in AI</a>, but it has been criticized for serious methodological limitations including <a href=\\\\\\\"https://aiguide.substack.com/p/do-half-of-ai-researchers-believe\\\\\\\">non</a>-<a href=\\\\\\\"https://www.scientificamerican.com/article/ai-survey-exaggerates-apocalyptic-risks/\\\\\\\">response</a> <a href=\\\\\\\"https://spectrum.ieee.org/ai-existential-risk-survey\\\\\\\">bias</a>. More importantly, it is unclear why aggregation should improve forecast accuracy: after all, most forecasters might share the same biases (and again, none of them have any basis for a reliable forecast).</p><p>There are many reasons why forecasters might systematically overestimate AI x-risk.<a class=\\\\\\\"footnote-anchor\\\\\\\" data-component-name=\\\\\\\"FootnoteAnchorToDOM\\\\\\\" id=\\\\\\\"footnote-anchor-3\\\\\\\" href=\\\\\\\"#footnote-3\\\\\\\" target=\\\\\\\"_self\\\\\\\">3</a> The first is selection bias. Take AI researchers: the belief that AI can change the world is one of the main motivations for becoming an AI researcher. And once someone enters this community, they are in an environment where that message is constantly reinforced. And if one believes that this technology is terrifyingly powerful, it is perfectly rational to think there is a serious chance that its world-altering effects will be negative rather than positive.</p><p>And in the AI safety subcommunity, which is a bit insular, the <a href=\\\\\\\"https://www.washingtonpost.com/technology/2023/07/05/ai-apocalypse-college-students/\\\\\\\">echo chamber</a> can be deafening. Claiming to have a high <em>p(doom)</em> (one\\u2019s estimate of the probability of AI doom) seems to have become a way to signal one\\u2019s identity and commitment to the cause.</p><p>There is a slightly different selection bias at play when it comes to forecasting experts. The forecasting community has a strong overlap with effective altruism and concerns about existential risk, especially AI risk. This doesn\\u2019t mean that individual forecasters are biased. But having a high <em>p(doom)</em> might make someone more inclined to take up forecasting as an activity. So the community as a whole is likely biased toward people with x-risk worries.&nbsp;</p><p>Forecasters are good at updating their beliefs in response to evidence, but the problem is that unlike, say, asteroid impact risk, there is little evidence that can change one\\u2019s beliefs one way or another when it comes to AI x-risk, so we suspect that forecasts are strongly influenced by the priors with which people enter the community. The <a href=\\\\\\\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf\\\\\\\">XPT report</a> notes that \\u201CFew minds were changed during the XPT, even among the most active participants, and despite monetary incentives for persuading others.\\u201D In a <a href=\\\\\\\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/65ef1ee52e64b52f145ebb49/1710169832137/AIcollaboration.pdf\\\\\\\">follow-up study</a>, they found that many of the disagreements were due to fundamental worldview differences that go beyond AI.</p><p>To reemphasize, our points about bias are specific to AI x-risk. If there were a community of election forecasters who were systematically biased (say, toward incumbents), this would become obvious after a few elections when comparing predictions with reality. But with AI x-risk, as we showed in the previous section, skill evaluation is insensitive to overestimation of tail risks.</p><p>Interestingly, skill evaluation is extremely sensitive to <em>underestimation</em> of tail risks: if you assign a probability of 0 for a rare event that actually ends up occurring, you incur an <em>infinite </em>penalty under the logarithmic scoring rule, from which you can never recover regardless of how well you predicted other events. This is considered one of the main benefits of the logarithmic score and is the reason it is adopted by Metaculus.</p><p>Now consider a forecaster who doesn\\u2019t have a precise estimate \\u2014 and surely no forecaster has a precise estimate for something with so many axes of uncertainty as AI x-risk. Given the asymmetric penalties, the rational thing to do is to go with the higher end of their range of estimates.<a class=\\\\\\\"footnote-anchor\\\\\\\" data-component-name=\\\\\\\"FootnoteAnchorToDOM\\\\\\\" id=\\\\\\\"footnote-anchor-4\\\\\\\" href=\\\\\\\"#footnote-4\\\\\\\" target=\\\\\\\"_self\\\\\\\">4</a></p><p>In any case, it\\u2019s not clear what forecasters actually report when their estimates are highly uncertain. Maybe they don\\u2019t respond to the incentives of the scoring function. After all, long-term forecasts won\\u2019t be resolved anytime soon. And recall that in the case of the XPT, the incentive is actually to predict each others\\u2019 forecasts to get around the problem of long time horizons. The reciprocal scoring paper argues that this will incentivize forecasters to submit their true, high-effort estimates, and considers various objections to this claim. Their defense of the method rests on two key assumptions: that by exerting more effort forecasters can get closer to the true estimate, and that they have no better way to predict what other forecasters will do.</p><p>What if these assumptions are not satisfied? As we have argued throughout this post, with AI x-risk, we shouldn\\u2019t expect evidence to change forecasters\\u2019 prior beliefs, so the first assumption is dubious. And now that one iteration of the XPT has concluded, the published median estimates from that tournament serve as a powerful anchor (a \\u201C<a href=\\\\\\\"https://en.wikipedia.org/wiki/Focal_point_(game_theory)\\\\\\\">focal point</a>\\u201D in game theory). It is possible that in the future, forecasters with reciprocal scoring incentives will use existing median forecasts as a starting point, only making minor adjustments to account for new information that has become available since the last tournament. The range of estimates might narrow as existing estimates serve as anchors for future estimates. All that is a roundabout way to say: the less actual evidence there is to draw upon, the more the risk of groupthink.</p><h3><strong>Beware Pascal\\u2019s wager: the dangers of utility maximization&nbsp;</strong></h3><p>For what it\\u2019s worth, here are the median estimates from the <a href=\\\\\\\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf\\\\\\\">XPT</a> of both extinction risk and sub-extinction catastrophic risks from AI:</p><div class=\\\\\\\"captioned-image-container\\\\\\\"><figure><a class=\\\\\\\"image-link is-viewable-img image2\\\\\\\" target=\\\\\\\"_blank\\\\\\\" href=\\\\\\\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png\\\\\\\" data-component-name=\\\\\\\"Image2ToDOM\\\\\\\"><div class=\\\\\\\"image2-inset\\\\\\\"><picture><source type=\\\\\\\"image/webp\\\\\\\" srcset=\\\\\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 1456w\\\\\\\" sizes=\\\\\\\"100vw\\\\\\\"><img src=\\\\\\\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png\\\\\\\" width=\\\\\\\"1456\\\\\\\" height=\\\\\\\"473\\\\\\\" data-attrs=\\\\\\\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:473,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:442848,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\\\\\\\" class=\\\\\\\"sizing-normal\\\\\\\" alt=\\\\\\\"\\\\\\\" srcset=\\\\\\\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 1456w\\\\\\\" sizes=\\\\\\\"100vw\\\\\\\" loading=\\\\\\\"lazy\\\\\\\"></picture><div class=\\\\\\\"image-link-expand\\\\\\\"><svg xmlns=\\\\\\\"http://www.w3.org/2000/svg\\\\\\\" width=\\\\\\\"20\\\\\\\" height=\\\\\\\"20\\\\\\\" viewBox=\\\\\\\"0 0 24 24\\\\\\\" fill=\\\\\\\"none\\\\\\\" stroke=\\\\\\\"currentColor\\\\\\\" stroke-width=\\\\\\\"2\\\\\\\" stroke-linecap=\\\\\\\"round\\\\\\\" stroke-linejoin=\\\\\\\"round\\\\\\\" class=\\\\\\\"lucide lucide-maximize2 \\\\\\\"><polyline points=\\\\\\\"15 3 21 3 21 9\\\\\\\"></polyline><polyline points=\\\\\\\"9 21 3 21 3 15\\\\\\\"></polyline><line x1=\\\\\\\"21\\\\\\\" x2=\\\\\\\"14\\\\\\\" y1=\\\\\\\"3\\\\\\\" y2=\\\\\\\"10\\\\\\\"></line><line x1=\\\\\\\"3\\\\\\\" x2=\\\\\\\"10\\\\\\\" y1=\\\\\\\"21\\\\\\\" y2=\\\\\\\"14\\\\\\\"></line></svg></div></div></a></figure></div><p>To reiterate, our view is that we shouldn\\u2019t take any of these numbers too seriously. They are a reflection of how much different samples of participants fret about AI than anything else.</p><p>As before, the estimates from forecasting experts (superforecasters) and AI experts differ by an order of magnitude or more. To the extent that we put any stock into these estimates, it should be the forecasting experts\\u2019 rather than the AI experts\\u2019 estimates. One important insight from past research is that domain experts perform worse than forecasting experts who have training in integrating diverse information and by minimizing psychological biases. Still, as we said above, even their forecasts may be vast overestimates, and we just can\\u2019t know for sure.</p><p>So what\\u2019s the big deal? So what if policymakers believe the risk over a certain timeframe is 1% instead of 0.01%? It seems pretty low in either case!</p><p>It depends on what they do with those probabilities. Most often, these estimates are merely a way to signal the fact that some group of experts thinks the risk is significant. If that\\u2019s all they are, so be it. But it\\u2019s not clear that all this elaborate effort at quantification is even helpful for this signaling purpose, given that different people interpret the same numbers wildly differently.&nbsp;</p><p>For example, Federal Trade Commission chair Lina Khan said her views on the matter were techno-optimistic since her <em>p(doom)</em> was <a href=\\\\\\\"https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html\\\\\\\">only 15%</a>, which left experts bewildered. (For what it\\u2019s worth, that number is about a thousandfold higher than what we would be comfortable labeling techno-optimist.) It takes a lot of quantitative training to be able to mentally process very small or very large numbers correctly in decision making, and not simply bucket them into categories like \\u201Cinsignificantly small\\u201D. Most people are not trained this way.&nbsp;</p><p>In short, what seems to be happening is that experts\\u2019 vague intuitions and fears are being translated into pseudo-precise numbers, and then translated back into vague intuitions and fears by policymakers. Let\\u2019s just cut the charade of quantification! The Center for AI Safety\\u2019s <a href=\\\\\\\"https://www.safe.ai/work/statement-on-ai-risk\\\\\\\">Statement on AI Risk</a> was admirably blunt in this regard (of course, we <a href=\\\\\\\"https://www.aisnakeoil.com/p/is-avoiding-extinction-from-ai-really\\\\\\\">strongly disagree with its substance</a>).</p><p>A principled, quantitative way to use probabilities in decision making is utility maximization through cost-benefit analysis. The idea is simple: if we consider an outcome to have a subjective value, or utility, of U (which can be positive or negative), and it has, say, a 10% probability of occurring, we can act as if it is certain to occur and has a value of 0.1 * U. We can then add up the costs and benefits for each option available to us, and choose the one that maximizes costs minus benefits (the \\u201Cexpected utility\\u201D).</p><p>This is where things get really problematic. First, some people might consider extinction to have an unfathomably large negative value, because it precludes the existence of all the human lives, physical or <a href=\\\\\\\"https://www.vox.com/future-perfect/23298870/effective-altruism-longtermism-will-macaskill-future\\\\\\\">simulated</a>, that might ever be born in the future. The logical conclusion is that x-risk should be everyone\\u2019s top priority all the time! It is reminiscent of <a href=\\\\\\\"https://en.wikipedia.org/wiki/Pascal%27s_wager\\\\\\\">Pascal\\u2019s wager</a>, the argument that it is rational believe in God because even if there is an infinitesimally small chance that God exists, the cost of non-belief is infinite (an eternity in hell as opposed to eternal happiness), and hence so is the expected utility. Fortunately, policymakers don\\u2019t give too much credence to decision making frameworks involving infinities. But the idea has taken a powerful hold of the AI safety community and drives some people\\u2019s conviction that AI x-risk should be society\\u2019s top priority.</p><p>Even if we limit ourselves to catastrophic but not existential risks, we are talking about billions of lives on the line, so the expected cost of even a 1% risk is so high that the policy implications are drastic \\u2014 governments should increase spending on AI x-risk mitigation by orders of magnitude and consider draconian measures such as stopping AI development. This is why it is so vital to understand that these estimates are not backed by any methodology. It would be incredibly unwise to make world-changing policy decisions based on so little evidence.</p><div class=\\\\\\\"subscription-widget-wrap-editor\\\\\\\" data-attrs=\\\\\\\"{&quot;url&quot;:&quot;https://www.aisnakeoil.com/subscribe?&quot;,&quot;text&quot;:&quot;Subscribe&quot;,&quot;language&quot;:&quot;en&quot;}\\\\\\\" data-component-name=\\\\\\\"SubscribeWidgetToDOM\\\\\\\"><div class=\\\\\\\"subscription-widget show-subscribe\\\\\\\"><div class=\\\\\\\"preamble\\\\\\\"><p class=\\\\\\\"cta-caption\\\\\\\">You\\u2019re reading AI Snake Oil, a blog about our <a href=\\\\\\\"https://www.aisnakeoil.com/p/ai-snake-oil-is-now-available-to\\\\\\\">book</a>. Subscribe to get new posts.</p></div><form class=\\\\\\\"subscription-widget-subscribe\\\\\\\"><input type=\\\\\\\"email\\\\\\\" class=\\\\\\\"email-input\\\\\\\" name=\\\\\\\"email\\\\\\\" placeholder=\\\\\\\"Type your email\\u2026\\\\\\\" tabindex=\\\\\\\"-1\\\\\\\"><input type=\\\\\\\"submit\\\\\\\" class=\\\\\\\"button primary\\\\\\\" value=\\\\\\\"Subscribe\\\\\\\"><div class=\\\\\\\"fake-input-wrapper\\\\\\\"><div class=\\\\\\\"fake-input\\\\\\\"></div><div class=\\\\\\\"fake-button\\\\\\\"></div></div></form></div></div><h3><strong>Forecasts of milestones suffer from outcome ambiguity</strong></h3><p>Is there a role for forecasting in AI policy? We think yes \\u2014 just not forecasting existential risk. Forecasting AI milestones, such as performance on certain capability benchmarks or economic impacts, is more achievable and meaningful. If a forecaster has demonstrated skill in predicting when various AI milestones would be reached, it does give us evidence that they will do well in the future. We are no longer talking about unique or rare events. And when considering lower-stakes policy interventions \\u2014 preparing for potential economic disruption rather than staving off killer robots \\u2014 it is less critical that forecasts be justified to the satisfaction of every reasonable person.&nbsp;</p><p>The forecasting community devotes a lot of energy to milestone forecasting. On Metaculus, the question \\u201CWill there be Human-machine intelligence parity before 2040?\\u201D has an aggregate prediction of <a href=\\\\\\\"https://www.metaculus.com/questions/384/humanmachine-intelligence-parity-by-2040/\\\\\\\">96%</a> based on over 1,300 forecasters. That\\u2019s remarkable! If we agreed with this forecast, we would be in favor of the position that managing the safe transition to AGI should be a global priority. Why don\\u2019t we?</p><p>The answer is in the fine print. There is no consensus on the definition of a fuzzy concept such as AGI. Even if we fix a definition, determining whether it has been achieved can be hard or impossible. For effective forecasting, it is extremely important to avoid ambiguous outcomes. The way the forecasting community gets around this is by defining it in terms of relatively narrow skills, such as exam performance.&nbsp;</p><p>The Metaculus intelligence parity question is defined in terms of the performance on graduate exams in math, physics, and computer science. Based on this definition, we do agree with the forecast of 96%. But we think the definition is so watered down that it doesn\\u2019t mean much for policy. Forget existential risk \\u2014 as we\\u2019ve written before, AI performance on exams has so little <a href=\\\\\\\"https://www.aisnakeoil.com/p/gpt-4-and-professional-benchmarks\\\\\\\">construct validity</a> that it doesn\\u2019t even let us predict whether AI will replace workers.</p><p>Other benchmarks <a href=\\\\\\\"https://www.aisnakeoil.com/p/new-paper-ai-agents-that-matter\\\\\\\">aren\\u2019t much better</a>. In short, forecasting AI capability timelines is tricky because of the huge gap between benchmarks and real-world implications. Fortunately, <a href=\\\\\\\"https://www.openphilanthropy.org/rfp-llm-benchmarks/\\\\\\\">better benchmarks</a> reflecting consequential real-world tasks are being developed. In addition to benchmarks, we need naturalistic evaluation, even if it is more costly. One type of naturalistic evaluation is to measure how people perform their jobs differently with AI assistance. Directly forecasting economic, social, or political impacts \\u2014 such as labor market transformation or AI-related spending by militaries \\u2014 could be even more useful, although harder to unambiguously define and measure.</p><h3><strong>Concluding thoughts</strong></h3><p>The responsibility for avoiding misuses of probability in policy lies with policymakers. We are not calling for forecasters to stop publishing forecasts in order to \\u201Cprotect\\u201D policymakers from being misled. That said, we think forecasts should be accompanied by a clear explanation of the process used and evidence considered. This would allow policymakers to make informed decisions about whether the justification presented meets the threshold that they are comfortable with. The XPT is a good example of transparency, as is <a href=\\\\\\\"https://arxiv.org/abs/2306.02519\\\\\\\">this paper</a> (though it is not about x-risk). On the other hand, simply surveying a bunch of researchers and presenting aggregate numbers is misinformative and should be ignored by policymakers.</p><p>So what should governments do about AI x-risk? Our view isn\\u2019t that they should do nothing. But they should reject the kind of policies that might seem compelling if we view x-risk as urgent and serious, notably: restricting AI development. As we\\u2019ll argue in a future essay in this series, not only are such policies unnecessary, they are likely to <em>increase</em> x-risk. Instead, governments should adopt policies that are compatible with a range of possible estimates of AI risk, and are on balance helpful even if the risk is negligible. Fortunately, such policies exist. Governments should also change policymaking <em>processes</em> so that they are more responsive to new evidence. More on all that soon.</p><h4><strong>Further reading</strong></h4><ul><li><p>The XPT report is titled <a href=\\\\\\\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf\\\\\\\">Forecasting Existential Risks: Evidence from a Long-Run Forecasting Tournament</a>.</p></li><li><p>Tetlock and Gardner\\u2019s book <a href=\\\\\\\"https://www.amazon.com/Superforecasting-Science-Prediction-Philip-Tetlock/dp/0804136718\\\\\\\">Superforecasting</a> summarizes research by Tetlock, Barbara Mellers, and others.</p></li><li><p>Scott Alexander refutes the claim that there is something wrong in principle with ascribing <a href=\\\\\\\"https://www.astralcodexten.com/p/in-continued-defense-of-non-frequentist\\\\\\\">probabilities to unique events</a> (we largely agree). Our argument differs in two key ways from the position he addresses. We aren\\u2019t talking about forecasting in general; just its application to policymaking. And we don\\u2019t object to it in principle. Our argument is empirical: AI x-risk forecasts are extremely unreliable and lack justification. Theoretically this could change in the future, though we aren\\u2019t holding our breath.</p></li><li><p>A <a href=\\\\\\\"https://www.jstor.org/stable/45094450?seq=1\\\\\\\">paper</a> by Friedman and Zuckhauser explains why probabilities aren\\u2019t the whole story: two forecasts that have the same risk estimate might have very different implications for policymakers. In our view, AI x-risk forecasts fare poorly on two of the three dimensions of confidence: a sound basis in evidence and a narrow range of reasonable opinion.</p></li><li><p>A paper by our Princeton CITP colleagues led by Shazeda Ahmed explains the <a href=\\\\\\\"https://firstmonday.org/ojs/index.php/fm/article/view/13626/11596\\\\\\\">epistemic culture of AI safety</a>, which consists of \\u201Ccohesive, interwoven social structures of knowledge-production and community-building\\u201D. It helps understand why practices such as forecasting have become pillars of how the AI safety community forms its beliefs, as opposed to the broader scientific community that centers practices such as peer review. Of course, we shouldn\\u2019t reject a view just because it doesn\\u2019t conform to scientific orthodoxy. But at the same time, we shouldn\\u2019t give any deference to the self-styled AI safety community\\u2019s views on AI safety. It is important to understand that the median member of the AI safety community holds one particular stance on AI safety \\u2014 a stance that is highly contested and in our view rather alarmist.</p></li><li><p>We have written extensively about <a href=\\\\\\\"https://www.aisnakeoil.com/t/ai-safety\\\\\\\">evidence-based AI safety</a>. Our best-known work includes the essay <a href=\\\\\\\"https://www.aisnakeoil.com/p/ai-safety-is-not-a-model-property\\\\\\\">AI safety is not a model property</a> and the paper titled <a href=\\\\\\\"https://crfm.stanford.edu/open-fms/\\\\\\\">On the Societal Impact of Open Foundation Models</a> which was the result of a large collaboration.</p></li></ul><p><strong>Acknowledgements.</strong> We are grateful to Benjamin Edelman, Ezra Karger, Matt Salganik, and Ollie Stephenson for feedback on a draft. This series of essays is based on an upcoming paper that benefited from feedback from many people, including Seth Lazar and members of the MINT lab at Australian National University, students in the Limits to Prediction course at Princeton, Shazeda Ahmed, and Zachary Siegel.</p><div class=\\\\\\\"footnote\\\\\\\" data-component-name=\\\\\\\"FootnoteToDOM\\\\\\\"><a id=\\\\\\\"footnote-1\\\\\\\" href=\\\\\\\"#footnote-anchor-1\\\\\\\" class=\\\\\\\"footnote-number\\\\\\\" contenteditable=\\\\\\\"false\\\\\\\" target=\\\\\\\"_self\\\\\\\">1</a><div class=\\\\\\\"footnote-content\\\\\\\"><p>Specifically, by using data from nuclear weapons tests and making a few physics-informed assumptions, we can calculate what it would take to kick up enough dust to darken the skies for a prolonged period and lead to a collapse of global agriculture.</p></div></div><div class=\\\\\\\"footnote\\\\\\\" data-component-name=\\\\\\\"FootnoteToDOM\\\\\\\"><a id=\\\\\\\"footnote-2\\\\\\\" href=\\\\\\\"#footnote-anchor-2\\\\\\\" class=\\\\\\\"footnote-number\\\\\\\" contenteditable=\\\\\\\"false\\\\\\\" target=\\\\\\\"_self\\\\\\\">2</a><div class=\\\\\\\"footnote-content\\\\\\\"><p>Suppose G, the conservative forecaster, has a floor of \\u03F5 for their forecasts (in our example, 0.01). There is only an \\u03F5 fraction of events where the difference between the two forecasters is even relevant. Even limiting ourselves to the set of events with a true probability less than \\u03F5, some calculation shows that F\\u2019s expected log score is better than G\\u2019s by a tiny amount \\u2014 O(\\u03F5), and with the Brier score, O(\\u03F5^2). So when looking at all events, the differences in expected scores are O(\\u03F5^2) and O(\\u03F5^3) respectively. Meanwhile, the variance in their scores comes out to about 0.1 for both scoring functions. To be able to confidently assert that the means of the two forecaster\\u2019s scores are different, we need the number of events N to be large enough that the standard deviation of the difference in mean scores, which is sqrt(0.1 / N), to be much less than the expected difference. So we need N ~ O(1/\\u03F5^4) for the log score and N ~ O(1/\\u03F5^6) for the Brier score.</p></div></div><div class=\\\\\\\"footnote\\\\\\\" data-component-name=\\\\\\\"FootnoteToDOM\\\\\\\"><a id=\\\\\\\"footnote-3\\\\\\\" href=\\\\\\\"#footnote-anchor-3\\\\\\\" class=\\\\\\\"footnote-number\\\\\\\" contenteditable=\\\\\\\"false\\\\\\\" target=\\\\\\\"_self\\\\\\\">3</a><div class=\\\\\\\"footnote-content\\\\\\\"><p>Of course, there are also reasons why forecasters might underestimate the risk. We find those reasons less persuasive, but more importantly, we consider them out of scope for this discussion. The reason is simple: the advocacy for governments to implement freedom-restricting policies is being justified by <em>high</em> x-risk estimates. The burden of evidence is thus asymmetric. Those who call for such policies must justify their probability estimates, including responding to concerns about upward biases. If, in some strange future world, some people advocate for restrictive policies based on <em>low</em> x-risk estimates, it will become important to subject potential <em>underestimation</em> to the same scrutiny.</p></div></div><div class=\\\\\\\"footnote\\\\\\\" data-component-name=\\\\\\\"FootnoteToDOM\\\\\\\"><a id=\\\\\\\"footnote-4\\\\\\\" href=\\\\\\\"#footnote-anchor-4\\\\\\\" class=\\\\\\\"footnote-number\\\\\\\" contenteditable=\\\\\\\"false\\\\\\\" target=\\\\\\\"_self\\\\\\\">4</a><div class=\\\\\\\"footnote-content\\\\\\\"><p>In the theory of forecasting, the logarithmic score and other so-called proper scoring rules are mathematically guaranteed to elicit the forecaster\\u2019s \\\\\\\"true belief\\\\\\\". But the definition of a proper scoring rule assumes that the true belief is a single value, whereas it tends to be a wide range. In such a scenario, the forecaster is incentivized to report the mean of their distribution, but what we intuitively mean by the \\u201Ctrue belief\\u201D might better correspond to the median.</p><p></p></div></div>\\\",\\\"longer_truncated_body_json\\\":null,\\\"longer_truncated_body_html\\\":null,\\\"truncated_body_text\\\":\\\"How seriously should governments take the threat of existential risk from AI, given the lack of consensus among researchers? On the one hand, existential risks (x-risks) are necessarily somewhat speculative: by the time there is concrete evidence, it may be too late. On the other hand, governments must prioritize \\u2014 after all, they don\\u2019t worry too much a\\u2026\\\",\\\"wordcount\\\":5726,\\\"postTags\\\":[{\\\"id\\\":\\\"37caad47-2b5a-49e8-bc6f-3e68e38cae48\\\",\\\"publication_id\\\":1008003,\\\"name\\\":\\\"AI policy\\\",\\\"slug\\\":\\\"ai-policy\\\",\\\"hidden\\\":false},{\\\"id\\\":\\\"87877975-29c1-4928-a0e0-472f006eec00\\\",\\\"publication_id\\\":1008003,\\\"name\\\":\\\"AI safety\\\",\\\"slug\\\":\\\"ai-safety\\\",\\\"hidden\\\":false}],\\\"postCountryBlocks\\\":[],\\\"publishedBylines\\\":[{\\\"id\\\":19265909,\\\"name\\\":\\\"Arvind Narayanan\\\",\\\"handle\\\":\\\"arvindnarayanan\\\",\\\"previous_name\\\":null,\\\"photo_url\\\":\\\"https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/149ffaff-ad45-4b76-9667-bbd39d3a3d23_1795x1795.jpeg\\\",\\\"bio\\\":null,\\\"profile_set_up_at\\\":\\\"2022-08-24T17:33:38.557Z\\\",\\\"publicationUsers\\\":[{\\\"id\\\":953575,\\\"user_id\\\":19265909,\\\"publication_id\\\":1008003,\\\"role\\\":\\\"admin\\\",\\\"public\\\":true,\\\"is_primary\\\":false,\\\"publication\\\":{\\\"id\\\":1008003,\\\"name\\\":\\\"AI Snake Oil\\\",\\\"subdomain\\\":\\\"aisnakeoil\\\",\\\"custom_domain\\\":\\\"www.aisnakeoil.com\\\",\\\"custom_domain_optional\\\":false,\\\"hero_text\\\":\\\"What Artificial Intelligence Can Do, What It Can\\u2019t, and How to Tell the Difference\\\",\\\"logo_url\\\":\\\"https://substack-post-media.s3.amazonaws.com/public/images/6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png\\\",\\\"author_id\\\":891603,\\\"theme_var_background_pop\\\":\\\"#B599F1\\\",\\\"created_at\\\":\\\"2022-07-19T16:17:19.439Z\\\",\\\"rss_website_url\\\":null,\\\"email_from_name\\\":\\\"Sayash and Arvind from AI Snake Oil\\\",\\\"copyright\\\":\\\"Sayash Kapoor and Arvind Narayanan\\\",\\\"founding_plan_name\\\":null,\\\"community_enabled\\\":true,\\\"invite_only\\\":false,\\\"payments_state\\\":\\\"disabled\\\",\\\"language\\\":null,\\\"explicit\\\":false,\\\"is_personal_mode\\\":false}}],\\\"is_guest\\\":false,\\\"bestseller_tier\\\":null},{\\\"id\\\":891603,\\\"name\\\":\\\"Sayash Kapoor\\\",\\\"handle\\\":\\\"sayash\\\",\\\"previous_name\\\":null,\\\"photo_url\\\":\\\"https://substack-post-media.s3.amazonaws.com/public/images/30f87ce8-8dbc-468f-8f8b-9fbf430e323c_976x974.png\\\",\\\"bio\\\":\\\"CS PhD candidate at Princeton. I study the societal impact of AI. Currently writing a book on AI Snake Oil: http://aisnakeoil.com \\\",\\\"profile_set_up_at\\\":\\\"2022-07-19T16:16:04.452Z\\\",\\\"publicationUsers\\\":[{\\\"id\\\":953545,\\\"user_id\\\":891603,\\\"publication_id\\\":1008003,\\\"role\\\":\\\"admin\\\",\\\"public\\\":true,\\\"is_primary\\\":false,\\\"publication\\\":{\\\"id\\\":1008003,\\\"name\\\":\\\"AI Snake Oil\\\",\\\"subdomain\\\":\\\"aisnakeoil\\\",\\\"custom_domain\\\":\\\"www.aisnakeoil.com\\\",\\\"custom_domain_optional\\\":false,\\\"hero_text\\\":\\\"What Artificial Intelligence Can Do, What It Can\\u2019t, and How to Tell the Difference\\\",\\\"logo_url\\\":\\\"https://substack-post-media.s3.amazonaws.com/public/images/6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png\\\",\\\"author_id\\\":891603,\\\"theme_var_background_pop\\\":\\\"#B599F1\\\",\\\"created_at\\\":\\\"2022-07-19T16:17:19.439Z\\\",\\\"rss_website_url\\\":null,\\\"email_from_name\\\":\\\"Sayash and Arvind from AI Snake Oil\\\",\\\"copyright\\\":\\\"Sayash Kapoor and Arvind Narayanan\\\",\\\"founding_plan_name\\\":null,\\\"community_enabled\\\":true,\\\"invite_only\\\":false,\\\"payments_state\\\":\\\"disabled\\\",\\\"language\\\":null,\\\"explicit\\\":false,\\\"is_personal_mode\\\":false}}],\\\"is_guest\\\":false,\\\"bestseller_tier\\\":null}],\\\"reaction\\\":null,\\\"reaction_count\\\":100,\\\"comment_count\\\":37,\\\"child_comment_count\\\":24,\\\"audio_items\\\":[{\\\"post_id\\\":147019742,\\\"voice_id\\\":\\\"en-US-JennyNeural\\\",\\\"audio_url\\\":\\\"https://substack-video.s3.amazonaws.com/video_upload/post/147019742/tts/30c2171a-ccb1-4fa1-a6c3-5d395a013fec/en-US-JennyNeural.mp3\\\",\\\"type\\\":\\\"tts\\\",\\\"status\\\":\\\"completed\\\"}],\\\"is_geoblocked\\\":false,\\\"hasCashtag\\\":false},\\\"comments\\\":[{\\\"id\\\":63384762,\\\"body\\\":\\\"Doesn't uncertainty cuts both ways? Sure, I can get on board with saying that forecasts are unreliable, there are no really good reference classes to use, etc. But doesn't that also mean you can't confidently state that \\\\\\\"not only are such policies unnecessary, they are likely to increase x-risk\\\\\\\"? And if you can't confidently state one way or the other, then it's not at all clear to me that the correct approach is to not restrict AI development. (It's also not at all clear to me that the correct approach is the opposite, of course.) So, sure, I am happy to get on board with, \\\\\\\"governments should adopt policies that are compatible with a range of possible estimates of AI risk, and are on balance helpful even if the risk is negligible.\\\\\\\" But shouldn't we also make sure that the policies are on balance helpful even if the risk is high?\\\",\\\"publication_id\\\":1008003,\\\"post_id\\\":147019742,\\\"user_id\\\":10448526,\\\"ancestor_path\\\":\\\"\\\",\\\"type\\\":\\\"comment\\\",\\\"deleted\\\":false,\\\"date\\\":\\\"2024-07-26T14:24:44.987Z\\\",\\\"edited_at\\\":null,\\\"reply_from_post_id\\\":null,\\\"status\\\":\\\"published\\\",\\\"pinned_by_user_id\\\":null,\\\"restacks\\\":0,\\\"name\\\":\\\"Aryeh L. Englander\\\",\\\"photo_url\\\":\\\"https://substack-post-media.s3.amazonaws.com/public/images/50a9f95a-009b-4da0-a1a4-2b18366ab848_241x241.jpeg\\\",\\\"handle\\\":\\\"aryehlenglander\\\",\\\"reactions\\\":{\\\"\\u2764\\\":19},\\\"reactor_names\\\":[],\\\"reaction\\\":null,\\\"reaction_count\\\":19,\\\"children\\\":[],\\\"bans\\\":[],\\\"suppressed\\\":false,\\\"user_banned\\\":false,\\\"user_banned_for_comment\\\":false,\\\"user_slug\\\":\\\"aryehlenglander\\\",\\\"metadata\\\":{\\\"is_author\\\":false,\\\"membership_state\\\":\\\"free_signup\\\",\\\"eligibleForGift\\\":false,\\\"author_on_other_pub\\\":{\\\"name\\\":\\\"Aryeh L. Englander\\\",\\\"id\\\":2931437,\\\"base_url\\\":\\\"https://aryehlenglander.substack.com\\\"}},\\\"user_bestseller_tier\\\":null,\\\"can_dm\\\":false,\\\"score\\\":20,\\\"children_count\\\":2,\\\"reported_by_user\\\":false,\\\"restacked\\\":false,\\\"childrenSummary\\\":\\\"2 replies\\\"},{\\\"id\\\":63417221,\\\"body\\\":\\\"This was clarifying, and there's another tricky issue that I'm curious to know your thoughts on, which is that policy-making requires a causal estimate of the impact of the proposed intervention, and it is unclear how \\\\\\\"P(doom)\\\\\\\" handles causality.\\\\n\\\\nFor the asteroid example, the causality issue is simple enough, since asteroid impacts are a natural phenomenon, so we can ignore human activity when making the estimate. But if you were to want an estimate of asteroid extinction risk that _includes_ human activity, the probability decreases: after all, if we did find a large asteroid on a collision course for Earth, we'd probably try to divert it, and there's a non-negligible chance that we'd succeed. But even if we thought that we'd certainly succeed at diverting the asteroid, it'd be incorrect to say \\\\\\\"we don't need to mitigate asteroid extinction because the probability is ~0%\\\\\\\", because choosing not to mitigate would raise the probability. So excluding human activity is clearly the right choice.\\\\n\\\\nWith AI x-risk though, if we exclude human activity, there is no risk, because AI is only developed as a result of human activity. It seems like forecasters implicitly try to handle this by drawing a distinction between \\\\\\\"AI capabilities\\\\\\\" and \\\\\\\"AI safety\\\\\\\", then imagining hypothetically increasing capabilities without increasing safety. But this hypothetical is hopelessly unrealistic: companies try to increase the controllability and reliability of their AI systems as a normal part of product development.\\\\n\\\\nEven in climate change, where the risks are caused by human activity, a reasonably clean separation between business-as-usual and mitigation is possible. In the absence of any incentives for mitigation, your own CO2 emissions are irrelevant. So while it may be very hard to determine which climate-mitigation actions are net beneficial, at least we have a well-defined no-mitigation baseline to compare against.\\\\n\\\\nWith AI, unlike with climate, it seems hopeless to try to find a well-defined no-mitigation baseline, because, as mentioned before, having an AI system do what you want is also a key aspect of being a good product. Surely this makes the probabilistic approach to AI x-risk entirely useless.\\\",\\\"publication_id\\\":1008003,\\\"post_id\\\":147019742,\\\"user_id\\\":18241816,\\\"ancestor_path\\\":\\\"\\\",\\\"type\\\":\\\"comment\\\",\\\"deleted\\\":false,\\\"date\\\":\\\"2024-07-26T18:52:47.362Z\\\",\\\"edited_at\\\":null,\\\"reply_from_post_id\\\":null,\\\"status\\\":\\\"published\\\",\\\"pinned_by_user_id\\\":null,\\\"restacks\\\":0,\\\"name\\\":\\\"Malcolm Sharpe\\\",\\\"photo_url\\\":\\\"https://substack-post-media.s3.amazonaws.com/public/images/ad2b8fee-820f-4544-95b7-afc7a0c9cc12_144x144.png\\\",\\\"handle\\\":\\\"malcolmsharpe924005\\\",\\\"reactions\\\":{\\\"\\u2764\\\":5},\\\"reactor_names\\\":[\\\"Arvind Narayanan\\\"],\\\"reaction\\\":null,\\\"reaction_count\\\":5,\\\"children\\\":[],\\\"bans\\\":[],\\\"suppressed\\\":false,\\\"user_banned\\\":false,\\\"user_banned_for_comment\\\":false,\\\"user_slug\\\":\\\"malcolmsharpe924005\\\",\\\"metadata\\\":{\\\"is_author\\\":false,\\\"membership_state\\\":\\\"free_signup\\\",\\\"eligibleForGift\\\":false},\\\"user_bestseller_tier\\\":null,\\\"can_dm\\\":false,\\\"score\\\":10,\\\"children_count\\\":1,\\\"reported_by_user\\\":false,\\\"restacked\\\":false,\\\"childrenSummary\\\":\\\"3 replies\\\"}],\\\"canonicalUrl\\\":\\\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities\\\",\\\"inlineComments\\\":false,\\\"readerIsSearchCrawler\\\":false,\\\"ogUrl\\\":\\\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities\\\",\\\"bannedFromNotes\\\":false,\\\"themeVariables\\\":{\\\"color_theme_bg_pop\\\":\\\"#fd6752\\\",\\\"background_pop\\\":\\\"#fd6752\\\",\\\"color_theme_bg_web\\\":\\\"#ffffff\\\",\\\"cover_bg_color\\\":\\\"#ffffff\\\",\\\"background_pop_darken\\\":\\\"#fd5139\\\",\\\"print_on_pop\\\":\\\"#ffffff\\\",\\\"color_theme_bg_pop_darken\\\":\\\"#fd5139\\\",\\\"color_theme_print_on_pop\\\":\\\"#ffffff\\\",\\\"border_subtle\\\":\\\"rgba(204, 204, 204, 0.5)\\\",\\\"background_subtle\\\":\\\"rgba(255, 232, 229, 0.4)\\\",\\\"print_pop\\\":\\\"#fd6752\\\",\\\"color_theme_accent\\\":\\\"#fd6752\\\",\\\"cover_print_primary\\\":\\\"#363737\\\",\\\"cover_print_secondary\\\":\\\"#757575\\\",\\\"cover_print_tertiary\\\":\\\"#b6b6b6\\\",\\\"cover_border_color\\\":\\\"#fd6752\\\",\\\"font_family_headings_preset\\\":\\\"'SF Pro Display', -apple-system, system-ui, BlinkMacSystemFont, 'Inter', 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'\\\",\\\"font_weight_headings_preset\\\":900,\\\"font_family_body_preset\\\":\\\"Lora,sans-serif\\\",\\\"font_weight_body_preset\\\":400,\\\"font_size_body_offset\\\":\\\"1px\\\",\\\"font_preset_heading\\\":\\\"heavy_sans\\\",\\\"font_preset_body\\\":\\\"fancy_serif\\\",\\\"home_hero\\\":\\\"newspaper\\\",\\\"home_posts\\\":\\\"custom\\\",\\\"web_bg_color\\\":\\\"#ffffff\\\",\\\"background_contrast_1\\\":\\\"#f0f0f0\\\",\\\"color_theme_bg_contrast_1\\\":\\\"#f0f0f0\\\",\\\"background_contrast_2\\\":\\\"#dddddd\\\",\\\"color_theme_bg_contrast_2\\\":\\\"#dddddd\\\",\\\"background_contrast_3\\\":\\\"#b7b7b7\\\",\\\"color_theme_bg_contrast_3\\\":\\\"#b7b7b7\\\",\\\"background_contrast_4\\\":\\\"#929292\\\",\\\"color_theme_bg_contrast_4\\\":\\\"#929292\\\",\\\"background_contrast_5\\\":\\\"#515151\\\",\\\"color_theme_bg_contrast_5\\\":\\\"#515151\\\",\\\"color_theme_detail\\\":\\\"#e6e6e6\\\",\\\"background_contrast_pop\\\":\\\"rgba(253, 103, 82, 0.4)\\\",\\\"color_theme_bg_contrast_pop\\\":\\\"rgba(253, 103, 82, 0.4)\\\",\\\"input_background\\\":\\\"#ffffff\\\",\\\"cover_input_background\\\":\\\"#ffffff\\\",\\\"tooltip_background\\\":\\\"#191919\\\",\\\"web_bg_color_h\\\":\\\"0\\\",\\\"web_bg_color_s\\\":\\\"0%\\\",\\\"web_bg_color_l\\\":\\\"100%\\\",\\\"print_on_web_bg_color\\\":\\\"#363737\\\",\\\"print_secondary_on_web_bg_color\\\":\\\"#868787\\\",\\\"selected_comment_background_color\\\":\\\"#fdf9f3\\\",\\\"background_pop_rgb\\\":\\\"253, 103, 82\\\",\\\"background_pop_rgb_pc\\\":\\\"253 103 82\\\",\\\"color_theme_bg_pop_rgb\\\":\\\"253, 103, 82\\\",\\\"color_theme_bg_pop_rgb_pc\\\":\\\"253 103 82\\\",\\\"color_theme_accent_rgb\\\":\\\"253, 103, 82\\\",\\\"color_theme_accent_rgb_pc\\\":\\\"253 103 82\\\"},\\\"recentEpisodes\\\":null,\\\"trackFrontendVisit\\\":true,\\\"isChatActive\\\":false,\\\"isMeetingsActive\\\":false,\\\"features\\\":{},\\\"showCookieBanner\\\":false,\\\"disabledCookies\\\":[],\\\"dd_env\\\":\\\"prod\\\",\\\"dd_ti\\\":true}\")</script>\n        <script>window._analyticsConfig = JSON.parse(\"{\\\"properties\\\":{\\\"subdomain\\\":\\\"aisnakeoil\\\",\\\"publication_id\\\":1008003,\\\"has_plans\\\":false,\\\"pub_community_enabled\\\":true,\\\"is_personal_publication\\\":false,\\\"is_subscribed\\\":false,\\\"is_free_subscribed\\\":false,\\\"is_author\\\":false,\\\"is_contributor\\\":false,\\\"is_admin\\\":false,\\\"is_founding\\\":false},\\\"adwordsAccountId\\\":\\\"AW-316245675\\\",\\\"adwordsEventSendTo\\\":\\\"Tf76CKqcyL4DEKuN5pYB\\\"}\")</script>\n        <script type=\"module\" src=\"https://substackcdn.com/bundle/assets/main-aebe673d.js\" charset=\"utf-8\"  defer></script>\n        <script nomodule>\n            (function() {\n                var message = 'Your browser does not support modern JavaScript modules. Please upgrade your browser for the best experience.';\n                var warningDiv = document.createElement('div');\n                warningDiv.style.color = 'red';\n                warningDiv.style.padding = '10px';\n                warningDiv.style.margin = '10px 0';\n                warningDiv.style.border = '1px solid red';\n                warningDiv.style.backgroundColor = 'lightyellow';\n                warningDiv.innerText = message;\n                document.body.prepend(warningDiv);\n            })();\n        </script>\n\n        \n            <!-- Datadog Analytics -->\n            <script>\n              (function(h,o,u,n,d) {\n                h=h[d]=h[d]||{q:[],onReady:function(c){h.q.push(c)}}\n                d=o.createElement(u);d.async=1;d.src=n\n                n=o.getElementsByTagName(u)[0];n.parentNode.insertBefore(d,n)\n              })(window,document,'script','https://www.datadoghq-browser-agent.com/us1/v5/datadog-rum.js','DD_RUM')\n              window.DD_RUM.onReady(function() {\n                window.DD_RUM.init({\n                  clientToken: 'puba71073f072643721169b68f352438710',\n                  applicationId: '2e321b35-c76b-4073-8d04-cc9a10461793',\n                  site: 'datadoghq.com',\n                  service: 'substack-web',\n                  env: window._preloads.dd_env,\n                  version: 'fffe4ad62a630bc7d36969307f4a3d831bdc356b',\n                  sessionSampleRate: 1,\n                  sessionReplaySampleRate: 100,\n                  trackUserInteractions: window._preloads.dd_ti,\n                  trackResources: true,\n                  trackLongTasks: true,\n                  defaultPrivacyLevel: 'mask-user-input',\n                  allowedTracingUrls: [/https?:\\/\\/(.+\\/.)?substack(cdn)?\\.com/]\n                });\n              })\n            </script>\n            <!-- End Datadog Analytics -->\n\n            <!-- Cloudflare Web Analytics -->\n            <script defer src=\"https://static.cloudflareinsights.com/beacon.min.js\" data-cf-beacon='{\"token\": \"216309cffb464db4b0e02daf0b8e8060\"}'></script>\n            <!-- End Cloudflare Web Analytics -->\n        \n\n        <!-- Fallback tracking pixels -->\n        \n\n        \n\n        <noscript>\n    <style>\n        #nojs-banner {\n            position: fixed;\n            bottom: 0;\n            left: 0;\n            padding: 16px 16px 16px 32px;\n            width: 100%;\n            box-sizing: border-box;\n            background: red;\n            color: white;\n            font-family: -apple-system, \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\";\n            font-size: 13px;\n            line-height: 13px;\n        }\n        #nojs-banner a {\n            color: inherit;\n            text-decoration: underline;\n        }\n    </style>\n\n    <div id=\"nojs-banner\">\n        This site requires JavaScript to run correctly. Please <a href=\"https://enable-javascript.com/\" target=\"_blank\">turn on JavaScript</a> or unblock scripts\n    </div>\n</noscript>\n\n\n        \n\n        \n\n        \n        \n    <script defer src=\"https://static.cloudflareinsights.com/beacon.min.js/vcd15cbe7772f49c399c6a5babf22c1241717689176015\" integrity=\"sha512-ZpsOmlRQV6y907TI0dKBHq9Md29nnaEIPlkf84rnaERnq6zvWvPUqr2ft8M1aS28oN72PdrCzSjY4U6VaAw1EQ==\" data-cf-beacon='{\"rayId\":\"8ba6023c8ab917b1\",\"version\":\"2024.8.0\",\"serverTiming\":{\"name\":{\"cfL4\":true}},\"token\":\"68cfe66b5c4749e2ba64d4d9640c04c0\",\"b\":1}' crossorigin=\"anonymous\"></script>\n</body>\n</html>\n","oembed":false,"readabilityObject":{"title":"AI existential risk probabilities are too unreliable to inform policy","content":"<div id=\"readability-page-1\" class=\"page\"><div dir=\"auto\"><p>How seriously should governments take the threat of existential risk from AI, given the lack of consensus among researchers? On the one hand, existential risks (x-risks) are necessarily somewhat speculative: by the time there is concrete evidence, it may be too late. On the other hand, governments must prioritize — after all, they don’t worry too much about x-risk from alien invasions.</p><p>This is the first in a series of essays laying out an evidence-based approach for policymakers concerned about AI x-risk, an approach that stays grounded in reality while acknowledging that there are “unknown unknowns”.&nbsp;</p><p>In this first essay, we look at one type of evidence: probability estimates. The AI safety community relies heavily on forecasting the probability of human extinction due to AI (in a given timeframe) in order to inform decision making and policy. An estimate of 10% over a few decades, for example, would obviously be high enough for the issue to be a top priority for society.&nbsp;</p><p>Our central claim is that AI x-risk forecasts are far too unreliable to be useful for policy, and in fact highly misleading.</p><p><span>If the two of us predicted an 80% probability of aliens landing on earth in the next ten years, would you take this possibility seriously? Of course not. You would ask to see our evidence. As obvious as this may seem, it seems to have been forgotten in the AI x-risk debate that probabilities carry no authority by themselves. Probabilities are </span><em>usually</em><span> derived from some grounded method, so we have a strong cognitive bias to view quantified risk estimates as more valid than qualitative ones. But it is possible for probabilities to be nothing more than guesses. Keep this in mind throughout this essay (and more broadly in the AI x-risk debate).</span></p><p><span>If we predicted odds for the Kentucky Derby, we don’t have to give you a reason — you can take it or leave it. But if a policymaker takes actions based on probabilities put forth by a forecaster, they had better be able to explain those probabilities to the public (and that explanation must in turn come from the forecaster). </span><a href=\"https://plato.stanford.edu/entries/justification-public/\" rel=\"\">Justification</a><span> is essential to legitimacy of government and the exercise of power. A core principle of liberal democracy is that the state should not limit people's freedom based on controversial beliefs that reasonable people can reject.&nbsp;</span></p><p>Explanation is especially important when the policies being considered are costly, and even more so when those costs are unevenly distributed among stakeholders. A good example is restricting open releases of AI models. Can governments convince people and companies who stand to benefit from open models that they should make this sacrifice because of a speculative future risk?</p><p>The main aim of this essay is analyzing whether there is any justification for any of the specific x-risk probability estimates that have been cited in the policy debate. We have no objection to AI x-risk forecasting as an academic activity, and forecasts may be helpful to companies and other private decision makers. We only question its use in the context of public policy.</p><p>There are basically only three known ways by which a forecaster can try to convince a skeptic: inductive, deductive, and subjective probability estimation. We consider each of these in the following sections. All three require both parties to agree on some basic assumptions about the world (which cannot themselves be proven). The three approaches differ in terms of the empirical and logical ways in which the probability estimate follows from that set of assumptions.</p><p>Most risk estimates are inductive: they are based on past observations. For example, insurers base their predictions of an individual’s car accident risk on data from past accidents about similar drivers. The set of observations used for probability estimation is called a reference class. A suitable reference class for car insurance might be the set of drivers who live in the same city. If the analyst has more information about the individual, such as their age or the type of car they drive, the reference class can be further refined.&nbsp;</p><p>For existential risk from AI, there is no reference class, as it is an event like no other. To be clear, this is a matter of degree, not kind. There is never a clear “correct” reference class to use, and the choice of a reference class in practice comes down to the analyst’s intuition.&nbsp;</p><p>The accuracy of the forecasts depends on the degree of similarity between the process that generates the event being forecast and the process that generated the events in the reference class, which can be seen as a spectrum. For predicting the outcome of a physical system such as a coin toss, past experience is a highly reliable guide. Next, for car accidents, risk estimates might vary by, say, 20% based on the past dataset used — good enough for insurance companies.&nbsp;</p><div><figure><a target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png\" data-component-name=\"Image2ToDOM\" rel=\"\"><div><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png\" width=\"1238\" height=\"334\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:334,&quot;width&quot;:1238,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:41682,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f3c0d76-b6af-4713-8bff-5c0f144c4f04_1238x334.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture></div></a></figure></div><p><span>Further along the spectrum are geopolitical events, where the choice of reference class gets even fuzzier. Forecasting expert Philip Tetlock </span><a href=\"https://www.jstor.org/stable/j.ctt1pk86s8.5\" rel=\"\">explains</a><span>: “Grexit may have looked sui generis, because no country had exited the Eurozone as of 2015, but it could also be viewed as just another instance of a broad comparison class, such as negotiation failures, or of a narrower class, such as a nation-states withdrawing from international agreements or, narrower still, of forced currency conversions.” He goes on to defend the idea that even seeming </span><a href=\"https://en.wikipedia.org/wiki/Black_swan_theory\" rel=\"\">Black Swan events</a><span> like the collapse of the USSR or the Arab Spring can be modeled as members of reference classes, and that inductive reasoning is useful even for this kind of event.</span></p><p><span>In Tetlock’s spectrum, these events represent the “peak” of uniqueness. When it comes to geopolitical events, that might be true. But even those events are far less unique than extinction from AI. Just look at the </span><a href=\"https://www.openphilanthropy.org/wp-content/uploads/2023.05.22-AI-Reference-Classes-Zachary-Freitas-Groff.pdf\" rel=\"\">attempts</a><span> to find reference classes for AI x-risk: animal extinction (as a reference class for human extinction), past global transformations such as the industrial revolution (as a reference class for socioeconomic transformation from AI), or accidents causing mass deaths (as a reference class for accidents causing global catastrophe). Let’s get real. None of those tell us anything about the possibility of developing superintelligent AI or losing control over such AI, which are the central sources of uncertainty for AI x-risk forecasting.&nbsp;</span></p><p>To summarize, human extinction due to AI is an outcome so far removed from anything that has happened in the past that we cannot use inductive methods to “predict” the odds. Of course, we can get qualitative insights from past technical breakthroughs as well as past catastrophic events, but AI risk is sufficiently different that quantitative estimates lack the kind of justification needed for legitimacy in policymaking.</p><p><span>In Conan Doyle’s </span><em>The Adventure of the Six Napoleons</em><span> </span><em>—</em><span> spoiler alert! </span><em>— </em><span>Sherlock Holmes announces before embarking on a stakeout that the probability of catching the suspect is exactly two-thirds. This seems bewildering — how can anything related to human behavior be ascribed a mathematically precise probability?</span></p><p><span>It turns out that Holmes has deduced the underlying series of events that gave rise to the suspect’s seemingly erratic observed behavior: the suspect is methodically searching for a jewel that is known to be hidden inside one of six busts of Napoleon owned by different people in and around London. The details aren’t too important, but the key is that neither the suspect nor the detectives know </span><em>which</em><span> of the six busts it is in, and everything else about the suspect’s behavior is (assumed to be) entirely predictable. Hence the precisely quantifiable uncertainty.</span></p><p>The point is that if we have a model of the world that we can rely upon, we can estimate risk through logical deduction, even without relying on past observations. Of course, outside of fictional scenarios, the world isn’t so neat, especially when we want to project far into the future.</p><p>When it comes to x-risk, there is an interesting exception to the general rule that we don’t have deductive models — asteroid impact. A combination of inductive and deductive risk estimation does allow us to estimate the probability of x-risk, only because we’re talking about a purely physical system. Let’s take a minute to review how this works, because it’s important to recognize that the methods are not generalizable to other types of x-risk.&nbsp;</p><p><span>The key is being able to </span><a href=\"https://www.nature.com/articles/367033a0\" rel=\"\">model</a><span> the relationship between the size of the asteroid (more precisely, the energy of impact) and the frequency of impact. Since we have observed thousands of small impacts, we can extrapolate to infer the frequency of large impacts that have never been directly observed. We can also estimate the threshold that would cause global catastrophe.</span><span><a data-component-name=\"FootnoteAnchorToDOM\" id=\"footnote-anchor-1-147019742\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities#footnote-1-147019742\" target=\"_self\" rel=\"\">1</a></span></p><div><figure><a target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png\" data-component-name=\"Image2ToDOM\" rel=\"\"><div><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png\" width=\"1456\" height=\"559\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:559,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1119384,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3483cbd6-7b7c-4fb8-833c-66c4c1f4591f_2396x920.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture></div></a><figcaption><span>Figure: data on small asteroid impacts (illustrated on the </span><a href=\"https://commons.wikimedia.org/wiki/File:Bolide_events_1994-2013.jpg\" rel=\"\">left</a><span>) can be extrapolated to extinction-level impacts (right).</span></figcaption></figure></div><p><span>With AI, the unknowns relate to </span><a href=\"https://www.aisnakeoil.com/p/ai-scaling-myths\" rel=\"\">technological progress</a><span> and governance rather than a physical system, so it isn’t clear how to model it mathematically. Still, people have tried. For example, in order to predict the computational requirements of a hypothetical AGI, </span><a href=\"https://arxiv.org/pdf/2306.02519\" rel=\"\">several</a><span> </span><a href=\"https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines\" rel=\"\">works</a><span> assume that an AI system would require roughly as many computations as the human brain, and further make assumptions about the number of computations required by the human brain. These assumptions are far more tenuous than those involved in asteroid modeling, and none of this even addresses the loss-of-control question.</span></p><p>Without the reference classes or grounded theories, forecasts are necessarily “subjective probabilities”, that is, guesses based on the forecaster’s judgment. Unsurprisingly, these vary by orders of magnitude.</p><p><span>Subjective probability estimation does not get around the need for having either an inductive or a deductive basis for probability estimates. It merely avoids the need for the forecaster to </span><em>explain</em><span> their estimate. Explanation can be hard due to humans’ limited ability to explain our intuitive reasoning, whether inductive, deductive, or a combination thereof. Essentially, it allows the forecaster to say: “even though I haven’t shown my methods, you can trust this estimate because of my track record” (we explain in the next section why even this breaks down for AI x-risk forecasting). But ultimately, lacking either an inductive or a deductive basis, all that forecasters can do is to make up a number, and those made-up numbers are all over the place.</span></p><div><figure><a target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png\" data-component-name=\"Image2ToDOM\" rel=\"\"><div><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png\" width=\"1272\" height=\"896\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:896,&quot;width&quot;:1272,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:172573,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ce8a04c-c569-4da7-9c8d-3d4b01ca20de_1272x896.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture></div></a></figure></div><p><span>Consider the </span><a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf\" rel=\"\">Existential Risk Persuasion Tournament</a><span> (XPT) conducted by the Forecasting Research Institute in late 2022, which we think is the most elaborate and well-executed x-risk forecasting exercise conducted to date. It involved various groups of forecasters, including AI experts and forecasting experts (“superforecasters” in the figure). For AI experts, the high end (75th percentile) of estimates for AI extinction risk by 2100 is 12%, the median estimate is 3%, and the low end (25th percentile) is 0.25%. For forecasting experts, even the high end (75th percentile) is only 1%, the median is a mere 0.38%, and the low end (25th percentile) is visually indistinguishable from zero on the graph. In other words, the 75th percentile AI expert forecast and the 25th percentile superforecaster forecast differ by at least a factor of 100.</span></p><p>All of these estimates are from people who have deep expertise on the topic and participated in a months-long tournament where they tried to persuade each other! If this range of forecasts here isn’t extreme enough, keep in mind that this whole exercise was conducted by one group at one point in time. We might get different numbers if the tournament were repeated today, if the questions were framed differently, etc.</p><p>What’s most telling is to look at the rationales that forecasters provided, which are extensively detailed in the report. They aren’t using quantitative models, especially when thinking about the likelihood of bad outcomes conditional on developing powerful AI. For the most part, forecasters are engaging in the same kind of speculation that everyday people do when they discuss superintelligent AI. Maybe AI will take over critical systems through superhuman persuasion of system operators. Maybe AI will seek to lower global temperatures because it helps computers run faster, and accidentally wipe out humanity. Or maybe AI will seek resources in space rather than Earth, so we don’t need to be as worried. There’s nothing wrong with such speculation. But we should be clear that when it comes to AI x-risk, forecasters aren’t drawing on any special knowledge, evidence, or models that make their hunches more credible than yours or ours or anyone else’s.&nbsp;&nbsp;</p><p><span>The term </span><a href=\"https://www.amazon.com/Superforecasting-Science-Prediction-Philip-Tetlock/dp/0804136718\" rel=\"\">superforecasting</a><span> comes from Philip Tetlock’s 20 year study of forecasting (he was also one of the organizers of the XPT). Superforecasters tend to be trained in methods to improve forecasts such as by integrating diverse information and by minimizing psychological biases. These methods have been shown to be effective in domains such as geopolitics. But no amount of training will lead to good forecasts if there isn’t much useful evidence to draw from.</span></p><p><span>Even if forecasters had credible quantitative models (they don’t), they must account for “unknown unknowns”, that is, the possibility that the model itself might be wrong. As noted x-risk philosopher Nick Bostrom </span><a href=\"https://existential-risk.com/concept.pdf\" rel=\"\">explains</a><span>: “The uncertainty and error-proneness of our first-order assessments of risk is itself something we must factor into our all-things-considered probability assignments. This factor often dominates in low-probability, high-consequence risks — especially those involving poorly understood natural phenomena, complex social dynamics, or new technology, or that are difficult to assess for other reasons.”&nbsp;</span></p><p><span>This is a reasonable perspective, and AI x-risk forecasters do worry a lot about uncertainty in risk assessment. But one consequence of this is that for those who follow this principle, forecasts are </span><em>guaranteed</em><span> to be guesses rather than the output of a model — after all, no model can be used to estimate the probability that the model itself is wrong, or what the risk would be if the model were wrong.</span></p><p><span>To recap, subjective AI-risk forecasts vary by orders of magnitude. But if we can measure forecasters’ track records, maybe we can use that to figure out which forecasters to trust. In contrast to the previous two approaches for justifying risk estimates (inductive and deductive), the forecaster doesn’t have to explain their estimate, but instead justifies it based on their demonstrated skill at predicting </span><em>other</em><span> outcomes in the past.&nbsp;</span></p><p><span>This has proved to be invaluable in the domain of geopolitical events, and the forecasting community spends a lot of effort on skill measurement. Many ways to evaluate forecasting skill exist, such as calibration, the Brier score, the logarithmic score, or the Peer score used on the forecasting competition website </span><a href=\"https://www.metaculus.com/\" rel=\"\">Metaculus</a><span>.&nbsp;</span></p><p>But regardless of which method is used, when it comes to existential risk, there are many barriers to assessing forecast skill for subjective probabilities: the lack of a reference class, the low base rate, and the long time horizon. Let’s look at each of these in turn.</p><p>Just as the reference class problem plagues the forecaster, it also affects the evaluator. Let’s return to the alien landing example. Consider a forecaster who has proved highly accurate at calling elections. Suppose this forecaster announces, without any evidence, that aliens will land on Earth within a year. Despite the forecaster’s demonstrated skill, this would not cause us to update our beliefs about an alien landing, because it is too dissimilar to election forecasting and we do not expect the forecaster’s skill to generalize. Similarly, AI x-risk is so dissimilar to any past events that have been forecast that there is no evidence of any forecaster’s skill at estimating AI x-risk.&nbsp;</p><p><span>Even if we somehow do away with the reference class problem, other problems remain — notably, the fact that extinction risks are “tail risks”, or risks that result from rare events. Suppose forecaster A says the probability of AI x-risk is 1%, and forecaster B says it is 1 in a million. Which forecast should we have more confidence in? We could look at their track records. Say we find that forecaster A (who has assigned a 1% probability to AI x-risk) has a better track record. It still doesn’t mean we should have more confidence in A’s forecast, because </span><em>skill evaluations are insensitive to overestimation of tail risks</em><span>. In other words, it could be that A scores higher overall because A is slightly better calibrated than B when it comes to everyday events that have a substantial probability of occurring, but tends to massively overestimate tail risks that occur rarely (for example, those with a probability of 1 in a million) by orders of magnitude. No scoring rule adequately penalizes this type of miscalibration.&nbsp;</span></p><p><span>Here’s a thought experiment to show why this is true. Suppose two forecasters F and G forecast two different sets of events, and the “true” probabilities of events in both sets are uniformly distributed between 0 and 1. We assume, highly optimistically, that both F and G know the true probability P[</span><em>e</em><span>] for every event </span><em>e</em><span> that they forecast. F always outputs P[</span><em>e</em><span>], but G is slightly conservative, never predicting a value less than 1%. That is, G outputs P[</span><em>e</em><span>] if P[</span><em>e</em><span>] &gt;= 1%, otherwise outputs 1%.</span></p><p><span>By construction, F is the better forecaster. But would this be evident from their track records? In other words, how many forecasts from each would we have to evaluate so that there’s a 95% chance that F outscores G? With the logarithmic scoring rule, it turns out to be on the order of a hundred million. With the Brier score, it is on the order of a trillion.</span><span><a data-component-name=\"FootnoteAnchorToDOM\" id=\"footnote-anchor-2-147019742\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities#footnote-2-147019742\" target=\"_self\" rel=\"\">2</a></span><span>&nbsp; We can quibble with the assumptions here but the point is that if a forecaster systematically overestimates tail risks, it is simply empirically undetectable.</span></p><p><span>The final barrier to assessing forecaster skill at predicting x-risk is that long-term forecasts take too long to evaluate (and extinction forecasts are of course impossible to evaluate). This can potentially be overcome. Researchers have developed a method called </span><a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3954498\" rel=\"\">reciprocal scoring</a><span> — where forecasters are rewarded based on how well they predict each others’ forecasts — and validate it in some real-world settings, such as predicting the effect of Covid-19 policies. In these settings, reciprocal scoring yielded forecasts that are as good as traditional scoring methods. Fair enough. But reciprocal scoring is not a way around the reference class problem or the tail risk problem.</span></p><div><figure><a target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png\" data-component-name=\"Image2ToDOM\" rel=\"\"><div><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png\" width=\"1456\" height=\"337\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:337,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:87385,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4779dde8-30c9-4883-b698-78ffd90e3a53_2150x498.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture></div></a><figcaption><em>Summary of our argument so far, showing why none of the three forecasting methods can yield credible estimates of AI x-risk.</em></figcaption></figure></div><p>To recap, inductive and deductive methods don’t work, subjective forecasts are all over the place, and there’s no way to tell which forecasts are more trustworthy.</p><p><span>So in an attempt to derive more reliable estimates that could potentially inform policy, some researchers have turned to forecast aggregation methods that combine the predictions of multiple forecasters. A notable effort is the </span><a href=\"https://wiki.aiimpacts.org/doku.php?id=ai_timelines:predictions_of_human-level_ai_timelines:ai_timeline_surveys:2022_expert_survey_on_progress_in_ai\" rel=\"\">AI Impacts Survey on Progress in AI</a><span>, but it has been criticized for serious methodological limitations including </span><a href=\"https://aiguide.substack.com/p/do-half-of-ai-researchers-believe\" rel=\"\">non</a><span>-</span><a href=\"https://www.scientificamerican.com/article/ai-survey-exaggerates-apocalyptic-risks/\" rel=\"\">response</a><span> </span><a href=\"https://spectrum.ieee.org/ai-existential-risk-survey\" rel=\"\">bias</a><span>. More importantly, it is unclear why aggregation should improve forecast accuracy: after all, most forecasters might share the same biases (and again, none of them have any basis for a reliable forecast).</span></p><p><span>There are many reasons why forecasters might systematically overestimate AI x-risk.</span><span><a data-component-name=\"FootnoteAnchorToDOM\" id=\"footnote-anchor-3-147019742\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities#footnote-3-147019742\" target=\"_self\" rel=\"\">3</a></span><span> The first is selection bias. Take AI researchers: the belief that AI can change the world is one of the main motivations for becoming an AI researcher. And once someone enters this community, they are in an environment where that message is constantly reinforced. And if one believes that this technology is terrifyingly powerful, it is perfectly rational to think there is a serious chance that its world-altering effects will be negative rather than positive.</span></p><p><span>And in the AI safety subcommunity, which is a bit insular, the </span><a href=\"https://www.washingtonpost.com/technology/2023/07/05/ai-apocalypse-college-students/\" rel=\"\">echo chamber</a><span> can be deafening. Claiming to have a high </span><em>p(doom)</em><span> (one’s estimate of the probability of AI doom) seems to have become a way to signal one’s identity and commitment to the cause.</span></p><p><span>There is a slightly different selection bias at play when it comes to forecasting experts. The forecasting community has a strong overlap with effective altruism and concerns about existential risk, especially AI risk. This doesn’t mean that individual forecasters are biased. But having a high </span><em>p(doom)</em><span> might make someone more inclined to take up forecasting as an activity. So the community as a whole is likely biased toward people with x-risk worries.&nbsp;</span></p><p><span>Forecasters are good at updating their beliefs in response to evidence, but the problem is that unlike, say, asteroid impact risk, there is little evidence that can change one’s beliefs one way or another when it comes to AI x-risk, so we suspect that forecasts are strongly influenced by the priors with which people enter the community. The </span><a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf\" rel=\"\">XPT report</a><span> notes that “Few minds were changed during the XPT, even among the most active participants, and despite monetary incentives for persuading others.” In a </span><a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/65ef1ee52e64b52f145ebb49/1710169832137/AIcollaboration.pdf\" rel=\"\">follow-up study</a><span>, they found that many of the disagreements were due to fundamental worldview differences that go beyond AI.</span></p><p>To reemphasize, our points about bias are specific to AI x-risk. If there were a community of election forecasters who were systematically biased (say, toward incumbents), this would become obvious after a few elections when comparing predictions with reality. But with AI x-risk, as we showed in the previous section, skill evaluation is insensitive to overestimation of tail risks.</p><p><span>Interestingly, skill evaluation is extremely sensitive to </span><em>underestimation</em><span> of tail risks: if you assign a probability of 0 for a rare event that actually ends up occurring, you incur an </span><em>infinite </em><span>penalty under the logarithmic scoring rule, from which you can never recover regardless of how well you predicted other events. This is considered one of the main benefits of the logarithmic score and is the reason it is adopted by Metaculus.</span></p><p><span>Now consider a forecaster who doesn’t have a precise estimate — and surely no forecaster has a precise estimate for something with so many axes of uncertainty as AI x-risk. Given the asymmetric penalties, the rational thing to do is to go with the higher end of their range of estimates.</span><span><a data-component-name=\"FootnoteAnchorToDOM\" id=\"footnote-anchor-4-147019742\" href=\"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities#footnote-4-147019742\" target=\"_self\" rel=\"\">4</a></span></p><p>In any case, it’s not clear what forecasters actually report when their estimates are highly uncertain. Maybe they don’t respond to the incentives of the scoring function. After all, long-term forecasts won’t be resolved anytime soon. And recall that in the case of the XPT, the incentive is actually to predict each others’ forecasts to get around the problem of long time horizons. The reciprocal scoring paper argues that this will incentivize forecasters to submit their true, high-effort estimates, and considers various objections to this claim. Their defense of the method rests on two key assumptions: that by exerting more effort forecasters can get closer to the true estimate, and that they have no better way to predict what other forecasters will do.</p><p><span>What if these assumptions are not satisfied? As we have argued throughout this post, with AI x-risk, we shouldn’t expect evidence to change forecasters’ prior beliefs, so the first assumption is dubious. And now that one iteration of the XPT has concluded, the published median estimates from that tournament serve as a powerful anchor (a “</span><a href=\"https://en.wikipedia.org/wiki/Focal_point_(game_theory)\" rel=\"\">focal point</a><span>” in game theory). It is possible that in the future, forecasters with reciprocal scoring incentives will use existing median forecasts as a starting point, only making minor adjustments to account for new information that has become available since the last tournament. The range of estimates might narrow as existing estimates serve as anchors for future estimates. All that is a roundabout way to say: the less actual evidence there is to draw upon, the more the risk of groupthink.</span></p><p><span>For what it’s worth, here are the median estimates from the </span><a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf\" rel=\"\">XPT</a><span> of both extinction risk and sub-extinction catastrophic risks from AI:</span></p><div><figure><a target=\"_blank\" href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png\" data-component-name=\"Image2ToDOM\" rel=\"\"><div><picture><source type=\"image/webp\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 1456w\" sizes=\"100vw\"><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png\" width=\"1456\" height=\"473\" data-attrs=\"{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:473,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:442848,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}\" alt=\"\" srcset=\"https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8655d15c-6f40-43a0-bf3e-1ca1aadb9cd0_1892x614.png 1456w\" sizes=\"100vw\" loading=\"lazy\"></picture></div></a></figure></div><p>To reiterate, our view is that we shouldn’t take any of these numbers too seriously. They are a reflection of how much different samples of participants fret about AI than anything else.</p><p>As before, the estimates from forecasting experts (superforecasters) and AI experts differ by an order of magnitude or more. To the extent that we put any stock into these estimates, it should be the forecasting experts’ rather than the AI experts’ estimates. One important insight from past research is that domain experts perform worse than forecasting experts who have training in integrating diverse information and by minimizing psychological biases. Still, as we said above, even their forecasts may be vast overestimates, and we just can’t know for sure.</p><p>So what’s the big deal? So what if policymakers believe the risk over a certain timeframe is 1% instead of 0.01%? It seems pretty low in either case!</p><p>It depends on what they do with those probabilities. Most often, these estimates are merely a way to signal the fact that some group of experts thinks the risk is significant. If that’s all they are, so be it. But it’s not clear that all this elaborate effort at quantification is even helpful for this signaling purpose, given that different people interpret the same numbers wildly differently.&nbsp;</p><p><span>For example, Federal Trade Commission chair Lina Khan said her views on the matter were techno-optimistic since her </span><em>p(doom)</em><span> was </span><a href=\"https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html\" rel=\"\">only 15%</a><span>, which left experts bewildered. (For what it’s worth, that number is about a thousandfold higher than what we would be comfortable labeling techno-optimist.) It takes a lot of quantitative training to be able to mentally process very small or very large numbers correctly in decision making, and not simply bucket them into categories like “insignificantly small”. Most people are not trained this way.&nbsp;</span></p><p><span>In short, what seems to be happening is that experts’ vague intuitions and fears are being translated into pseudo-precise numbers, and then translated back into vague intuitions and fears by policymakers. Let’s just cut the charade of quantification! The Center for AI Safety’s </span><a href=\"https://www.safe.ai/work/statement-on-ai-risk\" rel=\"\">Statement on AI Risk</a><span> was admirably blunt in this regard (of course, we </span><a href=\"https://www.aisnakeoil.com/p/is-avoiding-extinction-from-ai-really\" rel=\"\">strongly disagree with its substance</a><span>).</span></p><p>A principled, quantitative way to use probabilities in decision making is utility maximization through cost-benefit analysis. The idea is simple: if we consider an outcome to have a subjective value, or utility, of U (which can be positive or negative), and it has, say, a 10% probability of occurring, we can act as if it is certain to occur and has a value of 0.1 * U. We can then add up the costs and benefits for each option available to us, and choose the one that maximizes costs minus benefits (the “expected utility”).</p><p><span>This is where things get really problematic. First, some people might consider extinction to have an unfathomably large negative value, because it precludes the existence of all the human lives, physical or </span><a href=\"https://www.vox.com/future-perfect/23298870/effective-altruism-longtermism-will-macaskill-future\" rel=\"\">simulated</a><span>, that might ever be born in the future. The logical conclusion is that x-risk should be everyone’s top priority all the time! It is reminiscent of </span><a href=\"https://en.wikipedia.org/wiki/Pascal%27s_wager\" rel=\"\">Pascal’s wager</a><span>, the argument that it is rational believe in God because even if there is an infinitesimally small chance that God exists, the cost of non-belief is infinite (an eternity in hell as opposed to eternal happiness), and hence so is the expected utility. Fortunately, policymakers don’t give too much credence to decision making frameworks involving infinities. But the idea has taken a powerful hold of the AI safety community and drives some people’s conviction that AI x-risk should be society’s top priority.</span></p><p>Even if we limit ourselves to catastrophic but not existential risks, we are talking about billions of lives on the line, so the expected cost of even a 1% risk is so high that the policy implications are drastic — governments should increase spending on AI x-risk mitigation by orders of magnitude and consider draconian measures such as stopping AI development. This is why it is so vital to understand that these estimates are not backed by any methodology. It would be incredibly unwise to make world-changing policy decisions based on so little evidence.</p><p>Is there a role for forecasting in AI policy? We think yes — just not forecasting existential risk. Forecasting AI milestones, such as performance on certain capability benchmarks or economic impacts, is more achievable and meaningful. If a forecaster has demonstrated skill in predicting when various AI milestones would be reached, it does give us evidence that they will do well in the future. We are no longer talking about unique or rare events. And when considering lower-stakes policy interventions — preparing for potential economic disruption rather than staving off killer robots — it is less critical that forecasts be justified to the satisfaction of every reasonable person.&nbsp;</p><p><span>The forecasting community devotes a lot of energy to milestone forecasting. On Metaculus, the question “Will there be Human-machine intelligence parity before 2040?” has an aggregate prediction of </span><a href=\"https://www.metaculus.com/questions/384/humanmachine-intelligence-parity-by-2040/\" rel=\"\">96%</a><span> based on over 1,300 forecasters. That’s remarkable! If we agreed with this forecast, we would be in favor of the position that managing the safe transition to AGI should be a global priority. Why don’t we?</span></p><p>The answer is in the fine print. There is no consensus on the definition of a fuzzy concept such as AGI. Even if we fix a definition, determining whether it has been achieved can be hard or impossible. For effective forecasting, it is extremely important to avoid ambiguous outcomes. The way the forecasting community gets around this is by defining it in terms of relatively narrow skills, such as exam performance.&nbsp;</p><p><span>The Metaculus intelligence parity question is defined in terms of the performance on graduate exams in math, physics, and computer science. Based on this definition, we do agree with the forecast of 96%. But we think the definition is so watered down that it doesn’t mean much for policy. Forget existential risk — as we’ve written before, AI performance on exams has so little </span><a href=\"https://www.aisnakeoil.com/p/gpt-4-and-professional-benchmarks\" rel=\"\">construct validity</a><span> that it doesn’t even let us predict whether AI will replace workers.</span></p><p><span>Other benchmarks </span><a href=\"https://www.aisnakeoil.com/p/new-paper-ai-agents-that-matter\" rel=\"\">aren’t much better</a><span>. In short, forecasting AI capability timelines is tricky because of the huge gap between benchmarks and real-world implications. Fortunately, </span><a href=\"https://www.openphilanthropy.org/rfp-llm-benchmarks/\" rel=\"\">better benchmarks</a><span> reflecting consequential real-world tasks are being developed. In addition to benchmarks, we need naturalistic evaluation, even if it is more costly. One type of naturalistic evaluation is to measure how people perform their jobs differently with AI assistance. Directly forecasting economic, social, or political impacts — such as labor market transformation or AI-related spending by militaries — could be even more useful, although harder to unambiguously define and measure.</span></p><p><span>The responsibility for avoiding misuses of probability in policy lies with policymakers. We are not calling for forecasters to stop publishing forecasts in order to “protect” policymakers from being misled. That said, we think forecasts should be accompanied by a clear explanation of the process used and evidence considered. This would allow policymakers to make informed decisions about whether the justification presented meets the threshold that they are comfortable with. The XPT is a good example of transparency, as is </span><a href=\"https://arxiv.org/abs/2306.02519\" rel=\"\">this paper</a><span> (though it is not about x-risk). On the other hand, simply surveying a bunch of researchers and presenting aggregate numbers is misinformative and should be ignored by policymakers.</span></p><p><span>So what should governments do about AI x-risk? Our view isn’t that they should do nothing. But they should reject the kind of policies that might seem compelling if we view x-risk as urgent and serious, notably: restricting AI development. As we’ll argue in a future essay in this series, not only are such policies unnecessary, they are likely to </span><em>increase</em><span> x-risk. Instead, governments should adopt policies that are compatible with a range of possible estimates of AI risk, and are on balance helpful even if the risk is negligible. Fortunately, such policies exist. Governments should also change policymaking </span><em>processes</em><span> so that they are more responsive to new evidence. More on all that soon.</span></p><ul><li><p><span>The XPT report is titled </span><a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf\" rel=\"\">Forecasting Existential Risks: Evidence from a Long-Run Forecasting Tournament</a><span>.</span></p></li><li><p><span>Tetlock and Gardner’s book </span><a href=\"https://www.amazon.com/Superforecasting-Science-Prediction-Philip-Tetlock/dp/0804136718\" rel=\"\">Superforecasting</a><span> summarizes research by Tetlock, Barbara Mellers, and others.</span></p></li><li><p><span>Scott Alexander refutes the claim that there is something wrong in principle with ascribing </span><a href=\"https://www.astralcodexten.com/p/in-continued-defense-of-non-frequentist\" rel=\"\">probabilities to unique events</a><span> (we largely agree). Our argument differs in two key ways from the position he addresses. We aren’t talking about forecasting in general; just its application to policymaking. And we don’t object to it in principle. Our argument is empirical: AI x-risk forecasts are extremely unreliable and lack justification. Theoretically this could change in the future, though we aren’t holding our breath.</span></p></li><li><p><span>A </span><a href=\"https://www.jstor.org/stable/45094450?seq=1\" rel=\"\">paper</a><span> by Friedman and Zuckhauser explains why probabilities aren’t the whole story: two forecasts that have the same risk estimate might have very different implications for policymakers. In our view, AI x-risk forecasts fare poorly on two of the three dimensions of confidence: a sound basis in evidence and a narrow range of reasonable opinion.</span></p></li><li><p><span>A paper by our Princeton CITP colleagues led by Shazeda Ahmed explains the </span><a href=\"https://firstmonday.org/ojs/index.php/fm/article/view/13626/11596\" rel=\"\">epistemic culture of AI safety</a><span>, which consists of “cohesive, interwoven social structures of knowledge-production and community-building”. It helps understand why practices such as forecasting have become pillars of how the AI safety community forms its beliefs, as opposed to the broader scientific community that centers practices such as peer review. Of course, we shouldn’t reject a view just because it doesn’t conform to scientific orthodoxy. But at the same time, we shouldn’t give any deference to the self-styled AI safety community’s views on AI safety. It is important to understand that the median member of the AI safety community holds one particular stance on AI safety — a stance that is highly contested and in our view rather alarmist.</span></p></li><li><p><span>We have written extensively about </span><a href=\"https://www.aisnakeoil.com/t/ai-safety\" rel=\"\">evidence-based AI safety</a><span>. Our best-known work includes the essay </span><a href=\"https://www.aisnakeoil.com/p/ai-safety-is-not-a-model-property\" rel=\"\">AI safety is not a model property</a><span> and the paper titled </span><a href=\"https://crfm.stanford.edu/open-fms/\" rel=\"\">On the Societal Impact of Open Foundation Models</a><span> which was the result of a large collaboration.</span></p></li></ul><p><strong>Acknowledgements.</strong><span> We are grateful to Benjamin Edelman, Ezra Karger, Matt Salganik, and Ollie Stephenson for feedback on a draft. This series of essays is based on an upcoming paper that benefited from feedback from many people, including Seth Lazar and members of the MINT lab at Australian National University, students in the Limits to Prediction course at Princeton, Shazeda Ahmed, and Zachary Siegel.</span></p></div></div>","textContent":"How seriously should governments take the threat of existential risk from AI, given the lack of consensus among researchers? On the one hand, existential risks (x-risks) are necessarily somewhat speculative: by the time there is concrete evidence, it may be too late. On the other hand, governments must prioritize — after all, they don’t worry too much about x-risk from alien invasions.This is the first in a series of essays laying out an evidence-based approach for policymakers concerned about AI x-risk, an approach that stays grounded in reality while acknowledging that there are “unknown unknowns”. In this first essay, we look at one type of evidence: probability estimates. The AI safety community relies heavily on forecasting the probability of human extinction due to AI (in a given timeframe) in order to inform decision making and policy. An estimate of 10% over a few decades, for example, would obviously be high enough for the issue to be a top priority for society. Our central claim is that AI x-risk forecasts are far too unreliable to be useful for policy, and in fact highly misleading.If the two of us predicted an 80% probability of aliens landing on earth in the next ten years, would you take this possibility seriously? Of course not. You would ask to see our evidence. As obvious as this may seem, it seems to have been forgotten in the AI x-risk debate that probabilities carry no authority by themselves. Probabilities are usually derived from some grounded method, so we have a strong cognitive bias to view quantified risk estimates as more valid than qualitative ones. But it is possible for probabilities to be nothing more than guesses. Keep this in mind throughout this essay (and more broadly in the AI x-risk debate).If we predicted odds for the Kentucky Derby, we don’t have to give you a reason — you can take it or leave it. But if a policymaker takes actions based on probabilities put forth by a forecaster, they had better be able to explain those probabilities to the public (and that explanation must in turn come from the forecaster). Justification is essential to legitimacy of government and the exercise of power. A core principle of liberal democracy is that the state should not limit people's freedom based on controversial beliefs that reasonable people can reject. Explanation is especially important when the policies being considered are costly, and even more so when those costs are unevenly distributed among stakeholders. A good example is restricting open releases of AI models. Can governments convince people and companies who stand to benefit from open models that they should make this sacrifice because of a speculative future risk?The main aim of this essay is analyzing whether there is any justification for any of the specific x-risk probability estimates that have been cited in the policy debate. We have no objection to AI x-risk forecasting as an academic activity, and forecasts may be helpful to companies and other private decision makers. We only question its use in the context of public policy.There are basically only three known ways by which a forecaster can try to convince a skeptic: inductive, deductive, and subjective probability estimation. We consider each of these in the following sections. All three require both parties to agree on some basic assumptions about the world (which cannot themselves be proven). The three approaches differ in terms of the empirical and logical ways in which the probability estimate follows from that set of assumptions.Most risk estimates are inductive: they are based on past observations. For example, insurers base their predictions of an individual’s car accident risk on data from past accidents about similar drivers. The set of observations used for probability estimation is called a reference class. A suitable reference class for car insurance might be the set of drivers who live in the same city. If the analyst has more information about the individual, such as their age or the type of car they drive, the reference class can be further refined. For existential risk from AI, there is no reference class, as it is an event like no other. To be clear, this is a matter of degree, not kind. There is never a clear “correct” reference class to use, and the choice of a reference class in practice comes down to the analyst’s intuition. The accuracy of the forecasts depends on the degree of similarity between the process that generates the event being forecast and the process that generated the events in the reference class, which can be seen as a spectrum. For predicting the outcome of a physical system such as a coin toss, past experience is a highly reliable guide. Next, for car accidents, risk estimates might vary by, say, 20% based on the past dataset used — good enough for insurance companies. Further along the spectrum are geopolitical events, where the choice of reference class gets even fuzzier. Forecasting expert Philip Tetlock explains: “Grexit may have looked sui generis, because no country had exited the Eurozone as of 2015, but it could also be viewed as just another instance of a broad comparison class, such as negotiation failures, or of a narrower class, such as a nation-states withdrawing from international agreements or, narrower still, of forced currency conversions.” He goes on to defend the idea that even seeming Black Swan events like the collapse of the USSR or the Arab Spring can be modeled as members of reference classes, and that inductive reasoning is useful even for this kind of event.In Tetlock’s spectrum, these events represent the “peak” of uniqueness. When it comes to geopolitical events, that might be true. But even those events are far less unique than extinction from AI. Just look at the attempts to find reference classes for AI x-risk: animal extinction (as a reference class for human extinction), past global transformations such as the industrial revolution (as a reference class for socioeconomic transformation from AI), or accidents causing mass deaths (as a reference class for accidents causing global catastrophe). Let’s get real. None of those tell us anything about the possibility of developing superintelligent AI or losing control over such AI, which are the central sources of uncertainty for AI x-risk forecasting. To summarize, human extinction due to AI is an outcome so far removed from anything that has happened in the past that we cannot use inductive methods to “predict” the odds. Of course, we can get qualitative insights from past technical breakthroughs as well as past catastrophic events, but AI risk is sufficiently different that quantitative estimates lack the kind of justification needed for legitimacy in policymaking.In Conan Doyle’s The Adventure of the Six Napoleons — spoiler alert! — Sherlock Holmes announces before embarking on a stakeout that the probability of catching the suspect is exactly two-thirds. This seems bewildering — how can anything related to human behavior be ascribed a mathematically precise probability?It turns out that Holmes has deduced the underlying series of events that gave rise to the suspect’s seemingly erratic observed behavior: the suspect is methodically searching for a jewel that is known to be hidden inside one of six busts of Napoleon owned by different people in and around London. The details aren’t too important, but the key is that neither the suspect nor the detectives know which of the six busts it is in, and everything else about the suspect’s behavior is (assumed to be) entirely predictable. Hence the precisely quantifiable uncertainty.The point is that if we have a model of the world that we can rely upon, we can estimate risk through logical deduction, even without relying on past observations. Of course, outside of fictional scenarios, the world isn’t so neat, especially when we want to project far into the future.When it comes to x-risk, there is an interesting exception to the general rule that we don’t have deductive models — asteroid impact. A combination of inductive and deductive risk estimation does allow us to estimate the probability of x-risk, only because we’re talking about a purely physical system. Let’s take a minute to review how this works, because it’s important to recognize that the methods are not generalizable to other types of x-risk. The key is being able to model the relationship between the size of the asteroid (more precisely, the energy of impact) and the frequency of impact. Since we have observed thousands of small impacts, we can extrapolate to infer the frequency of large impacts that have never been directly observed. We can also estimate the threshold that would cause global catastrophe.1Figure: data on small asteroid impacts (illustrated on the left) can be extrapolated to extinction-level impacts (right).With AI, the unknowns relate to technological progress and governance rather than a physical system, so it isn’t clear how to model it mathematically. Still, people have tried. For example, in order to predict the computational requirements of a hypothetical AGI, several works assume that an AI system would require roughly as many computations as the human brain, and further make assumptions about the number of computations required by the human brain. These assumptions are far more tenuous than those involved in asteroid modeling, and none of this even addresses the loss-of-control question.Without the reference classes or grounded theories, forecasts are necessarily “subjective probabilities”, that is, guesses based on the forecaster’s judgment. Unsurprisingly, these vary by orders of magnitude.Subjective probability estimation does not get around the need for having either an inductive or a deductive basis for probability estimates. It merely avoids the need for the forecaster to explain their estimate. Explanation can be hard due to humans’ limited ability to explain our intuitive reasoning, whether inductive, deductive, or a combination thereof. Essentially, it allows the forecaster to say: “even though I haven’t shown my methods, you can trust this estimate because of my track record” (we explain in the next section why even this breaks down for AI x-risk forecasting). But ultimately, lacking either an inductive or a deductive basis, all that forecasters can do is to make up a number, and those made-up numbers are all over the place.Consider the Existential Risk Persuasion Tournament (XPT) conducted by the Forecasting Research Institute in late 2022, which we think is the most elaborate and well-executed x-risk forecasting exercise conducted to date. It involved various groups of forecasters, including AI experts and forecasting experts (“superforecasters” in the figure). For AI experts, the high end (75th percentile) of estimates for AI extinction risk by 2100 is 12%, the median estimate is 3%, and the low end (25th percentile) is 0.25%. For forecasting experts, even the high end (75th percentile) is only 1%, the median is a mere 0.38%, and the low end (25th percentile) is visually indistinguishable from zero on the graph. In other words, the 75th percentile AI expert forecast and the 25th percentile superforecaster forecast differ by at least a factor of 100.All of these estimates are from people who have deep expertise on the topic and participated in a months-long tournament where they tried to persuade each other! If this range of forecasts here isn’t extreme enough, keep in mind that this whole exercise was conducted by one group at one point in time. We might get different numbers if the tournament were repeated today, if the questions were framed differently, etc.What’s most telling is to look at the rationales that forecasters provided, which are extensively detailed in the report. They aren’t using quantitative models, especially when thinking about the likelihood of bad outcomes conditional on developing powerful AI. For the most part, forecasters are engaging in the same kind of speculation that everyday people do when they discuss superintelligent AI. Maybe AI will take over critical systems through superhuman persuasion of system operators. Maybe AI will seek to lower global temperatures because it helps computers run faster, and accidentally wipe out humanity. Or maybe AI will seek resources in space rather than Earth, so we don’t need to be as worried. There’s nothing wrong with such speculation. But we should be clear that when it comes to AI x-risk, forecasters aren’t drawing on any special knowledge, evidence, or models that make their hunches more credible than yours or ours or anyone else’s.  The term superforecasting comes from Philip Tetlock’s 20 year study of forecasting (he was also one of the organizers of the XPT). Superforecasters tend to be trained in methods to improve forecasts such as by integrating diverse information and by minimizing psychological biases. These methods have been shown to be effective in domains such as geopolitics. But no amount of training will lead to good forecasts if there isn’t much useful evidence to draw from.Even if forecasters had credible quantitative models (they don’t), they must account for “unknown unknowns”, that is, the possibility that the model itself might be wrong. As noted x-risk philosopher Nick Bostrom explains: “The uncertainty and error-proneness of our first-order assessments of risk is itself something we must factor into our all-things-considered probability assignments. This factor often dominates in low-probability, high-consequence risks — especially those involving poorly understood natural phenomena, complex social dynamics, or new technology, or that are difficult to assess for other reasons.” This is a reasonable perspective, and AI x-risk forecasters do worry a lot about uncertainty in risk assessment. But one consequence of this is that for those who follow this principle, forecasts are guaranteed to be guesses rather than the output of a model — after all, no model can be used to estimate the probability that the model itself is wrong, or what the risk would be if the model were wrong.To recap, subjective AI-risk forecasts vary by orders of magnitude. But if we can measure forecasters’ track records, maybe we can use that to figure out which forecasters to trust. In contrast to the previous two approaches for justifying risk estimates (inductive and deductive), the forecaster doesn’t have to explain their estimate, but instead justifies it based on their demonstrated skill at predicting other outcomes in the past. This has proved to be invaluable in the domain of geopolitical events, and the forecasting community spends a lot of effort on skill measurement. Many ways to evaluate forecasting skill exist, such as calibration, the Brier score, the logarithmic score, or the Peer score used on the forecasting competition website Metaculus. But regardless of which method is used, when it comes to existential risk, there are many barriers to assessing forecast skill for subjective probabilities: the lack of a reference class, the low base rate, and the long time horizon. Let’s look at each of these in turn.Just as the reference class problem plagues the forecaster, it also affects the evaluator. Let’s return to the alien landing example. Consider a forecaster who has proved highly accurate at calling elections. Suppose this forecaster announces, without any evidence, that aliens will land on Earth within a year. Despite the forecaster’s demonstrated skill, this would not cause us to update our beliefs about an alien landing, because it is too dissimilar to election forecasting and we do not expect the forecaster’s skill to generalize. Similarly, AI x-risk is so dissimilar to any past events that have been forecast that there is no evidence of any forecaster’s skill at estimating AI x-risk. Even if we somehow do away with the reference class problem, other problems remain — notably, the fact that extinction risks are “tail risks”, or risks that result from rare events. Suppose forecaster A says the probability of AI x-risk is 1%, and forecaster B says it is 1 in a million. Which forecast should we have more confidence in? We could look at their track records. Say we find that forecaster A (who has assigned a 1% probability to AI x-risk) has a better track record. It still doesn’t mean we should have more confidence in A’s forecast, because skill evaluations are insensitive to overestimation of tail risks. In other words, it could be that A scores higher overall because A is slightly better calibrated than B when it comes to everyday events that have a substantial probability of occurring, but tends to massively overestimate tail risks that occur rarely (for example, those with a probability of 1 in a million) by orders of magnitude. No scoring rule adequately penalizes this type of miscalibration. Here’s a thought experiment to show why this is true. Suppose two forecasters F and G forecast two different sets of events, and the “true” probabilities of events in both sets are uniformly distributed between 0 and 1. We assume, highly optimistically, that both F and G know the true probability P[e] for every event e that they forecast. F always outputs P[e], but G is slightly conservative, never predicting a value less than 1%. That is, G outputs P[e] if P[e] >= 1%, otherwise outputs 1%.By construction, F is the better forecaster. But would this be evident from their track records? In other words, how many forecasts from each would we have to evaluate so that there’s a 95% chance that F outscores G? With the logarithmic scoring rule, it turns out to be on the order of a hundred million. With the Brier score, it is on the order of a trillion.2  We can quibble with the assumptions here but the point is that if a forecaster systematically overestimates tail risks, it is simply empirically undetectable.The final barrier to assessing forecaster skill at predicting x-risk is that long-term forecasts take too long to evaluate (and extinction forecasts are of course impossible to evaluate). This can potentially be overcome. Researchers have developed a method called reciprocal scoring — where forecasters are rewarded based on how well they predict each others’ forecasts — and validate it in some real-world settings, such as predicting the effect of Covid-19 policies. In these settings, reciprocal scoring yielded forecasts that are as good as traditional scoring methods. Fair enough. But reciprocal scoring is not a way around the reference class problem or the tail risk problem.Summary of our argument so far, showing why none of the three forecasting methods can yield credible estimates of AI x-risk.To recap, inductive and deductive methods don’t work, subjective forecasts are all over the place, and there’s no way to tell which forecasts are more trustworthy.So in an attempt to derive more reliable estimates that could potentially inform policy, some researchers have turned to forecast aggregation methods that combine the predictions of multiple forecasters. A notable effort is the AI Impacts Survey on Progress in AI, but it has been criticized for serious methodological limitations including non-response bias. More importantly, it is unclear why aggregation should improve forecast accuracy: after all, most forecasters might share the same biases (and again, none of them have any basis for a reliable forecast).There are many reasons why forecasters might systematically overestimate AI x-risk.3 The first is selection bias. Take AI researchers: the belief that AI can change the world is one of the main motivations for becoming an AI researcher. And once someone enters this community, they are in an environment where that message is constantly reinforced. And if one believes that this technology is terrifyingly powerful, it is perfectly rational to think there is a serious chance that its world-altering effects will be negative rather than positive.And in the AI safety subcommunity, which is a bit insular, the echo chamber can be deafening. Claiming to have a high p(doom) (one’s estimate of the probability of AI doom) seems to have become a way to signal one’s identity and commitment to the cause.There is a slightly different selection bias at play when it comes to forecasting experts. The forecasting community has a strong overlap with effective altruism and concerns about existential risk, especially AI risk. This doesn’t mean that individual forecasters are biased. But having a high p(doom) might make someone more inclined to take up forecasting as an activity. So the community as a whole is likely biased toward people with x-risk worries. Forecasters are good at updating their beliefs in response to evidence, but the problem is that unlike, say, asteroid impact risk, there is little evidence that can change one’s beliefs one way or another when it comes to AI x-risk, so we suspect that forecasts are strongly influenced by the priors with which people enter the community. The XPT report notes that “Few minds were changed during the XPT, even among the most active participants, and despite monetary incentives for persuading others.” In a follow-up study, they found that many of the disagreements were due to fundamental worldview differences that go beyond AI.To reemphasize, our points about bias are specific to AI x-risk. If there were a community of election forecasters who were systematically biased (say, toward incumbents), this would become obvious after a few elections when comparing predictions with reality. But with AI x-risk, as we showed in the previous section, skill evaluation is insensitive to overestimation of tail risks.Interestingly, skill evaluation is extremely sensitive to underestimation of tail risks: if you assign a probability of 0 for a rare event that actually ends up occurring, you incur an infinite penalty under the logarithmic scoring rule, from which you can never recover regardless of how well you predicted other events. This is considered one of the main benefits of the logarithmic score and is the reason it is adopted by Metaculus.Now consider a forecaster who doesn’t have a precise estimate — and surely no forecaster has a precise estimate for something with so many axes of uncertainty as AI x-risk. Given the asymmetric penalties, the rational thing to do is to go with the higher end of their range of estimates.4In any case, it’s not clear what forecasters actually report when their estimates are highly uncertain. Maybe they don’t respond to the incentives of the scoring function. After all, long-term forecasts won’t be resolved anytime soon. And recall that in the case of the XPT, the incentive is actually to predict each others’ forecasts to get around the problem of long time horizons. The reciprocal scoring paper argues that this will incentivize forecasters to submit their true, high-effort estimates, and considers various objections to this claim. Their defense of the method rests on two key assumptions: that by exerting more effort forecasters can get closer to the true estimate, and that they have no better way to predict what other forecasters will do.What if these assumptions are not satisfied? As we have argued throughout this post, with AI x-risk, we shouldn’t expect evidence to change forecasters’ prior beliefs, so the first assumption is dubious. And now that one iteration of the XPT has concluded, the published median estimates from that tournament serve as a powerful anchor (a “focal point” in game theory). It is possible that in the future, forecasters with reciprocal scoring incentives will use existing median forecasts as a starting point, only making minor adjustments to account for new information that has become available since the last tournament. The range of estimates might narrow as existing estimates serve as anchors for future estimates. All that is a roundabout way to say: the less actual evidence there is to draw upon, the more the risk of groupthink.For what it’s worth, here are the median estimates from the XPT of both extinction risk and sub-extinction catastrophic risks from AI:To reiterate, our view is that we shouldn’t take any of these numbers too seriously. They are a reflection of how much different samples of participants fret about AI than anything else.As before, the estimates from forecasting experts (superforecasters) and AI experts differ by an order of magnitude or more. To the extent that we put any stock into these estimates, it should be the forecasting experts’ rather than the AI experts’ estimates. One important insight from past research is that domain experts perform worse than forecasting experts who have training in integrating diverse information and by minimizing psychological biases. Still, as we said above, even their forecasts may be vast overestimates, and we just can’t know for sure.So what’s the big deal? So what if policymakers believe the risk over a certain timeframe is 1% instead of 0.01%? It seems pretty low in either case!It depends on what they do with those probabilities. Most often, these estimates are merely a way to signal the fact that some group of experts thinks the risk is significant. If that’s all they are, so be it. But it’s not clear that all this elaborate effort at quantification is even helpful for this signaling purpose, given that different people interpret the same numbers wildly differently. For example, Federal Trade Commission chair Lina Khan said her views on the matter were techno-optimistic since her p(doom) was only 15%, which left experts bewildered. (For what it’s worth, that number is about a thousandfold higher than what we would be comfortable labeling techno-optimist.) It takes a lot of quantitative training to be able to mentally process very small or very large numbers correctly in decision making, and not simply bucket them into categories like “insignificantly small”. Most people are not trained this way. In short, what seems to be happening is that experts’ vague intuitions and fears are being translated into pseudo-precise numbers, and then translated back into vague intuitions and fears by policymakers. Let’s just cut the charade of quantification! The Center for AI Safety’s Statement on AI Risk was admirably blunt in this regard (of course, we strongly disagree with its substance).A principled, quantitative way to use probabilities in decision making is utility maximization through cost-benefit analysis. The idea is simple: if we consider an outcome to have a subjective value, or utility, of U (which can be positive or negative), and it has, say, a 10% probability of occurring, we can act as if it is certain to occur and has a value of 0.1 * U. We can then add up the costs and benefits for each option available to us, and choose the one that maximizes costs minus benefits (the “expected utility”).This is where things get really problematic. First, some people might consider extinction to have an unfathomably large negative value, because it precludes the existence of all the human lives, physical or simulated, that might ever be born in the future. The logical conclusion is that x-risk should be everyone’s top priority all the time! It is reminiscent of Pascal’s wager, the argument that it is rational believe in God because even if there is an infinitesimally small chance that God exists, the cost of non-belief is infinite (an eternity in hell as opposed to eternal happiness), and hence so is the expected utility. Fortunately, policymakers don’t give too much credence to decision making frameworks involving infinities. But the idea has taken a powerful hold of the AI safety community and drives some people’s conviction that AI x-risk should be society’s top priority.Even if we limit ourselves to catastrophic but not existential risks, we are talking about billions of lives on the line, so the expected cost of even a 1% risk is so high that the policy implications are drastic — governments should increase spending on AI x-risk mitigation by orders of magnitude and consider draconian measures such as stopping AI development. This is why it is so vital to understand that these estimates are not backed by any methodology. It would be incredibly unwise to make world-changing policy decisions based on so little evidence.Is there a role for forecasting in AI policy? We think yes — just not forecasting existential risk. Forecasting AI milestones, such as performance on certain capability benchmarks or economic impacts, is more achievable and meaningful. If a forecaster has demonstrated skill in predicting when various AI milestones would be reached, it does give us evidence that they will do well in the future. We are no longer talking about unique or rare events. And when considering lower-stakes policy interventions — preparing for potential economic disruption rather than staving off killer robots — it is less critical that forecasts be justified to the satisfaction of every reasonable person. The forecasting community devotes a lot of energy to milestone forecasting. On Metaculus, the question “Will there be Human-machine intelligence parity before 2040?” has an aggregate prediction of 96% based on over 1,300 forecasters. That’s remarkable! If we agreed with this forecast, we would be in favor of the position that managing the safe transition to AGI should be a global priority. Why don’t we?The answer is in the fine print. There is no consensus on the definition of a fuzzy concept such as AGI. Even if we fix a definition, determining whether it has been achieved can be hard or impossible. For effective forecasting, it is extremely important to avoid ambiguous outcomes. The way the forecasting community gets around this is by defining it in terms of relatively narrow skills, such as exam performance. The Metaculus intelligence parity question is defined in terms of the performance on graduate exams in math, physics, and computer science. Based on this definition, we do agree with the forecast of 96%. But we think the definition is so watered down that it doesn’t mean much for policy. Forget existential risk — as we’ve written before, AI performance on exams has so little construct validity that it doesn’t even let us predict whether AI will replace workers.Other benchmarks aren’t much better. In short, forecasting AI capability timelines is tricky because of the huge gap between benchmarks and real-world implications. Fortunately, better benchmarks reflecting consequential real-world tasks are being developed. In addition to benchmarks, we need naturalistic evaluation, even if it is more costly. One type of naturalistic evaluation is to measure how people perform their jobs differently with AI assistance. Directly forecasting economic, social, or political impacts — such as labor market transformation or AI-related spending by militaries — could be even more useful, although harder to unambiguously define and measure.The responsibility for avoiding misuses of probability in policy lies with policymakers. We are not calling for forecasters to stop publishing forecasts in order to “protect” policymakers from being misled. That said, we think forecasts should be accompanied by a clear explanation of the process used and evidence considered. This would allow policymakers to make informed decisions about whether the justification presented meets the threshold that they are comfortable with. The XPT is a good example of transparency, as is this paper (though it is not about x-risk). On the other hand, simply surveying a bunch of researchers and presenting aggregate numbers is misinformative and should be ignored by policymakers.So what should governments do about AI x-risk? Our view isn’t that they should do nothing. But they should reject the kind of policies that might seem compelling if we view x-risk as urgent and serious, notably: restricting AI development. As we’ll argue in a future essay in this series, not only are such policies unnecessary, they are likely to increase x-risk. Instead, governments should adopt policies that are compatible with a range of possible estimates of AI risk, and are on balance helpful even if the risk is negligible. Fortunately, such policies exist. Governments should also change policymaking processes so that they are more responsive to new evidence. More on all that soon.The XPT report is titled Forecasting Existential Risks: Evidence from a Long-Run Forecasting Tournament.Tetlock and Gardner’s book Superforecasting summarizes research by Tetlock, Barbara Mellers, and others.Scott Alexander refutes the claim that there is something wrong in principle with ascribing probabilities to unique events (we largely agree). Our argument differs in two key ways from the position he addresses. We aren’t talking about forecasting in general; just its application to policymaking. And we don’t object to it in principle. Our argument is empirical: AI x-risk forecasts are extremely unreliable and lack justification. Theoretically this could change in the future, though we aren’t holding our breath.A paper by Friedman and Zuckhauser explains why probabilities aren’t the whole story: two forecasts that have the same risk estimate might have very different implications for policymakers. In our view, AI x-risk forecasts fare poorly on two of the three dimensions of confidence: a sound basis in evidence and a narrow range of reasonable opinion.A paper by our Princeton CITP colleagues led by Shazeda Ahmed explains the epistemic culture of AI safety, which consists of “cohesive, interwoven social structures of knowledge-production and community-building”. It helps understand why practices such as forecasting have become pillars of how the AI safety community forms its beliefs, as opposed to the broader scientific community that centers practices such as peer review. Of course, we shouldn’t reject a view just because it doesn’t conform to scientific orthodoxy. But at the same time, we shouldn’t give any deference to the self-styled AI safety community’s views on AI safety. It is important to understand that the median member of the AI safety community holds one particular stance on AI safety — a stance that is highly contested and in our view rather alarmist.We have written extensively about evidence-based AI safety. Our best-known work includes the essay AI safety is not a model property and the paper titled On the Societal Impact of Open Foundation Models which was the result of a large collaboration.Acknowledgements. We are grateful to Benjamin Edelman, Ezra Karger, Matt Salganik, and Ollie Stephenson for feedback on a draft. This series of essays is based on an upcoming paper that benefited from feedback from many people, including Seth Lazar and members of the MINT lab at Australian National University, students in the Limits to Prediction course at Princeton, Shazeda Ahmed, and Zachary Siegel.","length":34984,"excerpt":"How speculation gets laundered through pseudo-quantification","byline":"Arvind Narayanan, Sayash Kapoor","dir":null,"siteName":"AI Snake Oil","lang":"en"},"finalizedMeta":{"title":"AI existential risk probabilities are too unreliable to inform policy","description":"How speculation gets laundered through pseudo-quantification","author":"Arvind Narayanan","creator":"Arvind Narayanan","publisher":"AI Snake Oil","date":"2024-07-26T11:29:25+00:00","image":{"@type":"ImageObject","url":"https://substack-post-media.s3.amazonaws.com/public/images/6418811e-f1c2-4127-b6dd-f243c4f458ef_1398x918.png"},"topics":[]},"jsonLd":{"@type":"NewsArticle","headline":"AI existential risk probabilities are too unreliable to inform policy","description":"How speculation gets laundered through pseudo-quantification","image":[{"@type":"ImageObject","url":"https://substack-post-media.s3.amazonaws.com/public/images/6418811e-f1c2-4127-b6dd-f243c4f458ef_1398x918.png"}],"mainEntityOfPage":"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities","datePublished":"2024-07-26T11:29:25+00:00","dateModified":"2024-07-26T11:29:25+00:00","isAccessibleForFree":true,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":[{"@type":"Person","name":"Arvind Narayanan","url":"https://substack.com/@arvindnarayanan","description":null,"identifier":"user:19265909","image":{"@type":"ImageObject","contentUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F149ffaff-ad45-4b76-9667-bbd39d3a3d23_1795x1795.jpeg","thumbnailUrl":"https://substackcdn.com/image/fetch/w_128,h_128,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F149ffaff-ad45-4b76-9667-bbd39d3a3d23_1795x1795.jpeg"}},{"@type":"Person","name":"Sayash Kapoor","url":"https://substack.com/@sayash","description":"CS PhD candidate at Princeton. I study the societal impact of AI. Currently writing a book on AI Snake Oil: http://aisnakeoil.com ","identifier":"user:891603","image":{"@type":"ImageObject","contentUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f87ce8-8dbc-468f-8f8b-9fbf430e323c_976x974.png","thumbnailUrl":"https://substackcdn.com/image/fetch/w_128,h_128,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f87ce8-8dbc-468f-8f8b-9fbf430e323c_976x974.png"}}],"publisher":{"@type":"Organization","name":"AI Snake Oil","url":"https://www.aisnakeoil.com","description":"What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference","interactionStatistic":{"@type":"InteractionCounter","name":"Subscribers","interactionType":"https://schema.org/SubscribeAction","userInteractionCount":10000},"identifier":"pub:1008003","logo":{"@type":"ImageObject","url":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png","contentUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png","thumbnailUrl":"https://substackcdn.com/image/fetch/w_128,h_128,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png"},"image":{"@type":"ImageObject","url":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png","contentUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png","thumbnailUrl":"https://substackcdn.com/image/fetch/w_128,h_128,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png"}},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"@context":"https://schema.org","url":"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities"},"twitterObj":false,"status":200,"metadata":{"author":"Arvind Narayanan","title":"AI existential risk probabilities are too unreliable to inform policy","description":"How speculation gets laundered through pseudo-quantification","canonical":"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities","keywords":[],"image":"https://substackcdn.com/image/fetch/w_96,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png","firstParagraph":"How seriously should governments take the threat of existential risk from AI, given the lack of consensus among researchers? On the one hand, existential risks (x-risks) are necessarily somewhat speculative: by the time there is concrete evidence, it may be too late. On the other hand, governments must prioritize — after all, they don’t worry too much about x-risk from alien invasions."},"dublinCore":{},"opengraph":{"title":"AI existential risk probabilities are too unreliable to inform policy","description":"How speculation gets laundered through pseudo-quantification","url":"https://www.aisnakeoil.com/p/ai-existential-risk-probabilities","site_name":false,"locale":false,"type":"article","typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":"https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6418811e-f1c2-4127-b6dd-f243c4f458ef_1398x918.png"},"twitter":{"site":false,"description":"How speculation gets laundered through pseudo-quantification","card":"summary_large_image","creator":false,"title":"AI existential risk probabilities are too unreliable to inform policy","image":"https://substackcdn.com/image/fetch/f_auto,q_auto:best,fl_progressive:steep/https%3A%2F%2Faisnakeoil.substack.com%2Fapi%2Fv1%2Fpost_preview%2F147019742%2Ftwitter.jpg%3Fversion%3D4"},"archivedData":{"link":false,"wayback":false}}}