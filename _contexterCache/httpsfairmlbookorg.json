{"initialLink":"https://fairmlbook.org/","sanitizedLink":"https://fairmlbook.org/","finalLink":"https://fairmlbook.org/","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://fairmlbook.org/\" itemprop=\"url\">Fairness and machine learning</a></contexter-box-head></contexter-box-head><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2023-04-24T14:13:30.535Z\">4/24/2023</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>We welcome your feedback, questions, and suggestions. You can reach us at contact@fairmlbook.org. If you taught from the book, we’d love to hear about it.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a is=\"contexter-link\" href=\"https://fairmlbook.org/\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"519396efab05dfb7d58655040893f4353bd47f6b","data":{"originalLink":"https://fairmlbook.org/","sanitizedLink":"https://fairmlbook.org/","canonical":"https://fairmlbook.org/","htmlText":"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"utf-8\">\n  <meta name=\"generator\" content=\"pandoc\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\">\n  <title>Fairness and machine learning</title>\n  <link rel=\"stylesheet\" href=\"style.css\">\n  <!--[if lt IE 9]>\n    <script src=\"//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js\"></script>\n  <![endif]-->\n</head>\n<body>\n<article>\n<header>\n<h1 class=\"booktitle\">Fairness and machine learning</h1>\n<h2 class=\"booksubtitle\">Limitations and Opportunities</h2>\n<h1 class=\"bookauthor\">Solon Barocas, Moritz Hardt, Arvind Narayanan</h2>\n</header>\n\n<div id=\"frontpage\">\n<div id=\"indexcontents\">\n<table>\n<thead>\n<tr>\n<td colspan=\"3\">\nCONTENTS\n</td>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n</td>\n<td>\n<a class=\"chaptername\" href=\"preface.html\">Preface</a>\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\n<a class=\"chaptername\" href=\"acknowledgments.html\">Acknowledgments</a>\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td class=\"chapternumber\">\n1\n</td>\n<td class=\"chaptername\">\n<a href=\"introduction.html\">Introduction</a>\n</td>\n<td>\n<a href=\"pdf/introduction.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td class=\"chapternumber\">\n2\n</td>\n<td class=\"chaptername\">\n<a href=\"legitimacy.html\">When is automated decision making legitimate?</a>\n</td>\n<td>\n<a href=\"pdf/legitimacy.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nWe explore what makes automated decision making a matter of normative concern, situated in bureaucratic decision making and its mechanical application of formalized rules.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td class=\"chapternumber\">\n3\n</td>\n<td>\n<a class=\"chaptername\" href=\"classification.html\">Classification</a>\n</td>\n<td>\n<a href=\"pdf/classification.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nWe introduce formal non-discrimination criteria in a decision-theoretic setting, establish their relationships, and illustrate their limitations.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td class=\"chapternumber\">\n4\n</td>\n<td>\n<a class=\"chaptername\" href=\"relative.html\">Relative notions of fairness</a>\n</td>\n<td>\n<a href=\"pdf/relative.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nWe explore the normative underpinnings of objections to systematic differences in the treatment of different groups and inequalities in the outcomes experienced by these groups.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td class=\"chapternumber\">\n5\n</td>\n<td>\n<a class=\"chaptername\" href=\"causal.html\">Causality</a>\n</td>\n<td>\n<a href=\"pdf/causal.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nWe dive into the technical repertoire of causality and how it helps articulate and address shortcomings of the classification paradigm, while raising new conceptual and normative questions.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td class=\"chapternumber\">\n6\n</td>\n<td>\n<a class=\"chaptername\" href=\"legal.html\">Understanding United States anti-discrimination law</a>\n</td>\n<td>\n<a href=\"pdf/legal.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nWe discuss what United States anti-discrimination law is and isn’t, how it navigates tradeoffs, its limits, and how it applies to machine learning.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td class=\"chapternumber\">\n7\n</td>\n<td>\n<a class=\"chaptername\" href=\"testing.html\">Testing discrimination in practice</a>\n</td>\n<td>\n<a href=\"pdf/testing.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nWe systematize tests of discrimination and discuss the practical complexities of applying them, both to traditional decision-making systems and to algorithmic systems.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td class=\"chapternumber\">\n8\n</td>\n<td>\n<a class=\"chaptername\" href=\"broader-view.html\">A broader view of discrimination</a>\n</td>\n<td>\n<a href=\"pdf/broader-view.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nWe review structural, organizational, and interpersonal discrimination in society, how machine learning interacts with them, and discuss a broad set of potential interventions.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td class=\"chapternumber\">\n9\n</td>\n<td>\n<a class=\"chaptername\" href=\"datasets.html\">Datasets</a>\n</td>\n<td>\n<a href=\"pdf/datasets.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nDatasets are the backbone of machine learning research and development. We critically examine their role, the harms associated with data, and survey improvements in data practices.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td class=\"chapternumber\">\n10\n</td>\n<td class=\"chaptername\">\nWhere to go from here\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nIn this online-only chapter, we survey and systematize recent work on algorithmic fairness.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td class=\"chapternumber\">\nA\n</td>\n<td class=\"chaptername\">\nProblems\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nProblem sets, available online\n</td>\n<td>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<section id=\"video-tutorials\" class=\"level1\">\n<h1>Video tutorials</h1>\n<ul>\n<li>Fairness and Machine Learning (<a href=\"https://www.youtube.com/watch?v=Igq_S_7IfOU\">Part 1</a>, <a href=\"https://www.youtube.com/watch?v=9oNVFQ9llPc\">Part 2</a>) (MLSS 2020)</li>\n<li><a href=\"tutorial1.html\">Fairness in machine learning</a> (NeurIPS 2017)</li>\n<li><a href=\"tutorial2.html\">21 fairness definitions and their politics</a> (FAccT 2018)</li>\n</ul>\n</section>\n<section id=\"course-materials\" class=\"level1\">\n<h1>Course materials</h1>\n<ul>\n<li><a href=\"https://fairmlclass.github.io/\">Berkeley CS 294: Fairness in machine learning</a></li>\n<li><a href=\"https://docs.google.com/document/d/1GV97qqvjQNvyM2I01vuRaAwHe9pQAZ9pbP7KkKveg1o/edit\">Cornell INFO 4270: Ethics and policy in data science</a></li>\n<li><a href=\"https://docs.google.com/document/d/1XnbJXELA0L3CX41MxySdPsZ-HNECxPtAw4-kZRc7OPI/edit?usp=sharing\">Princeton COS 597E: Fairness in machine learning</a></li>\n</ul>\n</section>\n<section id=\"contact-us\" class=\"level1\">\n<h1>Contact us</h1>\n<p>We welcome your feedback, questions, and suggestions. You can reach us at <em>contact@fairmlbook.org.</em> If you taught from the book, we’d love to hear about it.</p>\n</section>\n<section id=\"citations-license-typesetting\" class=\"level1\">\n<h1>Citations, license, typesetting</h1>\n<p>To cite this book, please use this bibtex entry:</p>\n<pre><code>@book{barocas-hardt-narayanan,\n  title = {Fairness and Machine Learning: Limitations and Opportunities},\n  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},\n  publisher = {fairmlbook.org},\n  note = {\\url{http://www.fairmlbook.org}},\n  year = {2019}\n}\n</code></pre>\n<ul>\n<li><p>A hardcover print edition will be published by MIT Press in 2023.</p></li>\n<li><p>The text available on this website is licensed under the <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons BY-NC-ND 4.0</a> license.</p></li>\n<li><p>This book is typeset using <a href=\"https://www.pandoc.org\">pandoc</a> with the <a href=\"https://github.com/mrtzh/unbuch\">unbuch</a> setup.</p></li>\n</ul>\n</section>\n</div>\n\n<div id=\"lastupdate\">\nLast updated: Mon Nov  7 17:03:56 CET 2022\n</div>\n</article>\n</body>\n</html>\n","oembed":false,"readabilityObject":{"title":"Fairness and machine learning","content":"<div id=\"readability-page-1\" class=\"page\"><div id=\"frontpage\">\n<div id=\"indexcontents\">\n<table>\n<thead>\n<tr>\n<td colspan=\"3\">\nCONTENTS\n</td>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n</td>\n<td>\n<a href=\"preface.html\">Preface</a>\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\n<a href=\"acknowledgments.html\">Acknowledgments</a>\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n1\n</td>\n<td>\n<a href=\"introduction.html\">Introduction</a>\n</td>\n<td>\n<a href=\"pdf/introduction.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n2\n</td>\n<td>\n<a href=\"legitimacy.html\">When is automated decision making legitimate?</a>\n</td>\n<td>\n<a href=\"pdf/legitimacy.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nWe explore what makes automated decision making a matter of normative concern, situated in bureaucratic decision making and its mechanical application of formalized rules.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n3\n</td>\n<td>\n<a href=\"classification.html\">Classification</a>\n</td>\n<td>\n<a href=\"pdf/classification.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nWe introduce formal non-discrimination criteria in a decision-theoretic setting, establish their relationships, and illustrate their limitations.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n4\n</td>\n<td>\n<a href=\"relative.html\">Relative notions of fairness</a>\n</td>\n<td>\n<a href=\"pdf/relative.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nWe explore the normative underpinnings of objections to systematic differences in the treatment of different groups and inequalities in the outcomes experienced by these groups.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n5\n</td>\n<td>\n<a href=\"causal.html\">Causality</a>\n</td>\n<td>\n<a href=\"pdf/causal.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nWe dive into the technical repertoire of causality and how it helps articulate and address shortcomings of the classification paradigm, while raising new conceptual and normative questions.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n6\n</td>\n<td>\n<a href=\"legal.html\">Understanding United States anti-discrimination law</a>\n</td>\n<td>\n<a href=\"pdf/legal.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nWe discuss what United States anti-discrimination law is and isn’t, how it navigates tradeoffs, its limits, and how it applies to machine learning.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n7\n</td>\n<td>\n<a href=\"testing.html\">Testing discrimination in practice</a>\n</td>\n<td>\n<a href=\"pdf/testing.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nWe systematize tests of discrimination and discuss the practical complexities of applying them, both to traditional decision-making systems and to algorithmic systems.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n8\n</td>\n<td>\n<a href=\"broader-view.html\">A broader view of discrimination</a>\n</td>\n<td>\n<a href=\"pdf/broader-view.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nWe review structural, organizational, and interpersonal discrimination in society, how machine learning interacts with them, and discuss a broad set of potential interventions.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n9\n</td>\n<td>\n<a href=\"datasets.html\">Datasets</a>\n</td>\n<td>\n<a href=\"pdf/datasets.pdf\">PDF</a>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nDatasets are the backbone of machine learning research and development. We critically examine their role, the harms associated with data, and survey improvements in data practices.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n10\n</td>\n<td>\nWhere to go from here\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nIn this online-only chapter, we survey and systematize recent work on algorithmic fairness.\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\nA\n</td>\n<td>\nProblems\n</td>\n<td>\n</td>\n</tr>\n<tr>\n<td>\n</td>\n<td>\nProblem sets, available online\n</td>\n<td>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<section id=\"video-tutorials\">\n<h2>Video tutorials</h2>\n<ul>\n<li>Fairness and Machine Learning (<a href=\"https://www.youtube.com/watch?v=Igq_S_7IfOU\">Part 1</a>, <a href=\"https://www.youtube.com/watch?v=9oNVFQ9llPc\">Part 2</a>) (MLSS 2020)</li>\n<li><a href=\"tutorial1.html\">Fairness in machine learning</a> (NeurIPS 2017)</li>\n<li><a href=\"tutorial2.html\">21 fairness definitions and their politics</a> (FAccT 2018)</li>\n</ul>\n</section>\n<section id=\"course-materials\">\n<h2>Course materials</h2>\n<ul>\n<li><a href=\"https://fairmlclass.github.io/\">Berkeley CS 294: Fairness in machine learning</a></li>\n<li><a href=\"https://docs.google.com/document/d/1GV97qqvjQNvyM2I01vuRaAwHe9pQAZ9pbP7KkKveg1o/edit\">Cornell INFO 4270: Ethics and policy in data science</a></li>\n<li><a href=\"https://docs.google.com/document/d/1XnbJXELA0L3CX41MxySdPsZ-HNECxPtAw4-kZRc7OPI/edit?usp=sharing\">Princeton COS 597E: Fairness in machine learning</a></li>\n</ul>\n</section>\n<section id=\"contact-us\">\n<h2>Contact us</h2>\n<p>We welcome your feedback, questions, and suggestions. You can reach us at <em>contact@fairmlbook.org.</em> If you taught from the book, we’d love to hear about it.</p>\n</section>\n<section id=\"citations-license-typesetting\">\n<h2>Citations, license, typesetting</h2>\n<p>To cite this book, please use this bibtex entry:</p>\n<pre><code>@book{barocas-hardt-narayanan,\n  title = {Fairness and Machine Learning: Limitations and Opportunities},\n  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},\n  publisher = {fairmlbook.org},\n  note = {\\url{http://www.fairmlbook.org}},\n  year = {2019}\n}\n</code></pre>\n<ul>\n<li><p>A hardcover print edition will be published by MIT Press in 2023.</p></li>\n<li><p>The text available on this website is licensed under the <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons BY-NC-ND 4.0</a> license.</p></li>\n<li><p>This book is typeset using <a href=\"https://www.pandoc.org/\">pandoc</a> with the <a href=\"https://github.com/mrtzh/unbuch\">unbuch</a> setup.</p></li>\n</ul>\n</section>\n</div></div>","textContent":"\n\n\n\n\n\nCONTENTS\n\n\n\n\n\n\n\n\nPreface\n\n\n\n\n\n\n\n\nAcknowledgments\n\n\n\n\n\n\n1\n\n\nIntroduction\n\n\nPDF\n\n\n\n\n2\n\n\nWhen is automated decision making legitimate?\n\n\nPDF\n\n\n\n\n\n\nWe explore what makes automated decision making a matter of normative concern, situated in bureaucratic decision making and its mechanical application of formalized rules.\n\n\n\n\n\n\n3\n\n\nClassification\n\n\nPDF\n\n\n\n\n\n\nWe introduce formal non-discrimination criteria in a decision-theoretic setting, establish their relationships, and illustrate their limitations.\n\n\n\n\n\n\n4\n\n\nRelative notions of fairness\n\n\nPDF\n\n\n\n\n\n\nWe explore the normative underpinnings of objections to systematic differences in the treatment of different groups and inequalities in the outcomes experienced by these groups.\n\n\n\n\n\n\n5\n\n\nCausality\n\n\nPDF\n\n\n\n\n\n\nWe dive into the technical repertoire of causality and how it helps articulate and address shortcomings of the classification paradigm, while raising new conceptual and normative questions.\n\n\n\n\n\n\n6\n\n\nUnderstanding United States anti-discrimination law\n\n\nPDF\n\n\n\n\n\n\nWe discuss what United States anti-discrimination law is and isn’t, how it navigates tradeoffs, its limits, and how it applies to machine learning.\n\n\n\n\n\n\n7\n\n\nTesting discrimination in practice\n\n\nPDF\n\n\n\n\n\n\nWe systematize tests of discrimination and discuss the practical complexities of applying them, both to traditional decision-making systems and to algorithmic systems.\n\n\n\n\n\n\n8\n\n\nA broader view of discrimination\n\n\nPDF\n\n\n\n\n\n\nWe review structural, organizational, and interpersonal discrimination in society, how machine learning interacts with them, and discuss a broad set of potential interventions.\n\n\n\n\n\n\n9\n\n\nDatasets\n\n\nPDF\n\n\n\n\n\n\nDatasets are the backbone of machine learning research and development. We critically examine their role, the harms associated with data, and survey improvements in data practices.\n\n\n\n\n\n\n10\n\n\nWhere to go from here\n\n\n\n\n\n\n\n\nIn this online-only chapter, we survey and systematize recent work on algorithmic fairness.\n\n\n\n\n\n\nA\n\n\nProblems\n\n\n\n\n\n\n\n\nProblem sets, available online\n\n\n\n\n\n\n\n\nVideo tutorials\n\nFairness and Machine Learning (Part 1, Part 2) (MLSS 2020)\nFairness in machine learning (NeurIPS 2017)\n21 fairness definitions and their politics (FAccT 2018)\n\n\n\nCourse materials\n\nBerkeley CS 294: Fairness in machine learning\nCornell INFO 4270: Ethics and policy in data science\nPrinceton COS 597E: Fairness in machine learning\n\n\n\nContact us\nWe welcome your feedback, questions, and suggestions. You can reach us at contact@fairmlbook.org. If you taught from the book, we’d love to hear about it.\n\n\nCitations, license, typesetting\nTo cite this book, please use this bibtex entry:\n@book{barocas-hardt-narayanan,\n  title = {Fairness and Machine Learning: Limitations and Opportunities},\n  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},\n  publisher = {fairmlbook.org},\n  note = {\\url{http://www.fairmlbook.org}},\n  year = {2019}\n}\n\n\nA hardcover print edition will be published by MIT Press in 2023.\nThe text available on this website is licensed under the Creative Commons BY-NC-ND 4.0 license.\nThis book is typeset using pandoc with the unbuch setup.\n\n\n","length":3131,"excerpt":"We welcome your feedback, questions, and suggestions. You can reach us at contact@fairmlbook.org. If you taught from the book, we’d love to hear about it.","byline":"Solon Barocas, Moritz Hardt, Arvind Narayanan","dir":null,"siteName":null,"lang":"en"},"finalizedMeta":{"title":"Fairness and machine learning","description":"We welcome your feedback, questions, and suggestions. You can reach us at contact@fairmlbook.org. If you taught from the book, we’d love to hear about it.","author":false,"creator":"","publisher":false,"date":"2023-04-24T14:13:30.535Z","topics":[]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"Fairness and machine learning","description":false,"canonical":"https://fairmlbook.org/","keywords":[],"image":false,"firstParagraph":"We welcome your feedback, questions, and suggestions. You can reach us at contact@fairmlbook.org. If you taught from the book, we’d love to hear about it."},"dublinCore":{},"opengraph":{"title":false,"description":false,"url":false,"site_name":false,"locale":false,"type":false,"typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":false},"twitter":{"site":false,"description":false,"card":false,"creator":false,"title":false,"image":false},"archivedData":{"link":false,"wayback":false}}}