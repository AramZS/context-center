{"initialLink":"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk","sanitizedLink":"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk","finalLink":"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"><img src=\"https://support.currentaffairs.org/hubfs/Imported_Blog_Media/torres2-1024x646.jpg\" alt=\"\" itemprop=\"image\" /></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk\" itemprop=\"url\">The Dangerous Ideas of “Longtermism” and “Existential Risk”</a></contexter-box-head></contexter-box-head><contexter-byline class=\"p-author author\" slot=\"author\"><span class=\"p-name byline\" rel=\"author\" itemprop=\"author\">Émile P. Torres</span></contexter-byline><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2021-07-28T04:00:00.000Z\">7/28/2021</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>So-called rationalists have created a disturbing secular religion that looks like it addresses humanity’s deepest problems, but actually justifies pursuing the social preferences of elites.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a is=\"contexter-link\" href=\"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"dbbee96297c99d78e3ecd7ab91851f977061f29f","data":{"originalLink":"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk","sanitizedLink":"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk","canonical":"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk","htmlText":"<!doctype html><html lang=\"en\"><head>\n    <meta charset=\"utf-8\">\n    <title>The Dangerous Ideas of “Longtermism” and “Existential Risk”</title>\n    <link rel=\"shortcut icon\" href=\"https://www.currentaffairs.org/hubfs/favicon.png\">\n    <meta name=\"description\" content=\"So-called rationalists have created a disturbing secular religion that looks like it addresses humanity’s deepest problems, but actually justifies pursuing the social preferences of elites.\">\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n\n    \n    <meta property=\"og:description\" content=\"So-called rationalists have created a disturbing secular religion that looks like it addresses humanity’s deepest problems, but actually justifies pursuing the social preferences of elites.\">\n    <meta property=\"og:title\" content=\"The Dangerous Ideas of “Longtermism” and “Existential Risk”\">\n    <meta name=\"twitter:description\" content=\"So-called rationalists have created a disturbing secular religion that looks like it addresses humanity’s deepest problems, but actually justifies pursuing the social preferences of elites.\">\n    <meta name=\"twitter:title\" content=\"The Dangerous Ideas of “Longtermism” and “Existential Risk”\">\n\n    \n\n    \n    <style>\na.cta_button{-moz-box-sizing:content-box !important;-webkit-box-sizing:content-box !important;box-sizing:content-box !important;vertical-align:middle}.hs-breadcrumb-menu{list-style-type:none;margin:0px 0px 0px 0px;padding:0px 0px 0px 0px}.hs-breadcrumb-menu-item{float:left;padding:10px 0px 10px 10px}.hs-breadcrumb-menu-divider:before{content:'›';padding-left:10px}.hs-featured-image-link{border:0}.hs-featured-image{float:right;margin:0 0 20px 20px;max-width:50%}@media (max-width: 568px){.hs-featured-image{float:none;margin:0;width:100%;max-width:100%}}.hs-screen-reader-text{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px}\n</style>\n\n<link rel=\"stylesheet\" href=\"https://www.currentaffairs.org/hs-fs/hub/43971025/hub_generated/template_assets/155741666336/1715269482131/current_affairs/css/main.min.css\">\n<link rel=\"stylesheet\" href=\"https://www.currentaffairs.org/hs-fs/hubfs/hub_generated/template_assets/1/155741183788/1738946593661/template_blog.min.css\">\n<link rel=\"stylesheet\" href=\"https://www.currentaffairs.org/hs-fs/hub/43971025/hub_generated/template_assets/155741666329/1717212216717/current_affairs/css/theme-overrides.min.css\">\n<link rel=\"stylesheet\" href=\"https://www.currentaffairs.org/hs-fs/hub/43971025/hub_generated/template_assets/157130953109/1717439550912/current_affairs/css/compiled.min.css\">\n<link rel=\"stylesheet\" href=\"https://www.currentaffairs.org/hs-fs/hubfs/hub_generated/template_assets/1/166937059869/1738877016568/template_styles.min.css\">\n<link rel=\"stylesheet\" href=\"https://www.currentaffairs.org/hs-fs/hubfs/hub_generated/module_assets/1/169311965992/1738875160565/module_CA_search_input_live.min.css\">\n\n<style>\n  #hs_cos_wrapper_module_171744129944114 .hs-search-field__bar>form { border-radius:px; }\n\n#hs_cos_wrapper_module_171744129944114 .hs-search-field__bar>form>label {}\n\n#hs_cos_wrapper_module_171744129944114 .hs-search-field__bar>form>.hs-search-field__input { border-radius:px; }\n\n#hs_cos_wrapper_module_171744129944114 .hs-search-field__button { border-radius:px; }\n\n#hs_cos_wrapper_module_171744129944114 .hs-search-field__button:hover,\n#hs_cos_wrapper_module_171744129944114 .hs-search-field__button:focus {}\n\n#hs_cos_wrapper_module_171744129944114 .hs-search-field__button:active {}\n\n#hs_cos_wrapper_module_171744129944114 .hs-search-field--open .hs-search-field__suggestions { border-radius:px; }\n\n#hs_cos_wrapper_module_171744129944114 .hs-search-field--open .hs-search-field__suggestions a {}\n\n#hs_cos_wrapper_module_171744129944114 .hs-search-field--open .hs-search-field__suggestions a:hover {}\n\n</style>\n\n<link rel=\"stylesheet\" href=\"https://www.currentaffairs.org/hs-fs/hub/43971025/hub_generated/module_assets/177418297697/1725370401391/module_177418297697_Announcing_Newest_Issue_Global.min.css\">\n<link rel=\"stylesheet\" href=\"https://www.currentaffairs.org/hs-fs/hub/43971025/hub_generated/module_assets/166857549683/1728675332526/module_166857549683_newsletter_signup.min.css\">\n<link rel=\"stylesheet\" href=\"https://www.currentaffairs.org/hs-fs/hub/43971025/hub_generated/module_assets/167619276679/1715879144804/module_167619276679_featured_product.min.css\">\n<link rel=\"stylesheet\" href=\"https://www.currentaffairs.org/hs-fs/hub/43971025/hub_generated/module_assets/167630589750/1715880071638/module_167630589750_Evergreen_Plugs_for_Blog.min.css\">\n<link rel=\"stylesheet\" href=\"https://www.currentaffairs.org/hs-fs/hub/43971025/hub_generated/module_assets/167630602843/1715880240262/module_167630602843_Subscribe_Block_for_Blog_Post.min.css\">\n<style>\n  @font-face {\n    font-family: \"EB Garamond\";\n    font-weight: 400;\n    font-style: normal;\n    font-display: swap;\n    src: url(\"/_hcms/googlefonts/EB_Garamond/regular.woff2\") format(\"woff2\"), url(\"/_hcms/googlefonts/EB_Garamond/regular.woff\") format(\"woff\");\n  }\n  @font-face {\n    font-family: \"EB Garamond\";\n    font-weight: 700;\n    font-style: normal;\n    font-display: swap;\n    src: url(\"/_hcms/googlefonts/EB_Garamond/700.woff2\") format(\"woff2\"), url(\"/_hcms/googlefonts/EB_Garamond/700.woff\") format(\"woff\");\n  }\n</style>\n\n    <script type=\"application/ld+json\">\n{\n  \"mainEntityOfPage\" : {\n    \"@type\" : \"WebPage\",\n    \"@id\" : \"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk\"\n  },\n  \"author\" : {\n    \"name\" : \"Émile P. Torres\",\n    \"url\" : \"https://www.currentaffairs.org/news/author/émile-p-torres\",\n    \"@type\" : \"Person\"\n  },\n  \"headline\" : \"The Dangerous Ideas of “Longtermism” and “Existential Risk”\",\n  \"datePublished\" : \"2021-07-28T04:00:00.000Z\",\n  \"dateModified\" : \"2024-05-30T21:49:02.676Z\",\n  \"publisher\" : {\n    \"name\" : \"Current Affairs\",\n    \"logo\" : {\n      \"@type\" : \"ImageObject\"\n    },\n    \"@type\" : \"Organization\"\n  },\n  \"@context\" : \"https://schema.org\",\n  \"@type\" : \"BlogPosting\",\n  \"image\" : [ \"https://support.currentaffairs.org/hubfs/Imported_Blog_Media/torres2-1024x646.jpg\" ]\n}\n</script>\n\n\n<script data-search_input-config=\"config_module_171744129944114\" type=\"application/json\">\n{\n  \"autosuggest_results_message\": \"Results for \\u201C[[search_term]]\\u201D\",\n  \"autosuggest_no_results_message\": \"There are no autosuggest results for \\u201C[[search_term]]\\u201D\",\n  \"sr_empty_search_field_message\": \"There are no suggestions because the search field is empty.\",\n  \"sr_autosuggest_results_message\": \"There are currently [[number_of_results]] auto-suggested results for [[search_term]]. Navigate to the results list by pressing the down arrow key, or press return to search for all results.\",\n  \"sr_search_field_aria_label\": \"This is a search field with an auto-suggest feature attached.\",\n  \"sr_search_button_aria_label\": \"Search\"\n}\n</script>\n\n\n    \n\n<!-- Site Schema by Hypha HubSpot Development // added 6/3/24 -->\n\n<script type=\"application/ld+json\">\n {\n     \"@context\": \"http://schema.org\",\n     \"@type\": \"Organization\",\n     \"mainEntityOfPage\":{\n          \"@type\":\"WebPage\",\n          \"@id\":\"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk\",\n          \"description\": \"So-called rationalists have created a disturbing secular religion that looks like it addresses humanity’s deepest problems, but actually justifies pursuing the social preferences of elites.\"\n     },\n     \"url\": \"/\",\n     \"logo\": \"\",\n     \"image\": \"https://support.currentaffairs.org/hubfs/images/ca-wordmark-black.jpg\",\n     \"name\": \"Current Affairs Inc\",\n     \"address\": {\n          \"@type\": \"PostalAddress\",\n          \"streetAddress\": \"300 Lafayette Street, Suite 210 \",\n          \"addressLocality\": \"New Orleans\",\n          \"addressRegion\": \"LA\",\n          \"addressCountry\": \"\",\n          \"postalCode\": \"70130\"\n     },\n     \"telephone\": \"\",\n     \"email\": \"REPLACEhello@hyphadev.ioREPLACE\",\n     \"sameAs\": [\n          \"https://twitter.com/curaffairs\",\n          \"https://facebook.com/curaffairs\",\n          \"https://instagram.com/curaffairsmag\"\n     ]\n}\n</script>\n<!-- End Schema -->\n\n<!-- Google tag (gtag.js) --> \n<script async src=\"https://www.googletagmanager.com/gtag/js?id=AW-16576808317\"></script> \n<script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'AW-16576808317'); </script>\n<meta property=\"og:image\" content=\"https://www.currentaffairs.org/hubfs/Imported_Blog_Media/torres2-1024x646.jpg\">\n<meta property=\"og:image:width\" content=\"1024\">\n<meta property=\"og:image:height\" content=\"646\">\n\n<meta name=\"twitter:image\" content=\"https://www.currentaffairs.org/hubfs/Imported_Blog_Media/torres2-1024x646.jpg\">\n\n\n<meta property=\"og:url\" content=\"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk\">\n<meta name=\"twitter:card\" content=\"summary_large_image\">\n\n<link rel=\"canonical\" href=\"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk\">\n<script async src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7247492731377136\" crossorigin=\"anonymous\"></script>\n<meta property=\"og:type\" content=\"article\">\n<link rel=\"alternate\" type=\"application/rss+xml\" href=\"https://www.currentaffairs.org/news/rss.xml\">\n<meta name=\"twitter:domain\" content=\"www.currentaffairs.org\">\n<script src=\"//platform.linkedin.com/in.js\" type=\"text/javascript\">\n    lang: en_US\n</script>\n\n<meta http-equiv=\"content-language\" content=\"en\">\n\n\n\n\n\n\n    <link rel=\"stylesheet\" href=\"https://use.typekit.net/aiq3flp.css\">\n    <link href=\"https://use.typekit.net/kta1qem.css\" rel=\"stylesheet\">\n    <script src=\"https://www.google.com/recaptcha/api.js\"></script>\n    <script src=\"https://www.currentaffairs.org/hubfs/javascript/jquery-3.3.1.min.js\"></script>\n    <script src=\"/static/javascripts/jquery.onscreen.js\"></script>\n    <script src=\"/static/javascripts/lazyload.min.js\"></script>\n    <script src=\"https://www.currentaffairs.org/hubfs/javascript/vh-check.min.js\"></script> \n\n    \n    <script>\n      (function () {\n        // initialize the test\n        var isNeeded = vhCheck('vh-test');\n      }());\n    </script>\n    <!--[if lt IE 9]>\n        <script src=\"https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js\"></script>\n    <![endif]-->\n        <script src=\"https://www.currentaffairs.org/static/javascripts/dom-scripts.js\"></script>\n    <!-- <script src=\"/static/js/slogans.js\"></script> -->\n\n    <script>\n document.addEventListener(\"DOMContentLoaded\", function() {\n  const menuButton = document.querySelector(\"[data-trigger='menu-takeover']\");\n  const menuTakeover = document.querySelector(\".menu-takeover\");\n  \n  menuButton.addEventListener(\"click\", function() {\n    document.body.classList.toggle(\"show-modal-takeover\");\n    document.documentElement.classList.toggle(\"show-modal-takeover\");\n\n    // Add or remove the 'ready-to-animate' class based on the menu visibility\n    menuTakeover.classList.toggle(\"ready-to-animate\", document.body.classList.contains(\"show-modal-takeover\"));\n  });\n\n  // Optionally, you might want to close the menu when the \"Hide menu\" button is clicked.\n  const hideMenuButton = document.querySelector(\".hide-menu-takeover\");\n  if (hideMenuButton) {\n    hideMenuButton.addEventListener(\"click\", function() {\n      document.body.classList.remove(\"show-modal-takeover\");\n      document.documentElement.classList.remove(\"show-modal-takeover\");\n\n      // Remove the 'ready-to-animate' class when the menu is hidden\n      menuTakeover.classList.remove(\"ready-to-animate\");\n    });\n  }\n});\n\n    </script>\n  <meta name=\"generator\" content=\"HubSpot\"></head>\n  <body class=\"ca-2019\">\n    <div class=\"body-wrapper   hs-content-id-167252930999 hs-blog-post hs-blog-id-163488204299\">\n      \n      <div data-global-resource-path=\"current affairs/templates/partials/header-mast.html\"><!-- Begin partial -->\n<header class=\"masthead\">\n  <div class=\"bound\">\n    <div class=\"plate\">\n\n\n      <h1 class=\"brand\">\n        <a class=\"wordmark\" href=\"https://www.currentaffairs.org/\" title=\"Current Affairs\"><i class=\"fill\">Current Affairs</i></a>\n      </h1>\n\n      <p class=\"slogan\">\n        <span id=\"slogan\">A Magazine of Politics and Culture</span>\n      </p>\n\n\n      <nav class=\"wayfinding-primary large\">\n\n        <ul>\n          <li><a href=\"/magazine\"><span>Magazine</span></a></li>\n          <li><a href=\"/issues\"><span>Issues</span></a></li>\n          <li><a href=\"https://www.currentaffairs.org/news\"><span>Articles</span></a></li>\n          <li><a href=\"http://patreon.com/currentaffairs\"><span>Podcast</span></a></li>\n          <li><a href=\"https://shop.currentaffairs.org\"><span>Shop</span></a></li>\n          <li><a href=\"/donate\"><span>Donate</span></a></li>\n          <li><a href=\"/news-briefing\"><span>News Briefing</span></a></li>\n          <li><a href=\"/membership\"><span>Subscribe</span></a></li>\n          <li><a href=\"/faq\"><span>FAQ</span></a></li>\n        </ul>\n\n      </nav>\n\n      <aside class=\"wayfinding-secondary search\">\n        <mark class=\"action\">\n          <a data-fancybox=\"\" data-src=\"#modal\" class=\"btn btn-primary\"><span>Search</span></a>\n        </mark>\n      </aside>\n\n\n\n      <aside class=\"wayfinding-secondary account unauthenticated\">\n        <mark class=\"action\">\n          <a style=\"font-family: 'futura-pt-bold';\" href=\"https://pay.currentaffairs.org/p/login/cN215N5Sm6Eg2t2000\" target=\"_blank\"><span>Sign In</span></a>\n        </mark>\n      </aside>\n\n\n\n\n      \n\n\n\n      <div aria-hidden=\"true\" class=\"mobile-menu-shortcuts\">\n        <ul class=\"triptych\">\n          <li class=\"parent menu\">\n            <button data-trigger=\"menu-takeover\">Menu</button>\n          </li>\n\n          \n          <li class=\"parent search\">\n            <button data-fancybox=\"\" data-src=\"#modal\">Search</button>\n          </li>\n          \n          <li class=\"parent account unauthenticated\">\n            <a style=\"-webkit-appearance: none;\n                      background: 0 0;\n                      border: none;\n                      color: #ff2395;\n                      cursor: pointer;\n                      display: inline-block;\n                      font: 700 normal 1rem / 1 futura-pt-bold, sans-serif;\n                      font-size: .8125rem;\n                      letter-spacing: .25em;\n                      line-height: 1;\n                      margin-right: 0;\n                      padding: 10px;\n                      text-transform: uppercase;\" href=\"https://pay.currentaffairs.org/p/login/cN215N5Sm6Eg2t2000\" target=\"_blank\">Log In</a>\n          </li>\n\n          \n        </ul>\n      </div>\n\n\n    </div><!--/ .plate -->\n  </div><!--/ .bound -->\n</header>\n\n<!--==== Mobile Menu ===-->\n<div class=\"menu-takeover\">\n\n  <div class=\"setting\">\n    <div class=\"plate\">\n      <div class=\"content\">\n\n        <button class=\"hide-menu-takeover\">Hide menu</button>\n\n        <h1>Menu</h1>\n\n        <ul>\n          <li><a href=\"/magazine\"><span>Magazine</span></a></li>\n          <li><a href=\"/issues\"><span>Issues</span></a></li>\n          <li><a href=\"https://www.currentaffairs.org/news\"><span>Articles</span></a></li>\n          <li><a href=\"http://patreon.com/currentaffairs\"><span>Podcast</span></a></li>\n          <li><a href=\"https://shop.currentaffairs.org\"><span>Shop</span></a></li>\n          <li><a href=\"/donate\"><span>Donate</span></a></li>\n          <li><a href=\"/news-briefing\"><span>News Briefing</span></a></li>\n          <li><a href=\"/membership\"><span>Subscribe</span></a></li>\n          <li><a href=\"/faq\"><span>FAQ</span></a></li>\n        </ul>\n\n\n\n        \n\n      </div>\n    </div>\n  </div>\n\n</div><!--/ .menu-takeover -->\n\n\n\n<div id=\"hs_cos_wrapper_module_171744129944114\" class=\"hs_cos_wrapper hs_cos_wrapper_widget hs_cos_wrapper_type_module\" style=\"\" data-hs-cos-general-type=\"widget\" data-hs-cos-type=\"module\"><link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.2/jquery.fancybox.min.css\">\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n<div style=\"display: none;\" id=\"modal\">\n  <div class=\"fullscreen-search-wrapper\">\n<div class=\"hs-search-field\">\n    <div class=\"hs-search-field__bar hs-search-field__bar--button-inline  \">\n      <form data-hs-do-not-collect=\"true\" class=\"hs-search-field__form\" action=\"/hs-search-results\">\n\n        <label class=\"hs-search-field__label show-for-sr\" for=\"module_171744129944114-input\">This is a search field with an auto-suggest feature attached.</label>\n\n        <input role=\"combobox\" aria-expanded=\"false\" aria-controls=\"autocomplete-results\" aria-label=\"This is a search field with an auto-suggest feature attached.\" type=\"search\" class=\"hs-search-field__input\" id=\"module_171744129944114-input\" name=\"q\" autocomplete=\"off\" aria-autocomplete=\"list\" placeholder=\"Search\">\n\n        \n          <input type=\"hidden\" name=\"type\" value=\"SITE_PAGE\">\n        \n        \n        \n          <input type=\"hidden\" name=\"type\" value=\"BLOG_POST\">\n          <input type=\"hidden\" name=\"type\" value=\"LISTING_PAGE\">\n        \n        \n        \n\n        <button class=\"hs-search-field__button \" aria-label=\"Search\">\n          <span id=\"hs_cos_wrapper_module_171744129944114_\" class=\"hs_cos_wrapper hs_cos_wrapper_widget hs_cos_wrapper_type_icon\" style=\"\" data-hs-cos-general-type=\"widget\" data-hs-cos-type=\"icon\"><svg version=\"1.0\" xmlns=\"http://www.w3.org/2000/svg\" viewbox=\"0 0 512 512\" aria-hidden=\"true\"><g id=\"search1_layer\"><path d=\"M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z\" /></g></svg></span>\n           </button>\n        <div class=\"hs-search-field__suggestions-container \">\n          <ul id=\"autocomplete-results\" role=\"listbox\" aria-label=\"term\" class=\"hs-search-field__suggestions\">\n            \n              <li role=\"option\" tabindex=\"-1\" aria-posinset=\"1\" aria-setsize=\"0\" class=\"results-for show-for-sr\">There are no suggestions because the search field is empty.</li>\n            \n          </ul>\n        </div>\n      </form>\n    </div>\n    <div id=\"sr-messenger\" class=\"hs-search-sr-message-container show-for-sr\" role=\"status\" aria-live=\"polite\" aria-atomic=\"true\">\n    </div>\n</div>\n  </div>\n</div>\n\n\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.2/jquery.fancybox.min.js\"></script></div>\n<!-- End partial --></div>\n      \n\n      \n\n      <main id=\"main-content\" class=\"body-container-wrapper\">\n        \n<script type=\"text/javascript\" src=\"https://platform-api.sharethis.com/js/sharethis.js#property=67a5272f3c2c870019108a02&amp;product=sop&amp;source=platform\" async></script>\n<div style=\"overflow: hidden;\">\n  <div style=\"padding: 0px 20px;\" class=\"sharethis-sticky-share-buttons\"></div>\n</div>\n<div class=\"body-container body-container--blog-post\">\n\n  \n\n  <div class=\"content-wrapper\">\n    <article class=\"blog-post\">\n      <div class=\"article-splash\">\n        \n        <div class=\"blog-post__image-wrapper\">\n          <img class=\"blog-post__image\" src=\"https://www.currentaffairs.org/hubfs/Imported_Blog_Media/torres2-1024x646.jpg\" loading=\"eager\" alt=\"\">\n        </div>\n        \n        <div class=\"titling\">\n          <h1 class=\"title\">\n            <span id=\"hs_cos_wrapper_name\" class=\"hs_cos_wrapper hs_cos_wrapper_meta_field hs_cos_wrapper_type_text\" style=\"\" data-hs-cos-general-type=\"meta_field\" data-hs-cos-type=\"text\">The Dangerous Ideas of “Longtermism” and “Existential Risk”</span>\n          </h1>\n          <div id=\"hs_cos_wrapper_module_17156154065863\" class=\"hs_cos_wrapper hs_cos_wrapper_widget hs_cos_wrapper_type_module\" style=\"\" data-hs-cos-general-type=\"widget\" data-hs-cos-type=\"module\">\n\n\n\n<aside class=\"stamp\" data-stamp-type=\"web\">\n</aside>\n</div>\n        </div>\n\n        <div class=\"details\">\n          <div class=\"primary\">\n            <div class=\"tagline\">\n              <p>So-called rationalists have created a disturbing secular religion that looks like it addresses humanity’s deepest problems, but actually justifies pursuing the social preferences of elites.</p>\n            </div>\n            <div class=\"bylines\">\n              <ul>\n                <li><a href=\"https://www.currentaffairs.org/news/author/émile-p-torres\" rel=\"author\">Émile P. Torres</a></li>\n              </ul>\n            </div>\n          </div>\n          <div class=\"secondary\">\n            <mark class=\"dateline\">\n              <span>filed <time style=\"display: inline;\" datetime=\"2021-07-28 04:00:00\" class=\"blog-post__timestamp\">28 July 2021\n                </time> in</span> \n              \n              <a href=\"https://www.currentaffairs.org/news/tag/tech\" rel=\"tag\">Tech</a>\n              \n              \n            </mark>\n          </div>\n        </div>\n\n      </div>\n      <div class=\"blog-post__body article-output\">\n        <span id=\"hs_cos_wrapper_post_body\" class=\"hs_cos_wrapper hs_cos_wrapper_meta_field hs_cos_wrapper_type_rich_text\" style=\"\" data-hs-cos-general-type=\"meta_field\" data-hs-cos-type=\"rich_text\"><div class=\"bound\"> \n <section class=\"essay-block wp-block-currentaffairs-group\" data-essay-block-type=\"plain\"> \n  <p>In a late-2020 <a href=\"https://www.cnbc.com/2020/12/29/skype-co-founder-jaan-tallinn-on-3-most-concerning-existential-risks-.html\">interview</a> with CNBC, Skype cofounder Jaan Tallinn made a perplexing statement. “Climate change,” he said, “is not going to be an existential risk unless there’s a runaway scenario.” A “runaway scenario” would occur if crossing one or more critical thresholds in the climate system causes Earth’s thermostat to rise uncontrollably. The hotter it <em>has</em> become, the hotter it <em>will</em> become, via self-amplifying processes. This is probably <a href=\"https://theconversation.com/venus-was-once-more-earth-like-but-climate-change-made-it-uninhabitable-150445\">what happened</a> a few billion years ago on our planetary neighbor Venus, a hellish cauldron whose average surface temperature is high enough to melt lead and zinc.</p> \n  <p>Fortunately, the best science today suggests that a runaway scenario is <a href=\"https://theconversation.com/climate-explained-rising-carbon-emissions-probably-wont-make-the-earth-uninhabitable-155447\">unlikely</a>, although not impossible. Yet even without a runaway scenario, the best science also frighteningly affirms that climate change will have devastating consequences. It will <a href=\"https://docs.wixstatic.com/ugd/d9aaad_b2e7f0f56bec40a195e551dd3e8c878e.pdf\">precipitate</a> lethal heatwaves, megadroughts, catastrophic wildfires (like those seen recently in the Western U.S.), desertification, sea-level rise, mass migrations, widespread political instability, food-supply disruptions/famines, extreme weather events (more dangerous hurricanes and flash floods), infectious disease outbreaks, biodiversity loss, mass extinctions, ecological collapse, socioeconomic upheaval, <a href=\"https://slate.com/technology/2015/03/study-climate-change-helped-spark-syrian-civil-war.html\">terrorism and wars</a>, etc. To quote an ominous <a href=\"https://academic.oup.com/bioscience/article/70/1/8/5610806\">2020 paper</a> co-signed by more than 11,000 scientists from around the world, “planet Earth is facing a climate emergency” that, unless immediate and drastic action is taken, will bring about “untold suffering.”</p> \n  <p>So why does Tallinn think that climate change <em>isn’t</em> an existential risk? Intuitively, if anything should count as an existential risk it’s climate change, right?</p> \n  <p>Cynical readers might suspect that, given Tallinn’s immense fortune of an <a href=\"https://fortune.com/2020/11/13/jaan-tallinn-ai-safety-bitcoin-cryptocurrency-elon-musk/\">estimated $900 million</a>, this might be just another case of a super-wealthy tech guy dismissing or minimizing threats that probably won’t directly harm <em>him personally</em>. Despite being <a href=\"https://www.cnbc.com/2021/01/26/oxfam-report-the-global-wealthy-are-main-drivers-of-climate-change.html\">disproportionately responsible</a> for the climate catastrophe, the <a href=\"https://www.technologyreview.com/2018/03/01/144958/if-youre-so-smart-why-arent-you-rich-turns-out-its-just-chance/\">super-rich</a> will be the least affected by it. Peter Thiel—the libertarian who voted for a <a href=\"https://www.politifact.com/factchecks/2016/jun/03/hillary-clinton/yes-donald-trump-did-call-climate-change-chinese-h/\">climate-denier</a> in 2016—has his “<a href=\"https://www.theguardian.com/news/2018/feb/15/why-silicon-valley-billionaires-are-prepping-for-the-apocalypse-in-new-zealand\">apocalypse retreat</a>” in New Zealand, Richard Branson owns his own <a href=\"https://www.businessinsider.com/hurricane-irma-richard-branson-private-island-bunker-2017-9\">hurricane-proof island</a>, Jeff Bezos bought some <a href=\"https://africa.businessinsider.com/finance-billionaires-are-stockpiling-land-that-could-be-used-in-the-apocalypse-heres/pmfybd4\">400,000 acres</a> in Texas, and Elon Musk wants to <a href=\"https://www.extremetech.com/extreme/318959-elon-musk-richest-man-mars-colony\">move to Mars</a>. Astoundingly, Reid Hoffman, the multi-billionaire who cofounded LinkedIn, <a href=\"https://www.imd.org/research-knowledge/articles/what-techs-survivalist-billionaires-should-be-doing-instead/\">reports</a> that “more than 50 percent of Silicon Valley’s billionaires have bought some level of ‘apocalypse insurance,’ such as an underground bunker.”</p> \n  <p>That’s one possibility, for sure. But I think there’s a deeper reason for Tallinn’s comments. It concerns an increasingly influential moral worldview called <em>longtermism</em>. This has roots in the work of philosopher Nick Bostrom, who <a href=\"https://nickbostrom.com/existential/risks.html\">coined</a> the term “existential risk” in 2002 and, three years later, founded the Future of Humanity Institute (FHI) based at the University of Oxford, which has received large sums of money from both <a href=\"https://fortune.com/2020/11/13/jaan-tallinn-ai-safety-bitcoin-cryptocurrency-elon-musk/\">Tallinn</a> and <a href=\"https://www.fhi.ox.ac.uk/elon-musk-funds-oxford-and-cambridge-university-research-on-safe-and-beneficial-artificial-intelligence/\">Musk</a>. Over the past decade, “longtermism” has become one of the main ideas promoted by the “<a href=\"https://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai\">Effective</a> <a href=\"https://www.lrb.co.uk/the-paper/v37/n18/amia-srinivasan/stop-the-robot-apocalypse\">Altruism</a>” (EA) movement, which generated controversy in the past for encouraging young people to work for <a href=\"https://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai\">Wall Street</a> and <a href=\"https://assets.ctfassets.net/es8pp29e1wp8/4C7WHsxZLWeaQgiIksMS0y/73404620d8c355f94f7d4d1130e967e8/Gabriel_published_14_April_3.pdf\">petrochemical companies</a> in order to donate part of their income to charity, an idea called “earn to give.” According to the longtermist Benjamin Todd, formerly at Oxford University, “<a href=\"https://80000hours.org/articles/future-generations/\">longtermism</a> might well turn out to be one of the most important discoveries of effective altruism&nbsp;so far.”</p> \n  <p>Longtermism should not be confused with “long-term thinking.” It goes <em>way beyond</em> the observation that our society is dangerously myopic, and that we should care about future generations no less than present ones. At the heart of this worldview, as delineated by Bostrom, is the idea that what <em>matters most</em> is for “Earth-originating intelligent life” to fulfill its <em>potential</em> in the cosmos. What exactly is “our potential”? As I have <a href=\"https://c8df8822-f112-4676-8332-ad89713358e3.filesusr.com/ugd/d9aaad_89094654cf0945738f5633b5d46653fd.pdf\">noted elsewhere</a>, it <a href=\"https://www.existential-risk.org/concept.html\">involves</a> subjugating nature, maximizing economic productivity, replacing humanity with a superior “posthuman” species, colonizing the universe, and ultimately creating an unfathomably huge population of conscious beings living what Bostrom <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522rich+and+happy+lives%2522+%2522nick+bostrom%2522\">describes</a> as “rich and happy lives” inside high-resolution computer simulations.</p> \n  <p>This is what “our potential” consists of, and it constitutes the ultimate aim toward which humanity as a whole, and each of us as <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">individuals</a>, are morally obligated to strive. An <em>existential risk</em>, then, is any event that would destroy this “<a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522vast+and+glorious%2522+%2522the+precipice%2522\">vast and glorious</a>” potential, as Toby Ord, a philosopher at the Future of Humanity Institute, <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522destruction+of+our+longterm+potential%2522+%2522toby+ord%2522\">writes</a> in his 2020 book <em>The Precipice</em>, which draws heavily <a href=\"https://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/ref=sr_1_1?dchild=1&amp;keywords=global+catastrophic+risks&amp;qid=1626689935&amp;sr=8-1\">from</a> <a href=\"https://www.amazon.com/Here-Be-Dragons-Technology-Humanity-ebook/dp/B018ZK17UI/ref=sr_1_1?dchild=1&amp;keywords=here+be+dragons+haggstrom&amp;qid=1626609967&amp;sr=8-1\">earlier</a> <a href=\"https://www.amazon.com/Morality-Foresight-Human-Flourishing-Introduction/dp/1634311426/ref=sr_1_15?dchild=1&amp;keywords=morality+foresight&amp;qid=1626609986&amp;sr=8-15\">work</a> in outlining the longtermist paradigm. (Note that Noam Chomsky just published a <a href=\"https://www.amazon.com/Precipice-Neoliberalism-Pandemic-Urgent-Radical/dp/164259458X/ref=sr_1_1?dchild=1&amp;keywords=the+precipice&amp;qid=1625949590&amp;sr=8-1\">book</a> also titled <em>The Precipice</em>.)</p> \n  <p>The point is that when one takes the cosmic view, it becomes clear that our civilization could persist for an <em>incredibly long time</em> and there could come to be an <em>unfathomably large number</em> of people in the future. Longtermists thus reason that the far future could contain <em>way more value</em> than exists today, or has existed so far in human history, which stretches back some 300,000 years. So, imagine a situation in which you could either lift 1 billion present people out of extreme poverty <em>or</em> benefit 0.00000000001 percent of the 10<sup>23</sup> biological humans who Bostrom <a href=\"https://www.nickbostrom.com/astronomical/waste.html\">calculates</a> could exist if we were to colonize our cosmic neighborhood, the Virgo Supercluster. Which option should you pick? For longtermists, the answer is obvious: you should pick the latter. Why? Well, just crunch the numbers: 0.00000000001 percent of 10<sup>23</sup> people is 10 billion people, which is <em>ten times greater</em> than 1 billion people. This means that if you want to do the <em>most good</em>, you should focus on these far-future people rather than on helping those in extreme poverty today. As the FHI longtermists Hilary Greaves and Will MacAskill—the latter of whom is <a href=\"https://www.simonknutsson.com/problems-in-effective-altruism-and-existential-risk-and-what-to-do-about-them/%23Potentially_dishonest_self-promotion\">said to have</a> cofounded the Effective Altruism movement with Toby Ord—<a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/2019/Greaves_MacAskill_The_Case_for_Strong_Longtermism.pdf\">write</a>, “for the purposes of evaluating actions, we can in the first instance often <em>simply ignore</em> all the effects contained in the first 100 (or even 1,000) years, focussing primarily on the further-future effects. Short-run effects act as little more than tie-breakers.”</p> \n  <hr class=\"wp-block-separator\"> \n  <p>This brings us back to climate change, which is expected to cause serious harms over precisely this time period: the next few decades and centuries. If what matters most is the very<em> far future</em>—thousands, millions, billions, and trillions of years from now—then climate change isn’t going to be high up on the list of global priorities <em>unless there’s a runaway scenario</em>. Sure, it will cause “untold suffering,” but think about the situation from the point of view of the universe itself. Whatever traumas and miseries, deaths and destruction, happen this century will <em>pale in comparison</em> to the astronomical amounts of “value” that could exist once humanity has colonized the universe, become posthuman, and created upwards of 10<sup>58</sup> (Bostrom’s later <a href=\"https://www.google.com/books/edition/Superintelligence/7_H8AwAAQBAJ?hl=en&amp;gbpv=1&amp;dq=%22human+lives+could+be+created+in+emulation+even+with+quite+conservative%22&amp;pg=PA103&amp;printsec=frontcover\">estimate</a>) conscious beings in computer simulations. Bostrom <a href=\"https://www.nickbostrom.com/papers/future.html\">makes this point</a> in terms of economic growth, which he and <a href=\"https://forum.effectivealtruism.org/posts/sFj7EstDYacf6GJWF/q-and-a-with-will-macaskill\">other longtermists</a> see as integral to fulfilling “our potential” in the universe:</p> \n  <blockquote class=\"wp-block-quote\"> \n   <p>“<em>In absolute terms, [non-runaway climate change] would be a huge harm. Yet over the course of the twentieth century, world GDP grew by some 3,700%, and per capita world GDP rose by some 860%. It seems safe to say that … whatever negative economic effects global warming will have, they will be completely swamped by other factors that will influence economic growth rates in this century.”</em></p> \n  </blockquote> \n  <p>In the same paper, Bostrom <a href=\"https://www.nickbostrom.com/papers/future.html\">declares</a> that even “a non-existential disaster causing the breakdown of global civilization is, from the perspective of humanity as a whole, a potentially recoverable setback,” describing this as “a giant massacre for man, a small misstep for mankind.” That’s of course cold comfort for those in the crosshairs of climate change—the residents of the Maldives who will <a href=\"https://www.cnbc.com/2021/05/19/maldives-calls-for-urgent-action-to-end-climate-change-sea-level-rise.html%23:~:text=Capital%2520Connection-,The%2520Maldives%2520could%2520disappear%2520by%2520the%2520end%2520of%2520the%2520century,environment,%2520climate%2520change%2520and%2520technology.%26text=and%2520the%2520sea.%25E2%2580%259D-,The%2520World%2520Economic%2520Forum%2520has%2520estimated%2520that%2520by%25202050,%252080,be%2520impacted%2520by%2520climate%2520change.\">lose their homeland</a>, the South Asians facing <a href=\"https://news.mit.edu/2017/deadly-heat-waves-could-hit-south-asia-century-0802\">lethal heat waves</a> above the 95-degree F wet-bulb threshold of survivability, and the 18 million people in Bangladesh who <a href=\"https://ejfoundation.org/reports/climate-displacement-in-bangladesh%23:~:text=Climate%2520Change%2520in%2520Bangladesh,exceptionally%2520vulnerable%2520to%2520climate%2520change.%26text=It%2520has%2520been%2520estimated%2520that,of%2520sea%2520level%2520rise%2520alone.\">may be displaced</a> by 2050. But, once again, when these losses are juxtaposed with the apparent immensity of our longterm “potential,” this suffering will hardly be a footnote to a footnote within humanity’s epic biography.</p> \n  <p>These aren’t the only incendiary remarks from Bostrom, the Father of Longtermism. In a paper that founded one half of longtermist research program, he <a href=\"https://nickbostrom.com/existential/risks.html\">characterizes</a> the most devastating disasters throughout human history, such as the two World Wars (including the Holocaust), Black Death, 1918 Spanish flu pandemic, major earthquakes, large volcanic eruptions, and so on, as “mere ripples” when viewed from “the perspective of humankind as a whole.” As he writes:&nbsp;</p> \n  <blockquote class=\"wp-block-quote\"> \n   <p><em>“Tragic as such events are to the people immediately affected, in the big picture of things … even the worst of these catastrophes are mere ripples on the surface of the great sea of life.”&nbsp;</em></p> \n  </blockquote> \n  <p>In other words, 40 million civilian deaths during WWII was awful, we can all agree about that. But think about this in terms of the 10<sup>58</sup> simulated people who could someday exist in computer simulations if we colonize space. It would require <em>trillions and trillions and trillions</em> of WWIIs one after another to even <em>approach</em> the loss of these unborn people if an existential catastrophe were to happen. This is the case even on the lower estimates of how many future people there could be. Take Greaves and MacAskill’s <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">figure</a> of 10<sup>18</sup> expected biological and digital beings on Earth alone (meaning that we don’t colonize space). That’s still a <em>way</em> bigger number than 40 million—analogous to a single grain of sand next to Mount Everest.</p> \n  <p>It’s this line of reasoning that leads Bostrom, Greaves, MacAskill, and others to argue that even the <em>tiniest</em> reductions in “existential risk” are <em>morally equivalent</em> to saving the lives of literally <em>billions</em> of living, breathing, actual people. For example, Bostrom <a href=\"https://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai\">writes</a> that if there is “a mere 1 percent chance” that 10<sup>54</sup> conscious beings (most living in computer simulations) come to exist in the future, then “we find that the expected value of reducing existential risk by a mere <em>one billionth of one billionth of one percentage point </em>is worth a hundred billion times as much as a billion human lives.” Greaves and MacAskill <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">echo this idea</a> in a 2021 paper by arguing that “even if there are ‘only’ 10<sup>14</sup> lives to come … , a reduction in near-term risk of extinction by one millionth of one percentage point would be equivalent in value to a million lives saved.”</p> \n  <p>To make this concrete, imagine Greaves and MacAskill in front of two buttons. If pushed, the first would save the lives of 1 million living, breathing, actual people. The second would increase the probability that 10<sup>14</sup> currently unborn people come into existence in the <em>far future</em> by a <em>teeny-tiny </em>amount. Because, on their longtermist view, there is no fundamental moral difference between saving <em>actual people</em> and <em>bringing new people</em> into existence, these options are <em>morally equivalent</em>. In other words, they’d have to flip a coin to decide which button to push. (Would you? I certainly hope not.) In Bostrom’s example, the morally right thing is <em>obviously</em> to sacrifice billions of living human beings for the sake of even <em>tinier</em> reductions in existential risk, assuming a minuscule 1 percent chance of a <em>larger</em> future population: 10<sup>54</sup> people.</p> \n  <p>All of this is to say that even if billions of people were to perish in the coming climate catastrophe, so long as humanity survives with enough of civilization intact to fulfill its supposed “potential,” we shouldn’t be <em>too</em> concerned. In the grand scheme of things, non-runaway climate change will prove to be nothing more than a “mere ripple” —a “small misstep for mankind,” however terrible a “massacre for man” it might otherwise be.</p> \n  <p>Even worse, since our resources for reducing existential risk are finite, Bostrom <a href=\"https://www.existential-risk.org/concept.html%23:~:text=An%2520existential%2520risk%2520is%2520one,future%2520development%2520(Bostrom%25202002).\">argues</a> that we must not “fritter [them] away” on what he describes as “feel-good projects of suboptimal efficacy.” Such projects would include, on this account, not just saving people in the Global South—those most vulnerable, especially <a href=\"https://www.bbc.com/news/science-environment-43294221\">women</a>—from the calamities of climate change, but <em>all other</em> non-existential philanthropic causes, too. As the Princeton philosopher Peter Singer <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522to+refer+to+donating+to+help+the+global+poor+or+reduce+animal+suffering+as+a+%25E2%2580%2598feel-good+project%25E2%2580%2599+on+which+resources+are+%25E2%2580%2598frittered+away%25E2%2580%2599+is+harsh+language%2522\">writes</a> about Bostrom in his 2015 book on Effective Altruism, “to refer to donating to help the global poor … as a ‘feel-good project’ on which resources are ‘frittered away’ is harsh language.” But it makes perfectly good sense within Bostrom’s longtermist framework, <a href=\"https://www.nickbostrom.com/astronomical/waste.html\">according to which</a> “priority number one, two, three, and four should … be to reduce existential risk.” Everything else is smaller fish not worth frying.</p> \n  <hr class=\"wp-block-separator\"> \n  <p>If this sounds appalling, it’s because it <em>is </em>appalling. By reducing morality to an abstract <a href=\"https://users.ox.ac.uk/~mert2255/papers/mu-about-pe.pdf\">numbers game</a>, and by declaring that what’s <a href=\"https://books.google.de/books?hl=en&amp;lr=&amp;id=iPerDwAAQBAJ&amp;oi=fnd&amp;pg=PA80&amp;dq=overwhelming+importance+far+future+beckstead&amp;ots=fJHd_shkvv&amp;sig=BJ3_Xf-5XpwWmTcz5RvPTEJKOcg&amp;redir_esc=y%23v=onepage&amp;q&amp;f=false\">most important</a> is fulfilling “our potential” by becoming simulated posthumans among the stars, longtermists not only trivialize past atrocities like WWII (and the Holocaust) but give themselves a “moral excuse” to dismiss or minimize comparable atrocities in the future. This is one reason that I’ve come to see longtermism as an <em>immensely dangerous ideology</em>. It is, indeed, akin to a <em>secular religion</em> built around the worship of “future value,” complete with its own “secularised doctrine of&nbsp;salvation,” as the Future of Humanity Institute historian Thomas Moynihan approvingly <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522secularised+doctrine+of+salvation%2522\">writes</a> in his book <em>X-Risk</em>. The popularity of this religion among wealthy people in the West—especially the socioeconomic elite—makes sense because it tells them exactly what they want to hear: not only are you <em>ethically excused</em> from worrying too much about sub-existential threats like non-runaway climate change and global poverty, but you are actually a <em>morally better person</em> for focusing instead on more important things—risk that could permanently destroy “our potential” as a species of Earth-originating intelligent life.</p> \n  <p>To drive home the point, consider an argument from the longtermist Nick Beckstead, who has <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/miscellaneous/future-humanity-institute-work-on-global-catastrophic-risks\">overseen</a> tens of millions of dollars in funding for the Future of Humanity Institute. Since shaping the far future “over the coming millions, billions, and trillions of years” is of “overwhelming importance,” he <a href=\"https://www.proquest.com/docview/1442191960?pq-origsite=gscholar&amp;fromopenview=true\">claims</a>, we should actually care more about people in rich countries than poor countries. This comes from a 2013 PhD dissertation that Ord <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522one+of+the+best+texts+on+existential+risk%2522\">describes</a> as “one of the best texts on existential risk,” and it’s cited on numerous Effective Altruist <a href=\"https://concepts.effectivealtruism.org/concepts/the-long-term-future/\">websites</a>, including some hosted by the Centre for Effective Altruism, which shares office space in Oxford with the Future of Humanity Institute. The passage is worth quoting in full:</p> \n  <blockquote class=\"wp-block-quote\"> \n   <p>“<em>Saving lives in poor countries may have significantly smaller ripple effects than saving and improving lives in rich countries. Why? Richer countries have substantially more innovation, and their workers are much more economically productive. By ordinary standards—at least by ordinary enlightened humanitarian standards—saving and improving lives in rich countries is about equally as important as saving and improving lives in poor countries, provided lives are improved by roughly comparable amounts. But it now seems more plausible to me that saving a life in a rich country is substantially more important than saving a life in a poor country, other things being equal.”</em></p> \n  </blockquote> \n  <p>Never mind the fact that many countries in the Global South are relatively poor precisely because of the long and sordid histories of Western colonialism, imperialism, exploitation, political meddling, pollution, and so on. What hangs in the balance is astronomical amounts of “value.” What shouldn’t we do to achieve this magnificent end? Why <em>not</em> prioritize lives in rich countries over those in poor countries, even if gross historical injustices remain inadequately addressed? Beckstead isn’t the only longtermist who’s explicitly endorsed this view, either. As Hilary Greaves <a href=\"https://youtu.be/d1jMlb8E08k?t=148\">states</a> in a 2020 interview with Theron Pummer, who co-edited the book <em>Effective Altruism</em> with her, if one’s “aim is doing the most good, improving the world by the most that I can,” then although “there’s a clear place for transferring resources from the affluent Western world to the global poor … <em>longtermist thought</em> suggests that something else may be <em>better still.”</em></p> \n  <hr class=\"wp-block-separator\"> \n  <p>Returning to climate change once again, we can see how Tallinn got the idea that our environmental impact probably isn’t existentially risky from academic longtermists like Bostrom. As alluded to above, Bostrom <a href=\"https://nickbostrom.com/existential/risks.html\">maintains</a> that non-runaway (which he calls “moderate”) global warming, as well as “threats to the biodiversity of Earth’s ecosphere,” as “endurable” rather than “terminal” for humanity. Similarly, Ord <a href=\"https://www.google.com/search?tbm=bks&amp;q=%25221+in+10000%2522+%2522the+precipice%2522+%25221+in+10%2522\">claims</a> in <em>The Precipice </em>that climate change poses a mere 1-in-1,000 chance of existential catastrophe, in contrast to a far greater 1-in-10 chance of catastrophe involving superintelligent machines (dubbed the “Robopocalypse” by <a href=\"https://www.google.com/search?tbm=bks&amp;q=%25E2%2580%259Ca+disaster+sometimes+called+the+Robopocalypse+and+commonly+illustrated+with+stills+from+the+Terminator+movies.%25E2%2580%259D\">some</a>). Although, like Bostrom, Ord acknowledges that the climate crisis could get very bad, he <a href=\"https://youtu.be/R9EtiNmYnQQ?t=510\">assures</a> us that “the typical scenarios of climate change would not destroy our potential.”</p> \n  <p>Within the billionaire world, these conclusions have been parroted by some of the most powerful men on the planet today (not just Tallinn). For example, Musk, an admirer of Bostrom’s who donated $10 million in 2015 to the Future of Life Institute, another longtermist organization that Tallinn cofounded, <a href=\"https://www.youtube.com/watch?v=Jvx_XIihmXs\">said</a> in an interview this year that his “concern with the CO2 is not kind of where we are today or even … the current rate of carbon generation.” Rather, the worry is that “if carbon generation keeps accelerating and … if we’re complacent then I think … there’s some risk of sort of non-linear climate change”—meaning, one surmises, a runaway scenario. Peter Thiel has also apparently held this view for some time, which is unsurprising given his history with longtermist thinking and the Effective Altruism movement. (He gave the <a href=\"https://www.youtube.com/watch?v=h8KkXcBwHec&amp;t=1439s\">keynote address</a> at the 2013 Effective Altruism Summit.) But Thiel also <a href=\"https://www.ft.com/content/abc942cc-5fb3-11e4-8c27-00144feabdc0\">declared</a> in 2014: “People are spending way too much time thinking about climate change” and “way too little thinking about AI.”</p> \n  <p>The reference to AI, or “artificial intelligence,” here is important. Not only do many longtermists believe that <a href=\"https://www.currentaffairs.org/2020/07/the-singularity-prophets\">superintelligent machines</a> pose the greatest single hazard to human survival, but they seem convinced that if humanity were to create a “friendly” superintelligence whose goals are properly “aligned” with our “human goals,” then a new Utopian age of unprecedented security and flourishing would suddenly commence. This eschatological vision is sometimes associated with the “Singularity,” made famous by futurists like Ray Kurzweil, which critics have facetiously dubbed the “<a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522Since+lucky+humans+will+at+that+point+merge+with+superintelligence+or+become+superintelligent,+some+refer+to+the+Singularity+as+the+'Techno-rapture',+pointing+out+the+similarity+ofthe+narrative+to+the+Christian+Rapture%2522\">techno-rapture</a>” or “<a href=\"https://git.jrtechs.net/jrtechs/FOSSRIT-hfoss/raw/commit/6a8f78c0dc52892458a4f12d2e322a1d2e1df6ac/static/books/Cory_Doctorow_and_Charles_Stross_-_Rapture_of_the_Nerds.pdf\">rapture of the nerds</a>” because of its obvious similarities to the Christian dispensationalist notion of the Rapture, when Jesus will swoop down to gather every believer on Earth and carry them back to heaven. As Bostrom <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522would+also+eliminate+or+reduce+many+anthropogenic+risks%2522\">writes</a> in his <a href=\"https://twitter.com/elonmusk/status/495759307346952192?lang=en\">Musk-endorsed</a> book <em>Superintelligence</em>, not only would the various existential risks posed by nature, such as asteroid impacts and supervolcanic eruptions, “be virtually eliminated,” but a friendly superintelligence “would also eliminate or reduce many anthropogenic risks” like climate change. “One might believe,” he <a href=\"https://www.nickbostrom.com/papers/future.html\">writes</a> elsewhere, that “the new civilization would [thus] have vastly improved survival prospects since it would be guided by superintelligent foresight and planning.”</p> \n  <p>Tallinn makes the same point during a Future of Life Institute podcast recorded this year. Whereas a runaway climate scenario is at best many decades away, if it could happen at all, Tallinn <a href=\"https://futureoflife.org/2021/04/20/jaan-tallinn-on-avoiding-civilizational-pitfalls-and-surviving-the-21st-century/\">speculates</a> that superintelligence will present “an existential risk in the next 10 or 50 years.” Thus, he says, “if you’re going to really get AI right [by making it ‘friendly’], it seems like all the other risks [that we might face] become much more manageable.” This is about as literal an interpretation of “<em>deus ex machina</em>” as one can get, and in my experience as someone who spent several months as a visiting scholar at the Centre for the Study of Existential Risk, which was cofounded by Tallinn, it’s a widely-held view among longtermists. In fact, Greaves and MacAskill <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">estimate</a> that every $100 spent on creating a “friendly” superintelligence would be morally equivalent to “saving one trillion [actual human] lives,” assuming that an additional 10<sup>24</sup> people could come to exist in the far future. Hence, they point out that focusing on superintelligence gets you a <em>way bigger</em> bang for your buck than, say, preventing people who exist right now from contracting malaria by distributing <a href=\"https://en.wikipedia.org/wiki/Mosquito_net\">mosquito nets</a>.</p> \n  <hr class=\"wp-block-separator\"> \n  <p>What I find most unsettling about the longtermist ideology isn’t just that it contains all the ingredients necessary for a <a href=\"https://www.google.com/search?q=haggstrom+%22you+must+be+willing+to+break+a+few+eggs,%E2%80%9D+which+has+typically+been+used+to+explain+that+a%22&amp;tbm=bks&amp;sxsrf=ALeKk00ecCa6cZZNkMrgVBD_IttGPwKLVg:1627464586070&amp;ei=iiMBYbXnA4S3gwfippaQBw&amp;oq=haggstrom+%22you+must+be+willing+to+break+a+few+eggs,%E2%80%9D+which+has+typically+been+used+to+explain+that+a%22&amp;gs_l=psy-ab.3...7009.7400.0.7668.3.3.0.0.0.0.144.247.0j2.2.0....0...1c.1j2.64.psy-ab..1.0.0....0.8Fa7eTgyzKA\">genocidal catastrophe</a> in the name of realizing astronomical amounts of far-future “value.” Nor is it that this religious ideology has already infiltrated the consciousness of powerful actors who could, <a href=\"https://www.globalcitizen.org/en/content/billionaires-bezos-branson-musk-space-world-hunger/\">for example</a>, “save 41 [million] people at risk of starvation” but instead use their wealth to <a href=\"https://www.theguardian.com/science/2021/jul/19/billionaires-space-tourism-environment-emissions\">fly themselves to space</a>. Even more chilling is that many people in the community believe that their <a href=\"https://www.google.com/search?q=%2522The+challenge+of+our+time+is+to+preserve+our+vast+potential,+and+to+protect+it+against+the+risk+of+future+destruction%2522&amp;tbm=bks&amp;sxsrf=ALeKk030oOJpgYZ9W7TnVS0jbg_cwv1mhQ:1626553426870&amp;ei=UjzzYInJNIm4sAfB24f4CA&amp;oq=%2522The+challenge+of+our+time+is+to+preserve+our+vast+potential,+and+to+protect+it+against+the+risk+of+future+destruction%2522&amp;gs_l=psy-ab.3...5781.5781.0.6014.1.1.0.0.0.0.0.0..0.0....0...1c.1.64.psy-ab..1.0.0....0.JjOFq9K0BOY\">mission to</a> “protect” and “preserve” humanity’s “longterm potential” is <em>so important</em> that they have little tolerance for dissenters. These include critics who might suggest that longtermism is dangerous, or that it supports what Frances Lee Ansley calls <a href=\"https://www.theatlantic.com/politics/archive/2017/10/the-language-of-white-supremacy/542148/\">white supremacy</a> (given the implication, outlined and defended by Beckstead, that we should prioritize the lives of people in rich countries). When one believes that <em>existential risk</em> is the most important concept ever invented, as someone at the Future of Humanity Institute once told me, and that failing to realize “our potential” would not merely be <em>wrong</em> but a moral catastrophe of literally <em>cosmic proportions</em>, one will naturally be inclined to react strongly against those who criticize this sacred dogma. When you believe the stakes are that high, you may be quite willing to use extraordinary means to stop anyone who stands in your way.&nbsp;</p> \n  <section class=\"essay-block wp-block-currentaffairs-group\" data-essay-block-type=\"excerpt\"> \n   <blockquote class=\"wp-block-quote\"> \n    <p><em>By reducing morality to an abstract numbers game, and by declaring that what’s most important is fulfilling “our potential” by becoming simulated posthumans among the stars, longtermists not only trivialize past atrocities like WWII (and the Holocaust) but give themselves a “moral excuse” to dismiss or minimize comparable atrocities in the future.</em></p> \n   </blockquote> \n  </section> \n  <p>In fact, numerous people have come forward, both publicly and privately, over the past few years with stories of being intimidated, silenced, or “canceled.” (Yes, “cancel culture” is a real problem here.) I personally have had three colleagues back out of collaborations with me after I self-published a <a href=\"https://c8df8822-f112-4676-8332-ad89713358e3.filesusr.com/ugd/d9aaad_89094654cf0945738f5633b5d46653fd.pdf\">short critique of longtermism</a>, not because they wanted to, but because they were pressured to do so from longtermists in the community. Others have expressed worries about the personal repercussions of openly criticizing Effective Altruism or the longtermist ideology. For example, the moral philosopher Simon Knutsson wrote a <a href=\"https://www.simonknutsson.com/problems-in-effective-altruism-and-existential-risk-and-what-to-do-about-them/\">critique</a> several years ago in which he notes, among other things, that Bostrom appears to have repeatedly misrepresented his academic achievements in claiming that, as he <a href=\"https://web.archive.org/web/20060701201235/https://nickbostrom.com/\">wrote on his website</a> in 2006, “my performance as an undergraduate set a national record in Sweden.” (There is no evidence that this is true.) The point is that, after doing this, Knutsson <a href=\"https://www.simonknutsson.com/problems-in-effective-altruism-and-existential-risk-and-what-to-do-about-them/%23Writing_this_text\">reports</a> that he became “concerned about his safety” given past efforts to censure certain ideas by longtermists with clout in the community.</p> \n  <p>This might sound hyperbolic, but it’s consistent with a pattern of questionable behavior from leaders in the Effective Altruism movement more generally. For example, one of the first people to become an Effective Altruist after the movement was born circa 2009, Simon Jenkins, reports an incident in which he criticized an idea within Effective Altruism on a Facebook group run by the community. Within an hour, not only had his post been deleted but someone who works for the Centre for Effective Altruism actually <em>called his personal phone</em> to instruct him not to question the movement. “We can’t have people posting anything that suggests that Giving What We Can [an organization founded by Ord] is bad,” as Jenkins recalls. These are just a few of several dozen stories that people have shared with me after I went public with some of my own unnerving experiences.</p> \n  <p>All of this is to say that I’m not especially optimistic about convincing longtermists that their obsession with our “vast and glorious” potential (<a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522vast+and+glorious%2522+%2522the+precipice%2522\">quoting</a> Ord again) could have profoundly harmful consequences if it were to guide actual policy in the world. As the Swedish scholar Olle Häggström has <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522It+is+simply+too+reminiscent+of+the+old+saying+%25E2%2580%259CIf+you+want+to+make+an+omelet,+you+must+be+willing+to+break+a+few+eggs,%25E2%2580%259D+which+has+typically+been+used+to+explain+that+a+bit+of+genocide+or+so+might+be+a+good+thing,+if+it+can+con-+tribute+to+the+goal+of+creating+a+future+utopia.%2522\">disquietingly noted</a>, if political leaders were to take seriously the claim that saving billions of living, breathing, actual people today is morally equivalent to <em>negligible</em> reductions in existential risk, who knows what atrocities this might excuse? If the ends justify the means, and the “end” in this case is a veritable techno-Utopian playground full of 10<sup>58</sup> simulated posthumans awash in “the pulsing ecstasy of love,” as Bostrom writes in his grandiloquent “<a href=\"https://www.nickbostrom.com/utopia.html\">Letter from Utopia</a>,” would <em>any</em> means be off-limits? While <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">some</a> longtermists have recently suggested that there should be constraints on which actions we can take for the far future, others like Bostrom have literally argued that <a href=\"https://nickbostrom.com/existential/risks.html\">preemptive violence</a> and even a <a href=\"https://www.nickbostrom.com/papers/vulnerable.pdf\">global surveillance system</a> should remain options for ensuring the realization of “our potential.” It’s not difficult to see how this way of thinking could have genocidally catastrophic consequences if political actors were to “[take] Bostrom’s argument to heart,” in Häggström’s <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522if+the+president+has+taken+Bostrom's+argument+to+heart%2522\">words</a>.</p> \n  <hr class=\"wp-block-separator\"> \n  <p>I should emphasize that rejecting longtermism does <em>not</em> mean that one must reject <em>long-term thinking</em>. You <em>ought to </em>care equally about people no matter when they exist, whether today, next year, or in a couple billion years henceforth. If we shouldn’t discriminate against people based on their spatial distance from us, we shouldn’t discriminate against them based on their temporal distance, either. Many of the problems we face today, such as climate change, will have devastating consequences for future generations hundreds or thousands of years in the future. That should matter. We should be willing to make sacrifices for their wellbeing, just as we make sacrifices for those alive today by donating to charities that fight global poverty. But this does not mean that one must genuflect before the altar of “future value” or “our potential,” understood in techno-Utopian terms of colonizing space, becoming posthuman, subjugating the natural world, maximizing economic productivity, and creating massive computer simulations stuffed with 10<sup>45</sup> digital beings (on Greaves and MacAskill’s <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">estimate</a> if we were to colonize the Milky Way).</p> \n  <p>Care about the long term, I like to say, but don’t be a<em> longtermist.</em> Superintelligent machines aren’t going to save us, and climate change <em>really should</em> be one of our top global priorities, <em>whether or not</em> it prevents us from becoming simulated posthumans in cosmic computers.</p> \n  <p>Although a handful of longtermists have recently written that the Effective Altruism movement should take climate change more seriously, among the main reasons given for doing so is that, to <a href=\"https://forum.effectivealtruism.org/posts/BwDAN9pGbmCYZGbgf/does-climate-change-deserve-more-attention-within-ea\">quote</a> an employee at the Centre for Effective Altruism, “by failing to show a sufficient appreciation of the severity of climate change, EA may risk losing credibility and alienating potential effective altruists.” In other words, community members should talk more about climate change not because of moral considerations relating to climate justice, the harms it will cause to poor people, and so on, but for <em>marketing reasons</em>. It would be “bad for business” if the public were to associate a dismissive attitude about climate change with Effective Altruism and its longtermist offshoot. As the same author reiterates later on, “I agree [with Bostrom, Ord, etc.] that it is much more important to work on x-risk … , but I wonder whether we are alienating potential EAs by not grappling with this issue.”</p> \n  <p>Yet even if longtermists were to come around to “caring” about climate change, this wouldn’t mean much if it were for the wrong reasons. Knutsson says:</p> \n  <blockquote class=\"wp-block-quote\"> \n   <p><em>“Like politicians, one cannot simply and naively assume that these people are being honest about their views, wishes, and what they would do. In the Effective Altruism and existential risk areas, some people seem super-strategic and willing to say whatever will achieve their goals, regardless of whether they believe the claims they make—even more so than in my experience of party politics.”&nbsp;</em></p> \n  </blockquote> \n  <p>Either way, the damage may already have been done, given that averting “untold suffering” from climate change will require <em>immediate action </em>from the Global North. Meanwhile, millionaires and billionaires under the influence of longtermist thinking are focused instead on superintelligent machines that they believe will magically solve the mess that, in <a href=\"https://www.oxfam.org/en/press-releases/carbon-emissions-richest-1-percent-more-double-emissions-poorest-half-humanity\">large part</a>, they themselves have created.</p> \n </section> \n</div></span>\n      </div>\n\n    </article>\n\n    <center>\n      <aside class=\"article-petition\">\n        <div class=\"\">\n          <div class=\"text\">\n\n            <div class=\"hs-cta-embed hs-cta-simple-placeholder hs-cta-embed-172753959542\" style=\"max-width:100%; max-height:100%; width:100%;height:303.927557px\" data-hubspot-wrapper-cta-id=\"172753959542\">\n              <a href=\"https://cta-service-cms2.hubspot.com/web-interactives/public/v1/track/redirect?encryptedPayload=AVxigLK%2BXThl%2Fyat45iwbDV3tF%2BCeZwFCtLvJFPaUIeHAlfK1rQX2QKRhYc58n40DyuL8hRhHdh8x%2FZgT4BuCrlEaqal4CfpaRvJm659gYTLRz1aa0xk93SQ1gAH2dotQmZWOPW26rGXFBRdKYmLGCqr8dSCzkZX0zkSsrveCHx2HGuqtudVqTwPk0E%3D&amp;webInteractiveContentId=172753959542&amp;portalId=43971025\" target=\"_blank\" rel=\"noopener\" crossorigin=\"anonymous\">\n                <img alt=\"WILL YOU SUPPORT CURRENT AFFAIRS AS WE BUILD INDEPENDENT MEDIA? &nbsp; &nbsp; Independent media is needed now more than ever, and our work is entirely made possible by our readers. We don’t have any ads. We’re a 501c3 nonprofit without any corporate sponsors. We just have a committed base of subscribers and donors who understand the importance of the work we do. The money we raise goes to paying writers, editors, artists, and researchers to produce excellent work. Please consider purchasing a subscription or making a donation today. &nbsp;\" loading=\"lazy\" src=\"https://no-cache.hubspot.com/cta/default/43971025/interactive-172753959542.png\" style=\"height: 100%; width: 100%; object-fit: fill\" onerror=\"this.style.display='none'\">\n              </a>\n            </div>\n          </div>\n        </div>\n      </aside>\n    </center>\n\n    <aside class=\"article-afterword\">\n      <div class=\"bound\">\n        <figure class=\"article-biography\" data-biography-type=\"writer\">\n          <p></p>\n          <figcaption><a href=\"https://www.currentaffairs.org/news/author/émile-p-torres\" rel=\"author\">More from Émile P. Torres</a></figcaption>\n        </figure>\n      </div>\n    </aside>\n\n  </div>\n\n\n  \n  \n   <!-- Counter for displayed posts -->\n\n  <section class=\"articles-roundup\">\n    <header class=\"quadtych-titling ribbon\">\n      <div class=\"bound\">\n        <h2 class=\"\">\n          <span>More In:</span>\n          <em>Tech</em>      \n        </h2>\n      </div>\n    </header>\n\n    <div class=\"quadtych-of-articles\">\n      <div class=\"bound\">\n        <ul class=\"columns\">\n          \n          \n          \n          \n          \n           <!-- Increment displayed posts counter -->\n          <li class=\"article\" aria-label=\"Blog post summary: Surely AI Safety Legislation Is A No-Brainer\">\n            <a href=\"https://www.currentaffairs.org/news/surely-ai-safety-legislation-is-a-no-brainer\" aria-label=\"Read full post: Surely AI Safety Legislation Is A No-Brainer\">\n              <div class=\"art lazy\" data-bg=\"url(https://support.currentaffairs.org/hubfs/newsom2.jpg)\" data-was-processed=\"true\" style=\"background-image: url(https://www.currentaffairs.org/hubfs/newsom2.jpg);\">\n                <i class=\"fill\"></i>\n              </div>\n              <h1 class=\"title\">Surely AI Safety Legislation Is A No-Brainer</h1>\n              <ul class=\"bylines\">\n                <li class=\"author\">Nathan J. Robinson</li>\n              </ul> \n            </a>\n          </li>\n          \n          \n          \n           <!-- Increment displayed posts counter -->\n          <li class=\"article\" aria-label=\"Blog post summary: Why The Left Should Care About Privacy\">\n            <a href=\"https://www.currentaffairs.org/news/why-the-left-should-care-about-privacy\" aria-label=\"Read full post: Why The Left Should Care About Privacy\">\n              <div class=\"art lazy\" data-bg=\"url(https://support.currentaffairs.org/hubfs/Online%20Article%20Images/Privacy-Online-Image.jpg)\" data-was-processed=\"true\" style=\"background-image: url(https://www.currentaffairs.org/hubfs/Online%20Article%20Images/Privacy-Online-Image.jpg);\">\n                <i class=\"fill\"></i>\n              </div>\n              <h1 class=\"title\">Why The Left Should Care About Privacy</h1>\n              <ul class=\"bylines\">\n                <li class=\"author\">Lauren Fadiman</li>\n              </ul> \n            </a>\n          </li>\n          \n          \n          \n           <!-- Increment displayed posts counter -->\n          <li class=\"article\" aria-label=\"Blog post summary: The Only Ethical Model for AI is Socialism\">\n            <a href=\"https://www.currentaffairs.org/news/the-only-ethical-model-for-ai-is-socialism\" aria-label=\"Read full post: The Only Ethical Model for AI is Socialism\">\n              <div class=\"art lazy\" data-bg=\"url(https://support.currentaffairs.org/hubfs/chataisocialism.jpg)\" data-was-processed=\"true\" style=\"background-image: url(https://www.currentaffairs.org/hubfs/chataisocialism.jpg);\">\n                <i class=\"fill\"></i>\n              </div>\n              <h1 class=\"title\">The Only Ethical Model for AI is Socialism</h1>\n              <ul class=\"bylines\">\n                <li class=\"author\">Richard Eskow</li>\n              </ul> \n            </a>\n          </li>\n          \n          \n          \n           <!-- Increment displayed posts counter -->\n          <li class=\"article\" aria-label=\"Blog post summary: The Voyager Probes Were a Triumph of Collective Endeavor\">\n            <a href=\"https://www.currentaffairs.org/news/2024/05/the-voyager-probes-were-a-triumph-of-collective-endeavo\" aria-label=\"Read full post: The Voyager Probes Were a Triumph of Collective Endeavor\">\n              <div class=\"art lazy\" data-bg=\"url(https://support.currentaffairs.org/hubfs/Imported%20sitepage%20images/voyager-1024x646.jpg)\" data-was-processed=\"true\" style=\"background-image: url(https://www.currentaffairs.org/hubfs/Imported%20sitepage%20images/voyager-1024x646.jpg);\">\n                <i class=\"fill\"></i>\n              </div>\n              <h1 class=\"title\">The Voyager Probes Were a Triumph of Collective Endeavor</h1>\n              <ul class=\"bylines\">\n                <li class=\"author\">Larry Gilman</li>\n              </ul> \n            </a>\n          </li>\n          \n          \n          \n          \n          \n          \n          \n          \n          \n          \n          \n          \n          \n          \n          \n        </ul>\n      </div>\n    </div>\n  </section>\n\n\n  <div id=\"hs_cos_wrapper_module_172537045637521\" class=\"hs_cos_wrapper hs_cos_wrapper_widget hs_cos_wrapper_type_module\" style=\"\" data-hs-cos-general-type=\"widget\" data-hs-cos-type=\"module\"><section class=\"newest-issue\">\n    <div class=\"bound\">\n      <div class=\"plate lazy\" data-bg=\"url(https://support.currentaffairs.org/hubfs/images/ornament-giant-fleur.png)\" data-was-processed=\"true\" style=\"background-image: url(https://www.currentaffairs.org/hubfs/images/ornament-giant-fleur.png);\">\n        <div class=\"columns\">\n          <div class=\"art\">\n            <div class=\"art-inner\">\n              <img alt=\"Cover of latest issue of print magazine\" class=\"lazy loaded\" data-src=\"https://support.currentaffairs.org/hubfs/CA-51-Cover.png\" src=\"https://www.currentaffairs.org/hubfs/CA-51-Cover.png\" data-was-processed=\"true\">\n            </div>\n          </div>\n          <div class=\"text\">\n            <div class=\"text-inner\">\n\n              <h1>\n                <span>Announcing Our</span>\n                <mark>Newest Issue</mark>\n              </h1>\n\n              <div class=\"teaser\">\n                <div class=\"teaser\">\n                  <h2>Featuring</h2>\n                  <p></p><p>Our final issue of 2024 contains sparkling essays and incredible art. We ask: Why is suburban sprawl everywhere and can it be stopped? Should FDR be the model for the future? What can Afrofuturism do to expand our minds? Plus other questions! There's more: we invent new vegetables, teach you a new get-rich-quick scheme, reveal hidden Kamala Harris strategy documents, explore Republican Sex Ed, and interview democratic socialist NYC mayoral candidate Zohran Mamdani!</p><p></p>\n                </div>\n              </div>\n\n              <footer class=\"action\">\n\n                <a class=\"boxy\" href=\"https://shop.currentaffairs.org/products/issue-51-nov-dec-2024\">Show Me More</a>\n              </footer>\n\n            </div>\n          </div>\n        </div>\n      </div>\n    </div>\n\n  </section></div>\n\n  \n  \n   <!-- Counter for displayed posts -->\n\n  <section class=\"articles-roundup\">\n    <header class=\"quadtych-titling wreath\">\n      <div class=\"bound\">\n        <h2>\n          <span>The Latest From <em>Current Affairs</em></span>\n        </h2>\n      </div>\n    </header>\n\n    <div class=\"quadtych-of-articles\">\n      <div class=\"bound\">\n        <ul class=\"columns\">\n          \n          \n          \n          \n          <li class=\"article\">\n            <a href=\"https://www.currentaffairs.org/news/let-food-stamp-recipients-eat-whatever-the-hell-they-want\">\n              <img class=\"blog-related-posts__image\" src=\"https://www.currentaffairs.org/hs-fs/hubfs/ebt.jpg?width=352&amp;name=ebt.jpg\" loading=\"lazy\" width=\"352\" alt=\"\" srcset=\"https://www.currentaffairs.org/hs-fs/hubfs/ebt.jpg?width=176&amp;name=ebt.jpg 176w, https://www.currentaffairs.org/hs-fs/hubfs/ebt.jpg?width=352&amp;name=ebt.jpg 352w, https://www.currentaffairs.org/hs-fs/hubfs/ebt.jpg?width=528&amp;name=ebt.jpg 528w, https://www.currentaffairs.org/hs-fs/hubfs/ebt.jpg?width=704&amp;name=ebt.jpg 704w, https://www.currentaffairs.org/hs-fs/hubfs/ebt.jpg?width=880&amp;name=ebt.jpg 880w, https://www.currentaffairs.org/hs-fs/hubfs/ebt.jpg?width=1056&amp;name=ebt.jpg 1056w\" sizes=\"(max-width: 352px) 100vw, 352px\">\n\n              <h1 class=\"title\">Let Food Stamp Recipients Eat Whatever The Hell They Want</h1>\n              <ul class=\"bylines\">\n                <li class=\"author\">Nathan J. Robinson</li>\n              </ul> \n            </a>\n          </li>\n          \n          \n          \n          \n          <li class=\"article\">\n            <a href=\"https://www.currentaffairs.org/news/starbucks-is-everything-wrong-with-american-capitalism\">\n              <img class=\"blog-related-posts__image\" src=\"https://www.currentaffairs.org/hs-fs/hubfs/sbux.jpg?width=352&amp;name=sbux.jpg\" loading=\"lazy\" width=\"352\" alt=\"\" srcset=\"https://www.currentaffairs.org/hs-fs/hubfs/sbux.jpg?width=176&amp;name=sbux.jpg 176w, https://www.currentaffairs.org/hs-fs/hubfs/sbux.jpg?width=352&amp;name=sbux.jpg 352w, https://www.currentaffairs.org/hs-fs/hubfs/sbux.jpg?width=528&amp;name=sbux.jpg 528w, https://www.currentaffairs.org/hs-fs/hubfs/sbux.jpg?width=704&amp;name=sbux.jpg 704w, https://www.currentaffairs.org/hs-fs/hubfs/sbux.jpg?width=880&amp;name=sbux.jpg 880w, https://www.currentaffairs.org/hs-fs/hubfs/sbux.jpg?width=1056&amp;name=sbux.jpg 1056w\" sizes=\"(max-width: 352px) 100vw, 352px\">\n\n              <h1 class=\"title\">Starbucks Is Everything Wrong With American Capitalism</h1>\n              <ul class=\"bylines\">\n                <li class=\"author\">Nathan J. Robinson</li>\n              </ul> \n            </a>\n          </li>\n          \n          \n          \n          \n          <li class=\"article\">\n            <a href=\"https://www.currentaffairs.org/news/krystal-ball\">\n              <img class=\"blog-related-posts__image\" src=\"https://www.currentaffairs.org/hs-fs/hubfs/Online%20Article%20Images/Krystal-Ball-Online-Image-Final.jpg?width=352&amp;name=Krystal-Ball-Online-Image-Final.jpg\" loading=\"lazy\" width=\"352\" alt=\"\" srcset=\"https://www.currentaffairs.org/hs-fs/hubfs/Online%20Article%20Images/Krystal-Ball-Online-Image-Final.jpg?width=176&amp;name=Krystal-Ball-Online-Image-Final.jpg 176w, https://www.currentaffairs.org/hs-fs/hubfs/Online%20Article%20Images/Krystal-Ball-Online-Image-Final.jpg?width=352&amp;name=Krystal-Ball-Online-Image-Final.jpg 352w, https://www.currentaffairs.org/hs-fs/hubfs/Online%20Article%20Images/Krystal-Ball-Online-Image-Final.jpg?width=528&amp;name=Krystal-Ball-Online-Image-Final.jpg 528w, https://www.currentaffairs.org/hs-fs/hubfs/Online%20Article%20Images/Krystal-Ball-Online-Image-Final.jpg?width=704&amp;name=Krystal-Ball-Online-Image-Final.jpg 704w, https://www.currentaffairs.org/hs-fs/hubfs/Online%20Article%20Images/Krystal-Ball-Online-Image-Final.jpg?width=880&amp;name=Krystal-Ball-Online-Image-Final.jpg 880w, https://www.currentaffairs.org/hs-fs/hubfs/Online%20Article%20Images/Krystal-Ball-Online-Image-Final.jpg?width=1056&amp;name=Krystal-Ball-Online-Image-Final.jpg 1056w\" sizes=\"(max-width: 352px) 100vw, 352px\">\n\n              <h1 class=\"title\">Krystal Ball on How To Fix American Politics and Media</h1>\n              <ul class=\"bylines\">\n                <li class=\"author\">Current Affairs</li>\n              </ul> \n            </a>\n          </li>\n          \n          \n          \n          \n          <li class=\"article\">\n            <a href=\"https://www.currentaffairs.org/news/passing-the-torch-0\">\n              <img class=\"blog-related-posts__image\" src=\"https://www.currentaffairs.org/hs-fs/hubfs/Online%20Article%20Images/CA-51-Olympics-Online-Image.jpg?width=352&amp;name=CA-51-Olympics-Online-Image.jpg\" loading=\"lazy\" width=\"352\" alt=\"\" srcset=\"https://www.currentaffairs.org/hs-fs/hubfs/Online%20Article%20Images/CA-51-Olympics-Online-Image.jpg?width=176&amp;name=CA-51-Olympics-Online-Image.jpg 176w, https://www.currentaffairs.org/hs-fs/hubfs/Online%20Article%20Images/CA-51-Olympics-Online-Image.jpg?width=352&amp;name=CA-51-Olympics-Online-Image.jpg 352w, https://www.currentaffairs.org/hs-fs/hubfs/Online%20Article%20Images/CA-51-Olympics-Online-Image.jpg?width=528&amp;name=CA-51-Olympics-Online-Image.jpg 528w, https://www.currentaffairs.org/hs-fs/hubfs/Online%20Article%20Images/CA-51-Olympics-Online-Image.jpg?width=704&amp;name=CA-51-Olympics-Online-Image.jpg 704w, https://www.currentaffairs.org/hs-fs/hubfs/Online%20Article%20Images/CA-51-Olympics-Online-Image.jpg?width=880&amp;name=CA-51-Olympics-Online-Image.jpg 880w, https://www.currentaffairs.org/hs-fs/hubfs/Online%20Article%20Images/CA-51-Olympics-Online-Image.jpg?width=1056&amp;name=CA-51-Olympics-Online-Image.jpg 1056w\" sizes=\"(max-width: 352px) 100vw, 352px\">\n\n              <h1 class=\"title\">Passing the Torch of LGBTQ Activism</h1>\n              <ul class=\"bylines\">\n                <li class=\"author\">Noah Prejean</li>\n              </ul> \n            </a>\n          </li>\n          \n          \n          \n          \n          \n          \n          \n          \n          \n          \n          \n          \n          \n          \n\n        </ul>\n      </div>\n    </div>\n  </section>\n\n  <div id=\"hs_cos_wrapper_module_17158788540125\" class=\"hs_cos_wrapper hs_cos_wrapper_widget hs_cos_wrapper_type_module\" style=\"\" data-hs-cos-general-type=\"widget\" data-hs-cos-type=\"module\"><aside class=\"newsletter-signup\">\n  <div class=\"bound\">\n\n    <div class=\"pithy-appeal\">\n      <span class=\"sneer\">\n        <i>Why,</i>\n        <i>you’d have</i>\n        <i>to be an</i>\n        <i>uncultured</i>\n        <i>rube to not</i>\n        <i>sign</i>\n        <i>up for</i>\n      </span>\n      <mark class=\"proclamation\">\n        <i class=\"definite-article\">The</i>\n        <i class=\"title\">Current Affairs <b class=\"hyphenate\">News­letter</b></i>\n      </mark>\n    </div>\n\n\n\n    <div class=\"substantive-signup-form\">\n<script charset=\"utf-8\" type=\"text/javascript\" src=\"//js.hsforms.net/forms/embed/v2.js\"></script>\n    <script>\n      hbspt.forms.create({\n        region: \"na1\",\n        portalId: \"43971025\",\n        css: \"\",\n        formId: \"b57997e5-668d-4976-a3ee-a34f49f81d53\"\n      });\n    </script>\n    </div><!--/ .substantive-signup-form -->\n\n\n\n  </div>\n</aside></div>\n  <div id=\"hs_cos_wrapper_module_17158789824228\" class=\"hs_cos_wrapper hs_cos_wrapper_widget hs_cos_wrapper_type_module\" style=\"\" data-hs-cos-general-type=\"widget\" data-hs-cos-type=\"module\"><aside class=\"featured-product\">\n  <div class=\"bound\">\n    <div class=\"plate\">\n      <div class=\"columns\">\n        <div class=\"art\">\n          <div class=\"art-inner\">\n            <img alt=\"BOOK: Welcome To This Strange Thing Called Life\" class=\"lazy loaded\" src=\"https://www.currentaffairs.org/hubfs/3232793916138716292.jpg\">\n          </div>\n        </div>\n        <div class=\"text\">\n          <div class=\"text-inner\">\n\n            <h1 class=\"title\">BOOK: Welcome To This Strange Thing Called Life</h1>\n            <div class=\"desc\">\n              A charming monologue to introduce newborn infants to the human world. Illustrated by Ellen Burch, words by Nathan Robinson.\n            </div>\n            <footer class=\"action\">\n              <a target=\"_blank\" href=\"https://shop.currentaffairs.org/products/welcome-to-this-strange-thing-called-life\">Buy Now</a>\n            </footer>\n\n          </div>\n        </div>\n      </div>\n    </div>\n  </div>\n</aside></div>  \n  <div id=\"hs_cos_wrapper_module_171588000077423\" class=\"hs_cos_wrapper hs_cos_wrapper_widget hs_cos_wrapper_type_module\" style=\"\" data-hs-cos-general-type=\"widget\" data-hs-cos-type=\"module\"><nav class=\"evergreen-plugs-triptych\">\n  <div class=\"bound\">\n    <ul class=\"columns\">\n\n      <li>\n        <dl>\n          <dt>\n            <a href=\"http://patreon.com/currentaffairs\">Podcast</a>\n          </dt>\n          <dd>\n            <p>Every other week our editorial team brings you a mixture of discussion, analysis, and whimsy.</p>\n          </dd>\n        </dl>\n      </li>\n\n      <li>\n        <dl>\n          <dt>\n            <a href=\"/store\">Shop</a>\n          </dt>\n          <dd>\n            <p>An excellent way to demonstrate to passersby that you are an individual of unusually well-cultivated taste.</p>\n          </dd>\n        </dl>\n      </li>\n\n      <li>\n        <dl>\n          <dt>\n            <a href=\"/donate\">Donate</a>\n          </dt>\n          <dd>\n            <p>We have two missions: to produce the world's first readable political publication and to make life joyful again.</p>\n          </dd>\n        </dl>\n      </li>\n\n    </ul>\n  </div>\n</nav></div>\n  <div id=\"hs_cos_wrapper_module_171588012409227\" class=\"hs_cos_wrapper hs_cos_wrapper_widget hs_cos_wrapper_type_module\" style=\"\" data-hs-cos-general-type=\"widget\" data-hs-cos-type=\"module\"><aside class=\"subscribe-block\">\n  <div class=\"bound\">\n    <img alt=\"Montage of pages from the Current Affairs print magazine\" class=\"lazy loaded\" src=\"https://www.currentaffairs.org/hubfs/images/ornament-subscribe-motif-new.png\" data-was-processed=\"true\">\n    <h2>Subscribe</h2>\n    <div class=\"essay\">\n      <p>A <em>Current Affairs</em> subscription is one of the best known ways to improve your life in a hurry. Our print magazine is released six times a year, in a beautiful full-color edition full of elegant design, sophisticated prose, and satirical advertisements.</p>\n    </div>\n    <footer class=\"action\">\n      <a class=\"boxy\" href=\"/subscribe\">Tell me more</a>\n    </footer>\n  </div>\n</aside></div>\n\n\n</div>\n\n      </main>\n\n      \n      <div data-global-resource-path=\"current affairs/templates/partials/footer.html\"><footer class=\"site-footer\">\n  <div class=\"bound\">\n    <h1 class=\"brand\">\n      <a href=\"https://www.currentaffairs.org/\">Current Affairs</a>\n    </h1>\n    <div class=\"quadtych\">\n\n        <ul>\n          <li>© 2025 Current Affairs</li>\n          <li><a href=\"/about-us/\">About Us</a></li>\n          <li><a href=\"/faq/\">FAQ</a></li>\n          <li><a href=\"/policies/\">Policies</a></li>\n          <li><a href=\"https://docs.google.com/forms/d/e/1FAIpQLScV9uqXA9HPNhbFcoRCoz9crYHsyBopIhWM6Ttm-z8lviv7FA/viewform\">Pitch</a></li>\n          <li class=\"social\"><a class=\"twitter\" href=\"https://twitter.com/curaffairs\">/curaffairs</a></li>\n          <li class=\"social\"><a class=\"facebook\" href=\"https://facebook.com/curaffairs\">/curaffairs</a></li>\n          <li class=\"social\"><a class=\"instagram\" href=\"https://instagram.com/currentaffairsmag\">/currentaffairsmag</a></li>\n        </ul>\n\n    </div>\n    <aside class=\"cities\">\n \n    </aside>\n  </div>\n</footer></div>\n      \n    </div>\n    \n    \n    <script src=\"/hs/hsstatic/jquery-libs/static-1.1/jquery/jquery-1.7.1.js\"></script>\n<script>hsjQuery = window['jQuery'];</script>\n<!-- HubSpot performance collection script -->\n<script defer src=\"/hs/hsstatic/content-cwv-embed/static-1.1293/embed.js\"></script>\n<script src=\"https://www.currentaffairs.org/hs-fs/hub/43971025/hub_generated/template_assets/155741666332/1714752890144/current_affairs/js/main.min.js\"></script>\n<script>\nvar hsVars = hsVars || {}; hsVars['language'] = 'en';\n</script>\n\n<script src=\"/hs/hsstatic/cos-i18n/static-1.53/bundles/project.js\"></script>\n<script src=\"https://www.currentaffairs.org/hs-fs/hubfs/hub_generated/module_assets/1/169311965992/1738875160565/module_CA_search_input_live.min.js\"></script>\n\n<!-- Start of HubSpot Analytics Code -->\n<script type=\"text/javascript\">\nvar _hsq = _hsq || [];\n_hsq.push([\"setContentType\", \"blog-post\"]);\n_hsq.push([\"setCanonicalUrl\", \"https:\\/\\/www.currentaffairs.org\\/news\\/2021\\/07\\/the-dangerous-ideas-of-longtermism-and-existential-risk\"]);\n_hsq.push([\"setPageId\", \"167252930999\"]);\n_hsq.push([\"setContentMetadata\", {\n    \"contentPageId\": 167252930999,\n    \"legacyPageId\": \"167252930999\",\n    \"contentFolderId\": null,\n    \"contentGroupId\": 163488204299,\n    \"abTestId\": null,\n    \"languageVariantId\": 167252930999,\n    \"languageCode\": \"en\",\n    \n    \n}]);\n</script>\n\n<script type=\"text/javascript\" id=\"hs-script-loader\" async defer src=\"/hs/scriptloader/43971025.js\"></script>\n<!-- End of HubSpot Analytics Code -->\n\n\n<script type=\"text/javascript\">\nvar hsVars = {\n    render_id: \"b34abd75-43cb-4ee4-a491-6933f5af6351\",\n    ticks: 1738946929630,\n    page_id: 167252930999,\n    \n    content_group_id: 163488204299,\n    portal_id: 43971025,\n    app_hs_base_url: \"https://app.hubspot.com\",\n    cp_hs_base_url: \"https://cp.hubspot.com\",\n    language: \"en\",\n    analytics_page_type: \"blog-post\",\n    scp_content_type: \"\",\n    analytics_page_id: \"167252930999\",\n    category_id: 3,\n    folder_id: 0,\n    is_hubspot_user: false\n}\n</script>\n\n\n<script defer src=\"/hs/hsstatic/HubspotToolsMenu/static-1.393/js/index.js\"></script>\n\n\n\n<div id=\"fb-root\"></div>\n  <script>(function(d, s, id) {\n  var js, fjs = d.getElementsByTagName(s)[0];\n  if (d.getElementById(id)) return;\n  js = d.createElement(s); js.id = id;\n  js.src = \"//connect.facebook.net/en_GB/sdk.js#xfbml=1&version=v3.0\";\n  fjs.parentNode.insertBefore(js, fjs);\n }(document, 'script', 'facebook-jssdk'));</script> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=\"https://platform.twitter.com/widgets.js\";fjs.parentNode.insertBefore(js,fjs);}}(document,\"script\",\"twitter-wjs\");</script>\n \n\n\n  \n</body></html>","oembed":false,"readabilityObject":{"title":"The Dangerous Ideas of “Longtermism” and “Existential Risk”","content":"<div id=\"readability-page-1\" class=\"page\"><div>\n        \n        <p><img src=\"https://www.currentaffairs.org/hubfs/Imported_Blog_Media/torres2-1024x646.jpg\" loading=\"eager\" alt=\"\">\n        </p>\n        \n        \n\n        <div>\n          <div>\n            <p>So-called rationalists have created a disturbing secular religion that looks like it addresses humanity’s deepest problems, but actually justifies pursuing the social preferences of elites.</p>\n            \n          </div>\n          <p><mark>\n              <span>filed <time datetime=\"2021-07-28 04:00:00\">28 July 2021\n                </time> in</span> \n              \n              <a href=\"https://www.currentaffairs.org/news/tag/tech\" rel=\"tag\">Tech</a>\n              \n              \n            </mark>\n          </p>\n        </div>\n\n      </div><p><span id=\"hs_cos_wrapper_post_body\" data-hs-cos-general-type=\"meta_field\" data-hs-cos-type=\"rich_text\"><section data-essay-block-type=\"plain\"> \n  <p>In a late-2020 <a href=\"https://www.cnbc.com/2020/12/29/skype-co-founder-jaan-tallinn-on-3-most-concerning-existential-risks-.html\">interview</a> with CNBC, Skype cofounder Jaan Tallinn made a perplexing statement. “Climate change,” he said, “is not going to be an existential risk unless there’s a runaway scenario.” A “runaway scenario” would occur if crossing one or more critical thresholds in the climate system causes Earth’s thermostat to rise uncontrollably. The hotter it <em>has</em> become, the hotter it <em>will</em> become, via self-amplifying processes. This is probably <a href=\"https://theconversation.com/venus-was-once-more-earth-like-but-climate-change-made-it-uninhabitable-150445\">what happened</a> a few billion years ago on our planetary neighbor Venus, a hellish cauldron whose average surface temperature is high enough to melt lead and zinc.</p> \n  <p>Fortunately, the best science today suggests that a runaway scenario is <a href=\"https://theconversation.com/climate-explained-rising-carbon-emissions-probably-wont-make-the-earth-uninhabitable-155447\">unlikely</a>, although not impossible. Yet even without a runaway scenario, the best science also frighteningly affirms that climate change will have devastating consequences. It will <a href=\"https://docs.wixstatic.com/ugd/d9aaad_b2e7f0f56bec40a195e551dd3e8c878e.pdf\">precipitate</a> lethal heatwaves, megadroughts, catastrophic wildfires (like those seen recently in the Western U.S.), desertification, sea-level rise, mass migrations, widespread political instability, food-supply disruptions/famines, extreme weather events (more dangerous hurricanes and flash floods), infectious disease outbreaks, biodiversity loss, mass extinctions, ecological collapse, socioeconomic upheaval, <a href=\"https://slate.com/technology/2015/03/study-climate-change-helped-spark-syrian-civil-war.html\">terrorism and wars</a>, etc. To quote an ominous <a href=\"https://academic.oup.com/bioscience/article/70/1/8/5610806\">2020 paper</a> co-signed by more than 11,000 scientists from around the world, “planet Earth is facing a climate emergency” that, unless immediate and drastic action is taken, will bring about “untold suffering.”</p> \n  <p>So why does Tallinn think that climate change <em>isn’t</em> an existential risk? Intuitively, if anything should count as an existential risk it’s climate change, right?</p> \n  <p>Cynical readers might suspect that, given Tallinn’s immense fortune of an <a href=\"https://fortune.com/2020/11/13/jaan-tallinn-ai-safety-bitcoin-cryptocurrency-elon-musk/\">estimated $900 million</a>, this might be just another case of a super-wealthy tech guy dismissing or minimizing threats that probably won’t directly harm <em>him personally</em>. Despite being <a href=\"https://www.cnbc.com/2021/01/26/oxfam-report-the-global-wealthy-are-main-drivers-of-climate-change.html\">disproportionately responsible</a> for the climate catastrophe, the <a href=\"https://www.technologyreview.com/2018/03/01/144958/if-youre-so-smart-why-arent-you-rich-turns-out-its-just-chance/\">super-rich</a> will be the least affected by it. Peter Thiel—the libertarian who voted for a <a href=\"https://www.politifact.com/factchecks/2016/jun/03/hillary-clinton/yes-donald-trump-did-call-climate-change-chinese-h/\">climate-denier</a> in 2016—has his “<a href=\"https://www.theguardian.com/news/2018/feb/15/why-silicon-valley-billionaires-are-prepping-for-the-apocalypse-in-new-zealand\">apocalypse retreat</a>” in New Zealand, Richard Branson owns his own <a href=\"https://www.businessinsider.com/hurricane-irma-richard-branson-private-island-bunker-2017-9\">hurricane-proof island</a>, Jeff Bezos bought some <a href=\"https://africa.businessinsider.com/finance-billionaires-are-stockpiling-land-that-could-be-used-in-the-apocalypse-heres/pmfybd4\">400,000 acres</a> in Texas, and Elon Musk wants to <a href=\"https://www.extremetech.com/extreme/318959-elon-musk-richest-man-mars-colony\">move to Mars</a>. Astoundingly, Reid Hoffman, the multi-billionaire who cofounded LinkedIn, <a href=\"https://www.imd.org/research-knowledge/articles/what-techs-survivalist-billionaires-should-be-doing-instead/\">reports</a> that “more than 50 percent of Silicon Valley’s billionaires have bought some level of ‘apocalypse insurance,’ such as an underground bunker.”</p> \n  <p>That’s one possibility, for sure. But I think there’s a deeper reason for Tallinn’s comments. It concerns an increasingly influential moral worldview called <em>longtermism</em>. This has roots in the work of philosopher Nick Bostrom, who <a href=\"https://nickbostrom.com/existential/risks.html\">coined</a> the term “existential risk” in 2002 and, three years later, founded the Future of Humanity Institute (FHI) based at the University of Oxford, which has received large sums of money from both <a href=\"https://fortune.com/2020/11/13/jaan-tallinn-ai-safety-bitcoin-cryptocurrency-elon-musk/\">Tallinn</a> and <a href=\"https://www.fhi.ox.ac.uk/elon-musk-funds-oxford-and-cambridge-university-research-on-safe-and-beneficial-artificial-intelligence/\">Musk</a>. Over the past decade, “longtermism” has become one of the main ideas promoted by the “<a href=\"https://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai\">Effective</a> <a href=\"https://www.lrb.co.uk/the-paper/v37/n18/amia-srinivasan/stop-the-robot-apocalypse\">Altruism</a>” (EA) movement, which generated controversy in the past for encouraging young people to work for <a href=\"https://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai\">Wall Street</a> and <a href=\"https://assets.ctfassets.net/es8pp29e1wp8/4C7WHsxZLWeaQgiIksMS0y/73404620d8c355f94f7d4d1130e967e8/Gabriel_published_14_April_3.pdf\">petrochemical companies</a> in order to donate part of their income to charity, an idea called “earn to give.” According to the longtermist Benjamin Todd, formerly at Oxford University, “<a href=\"https://80000hours.org/articles/future-generations/\">longtermism</a> might well turn out to be one of the most important discoveries of effective altruism&nbsp;so far.”</p> \n  <p>Longtermism should not be confused with “long-term thinking.” It goes <em>way beyond</em> the observation that our society is dangerously myopic, and that we should care about future generations no less than present ones. At the heart of this worldview, as delineated by Bostrom, is the idea that what <em>matters most</em> is for “Earth-originating intelligent life” to fulfill its <em>potential</em> in the cosmos. What exactly is “our potential”? As I have <a href=\"https://c8df8822-f112-4676-8332-ad89713358e3.filesusr.com/ugd/d9aaad_89094654cf0945738f5633b5d46653fd.pdf\">noted elsewhere</a>, it <a href=\"https://www.existential-risk.org/concept.html\">involves</a> subjugating nature, maximizing economic productivity, replacing humanity with a superior “posthuman” species, colonizing the universe, and ultimately creating an unfathomably huge population of conscious beings living what Bostrom <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522rich+and+happy+lives%2522+%2522nick+bostrom%2522\">describes</a> as “rich and happy lives” inside high-resolution computer simulations.</p> \n  <p>This is what “our potential” consists of, and it constitutes the ultimate aim toward which humanity as a whole, and each of us as <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">individuals</a>, are morally obligated to strive. An <em>existential risk</em>, then, is any event that would destroy this “<a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522vast+and+glorious%2522+%2522the+precipice%2522\">vast and glorious</a>” potential, as Toby Ord, a philosopher at the Future of Humanity Institute, <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522destruction+of+our+longterm+potential%2522+%2522toby+ord%2522\">writes</a> in his 2020 book <em>The Precipice</em>, which draws heavily <a href=\"https://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/ref=sr_1_1?dchild=1&amp;keywords=global+catastrophic+risks&amp;qid=1626689935&amp;sr=8-1\">from</a> <a href=\"https://www.amazon.com/Here-Be-Dragons-Technology-Humanity-ebook/dp/B018ZK17UI/ref=sr_1_1?dchild=1&amp;keywords=here+be+dragons+haggstrom&amp;qid=1626609967&amp;sr=8-1\">earlier</a> <a href=\"https://www.amazon.com/Morality-Foresight-Human-Flourishing-Introduction/dp/1634311426/ref=sr_1_15?dchild=1&amp;keywords=morality+foresight&amp;qid=1626609986&amp;sr=8-15\">work</a> in outlining the longtermist paradigm. (Note that Noam Chomsky just published a <a href=\"https://www.amazon.com/Precipice-Neoliberalism-Pandemic-Urgent-Radical/dp/164259458X/ref=sr_1_1?dchild=1&amp;keywords=the+precipice&amp;qid=1625949590&amp;sr=8-1\">book</a> also titled <em>The Precipice</em>.)</p> \n  <p>The point is that when one takes the cosmic view, it becomes clear that our civilization could persist for an <em>incredibly long time</em> and there could come to be an <em>unfathomably large number</em> of people in the future. Longtermists thus reason that the far future could contain <em>way more value</em> than exists today, or has existed so far in human history, which stretches back some 300,000 years. So, imagine a situation in which you could either lift 1 billion present people out of extreme poverty <em>or</em> benefit 0.00000000001 percent of the 10<sup>23</sup> biological humans who Bostrom <a href=\"https://www.nickbostrom.com/astronomical/waste.html\">calculates</a> could exist if we were to colonize our cosmic neighborhood, the Virgo Supercluster. Which option should you pick? For longtermists, the answer is obvious: you should pick the latter. Why? Well, just crunch the numbers: 0.00000000001 percent of 10<sup>23</sup> people is 10 billion people, which is <em>ten times greater</em> than 1 billion people. This means that if you want to do the <em>most good</em>, you should focus on these far-future people rather than on helping those in extreme poverty today. As the FHI longtermists Hilary Greaves and Will MacAskill—the latter of whom is <a href=\"https://www.simonknutsson.com/problems-in-effective-altruism-and-existential-risk-and-what-to-do-about-them/%23Potentially_dishonest_self-promotion\">said to have</a> cofounded the Effective Altruism movement with Toby Ord—<a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/2019/Greaves_MacAskill_The_Case_for_Strong_Longtermism.pdf\">write</a>, “for the purposes of evaluating actions, we can in the first instance often <em>simply ignore</em> all the effects contained in the first 100 (or even 1,000) years, focussing primarily on the further-future effects. Short-run effects act as little more than tie-breakers.”</p> \n  <hr> \n  <p>This brings us back to climate change, which is expected to cause serious harms over precisely this time period: the next few decades and centuries. If what matters most is the very<em> far future</em>—thousands, millions, billions, and trillions of years from now—then climate change isn’t going to be high up on the list of global priorities <em>unless there’s a runaway scenario</em>. Sure, it will cause “untold suffering,” but think about the situation from the point of view of the universe itself. Whatever traumas and miseries, deaths and destruction, happen this century will <em>pale in comparison</em> to the astronomical amounts of “value” that could exist once humanity has colonized the universe, become posthuman, and created upwards of 10<sup>58</sup> (Bostrom’s later <a href=\"https://www.google.com/books/edition/Superintelligence/7_H8AwAAQBAJ?hl=en&amp;gbpv=1&amp;dq=%22human+lives+could+be+created+in+emulation+even+with+quite+conservative%22&amp;pg=PA103&amp;printsec=frontcover\">estimate</a>) conscious beings in computer simulations. Bostrom <a href=\"https://www.nickbostrom.com/papers/future.html\">makes this point</a> in terms of economic growth, which he and <a href=\"https://forum.effectivealtruism.org/posts/sFj7EstDYacf6GJWF/q-and-a-with-will-macaskill\">other longtermists</a> see as integral to fulfilling “our potential” in the universe:</p> \n  <blockquote> \n   <p>“<em>In absolute terms, [non-runaway climate change] would be a huge harm. Yet over the course of the twentieth century, world GDP grew by some 3,700%, and per capita world GDP rose by some 860%. It seems safe to say that … whatever negative economic effects global warming will have, they will be completely swamped by other factors that will influence economic growth rates in this century.”</em></p> \n  </blockquote> \n  <p>In the same paper, Bostrom <a href=\"https://www.nickbostrom.com/papers/future.html\">declares</a> that even “a non-existential disaster causing the breakdown of global civilization is, from the perspective of humanity as a whole, a potentially recoverable setback,” describing this as “a giant massacre for man, a small misstep for mankind.” That’s of course cold comfort for those in the crosshairs of climate change—the residents of the Maldives who will <a href=\"https://www.cnbc.com/2021/05/19/maldives-calls-for-urgent-action-to-end-climate-change-sea-level-rise.html%23:~:text=Capital%2520Connection-,The%2520Maldives%2520could%2520disappear%2520by%2520the%2520end%2520of%2520the%2520century,environment,%2520climate%2520change%2520and%2520technology.%26text=and%2520the%2520sea.%25E2%2580%259D-,The%2520World%2520Economic%2520Forum%2520has%2520estimated%2520that%2520by%25202050,%252080,be%2520impacted%2520by%2520climate%2520change.\">lose their homeland</a>, the South Asians facing <a href=\"https://news.mit.edu/2017/deadly-heat-waves-could-hit-south-asia-century-0802\">lethal heat waves</a> above the 95-degree F wet-bulb threshold of survivability, and the 18 million people in Bangladesh who <a href=\"https://ejfoundation.org/reports/climate-displacement-in-bangladesh%23:~:text=Climate%2520Change%2520in%2520Bangladesh,exceptionally%2520vulnerable%2520to%2520climate%2520change.%26text=It%2520has%2520been%2520estimated%2520that,of%2520sea%2520level%2520rise%2520alone.\">may be displaced</a> by 2050. But, once again, when these losses are juxtaposed with the apparent immensity of our longterm “potential,” this suffering will hardly be a footnote to a footnote within humanity’s epic biography.</p> \n  <p>These aren’t the only incendiary remarks from Bostrom, the Father of Longtermism. In a paper that founded one half of longtermist research program, he <a href=\"https://nickbostrom.com/existential/risks.html\">characterizes</a> the most devastating disasters throughout human history, such as the two World Wars (including the Holocaust), Black Death, 1918 Spanish flu pandemic, major earthquakes, large volcanic eruptions, and so on, as “mere ripples” when viewed from “the perspective of humankind as a whole.” As he writes:&nbsp;</p> \n  <blockquote> \n   <p><em>“Tragic as such events are to the people immediately affected, in the big picture of things … even the worst of these catastrophes are mere ripples on the surface of the great sea of life.”&nbsp;</em></p> \n  </blockquote> \n  <p>In other words, 40 million civilian deaths during WWII was awful, we can all agree about that. But think about this in terms of the 10<sup>58</sup> simulated people who could someday exist in computer simulations if we colonize space. It would require <em>trillions and trillions and trillions</em> of WWIIs one after another to even <em>approach</em> the loss of these unborn people if an existential catastrophe were to happen. This is the case even on the lower estimates of how many future people there could be. Take Greaves and MacAskill’s <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">figure</a> of 10<sup>18</sup> expected biological and digital beings on Earth alone (meaning that we don’t colonize space). That’s still a <em>way</em> bigger number than 40 million—analogous to a single grain of sand next to Mount Everest.</p> \n  <p>It’s this line of reasoning that leads Bostrom, Greaves, MacAskill, and others to argue that even the <em>tiniest</em> reductions in “existential risk” are <em>morally equivalent</em> to saving the lives of literally <em>billions</em> of living, breathing, actual people. For example, Bostrom <a href=\"https://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai\">writes</a> that if there is “a mere 1 percent chance” that 10<sup>54</sup> conscious beings (most living in computer simulations) come to exist in the future, then “we find that the expected value of reducing existential risk by a mere <em>one billionth of one billionth of one percentage point </em>is worth a hundred billion times as much as a billion human lives.” Greaves and MacAskill <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">echo this idea</a> in a 2021 paper by arguing that “even if there are ‘only’ 10<sup>14</sup> lives to come … , a reduction in near-term risk of extinction by one millionth of one percentage point would be equivalent in value to a million lives saved.”</p> \n  <p>To make this concrete, imagine Greaves and MacAskill in front of two buttons. If pushed, the first would save the lives of 1 million living, breathing, actual people. The second would increase the probability that 10<sup>14</sup> currently unborn people come into existence in the <em>far future</em> by a <em>teeny-tiny </em>amount. Because, on their longtermist view, there is no fundamental moral difference between saving <em>actual people</em> and <em>bringing new people</em> into existence, these options are <em>morally equivalent</em>. In other words, they’d have to flip a coin to decide which button to push. (Would you? I certainly hope not.) In Bostrom’s example, the morally right thing is <em>obviously</em> to sacrifice billions of living human beings for the sake of even <em>tinier</em> reductions in existential risk, assuming a minuscule 1 percent chance of a <em>larger</em> future population: 10<sup>54</sup> people.</p> \n  <p>All of this is to say that even if billions of people were to perish in the coming climate catastrophe, so long as humanity survives with enough of civilization intact to fulfill its supposed “potential,” we shouldn’t be <em>too</em> concerned. In the grand scheme of things, non-runaway climate change will prove to be nothing more than a “mere ripple” —a “small misstep for mankind,” however terrible a “massacre for man” it might otherwise be.</p> \n  <p>Even worse, since our resources for reducing existential risk are finite, Bostrom <a href=\"https://www.existential-risk.org/concept.html%23:~:text=An%2520existential%2520risk%2520is%2520one,future%2520development%2520(Bostrom%25202002).\">argues</a> that we must not “fritter [them] away” on what he describes as “feel-good projects of suboptimal efficacy.” Such projects would include, on this account, not just saving people in the Global South—those most vulnerable, especially <a href=\"https://www.bbc.com/news/science-environment-43294221\">women</a>—from the calamities of climate change, but <em>all other</em> non-existential philanthropic causes, too. As the Princeton philosopher Peter Singer <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522to+refer+to+donating+to+help+the+global+poor+or+reduce+animal+suffering+as+a+%25E2%2580%2598feel-good+project%25E2%2580%2599+on+which+resources+are+%25E2%2580%2598frittered+away%25E2%2580%2599+is+harsh+language%2522\">writes</a> about Bostrom in his 2015 book on Effective Altruism, “to refer to donating to help the global poor … as a ‘feel-good project’ on which resources are ‘frittered away’ is harsh language.” But it makes perfectly good sense within Bostrom’s longtermist framework, <a href=\"https://www.nickbostrom.com/astronomical/waste.html\">according to which</a> “priority number one, two, three, and four should … be to reduce existential risk.” Everything else is smaller fish not worth frying.</p> \n  <hr> \n  <p>If this sounds appalling, it’s because it <em>is </em>appalling. By reducing morality to an abstract <a href=\"https://users.ox.ac.uk/~mert2255/papers/mu-about-pe.pdf\">numbers game</a>, and by declaring that what’s <a href=\"https://books.google.de/books?hl=en&amp;lr=&amp;id=iPerDwAAQBAJ&amp;oi=fnd&amp;pg=PA80&amp;dq=overwhelming+importance+far+future+beckstead&amp;ots=fJHd_shkvv&amp;sig=BJ3_Xf-5XpwWmTcz5RvPTEJKOcg&amp;redir_esc=y%23v=onepage&amp;q&amp;f=false\">most important</a> is fulfilling “our potential” by becoming simulated posthumans among the stars, longtermists not only trivialize past atrocities like WWII (and the Holocaust) but give themselves a “moral excuse” to dismiss or minimize comparable atrocities in the future. This is one reason that I’ve come to see longtermism as an <em>immensely dangerous ideology</em>. It is, indeed, akin to a <em>secular religion</em> built around the worship of “future value,” complete with its own “secularised doctrine of&nbsp;salvation,” as the Future of Humanity Institute historian Thomas Moynihan approvingly <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522secularised+doctrine+of+salvation%2522\">writes</a> in his book <em>X-Risk</em>. The popularity of this religion among wealthy people in the West—especially the socioeconomic elite—makes sense because it tells them exactly what they want to hear: not only are you <em>ethically excused</em> from worrying too much about sub-existential threats like non-runaway climate change and global poverty, but you are actually a <em>morally better person</em> for focusing instead on more important things—risk that could permanently destroy “our potential” as a species of Earth-originating intelligent life.</p> \n  <p>To drive home the point, consider an argument from the longtermist Nick Beckstead, who has <a href=\"https://www.openphilanthropy.org/focus/global-catastrophic-risks/miscellaneous/future-humanity-institute-work-on-global-catastrophic-risks\">overseen</a> tens of millions of dollars in funding for the Future of Humanity Institute. Since shaping the far future “over the coming millions, billions, and trillions of years” is of “overwhelming importance,” he <a href=\"https://www.proquest.com/docview/1442191960?pq-origsite=gscholar&amp;fromopenview=true\">claims</a>, we should actually care more about people in rich countries than poor countries. This comes from a 2013 PhD dissertation that Ord <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522one+of+the+best+texts+on+existential+risk%2522\">describes</a> as “one of the best texts on existential risk,” and it’s cited on numerous Effective Altruist <a href=\"https://concepts.effectivealtruism.org/concepts/the-long-term-future/\">websites</a>, including some hosted by the Centre for Effective Altruism, which shares office space in Oxford with the Future of Humanity Institute. The passage is worth quoting in full:</p> \n  <blockquote> \n   <p>“<em>Saving lives in poor countries may have significantly smaller ripple effects than saving and improving lives in rich countries. Why? Richer countries have substantially more innovation, and their workers are much more economically productive. By ordinary standards—at least by ordinary enlightened humanitarian standards—saving and improving lives in rich countries is about equally as important as saving and improving lives in poor countries, provided lives are improved by roughly comparable amounts. But it now seems more plausible to me that saving a life in a rich country is substantially more important than saving a life in a poor country, other things being equal.”</em></p> \n  </blockquote> \n  <p>Never mind the fact that many countries in the Global South are relatively poor precisely because of the long and sordid histories of Western colonialism, imperialism, exploitation, political meddling, pollution, and so on. What hangs in the balance is astronomical amounts of “value.” What shouldn’t we do to achieve this magnificent end? Why <em>not</em> prioritize lives in rich countries over those in poor countries, even if gross historical injustices remain inadequately addressed? Beckstead isn’t the only longtermist who’s explicitly endorsed this view, either. As Hilary Greaves <a href=\"https://youtu.be/d1jMlb8E08k?t=148\">states</a> in a 2020 interview with Theron Pummer, who co-edited the book <em>Effective Altruism</em> with her, if one’s “aim is doing the most good, improving the world by the most that I can,” then although “there’s a clear place for transferring resources from the affluent Western world to the global poor … <em>longtermist thought</em> suggests that something else may be <em>better still.”</em></p> \n  <hr> \n  <p>Returning to climate change once again, we can see how Tallinn got the idea that our environmental impact probably isn’t existentially risky from academic longtermists like Bostrom. As alluded to above, Bostrom <a href=\"https://nickbostrom.com/existential/risks.html\">maintains</a> that non-runaway (which he calls “moderate”) global warming, as well as “threats to the biodiversity of Earth’s ecosphere,” as “endurable” rather than “terminal” for humanity. Similarly, Ord <a href=\"https://www.google.com/search?tbm=bks&amp;q=%25221+in+10000%2522+%2522the+precipice%2522+%25221+in+10%2522\">claims</a> in <em>The Precipice </em>that climate change poses a mere 1-in-1,000 chance of existential catastrophe, in contrast to a far greater 1-in-10 chance of catastrophe involving superintelligent machines (dubbed the “Robopocalypse” by <a href=\"https://www.google.com/search?tbm=bks&amp;q=%25E2%2580%259Ca+disaster+sometimes+called+the+Robopocalypse+and+commonly+illustrated+with+stills+from+the+Terminator+movies.%25E2%2580%259D\">some</a>). Although, like Bostrom, Ord acknowledges that the climate crisis could get very bad, he <a href=\"https://youtu.be/R9EtiNmYnQQ?t=510\">assures</a> us that “the typical scenarios of climate change would not destroy our potential.”</p> \n  <p>Within the billionaire world, these conclusions have been parroted by some of the most powerful men on the planet today (not just Tallinn). For example, Musk, an admirer of Bostrom’s who donated $10 million in 2015 to the Future of Life Institute, another longtermist organization that Tallinn cofounded, <a href=\"https://www.youtube.com/watch?v=Jvx_XIihmXs\">said</a> in an interview this year that his “concern with the CO2 is not kind of where we are today or even … the current rate of carbon generation.” Rather, the worry is that “if carbon generation keeps accelerating and … if we’re complacent then I think … there’s some risk of sort of non-linear climate change”—meaning, one surmises, a runaway scenario. Peter Thiel has also apparently held this view for some time, which is unsurprising given his history with longtermist thinking and the Effective Altruism movement. (He gave the <a href=\"https://www.youtube.com/watch?v=h8KkXcBwHec&amp;t=1439s\">keynote address</a> at the 2013 Effective Altruism Summit.) But Thiel also <a href=\"https://www.ft.com/content/abc942cc-5fb3-11e4-8c27-00144feabdc0\">declared</a> in 2014: “People are spending way too much time thinking about climate change” and “way too little thinking about AI.”</p> \n  <p>The reference to AI, or “artificial intelligence,” here is important. Not only do many longtermists believe that <a href=\"https://www.currentaffairs.org/2020/07/the-singularity-prophets\">superintelligent machines</a> pose the greatest single hazard to human survival, but they seem convinced that if humanity were to create a “friendly” superintelligence whose goals are properly “aligned” with our “human goals,” then a new Utopian age of unprecedented security and flourishing would suddenly commence. This eschatological vision is sometimes associated with the “Singularity,” made famous by futurists like Ray Kurzweil, which critics have facetiously dubbed the “<a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522Since+lucky+humans+will+at+that+point+merge+with+superintelligence+or+become+superintelligent,+some+refer+to+the+Singularity+as+the+%27Techno-rapture%27,+pointing+out+the+similarity+ofthe+narrative+to+the+Christian+Rapture%2522\">techno-rapture</a>” or “<a href=\"https://git.jrtechs.net/jrtechs/FOSSRIT-hfoss/raw/commit/6a8f78c0dc52892458a4f12d2e322a1d2e1df6ac/static/books/Cory_Doctorow_and_Charles_Stross_-_Rapture_of_the_Nerds.pdf\">rapture of the nerds</a>” because of its obvious similarities to the Christian dispensationalist notion of the Rapture, when Jesus will swoop down to gather every believer on Earth and carry them back to heaven. As Bostrom <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522would+also+eliminate+or+reduce+many+anthropogenic+risks%2522\">writes</a> in his <a href=\"https://twitter.com/elonmusk/status/495759307346952192?lang=en\">Musk-endorsed</a> book <em>Superintelligence</em>, not only would the various existential risks posed by nature, such as asteroid impacts and supervolcanic eruptions, “be virtually eliminated,” but a friendly superintelligence “would also eliminate or reduce many anthropogenic risks” like climate change. “One might believe,” he <a href=\"https://www.nickbostrom.com/papers/future.html\">writes</a> elsewhere, that “the new civilization would [thus] have vastly improved survival prospects since it would be guided by superintelligent foresight and planning.”</p> \n  <p>Tallinn makes the same point during a Future of Life Institute podcast recorded this year. Whereas a runaway climate scenario is at best many decades away, if it could happen at all, Tallinn <a href=\"https://futureoflife.org/2021/04/20/jaan-tallinn-on-avoiding-civilizational-pitfalls-and-surviving-the-21st-century/\">speculates</a> that superintelligence will present “an existential risk in the next 10 or 50 years.” Thus, he says, “if you’re going to really get AI right [by making it ‘friendly’], it seems like all the other risks [that we might face] become much more manageable.” This is about as literal an interpretation of “<em>deus ex machina</em>” as one can get, and in my experience as someone who spent several months as a visiting scholar at the Centre for the Study of Existential Risk, which was cofounded by Tallinn, it’s a widely-held view among longtermists. In fact, Greaves and MacAskill <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">estimate</a> that every $100 spent on creating a “friendly” superintelligence would be morally equivalent to “saving one trillion [actual human] lives,” assuming that an additional 10<sup>24</sup> people could come to exist in the far future. Hence, they point out that focusing on superintelligence gets you a <em>way bigger</em> bang for your buck than, say, preventing people who exist right now from contracting malaria by distributing <a href=\"https://en.wikipedia.org/wiki/Mosquito_net\">mosquito nets</a>.</p> \n  <hr> \n  <p>What I find most unsettling about the longtermist ideology isn’t just that it contains all the ingredients necessary for a <a href=\"https://www.google.com/search?q=haggstrom+%22you+must+be+willing+to+break+a+few+eggs,%E2%80%9D+which+has+typically+been+used+to+explain+that+a%22&amp;tbm=bks&amp;sxsrf=ALeKk00ecCa6cZZNkMrgVBD_IttGPwKLVg:1627464586070&amp;ei=iiMBYbXnA4S3gwfippaQBw&amp;oq=haggstrom+%22you+must+be+willing+to+break+a+few+eggs,%E2%80%9D+which+has+typically+been+used+to+explain+that+a%22&amp;gs_l=psy-ab.3...7009.7400.0.7668.3.3.0.0.0.0.144.247.0j2.2.0....0...1c.1j2.64.psy-ab..1.0.0....0.8Fa7eTgyzKA\">genocidal catastrophe</a> in the name of realizing astronomical amounts of far-future “value.” Nor is it that this religious ideology has already infiltrated the consciousness of powerful actors who could, <a href=\"https://www.globalcitizen.org/en/content/billionaires-bezos-branson-musk-space-world-hunger/\">for example</a>, “save 41 [million] people at risk of starvation” but instead use their wealth to <a href=\"https://www.theguardian.com/science/2021/jul/19/billionaires-space-tourism-environment-emissions\">fly themselves to space</a>. Even more chilling is that many people in the community believe that their <a href=\"https://www.google.com/search?q=%2522The+challenge+of+our+time+is+to+preserve+our+vast+potential,+and+to+protect+it+against+the+risk+of+future+destruction%2522&amp;tbm=bks&amp;sxsrf=ALeKk030oOJpgYZ9W7TnVS0jbg_cwv1mhQ:1626553426870&amp;ei=UjzzYInJNIm4sAfB24f4CA&amp;oq=%2522The+challenge+of+our+time+is+to+preserve+our+vast+potential,+and+to+protect+it+against+the+risk+of+future+destruction%2522&amp;gs_l=psy-ab.3...5781.5781.0.6014.1.1.0.0.0.0.0.0..0.0....0...1c.1.64.psy-ab..1.0.0....0.JjOFq9K0BOY\">mission to</a> “protect” and “preserve” humanity’s “longterm potential” is <em>so important</em> that they have little tolerance for dissenters. These include critics who might suggest that longtermism is dangerous, or that it supports what Frances Lee Ansley calls <a href=\"https://www.theatlantic.com/politics/archive/2017/10/the-language-of-white-supremacy/542148/\">white supremacy</a> (given the implication, outlined and defended by Beckstead, that we should prioritize the lives of people in rich countries). When one believes that <em>existential risk</em> is the most important concept ever invented, as someone at the Future of Humanity Institute once told me, and that failing to realize “our potential” would not merely be <em>wrong</em> but a moral catastrophe of literally <em>cosmic proportions</em>, one will naturally be inclined to react strongly against those who criticize this sacred dogma. When you believe the stakes are that high, you may be quite willing to use extraordinary means to stop anyone who stands in your way.&nbsp;</p> \n  <section data-essay-block-type=\"excerpt\"> \n   <blockquote> \n    <p><em>By reducing morality to an abstract numbers game, and by declaring that what’s most important is fulfilling “our potential” by becoming simulated posthumans among the stars, longtermists not only trivialize past atrocities like WWII (and the Holocaust) but give themselves a “moral excuse” to dismiss or minimize comparable atrocities in the future.</em></p> \n   </blockquote> \n  </section> \n  <p>In fact, numerous people have come forward, both publicly and privately, over the past few years with stories of being intimidated, silenced, or “canceled.” (Yes, “cancel culture” is a real problem here.) I personally have had three colleagues back out of collaborations with me after I self-published a <a href=\"https://c8df8822-f112-4676-8332-ad89713358e3.filesusr.com/ugd/d9aaad_89094654cf0945738f5633b5d46653fd.pdf\">short critique of longtermism</a>, not because they wanted to, but because they were pressured to do so from longtermists in the community. Others have expressed worries about the personal repercussions of openly criticizing Effective Altruism or the longtermist ideology. For example, the moral philosopher Simon Knutsson wrote a <a href=\"https://www.simonknutsson.com/problems-in-effective-altruism-and-existential-risk-and-what-to-do-about-them/\">critique</a> several years ago in which he notes, among other things, that Bostrom appears to have repeatedly misrepresented his academic achievements in claiming that, as he <a href=\"https://web.archive.org/web/20060701201235/https://nickbostrom.com/\">wrote on his website</a> in 2006, “my performance as an undergraduate set a national record in Sweden.” (There is no evidence that this is true.) The point is that, after doing this, Knutsson <a href=\"https://www.simonknutsson.com/problems-in-effective-altruism-and-existential-risk-and-what-to-do-about-them/%23Writing_this_text\">reports</a> that he became “concerned about his safety” given past efforts to censure certain ideas by longtermists with clout in the community.</p> \n  <p>This might sound hyperbolic, but it’s consistent with a pattern of questionable behavior from leaders in the Effective Altruism movement more generally. For example, one of the first people to become an Effective Altruist after the movement was born circa 2009, Simon Jenkins, reports an incident in which he criticized an idea within Effective Altruism on a Facebook group run by the community. Within an hour, not only had his post been deleted but someone who works for the Centre for Effective Altruism actually <em>called his personal phone</em> to instruct him not to question the movement. “We can’t have people posting anything that suggests that Giving What We Can [an organization founded by Ord] is bad,” as Jenkins recalls. These are just a few of several dozen stories that people have shared with me after I went public with some of my own unnerving experiences.</p> \n  <p>All of this is to say that I’m not especially optimistic about convincing longtermists that their obsession with our “vast and glorious” potential (<a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522vast+and+glorious%2522+%2522the+precipice%2522\">quoting</a> Ord again) could have profoundly harmful consequences if it were to guide actual policy in the world. As the Swedish scholar Olle Häggström has <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522It+is+simply+too+reminiscent+of+the+old+saying+%25E2%2580%259CIf+you+want+to+make+an+omelet,+you+must+be+willing+to+break+a+few+eggs,%25E2%2580%259D+which+has+typically+been+used+to+explain+that+a+bit+of+genocide+or+so+might+be+a+good+thing,+if+it+can+con-+tribute+to+the+goal+of+creating+a+future+utopia.%2522\">disquietingly noted</a>, if political leaders were to take seriously the claim that saving billions of living, breathing, actual people today is morally equivalent to <em>negligible</em> reductions in existential risk, who knows what atrocities this might excuse? If the ends justify the means, and the “end” in this case is a veritable techno-Utopian playground full of 10<sup>58</sup> simulated posthumans awash in “the pulsing ecstasy of love,” as Bostrom writes in his grandiloquent “<a href=\"https://www.nickbostrom.com/utopia.html\">Letter from Utopia</a>,” would <em>any</em> means be off-limits? While <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">some</a> longtermists have recently suggested that there should be constraints on which actions we can take for the far future, others like Bostrom have literally argued that <a href=\"https://nickbostrom.com/existential/risks.html\">preemptive violence</a> and even a <a href=\"https://www.nickbostrom.com/papers/vulnerable.pdf\">global surveillance system</a> should remain options for ensuring the realization of “our potential.” It’s not difficult to see how this way of thinking could have genocidally catastrophic consequences if political actors were to “[take] Bostrom’s argument to heart,” in Häggström’s <a href=\"https://www.google.com/search?tbm=bks&amp;q=%2522if+the+president+has+taken+Bostrom%27s+argument+to+heart%2522\">words</a>.</p> \n  <hr> \n  <p>I should emphasize that rejecting longtermism does <em>not</em> mean that one must reject <em>long-term thinking</em>. You <em>ought to </em>care equally about people no matter when they exist, whether today, next year, or in a couple billion years henceforth. If we shouldn’t discriminate against people based on their spatial distance from us, we shouldn’t discriminate against them based on their temporal distance, either. Many of the problems we face today, such as climate change, will have devastating consequences for future generations hundreds or thousands of years in the future. That should matter. We should be willing to make sacrifices for their wellbeing, just as we make sacrifices for those alive today by donating to charities that fight global poverty. But this does not mean that one must genuflect before the altar of “future value” or “our potential,” understood in techno-Utopian terms of colonizing space, becoming posthuman, subjugating the natural world, maximizing economic productivity, and creating massive computer simulations stuffed with 10<sup>45</sup> digital beings (on Greaves and MacAskill’s <a href=\"https://globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf\">estimate</a> if we were to colonize the Milky Way).</p> \n  <p>Care about the long term, I like to say, but don’t be a<em> longtermist.</em> Superintelligent machines aren’t going to save us, and climate change <em>really should</em> be one of our top global priorities, <em>whether or not</em> it prevents us from becoming simulated posthumans in cosmic computers.</p> \n  <p>Although a handful of longtermists have recently written that the Effective Altruism movement should take climate change more seriously, among the main reasons given for doing so is that, to <a href=\"https://forum.effectivealtruism.org/posts/BwDAN9pGbmCYZGbgf/does-climate-change-deserve-more-attention-within-ea\">quote</a> an employee at the Centre for Effective Altruism, “by failing to show a sufficient appreciation of the severity of climate change, EA may risk losing credibility and alienating potential effective altruists.” In other words, community members should talk more about climate change not because of moral considerations relating to climate justice, the harms it will cause to poor people, and so on, but for <em>marketing reasons</em>. It would be “bad for business” if the public were to associate a dismissive attitude about climate change with Effective Altruism and its longtermist offshoot. As the same author reiterates later on, “I agree [with Bostrom, Ord, etc.] that it is much more important to work on x-risk … , but I wonder whether we are alienating potential EAs by not grappling with this issue.”</p> \n  <p>Yet even if longtermists were to come around to “caring” about climate change, this wouldn’t mean much if it were for the wrong reasons. Knutsson says:</p> \n  <blockquote> \n   <p><em>“Like politicians, one cannot simply and naively assume that these people are being honest about their views, wishes, and what they would do. In the Effective Altruism and existential risk areas, some people seem super-strategic and willing to say whatever will achieve their goals, regardless of whether they believe the claims they make—even more so than in my experience of party politics.”&nbsp;</em></p> \n  </blockquote> \n  <p>Either way, the damage may already have been done, given that averting “untold suffering” from climate change will require <em>immediate action </em>from the Global North. Meanwhile, millionaires and billionaires under the influence of longtermist thinking are focused instead on superintelligent machines that they believe will magically solve the mess that, in <a href=\"https://www.oxfam.org/en/press-releases/carbon-emissions-richest-1-percent-more-double-emissions-poorest-half-humanity\">large part</a>, they themselves have created.</p> \n </section></span>\n      </p></div>","textContent":"\n        \n        \n        \n        \n        \n\n        \n          \n            So-called rationalists have created a disturbing secular religion that looks like it addresses humanity’s deepest problems, but actually justifies pursuing the social preferences of elites.\n            \n          \n          \n              filed 28 July 2021\n                 in \n              \n              Tech\n              \n              \n            \n          \n        \n\n       \n  In a late-2020 interview with CNBC, Skype cofounder Jaan Tallinn made a perplexing statement. “Climate change,” he said, “is not going to be an existential risk unless there’s a runaway scenario.” A “runaway scenario” would occur if crossing one or more critical thresholds in the climate system causes Earth’s thermostat to rise uncontrollably. The hotter it has become, the hotter it will become, via self-amplifying processes. This is probably what happened a few billion years ago on our planetary neighbor Venus, a hellish cauldron whose average surface temperature is high enough to melt lead and zinc. \n  Fortunately, the best science today suggests that a runaway scenario is unlikely, although not impossible. Yet even without a runaway scenario, the best science also frighteningly affirms that climate change will have devastating consequences. It will precipitate lethal heatwaves, megadroughts, catastrophic wildfires (like those seen recently in the Western U.S.), desertification, sea-level rise, mass migrations, widespread political instability, food-supply disruptions/famines, extreme weather events (more dangerous hurricanes and flash floods), infectious disease outbreaks, biodiversity loss, mass extinctions, ecological collapse, socioeconomic upheaval, terrorism and wars, etc. To quote an ominous 2020 paper co-signed by more than 11,000 scientists from around the world, “planet Earth is facing a climate emergency” that, unless immediate and drastic action is taken, will bring about “untold suffering.” \n  So why does Tallinn think that climate change isn’t an existential risk? Intuitively, if anything should count as an existential risk it’s climate change, right? \n  Cynical readers might suspect that, given Tallinn’s immense fortune of an estimated $900 million, this might be just another case of a super-wealthy tech guy dismissing or minimizing threats that probably won’t directly harm him personally. Despite being disproportionately responsible for the climate catastrophe, the super-rich will be the least affected by it. Peter Thiel—the libertarian who voted for a climate-denier in 2016—has his “apocalypse retreat” in New Zealand, Richard Branson owns his own hurricane-proof island, Jeff Bezos bought some 400,000 acres in Texas, and Elon Musk wants to move to Mars. Astoundingly, Reid Hoffman, the multi-billionaire who cofounded LinkedIn, reports that “more than 50 percent of Silicon Valley’s billionaires have bought some level of ‘apocalypse insurance,’ such as an underground bunker.” \n  That’s one possibility, for sure. But I think there’s a deeper reason for Tallinn’s comments. It concerns an increasingly influential moral worldview called longtermism. This has roots in the work of philosopher Nick Bostrom, who coined the term “existential risk” in 2002 and, three years later, founded the Future of Humanity Institute (FHI) based at the University of Oxford, which has received large sums of money from both Tallinn and Musk. Over the past decade, “longtermism” has become one of the main ideas promoted by the “Effective Altruism” (EA) movement, which generated controversy in the past for encouraging young people to work for Wall Street and petrochemical companies in order to donate part of their income to charity, an idea called “earn to give.” According to the longtermist Benjamin Todd, formerly at Oxford University, “longtermism might well turn out to be one of the most important discoveries of effective altruism so far.” \n  Longtermism should not be confused with “long-term thinking.” It goes way beyond the observation that our society is dangerously myopic, and that we should care about future generations no less than present ones. At the heart of this worldview, as delineated by Bostrom, is the idea that what matters most is for “Earth-originating intelligent life” to fulfill its potential in the cosmos. What exactly is “our potential”? As I have noted elsewhere, it involves subjugating nature, maximizing economic productivity, replacing humanity with a superior “posthuman” species, colonizing the universe, and ultimately creating an unfathomably huge population of conscious beings living what Bostrom describes as “rich and happy lives” inside high-resolution computer simulations. \n  This is what “our potential” consists of, and it constitutes the ultimate aim toward which humanity as a whole, and each of us as individuals, are morally obligated to strive. An existential risk, then, is any event that would destroy this “vast and glorious” potential, as Toby Ord, a philosopher at the Future of Humanity Institute, writes in his 2020 book The Precipice, which draws heavily from earlier work in outlining the longtermist paradigm. (Note that Noam Chomsky just published a book also titled The Precipice.) \n  The point is that when one takes the cosmic view, it becomes clear that our civilization could persist for an incredibly long time and there could come to be an unfathomably large number of people in the future. Longtermists thus reason that the far future could contain way more value than exists today, or has existed so far in human history, which stretches back some 300,000 years. So, imagine a situation in which you could either lift 1 billion present people out of extreme poverty or benefit 0.00000000001 percent of the 1023 biological humans who Bostrom calculates could exist if we were to colonize our cosmic neighborhood, the Virgo Supercluster. Which option should you pick? For longtermists, the answer is obvious: you should pick the latter. Why? Well, just crunch the numbers: 0.00000000001 percent of 1023 people is 10 billion people, which is ten times greater than 1 billion people. This means that if you want to do the most good, you should focus on these far-future people rather than on helping those in extreme poverty today. As the FHI longtermists Hilary Greaves and Will MacAskill—the latter of whom is said to have cofounded the Effective Altruism movement with Toby Ord—write, “for the purposes of evaluating actions, we can in the first instance often simply ignore all the effects contained in the first 100 (or even 1,000) years, focussing primarily on the further-future effects. Short-run effects act as little more than tie-breakers.” \n   \n  This brings us back to climate change, which is expected to cause serious harms over precisely this time period: the next few decades and centuries. If what matters most is the very far future—thousands, millions, billions, and trillions of years from now—then climate change isn’t going to be high up on the list of global priorities unless there’s a runaway scenario. Sure, it will cause “untold suffering,” but think about the situation from the point of view of the universe itself. Whatever traumas and miseries, deaths and destruction, happen this century will pale in comparison to the astronomical amounts of “value” that could exist once humanity has colonized the universe, become posthuman, and created upwards of 1058 (Bostrom’s later estimate) conscious beings in computer simulations. Bostrom makes this point in terms of economic growth, which he and other longtermists see as integral to fulfilling “our potential” in the universe: \n   \n   “In absolute terms, [non-runaway climate change] would be a huge harm. Yet over the course of the twentieth century, world GDP grew by some 3,700%, and per capita world GDP rose by some 860%. It seems safe to say that … whatever negative economic effects global warming will have, they will be completely swamped by other factors that will influence economic growth rates in this century.” \n   \n  In the same paper, Bostrom declares that even “a non-existential disaster causing the breakdown of global civilization is, from the perspective of humanity as a whole, a potentially recoverable setback,” describing this as “a giant massacre for man, a small misstep for mankind.” That’s of course cold comfort for those in the crosshairs of climate change—the residents of the Maldives who will lose their homeland, the South Asians facing lethal heat waves above the 95-degree F wet-bulb threshold of survivability, and the 18 million people in Bangladesh who may be displaced by 2050. But, once again, when these losses are juxtaposed with the apparent immensity of our longterm “potential,” this suffering will hardly be a footnote to a footnote within humanity’s epic biography. \n  These aren’t the only incendiary remarks from Bostrom, the Father of Longtermism. In a paper that founded one half of longtermist research program, he characterizes the most devastating disasters throughout human history, such as the two World Wars (including the Holocaust), Black Death, 1918 Spanish flu pandemic, major earthquakes, large volcanic eruptions, and so on, as “mere ripples” when viewed from “the perspective of humankind as a whole.” As he writes:  \n   \n   “Tragic as such events are to the people immediately affected, in the big picture of things … even the worst of these catastrophes are mere ripples on the surface of the great sea of life.”  \n   \n  In other words, 40 million civilian deaths during WWII was awful, we can all agree about that. But think about this in terms of the 1058 simulated people who could someday exist in computer simulations if we colonize space. It would require trillions and trillions and trillions of WWIIs one after another to even approach the loss of these unborn people if an existential catastrophe were to happen. This is the case even on the lower estimates of how many future people there could be. Take Greaves and MacAskill’s figure of 1018 expected biological and digital beings on Earth alone (meaning that we don’t colonize space). That’s still a way bigger number than 40 million—analogous to a single grain of sand next to Mount Everest. \n  It’s this line of reasoning that leads Bostrom, Greaves, MacAskill, and others to argue that even the tiniest reductions in “existential risk” are morally equivalent to saving the lives of literally billions of living, breathing, actual people. For example, Bostrom writes that if there is “a mere 1 percent chance” that 1054 conscious beings (most living in computer simulations) come to exist in the future, then “we find that the expected value of reducing existential risk by a mere one billionth of one billionth of one percentage point is worth a hundred billion times as much as a billion human lives.” Greaves and MacAskill echo this idea in a 2021 paper by arguing that “even if there are ‘only’ 1014 lives to come … , a reduction in near-term risk of extinction by one millionth of one percentage point would be equivalent in value to a million lives saved.” \n  To make this concrete, imagine Greaves and MacAskill in front of two buttons. If pushed, the first would save the lives of 1 million living, breathing, actual people. The second would increase the probability that 1014 currently unborn people come into existence in the far future by a teeny-tiny amount. Because, on their longtermist view, there is no fundamental moral difference between saving actual people and bringing new people into existence, these options are morally equivalent. In other words, they’d have to flip a coin to decide which button to push. (Would you? I certainly hope not.) In Bostrom’s example, the morally right thing is obviously to sacrifice billions of living human beings for the sake of even tinier reductions in existential risk, assuming a minuscule 1 percent chance of a larger future population: 1054 people. \n  All of this is to say that even if billions of people were to perish in the coming climate catastrophe, so long as humanity survives with enough of civilization intact to fulfill its supposed “potential,” we shouldn’t be too concerned. In the grand scheme of things, non-runaway climate change will prove to be nothing more than a “mere ripple” —a “small misstep for mankind,” however terrible a “massacre for man” it might otherwise be. \n  Even worse, since our resources for reducing existential risk are finite, Bostrom argues that we must not “fritter [them] away” on what he describes as “feel-good projects of suboptimal efficacy.” Such projects would include, on this account, not just saving people in the Global South—those most vulnerable, especially women—from the calamities of climate change, but all other non-existential philanthropic causes, too. As the Princeton philosopher Peter Singer writes about Bostrom in his 2015 book on Effective Altruism, “to refer to donating to help the global poor … as a ‘feel-good project’ on which resources are ‘frittered away’ is harsh language.” But it makes perfectly good sense within Bostrom’s longtermist framework, according to which “priority number one, two, three, and four should … be to reduce existential risk.” Everything else is smaller fish not worth frying. \n   \n  If this sounds appalling, it’s because it is appalling. By reducing morality to an abstract numbers game, and by declaring that what’s most important is fulfilling “our potential” by becoming simulated posthumans among the stars, longtermists not only trivialize past atrocities like WWII (and the Holocaust) but give themselves a “moral excuse” to dismiss or minimize comparable atrocities in the future. This is one reason that I’ve come to see longtermism as an immensely dangerous ideology. It is, indeed, akin to a secular religion built around the worship of “future value,” complete with its own “secularised doctrine of salvation,” as the Future of Humanity Institute historian Thomas Moynihan approvingly writes in his book X-Risk. The popularity of this religion among wealthy people in the West—especially the socioeconomic elite—makes sense because it tells them exactly what they want to hear: not only are you ethically excused from worrying too much about sub-existential threats like non-runaway climate change and global poverty, but you are actually a morally better person for focusing instead on more important things—risk that could permanently destroy “our potential” as a species of Earth-originating intelligent life. \n  To drive home the point, consider an argument from the longtermist Nick Beckstead, who has overseen tens of millions of dollars in funding for the Future of Humanity Institute. Since shaping the far future “over the coming millions, billions, and trillions of years” is of “overwhelming importance,” he claims, we should actually care more about people in rich countries than poor countries. This comes from a 2013 PhD dissertation that Ord describes as “one of the best texts on existential risk,” and it’s cited on numerous Effective Altruist websites, including some hosted by the Centre for Effective Altruism, which shares office space in Oxford with the Future of Humanity Institute. The passage is worth quoting in full: \n   \n   “Saving lives in poor countries may have significantly smaller ripple effects than saving and improving lives in rich countries. Why? Richer countries have substantially more innovation, and their workers are much more economically productive. By ordinary standards—at least by ordinary enlightened humanitarian standards—saving and improving lives in rich countries is about equally as important as saving and improving lives in poor countries, provided lives are improved by roughly comparable amounts. But it now seems more plausible to me that saving a life in a rich country is substantially more important than saving a life in a poor country, other things being equal.” \n   \n  Never mind the fact that many countries in the Global South are relatively poor precisely because of the long and sordid histories of Western colonialism, imperialism, exploitation, political meddling, pollution, and so on. What hangs in the balance is astronomical amounts of “value.” What shouldn’t we do to achieve this magnificent end? Why not prioritize lives in rich countries over those in poor countries, even if gross historical injustices remain inadequately addressed? Beckstead isn’t the only longtermist who’s explicitly endorsed this view, either. As Hilary Greaves states in a 2020 interview with Theron Pummer, who co-edited the book Effective Altruism with her, if one’s “aim is doing the most good, improving the world by the most that I can,” then although “there’s a clear place for transferring resources from the affluent Western world to the global poor … longtermist thought suggests that something else may be better still.” \n   \n  Returning to climate change once again, we can see how Tallinn got the idea that our environmental impact probably isn’t existentially risky from academic longtermists like Bostrom. As alluded to above, Bostrom maintains that non-runaway (which he calls “moderate”) global warming, as well as “threats to the biodiversity of Earth’s ecosphere,” as “endurable” rather than “terminal” for humanity. Similarly, Ord claims in The Precipice that climate change poses a mere 1-in-1,000 chance of existential catastrophe, in contrast to a far greater 1-in-10 chance of catastrophe involving superintelligent machines (dubbed the “Robopocalypse” by some). Although, like Bostrom, Ord acknowledges that the climate crisis could get very bad, he assures us that “the typical scenarios of climate change would not destroy our potential.” \n  Within the billionaire world, these conclusions have been parroted by some of the most powerful men on the planet today (not just Tallinn). For example, Musk, an admirer of Bostrom’s who donated $10 million in 2015 to the Future of Life Institute, another longtermist organization that Tallinn cofounded, said in an interview this year that his “concern with the CO2 is not kind of where we are today or even … the current rate of carbon generation.” Rather, the worry is that “if carbon generation keeps accelerating and … if we’re complacent then I think … there’s some risk of sort of non-linear climate change”—meaning, one surmises, a runaway scenario. Peter Thiel has also apparently held this view for some time, which is unsurprising given his history with longtermist thinking and the Effective Altruism movement. (He gave the keynote address at the 2013 Effective Altruism Summit.) But Thiel also declared in 2014: “People are spending way too much time thinking about climate change” and “way too little thinking about AI.” \n  The reference to AI, or “artificial intelligence,” here is important. Not only do many longtermists believe that superintelligent machines pose the greatest single hazard to human survival, but they seem convinced that if humanity were to create a “friendly” superintelligence whose goals are properly “aligned” with our “human goals,” then a new Utopian age of unprecedented security and flourishing would suddenly commence. This eschatological vision is sometimes associated with the “Singularity,” made famous by futurists like Ray Kurzweil, which critics have facetiously dubbed the “techno-rapture” or “rapture of the nerds” because of its obvious similarities to the Christian dispensationalist notion of the Rapture, when Jesus will swoop down to gather every believer on Earth and carry them back to heaven. As Bostrom writes in his Musk-endorsed book Superintelligence, not only would the various existential risks posed by nature, such as asteroid impacts and supervolcanic eruptions, “be virtually eliminated,” but a friendly superintelligence “would also eliminate or reduce many anthropogenic risks” like climate change. “One might believe,” he writes elsewhere, that “the new civilization would [thus] have vastly improved survival prospects since it would be guided by superintelligent foresight and planning.” \n  Tallinn makes the same point during a Future of Life Institute podcast recorded this year. Whereas a runaway climate scenario is at best many decades away, if it could happen at all, Tallinn speculates that superintelligence will present “an existential risk in the next 10 or 50 years.” Thus, he says, “if you’re going to really get AI right [by making it ‘friendly’], it seems like all the other risks [that we might face] become much more manageable.” This is about as literal an interpretation of “deus ex machina” as one can get, and in my experience as someone who spent several months as a visiting scholar at the Centre for the Study of Existential Risk, which was cofounded by Tallinn, it’s a widely-held view among longtermists. In fact, Greaves and MacAskill estimate that every $100 spent on creating a “friendly” superintelligence would be morally equivalent to “saving one trillion [actual human] lives,” assuming that an additional 1024 people could come to exist in the far future. Hence, they point out that focusing on superintelligence gets you a way bigger bang for your buck than, say, preventing people who exist right now from contracting malaria by distributing mosquito nets. \n   \n  What I find most unsettling about the longtermist ideology isn’t just that it contains all the ingredients necessary for a genocidal catastrophe in the name of realizing astronomical amounts of far-future “value.” Nor is it that this religious ideology has already infiltrated the consciousness of powerful actors who could, for example, “save 41 [million] people at risk of starvation” but instead use their wealth to fly themselves to space. Even more chilling is that many people in the community believe that their mission to “protect” and “preserve” humanity’s “longterm potential” is so important that they have little tolerance for dissenters. These include critics who might suggest that longtermism is dangerous, or that it supports what Frances Lee Ansley calls white supremacy (given the implication, outlined and defended by Beckstead, that we should prioritize the lives of people in rich countries). When one believes that existential risk is the most important concept ever invented, as someone at the Future of Humanity Institute once told me, and that failing to realize “our potential” would not merely be wrong but a moral catastrophe of literally cosmic proportions, one will naturally be inclined to react strongly against those who criticize this sacred dogma. When you believe the stakes are that high, you may be quite willing to use extraordinary means to stop anyone who stands in your way.  \n   \n    \n    By reducing morality to an abstract numbers game, and by declaring that what’s most important is fulfilling “our potential” by becoming simulated posthumans among the stars, longtermists not only trivialize past atrocities like WWII (and the Holocaust) but give themselves a “moral excuse” to dismiss or minimize comparable atrocities in the future. \n    \n   \n  In fact, numerous people have come forward, both publicly and privately, over the past few years with stories of being intimidated, silenced, or “canceled.” (Yes, “cancel culture” is a real problem here.) I personally have had three colleagues back out of collaborations with me after I self-published a short critique of longtermism, not because they wanted to, but because they were pressured to do so from longtermists in the community. Others have expressed worries about the personal repercussions of openly criticizing Effective Altruism or the longtermist ideology. For example, the moral philosopher Simon Knutsson wrote a critique several years ago in which he notes, among other things, that Bostrom appears to have repeatedly misrepresented his academic achievements in claiming that, as he wrote on his website in 2006, “my performance as an undergraduate set a national record in Sweden.” (There is no evidence that this is true.) The point is that, after doing this, Knutsson reports that he became “concerned about his safety” given past efforts to censure certain ideas by longtermists with clout in the community. \n  This might sound hyperbolic, but it’s consistent with a pattern of questionable behavior from leaders in the Effective Altruism movement more generally. For example, one of the first people to become an Effective Altruist after the movement was born circa 2009, Simon Jenkins, reports an incident in which he criticized an idea within Effective Altruism on a Facebook group run by the community. Within an hour, not only had his post been deleted but someone who works for the Centre for Effective Altruism actually called his personal phone to instruct him not to question the movement. “We can’t have people posting anything that suggests that Giving What We Can [an organization founded by Ord] is bad,” as Jenkins recalls. These are just a few of several dozen stories that people have shared with me after I went public with some of my own unnerving experiences. \n  All of this is to say that I’m not especially optimistic about convincing longtermists that their obsession with our “vast and glorious” potential (quoting Ord again) could have profoundly harmful consequences if it were to guide actual policy in the world. As the Swedish scholar Olle Häggström has disquietingly noted, if political leaders were to take seriously the claim that saving billions of living, breathing, actual people today is morally equivalent to negligible reductions in existential risk, who knows what atrocities this might excuse? If the ends justify the means, and the “end” in this case is a veritable techno-Utopian playground full of 1058 simulated posthumans awash in “the pulsing ecstasy of love,” as Bostrom writes in his grandiloquent “Letter from Utopia,” would any means be off-limits? While some longtermists have recently suggested that there should be constraints on which actions we can take for the far future, others like Bostrom have literally argued that preemptive violence and even a global surveillance system should remain options for ensuring the realization of “our potential.” It’s not difficult to see how this way of thinking could have genocidally catastrophic consequences if political actors were to “[take] Bostrom’s argument to heart,” in Häggström’s words. \n   \n  I should emphasize that rejecting longtermism does not mean that one must reject long-term thinking. You ought to care equally about people no matter when they exist, whether today, next year, or in a couple billion years henceforth. If we shouldn’t discriminate against people based on their spatial distance from us, we shouldn’t discriminate against them based on their temporal distance, either. Many of the problems we face today, such as climate change, will have devastating consequences for future generations hundreds or thousands of years in the future. That should matter. We should be willing to make sacrifices for their wellbeing, just as we make sacrifices for those alive today by donating to charities that fight global poverty. But this does not mean that one must genuflect before the altar of “future value” or “our potential,” understood in techno-Utopian terms of colonizing space, becoming posthuman, subjugating the natural world, maximizing economic productivity, and creating massive computer simulations stuffed with 1045 digital beings (on Greaves and MacAskill’s estimate if we were to colonize the Milky Way). \n  Care about the long term, I like to say, but don’t be a longtermist. Superintelligent machines aren’t going to save us, and climate change really should be one of our top global priorities, whether or not it prevents us from becoming simulated posthumans in cosmic computers. \n  Although a handful of longtermists have recently written that the Effective Altruism movement should take climate change more seriously, among the main reasons given for doing so is that, to quote an employee at the Centre for Effective Altruism, “by failing to show a sufficient appreciation of the severity of climate change, EA may risk losing credibility and alienating potential effective altruists.” In other words, community members should talk more about climate change not because of moral considerations relating to climate justice, the harms it will cause to poor people, and so on, but for marketing reasons. It would be “bad for business” if the public were to associate a dismissive attitude about climate change with Effective Altruism and its longtermist offshoot. As the same author reiterates later on, “I agree [with Bostrom, Ord, etc.] that it is much more important to work on x-risk … , but I wonder whether we are alienating potential EAs by not grappling with this issue.” \n  Yet even if longtermists were to come around to “caring” about climate change, this wouldn’t mean much if it were for the wrong reasons. Knutsson says: \n   \n   “Like politicians, one cannot simply and naively assume that these people are being honest about their views, wishes, and what they would do. In the Effective Altruism and existential risk areas, some people seem super-strategic and willing to say whatever will achieve their goals, regardless of whether they believe the claims they make—even more so than in my experience of party politics.”  \n   \n  Either way, the damage may already have been done, given that averting “untold suffering” from climate change will require immediate action from the Global North. Meanwhile, millionaires and billionaires under the influence of longtermist thinking are focused instead on superintelligent machines that they believe will magically solve the mess that, in large part, they themselves have created. \n \n      ","length":29877,"excerpt":"So-called rationalists have created a disturbing secular religion that looks like it addresses humanity’s deepest problems, but actually justifies pursuing the social preferences of elites.","byline":"Émile P. Torres","dir":null,"siteName":"Current Affairs","lang":"en"},"finalizedMeta":{"title":"The Dangerous Ideas of “Longtermism” and “Existential Risk”","description":"So-called rationalists have created a disturbing secular religion that looks like it addresses humanity’s deepest problems, but actually justifies pursuing the social preferences of elites.","author":"Émile P. Torres","creator":"Émile P. Torres","publisher":"Current Affairs","date":"2024-05-30T21:49:02.676Z","image":"https://support.currentaffairs.org/hubfs/Imported_Blog_Media/torres2-1024x646.jpg","topics":[]},"jsonLd":{"@type":"BlogPosting","headline":"The Dangerous Ideas of “Longtermism” and “Existential Risk”","description":false,"image":["https://support.currentaffairs.org/hubfs/Imported_Blog_Media/torres2-1024x646.jpg"],"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk"},"datePublished":"2021-07-28T04:00:00.000Z","dateModified":"2024-05-30T21:49:02.676Z","isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"name":"Émile P. Torres","url":"https://www.currentaffairs.org/news/author/émile-p-torres","@type":"Person"},"publisher":{"name":"Current Affairs","logo":{"@type":"ImageObject"},"@type":"Organization"},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"@context":"https://schema.org"},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"The Dangerous Ideas of “Longtermism” and “Existential Risk”","description":"So-called rationalists have created a disturbing secular religion that looks like it addresses humanity’s deepest problems, but actually justifies pursuing the social preferences of elites.","canonical":"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk","keywords":[],"image":"https://www.currentaffairs.org/hubfs/Imported_Blog_Media/torres2-1024x646.jpg","firstParagraph":"\n        A Magazine of Politics and Culture\n      "},"dublinCore":{},"opengraph":{"title":"The Dangerous Ideas of “Longtermism” and “Existential Risk”","description":"So-called rationalists have created a disturbing secular religion that looks like it addresses humanity’s deepest problems, but actually justifies pursuing the social preferences of elites.","url":"https://www.currentaffairs.org/news/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk","site_name":false,"locale":false,"type":"article","typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":"https://www.currentaffairs.org/hubfs/Imported_Blog_Media/torres2-1024x646.jpg","image:width":"1024","image:height":"646"},"twitter":{"site":false,"description":"So-called rationalists have created a disturbing secular religion that looks like it addresses humanity’s deepest problems, but actually justifies pursuing the social preferences of elites.","card":"summary_large_image","creator":false,"title":"The Dangerous Ideas of “Longtermism” and “Existential Risk”","image":"https://www.currentaffairs.org/hubfs/Imported_Blog_Media/torres2-1024x646.jpg","domain":"www.currentaffairs.org"},"archivedData":{"link":false,"wayback":false}}}