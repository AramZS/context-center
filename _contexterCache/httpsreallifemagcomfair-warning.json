{"initialLink":"https://reallifemag.com/fair-warning/","sanitizedLink":"https://reallifemag.com/fair-warning/","finalLink":"https://reallifemag.com/fair-warning/","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://reallifemag.com/fair-warning/\" itemprop=\"url\">Fair Warning — Real Life</a></contexter-box-head></contexter-box-head><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2025-05-10T20:11:09.217Z\">5/10/2025</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>For as long as there has been AI research, there have been credible critiques about the risks of AI boosterism</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a href=\"https://web.archive.org/web/20250510201115/https://reallifemag.com/fair-warning/\" is=\"contexter-link\" target=\"_blank\" rel=\"timemap\" class=\"read-link archive-link\" itemprop=\"archivedAt\" slot=\"archive-link\">Archived</a><a is=\"contexter-link\" href=\"https://reallifemag.com/fair-warning/\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"ca9f1b1a904e91e278f0f6660ccca79156c40af7","data":{"originalLink":"https://reallifemag.com/fair-warning/","sanitizedLink":"https://reallifemag.com/fair-warning/","canonical":"https://reallifemag.com/fair-warning/","htmlText":"<!DOCTYPE html>\n<html>\n<head>\n\t<meta charset=\"utf-8\">\n\t<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\">\n\t<meta name=\"fragment\" content=\"!\">\n\t<base href=\"/\">\n\t<link rel=\"shortcut icon\" href=\"https://reallifemag.com/favicon.ico\" />\n\t<link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"/apple-touch-icon.png\">\n\t<link rel=\"icon\" type=\"image/png\" href=\"/favicon-32x32.png\" sizes=\"32x32\">\n\t<link rel=\"icon\" type=\"image/png\" href=\"/favicon-16x16.png\" sizes=\"16x16\">\n\t<link rel=\"manifest\" href=\"/manifest.json\">\n\t<link rel=\"mask-icon\" href=\"/safari-pinned-tab.svg\" color=\"#2cfd7f\">\n\t<meta name=\"theme-color\" content=\"#ffffff\">\n\t<meta name='robots' content='max-image-preview:large' />\n\t<style>img:is([sizes=\"auto\" i], [sizes^=\"auto,\" i]) { contain-intrinsic-size: 3000px 1500px }</style>\n\t<link rel='dns-prefetch' href='//code.jquery.com' />\n<link rel='stylesheet' id='chips-styles-css' href='https://reallifemag.com/wp-content/themes/reallife2/css/styles.css?v=0.009&#038;ver=6.7.1' type='text/css' media='all' />\n<link rel=\"EditURI\" type=\"application/rsd+xml\" title=\"RSD\" href=\"https://reallifemag.com/xmlrpc.php?rsd\" />\n<link rel=\"canonical\" href=\"https://reallifemag.com/fair-warning/\" />\n \n\n\t<title>Fair Warning — Real Life</title>\n\t<meta name=\"description\" content=\"For as long as there has been AI research, there have been credible critiques about the risks of AI boosterism\">\t<meta property=\"og:title\" content=\"Fair Warning — Real Life\" />\n\t<meta property=\"og:description\" content=\"For as long as there has been AI research, there have been credible critiques about the risks of AI boosterism\" />\t<meta property=\"og:site_name\" content=\"Real Life\" />\n\t<meta property=\"og:type\" content=\"article\" />\n\t\t\t\t<meta property=\"og:url\" content=\"https://reallifemag.com/fair-warning/\" />\n\t\t\n\n\t\t<meta property=\"og:image\" content=\"https://reallifemag.com/wp-content/uploads/2020/02/tabitarezaire-1024x569.jpg\" />\t<meta property=\"og:image:width\" content=\"1024\" />\t<meta property=\"og:image:height\" content=\"569\" />\n\t<meta name=\"twitter:card\" content=\"summary\"/>\n\n\t<meta name=\"twitter:description\" content=\"For as long as there has been AI research, there have been credible critiques about the risks of AI boosterism\"/>\t<meta name=\"twitter:title\" content=\"Fair Warning — Real Life\"/>\n\t<meta name=\"twitter:site\" content=\"@_reallifemag\">\n\t\t<meta name=\"twitter:image\" content=\"https://reallifemag.com/wp-content/uploads/2020/02/tabitarezaire-1024x569.jpg\"/>\n\n\t<link rel=\"alternate\" type=\"application/rss+xml\" title=\"RSS Feed for reallifemag.com\" href=\"/feed/\" />\n</head>\n\n<body data-barba=\"wrapper\" class=\"post-template-default single single-post postid-3089 single-format-standard\">\n<header>\n\t<div class=\"svg-parent maxW\">\n\t\t<div class=\"svg-wrap\">\n\t\t\t<svg id=\"reallifelogo_dt\" data-name=\"reallifelogo_dt 1\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 600 90\"><defs/><g id=\"REALR\"><path class=\"cls-2\" d=\"M68.606 38.462a9.428 9.428 0 008.227-8.236H90.62a23.105 23.105 0 01-22.014 22.036v-13.8z\"/><path class=\"cls-2\" d=\"M76.833 28.06a9.428 9.428 0 00-8.227-8.236v-13.8A23.104 23.104 0 0190.62 28.061H76.833z\"/><path class=\"cls-2\" d=\"M68.697 22.019a7.22 7.22 0 010 14.249v-14.25z\"/><path class=\"cls-2\" d=\"M66.495 22.019a7.22 7.22 0 000 14.249v-14.25z\"/><rect class=\"cls-2\" x=\"21.5002\" y=\"6\" width=\"13.9863\" height=\"13.9854\"/><polygon class=\"cls-2\" points=\"37.651 19.985 37.651 6 66.441 6 66.441 19.752 66.438 19.985 37.651 19.985\"/><rect class=\"cls-2\" x=\"37.6511\" y=\"38.3008\" width=\"28.79\" height=\"13.9854\"/><polygon class=\"cls-2\" points=\"69.981 84.587 55.771 54.45 71.748 54.45 86.002 84.587 69.981 84.587\"/><rect class=\"cls-2\" x=\"21.5002\" y=\"54.4502\" width=\"13.9863\" height=\"30.1367\"/><rect class=\"cls-2\" x=\"21.5002\" y=\"22.1504\" width=\"13.9863\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"21.5002\" y=\"38.3008\" width=\"13.9863\" height=\"13.9854\"/></g><g id=\"REALE\"><rect class=\"cls-2\" x=\"104.2654\" y=\"6\" width=\"13.9863\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"120.4163\" y=\"6\" width=\"42.2485\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"120.4163\" y=\"70.6016\" width=\"42.2485\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"120.4163\" y=\"38.3008\" width=\"28.7896\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"104.2654\" y=\"22.1504\" width=\"13.9863\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"104.2654\" y=\"54.4502\" width=\"13.9863\" height=\"13.9863\"/><rect class=\"cls-2\" x=\"104.2654\" y=\"38.3008\" width=\"13.9863\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"104.2654\" y=\"70.6016\" width=\"13.9863\" height=\"13.9854\"/></g><g id=\"REALA\"><polygon class=\"cls-2\" points=\"196.468 68.324 201.666 54.85 219.49 54.85 224.684 68.324 196.468 68.324\"/><polygon class=\"cls-2\" points=\"233.272 84.587 227.787 70.497 243.682 70.497 249.359 84.587 233.272 84.587\"/><polygon class=\"cls-2\" points=\"220.977 52.685 211.744 28.732 219.774 7.924 237.054 52.685 220.977 52.685\"/><polygon class=\"cls-2\" points=\"227.005 68.324 221.811 54.85 237.89 54.85 243.092 68.324 227.005 68.324\"/><polygon class=\"cls-2\" points=\"171.776 84.587 177.224 70.487 193.322 70.46 187.872 84.587 171.776 84.587\"/><polygon class=\"cls-2\" points=\"184.104 52.685 201.403 7.924 209.424 28.733 200.182 52.685 184.104 52.685\"/><polygon class=\"cls-2\" points=\"178.06 68.324 183.268 54.85 199.346 54.85 194.147 68.324 178.06 68.324\"/><polygon class=\"cls-2\" points=\"202.982 6 218.196 6 210.585 25.725 202.982 6\"/></g><g id=\"REALL\"><rect class=\"cls-2\" x=\"275.7258\" y=\"70.6016\" width=\"26.0986\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"259.5759\" y=\"22.1504\" width=\"13.9854\" height=\"46.2861\"/><rect class=\"cls-2\" x=\"259.5759\" y=\"70.6016\" width=\"13.9854\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"303.989\" y=\"70.6016\" width=\"13.9863\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"259.5759\" y=\"6\" width=\"13.9854\" height=\"13.9854\"/></g><g id=\"LIFEL\"><rect class=\"cls-2\" x=\"364.5828\" y=\"70.6016\" width=\"26.0986\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"348.4324\" y=\"22.1504\" width=\"13.9863\" height=\"46.2861\"/><rect class=\"cls-2\" x=\"348.4324\" y=\"70.6016\" width=\"13.9863\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"392.8464\" y=\"70.6016\" width=\"13.9854\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"348.4324\" y=\"6\" width=\"13.9863\" height=\"13.9854\"/></g><g id=\"LIFEI\"><rect class=\"cls-2\" x=\"417.5593\" y=\"22.1504\" width=\"13.9854\" height=\"46.2861\"/><rect class=\"cls-2\" x=\"417.5593\" y=\"70.6016\" width=\"13.9854\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"417.5593\" y=\"6\" width=\"13.9854\" height=\"13.9854\"/></g><g id=\"LIFEF\"><rect class=\"cls-2\" x=\"449.3572\" y=\"6\" width=\"13.9844\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"465.5066\" y=\"6\" width=\"42.25\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"465.5066\" y=\"38.3008\" width=\"28.791\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"449.3572\" y=\"22.1504\" width=\"13.9844\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"449.3572\" y=\"54.4502\" width=\"13.9844\" height=\"30.1367\"/><rect class=\"cls-2\" x=\"449.3572\" y=\"38.3008\" width=\"13.9844\" height=\"13.9854\"/></g><g id=\"LIFEE\"><rect class=\"cls-2\" x=\"520.1003\" y=\"6\" width=\"13.9854\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"536.2507\" y=\"6\" width=\"42.249\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"536.2507\" y=\"70.6016\" width=\"42.249\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"536.2507\" y=\"38.3008\" width=\"28.791\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"520.1003\" y=\"54.4502\" width=\"13.9854\" height=\"13.9863\"/><rect class=\"cls-2\" x=\"520.1003\" y=\"22.1504\" width=\"13.9854\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"520.1003\" y=\"38.3008\" width=\"13.9854\" height=\"13.9854\"/><rect class=\"cls-2\" x=\"520.1003\" y=\"70.6016\" width=\"13.9854\" height=\"13.9854\"/></g></svg>\n\t\t</div>\n\t\t<div class=\"svg-wrap mobile-svg-wrap\">\n\t\t\t<svg id=\"reallifelogo_mo\" data-name=\"reallifelogo_mo\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 256 155\"><rect class=\"cls-1\" width=\"256\" height=\"155\"/><g id=\"REALR\"><path class=\"cls-2\" d=\"M49.18 35.621a7.441 7.441 0 006.493-6.5h10.882a18.236 18.236 0 01-17.376 17.392V35.621z\"/><path class=\"cls-2\" d=\"M55.673 27.412a7.441 7.441 0 00-6.494-6.5V10.018a18.235 18.235 0 0117.376 17.393H55.673z\"/><path class=\"cls-2\" d=\"M49.25 22.643a5.7 5.7 0 010 11.246V22.643z\"/><path class=\"cls-2\" d=\"M47.513 22.643a5.7 5.7 0 000 11.246V22.643z\"/><rect id=\"square\" class=\"cls-2\" x=\"12\" y=\"10\" width=\"11.039\" height=\"11.0383\"/><polygon class=\"cls-2\" points=\"24.747 21.038 24.747 10 47.471 10 47.471 20.854 47.468 21.038 24.747 21.038\"/><rect class=\"cls-2\" x=\"24.7475\" y=\"35.4942\" width=\"22.7232\" height=\"11.0383\"/><polygon class=\"cls-2\" points=\"50.264 72.027 39.049 48.24 51.659 48.24 62.91 72.027 50.264 72.027\"/><rect class=\"cls-2\" x=\"12\" y=\"48.2405\" width=\"11.039\" height=\"23.7861\"/><rect id=\"square-2\" data-name=\"square\" class=\"cls-2\" x=\"12\" y=\"22.7471\" width=\"11.039\" height=\"11.0383\"/><rect id=\"square-3\" data-name=\"square\" class=\"cls-2\" x=\"12\" y=\"35.4942\" width=\"11.039\" height=\"11.0383\"/></g><g id=\"REALE\"><rect id=\"square-4\" data-name=\"square\" class=\"cls-2\" x=\"77.3243\" y=\"10\" width=\"11.039\" height=\"11.0383\"/><rect class=\"cls-2\" x=\"90.0718\" y=\"10\" width=\"33.3457\" height=\"11.0383\"/><rect class=\"cls-2\" x=\"90.0718\" y=\"60.9883\" width=\"33.3457\" height=\"11.0383\"/><rect class=\"cls-2\" x=\"90.0718\" y=\"35.4942\" width=\"22.7228\" height=\"11.0383\"/><rect id=\"square-5\" data-name=\"square\" class=\"cls-2\" x=\"77.3243\" y=\"22.7471\" width=\"11.039\" height=\"11.0383\"/><rect id=\"square-6\" data-name=\"square\" class=\"cls-2\" x=\"77.3243\" y=\"48.2405\" width=\"11.039\" height=\"11.039\"/><rect id=\"square-7\" data-name=\"square\" class=\"cls-2\" x=\"77.3243\" y=\"35.4942\" width=\"11.039\" height=\"11.0383\"/><rect id=\"square-8\" data-name=\"square\" class=\"cls-2\" x=\"77.3243\" y=\"60.9883\" width=\"11.039\" height=\"11.0383\"/></g><g id=\"REALA\"><polygon class=\"cls-2\" points=\"150.097 59.191 154.2 48.556 168.268 48.556 172.368 59.191 150.097 59.191\"/><polygon class=\"cls-2\" points=\"179.146 72.027 174.817 60.906 187.362 60.906 191.843 72.027 179.146 72.027\"/><polygon class=\"cls-2\" points=\"169.442 46.847 162.155 27.942 168.492 11.518 182.131 46.847 169.442 46.847\"/><polygon class=\"cls-2\" points=\"174.199 59.191 170.1 48.556 182.791 48.556 186.897 59.191 174.199 59.191\"/><polygon class=\"cls-2\" points=\"130.608 72.027 134.909 60.898 147.614 60.877 143.313 72.027 130.608 72.027\"/><polygon class=\"cls-2\" points=\"140.339 46.847 153.993 11.518 160.323 27.943 153.029 46.847 140.339 46.847\"/><polygon class=\"cls-2\" points=\"135.569 59.191 139.679 48.556 152.369 48.556 148.266 59.191 135.569 59.191\"/><polygon class=\"cls-2\" points=\"155.239 10 167.246 10 161.239 25.568 155.239 10\"/></g><g id=\"REALL\"><rect class=\"cls-2\" x=\"212.6536\" y=\"60.9883\" width=\"20.599\" height=\"11.0383\"/><rect class=\"cls-2\" x=\"199.9069\" y=\"22.7471\" width=\"11.0383\" height=\"36.5324\"/><rect id=\"square-9\" data-name=\"square\" class=\"cls-2\" x=\"199.9069\" y=\"60.9883\" width=\"11.0383\" height=\"11.0383\"/><rect id=\"square-10\" data-name=\"square\" class=\"cls-2\" x=\"234.961\" y=\"60.9883\" width=\"11.039\" height=\"11.0383\"/><rect id=\"square-11\" data-name=\"square\" class=\"cls-2\" x=\"199.9069\" y=\"10\" width=\"11.0383\" height=\"11.0383\"/></g><g id=\"LIFEL\"><rect class=\"cls-2\" x=\"24.7471\" y=\"133.6015\" width=\"20.599\" height=\"11.0383\"/><rect class=\"cls-2\" x=\"12\" y=\"95.3603\" width=\"11.039\" height=\"36.5324\"/><rect id=\"square-12\" data-name=\"square\" class=\"cls-2\" x=\"12\" y=\"133.6015\" width=\"11.039\" height=\"11.0383\"/><rect id=\"square-13\" data-name=\"square\" class=\"cls-2\" x=\"47.0549\" y=\"133.6015\" width=\"11.0383\" height=\"11.0383\"/><rect id=\"square-14\" data-name=\"square\" class=\"cls-2\" x=\"12\" y=\"82.6132\" width=\"11.039\" height=\"11.0383\"/></g><g id=\"LIFEI\"><rect class=\"cls-2\" x=\"66.5601\" y=\"95.3603\" width=\"11.0383\" height=\"36.5324\"/><rect id=\"square-15\" data-name=\"square\" class=\"cls-2\" x=\"66.5601\" y=\"133.6015\" width=\"11.0383\" height=\"11.0383\"/><rect id=\"square-16\" data-name=\"square\" class=\"cls-2\" x=\"66.5601\" y=\"82.6132\" width=\"11.0383\" height=\"11.0383\"/></g><g id=\"LIFEF\"><rect id=\"square-17\" data-name=\"square\" class=\"cls-2\" x=\"91.6573\" y=\"82.6132\" width=\"11.0375\" height=\"11.0383\"/><rect class=\"cls-2\" x=\"104.4036\" y=\"82.6132\" width=\"33.3468\" height=\"11.0383\"/><rect class=\"cls-2\" x=\"104.4036\" y=\"108.1073\" width=\"22.724\" height=\"11.0383\"/><rect id=\"square-18\" data-name=\"square\" class=\"cls-2\" x=\"91.6573\" y=\"95.3603\" width=\"11.0375\" height=\"11.0383\"/><rect class=\"cls-2\" x=\"91.6573\" y=\"120.8536\" width=\"11.0375\" height=\"23.7861\"/><rect id=\"square-19\" data-name=\"square\" class=\"cls-2\" x=\"91.6573\" y=\"108.1073\" width=\"11.0375\" height=\"11.0383\"/></g><g id=\"LIFEE\"><rect id=\"square-20\" data-name=\"square\" class=\"cls-2\" x=\"147.493\" y=\"82.6132\" width=\"11.0383\" height=\"11.0383\"/><rect class=\"cls-2\" x=\"160.2401\" y=\"82.6132\" width=\"33.346\" height=\"11.0383\"/><rect class=\"cls-2\" x=\"160.2401\" y=\"133.6015\" width=\"33.346\" height=\"11.0383\"/><rect class=\"cls-2\" x=\"160.2401\" y=\"108.1073\" width=\"22.724\" height=\"11.0383\"/><rect id=\"square-21\" data-name=\"square\" class=\"cls-2\" x=\"147.493\" y=\"120.8536\" width=\"11.0383\" height=\"11.039\"/><rect id=\"square-22\" data-name=\"square\" class=\"cls-2\" x=\"147.493\" y=\"95.3603\" width=\"11.0383\" height=\"11.0383\"/><rect id=\"square-23\" data-name=\"square\" class=\"cls-2\" x=\"147.493\" y=\"108.1073\" width=\"11.0383\" height=\"11.0383\"/><rect id=\"square-24\" data-name=\"square\" class=\"cls-2\" x=\"147.493\" y=\"133.6015\" width=\"11.0383\" height=\"11.0383\"/></g></svg>\n\t\t</div>\n\t\t<a href=\"/\" class=\"overlayLink\" data-event-category=\"Main Logo\" data-event-action=\"click\">Home</a>\n\t</div>\n</header>\n<div class=\"mobile-menu\">\n\t<span></span>\n\t<a href=\"/\" class=\"sub-logo\" data-event-category=\"Sidebar Logo\" data-event-action=\"click\"><small class=\"pupil\"></small>Real Life</a>\n\t<div class=\"menu-bg\"></div>\n</div>\n<div class=\"nav-wrap courier-standard\">\n\t<a href=\"/\" class=\"sub-logo sub-logo-desktop\" data-event-category=\"Sidebar Logo\" data-event-action=\"click\"><span class=\"pupil\"></span>Real Life</a>\n\t<nav>\n\t\t<a class=\"aboutToggle\" href=\"/about\"><span>About</span></a>\n\t\t<a class=\"dispatchToggle\" href=\"/newsletter/\"><span>Newsletter</span></a>\n\t\t<a class=\"topicsToggle\" href=\"/topics\"><span>Topics</span></a>\n\t\t<a class=\"contributorsToggle\" href=\"/contributors\"><span>Contributors</span></a>\n\t\t<a class=\"searchToggle\" href=\"/search/\"><span>Search</span></a>\n\t</nav>\n\t</div>\n<div class=\"nav-btm\"></div>\n\n<main data-barba=\"container\" data-barba-namespace=\"single\"><div id=\"main\" role=\"main\"><article class=\"clearfix instapaper_body \" data-autobg=\"https://reallifemag.com/wp-content/uploads/2020/02/tabitarezaire-150x150.jpg\">\n\t\t\t<div class=\"article-head\">\n\t\t\t\t<h1>Fair Warning</h1><div class=\"dek\"><p>For as long as there has been AI research, there have been credible critiques about the risks of AI boosterism</p>\n</div>\n\t\t\t\t\t<div class=\"byline-wrap\">\n\t\t\t\t\t\t<p><span class=\"author\"><a data-event-category=\"Author Name\" data-event-action=\"click\" href=\"/contributors/abeba-birhane\">\n\t\t\t\t\t\t\t\tAbeba Birhane\n\t\t\t\t\t\t\t\t</a></span> <span class=\"date\">February 24, 2020</span> <span class=\"share-toggle\">share</span></p>\n\t\t\t\t\t</div></div><div class=\"featured-image \"><img src=\"https://reallifemag.com/wp-content/uploads/2020/02/tabitarezaire-1024x569.jpg\"><div class=\"featured-image-caption\"><p>Image: <em>Premium Connect</em> (2017) by <a href=\"https://www.tabitarezaire.com/\">Tabita Rezaire</a>. Courtesy of the artist and Goodman Gallery, South Africa.</p></div></div>\n\t\t\t<div class=\"article-body\"><p>The tech industry currently holds unprecedented power and influence. Its companies have reached vast market capitalizations, employing hundreds of thousands of workers, and reshaping a range of other industries to accommodate its prerogatives, when it doesn’t absorb them outright. Its international political reach is expanding, as companies are <a href=\"https://www.theguardian.com/technology/2018/jul/11/facebook-fined-for-data-breaches-in-cambridge-analytica-scandal\">intervening in political elections</a> and <a href=\"https://www.vox.com/2019/1/23/18194328/google-amazon-facebook-lobby-record\">influencing policy and regulations</a> through millions of dollars spent lobbying. On the academic front, work in tech-related fields such as artificial intelligence and machine learning secures funding with relative ease, its merit and necessity seemingly taken for granted. Directly or indirectly, tech companies continue to invest handsomely in creating an attractive image of the industry, the hackers behind the code, and the technologization of society in general.</p>\n<p>What emerges from this is a portrait of technology as inevitable progress that must, despite its inevitability, be fully embraced without hesitation. For the tech evangelist, artificial intelligence research is self-evidently necessary, the next triumph in the ever-rising pyramid of human achievement and progress. In this discourse, AI allows humans to surpass their own limitations, biases and prejudices. This view prefers to imagine worst-case scenarios in science-fiction terms: Will AI take over humanity? Will we ever create sentient machines, and if so should we give it rights on a par with human beings? These First World armchair <a href=\"https://arxiv.org/abs/2001.05046\">contemplations</a>, far removed from the current concrete harms, preoccupy those that supposedly examine the moral dimensions of AI.</p>\n<blockquote class=\"pull-quote left-pull instapaper_ignore\">\n<p>Computer scientists (then and now) shared the fantasy that human thought could be treated as entirely computable</p>\n</blockquote>\n<p>The truth, however, is that the tech industry hardly concerns itself with human welfare and justice. Its practices have been starkly opposed to protecting the welfare of society’s most vulnerable, whether it’s <a href=\"https://thenextweb.com/tech/2019/06/25/googles-staff-banned-from-protesting-youtube-under-the-companys-banner/\">prohibiting employees from protesting against exploitation of the LGBT community</a>, or <a href=\"https://www.propublica.org/article/facebook-hate-speech-censorship-internal-documents-algorithms\">protecting white men (but not black children) from hate speech</a>, or <a href=\"https://www.businessinsider.com/amazon-defends-worker-cage-patent-that-was-recently-unearthed-2018-9?r=US&amp;IR=T\">treating its low-paid workers poorly</a>, or <a href=\"https://www.theguardian.com/technology/2019/mar/02/facebook-global-lobbying-campaign-against-data-privacy-laws-investment\">spending millions of dollars lobbying against regulations</a> that protect the disfranchised and vulnerable, or <a href=\"https://www.newsbusters.org/blogs/techwatch/alexander-hall/2019/02/11/google-apple-allow-saudi-sharia-law-wife-tracking-app\">developing such despicable technologies as a “wife tracking app</a>.”</p>\n<p>This is not a matter of the industry becoming more conservative as it has assumed more power and has more to gain from preserving the status quo. For all its celebration of disruption and innovation, the tech industry has always tended to serve existing power relations. In <a href=\"http://tech.mit.edu/V105/N16/weisen.16n.html\">a 1985 interview</a> with<em> The Tech</em>, an MIT News service, Joseph Weizenbaum — the computer scientist who developed ELIZA, the first chatbot in 1964 — pointed out that “the computer has from the beginning been a fundamentally conservative force … a force which kept power or even solidified power where it already existed.” In place of fundamental social changes, the computer allows technical solutions to be proposed that would allow existing power hierarchies to remain intact.</p>\n<p>Weizenbaum had recognized this pattern decades earlier. During the 1950s, he helped design the first computer banking systems in the U.S. for Bank of America, computerizing the banking process as the banks faced rapid growth. He saw first-hand how the introduction of computers allowed such institutions and their priorities to remain pretty much as they were rather than consider social change, decentralization, or some other form of change to address the rapid growth.</p>\n<p>But Weizenbaum’s turn toward critique started with the reception of ELIZA, which he built to imitate Rogerian therapy (an approach that often relies on mirroring patients’ statements back to them). Although he was explicit that ELIZA had nothing to do with psychotherapy, others, such as Stanford psychiatrist Kenneth Colby, hailed it as a first step toward finding a potential substitute for psychiatrists. Weizenbaum’s colleagues, who supposedly had a sophisticated understanding of computers, enormously exaggerated ELIZA’s capabilities, with some arguing that it <em>understood</em> language. And people interacting with ELIZA, he discovered, would open their heart to it. He would later write in his book <em>Computer Power and Human Reason: From Judgement to Calculation</em> (1976) that he was “startled to see how quickly and how very deeply people conversing with ELIZA became emotionally involved with the computer and how unequivocally they anthropomorphized it.” He would ultimately criticize the artificial intelligence project as “a fraud that played on the trusting instincts of people.”</p>\n<p>Computer scientists then (and now) shared the fantasy that human thought could be treated as entirely computable, but in <em>Computer Power and Human Reason</em>, Weizenbaum insisted on crucial differences between humans and machines, arguing that there are certain domains that involve interpersonal connection, respect, affection, and understanding into which computers ought not to intrude, regardless of whether it appears they can. “No other organism, and certainly no computer, can be made to confront genuine human problems in human terms,” he wrote.</p>\n<p>Upon its advent at the 1956 Dartmouth Workshop, “artificial intelligence” was conceived as a project tasked with developing a model of the human mind. Key figures such as John McCarthy, Marvin Minsky, and Claude Shannon, now considered the pioneers of AI, attended the conference and played a central role in developing AI as an academic field. Inspired by the idea of the Turing Machine and enabled by computer programing, a machine to simulate human intelligence seemed a natural next step. But as the AI project has progressed, it has gradually become less about simulating the human mind and more about creating financial empires.</p>\n<p>From the beginning, this enterprise has been epitomized by the attitude that the hacker behind the code can produce a solution for any given problem, that in fact he (yes, always a he) alone is capable of doing so. This eventually paved the way to the hacker culture that ultimately spawned the likes of Bill Gates, Jeff Bezos, and Mark Zuckerberg — “the Know-It-Alls,” as journalist Noam Cohen labeled them in 2017 in his <a href=\"https://oneworld-publications.com/the-know-it-alls.html\">book</a> about Silicon Valley’s rise to political prominence. Although the boundaries between AI as a model of the mind and AI as surveillance tools are blurry in the current state of the field, there is no question that AI is a tool for profit maximization.</p>\n<p>Weizenbaum, initially part of the project to simulate human thought, came to see that approach as resting on a gross misunderstanding of humans as mere “information processing systems,” and began to warn against the “artificial intelligentsia” promoting that agenda. In <em>Computer Power and Human Reason, </em>Weizenbaum insists that “humans and computers are not species of the same genus,” since humans “face problems no machine could possibly be made to face. Although we process information, we do not do it the way that computers do.” Even to ask the question, he argues, of “whether a computer has captured the essence of human reason is a diversion, if not a trap, because the real question — do humans understand the essence of humans? — cannot be answered or resolved by technology.”</p>\n<hr />\n<p>Beyond being skeptical about the prospects for an “intelligent machine,” Weizenbaum also recognized how computers were beginning to be invoked as an easy way out of complex, contingent, and multifaceted challenges. This attitude — now widespread— was particularly evident in the education field. In the 1985 interview with <em>The Tech</em>, Weizenbaum was asked about the benefits of having computers in the classroom. He promptly dismissed the question as wrongheaded and “upside-down,” loaded with unwarranted assumptions. If bettering education is at stake, Weizenbaum replies, then the question should begin with “what education should accomplish and what the priorities should be” and not “how computers can be used in the classroom.”</p>\n<blockquote class=\"pull-quote left-pull instapaper_ignore\">\n<p>As the AI project progressed, it has gradually become less about simulating the human mind and more about creating a financial empire</p>\n</blockquote>\n<p>Once the emphasis is shifted to educational goals, a different set of more far-reaching questions is necessarily raised, about how and why schools fail to address these priorities. Among the reasons such questioning might uncover are students coming to school hungry or coming from a milieu in which reading is regarded as irrelevant to the concrete problems of survival. We might then ask, why is there so much poverty in our world, especially in large cities, and even in supposedly prosperous countries like the U.S.? Why is it that classes are so large? Why are fully half the science and math teachers in the U.S. underqualified and operating on emergency certificates? These questions, Weizenbaum argues, would reveal that “education has a very much lower priority in the United States than do a great many other things, most particularly the military.” The issue is not a shortfall of technology in education but a host of contingent factors, including an ever-widening systemic inequality.</p>\n<p>But rather than confront these “ugly social realities,” Weizenbaum says, “it is much nicer, it is much more comfortable, to have some device, say the computer, with which to flood the schools with, and then to sit back and say, ‘You see, we are doing something about it.’” Bringing the computer into the classroom deludes us into thinking that we have solved a problem when in fact we are hiding it and misplacing its root causes.</p>\n<p>Today, this flawed approach of turning to computational tools such as software, algorithms, and apps has become default thinking across Western society and increasingly in the Global South. In the education field alone, computers and other surveillance tools are put forward as a solution to <a href=\"https://www.washingtonpost.com/gdpr-consent/?destination=%2ftechnology%2f2019%2f12%2f24%2fcolleges-are-turning-students-phones-into-surveillance-machines-tracking-locations-hundreds-thousands%2f%3f\">the student dropout crisis</a>, to tackling the <a href=\"https://spectrum.ieee.org/the-human-os/biomedical/devices/ai-tracks-emotions-in-the-classroom%255\">supposed lack of student attentiveness</a>, and to the <a href=\"https://aeon.co/essays/children-learn-best-when-engaged-in-the-living-world-not-on-screens\">pervasive attitude</a> that <a href=\"https://researched.org.uk/challenging-the-education-is-broken-and-silicon-valley-narratives/\">aggressively pushes the computer</a> as an inevitable part of learning. Implementing these technologies bypasses confronting ugly social realities — the financial challenges and extracurricular workloads that deplete students’ attention or lead to their dropping out. These factors could be better understood not through more surveillance and pervasive tech but by actually talking to the students directly, as well as approaching the challenges they face as multifaceted and structural.</p>\n<p>But when one is steeped deep in tech-solutionism discourse, the first step of consulting those at the receiving end of some technology is not so obvious. It might even seem irrelevant. The AI field continues to be marked by utopic visions and immense optimism, and discussions of moral responsibility and structural power dynamics are taxing — tiring even for the dedicated humanitarian. In the face of optimism, “potential,” and excitement, continually pointing out negative impacts is rarely rewarded. In fact, at times, such work can be perceived as a threat to corporations, which results in punishment, retaliation, or suppression of dissenting voices. It is far easier to preach progress and get behind the AI bandwagon or be a “game-changer.” The challenging work of examining inaccuracies, harms, false claims, and promises, on the other hand, casts one as a Luddite. Yet back in 1985 Weizenbaum was already arguing that &#8220;it is not reasonable for a scientist or technologist to insist that he or she does not know — or can not know — how [the technology they are creating] is going to be used.&#8221;</p>\n<hr />\n<p>Many of the “problems” in the social sphere are moving targets, challenges that require continual negotiations, revisions, and iterations – not static and neat problems that we can “solve” once and for all. This attitude is so well engrained within the computational enterprise, a field built on “solving problems,” that every messy social situation is packaged into a neat “problem &#8211;&gt; solution” approach. In the process, challenges that cannot be formulated into neat “problems” are either left behind or stripped off their rich complexities.</p>\n<p>In 1972, in <a href=\"https://www.jstor.org/stable/1734465?seq=1\">an article</a> for <em>Science</em><em>,</em> Weizenbaum called attention to how the AI field masked its fundamental conservatism with a blend of optimistic cheerleading and pragmatic fatalism. This could be found in “the structure of the typical essay on ‘The impact of computers on society,’” of which he offered this description:</p>\n<blockquote><p>First there is an &#8220;on the one hand&#8221; statement. It tells all the good things computers have already done for society and often even attempts to argue that the social order would already have collapsed were it not for the &#8220;computer revolution.&#8221; This is usually followed by an &#8220;on the other hand&#8221; caution which tells of certain problems the introduction of computers brings in its wake. The threat posed to individual privacy by large data banks and the danger of large-scale unemployment induced by industrial automation are usually mentioned. Finally, the glorious present and prospective achievements of the computer are applauded, while the dangers alluded to in the second part are shown to be capable of being alleviated by sophisticated technological fixes. The closing paragraph consists of a plea for generous societal support for more, and more large-scale, computer research and development. This is usually coupled to the more or less subtle assertion that only computer science, hence only the computer scientist, can guard the world against the admittedly hazardous fallout of applied computer technology.</p></blockquote>\n<p>This same pattern persists in many articles about emerging technologies: The potential achievements are applauded, while the dangers are regarded as further proof that the technology is desperately needed, along with more generous societal support.</p>\n<p>Today, the rhetorical pattern that Weizenbaum decried looks like this among AI’s current boosters: A machine learning model is put forward as doing something better than humans can or offering a computational shortcut to a complex challenge. It receives unprecedented praise and coverage from technologists and journalists alike. Then critics will begin to call attention to flaws, gross simplification, inaccuracies, methodological problems, or limitations of the data sets. In almost all machine-learning models deployed within the social sphere, the accuracy of the proposed solution will be shown to be grossly inflated, as well as harmful and discriminatory in some cases. Individuals who experience that discrimination <a href=\"https://twitter.com/CatHallam1/status/1114590857397788673?s=20\">will</a> <a href=\"https://twitter.com/EmilyEAckerman/status/1186363305851576321?s=20\">take</a> to social media. In some cases in fields such as medicine, neuroscience, or psychology, where the model is providing “cutting-edge” solution, historians will point out how the particular technological approach revives long discredited and pseudoscientific practices like <a href=\"https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a\">phrenology</a> or <a href=\"https://jme.bmj.com/content/40/11/725\">eugenics</a>. Domain-specific experts (be it in medicine, social care, cognitive science) will expose the lack of nuanced understanding of the problem.</p>\n<blockquote class=\"pull-quote left-pull instapaper_ignore\">\n<p>Although the boundaries between AI as a model of the mind and AI as surveillance tools are blurry, a tool for &#8220;profit maximization&#8221; captures current AI</p>\n</blockquote>\n<p>But the outrage and calls for caution and critical assessment will be drowned out with promotion of the next great state-of-the-art tech “invention,” the cry for emphasis on the potential such tech holds, and championing of further technological solutions for problems brought about by the previous tech solutions in the first place.</p>\n<p>Among the standard justifications for developing and deploying harmful technology is the claim of their inevitability: <em>It’s going to be developed by someone, so it might as well be me.</em> <a href=\"https://www.bloomberg.com/opinion/articles/2017-09-25/-gaydar-shows-how-creepy-algorithms-can-get\">See</a>, <a href=\"https://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477\">for example</a>, the reasons offered by the researchers who tried to develop algorithms to identify sexual orientation. In his 1985 interview, Weizenbaum rejected such reasoning as absurd, claiming it is like saying, “it is a fact that women will be raped every day and if I don’t do it, someone else will so it might as well be me.”</p>\n<p>Another justification is to dismiss the limitations, problems, and harms as minor issues compared with the advantages and potential. When confronted, for example, with the knowledge that data brokers and tech companies collect huge amounts of data on us, apologists may try to dismiss it as <em>just</em> a matter of targeted ads. How bad can it be? You can always ignore them. This may be true if one is in a privileged, nonmarginalized position. But “targeted ads” have deeper and more insidious consequences for those without such privileges — for some, “targeted ads” mean <a href=\"https://www.theverge.com/2019/2/1/18205174/automation-background-check-criminal-records-corelogic\">unjust exclusion from housing</a>, <a href=\"https://er.educause.edu/articles/2017/7/pedagogy-and-the-logic-of-platforms\">education</a>, or <a href=\"https://www.vice.com/en_us/article/59x79k/researchers-find-facebook-ad-targeting-algorithm-is-inherently-biased\">job opportunities</a>.</p>\n<p>Adopting an AI or machine-learning “solution” rather than a more comprehensive approach to social issues remains widespread. It can be seen in “<a href=\"https://aiforsocialgood.github.io/neurips2019/accepted/track3/pdfs/67_aisg_neurips2019.pdf\">technology for social good</a>” initiatives, which reduce intricate geo-sociopolitical and cultural challenges to formal code and prioritize technological solutions. It is also evident in automated decision-making in regard to welfare systems (as Virginia Eubanks <a href=\"https://books.google.ie/books?hl=en&amp;lr=&amp;id=pn4pDwAAQBAJ&amp;oi=fnd&amp;pg=PP10&amp;dq=Virginia+Eubanks+(2018)+Automating+Inequality:&amp;ots=gD4JMcfxpj&amp;sig=Tc98TvT2OObvcWpAhFGwbnMXvUY&amp;redir_esc=y#v=onepage&amp;q=Virginia%20Eubanks%20(2018)%20Automating%20Inequality%3A&amp;f=false\">has detailed</a>), which are interwoven with infinite contingent socioeconomic factors which are either seen as inconsequential or totally ignored. It is seen in <a href=\"https://www.medicalnewstoday.com/articles/319920.php#1\">algorithmic</a> <a href=\"https://www.samaritans.org/ireland/about-samaritans/research-policy/internet-suicide/samaritans-radar/\">approaches</a> to mental health issues, which require the utmost sensitivity and delicacy rather than unilateral interventions and gross simplification of nuances and contexts. And it is becoming an integral <a href=\"http://news.met.police.uk/news/met-begins-operational-use-of-live-facial-recognition-lfr-technology-392451\">part of criminal justice systems and policing</a>. It is so prevalent that some view it as a legitimate intervention into geopolitical conflicts (like <a href=\"https://www.bbc.com/news/uk-northern-ireland-47497836\">this one</a> over the border in Northern Ireland), a substitute for political will and decades of negotiation.</p>\n<p>Technology has become the almighty hammer to bash every conceivable nail with. And even when its overinflated capabilities, limitations, and harms are brought to the fore — the injustice it perpetuates; the protections and privacies it erodes — the response is not to confront ugly social realities and ask meaningful questions such as “Is this piece of tech needed in the first place?” Rather, what often happens is a call for more data, further tech, and the pledge to highlight the potential for good, just as Weizenbaum had noted in 1972. Any notice of the limits of technological solutions is followed by a “plea for generous societal support for more, and large-scale, computer research development.”</p>\n<p>The arguments about technology in this essay are not new, but history has shown they still need to be reemphasized and reiterated. Fortunately, in the light of the repeated exposure of Silicon Valley’s insidious motives and unprecedented power, individual people, especially black women, (both from within and outside) continue to challenge powerful tech empires. You’ll have read this piece nodding your head in agreement — if you got this far. However, the pattern that Weizenbaum described persists. The points raised by this piece and the call for caution and critical engagement may very well disappear into the background, replaced by hype for the new exciting and state-of-the-art tech that will appear tomorrow.</p>\n</div><div class=\"contributor-description\"><p><a href=\"https://twitter.com/Abebab\">Abeba Birhane</a> is a PhD candidate in Cognitive Science at University College Dublin. Her interdisciplinary research, which intersects between embodied cognition, digital technology studies, and critical data science, explores the dynamic and reciprocal relationships between individuals, society and digital technologies. She is a contributor to <a href=\"https://aeon.co/ideas/descartes-was-wrong-a-person-is-a-person-through-other-persons\">Aeon Magazine</a> and <a href=\"https://abebabirhane.wordpress.com/\">blogs</a> regularly about cognition, AI, ethics and data science.</p>\n</div></article></div></main><canvas class=\"blurCanvas\" id=\"blurCanvas\" width=\"200\" height=\"200\" data-canvas></canvas>\n<div class=\"blur-wrap\"></div>\n<div class=\"moire-wrap\"></div>\n<script type='text/javascript'>\n/* <![CDATA[ */\nvar cbVariable = \"46700a0b293bc8b76c35157a51a569cd\";\n/* ]]> */\n</script>\n<script type=\"text/javascript\" src=\"//code.jquery.com/jquery-3.5.1.min.js?ver=3.5.1\" id=\"jquery-js\"></script>\n<script type=\"text/javascript\" src=\"https://reallifemag.com/wp-content/themes/reallife2/js/vendor/stackblur.js?v=0.01&amp;ver=6.7.1\" id=\"stackblur-js\"></script>\n<script type=\"text/javascript\" src=\"https://reallifemag.com/wp-content/themes/reallife2/js/vendor-prod.js?v=0.001&amp;ver=6.7.1\" id=\"scriptsVendor-js\"></script>\n<script type=\"text/javascript\" src=\"https://reallifemag.com/wp-content/themes/reallife2/js/vendor/gsap-public/minified/gsap.min.js?v=0.001&amp;ver=6.7.1\" id=\"gsap-js\"></script>\n<script type=\"text/javascript\" src=\"https://reallifemag.com/wp-content/themes/reallife2/js/app.js?v=0.002&amp;ver=6.7.1\" id=\"scriptsJS-js\"></script>\n<link rel='stylesheet' id='classic-theme-styles-css' href='https://reallifemag.com/wp-includes/css/classic-themes.min.css?ver=6.7.1' type='text/css' media='all' />\n\n</body>\n</html>","oembed":false,"readabilityObject":{"title":"Fair Warning — Real Life","content":"<div id=\"readability-page-1\" class=\"page\"><div><p>The tech industry currently holds unprecedented power and influence. Its companies have reached vast market capitalizations, employing hundreds of thousands of workers, and reshaping a range of other industries to accommodate its prerogatives, when it doesn’t absorb them outright. Its international political reach is expanding, as companies are <a href=\"https://www.theguardian.com/technology/2018/jul/11/facebook-fined-for-data-breaches-in-cambridge-analytica-scandal\">intervening in political elections</a> and <a href=\"https://www.vox.com/2019/1/23/18194328/google-amazon-facebook-lobby-record\">influencing policy and regulations</a> through millions of dollars spent lobbying. On the academic front, work in tech-related fields such as artificial intelligence and machine learning secures funding with relative ease, its merit and necessity seemingly taken for granted. Directly or indirectly, tech companies continue to invest handsomely in creating an attractive image of the industry, the hackers behind the code, and the technologization of society in general.</p>\n<p>What emerges from this is a portrait of technology as inevitable progress that must, despite its inevitability, be fully embraced without hesitation. For the tech evangelist, artificial intelligence research is self-evidently necessary, the next triumph in the ever-rising pyramid of human achievement and progress. In this discourse, AI allows humans to surpass their own limitations, biases and prejudices. This view prefers to imagine worst-case scenarios in science-fiction terms: Will AI take over humanity? Will we ever create sentient machines, and if so should we give it rights on a par with human beings? These First World armchair <a href=\"https://arxiv.org/abs/2001.05046\">contemplations</a>, far removed from the current concrete harms, preoccupy those that supposedly examine the moral dimensions of AI.</p>\n<blockquote>\n<p>Computer scientists (then and now) shared the fantasy that human thought could be treated as entirely computable</p>\n</blockquote>\n<p>The truth, however, is that the tech industry hardly concerns itself with human welfare and justice. Its practices have been starkly opposed to protecting the welfare of society’s most vulnerable, whether it’s <a href=\"https://thenextweb.com/tech/2019/06/25/googles-staff-banned-from-protesting-youtube-under-the-companys-banner/\">prohibiting employees from protesting against exploitation of the LGBT community</a>, or <a href=\"https://www.propublica.org/article/facebook-hate-speech-censorship-internal-documents-algorithms\">protecting white men (but not black children) from hate speech</a>, or <a href=\"https://www.businessinsider.com/amazon-defends-worker-cage-patent-that-was-recently-unearthed-2018-9?r=US&amp;IR=T\">treating its low-paid workers poorly</a>, or <a href=\"https://www.theguardian.com/technology/2019/mar/02/facebook-global-lobbying-campaign-against-data-privacy-laws-investment\">spending millions of dollars lobbying against regulations</a> that protect the disfranchised and vulnerable, or <a href=\"https://www.newsbusters.org/blogs/techwatch/alexander-hall/2019/02/11/google-apple-allow-saudi-sharia-law-wife-tracking-app\">developing such despicable technologies as a “wife tracking app</a>.”</p>\n<p>This is not a matter of the industry becoming more conservative as it has assumed more power and has more to gain from preserving the status quo. For all its celebration of disruption and innovation, the tech industry has always tended to serve existing power relations. In <a href=\"http://tech.mit.edu/V105/N16/weisen.16n.html\">a 1985 interview</a> with<em> The Tech</em>, an MIT News service, Joseph Weizenbaum — the computer scientist who developed ELIZA, the first chatbot in 1964 — pointed out that “the computer has from the beginning been a fundamentally conservative force … a force which kept power or even solidified power where it already existed.” In place of fundamental social changes, the computer allows technical solutions to be proposed that would allow existing power hierarchies to remain intact.</p>\n<p>Weizenbaum had recognized this pattern decades earlier. During the 1950s, he helped design the first computer banking systems in the U.S. for Bank of America, computerizing the banking process as the banks faced rapid growth. He saw first-hand how the introduction of computers allowed such institutions and their priorities to remain pretty much as they were rather than consider social change, decentralization, or some other form of change to address the rapid growth.</p>\n<p>But Weizenbaum’s turn toward critique started with the reception of ELIZA, which he built to imitate Rogerian therapy (an approach that often relies on mirroring patients’ statements back to them). Although he was explicit that ELIZA had nothing to do with psychotherapy, others, such as Stanford psychiatrist Kenneth Colby, hailed it as a first step toward finding a potential substitute for psychiatrists. Weizenbaum’s colleagues, who supposedly had a sophisticated understanding of computers, enormously exaggerated ELIZA’s capabilities, with some arguing that it <em>understood</em> language. And people interacting with ELIZA, he discovered, would open their heart to it. He would later write in his book <em>Computer Power and Human Reason: From Judgement to Calculation</em> (1976) that he was “startled to see how quickly and how very deeply people conversing with ELIZA became emotionally involved with the computer and how unequivocally they anthropomorphized it.” He would ultimately criticize the artificial intelligence project as “a fraud that played on the trusting instincts of people.”</p>\n<p>Computer scientists then (and now) shared the fantasy that human thought could be treated as entirely computable, but in <em>Computer Power and Human Reason</em>, Weizenbaum insisted on crucial differences between humans and machines, arguing that there are certain domains that involve interpersonal connection, respect, affection, and understanding into which computers ought not to intrude, regardless of whether it appears they can. “No other organism, and certainly no computer, can be made to confront genuine human problems in human terms,” he wrote.</p>\n<p>Upon its advent at the 1956 Dartmouth Workshop, “artificial intelligence” was conceived as a project tasked with developing a model of the human mind. Key figures such as John McCarthy, Marvin Minsky, and Claude Shannon, now considered the pioneers of AI, attended the conference and played a central role in developing AI as an academic field. Inspired by the idea of the Turing Machine and enabled by computer programing, a machine to simulate human intelligence seemed a natural next step. But as the AI project has progressed, it has gradually become less about simulating the human mind and more about creating financial empires.</p>\n<p>From the beginning, this enterprise has been epitomized by the attitude that the hacker behind the code can produce a solution for any given problem, that in fact he (yes, always a he) alone is capable of doing so. This eventually paved the way to the hacker culture that ultimately spawned the likes of Bill Gates, Jeff Bezos, and Mark Zuckerberg — “the Know-It-Alls,” as journalist Noam Cohen labeled them in 2017 in his <a href=\"https://oneworld-publications.com/the-know-it-alls.html\">book</a> about Silicon Valley’s rise to political prominence. Although the boundaries between AI as a model of the mind and AI as surveillance tools are blurry in the current state of the field, there is no question that AI is a tool for profit maximization.</p>\n<p>Weizenbaum, initially part of the project to simulate human thought, came to see that approach as resting on a gross misunderstanding of humans as mere “information processing systems,” and began to warn against the “artificial intelligentsia” promoting that agenda. In <em>Computer Power and Human Reason, </em>Weizenbaum insists that “humans and computers are not species of the same genus,” since humans “face problems no machine could possibly be made to face. Although we process information, we do not do it the way that computers do.” Even to ask the question, he argues, of “whether a computer has captured the essence of human reason is a diversion, if not a trap, because the real question — do humans understand the essence of humans? —&nbsp;cannot be answered or resolved by technology.”</p>\n<hr>\n<p>Beyond being skeptical about the prospects for an “intelligent machine,” Weizenbaum also recognized how computers were beginning to be invoked as an easy way out of complex, contingent, and multifaceted challenges. This attitude — now widespread—&nbsp;was particularly evident in the education field. In the 1985 interview with <em>The Tech</em>, Weizenbaum was asked about the benefits of having computers in the classroom. He promptly dismissed the question as wrongheaded and “upside-down,” loaded with unwarranted assumptions. If bettering education is at stake, Weizenbaum replies, then the question should begin with “what education should accomplish and what the priorities should be” and not “how computers can be used in the classroom.”</p>\n<blockquote>\n<p>As the AI project progressed, it has gradually become less about simulating the human mind and more about creating a financial empire</p>\n</blockquote>\n<p>Once the emphasis is shifted to educational goals, a different set of more far-reaching questions is necessarily raised, about how and why schools fail to address these priorities. Among the reasons such questioning might uncover are students coming to school hungry or coming from a milieu in which reading is regarded as irrelevant to the concrete problems of survival. We might then ask, why is there so much poverty in our world, especially in large cities, and even in supposedly prosperous countries like the U.S.? Why is it that classes are so large? Why are fully half the science and math teachers in the U.S. underqualified and operating on emergency certificates? These questions, Weizenbaum argues, would reveal that “education has a very much lower priority in the United States than do a great many other things, most particularly the military.” The issue is not a shortfall of technology in education but a host of contingent factors, including an ever-widening systemic inequality.</p>\n<p>But rather than confront these “ugly social realities,” Weizenbaum says, “it is much nicer, it is much more comfortable, to have some device, say the computer, with which to flood the schools with, and then to sit back and say, ‘You see, we are doing something about it.’” Bringing the computer into the classroom deludes us into thinking that we have solved a problem when in fact we are hiding it and misplacing its root causes.</p>\n<p>Today, this flawed approach of turning to computational tools such as software, algorithms, and apps has become default thinking across Western society and increasingly in the Global South. In the education field alone, computers and other surveillance tools are put forward as a solution to <a href=\"https://www.washingtonpost.com/gdpr-consent/?destination=%2ftechnology%2f2019%2f12%2f24%2fcolleges-are-turning-students-phones-into-surveillance-machines-tracking-locations-hundreds-thousands%2f%3f\">the student dropout crisis</a>, to tackling the <a href=\"https://spectrum.ieee.org/the-human-os/biomedical/devices/ai-tracks-emotions-in-the-classroom%255\">supposed lack of student attentiveness</a>, and to the <a href=\"https://aeon.co/essays/children-learn-best-when-engaged-in-the-living-world-not-on-screens\">pervasive attitude</a> that <a href=\"https://researched.org.uk/challenging-the-education-is-broken-and-silicon-valley-narratives/\">aggressively pushes the computer</a> as an inevitable part of learning. Implementing these technologies bypasses confronting ugly social realities — the financial challenges and extracurricular workloads that deplete students’ attention or lead to their dropping out. These factors could be better understood not through more surveillance and pervasive tech but by actually talking to the students directly, as well as approaching the challenges they face as multifaceted and structural.</p>\n<p>But when one is steeped deep in tech-solutionism discourse, the first step of consulting those at the receiving end of some technology is not so obvious. It might even seem irrelevant. The AI field continues to be marked by utopic visions and immense optimism, and discussions of moral responsibility and structural power dynamics are taxing — tiring even for the dedicated humanitarian. In the face of optimism, “potential,” and excitement, continually pointing out negative impacts is rarely rewarded. In fact, at times, such work can be perceived as a threat to corporations, which results in punishment, retaliation, or suppression of dissenting voices. It is far easier to preach progress and get behind the AI bandwagon or be a “game-changer.” The challenging work of examining inaccuracies, harms, false claims, and promises, on the other hand, casts one as a Luddite. Yet back in 1985 Weizenbaum was already arguing that “it is not reasonable for a scientist or technologist to insist that he or she does not know — or can not know — how [the technology they are creating] is going to be used.”</p>\n<hr>\n<p>Many of the “problems” in the social sphere are moving targets, challenges that require continual negotiations, revisions, and iterations – not static and neat problems that we can “solve” once and for all. This attitude is so well engrained within the computational enterprise, a field built on “solving problems,” that every messy social situation is packaged into a neat “problem –&gt; solution” approach. In the process, challenges that cannot be formulated into neat “problems” are either left behind or stripped off their rich complexities.</p>\n<p>In 1972, in <a href=\"https://www.jstor.org/stable/1734465?seq=1\">an article</a> for <em>Science</em><em>,</em> Weizenbaum called attention to how the AI field masked its fundamental conservatism with a blend of optimistic cheerleading and pragmatic fatalism. This could be found in “the structure of the typical essay on ‘The impact of computers on society,’” of which he offered this description:</p>\n<blockquote><p>First there is an “on the one hand” statement. It tells all the good things computers have already done for society and often even attempts to argue that the social order would already have collapsed were it not for the “computer revolution.” This is usually followed by an “on the other hand” caution which tells of certain problems the introduction of computers brings in its wake. The threat posed to individual privacy by large data banks and the danger of large-scale unemployment induced by industrial automation are usually mentioned. Finally, the glorious present and prospective achievements of the computer are applauded, while the dangers alluded to in the second part are shown to be capable of being alleviated by sophisticated technological fixes. The closing paragraph consists of a plea for generous societal support for more, and more large-scale, computer research and development. This is usually coupled to the more or less subtle assertion that only computer science, hence only the computer scientist, can guard the world against the admittedly hazardous fallout of applied computer technology.</p></blockquote>\n<p>This same pattern persists in many articles about emerging technologies: The potential achievements are applauded, while the dangers are regarded as further proof that the technology is desperately needed, along with more generous societal support.</p>\n<p>Today, the rhetorical pattern that Weizenbaum decried looks like this among AI’s current boosters: A machine learning model is put forward as doing something better than humans can or offering a computational shortcut to a complex challenge. It receives unprecedented praise and coverage from technologists and journalists alike. Then critics will begin to call attention to flaws, gross simplification, inaccuracies, methodological problems, or limitations of the data sets. In almost all machine-learning models deployed within the social sphere, the accuracy of the proposed solution will be shown to be grossly inflated, as well as harmful and discriminatory in some cases. Individuals who experience that discrimination <a href=\"https://twitter.com/CatHallam1/status/1114590857397788673?s=20\">will</a> <a href=\"https://twitter.com/EmilyEAckerman/status/1186363305851576321?s=20\">take</a> to social media. In some cases in fields such as medicine, neuroscience, or psychology, where the model is providing “cutting-edge” solution, historians will point out how the particular technological approach revives long discredited and pseudoscientific practices like <a href=\"https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a\">phrenology</a> or <a href=\"https://jme.bmj.com/content/40/11/725\">eugenics</a>. Domain-specific experts (be it in medicine, social care, cognitive science) will expose the lack of nuanced understanding of the problem.</p>\n<blockquote>\n<p>Although the boundaries between AI as a model of the mind and AI as surveillance tools are blurry, a tool for “profit maximization” captures current AI</p>\n</blockquote>\n<p>But the outrage and calls for caution and critical assessment will be drowned out with promotion of the next great state-of-the-art tech “invention,” the cry for emphasis on the potential such tech holds, and championing of further technological solutions for problems brought about by the previous tech solutions in the first place.</p>\n<p>Among the standard justifications for developing and deploying harmful technology is the claim of their inevitability: <em>It’s going to be developed by someone, so it might as well be me.</em> <a href=\"https://www.bloomberg.com/opinion/articles/2017-09-25/-gaydar-shows-how-creepy-algorithms-can-get\">See</a>, <a href=\"https://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477\">for example</a>, the reasons offered by the researchers who tried to develop algorithms to identify sexual orientation. In his 1985 interview, Weizenbaum rejected such reasoning as absurd, claiming it is like saying, “it is a fact that women will be raped every day and if I don’t do it, someone else will so it might as well be me.”</p>\n<p>Another justification is to dismiss the limitations, problems, and harms as minor issues compared with the advantages and potential. When confronted, for example, with the knowledge that data brokers and tech companies collect huge amounts of data on us, apologists may try to dismiss it as <em>just</em> a matter of targeted ads. How bad can it be? You can always ignore them. This may be true if one is in a privileged, nonmarginalized position. But “targeted ads” have deeper and more insidious consequences for those without such privileges — for some, “targeted ads” mean <a href=\"https://www.theverge.com/2019/2/1/18205174/automation-background-check-criminal-records-corelogic\">unjust exclusion from housing</a>, <a href=\"https://er.educause.edu/articles/2017/7/pedagogy-and-the-logic-of-platforms\">education</a>, or <a href=\"https://www.vice.com/en_us/article/59x79k/researchers-find-facebook-ad-targeting-algorithm-is-inherently-biased\">job opportunities</a>.</p>\n<p>Adopting an AI or machine-learning “solution” rather than a more comprehensive approach to social issues remains widespread. It can be seen in “<a href=\"https://aiforsocialgood.github.io/neurips2019/accepted/track3/pdfs/67_aisg_neurips2019.pdf\">technology for social good</a>” initiatives, which reduce intricate geo-sociopolitical and cultural challenges to formal code and prioritize technological solutions. It is also evident in automated decision-making in regard to welfare systems (as Virginia Eubanks <a href=\"https://books.google.ie/books?hl=en&amp;lr=&amp;id=pn4pDwAAQBAJ&amp;oi=fnd&amp;pg=PP10&amp;dq=Virginia+Eubanks+(2018)+Automating+Inequality:&amp;ots=gD4JMcfxpj&amp;sig=Tc98TvT2OObvcWpAhFGwbnMXvUY&amp;redir_esc=y#v=onepage&amp;q=Virginia%20Eubanks%20(2018)%20Automating%20Inequality%3A&amp;f=false\">has detailed</a>), which are interwoven with infinite contingent socioeconomic factors which are either seen as inconsequential or totally ignored. It is seen in <a href=\"https://www.medicalnewstoday.com/articles/319920.php#1\">algorithmic</a> <a href=\"https://www.samaritans.org/ireland/about-samaritans/research-policy/internet-suicide/samaritans-radar/\">approaches</a> to mental health issues, which require the utmost sensitivity and delicacy rather than unilateral interventions and gross simplification of nuances and contexts. And it is becoming an integral <a href=\"http://news.met.police.uk/news/met-begins-operational-use-of-live-facial-recognition-lfr-technology-392451\">part of criminal justice systems and policing</a>. It is so prevalent that some view it as a legitimate intervention into geopolitical conflicts (like <a href=\"https://www.bbc.com/news/uk-northern-ireland-47497836\">this one</a> over the border in Northern Ireland), a substitute for political will and decades of negotiation.</p>\n<p>Technology has become the almighty hammer to bash every conceivable nail with. And even when its overinflated capabilities, limitations, and harms are brought to the fore —&nbsp;the injustice it perpetuates; the protections and privacies it erodes — the response is not to confront ugly social realities and ask meaningful questions such as “Is this piece of tech needed in the first place?” Rather, what often happens is a call for more data, further tech, and the pledge to highlight the potential for good, just as Weizenbaum had noted in 1972. Any notice of the limits of technological solutions is followed by a “plea for generous societal support for more, and large-scale, computer research development.”</p>\n<p>The arguments about technology in this essay are not new, but history has shown they still need to be reemphasized and reiterated. Fortunately, in the light of the repeated exposure of Silicon Valley’s insidious motives and unprecedented power, individual people, especially black women, (both from within and outside) continue to challenge powerful tech empires. You’ll have read this piece nodding your head in agreement — if you got this far. However, the pattern that Weizenbaum described persists. The points raised by this piece and the call for caution and critical engagement may very well disappear into the background, replaced by hype for the new exciting and state-of-the-art tech that will appear tomorrow.</p>\n</div><p><a href=\"https://twitter.com/Abebab\">Abeba Birhane</a> is a PhD candidate in Cognitive Science at University College Dublin. Her interdisciplinary research, which intersects between embodied cognition, digital technology studies, and critical data science, explores the dynamic and reciprocal relationships between individuals, society and digital technologies. She is a contributor to <a href=\"https://aeon.co/ideas/descartes-was-wrong-a-person-is-a-person-through-other-persons\">Aeon Magazine</a> and <a href=\"https://abebabirhane.wordpress.com/\">blogs</a> regularly about cognition, AI, ethics and data science.</p></div>","textContent":"The tech industry currently holds unprecedented power and influence. Its companies have reached vast market capitalizations, employing hundreds of thousands of workers, and reshaping a range of other industries to accommodate its prerogatives, when it doesn’t absorb them outright. Its international political reach is expanding, as companies are intervening in political elections and influencing policy and regulations through millions of dollars spent lobbying. On the academic front, work in tech-related fields such as artificial intelligence and machine learning secures funding with relative ease, its merit and necessity seemingly taken for granted. Directly or indirectly, tech companies continue to invest handsomely in creating an attractive image of the industry, the hackers behind the code, and the technologization of society in general.\nWhat emerges from this is a portrait of technology as inevitable progress that must, despite its inevitability, be fully embraced without hesitation. For the tech evangelist, artificial intelligence research is self-evidently necessary, the next triumph in the ever-rising pyramid of human achievement and progress. In this discourse, AI allows humans to surpass their own limitations, biases and prejudices. This view prefers to imagine worst-case scenarios in science-fiction terms: Will AI take over humanity? Will we ever create sentient machines, and if so should we give it rights on a par with human beings? These First World armchair contemplations, far removed from the current concrete harms, preoccupy those that supposedly examine the moral dimensions of AI.\n\nComputer scientists (then and now) shared the fantasy that human thought could be treated as entirely computable\n\nThe truth, however, is that the tech industry hardly concerns itself with human welfare and justice. Its practices have been starkly opposed to protecting the welfare of society’s most vulnerable, whether it’s prohibiting employees from protesting against exploitation of the LGBT community, or protecting white men (but not black children) from hate speech, or treating its low-paid workers poorly, or spending millions of dollars lobbying against regulations that protect the disfranchised and vulnerable, or developing such despicable technologies as a “wife tracking app.”\nThis is not a matter of the industry becoming more conservative as it has assumed more power and has more to gain from preserving the status quo. For all its celebration of disruption and innovation, the tech industry has always tended to serve existing power relations. In a 1985 interview with The Tech, an MIT News service, Joseph Weizenbaum — the computer scientist who developed ELIZA, the first chatbot in 1964 — pointed out that “the computer has from the beginning been a fundamentally conservative force … a force which kept power or even solidified power where it already existed.” In place of fundamental social changes, the computer allows technical solutions to be proposed that would allow existing power hierarchies to remain intact.\nWeizenbaum had recognized this pattern decades earlier. During the 1950s, he helped design the first computer banking systems in the U.S. for Bank of America, computerizing the banking process as the banks faced rapid growth. He saw first-hand how the introduction of computers allowed such institutions and their priorities to remain pretty much as they were rather than consider social change, decentralization, or some other form of change to address the rapid growth.\nBut Weizenbaum’s turn toward critique started with the reception of ELIZA, which he built to imitate Rogerian therapy (an approach that often relies on mirroring patients’ statements back to them). Although he was explicit that ELIZA had nothing to do with psychotherapy, others, such as Stanford psychiatrist Kenneth Colby, hailed it as a first step toward finding a potential substitute for psychiatrists. Weizenbaum’s colleagues, who supposedly had a sophisticated understanding of computers, enormously exaggerated ELIZA’s capabilities, with some arguing that it understood language. And people interacting with ELIZA, he discovered, would open their heart to it. He would later write in his book Computer Power and Human Reason: From Judgement to Calculation (1976) that he was “startled to see how quickly and how very deeply people conversing with ELIZA became emotionally involved with the computer and how unequivocally they anthropomorphized it.” He would ultimately criticize the artificial intelligence project as “a fraud that played on the trusting instincts of people.”\nComputer scientists then (and now) shared the fantasy that human thought could be treated as entirely computable, but in Computer Power and Human Reason, Weizenbaum insisted on crucial differences between humans and machines, arguing that there are certain domains that involve interpersonal connection, respect, affection, and understanding into which computers ought not to intrude, regardless of whether it appears they can. “No other organism, and certainly no computer, can be made to confront genuine human problems in human terms,” he wrote.\nUpon its advent at the 1956 Dartmouth Workshop, “artificial intelligence” was conceived as a project tasked with developing a model of the human mind. Key figures such as John McCarthy, Marvin Minsky, and Claude Shannon, now considered the pioneers of AI, attended the conference and played a central role in developing AI as an academic field. Inspired by the idea of the Turing Machine and enabled by computer programing, a machine to simulate human intelligence seemed a natural next step. But as the AI project has progressed, it has gradually become less about simulating the human mind and more about creating financial empires.\nFrom the beginning, this enterprise has been epitomized by the attitude that the hacker behind the code can produce a solution for any given problem, that in fact he (yes, always a he) alone is capable of doing so. This eventually paved the way to the hacker culture that ultimately spawned the likes of Bill Gates, Jeff Bezos, and Mark Zuckerberg — “the Know-It-Alls,” as journalist Noam Cohen labeled them in 2017 in his book about Silicon Valley’s rise to political prominence. Although the boundaries between AI as a model of the mind and AI as surveillance tools are blurry in the current state of the field, there is no question that AI is a tool for profit maximization.\nWeizenbaum, initially part of the project to simulate human thought, came to see that approach as resting on a gross misunderstanding of humans as mere “information processing systems,” and began to warn against the “artificial intelligentsia” promoting that agenda. In Computer Power and Human Reason, Weizenbaum insists that “humans and computers are not species of the same genus,” since humans “face problems no machine could possibly be made to face. Although we process information, we do not do it the way that computers do.” Even to ask the question, he argues, of “whether a computer has captured the essence of human reason is a diversion, if not a trap, because the real question — do humans understand the essence of humans? — cannot be answered or resolved by technology.”\n\nBeyond being skeptical about the prospects for an “intelligent machine,” Weizenbaum also recognized how computers were beginning to be invoked as an easy way out of complex, contingent, and multifaceted challenges. This attitude — now widespread— was particularly evident in the education field. In the 1985 interview with The Tech, Weizenbaum was asked about the benefits of having computers in the classroom. He promptly dismissed the question as wrongheaded and “upside-down,” loaded with unwarranted assumptions. If bettering education is at stake, Weizenbaum replies, then the question should begin with “what education should accomplish and what the priorities should be” and not “how computers can be used in the classroom.”\n\nAs the AI project progressed, it has gradually become less about simulating the human mind and more about creating a financial empire\n\nOnce the emphasis is shifted to educational goals, a different set of more far-reaching questions is necessarily raised, about how and why schools fail to address these priorities. Among the reasons such questioning might uncover are students coming to school hungry or coming from a milieu in which reading is regarded as irrelevant to the concrete problems of survival. We might then ask, why is there so much poverty in our world, especially in large cities, and even in supposedly prosperous countries like the U.S.? Why is it that classes are so large? Why are fully half the science and math teachers in the U.S. underqualified and operating on emergency certificates? These questions, Weizenbaum argues, would reveal that “education has a very much lower priority in the United States than do a great many other things, most particularly the military.” The issue is not a shortfall of technology in education but a host of contingent factors, including an ever-widening systemic inequality.\nBut rather than confront these “ugly social realities,” Weizenbaum says, “it is much nicer, it is much more comfortable, to have some device, say the computer, with which to flood the schools with, and then to sit back and say, ‘You see, we are doing something about it.’” Bringing the computer into the classroom deludes us into thinking that we have solved a problem when in fact we are hiding it and misplacing its root causes.\nToday, this flawed approach of turning to computational tools such as software, algorithms, and apps has become default thinking across Western society and increasingly in the Global South. In the education field alone, computers and other surveillance tools are put forward as a solution to the student dropout crisis, to tackling the supposed lack of student attentiveness, and to the pervasive attitude that aggressively pushes the computer as an inevitable part of learning. Implementing these technologies bypasses confronting ugly social realities — the financial challenges and extracurricular workloads that deplete students’ attention or lead to their dropping out. These factors could be better understood not through more surveillance and pervasive tech but by actually talking to the students directly, as well as approaching the challenges they face as multifaceted and structural.\nBut when one is steeped deep in tech-solutionism discourse, the first step of consulting those at the receiving end of some technology is not so obvious. It might even seem irrelevant. The AI field continues to be marked by utopic visions and immense optimism, and discussions of moral responsibility and structural power dynamics are taxing — tiring even for the dedicated humanitarian. In the face of optimism, “potential,” and excitement, continually pointing out negative impacts is rarely rewarded. In fact, at times, such work can be perceived as a threat to corporations, which results in punishment, retaliation, or suppression of dissenting voices. It is far easier to preach progress and get behind the AI bandwagon or be a “game-changer.” The challenging work of examining inaccuracies, harms, false claims, and promises, on the other hand, casts one as a Luddite. Yet back in 1985 Weizenbaum was already arguing that “it is not reasonable for a scientist or technologist to insist that he or she does not know — or can not know — how [the technology they are creating] is going to be used.”\n\nMany of the “problems” in the social sphere are moving targets, challenges that require continual negotiations, revisions, and iterations – not static and neat problems that we can “solve” once and for all. This attitude is so well engrained within the computational enterprise, a field built on “solving problems,” that every messy social situation is packaged into a neat “problem –> solution” approach. In the process, challenges that cannot be formulated into neat “problems” are either left behind or stripped off their rich complexities.\nIn 1972, in an article for Science, Weizenbaum called attention to how the AI field masked its fundamental conservatism with a blend of optimistic cheerleading and pragmatic fatalism. This could be found in “the structure of the typical essay on ‘The impact of computers on society,’” of which he offered this description:\nFirst there is an “on the one hand” statement. It tells all the good things computers have already done for society and often even attempts to argue that the social order would already have collapsed were it not for the “computer revolution.” This is usually followed by an “on the other hand” caution which tells of certain problems the introduction of computers brings in its wake. The threat posed to individual privacy by large data banks and the danger of large-scale unemployment induced by industrial automation are usually mentioned. Finally, the glorious present and prospective achievements of the computer are applauded, while the dangers alluded to in the second part are shown to be capable of being alleviated by sophisticated technological fixes. The closing paragraph consists of a plea for generous societal support for more, and more large-scale, computer research and development. This is usually coupled to the more or less subtle assertion that only computer science, hence only the computer scientist, can guard the world against the admittedly hazardous fallout of applied computer technology.\nThis same pattern persists in many articles about emerging technologies: The potential achievements are applauded, while the dangers are regarded as further proof that the technology is desperately needed, along with more generous societal support.\nToday, the rhetorical pattern that Weizenbaum decried looks like this among AI’s current boosters: A machine learning model is put forward as doing something better than humans can or offering a computational shortcut to a complex challenge. It receives unprecedented praise and coverage from technologists and journalists alike. Then critics will begin to call attention to flaws, gross simplification, inaccuracies, methodological problems, or limitations of the data sets. In almost all machine-learning models deployed within the social sphere, the accuracy of the proposed solution will be shown to be grossly inflated, as well as harmful and discriminatory in some cases. Individuals who experience that discrimination will take to social media. In some cases in fields such as medicine, neuroscience, or psychology, where the model is providing “cutting-edge” solution, historians will point out how the particular technological approach revives long discredited and pseudoscientific practices like phrenology or eugenics. Domain-specific experts (be it in medicine, social care, cognitive science) will expose the lack of nuanced understanding of the problem.\n\nAlthough the boundaries between AI as a model of the mind and AI as surveillance tools are blurry, a tool for “profit maximization” captures current AI\n\nBut the outrage and calls for caution and critical assessment will be drowned out with promotion of the next great state-of-the-art tech “invention,” the cry for emphasis on the potential such tech holds, and championing of further technological solutions for problems brought about by the previous tech solutions in the first place.\nAmong the standard justifications for developing and deploying harmful technology is the claim of their inevitability: It’s going to be developed by someone, so it might as well be me. See, for example, the reasons offered by the researchers who tried to develop algorithms to identify sexual orientation. In his 1985 interview, Weizenbaum rejected such reasoning as absurd, claiming it is like saying, “it is a fact that women will be raped every day and if I don’t do it, someone else will so it might as well be me.”\nAnother justification is to dismiss the limitations, problems, and harms as minor issues compared with the advantages and potential. When confronted, for example, with the knowledge that data brokers and tech companies collect huge amounts of data on us, apologists may try to dismiss it as just a matter of targeted ads. How bad can it be? You can always ignore them. This may be true if one is in a privileged, nonmarginalized position. But “targeted ads” have deeper and more insidious consequences for those without such privileges — for some, “targeted ads” mean unjust exclusion from housing, education, or job opportunities.\nAdopting an AI or machine-learning “solution” rather than a more comprehensive approach to social issues remains widespread. It can be seen in “technology for social good” initiatives, which reduce intricate geo-sociopolitical and cultural challenges to formal code and prioritize technological solutions. It is also evident in automated decision-making in regard to welfare systems (as Virginia Eubanks has detailed), which are interwoven with infinite contingent socioeconomic factors which are either seen as inconsequential or totally ignored. It is seen in algorithmic approaches to mental health issues, which require the utmost sensitivity and delicacy rather than unilateral interventions and gross simplification of nuances and contexts. And it is becoming an integral part of criminal justice systems and policing. It is so prevalent that some view it as a legitimate intervention into geopolitical conflicts (like this one over the border in Northern Ireland), a substitute for political will and decades of negotiation.\nTechnology has become the almighty hammer to bash every conceivable nail with. And even when its overinflated capabilities, limitations, and harms are brought to the fore — the injustice it perpetuates; the protections and privacies it erodes — the response is not to confront ugly social realities and ask meaningful questions such as “Is this piece of tech needed in the first place?” Rather, what often happens is a call for more data, further tech, and the pledge to highlight the potential for good, just as Weizenbaum had noted in 1972. Any notice of the limits of technological solutions is followed by a “plea for generous societal support for more, and large-scale, computer research development.”\nThe arguments about technology in this essay are not new, but history has shown they still need to be reemphasized and reiterated. Fortunately, in the light of the repeated exposure of Silicon Valley’s insidious motives and unprecedented power, individual people, especially black women, (both from within and outside) continue to challenge powerful tech empires. You’ll have read this piece nodding your head in agreement — if you got this far. However, the pattern that Weizenbaum described persists. The points raised by this piece and the call for caution and critical engagement may very well disappear into the background, replaced by hype for the new exciting and state-of-the-art tech that will appear tomorrow.\nAbeba Birhane is a PhD candidate in Cognitive Science at University College Dublin. Her interdisciplinary research, which intersects between embodied cognition, digital technology studies, and critical data science, explores the dynamic and reciprocal relationships between individuals, society and digital technologies. She is a contributor to Aeon Magazine and blogs regularly about cognition, AI, ethics and data science.","length":19491,"excerpt":"For as long as there has been AI research, there have been credible critiques about the risks of AI boosterism","byline":"Abeba Birhane\n\t\t\t\t\t\t\t\t February 24, 2020 share","dir":null,"siteName":"Real Life","lang":null},"finalizedMeta":{"title":"Fair Warning — Real Life","description":"For as long as there has been AI research, there have been credible critiques about the risks of AI boosterism","author":false,"creator":"","publisher":false,"date":"2025-05-10T20:11:09.217Z","topics":[]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"Fair Warning — Real Life","description":"For as long as there has been AI research, there have been credible critiques about the risks of AI boosterism","canonical":"https://reallifemag.com/fair-warning/","keywords":[],"image":"https://reallifemag.com/wp-content/uploads/2020/02/tabitarezaire-1024x569.jpg","firstParagraph":"For as long as there has been AI research, there have been credible critiques about the risks of AI boosterism"},"dublinCore":{},"opengraph":{"title":"Fair Warning — Real Life","description":"For as long as there has been AI research, there have been credible critiques about the risks of AI boosterism","url":"https://reallifemag.com/fair-warning/","site_name":"Real Life","locale":false,"type":"article","typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":"https://reallifemag.com/wp-content/uploads/2020/02/tabitarezaire-1024x569.jpg","image:width":"1024","image:height":"569"},"twitter":{"site":"@_reallifemag","description":"For as long as there has been AI research, there have been credible critiques about the risks of AI boosterism","card":"summary","creator":false,"title":"Fair Warning — Real Life","image":"https://reallifemag.com/wp-content/uploads/2020/02/tabitarezaire-1024x569.jpg"},"archivedData":{"link":"https://web.archive.org/web/20250510201115/https://reallifemag.com/fair-warning/","wayback":"https://web.archive.org/web/20250510201115/https://reallifemag.com/fair-warning/"}}}