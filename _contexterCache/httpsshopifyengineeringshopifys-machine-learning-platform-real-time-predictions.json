{"initialLink":"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions","sanitizedLink":"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions","finalLink":"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions\" itemprop=\"url\">Unlocking Real-time Predictions with Shopify's Machine Learning Platform</a></contexter-box-head></contexter-box-head><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2023-04-24T14:13:33.473Z\">4/24/2023</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>Learn how Shopify Data built new online inference capabilities into its Machine Learning Platform to deploy and serve models for real-time prediction at scale.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a is=\"contexter-link\" href=\"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"096b153715ce732a65eaa3816ea1389392471b30","data":{"originalLink":"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions","sanitizedLink":"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions","canonical":"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions","htmlText":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<!DOCTYPE html>\n<html class=\"no-js\" lang=\"en\">\n<head>\n  <meta charset=\"utf-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n    <link rel=\"preconnect\" href=\"https://cdn.shopify.com\" /><link rel=\"preconnect\" href=\"https://monorail-edge.shopifysvc.com\" /><link rel=\"preconnect\" href=\"https://www.google-analytics.com\" /><link rel=\"preconnect\" href=\"https://www.googletagmanager.com\" /><link rel=\"preconnect\" href=\"https://bat.bing.com\" /><link rel=\"preconnect\" href=\"https://www.facebook.com\" /><link rel=\"preconnect\" href=\"https://connect.facebook.net\" /><link rel=\"preconnect\" href=\"https://tags.tiqcdn.com\" />\n    <link rel=\"preload\" as=\"font\" crossorigin=\"anonymous\" type=\"font/woff2\" href=\"https://cdn.shopify.com/static/fonts/ShopifySans--regular.woff2\" /><link rel=\"preload\" as=\"font\" crossorigin=\"anonymous\" type=\"font/woff2\" href=\"https://cdn.shopify.com/static/fonts/ShopifySans--bold.woff2\" />\n  <link rel=\"icon\" type=\"image/png\" href=\"https://cdn.shopify.com/static/shopify-favicon.png\" />\n  <title>Unlocking Real-time Predictions with Shopify&#39;s Machine Learning Platform (2023)</title>\n  <meta name=\"description\" content=\"Learn how Shopify Data built new online inference capabilities into its Machine Learning Platform to deploy and serve models for real-time prediction at scale.\" />\n\n    <link rel=\"canonical\" href=\"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions\">\n\n\n  <script nonce=\"\">document.documentElement.classList.remove('no-js'); document.documentElement.classList.add('js')</script>\n    <meta name=\"twitter:label1\" content=\"Reading time\" />\n  <meta name=\"twitter:data1\" content=\"12 minutes\" />\n  <link rel=\"alternate\" type=\"application/atom+xml\" href=\"https://shopify.engineering/blog.atom\" />\n    <link rel=\"stylesheet\" href=\"https://cdn.shopify.com/shopifycloud/brochure/assets/standalone/blog/application-4483de23e2c39404e7eb55a4017f67cf106bf8cb19e6a9d4a0aacd6cefcbbacc.css\" media=\"all\" />\n  \n  <link rel=\"stylesheet\" href=\"https://cdn.shopify.com/shopifycloud/brochure/assets/manifests/engineering-blog-article-68e83d0b3f98df2fe1bb16f7d97fb532c412dd7782ce23b25a3eb2f76650672c.css\" />    <meta property=\"fb:app_id\" content=\"847460188612391\">\n\n  <meta property=\"fb:pages\" content=\"20409006880\">\n\n      <meta property=\"og:type\" content=\"article\" />\n    <meta property=\"og:site_name\" content=\"Shopify\" />\n    <meta property=\"og:title\" content=\"Unlocking Real-time Predictions with Shopify&#39;s Machine Learning Platform\" />\n    <meta property=\"og:description\" content=\"Learn how Shopify Data built new online inference capabilities into its Machine Learning Platform to deploy and serve models for real-time prediction at scale.\" />\n    <meta property=\"og:image\" content=\"https://cdn.shopify.com/s/files/1/0779/4361/articles/ShopifyEng_BlogIllustrations_220216_72ppi_03_TheMagicOfMerlin-ShopifysMachineLearningPlatform_3e07a5ed-cd94-44e2-b4ca-c6db1ec81bd6.jpg?v=1678451850\" />\n    <meta property=\"og:url\" content=\"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions\" />\n    <meta property=\"twitter:card\" content=\"summary_large_image\" />\n    <meta property=\"twitter:site\" content=\"@Shopify\" />\n    <meta property=\"twitter:account_id\" content=\"17136315\" />\n    <meta property=\"twitter:title\" content=\"Unlocking Real-time Predictions with Shopify&#39;s Machine Learning Platform\" />\n    <meta property=\"twitter:description\" content=\"Learn how Shopify Data built new online inference capabilities into its Machine Learning Platform to deploy and serve models for real-time prediction at scale.\" />\n    <meta property=\"twitter:image\" content=\"https://cdn.shopify.com/s/files/1/0779/4361/articles/ShopifyEng_BlogIllustrations_220216_72ppi_03_TheMagicOfMerlin-ShopifysMachineLearningPlatform_3e07a5ed-cd94-44e2-b4ca-c6db1ec81bd6.jpg?v=1678451850\" />\n\n    <meta name=\"google-site-verification\" content=\"N85Cq7pQ3Vyw32vBkUwEkVuV-cWDkVOtPqKZrpc_oeg\" />\n  <meta name=\"verify-a\" content=\"c56c27a8f68c6ddcafdf\">\n\n  <script type=\"text/javascript\">\n  function getCookie(cookieName) {\n    var regexp = new RegExp('(?:^|; )' + cookieName + '=([^;]*)(?:$|; )');\n    var match = document.cookie.match(regexp);\n    return match ? decodeURIComponent(match[1]) : \"\";\n  }\n\n  function getUrlParams(key){\n    var match = window.location.href.match('[?&]' + key + '=([^&]+)');\n    return match ? match[1] : \"\";\n  }\n\n  function checkMobile() {\n    testString = navigator.userAgent || navigator.vendor || window.opera;\n    regex = /(android|iphone|ipad|mobile|phone|mobi|blackberry)/i;\n    return regex.test(testString) ? \"mobile\" : \"desktop\";\n  }\n\n  var countryCode = \"US\";\n  var utag_data = {\n      \"affiliate\"                : \"\",\n      \"blog_id\"                  : \"7794361_557884178488\",\n      \"blog_category\"            : \"Data Science &amp; Engineering\",\n      \"canonical_url\"            : \"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions\",\n      \"compliance_zone\"          : \"none\",\n      \"continent_code\"           : \"NA\",\n      \"country_code\"             : countryCode,\n      \"currency_code\"            : \"USD\",\n      \"display_cookies_notice\"   : \"false\",\n      \"environment\"              : \"production\",\n      \"experiment_variation_id\"  : \"\",\n      \"http_code\"                : \"200\",\n      \"is_new_user\"              : \"\",\n      \"itcat\"                    : getUrlParams('itcat'),\n      \"itterm\"                   : getUrlParams('itterm'),\n      \"language\"                 : \"en\",\n      \"last_shop_domain\"         : getCookie('last_shop'),\n      \"last_shop_id\"             : \"\",\n      \"opt_in\"                   : \"\",\n      \"page_category\"            : \"\",\n      \"page_group\"               : \"\",\n      \"page_language\"            : \"en\",\n      \"page_name\"                : location.pathname,\n      \"page_subtopic\"            : \"\",\n      \"page_title\"               : document.title,\n      \"page_topic\"               : \"shopifys-machine-learning-platform-real-time-predictions\",\n      \"page_url\"                 : location.toString(),\n      \"page_variation\"           : getUrlParams('dest'),\n      \"project\"                  : \"Shopify\",\n      \"site_country_code\"        : \"US\",\n      \"site_domain\"              : \"shopify.engineering\",\n      \"site_display_format\"      : checkMobile(),\n      \"tealium_event\"            : \"page_view\",\n      \"user_language\"            : navigator.language,\n      \"user_token\"               : getCookie('_shopify_y'),\n      \"utm_campaign\"             : getUrlParams('utm_campaign'),\n      \"utm_content\"              : getUrlParams('utm_content'),\n      \"utm_medium\"               : getUrlParams('utm_medium'),\n      \"utm_source\"               : getUrlParams('utm_source'),\n      \"utm_term\"                 : getUrlParams('utm_term'),\n      \"path_prefix\"              : \"\"\n  };\n\n  var activeConsentNotice = false\n  var enableGtm = !(getUrlParams('debug_disable_gtm') === 'true');\n  var enableTealium = (getUrlParams('debug_enable_tealium') === 'true');\n  function loadGtm() {\n    if (!enableGtm) return;\n    w=window;\n    w.dataLayer = w.dataLayer || [];\n    w.dataLayer.push(utag_data);\n    w.dataLayer.push({\n      'gtm.start': new Date().getTime(),\n      event: 'gtm.js',\n    });\n\n    a='//www.googletagmanager.com/gtm.js?id=GTM-TZ26LP8&dl=dataLayer';\n    b=document;c='script';d=b.createElement(c);d.src=a;d.type='text/java'+c;d.async=true;\n    a=b.getElementsByTagName(c)[0];a.parentNode.insertBefore(d,a);\n  }\n\n  function loadTealium(){\n    if (!enableTealium) return;\n    a='//tags.tiqcdn.com/utag/shopify/main/prod/utag.js';\n    b=document;c='script';d=b.createElement(c);d.src=a;d.type='text/java'+c;d.async=true;\n    a=b.getElementsByTagName(c)[0];a.parentNode.insertBefore(d,a);\n  }\n\n  var waitForCmpApi = function () {\n    if (window.cmp && typeof window.cmp === 'function') {\n      const onConsentChanged = (props) => {\n        if (props && props.hasConsentedAll) {\n          loadGtm();\n          loadTealium();\n        }\n      }\n      cmp().updateOnChangeCallback(onConsentChanged);\n      onConsentChanged(cmp());\n    } else {\n      setTimeout(waitForCmpApi, 50);\n    }\n  };\n\n  if (activeConsentNotice) {\n    waitForCmpApi();\n  } else {\n    loadGtm();\n    loadTealium();\n  }\n</script>\n\n\n<!-- Load trekkie even if E2E tests is running because it controls A/B tests and\nduplicate elements can get rendered if it's not run -->\n<script id=\"TrekkieScript\" type=\"text/javascript\">\n  (function(){\n    function getCookie(cookieName) {\n      var regexp = new RegExp('(?:^|; )' + cookieName + '=([^;]*)(?:$|; )');\n      var match = document.cookie.match(regexp);\n      return match ? decodeURIComponent(match[1]) : \"\";\n    }\n\n    var config = {\n      'Trekkie': {\n        'appName': 'brochure',\n        'defaultAttributes': {\n          'application': 'brochure2-rails',\n          'locale': 'en',\n          'page_purpose': '',\n          'http_code': '200',\n          'page_english_url': 'https://www.shopify.com',\n          'env_name': 'production',\n          'user_language': navigator.language,\n          'page_name': '',\n          'env_currency': 'USD',\n          'experiment_variation_id': '',\n          'page_topic': 'shopifys-machine-learning-platform-real-time-predictions',\n          'page_subtopic': '',\n          'affiliate': getCookie('source'),\n          'page_owner': '',\n          'path_prefix': ''\n        },\n        'isServerSideCookieWritingEnabled': false\n      },\n      'CrossDomainTracking': {},\n    };\n\n    var trekkie_version = '2022.08.31';\n    var analytics = window.analytics = window.analytics || [];\n\n    if (analytics.integrations) {\n      return;\n    }\n\n    analytics.methods = [\n      'identify',\n      'page',\n      'ready',\n      'track',\n    ];\n    analytics.factory = function(method) {\n      return function() {\n        var args = Array.prototype.slice.call(arguments);\n        args.unshift(method);\n        analytics.push(args);\n        return analytics;\n      };\n    };\n    for (var i = 0; i < analytics.methods.length; i++) {\n      var key = analytics.methods[i];\n      analytics[key] = analytics.factory(key);\n    }\n    analytics.load = function(config) {\n      analytics.config = config;\n      var script = document.createElement('script');\n      script.type = 'text/javascript';\n      script.async = true;\n      script.src = 'https://cdn.shopify.com/s/javascripts/tricorder/trekkie.' + config.Trekkie.appName + '.min.js?v=' + trekkie_version;\n      var first = document.getElementsByTagName('script')[0];\n      first.parentNode.insertBefore(script, first);\n    };\n\n    analytics.ready(function(){\n      const logExperimentAssignment = (experiment, variant, start_date) => {\n        if (window.analytics.trekkie.defaultAttributes.microSessionId) {\n          window.analytics.track('monorail://experiments_dotcom_assign/3.0', {\n            uniqToken: window.analytics.trekkie.defaultAttributes.uniqToken,\n            visitToken: window.analytics.trekkie.defaultAttributes.visitToken,\n            microSessionId: window.analytics.trekkie.defaultAttributes.microSessionId,\n            page_url: window.location.href,\n            source_app: 'brochure2',\n            experiment_handle: experiment,\n            variation_name: variant,\n            experiment_start_at: start_date,\n          });\n        }\n      }\n\n    });\n\n    analytics.load(config);\n    analytics.page();\n  })();\n</script>\n\n\n\n  <link rel=\"alternate\" hreflang=\"en\" href=\"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions\" />\n</head>\n\n<body class=\"page--has-secondary-nav page--blog-show shopify-sans-supported-lang\">\n  \n  <div id=\"GlobalIconSymbols\"><svg xmlns=\"http://www.w3.org/2000/svg\"><symbol id=\"modules-email-reverse\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 81 63\"><path d=\"M40.386 29.514L81 8.152V6.784C81 3.05 77.83 0 73.352 0H8.468C3.988 0 0 3.05 0 6.784v1.702l40.386 21.028z\"/><path d=\"M40.4 35.232L0 14.197V56.35C0 60.08 3.988 63 8.468 63h64.885C77.83 63 81 60.08 81 56.35V13.874L40.4 35.232z\"/></svg></symbol><symbol id=\"modules-facebook\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"-46.4 101.9 241.4 429.1\"><path d=\"M195 182.4h-53.6c-8.1 0-26.8 12.7-26.8 26.8v53.6H195v80.5h-80.5V531H34V343.3h-80.4v-80.5H34v-53.6c0-59.1 52.6-107.3 109-107.3h52v80.5z\"/></svg></symbol><symbol id=\"modules-twitter\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"-152.9 139.6 347.8 282.7\"><path d=\"M194.9 173.1c-12.8 5.7-26.5 9.5-41 11.2 14.7-8.8 26-22.8 31.4-39.5-13.8 8.2-29.1 14.1-45.3 17.3-13-13.9-31.6-22.5-52.1-22.5-39.4 0-71.4 32-71.4 71.4 0 5.6.6 11 1.8 16.3-59.3-3-111.9-31.4-147.1-74.5-6.1 10.5-9.6 22.8-9.6 35.9 0 24.7 16.5 46.6 35.6 59.4-11.7-.4-28.5-3.6-28.5-9v.9c0 34.6 20.7 63.4 53.4 70-6 1.6-14.2 2.5-20.7 2.5-4.6 0-10-.5-14.4-1.3 9.1 28.4 35 49 66.2 49.6-24.4 19.1-55.5 30.5-88.9 30.5-5.8 0-11.6-.3-17.2-1 31.6 20.2 69.1 32 109.4 32 131.3 0 203.1-108.7 203.1-203 0-3.1-.1-6.2-.2-9.3 13.8-10 25.9-22.6 35.5-36.9z\"/></svg></symbol><symbol id=\"modules-linkedin\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"-137 109.5 316 316.1\"><path d=\"M-132.1 425.6h65.3V214.2h-65.3v211.4zm33-316.1c-21.1 0-37.9 16.9-37.9 37.9s16.9 37.9 37.9 37.9 37.9-16.9 37.9-37.9-16.8-37.9-37.9-37.9zm199.5 99.7c-31.6 0-53.4 17.6-61.8 34.4h-.7v-28.8h-62.5v210.7h65.3V320.9c0-27.4 4.9-54.1 39.3-54.1 33.7 0 33.7 31.6 33.7 56.2v102.6H179V309.7c0-56.9-11.9-100.5-78.6-100.5z\"/></svg></symbol><symbol id=\"spot-blog-post\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 45 45\"><path d=\"M35.61 19.88l-2.37-2.37 1.87-1.88L37.48 18l-1.87 1.88zM25.1 30.38l-2.37-2.37 8.38-8.37 2.37 2.37-8.38 8.37zm-4.8 2.43l.85-2.13 1.28 1.28-2.13.85zm20.37-15.87l-4.5-4.5a1.52 1.52 0 00-2.13 0l-4 4L19.6 26.89l-.06.06c-.02.02-.04.05-.06.07a1.3 1.3 0 00-.27.43l-.01.03-2.99 7.47c-.22.56-.09 1.19.33 1.62.29.28.68.44 1.07.44.18 0 .37-.04.55-.11l7.45-2.98.05-.02c.05-.02.1-.04.14-.07a.77.77 0 00.21-.14c.08-.04.15-.11.21-.18l10.4-10.4a.147.147 0 00.05-.04c.02-.02.04-.04.05-.06l3.95-3.95a1.5 1.5 0 000-2.12zm-21.13 10v.01l.06-.06-.06.05zm1.566-5.435h-8.5a1.5 1.5 0 110-3h8.5a1.5 1.5 0 010 3zm6.5-6h-15a1.5 1.5 0 110-3h15a1.5 1.5 0 010 3zm8.004-6.494v-3.01c0-1.65-1.35-3-3-3H7a3 3 0 00-3 3v33.01A2.99 2.99 0 006.99 42h25.62c1.65 0 3-1.35 3-3v-7.782a.999.999 0 00-1-.999h-1a.999.999 0 00-1 1V39L7 39.01V6h25.61v3.011a1 1 0 001 1h1.002a.999.999 0 00.999-1z\"/></svg></symbol><symbol id=\"spot-read\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 45 45\"><path d=\"M34 34.69V29h5.35L34 34.69zM23.5 35V9.63l1.65-.66c1.61-.64 3.31-.97 5.04-.97H40v18h-7.5c-.83 0-1.5.67-1.5 1.5V35h-7.5zm-3 0H5V8h8.81c1.73 0 3.43.33 5.04.97l1.65.66V35zM40 5h-9.81c-2.11 0-4.2.4-6.15 1.19L22 7l-2.04-.81C18.01 5.4 15.92 5 13.81 5H5C3.34 5 2 6.34 2 8v27c0 1.66 1.34 3 3 3h15.5v.5c0 .83.67 1.5 1.5 1.5s1.5-.67 1.5-1.5V38h10.21c.82 0 1.61-.34 2.18-.94l6.3-6.69c.52-.56.81-1.29.81-2.06V8c0-1.66-1.34-3-3-3z\"/></svg></symbol><symbol id=\"spot-boost\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 45 45\"><path d=\"M31 18h-3.59L26 41.59A1.5 1.5 0 0124.5 43h-.09A1.5 1.5 0 0123 41.41l1.5-25A1.5 1.5 0 0126 15h3l-7.56-9.68L14 15h3a1.5 1.5 0 011.5 1.41l1.5 25a1.5 1.5 0 11-3 .18L15.59 18H12a2 2 0 01-1.64-3.15L19.44 3a2.48 2.48 0 012-1 2.51 2.51 0 012 1l9.15 11.74a2 2 0 01.18 2.13A2 2 0 0131 18zm2.72 5A1.5 1.5 0 0032 24.24l-3 17A1.5 1.5 0 0030.24 43h.26a1.5 1.5 0 001.5-1.24l3-17A1.5 1.5 0 0033.76 23zM12 29.25a1.5 1.5 0 10-3 .49l2 12A1.5 1.5 0 0012.5 43h.25A1.5 1.5 0 0014 41.25z\"/></svg></symbol><symbol id=\"spot-quick\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 45 45\"><path d=\"M22.5 2A20.5 20.5 0 1043 22.5 20.52 20.52 0 0022.5 2zm0 38A17.5 17.5 0 1140 22.5 17.52 17.52 0 0122.5 40zm4.44-27.07l-3 10a1.5 1.5 0 01-.45.7l-8 7a1.5 1.5 0 01-2-2.26l7.68-6.72 2.88-9.59a1.5 1.5 0 112.87.86z\"/></svg></symbol><symbol id=\"modules-arrow-right\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 20 20\"><path d=\"M17.707 9.293l-5-5a1 1 0 00-1.414 1.414L14.586 9H3a1 1 0 100 2h11.586l-3.293 3.293a1 1 0 001.414 1.414l5-5a1 1 0 000-1.414z\"/></svg></symbol><symbol id=\"logos-shopify-white\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 608 173.7\"><path fill=\"#95BF47\" d=\"M130.7 32.9c-.1-.9-.9-1.3-1.5-1.4-.6-.1-12.6-.2-12.6-.2s-10.1-9.8-11.1-10.8-2.9-.7-3.7-.5c0 0-1.9.6-5.1 1.6-.5-1.7-1.3-3.8-2.4-5.9-3.6-6.9-8.8-10.5-15.2-10.5-.4 0-.9 0-1.3.1-.2-.2-.4-.4-.6-.7-2.8-3-6.3-4.4-10.5-4.3-8.2.2-16.3 6.1-23 16.7-4.7 7.4-8.2 16.7-9.2 23.9-9.4 2.9-16 4.9-16.1 5-4.7 1.5-4.9 1.6-5.5 6.1C12.4 55.3 0 151.4 0 151.4l104.1 18 45.1-11.2S130.8 33.7 130.7 32.9zm-39.2-9.7c-2.4.7-5.1 1.6-8.1 2.5-.1-4.1-.6-9.9-2.5-14.9 6.3 1.2 9.3 8.2 10.6 12.4zM78 27.4c-5.5 1.7-11.4 3.5-17.4 5.4 1.7-6.4 4.9-12.8 8.8-17 1.5-1.6 3.5-3.3 5.9-4.3 2.3 4.7 2.7 11.4 2.7 15.9zM66.8 5.8c1.9 0 3.5.4 4.9 1.3-2.2 1.1-4.4 2.8-6.4 5-5.2 5.6-9.2 14.2-10.8 22.6-5 1.5-9.8 3-14.3 4.4 3-13.2 14-32.9 26.6-33.3z\"/><path fill=\"#5E8E3E\" d=\"M129.2 31.5c-.6-.1-12.6-.2-12.6-.2s-10.1-9.8-11.1-10.8c-.4-.4-.9-.6-1.4-.6v149.5l45.1-11.2S130.8 33.8 130.7 32.9c-.2-.9-.9-1.3-1.5-1.4z\"/><path fill=\"#FFF\" d=\"M79.1 54.7l-5.2 19.6s-5.8-2.7-12.8-2.2c-10.2.6-10.3 7-10.2 8.7.6 8.8 23.6 10.7 24.9 31.2 1 16.2-8.6 27.2-22.4 28.1-16.6 1-25.7-8.7-25.7-8.7l3.5-14.9s9.2 6.9 16.5 6.5c4.8-.3 6.5-4.2 6.3-7-.7-11.4-19.5-10.8-20.7-29.5-1-15.8 9.4-31.8 32.3-33.3 9-.8 13.5 1.5 13.5 1.5zM210.3 96.5c-5.2-2.8-7.9-5.2-7.9-8.5 0-4.2 3.7-6.9 9.6-6.9 6.8 0 12.8 2.8 12.8 2.8l4.8-14.6s-4.4-3.4-17.3-3.4c-18 0-30.5 10.3-30.5 24.8 0 8.2 5.8 14.5 13.6 19 6.3 3.6 8.5 6.1 8.5 9.9 0 3.9-3.1 7-9 7-8.7 0-16.9-4.5-16.9-4.5l-5.1 14.6s7.6 5.1 20.3 5.1c18.5 0 31.8-9.1 31.8-25.5.1-8.9-6.6-15.2-14.7-19.8zm73.8-30.8c-9.1 0-16.3 4.3-21.8 10.9l-.3-.1 7.9-41.4h-20.6l-20 105.3h20.6l6.9-36c2.7-13.6 9.7-22 16.3-22 4.6 0 6.4 3.1 6.4 7.6 0 2.8-.3 6.3-.9 9.1l-7.8 41.2h20.6l8.1-42.6c.9-4.5 1.5-9.9 1.5-13.4 0-11.5-6.2-18.6-16.9-18.6zm63.5 0c-24.8 0-41.2 22.4-41.2 47.4 0 16 9.9 28.8 28.4 28.8 24.3 0 40.8-21.8 40.8-47.4-.1-14.7-8.8-28.8-28-28.8zm-10.2 60.4c-7 0-10-6-10-13.4 0-11.8 6.1-31.1 17.3-31.1 7.3 0 9.7 6.3 9.7 12.4 0 12.7-6.1 32.1-17 32.1zm90.8-60.4c-13.9 0-21.8 12.2-21.8 12.2h-.3l1.2-11.1h-18.2c-.9 7.5-2.5 18.8-4.2 27.3l-14.3 75.4h20.6l5.7-30.5h.4s4.2 2.7 12.1 2.7c24.2 0 40-24.8 40-49.9.1-13.7-6.1-26.1-21.2-26.1zm-19.7 60.7c-5.4 0-8.5-3-8.5-3l3.4-19.3c2.4-12.8 9.1-21.4 16.3-21.4 6.3 0 8.2 5.8 8.2 11.4 0 13.3-7.9 32.3-19.4 32.3zm70.4-90.2c-6.6 0-11.8 5.2-11.8 12 0 6.1 3.9 10.3 9.7 10.3h.3c6.4 0 12-4.3 12.1-12 0-6-4-10.3-10.3-10.3zm-28.8 104.2h20.6l14-73h-20.8zm87-73.2h-14.3l.7-3.4c1.2-7 5.4-13.3 12.2-13.3 3.7 0 6.6 1 6.6 1l4-16.1s-3.6-1.8-11.2-1.8c-7.3 0-14.6 2.1-20.2 6.9-7 6-10.3 14.6-12 23.3l-.6 3.4h-9.6l-3 15.5h9.6l-10.9 57.7H509l10.9-57.7h14.2l3-15.5zm49.6.2s-12.9 32.5-18.7 50.2h-.3c-.4-5.7-5.1-50.2-5.1-50.2H541l12.4 67.1c.3 1.5.1 2.4-.4 3.4-2.4 4.6-6.4 9.1-11.2 12.4-3.9 2.8-8.2 4.6-11.7 5.8l5.7 17.5c4.2-.9 12.8-4.3 20.2-11.2 9.4-8.8 18.1-22.4 27-40.9l25.2-54.1h-21.5z\"/></svg></symbol><symbol id=\"modules-nav-external-indicator\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 7 7\"><path d=\"M7 7V0H0l7 7z\"/></svg></symbol><symbol id=\"modules-mobile-hamburger\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 20 20\"><path d=\"M19 11H1a1 1 0 110-2h18a1 1 0 110 2zm0-7H1a1 1 0 110-2h18a1 1 0 110 2zm0 14H1a1 1 0 110-2h18a1 1 0 110 2z\"/></svg></symbol><symbol id=\"modules-nav-arrow-down\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 10 5\"><path d=\"M0 0l5 5 5-5H0z\"/></svg></symbol><symbol id=\"logos-shopify-black\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 608 173.7\"><path fill=\"#95BF47\" d=\"M130.7 32.9c-.1-.9-.9-1.3-1.5-1.4-.6-.1-12.6-.2-12.6-.2s-10.1-9.8-11.1-10.8-2.9-.7-3.7-.5c0 0-1.9.6-5.1 1.6-.5-1.7-1.3-3.8-2.4-5.9-3.6-6.9-8.8-10.5-15.2-10.5-.4 0-.9 0-1.3.1-.2-.2-.4-.4-.6-.7-2.8-3-6.3-4.4-10.5-4.3-8.2.2-16.3 6.1-23 16.7-4.7 7.4-8.2 16.7-9.2 23.9-9.4 2.9-16 4.9-16.1 5-4.7 1.5-4.9 1.6-5.5 6.1C12.4 55.3 0 151.4 0 151.4l104.1 18 45.1-11.2S130.8 33.7 130.7 32.9zm-39.2-9.7c-2.4.7-5.1 1.6-8.1 2.5-.1-4.1-.6-9.9-2.5-14.9 6.3 1.2 9.3 8.2 10.6 12.4zM78 27.4c-5.5 1.7-11.4 3.5-17.4 5.4 1.7-6.4 4.9-12.8 8.8-17 1.5-1.6 3.5-3.3 5.9-4.3 2.3 4.7 2.7 11.4 2.7 15.9zM66.8 5.8c1.9 0 3.5.4 4.9 1.3-2.2 1.1-4.4 2.8-6.4 5-5.2 5.6-9.2 14.2-10.8 22.6-5 1.5-9.8 3-14.3 4.4 3-13.2 14-32.9 26.6-33.3z\"/><path fill=\"#5E8E3E\" d=\"M129.2 31.5c-.6-.1-12.6-.2-12.6-.2s-10.1-9.8-11.1-10.8c-.4-.4-.9-.6-1.4-.6v149.5l45.1-11.2S130.8 33.8 130.7 32.9c-.2-.9-.9-1.3-1.5-1.4z\"/><path fill=\"#FFF\" d=\"M79.1 54.7l-5.2 19.6s-5.8-2.7-12.8-2.2c-10.2.6-10.3 7-10.2 8.7.6 8.8 23.6 10.7 24.9 31.2 1 16.2-8.6 27.2-22.4 28.1-16.6 1-25.7-8.7-25.7-8.7l3.5-14.9s9.2 6.9 16.5 6.5c4.8-.3 6.5-4.2 6.3-7-.7-11.4-19.5-10.8-20.7-29.5-1-15.8 9.4-31.8 32.3-33.3 9-.8 13.5 1.5 13.5 1.5z\"/><path d=\"M210.3 96.5c-5.2-2.8-7.9-5.2-7.9-8.5 0-4.2 3.7-6.9 9.6-6.9 6.8 0 12.8 2.8 12.8 2.8l4.8-14.6s-4.4-3.4-17.3-3.4c-18 0-30.5 10.3-30.5 24.8 0 8.2 5.8 14.5 13.6 19 6.3 3.6 8.5 6.1 8.5 9.9 0 3.9-3.1 7-9 7-8.7 0-16.9-4.5-16.9-4.5l-5.1 14.6s7.6 5.1 20.3 5.1c18.5 0 31.8-9.1 31.8-25.5.1-8.9-6.6-15.2-14.7-19.8zm73.8-30.8c-9.1 0-16.3 4.3-21.8 10.9l-.3-.1 7.9-41.4h-20.6l-20 105.3h20.6l6.9-36c2.7-13.6 9.7-22 16.3-22 4.6 0 6.4 3.1 6.4 7.6 0 2.8-.3 6.3-.9 9.1l-7.8 41.2h20.6l8.1-42.6c.9-4.5 1.5-9.9 1.5-13.4 0-11.5-6.2-18.6-16.9-18.6zm63.5 0c-24.8 0-41.2 22.4-41.2 47.4 0 16 9.9 28.8 28.4 28.8 24.3 0 40.8-21.8 40.8-47.4-.1-14.7-8.8-28.8-28-28.8zm-10.2 60.4c-7 0-10-6-10-13.4 0-11.8 6.1-31.1 17.3-31.1 7.3 0 9.7 6.3 9.7 12.4 0 12.7-6.1 32.1-17 32.1zm90.8-60.4c-13.9 0-21.8 12.2-21.8 12.2h-.3l1.2-11.1h-18.2c-.9 7.5-2.5 18.8-4.2 27.3l-14.3 75.4h20.6l5.7-30.5h.4s4.2 2.7 12.1 2.7c24.2 0 40-24.8 40-49.9.1-13.7-6.1-26.1-21.2-26.1zm-19.7 60.7c-5.4 0-8.5-3-8.5-3l3.4-19.3c2.4-12.8 9.1-21.4 16.3-21.4 6.3 0 8.2 5.8 8.2 11.4 0 13.3-7.9 32.3-19.4 32.3zm70.4-90.2c-6.6 0-11.8 5.2-11.8 12 0 6.1 3.9 10.3 9.7 10.3h.3c6.4 0 12-4.3 12.1-12 0-6-4-10.3-10.3-10.3zm-28.8 104.2h20.6l14-73h-20.8zm87-73.2h-14.3l.7-3.4c1.2-7 5.4-13.3 12.2-13.3 3.7 0 6.6 1 6.6 1l4-16.1s-3.6-1.8-11.2-1.8c-7.3 0-14.6 2.1-20.2 6.9-7 6-10.3 14.6-12 23.3l-.6 3.4h-9.6l-3 15.5h9.6l-10.9 57.7H509l10.9-57.7h14.2l3-15.5zm49.6.2s-12.9 32.5-18.7 50.2h-.3c-.4-5.7-5.1-50.2-5.1-50.2H541l12.4 67.1c.3 1.5.1 2.4-.4 3.4-2.4 4.6-6.4 9.1-11.2 12.4-3.9 2.8-8.2 4.6-11.7 5.8l5.7 17.5c4.2-.9 12.8-4.3 20.2-11.2 9.4-8.8 18.1-22.4 27-40.9l25.2-54.1h-21.5z\"/></svg></symbol><symbol id=\"modules-cancel\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 20 20\"><path d=\"M11.414 10l6.293-6.293a1 1 0 00-1.414-1.414L10 8.586 3.707 2.293a1 1 0 00-1.414 1.414L8.586 10l-6.293 6.293a1 1 0 001.414 1.414L10 11.414l6.293 6.293a.996.996 0 001.414 0 1 1 0 000-1.414L11.414 10z\"/></svg></symbol><symbol id=\"modules-social-facebook\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 30 30\"><path d=\"M15.6 30V19.4h5V15h-5v-3.1c0-1 .6-1.9 1.3-1.9h3.8V5.6h-3.8c-3.1 0-5.6 2.8-5.6 6.3V15H7.5v4.4h3.8v10.1C4.8 27.9 0 22 0 15 0 6.7 6.7 0 15 0s15 6.7 15 15c0 8.1-6.4 14.7-14.4 15z\"/></svg></symbol><symbol id=\"modules-social-twitter\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 30 30\"><path d=\"M30 15c0 8.3-6.7 15-15 15S0 23.3 0 15 6.7 0 15 0s15 6.7 15 15zm-3.7-5.5c-.8.3-1.6.6-2.5.7.9-.5 1.5-1.4 1.8-2.4-.8.5-1.7.8-2.7 1-.8-.8-1.9-1.4-3.1-1.4-2.4 0-4.2 1.9-4.2 4.3 0 .3 0 .7.1 1-3.5-.2-6.7-1.9-8.8-4.5-.3.7-.6 1.4-.6 2.2 0 1.5.7 2.8 1.9 3.5-.7 0-1.4-.2-1.9-.5 0 2.1 1.5 3.8 3.4 4.2-.3.1-.7.1-1.1.1-.3 0-.6 0-.8-.1.5 1.7 2.1 2.8 4 2.9-1.5 1.1-3.3 1.9-5.3 1.9-.3 0-.7 0-1-.1 1.9 1.2 4.1 1.9 6.5 1.9 7.8 0 12.1-6.5 12.1-12.1v-.6c.9-.4 1.6-1.2 2.2-2z\"/></svg></symbol><symbol id=\"modules-social-youtube\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 30 30\"><path d=\"M30 15c0 8.3-6.7 15-15 15S0 23.3 0 15 6.7 0 15 0s15 6.7 15 15zm-5.6 0c0-6.9 0-6.9-9.4-6.9s-9.4 0-9.4 6.9 0 6.9 9.4 6.9 9.4 0 9.4-6.9zm-11.9-3.7l6.3 3.8-6.3 3.8v-7.6z\"/></svg></symbol><symbol id=\"modules-social-instagram\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 18 18\"><defs/><path fill-rule=\"evenodd\" d=\"M9 0c5 0 9 4 9 9s-4 9-9 9-9-4-9-9 4-9 9-9zM6.7 3.5h4.7c1.8 0 3.2 1.4 3.2 3.1v4.7c0 1.7-1.4 3.1-3.2 3.1H6.7c-1.8 0-3.2-1.4-3.2-3.1V6.7c0-1.8 1.4-3.2 3.2-3.2zM9 6.2c1.6 0 2.9 1.3 2.9 2.9S10.6 12 9 12s-3-1.3-3-2.9 1.3-2.9 3-2.9zm0 1.2c.9 0 1.7.8 1.7 1.7 0 .9-.8 1.7-1.7 1.7-.9 0-1.7-.8-1.7-1.7 0-.9.7-1.7 1.7-1.7zm2.8-1.7c.3 0 .6.2.6.6 0 .3-.2.6-.6.6-.3 0-.6-.2-.6-.6 0-.4.3-.6.6-.6zm-4.6-1h3.7c1.4 0 2.5 1.1 2.5 2.5v3.7c0 1.4-1.1 2.5-2.5 2.5H7.2c-1.4 0-2.5-1.1-2.5-2.5V7.2c-.1-1.4 1.1-2.5 2.5-2.5z\" clip-rule=\"evenodd\"/></svg>\n</symbol><symbol id=\"modules-social-linkedin\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 30 30\"><path d=\"M30 15c0 8.3-6.7 15-15 15S0 23.3 0 15 6.7 0 15 0s15 6.7 15 15zM11.3 7.8c0-1.2-1-2.2-2.5-2.2s-2.5.9-2.5 2.2c0 1.2 1 2.2 2.5 2.2s2.5-1 2.5-2.2zm-.7 4.1H6.9v10.6h3.8V11.9zm13.8 5c0-3.4-1.7-5.6-4.4-5.6-1.5 0-2.6.9-3.1 2.3l-.1-1.6H13c0 .4.1 2.5.1 2.5v8.1h3.8V17c0-1.5.7-2.5 1.8-2.5s1.9.6 1.9 2.5v5.6h3.8v-5.7z\"/></svg></symbol><symbol id=\"modules-social-pinterest\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 30 30\"><path d=\"M15 30c-1.5 0-2.9-.2-4.3-.6.6-.9 1.2-2 1.5-3.2.2-.7 1-4.1 1-4.1.5 1 2 1.9 3.7 1.9 4.8 0 8.1-4.4 8.1-10.3 0-4.4-3.8-8.6-9.5-8.6-7.1-.1-10.6 5-10.6 9.3 0 2.6 1 4.9 3.1 5.7.3.1.7 0 .8-.4.1-.2.2-.9.3-1.2.1-.4 0-.5-.2-.8-.6-.7-1-1.6-1-3 0-3.8 2.8-7.2 7.4-7.2 4 0 6.2 2.5 6.2 5.8 0 4.3-1.9 8-4.8 8-1.6 0-2.7-1.3-2.4-2.9.5-1.9 1.3-4 1.3-5.3 0-1.2-.7-2.3-2-2.3-1.6 0-2.9 1.7-2.9 3.9 0 1.4.5 2.4.5 2.4S9.5 24 9.3 25.3c-.3 1.1-.4 2.4-.3 3.5-5.3-2.4-9-7.7-9-13.8C0 6.7 6.7 0 15 0s15 6.7 15 15-6.7 15-15 15z\"/></svg></symbol><symbol id=\"spot-global\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 45 45\"><path d=\"M22.5 2A20.5 20.5 0 1043 22.5 20.52 20.52 0 0022.5 2zm-4.23 28.79A33.67 33.67 0 0022 31a34.54 34.54 0 004.76-.33 30.45 30.45 0 01-4.26 9 30.41 30.41 0 01-4.23-8.88zM22 28a29.86 29.86 0 01-4.47-.34A31 31 0 0117 22a29.87 29.87 0 01.36-4.64A30.13 30.13 0 0122 17a31 31 0 015.66.53A29.85 29.85 0 0128 22a31 31 0 01-.5 5.5 31 31 0 01-5.5.5zm-7.64-1a25.22 25.22 0 01-9.15-4.5 24.89 24.89 0 019-4.48A33.56 33.56 0 0014 22a34.61 34.61 0 00.36 5zM22.5 5.21a25.25 25.25 0 014.5 9.15 34.61 34.61 0 00-5-.36 33.58 33.58 0 00-4 .24 25.24 25.24 0 014.5-9.03zm8.29 13.06a30.41 30.41 0 018.89 4.23 30.46 30.46 0 01-9 4.26A34.54 34.54 0 0031 22a33.67 33.67 0 00-.21-3.73zm8.82.59A33.75 33.75 0 0030.26 15a29.2 29.2 0 00-3.89-9.56 17.54 17.54 0 0113.24 13.42zm-21-13.41a29.16 29.16 0 00-3.85 9.35 29.16 29.16 0 00-9.35 3.85A17.54 17.54 0 0118.64 5.44zM5.44 26.36a29.2 29.2 0 009.56 3.9 33.75 33.75 0 003.86 9.36A17.54 17.54 0 015.44 26.36zm20.7 13.25a33.73 33.73 0 003.91-9.56 33.73 33.73 0 009.56-3.91 17.55 17.55 0 01-13.47 13.47z\"/></svg></symbol></svg></div>      <div id=\"Announcements\"></div>    \n\n<header id=\"SiteNavContainer\" class=\"site-nav-container is-sticky\">\n  <div class=\"marketing-nav-wrapper\">\n    \n<a class=\"in-page-link skip-to-main visuallyhidden focusable\" data-ga-event=\"Main Nav\" data-ga-action=\"Skip to content\" data-trekkie-event=\"Main Nav\" data-trekkie-action=\"Skip to content\" data-trekkie-id=\"Main Nav Skip to content\" href=\"#Main\">Skip to Content</a>\n\n<nav\n  class=\"marketing-nav--skin-dark marketing-nav marketing-nav--primary\"\n  id=\"ShopifyMainNav\"\n  itemscope=\"itemscope\"\n  itemtype=\"https://schema.org/SiteNavigationElement\">\n  \n      <div class=\"marketing-nav__logo \">\n        <a href=\"/\" class=\"marketing-nav__logo__shopify\" data-ga-event=\"Main Nav\" data-ga-action=\"Logo\" data-trekkie-event=\"Main Nav\" data-trekkie-action=\"Logo\" data-trekkie-id=\"Main Nav Logo\">\n\n          <svg class=\"icon\" aria-labelledby=\"icon-logos-shopify-white-15-title\" role=\"img\"><title id=\"icon-logos-shopify-white-15-title\">Shopify</title> <use xlink:href=\"#logos-shopify-white\" /> </svg>\n</a>      </div>\n\n      <ul class=\"marketing-nav__items display--expanded-nav\">\n        <li><a href=\"/\" class=\"marketing-nav__item marketing-nav__item--primary\" itemprop=\"name\">Engineering Blog</a></li><li><a href=\"/work-with-us\" class=\"marketing-nav__item marketing-nav__item--primary\" itemprop=\"name\">Working at Shopify</a></li><li><a href=\"http://shopify.github.io/\" class=\"marketing-nav__item marketing-nav__item--primary\" itemprop=\"name\">Open Source at Shopify <svg class=\"icon marketing-nav__external-indicator\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#modules-nav-external-indicator\" /> </svg></a></li><li><a href=\"https://devdegree.ca\" class=\"marketing-nav__item marketing-nav__item--primary\" itemprop=\"name\">Dev Degree <svg class=\"icon marketing-nav__external-indicator\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#modules-nav-external-indicator\" /> </svg></a></li>\n      </ul>\n\n      \n\n        <ul class=\"marketing-nav__items marketing-nav__user display--expanded-nav\">\n          <li><a href=\"https://www.shopify.com/careers?itcat=EngBlog&amp;itterm=CTA\" class=\"marketing-nav__button marketing-button marketing-button--small start-free-trial__button\" itemprop=\"name\" data-event-page=\"engineering-blog\" data-event-category=\"link\" data-event-action=\"click-link-nav\" data-event-label=\"careers-open-roles\">See open roles</a></li>\n        </ul>\n\n        <button name=\"button\" type=\"button\" class=\"marketing-nav__hamburger hide--expanded-nav js-drawer-open-right\" aria-haspopup=\"dialog\" aria-expanded=\"false\" data-ga-event=\"Main Nav\" data-ga-action=\"Hamburger\">\n            <svg class=\"icon\" aria-labelledby=\"icon-modules-mobile-hamburger-18-title\" role=\"img\"><title id=\"icon-modules-mobile-hamburger-18-title\">Open Main Navigation</title> <use xlink:href=\"#modules-mobile-hamburger\" /> </svg>\n</button>\n</nav>\n\n      <section class=\"section background-lowlight color-white section--tight blog__header blog__header--engineering blog__header-secondary--engineering\" data-blog-sticky-banner-waypoint=\"\">\n  <div class=\"grid\">\n    <div class=\"grid__item\">\n      <div class=\"section-heading section-heading--lowlight section-heading--tablet-up-align-left gutter-bottom--reset\">\n        <p class=\"section-heading__heading heading--1\"><a class=\"color-white\" href=\"/\">Shopify Engineering</a></p>\n</div></div></div></section>\n      \n<nav class=\"marketing-nav marketing-nav--secondary\" aria-label=\"Section Navigation\">\n  <button name=\"button\" type=\"button\" id=\"ShopifyNavMobileSelect\" class=\"heading--4 marketing-nav__secondary-button gutter-bottom--reset\" aria-label=\"Toggle Section Navigation\" aria-controls=\"ShopifySubNavList\" aria-expanded=\"false\">\n\n    <span class=\"marketing-nav__secondary__action\">\n      Show\n    </span>\n\n    <svg class=\"icon marketing-nav__arrow\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#modules-nav-arrow-down\" /> </svg>\n</button>\n  <ul id=\"ShopifySubNavList\" class=\"marketing-nav__items\">\n    <li><a href=\"/\" class=\"marketing-nav__item marketing-nav__item--secondary\" itemprop=\"name\">Latest articles</a></li><li><a href=\"/topics/development\" class=\"marketing-nav__item marketing-nav__item--secondary\" itemprop=\"name\">Development</a></li><li><a href=\"/topics/infrastructure\" class=\"marketing-nav__item marketing-nav__item--secondary\" itemprop=\"name\">Infrastructure</a></li><li><a href=\"/topics/mobile\" class=\"marketing-nav__item marketing-nav__item--secondary\" itemprop=\"name\">Mobile</a></li><li><a href=\"/topics/developer-tooling\" class=\"marketing-nav__item marketing-nav__item--secondary\" itemprop=\"name\">Developer Tooling</a></li><li><a href=\"/topics/security\" class=\"marketing-nav__item marketing-nav__item--secondary\" itemprop=\"name\">Security</a></li><li><a href=\"/topics/data-science-engineering\" class=\"marketing-nav__item marketing-nav__item--secondary\" itemprop=\"name\">Data Science &amp; Engineering</a></li><li><a href=\"/topics/culture\" class=\"marketing-nav__item marketing-nav__item--secondary\" itemprop=\"name\">Culture</a></li>\n  </ul>\n</nav>\n\n  </div>\n\n    \n<div id=\"NavDrawer\"\n  class=\"drawer drawer--right\"\n  role=\"dialog\"\n  aria-modal=\"true\"\n  aria-labelledby=\"DrawerTitle\">\n  <div class=\"drawer__inner\">\n    <div class=\"drawer__top\">\n      <div class=\"marketing-nav__logo\">\n        <a href=\"/\" class=\"marketing-nav__logo__shopify\">\n\n          <svg class=\"icon\" aria-labelledby=\"icon-logos-shopify-black-20-title\" role=\"img\"><title id=\"icon-logos-shopify-black-20-title\">Home</title> <use xlink:href=\"#logos-shopify-black\" /> </svg>\n</a>      </div>\n\n      <button name=\"button\" type=\"button\" class=\"drawer__close-button js-drawer-close\" data-ga-event=\"Main Nav\" data-ga-action=\"close navigation\">\n        <svg class=\"icon\" aria-labelledby=\"icon-modules-cancel-21-title\" role=\"img\"><title id=\"icon-modules-cancel-21-title\">Close Main Navigation</title> <use xlink:href=\"#modules-cancel\" /> </svg>\n</button>    </div>\n\n    <nav id=\"DrawerTitle\" aria-label=\"Main Navigation\">\n        <ul class=\"drawer__items drawer__items--primary\" id=\"DrawerNavPrimaryAccordion\">\n          <li><a href=\"/\" class=\"drawer__item drawer__item--primary\">Engineering Blog</a></li><li><a href=\"/work-with-us\" class=\"drawer__item drawer__item--primary\">Working at Shopify</a></li><li><a href=\"http://shopify.github.io/\" class=\"drawer__item drawer__item--primary\">Open Source at Shopify <svg class=\"icon marketing-nav__external-indicator\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#modules-nav-external-indicator\" /> </svg></a></li><li><a href=\"https://devdegree.ca\" class=\"drawer__item drawer__item--primary\">Dev Degree <svg class=\"icon marketing-nav__external-indicator\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#modules-nav-external-indicator\" /> </svg></a></li>\n        </ul>\n\n        <ul class=\"drawer__items drawer__items--user\" id=\"DrawerNavSecondaryAccordion\">\n          <li><a href=\"https://www.shopify.com/careers?itcat=EngBlog&amp;itterm=CTA\" class=\"drawer__item\" data-event-page=\"engineering-blog\" data-event-category=\"link\" data-event-action=\"click-link-nav\" data-event-label=\"careers-open-roles\">See open roles</a></li>\n        </ul>\n\n    </nav>\n  </div>\n</div>\n\n</header>\n\n\n<div hidden=\"hidden\"><span id=\"NewWindow\">Opens in a new window</span><span id=\"ExternalSite\">Opens an external site</span><span id=\"NewWindowExternalSite\">Opens an external site in a new window</span></div>\n\n<div id=\"PageContainer\">\n  <main role=\"main\" id=\"Main\">\n    \n\n\n\n\n\n\n<section class=\"section section--tight\">\n  <div class=\"grid\">\n    <article class=\"grid__item grid__item--desktop-up-two-thirds\" itemscope=\"itemscope\" itemtype=\"https://schema.org/Article\" data-category=\"data-science-engineering\" id=\"article\">\n      <meta itemprop=\"mainEntityOfPage\" content=\"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions\">\n  <meta itemprop=\"dateModified\" content=\"2023-03-10T16:16:27Z\">\n\n<header class=\"article__header\">\n  <h1 class=\"article__title\" itemprop=\"headline\">Unlocking Real-time Predictions with Shopify&#39;s Machine Learning Platform</h1>\n\n  <div class=\"grid grid--vertically-centered\">\n    <div class=\"grid__item grid__item--tablet-up-two-thirds\">\n      <ul class=\"article__meta gutter-bottom--reset\">\n          <li>\n            by <a href=\"/search?link_search=true&amp;q=Isaac+Vidas\">Isaac Vidas</a>\n\n          </li>\n          <li>\n            <a href=\"/topics/data-science-engineering\">Data Science &amp; Engineering</a>\n          </li>\n      </ul>\n\n      <ul class=\"article__meta\">\n          <li>\n            <time itemprop=\"datePublished\" datetime=\"2023-03-10T16:16:27Z\">Mar 10, 2023</time>\n          </li>\n        <li>\n          12 minute read\n        </li>\n      </ul>\n</div>\n    <div class=\"grid__item grid__item--tablet-up-third\">\n      <ul role=\"list\" class=\"social-shares\">\n        <li class=\"social-shares_list-item\"><a data-share-url=\"mailto:?body=https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions&amp;subject=Unlocking%20Real-time%20Predictions%20with%20Shopify%27s%20Machine%20Learning%20Platform\" data-ga-action=\"Email share\" data-ga-event=\"Blog\" aria-describedby=\"NewWindow\" class=\"social-shares__icon-wrapper social-shares__icon-wrapper--email\" href=\"mailto:?body=https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions&amp;subject=Unlocking%20Real-time%20Predictions%20with%20Shopify%27s%20Machine%20Learning%20Platform\"><svg class=\"icon icon--fill-white social-shares__icon\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#modules-email-reverse\" /> </svg><span class=\"visuallyhidden\">Email</span></a></li>\n        <li class=\"social-shares_list-item\"><a data-share-url=\"https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions\" data-ga-action=\"Facebook share\" data-ga-event=\"Blog\" aria-describedby=\"NewWindow\" class=\"social-shares__icon-wrapper social-shares__icon-wrapper--facebook js-social-share\" href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions\"><svg class=\"icon icon--fill-white social-shares__icon\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#modules-facebook\" /> </svg><span class=\"visuallyhidden\">Facebook</span></a></li>\n        <li class=\"social-shares_list-item\"><a data-share-url=\"https://twitter.com/intent/tweet?text=Just+finished+reading+Unlocking+Real-time+Predictions+with+Shopify%27s+Machine+Learning+Platform+https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions&amp;via=Shopify\" data-ga-action=\"Twitter share\" data-ga-event=\"Blog\" aria-describedby=\"NewWindow\" class=\"social-shares__icon-wrapper social-shares__icon-wrapper--twitter js-social-share\" href=\"https://twitter.com/intent/tweet?text=Just+finished+reading+Unlocking+Real-time+Predictions+with+Shopify%27s+Machine+Learning+Platform+https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions&amp;via=Shopify\"><svg class=\"icon icon--fill-white social-shares__icon\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#modules-twitter\" /> </svg><span class=\"visuallyhidden\">Twitter</span></a></li>\n        <li class=\"social-shares_list-item\"><a data-share-url=\"https://www.linkedin.com/shareArticle?mini=true&amp;source=Shopify&amp;title=Unlocking+Real-time+Predictions+with+Shopify%27s+Machine+Learning+Platform&amp;url=https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions\" data-ga-action=\"LinkedIn share\" data-ga-event=\"Blog\" aria-describedby=\"NewWindow\" class=\"social-shares__icon-wrapper social-shares__icon-wrapper--linkedin js-social-share\" href=\"https://www.linkedin.com/shareArticle?mini=true&amp;source=Shopify&amp;title=Unlocking+Real-time+Predictions+with+Shopify%27s+Machine+Learning+Platform&amp;url=https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions\"><svg class=\"icon icon--fill-white social-shares__icon\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#modules-linkedin\" /> </svg><span class=\"visuallyhidden\">LinkedIn</span></a></li>\n</ul></div></div></header>\n\n  <figure class=\"article__image--featured\">\n    <meta itemprop=\"image\" content=\"https://cdn.shopify.com/s/files/1/0779/4361/articles/ShopifyEng_BlogIllustrations_220216_72ppi_03_TheMagicOfMerlin-ShopifysMachineLearningPlatform_3e07a5ed-cd94-44e2-b4ca-c6db1ec81bd6.jpg?v=1678451850\">\n    <img class=\"image\" height=\"485\" width=\"924\" sizes=\"100vw\" srcset=\"https://cdn.shopify.com/s/files/1/0779/4361/articles/ShopifyEng_BlogIllustrations_220216_72ppi_03_TheMagicOfMerlin-ShopifysMachineLearningPlatform_3e07a5ed-cd94-44e2-b4ca-c6db1ec81bd6.jpg?v=1678451850&amp;width=1024\" alt=\"\" />\n  </figure>\n\n<div class=\"article__content long-form-content\" itemprop=\"articleBody\">\n  <p>In 2022, we shipped Merlin, Shopify’s new and improved machine learning platform built on Ray. We built a flexible platform to support the varying requirements, inputs, data types, dependencies and integrations we deal with at Shopify (you can read all about Merlin’s architecture in our previous <a href=\"https://shopify.engineering/merlin-shopify-machine-learning-platform\" title=\"Shopify's Machine Learning Platform\" target=\"_blank\">blog</a>). Since then, we’ve enhanced the platform by onboarding more use cases and adding features to complete the end-to-end machine learning workflow, including:</p>\n<ol>\n<li>\n<strong>Merlin Online Inference, </strong>which provides the ability to deploy and serve machine learning models for real-time predictions.</li>\n<li>\n<strong>Model Registry</strong> and <strong>experiment tracking</strong> with <a href=\"https://www.comet.com/site/\" title=\"CometML\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Comet ML</a>.</li>\n<li>\n<strong>Merlin Pipelines</strong>, a framework for reproducible machine learning pipelines on top of Merlin.</li>\n<li>\n<strong>Pano Feature Store</strong>, an offline / online feature store built on top of an open source feature store, <a href=\"https://feast.dev/\" title=\"Feast's Feature Store\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Feast</a>.</li>\n</ol>\n<p>The ability to provide real-time predictions for user-facing applications and services is an important requirement at Shopify, and will become increasingly critical as more machine learning models are integrated closer to user-facing products. But it’s a challenging requirement. As machine learning is utilized by many different teams across Shopify, each with its own use-cases and requirements, we had to ensure that Merlin’s online inference could be an effective, generalized solution. We needed to build something robust that could be used by all of our use-cases and allow low-latency, while serving machine learning models at Shopify scale.</p>\n<p>In this blog post, we’ll walk through how we built Merlin’s online inference capabilities to deploy and serve machine learning models for real-time prediction at scale. We’ll cover everything from the serving layer where our users can focus solely on their specific inference logic, to service deployment where we utilized our internal service ecosystem to ensure that online inference services can scale to Shopify’s capabilities.</p>\n<h1>What is online inference?</h1>\n<p>In machine learning workflows, once a model has been trained, evaluated and productionized, it can be applied on input data to return predictions. This is called machine learning inference. There are two main types of inference, batch and online inference.</p>\n<p>While batch inference can run periodically on a finite set of data, online inference is the ability to compute predictions in real-time as the input becomes available. With online inference, the observations that we produce predictions for can be infinite, as we’re using streams of data that are generated over time.</p>\n<p>Popular examples for online inference and real-time predictions can be found in many use cases. Some examples are <a href=\"https://shopify.engineering/evaluating-search-algorithms\" title=\"Shopify Evaluating Search Algorithms\" target=\"_blank\">recommender systems</a>, where having real-time predictions can provide users with the most relevant results, or fraud detection, where the ability to detect fraud is required as soon as it happens. Other Shopify specific use cases that benefit from online inference are <a href=\"https://shopify.engineering/using-rich-image-text-data-categorize-products\" title=\"Shopify Product Categorization\" target=\"_blank\">product categorization</a> and <a href=\"https://shopify.engineering/shopify-inbox-message-classification-model\" title=\"Shopify Data Centric Machine Learning\" target=\"_blank\">inbox classification</a>.</p>\n<p>When serving machine learning models for real time predictions, latency becomes much more important compared to batch jobs, as the latency of machine learning models can impact the performance of user facing services and impact the business. When considering online inference for a machine learning use case, it is important to consider:&nbsp;</p>\n<ul>\n<li>The cost</li>\n<li>The use-case requirements&nbsp;</li>\n<li>The skillset of the team members (e.g. being able to handle running and maintaining machine learning services in production versus running them in batch jobs).</li>\n</ul>\n<p>As we prepared to expand Merlin for online inference, we first interviewed our internal stakeholders to better understand their use-case requirements more in detail. Once we had these, we could design the system, its architecture and infrastructure in ways that optimize for cost and performance, as well as advise the machine learning teams on staffing and required support, based on the service-level objectives of their use-case.</p>\n<h1>Online Inference with Merlin</h1>\n<p>There are several requirements that we set out to provide with Merlin Online Inference:</p>\n<ul>\n<li>\n<strong>Robust and flexible serving of machine learning models</strong>, to enable us to serve and deploy different models and machine learning libraries used at Shopify (e.g. TensorFlow, PyTorch, XGBoost).</li>\n<li>\n<strong>Low latency</strong>, in order to optimize processing of an inference request to provide responses with minimal delay.</li>\n<li>\n<strong>State-of-the-art features for online inference services</strong>, like rolling deployments, autoscaling, observability, automatic model updates, etc.</li>\n<li>\n<strong>Integration with the Merlin machine learning platform</strong>, so users can utilize the different Merlin features, such as model registry and Pano online feature store.</li>\n<li>\n<strong>Seamless and streamlined service creation, management and deployment</strong> for machine learning models.</li>\n</ul>\n<p>With these requirements in mind, we aimed to build Merlin Online Inference to enable deploying and serving machine learning models for real-time predictions.</p>\n<h1>Merlin Online Inference Architecture</h1>\n<p>In our previous Merlin blog post, we described how Merlin is used for training machine learning models. With Merlin Online Inference, we can provide our data scientists and machine learning engineers with the tools to <em>deploy and <em>serve their machine learning models and use cases.</em></em></p>\n<p>Every machine learning use case that requires online inference runs as its own dedicated service. These services are deployed on Shopify’s Kubernetes clusters (<a href=\"https://cloud.google.com/kubernetes-engine\" title=\"Google Kubernetes Engine\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Google Kubernetes Engine</a>) as any other Shopify service. Each service lives in its own Kubernetes namespace and can be configured individually, for example, to autoscale based on different parameters and metrics.</p>\n<figure><img alt=\"Merlin Online Inference Architecture\" data-src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Merlin_Online_Inference_-_Architecture_5bf531f3-0a20-4ef6-b439-3f300493c58b.png?format=webp&amp;v=1678456762\" class=\"lazyload\">\n<figcaption>A high level architecture diagram of Merlin Online Inference</figcaption>\n</figure>\n<p>Each service loads its dedicated machine learning model from our model registry, Comet ML, as well as any other required artifacts. Different clients can call an inference endpoint to generate predictions in real-time. The main clients that use Merlin Online Inference services are Shopify’s core services (or any other internal service that requires real-time inference), as well as <a href=\"https://shopify.engineering/optimizing-apache-flink-applications-tips\" title=\"Shopify's Tips for Apache Flink\" target=\"_blank\">streaming pipelines on Flink</a> for near real-time predictions. Pano, our feature store, can be used to access features in low latency during inference both from the Merlin Online Inference service or from the different clients that send requests to the service.</p>\n<p>Each service has a monitoring dashboard with predefined metrics such as latency, requests per second, CPU, etc. This can be used to observe the health of the service, and can be further customized per service.</p>\n<p>Each Merlin Online Inference service has two main components:</p>\n<ul>\n<li>\n<strong>Serving layer</strong>: the API that enables an endpoint to return predictions from a model.</li>\n<li>\n<strong>Deployment</strong>: how the service will be deployed in Shopify’s infrastructure.</li>\n</ul>\n<h2>Serving Layer</h2>\n<p>The Merlin Online Inference serving layer is the API that serves the model (or models). It exposes an endpoint for processing inputs from the clients, and returns predictions from the model. The serving layer accomplishes the following:</p>\n<ol>\n<li>Starts a web server for the service</li>\n<li>Loads the model and additional artifacts to memory as part of the initialization process</li>\n<li>Exposes an endpoint for the inference function that will take features as inputs and return predictions</li>\n</ol>\n<figure><img alt=\"Merlin Online Inference Serving Layer\" style=\"display: block; margin-left: auto; margin-right: auto;\" data-src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Merlin_Online_Inference_Serving_Layer_4c62d0a7-84b8-436e-a8f9-ec664beb319f.png?format=webp&amp;v=1678457450\" class=\"lazyload\">\n<figcaption>A high level diagram of the serving layer</figcaption>\n</figure>\n<p>The serving layer is written in Python, which makes it easier for our data scientists and machine learning engineers to implement it. It’s defined in the Merlin Project of each use case. A Merlin Project is a folder in our Merlin mono-repo where the code, configuration and tests of the use case are kept. This allows the serving layer to reuse different parts of the machine learning workflow logic. The serving layer is added to the Merlin Docker image, which is created by Podman from a dedicated Dockerfile. It is then deployed in the Merlin Online Inference Kubernetes clusters.</p>\n<h3>Serving Layer Types</h3>\n<p>When analyzing the requirements of our users, we identified a need to support multiple types of serving layers. We wanted to make sure that we can abstract from our users a lot of the hassle of writing a complete API for every machine learning use case, while also enabling other more custom services.</p>\n<p>In order to do that, we currently support two types of serving libraries with Merlin Online Inference:</p>\n<ol>\n<li>\n<a href=\"https://mlserver.readthedocs.io/en/latest/\" title=\"MLServer Documentation\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>MLServer</strong></a>: An open source inference server for machine learning models built by <a href=\"https://www.seldon.io/\" title=\"Seldon Model Deployment\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Seldon</a>. MLServer supports REST and gRPC interfaces out of the box as well the ability to batch requests. It supports the <a href=\"https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html\" title=\"Seldon V2 Inference Protocol\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">V2 inference protocol</a> that standardizes communication protocols around with different inference servers and by that increases their utility and portability.</li>\n<li>\n<a href=\"https://fastapi.tiangolo.com/\" title=\"FastAPI\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>FastAPI</strong></a><strong>:</strong> A fast and high-performance web framework for building APIs with Python.</li>\n</ol>\n<p>These two libraries enable three different methods for our users to implement their online inference serving layer, starting from no code or low code with MLServer, to a fully customizable API using FastAPI. The following table describes the differences between these approaches:</p>\n<h3></h3>\n<table style=\"width: 101.767%;\">\n<tbody>\n<tr>\n<td style=\"width: 25%;\"><strong>Serving Layer Type</strong></td>\n<td style=\"width: 34.0175%;\"><strong>Description</strong></td>\n<td style=\"width: 31.9825%;\"><strong>What to Use</strong></td>\n</tr>\n<tr>\n<td style=\"width: 25%;\"><strong>No Code</strong></td>\n<td style=\"width: 34.0175%;\">\n<p>Uses MLServer’s pre-built <a href=\"https://mlserver.readthedocs.io/en/latest/index.html#inference-runtimes\" title=\"MLServer Inference Implementations\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">inference serving implementations.</a>&nbsp;Only requires configuration changes in MLServer json files.</p>\n</td>\n<td style=\"width: 31.9825%;\">\n<p>For this serving layer type, you have a model and all you need is to deploy it behind an end point such as Scikit-learn, XGBoost, LightGBM, ect.</p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 25%;\"><strong>Low Code</strong></td>\n<td style=\"width: 34.0175%;\">\n<p>Uses MLServer’s <a href=\"https://mlserver.readthedocs.io/en/latest/runtimes/custom.html\" title=\"MLServer Custom Inference Implementations\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">custom serving implementation</a>.&nbsp; Requires minimal code implementation of the serving class.</p>\n</td>\n<td style=\"width: 31.9825%;\">\n<p>Include transformation or business logic&nbsp;in the serving layer, or a model that uses an unsupported ML library in MLServer.</p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 25%;\"><strong>Full Custom</strong></td>\n<td style=\"width: 34.0175%;\">In this case, the user gets pre-defined boilerplate code for a FastAPI serving layer which they can fully customize.</td>\n<td style=\"width: 31.9825%;\">The machine learning use case needs to expose additional endpoints or a requirement that is unsupported in MLServer.&nbsp;</td>\n</tr>\n</tbody>\n</table>\n<h3></h3>\n<h3>Creating the Serving Layer in Merlin</h3>\n<p>In order to abstract away much of the complexity of starting to write a serving layer from scratch, we wrote Merlin CLI, which uses input from the user to build a custom serving layer. This serving layer will be generated from a predefined cookiecutter, and will provide boilerplate code for the user to build on.&nbsp;</p>\n<figure><img alt=\"Creating the serving layer in Merlin\" data-src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.33.50_AM.png?format=webp&amp;v=1678458860\" class=\"lazyload\">\n<figcaption>Creating the serving layer in Merlin</figcaption>\n</figure>\n<p>In the image above, the user can choose the inference library that they want to implement their serving layer in, then pick the machine learning library that they will use to load and serve their model.</p>\n<h3>Example of Serving a Custom Model with MLServer</h3>\n<p>Once we’ve created the boilerplate code for the serving layer, we can start implementing the specific logic for our use-case. In the following example, we’re serving a <a href=\"https://huggingface.co/\" title=\"Hugging Face\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">🤗Hugging Face 🤗</a> model to translate English to French with online inference.</p>\n<p>\n<script src=\"https://gist.github.com/ShopifyEng/68862306cb372e4d1bc581aab64cffe2.js\"></script>\n</p>\n<p>We create a class for our model which inherits from <em>mlserver.MLModel and implements the following methods:</em></p>\n<ol>\n<li>\n<strong>Load: </strong>loads the model and any additional required artifact to memory.</li>\n<li>\n<strong>Predict:</strong> generates predictions from the model based on a payload that the method received.</li>\n</ol>\n<p>In addition, with MLServer, there’s an option to serve models as a <a href=\"https://mlserver.readthedocs.io/en/latest/runtimes/huggingface.html\" title=\"HuggingFace runtime for MLServer\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">pre-built HuggingFace runtime</a> using only configuration files:</p>\n<p>\n<script src=\"https://gist.github.com/ShopifyEng/a8e0a5910ae275c10c3c6c3ecafd8dab.js\"></script>\n</p>\n<p>The above is a very basic example of how low code and no-code examples can be used with MLServer without the need to define the whole API from scratch. This allows our users to focus only on what matters most to them, which is loading the model and serving it to generate inference.</p>\n<h3>Testing the Serving Layer with Merlin Workspaces</h3>\n<p>For testing purposes, our users can take advantage of the tools we already have in Merlin. They can create Merlin Workspaces, which are dedicated environments that can be defined by code, dependencies and required resources. These dedicated environments also enable distributed computing and scalability for the machine learning tasks that run on them. They are intended to be short lived, and our users can create them, run the serving layer in them, and expose a temporary endpoint for development, debugging and stress testing. This enables fast iterations on the serving layer, as it abstracts the friction of deploying a complete service between each iteration.&nbsp;</p>\n<figure><img alt=\"Merlin Workspaces Architecture\" data-src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.32.14_AM.png?format=webp&amp;v=1678458756\" class=\"lazyload\">\n<figcaption>A high level architecture diagram of Merlin Workspaces</figcaption>\n</figure>\n<p>In the diagram above, we can see how different use cases run with their respective Merlin Workspaces on Merlin. Each use case can define their own infrastructure resources, machine learning libraries, Python packages, serving layer, etc. and run them in an isolated and scalable environment. This is where our users can iterate on their use cases while developing their model and serving layer, in the same way that they would for any other part of their machine learning workflow.</p>\n<p>The Merlin Workspace enables our users to access the swagger page of their API. This enables them to test out their code and make sure that it works before deploying it as a service.&nbsp;&nbsp;</p>\n<h2>Merlin Online Inference Service Deployment</h2>\n<p>Once the serving layer has been tested and validated, it’s ready to be deployed to production as a service. The deployment phase is where all of the Merlin components will be put together to form the deployed service. These components include the serving layer code, API, model, artifacts, libraries and requirements, image container that was created in the CI/CD pipelines and any additional configuration libraries and package requirements to form the Merlin Service.</p>\n<figure><img alt=\"Merlin Service Components\" data-src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.28.34_AM.png?format=webp&amp;v=1678458552\" class=\"lazyload\">\n<figcaption>The components that form a Merlin Service</figcaption>\n</figure>\n<h3>Creating a Merlin Service</h3>\n<p>Similarly to the serving layer creation, we leverage the same Merlin CLI to create a Merlin Service in Shopify’s ecosystem. When generating the service, we utilize as much of Shopify’s service infrastructure to deploy Merlin Services as any other Shopify service. This ensures that Merlin Services can scale to Shopify’s capabilities, as well as allowing them to integrate and benefit from the existing tooling that we have in place.</p>\n<p>When a Merlin Service is created, it is registered to Shopify’s Services DB. Services DB is an internal tool used to track all production services running at Shopify. It supports creating new services and provides comprehensive views and tools to help development teams maintain and operate their services with high quality.</p>\n<figure><img alt=\"Merlin Service Deployment Process\" data-src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.26.27_AM.png?format=webp&amp;v=1678458435\" class=\"lazyload\">\n<figcaption>The deployment process of a Merlin Service</figcaption>\n</figure>\n<p>Once the Merlin Service is created, the entire build and deployment workflow is automatically generated for it. When a user merges new changes to their repository, Shopify’s Buildkite pipeline is automatically triggered and among other actions, builds the image for the service. In the next step of the workflow, that image is then deployed on Shopify’s Kubernetes clusters using our <a href=\"https://github.com/Shopify/shipit-engine\" title=\"Shopify shipit-engine\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">internal Shipit pipelines</a>.</p>\n<h3>Merlin Service Configuration</h3>\n<p>Each Merlin Service is created with two configuration files, one for a production environment and one for a staging environment. These include settings for the resources, parameters and related artifacts of the service. Having a different configuration for each environment allows the user to define a different set of resources and parameters per environment. This can help optimize the resources used by the service, which in turn can reduce infrastructure cost. In addition, our users can leverage the staging environment of Merlin Services to test new model versions or configuration settings before deploying them to production.</p>\n<p>The following is an example of a Merlin Service configuration file which contains different parameters such as the project name, metadata, artifact paths, CPU, memory, GPUs, autoscaling configuration, etc.:&nbsp;</p>\n<p>\n<script src=\"https://gist.github.com/ShopifyEng/7b93b78bbb4009674067349574433349.js\"></script>\n</p>\n<p>This example shows a configuration file for the <em>classification_model_example project which uses MLServer to serve its model. It uses at least 3 replicas that can scale to 10 replicas, and each one has 2 CPUs, 32 GB of memory and <em>nvidia-tesla-t4 GPU. In addition, when the service starts, it will load a model from our model registry.</em></em></p>\n<h1>What’s Next for Merlin Online Inference</h1>\n<p>And there you have it, our path for deploying and serving machine learning models for real-time predictions with Merlin. To sum it up, our users start by creating a Merlin Project that contains everything required for their machine learning use case. An image is automatically built for their project which is then used in the training pipeline, resulting in a trained model that is saved to our model registry. If the use case requires online inference, Merlin can be used to create a serving layer and a dedicated Merlin Service. Once the service is deployed to production, Merlin users can continue iterating on their models and deploy new versions as they become available.</p>\n<figure><img alt=\"Merlin Online Inference User Journey\" data-src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.24.04_AM.png?format=webp&amp;v=1678458272\" class=\"lazyload\">\n<figcaption>A high level overview of the user journey</figcaption>\n</figure>\n<p>As we onboard new online inference use cases to Merlin we plan to tackle additional areas in order to enable:</p>\n<ol>\n<li>\n<strong>Ensemble models / inference graphs:&nbsp;</strong>while we have the ability to deploy and serve machine learning models, we are aware that in some cases we will need to combine multiple models in the inference process. We are looking into leveraging some open source tools that can help us achieve that with&nbsp;<a href=\"https://docs.ray.io/en/latest/serve/index.html\" title=\"Ray Serve\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Ray Serve,</a>&nbsp;<a href=\"https://www.seldon.io/solutions/open-source-projects/core\" title=\"Seldon Core\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Seldon Core</a>&nbsp;or&nbsp;<a href=\"https://www.bentoml.com/\" title=\"BentoML\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">BentoML</a>.</li>\n<li>\n<strong>Monitoring for online inference:&nbsp;</strong>with our current abilities, it is possible to create workflows, metrics and dashboards to detect different drifts. However, at the moment this step is completely manual and requires a lot of effort from our users. We want to enable a platform-based monitoring solution that will seamlessly integrate with the rest of Merlin.</li>\n<li>\n<strong>Continuous training:&nbsp;</strong>as the amount of input and data which are used for predictions increases, some of our use cases will need to start training their models more frequently and will require an automated and easier deployment process. We are looking into automating more of the service management process and lifecycle of our online inference models.</li>\n</ol>\n<p>While online inference is still a new part of Merlin, it’s already empowering our users and data science teams with the low latency, scalability and fast iterations that we had in mind when designing it. We're excited to keep building the platform and onboarding new use cases, so we can continue to unlock new possibilities to keep merchants on the cutting edge of innovation. With Merlin we help enable the millions of businesses powered by Shopify.</p>\n<div class=\"marketing-block marketing-block--light marketing-block--padded\">\n<p><strong>Isaac Vidas&nbsp;</strong>is a tech lead on the ML Platform team, focusing on designing and building Merlin, Shopify’s machine learning platform. Connect with Isaac on <a href=\"https://www.linkedin.com/in/isaac-vidas/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">LinkedIn</a>.</p>\n</div>\n<hr>\n<p>Are you passionate about solving data problems and eager to learn more about Shopify? Check out openings on our <a href=\"https://www.shopify.com/ca/careers\" title=\"Shopify Careers\" target=\"_blank\">careers page</a>.</p>\n</div>\n\n\n\n<ul class=\"social-shares--bordered gutter-bottom social-shares\" role=\"list\">\n  <li class=\"social-shares_list-item\"><a data-share-url=\"mailto:?body=https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions&amp;subject=Unlocking%20Real-time%20Predictions%20with%20Shopify%27s%20Machine%20Learning%20Platform\" data-ga-action=\"Email share\" data-ga-event=\"Blog\" aria-describedby=\"NewWindow\" class=\"social-shares__icon-wrapper social-shares__icon-wrapper--email\" href=\"mailto:?body=https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions&amp;subject=Unlocking%20Real-time%20Predictions%20with%20Shopify%27s%20Machine%20Learning%20Platform\"><svg class=\"icon icon--fill-white social-shares__icon\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#modules-email-reverse\" /> </svg><span class=\"visuallyhidden\">Email</span></a></li>\n  <li class=\"social-shares_list-item\"><a data-share-url=\"https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions\" data-ga-action=\"Facebook share\" data-ga-event=\"Blog\" aria-describedby=\"NewWindow\" class=\"social-shares__icon-wrapper social-shares__icon-wrapper--facebook js-social-share\" href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions\"><svg class=\"icon icon--fill-white social-shares__icon\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#modules-facebook\" /> </svg><span class=\"visuallyhidden\">Facebook</span></a></li>\n  <li class=\"social-shares_list-item\"><a data-share-url=\"https://twitter.com/intent/tweet?text=Just+finished+reading+Unlocking+Real-time+Predictions+with+Shopify%27s+Machine+Learning+Platform+https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions&amp;via=Shopify\" data-ga-action=\"Twitter share\" data-ga-event=\"Blog\" aria-describedby=\"NewWindow\" class=\"social-shares__icon-wrapper social-shares__icon-wrapper--twitter js-social-share\" href=\"https://twitter.com/intent/tweet?text=Just+finished+reading+Unlocking+Real-time+Predictions+with+Shopify%27s+Machine+Learning+Platform+https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions&amp;via=Shopify\"><svg class=\"icon icon--fill-white social-shares__icon\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#modules-twitter\" /> </svg><span class=\"visuallyhidden\">Twitter</span></a></li>\n  <li class=\"social-shares_list-item\"><a data-share-url=\"https://www.linkedin.com/shareArticle?mini=true&amp;source=Shopify&amp;title=Unlocking+Real-time+Predictions+with+Shopify%27s+Machine+Learning+Platform&amp;url=https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions\" data-ga-action=\"LinkedIn share\" data-ga-event=\"Blog\" aria-describedby=\"NewWindow\" class=\"social-shares__icon-wrapper social-shares__icon-wrapper--linkedin js-social-share\" href=\"https://www.linkedin.com/shareArticle?mini=true&amp;source=Shopify&amp;title=Unlocking+Real-time+Predictions+with+Shopify%27s+Machine+Learning+Platform&amp;url=https%3A%2F%2Fshopify.engineering%2Fshopifys-machine-learning-platform-real-time-predictions\"><svg class=\"icon icon--fill-white social-shares__icon\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#modules-linkedin\" /> </svg><span class=\"visuallyhidden\">LinkedIn</span></a></li>\n</ul>\n<div class=\"subscribe subscribe--centered subscribe--padded js-subscribe gutter-bottom blog__subscribe\"><div class=\"subscribe__content-wrapper\"><div class=\"subscribe__content\"><svg class=\"icon icon--size-small icon--fill-white subscribe__icon\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#spot-blog-post\" /> </svg><h3 class=\"subscribe__heading color-white\">Get stories like this in your inbox!</h3><p class=\"subscribe__subhead color-white\">Stories from the teams who build and scale Shopify. The commerce platform powering millions of businesses worldwide.</p><form class=\"marketing-form marketing-form--inline subscribe__form\" novalidate=\"novalidate\" action=\"/content-services/subscribers\" accept-charset=\"UTF-8\" method=\"post\"><div class=\"marketing-input-wrapper marketing-input-button-pair\"><div class=\"marketing-input-button-pair__field-wrapper\"><label class=\"marketing-label marketing-label--in-field marketing-label--floating marketing-input-button-pair__label\" for=\"SubscribeEmail-2079\">Email address</label><input autocomplete=\"email\" placeholder=\"Email address\" aria-describedby=\"MessageId_f8d6\" name=\"email\" id=\"SubscribeEmail-2079\" class=\"marketing-input-button-pair__input marketing-input marketing-input--floating subscribe__email\" type=\"email\" /><button class=\"marketing-button marketing-form__button marketing-input-button-pair__button subscribe__submit\" name=\"button\" type=\"submit\">Yes, sign me up!</button></div></div><span class=\"marketing-form__messages\" id=\"MessageId_f8d6\"></span><input type=\"hidden\" name=\"data_extension_key\" value=\"engineering-blog-subscribers\" autocomplete=\"off\" />\n        <input type=\"hidden\" name=\"signup_page\" value=\"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions\" autocomplete=\"off\" />\n</form><p class=\"subscribe__disclaimer gutter-bottom--reset\">Share your email with us and receive monthly updates.</p></div><div class=\"block block--lowlight subscribe__success\" aria-hidden=\"true\">\n    <h3 class=\"block__heading heading--4\">Thanks for subscribing.</h3>\n    <p class=\"block__content\">You’ll start receiving free tips and resources soon.</p>\n</div></div></div>\n\n<div itemprop=\"publisher\" itemscope itemtype=\"https://schema.org/Organization\">\n  <meta itemprop=\"name\" content=\"Shopify\">\n  <div itemprop=\"logo\" itemscope itemtype=\"https://schema.org/ImageObject\">\n    <meta itemprop=\"url\" content=\"https://cdn.shopify.com/assets/images/logos/shopify_logo_black.png\">\n    <meta itemprop=\"width\" content=\"210\">\n    <meta itemprop=\"height\" content=\"60\">\n  </div>\n</div>\n\n</article>\n    <aside class=\"grid__item grid__item--desktop-up-third blog__sidebar\">\n      <div class='display--desktop'>\n  <form class=\"search-form\" action=\"/search\" accept-charset=\"UTF-8\" method=\"get\">\n\n  <label class=\"marketing-input-wrapper\"><span class=\"marketing-label marketing-label--hidden visuallyhidden\">Search articles</span><input aria-describedby=\"MessageId_d591\" class=\"marketing-input search-form__input\" id=\"SidebarSearch\" placeholder=\"Search articles\" type=\"search\" name=\"q\" /></label><span class=\"marketing-form__messages\" id=\"MessageId_d591\"></span>\n\n  <button type='submit' class='search-form__submit'>\n    <span class='visuallyhidden'>Search</span>\n  </button>\n</form>\n\n  <div class=\"subscribe subscribe--centered subscribe--padded js-subscribe gutter-bottom blog__subscribe\"><div class=\"subscribe__content-wrapper\"><div class=\"subscribe__content\"><svg class=\"icon icon--size-small icon--fill-white subscribe__icon\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#spot-blog-post\" /> </svg><h3 class=\"subscribe__heading color-white\">Get stories like this in your inbox!</h3><p class=\"subscribe__subhead color-white\">Stories from the teams who build and scale Shopify. The commerce platform powering millions of businesses worldwide.</p><form class=\"marketing-form marketing-form--inline subscribe__form\" novalidate=\"novalidate\" action=\"/content-services/subscribers\" accept-charset=\"UTF-8\" method=\"post\"><div class=\"marketing-input-wrapper marketing-input-button-pair\"><div class=\"marketing-input-button-pair__field-wrapper\"><label class=\"marketing-label marketing-label--in-field marketing-label--floating marketing-input-button-pair__label\" for=\"SubscribeEmail-ed74\">Email address</label><input autocomplete=\"email\" placeholder=\"Email address\" aria-describedby=\"MessageId_5ea5\" name=\"email\" id=\"SubscribeEmail-ed74\" class=\"marketing-input-button-pair__input marketing-input marketing-input--floating subscribe__email\" type=\"email\" /><button class=\"marketing-button marketing-form__button marketing-input-button-pair__button subscribe__submit\" name=\"button\" type=\"submit\">Yes, sign me up!</button></div></div><span class=\"marketing-form__messages\" id=\"MessageId_5ea5\"></span><input type=\"hidden\" name=\"data_extension_key\" value=\"engineering-blog-subscribers\" autocomplete=\"off\" />\n        <input type=\"hidden\" name=\"signup_page\" value=\"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions\" autocomplete=\"off\" />\n</form><p class=\"subscribe__disclaimer gutter-bottom--reset\">Share your email with us and receive monthly updates.</p></div><div class=\"block block--lowlight subscribe__success\" aria-hidden=\"true\">\n    <h3 class=\"block__heading heading--4\">Thanks for subscribing.</h3>\n    <p class=\"block__content\">You’ll start receiving free tips and resources soon.</p>\n</div></div></div>\n\n  \n\n</div>\n\n<nav id='SidebarAccordion'>\n  <div class='preview-links accordion-item--mobile ' id=\"\">\n  <button class='accordion-link heading--5'>\n    <svg class=\"icon icon--fill-primary icon--size-tiny\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#spot-read\" /> </svg>\n    Resources\n  </button>\n\n  <div class='accordion-content'>\n          <a class=\"link__title heading--4\" href=\"https://stackshare.io/shopify/shopify\">\n            Our Tech Stack\n</a>            <p class='txt--minor divider'>Curious about what’s in our tech stack.</p>\n          <a class=\"link__title heading--4\" href=\"/sponsorship\">\n            Sponsorship\n</a>            <p class='txt--minor divider'>We’re looking to partner with you.</p>\n          <a class=\"link__title heading--4\" href=\"https://www.shopify.com/careers/work-anywhere\">\n            Working Anywhere at Shopify\n</a>            <p class='txt--minor divider'>Learn about Digital by Design</p>\n          <a class=\"link__title heading--4\" href=\"https://developers.shopify.com/\">\n            Shopify Partner Developers\n</a>            <p class='txt--minor divider'>Become a Shopify developer and earn money by building apps or working with businesses</p>\n          <a class=\"link__title heading--4\" href=\"https://twitter.com/ShopifyEng\">\n            Shopify Engineering on Twitter\n</a>            <p class='txt--minor divider'>Connect with us on Twitter</p>\n          <a class=\"link__title heading--4\" href=\"https://www.youtube.com/channel/UCcSb55kxVx_euWDDRfWFxrA\">\n            Shopify Engineering YouTube\n</a>            <p class='txt--minor divider'>Connect with us on YouTube</p>\n  </div>\n</div>\n\n\n  <div class='preview-links accordion-item--mobile ' id=\"\">\n  <button class='accordion-link heading--5'>\n    <svg class=\"icon icon--fill-primary icon--size-tiny\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#spot-boost\" /> </svg>\n    Popular\n  </button>\n\n  <div class='accordion-content'>\n          <a class=\"link__title heading--4 divider\" href=\"/migrating-our-largest-mobile-app-to-react-native\">\n            Migrating our Largest Mobile App to React Native\n</a>          <a class=\"link__title heading--4 divider\" href=\"/ruby-yjit-is-production-ready\">\n            Ruby 3.2’s YJIT is Production-Ready\n</a>          <a class=\"link__title heading--4 divider\" href=\"/overhauling-shopify-cli-for-a-better-developer-experience\">\n            From Ruby to Node: Overhauling Shopify’s CLI for a Better Developer Experience\n</a>          <a class=\"link__title heading--4 divider\" href=\"/technical-debt-25-percent-rule\">\n            The 25 Percent Rule for Tackling Technical Debt\n</a>          <a class=\"link__title heading--4 divider\" href=\"/the-case-against-monkey-patching\">\n            The Case Against Monkey Patching, From a Rails Core Team Member\n</a>          <a class=\"link__title heading--4 divider\" href=\"/building-resilient-payment-systems\">\n            10 Tips for Building Resilient Payment Systems\n</a>          <a class=\"link__title heading--4 divider\" href=\"/good-documentation-productivity\">\n            How Good Documentation Can Improve Productivity\n</a>          <a class=\"link__title heading--4 divider\" href=\"/five-common-data-stores-usage\">\n            Five Common Data Stores and When to Use Them\n</a>          <a class=\"link__title heading--4 divider\" href=\"/deconstructing-monolith-designing-software-maximizes-developer-productivity\">\n            Deconstructing the Monolith: Designing Software that Maximizes Developer Productivity\n</a>  </div>\n</div>\n\n\n  <div class='preview-links accordion-item--mobile ' id=\"\">\n  <button class='accordion-link heading--5'>\n    <svg class=\"icon icon--fill-primary icon--size-tiny\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#spot-quick\" /> </svg>\n    Latest\n  </button>\n\n  <div class='accordion-content'>\n          <a class=\"link__title heading--4 divider\" href=\"/creating-a-flexible-order-routing-system-with-shopify-functions\">\n            Creating a Flexible Order Routing System with Shopify Functions\n</a>          <a class=\"link__title heading--4 divider\" href=\"/adventures-in-garbage-collection\">\n            Adventures in Garbage Collection: Improving GC Performance in our Massive Monolith\n</a>          <a class=\"link__title heading--4 divider\" href=\"/react-redux-toolkit-migration\">\n            How Migrating from Vanilla Redux to Redux Toolkit Improved State Management in Shopify POS\n</a>          <a class=\"link__title heading--4 divider\" href=\"/what-being-a-staff-developer-means-at-shopify\">\n            What Being a Staff Developer Means at Shopify\n</a>          <a class=\"link__title heading--4 divider\" href=\"/supporting-passkeys-in-shop-authentication-flows\">\n            Supporting Passkeys in Shop&#39;s Authentication Flows\n</a>          <a class=\"link__title heading--4 divider\" href=\"/internationalization-i18n-best-practices-front-end-developers\">\n            Lessons From Linguistics: i18n Best Practices for Front-End Developers\n</a>          <a class=\"link__title heading--4 divider\" href=\"/shopify-tophat-mobile-developer-testing\">\n            Tophat: Crafting a Delightful Mobile Developer Experience\n</a>          <a class=\"link__title heading--4 divider\" href=\"/shopifys-machine-learning-platform-real-time-predictions\">\n            Unlocking Real-time Predictions with Shopify&#39;s Machine Learning Platform\n</a>          <a class=\"link__title heading--4 divider\" href=\"/improving-the-developer-experience-with-ruby-lsp\">\n            Improving the Developer Experience with the Ruby LSP\n</a>          <a class=\"link__title heading--4 divider\" href=\"/the-case-against-monkey-patching\">\n            The Case Against Monkey Patching, From a Rails Core Team Member\n</a>  </div>\n</div>\n\n</nav>\n\n</aside></div></section>\n    \n  </main>\n    <section class=\"section section--tight section--border engineering-blog__footer background-green-70 text-center\">\n  <div class=\"grid grid--vertically-centered engineering-blog__footer-container\">\n    <div class=\"grid__item\">\n      <div class=\"section-heading\">\n        <h2 class=\"section-heading__heading heading--jumbo engineering-blog__footer-heading\">Ready to tackle frontend, backend, infrastructure, data, or security challenges?</h2>\n</div>\n      <a class=\"engineering-blog__footer-cta link link--secondary\" href=\"https://www.shopify.com/careers\">Explore all of our available roles&nbsp;<span class=\"link__arrow\"><svg class=\"icon link__arrow-inner\" aria-hidden=\"true\" focusable=\"false\"> <use xlink:href=\"#modules-arrow-right\" /> </svg></span></a>\n</div></div></section>\n\n</div>\n\n    <script>\n  window.App = window.App || {};\n  window.App.config = {\n    signupHost: \"accounts.shopify.com\",\n    railsEnv: \"production\",\n    splitSignupURL: null,\n    signupStoreCreateURL: \"https://accounts.shopify.com/store-create\"\n  };\n</script>\n\n<script>\n//<![CDATA[\nwindow.I18n = window.I18n || {}; window.I18n.data = {\"modal\":{\"home\":\"Home\",\"close\":\"Close\"},\"signup\":{\"header\":\"Start your free %{trial_length}-day trial of Shopify\",\"custom_header\":\"%{custom_signup_header}\",\"create_now\":\"Create your store\",\"labels\":{\"email\":\"Email address\",\"password\":\"Password\",\"storename\":\"Store name\",\"shop_name\":\"Your store name\",\"promotional_program_promo_code\":\"Discount code\",\"subdomain\":\"Store URL\"},\"subtitles\":{\"shop_name\":\"This can be changed later.\",\"promotional_program_promo_code\":\"Enter the promo code that was provided to you.\",\"subdomain\":\"This will be the default domain for your store, but you can add different domains later. You'll also use your Store URL to log in.\"},\"placeholders\":{\"password\":\"Minimum 5 characters\"},\"success_messages\":{\"email\":\"Got it\",\"password\":\"Looks great\",\"shop_name\":\"That name is available!\",\"subdomain\":\"You‘ll use this to log in to your store\"},\"store_address_suffix\":\".myshopify.com\",\"hint_messages\":{\"email_typo_html\":\"Did you mean \\u003cbutton type=\\\"button\\\" data-bind-event-click=\\\"%{on_click}\\\"\\u003e%{suggestion}\\u003c/button\\u003e?\"},\"page_title\":\"Become a Shopify Affiliate.\",\"meta_description\":\"Become a Shopify Affiliate.\",\"hero\":{\"heading\":\"Become a Shopify Affiliate.\",\"top_promo_copy\":\"Grow your brand with Shopify, access exclusive opportunities, and get $150 USD per referral.\",\"signup\":\"Sign-up\",\"bottom_promo_copy\":\"By entering your email, you agree to receive marketing emails from Shopify.\",\"video_alt_text\":\"Become a Shopify Affiliate.\"},\"features\":{\"feature_0\":{\"heading\":\"We’re committed to your success\",\"content\":\"Make the most referrals possible with resources to support your content development and assist your referral traffic’s conversion rate. Get access to a library of global creative, and ready-made education and lead magnets.\"},\"feature_1\":{\"heading\":\"Make data informed decisions\",\"content\":\"Gain detailed performance insights through your personal dashboard and make informed decisions to support your business growth.\"},\"feature_2\":{\"heading\":\"Earn competitive comissions\",\"content\":\"Get $150 USD per successful referral to Shopify. Eligible partners will also receive opportunities to expand their audience through Shopify and accelerate their referral revenue.\"}},\"bio\":{\"text_markdown\":\"Join thousands of affiliates from across the globe!\\n\\nContent creators, course educators, influencers, and review sites are helping their audiences launch successful businesses on Shopify, while earning competitive referral commissions, and elevating themselves as leaders in the industry.\",\"affiliate\":\"Shopify\",\"brand\":\"Affiliate Program\"},\"quote\":{\"content\":\"“You have an opportunity to partner up with the best company in the industry. To get the social proof, endorsement and edification of a brand like Shopify creates unlimited opportunities for you as a creator, influencer, and affiliate. If you're considering partnering with Shopify, do it!“\",\"author\":\"EZRA FIRESTONE, SHOPIFY AFFILIATE \\u0026 COMMERCE COACH\"}},\"forms\":{\"errors\":{\"throttled\":\"Too many requests from this IP, try again later.\",\"global\":{\"invalid\":\"Please enter a valid email address\",\"required\":\"This field is required.\",\"generic\":\"Sorry, something went wrong. Please try again later.\",\"throttled\":\"Too many requests from this IP, try again later.\"},\"shop_name\":{\"empty\":\"Please enter a store name\",\"minlength\":\"Your store name must be at least 4 characters\",\"maxlength\":\"Your store name can’t be longer than 60 characters\",\"existingAdmin\":\"A store with that name already exists. If you are the owner you can \\u003ca href=\\\"https://%{err}/admin\\\"\\u003elog in here\\u003c/a\\u003e\",\"message\":\"%{err}\",\"matchesPassword\":\"Your store name can’t be the same as your password\",\"disallowed\":\"Your store name can’t contain the word \\u003cstrong\\u003e%{err}\\u003c/strong\\u003e. Try another.\"},\"email\":{\"empty\":\"Please enter an email address\",\"invalid\":\"Please enter a valid email address\",\"member_exists\":\"You are already subscribed to this list\",\"generic\":\"Sorry, something went wrong. Please try again later.\"},\"password\":{\"empty\":\"Please enter a password\",\"minlength\":\"Password must be at least 5 characters\",\"spaces\":\"Password cannot start or end with a space\"},\"subdomain\":{\"empty\":\"Please enter a subdomain\",\"minlength\":\"Your subdomain must be at least 4 characters\",\"multiple\":\"Multiple subdomains are associated with this email\",\"suggest\":\"Did you mean %{err}?\",\"invalid\":\"Please enter a valid subdomain\",\"existingAdmin\":\"A store with that subdomain already exists. If you are the owner you can \\u003ca href=\\\"https://%{err}/admin\\\"\\u003elog in here\\u003c/a\\u003e\",\"disallowed\":\"Your subdomain can’t contain the word \\u003cstrong\\u003e%{err}\\u003c/strong\\u003e. Try another.\",\"message\":\"%{err}\"},\"promotional_program_promo_code\":{\"message\":\"%{err}\"},\"restricted_scope_exception_request[partner_id]\":{\"maxLengthError\":\"Please check the input lenght and try again.\",\"nonInteger\":\"Please verify your Partner ID.\"},\"restricted_scope_exception_request[app_id]\":{\"maxLengthError\":\"Please check the input length and try again.\",\"nonInteger\":\"Please verify your App ID.\"},\"restricted_scope_exception_request[other_scopes]\":{\"missingScope\":\"Please list any scopes that apply.\"},\"restricted_scope_exception_request[email_confirmation]\":{\"confirmationError\":\"Please check your email address and try again.\"},\"storename\":{\"empty\":\"Please enter a store name\",\"minlength\":\"Store name has to be more than 4 characters\",\"maxlength\":\"Store name has to be less than 60 characters\",\"disallowed\":\"Store name can only contain letters and numbers\",\"disallowedwords\":\"Store name contains disallowed words. Please check your input\"}},\"fields\":{\"storename\":{\"tooltip\":\"If you don't have a name, you can simply write keywords related to your business. This name can be changed anytime.\"}},\"create_account\":\"Create your account\"},\"landers\":{\"short\":{\"free_trial\":{\"heading\":\"Sign up for Shopify\",\"edit_button\":\"Edit\",\"learn_button\":\"Learn more\",\"cta\":\"Create your store\",\"existing_admin_error\":\"A store with that URL already exists. If you are the owner you can \\u003ca href=https://%{shop_name}.myshopify.com/admin\\u003elog in here\\u003c/a\\u003e\",\"help_text\":\"This is the URL that customers will use to visit your store. You can also buy a custom domain like %{query}.com and connect it to this store.\",\"help_modal\":{\"domain\":{\"question\":\"\\u003ch4\\u003eWhat is a domain?\\u003c/h4\\u003e\",\"answer\":\"\\u003cp\\u003eA domain is a website address used to access your store. When you create a store, we create a domain at myshopify.com for you.\\u003c/p\\u003e\"},\"shopify_domain\":{\"question\":\"\\u003ch4\\u003eWhat is a myshopify.com domain?\\u003c/h4\\u003e\",\"answer\":\"\\u003cp\\u003eYour myshopify.com domain will look like this:\\u003c/p\\u003e \\u003ccode\\u003eyour-store-name.myshopify.com\\u003c/code\\u003e \\u003cp\\u003eYour myshopify.com is used:\\u003c/p\\u003e\\n  \\u003cul\\u003e\\n      \\u003cli\\u003eso you and your customers can access your store\\u003c/li\\u003e\\n      \\u003cli\\u003eas a unique way to identify your store when contacting support\\u003c/li\\u003e\\n  \\u003c/ul\\u003e\\n\\u003cp\\u003e\\u003cstrong\\u003eYour myshopify.com domain cannot be changed,\\u003c/strong\\u003e but you can always purchase a custom domain instead.\\u003c/p\\u003e\"},\"custom_domain\":{\"question\":\"\\u003ch4\\u003eWhat is a custom domain?\\u003c/h4\\u003e\",\"answer\":\"\\u003cp\\u003eYou can also buy a custom domain that better reflects your brand:\\u003c/p\\u003e \\u003ccode\\u003eyour-brand-name.com\\u003c/code\\u003e \\u003cp\\u003eYou can buy a custom domain from Shopify or any 3rd party, and connect it to your store at any time.\\u003c/p\\u003e\"}},\"generating_name\":\"Generating name\"}}}}; window.I18n.globals = {\"total_blog_subscribers\":\"446,005\",\"trial_length\":3,\"custom_signup_header\":\"Try Shopify for free now and enjoy $1/month for 3 months on select plans\",\"total_remote_employees\":1400,\"currency_code\":null,\"current_year\":\"2023\",\"supported_languages\":\"50\",\"total_active_users\":\"2,100,000\",\"total_apps\":\"6,000\",\"total_buyers\":644000000,\"total_buyers_shop_pay\":35000000,\"total_countries\":\"175\",\"total_countries_partners\":50,\"total_domain_extensions\":50,\"total_data_processed_pb\":1200000,\"total_employees\":\"10,000\",\"total_experts\":\"780\",\"total_gateways\":\"100\",\"total_gmv_billions\":\"496\",\"total_orders\":5500000000,\"total_stores\":\"1,700,000\",\"total_themes\":\"70\",\"uptime\":99.98,\"hiring_2021\":2021,\"total_developers\":\"3,800\",\"total_developer_earnings_millions\":230,\"total_app_installs_millions\":16,\"current_domain\":\"shopify.engineering\",\"lite_plan_currency\":\"USD\",\"lite_plan_monthly_price\":\"$9\",\"minimum_geolocated_monthly_price_currency\":\"USD\",\"minimum_geolocated_monthly_price\":\"$5\",\"minimum_monthly_price_currency\":\"USD\",\"minimum_monthly_price_number\":5.0,\"minimum_monthly_price\":\"$5\",\"minimum_non_lite_monthly_price\":\"$39\",\"basic_annual_monthly_price\":\"$29.00 USD\",\"basic_annual_price\":\"\\u003cspan\\u003e$29.00 USD\\u003c/span\\u003e\",\"standard_annual_price\":\"\\u003cspan\\u003e$79.00 USD\\u003c/span\\u003e\",\"advanced_annual_price\":\"\\u003cspan\\u003e$299.00 USD\\u003c/span\\u003e\",\"pos_pro_price_usd\":\"$89\",\"pos_retail_locations\":\"1,000\",\"starter_faq_fulfillment_network\":\"\",\"pricing_faq_answer_with_discount\":\"Yes, we offer a 25% discount on yearly plans.\",\"paid_trial_amount\":\"$1\",\"paid_trial_months\":3,\"tse_usd\":\"$9\",\"usd\":\"USD\",\"lang\":\"en\"};\n//]]>\n</script>\n\n  \n    <script src=\"https://cdn.shopify.com/shopifycloud/brochure/bundles/baseline/runtime-23b5c454c40acb266825870dc815b752892443b6b2f792756142f563ba0f669b.js\"></script>\n<script src=\"https://cdn.shopify.com/shopifycloud/brochure/bundles/baseline/vendor-6da38172ddd32fd903465a5fa16baa11c8fc2f0fa6bdc407d064f44fc6a4e570.js\"></script>\n<script src=\"https://cdn.shopify.com/shopifycloud/brochure/bundles/baseline/manifests/about~manifests/app-store-principles~manifests/apple-pay~manifests/augmented-reality~manif~00e201c3-77062639a7cd38fa8e0e98779ac3ad49403d773b7f9926f0c6799430532c027e.js\"></script>\n<script src=\"https://cdn.shopify.com/shopifycloud/brochure/bundles/baseline/manifests/blog-fc3546ff5a0526daedc06320f63114d0f0ba32a8a38bae6ca9e222f476a258fb.js\"></script>\n    </body>\n</html>\n\n\n\n\n","oembed":false,"readabilityObject":{"title":"Unlocking Real-time Predictions with Shopify's Machine Learning Platform","content":"<div id=\"readability-page-1\" class=\"page\"><div itemprop=\"articleBody\">\n  <p>In 2022, we shipped Merlin, Shopify’s new and improved machine learning platform built on Ray. We built a flexible platform to support the varying requirements, inputs, data types, dependencies and integrations we deal with at Shopify (you can read all about Merlin’s architecture in our previous <a href=\"https://shopify.engineering/merlin-shopify-machine-learning-platform\" title=\"Shopify's Machine Learning Platform\" target=\"_blank\">blog</a>). Since then, we’ve enhanced the platform by onboarding more use cases and adding features to complete the end-to-end machine learning workflow, including:</p>\n<ol>\n<li>\n<strong>Merlin Online Inference, </strong>which provides the ability to deploy and serve machine learning models for real-time predictions.</li>\n<li>\n<strong>Model Registry</strong> and <strong>experiment tracking</strong> with <a href=\"https://www.comet.com/site/\" title=\"CometML\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Comet ML</a>.</li>\n<li>\n<strong>Merlin Pipelines</strong>, a framework for reproducible machine learning pipelines on top of Merlin.</li>\n<li>\n<strong>Pano Feature Store</strong>, an offline / online feature store built on top of an open source feature store, <a href=\"https://feast.dev/\" title=\"Feast's Feature Store\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Feast</a>.</li>\n</ol>\n<p>The ability to provide real-time predictions for user-facing applications and services is an important requirement at Shopify, and will become increasingly critical as more machine learning models are integrated closer to user-facing products. But it’s a challenging requirement. As machine learning is utilized by many different teams across Shopify, each with its own use-cases and requirements, we had to ensure that Merlin’s online inference could be an effective, generalized solution. We needed to build something robust that could be used by all of our use-cases and allow low-latency, while serving machine learning models at Shopify scale.</p>\n<p>In this blog post, we’ll walk through how we built Merlin’s online inference capabilities to deploy and serve machine learning models for real-time prediction at scale. We’ll cover everything from the serving layer where our users can focus solely on their specific inference logic, to service deployment where we utilized our internal service ecosystem to ensure that online inference services can scale to Shopify’s capabilities.</p>\n<h2>What is online inference?</h2>\n<p>In machine learning workflows, once a model has been trained, evaluated and productionized, it can be applied on input data to return predictions. This is called machine learning inference. There are two main types of inference, batch and online inference.</p>\n<p>While batch inference can run periodically on a finite set of data, online inference is the ability to compute predictions in real-time as the input becomes available. With online inference, the observations that we produce predictions for can be infinite, as we’re using streams of data that are generated over time.</p>\n<p>Popular examples for online inference and real-time predictions can be found in many use cases. Some examples are <a href=\"https://shopify.engineering/evaluating-search-algorithms\" title=\"Shopify Evaluating Search Algorithms\" target=\"_blank\">recommender systems</a>, where having real-time predictions can provide users with the most relevant results, or fraud detection, where the ability to detect fraud is required as soon as it happens. Other Shopify specific use cases that benefit from online inference are <a href=\"https://shopify.engineering/using-rich-image-text-data-categorize-products\" title=\"Shopify Product Categorization\" target=\"_blank\">product categorization</a> and <a href=\"https://shopify.engineering/shopify-inbox-message-classification-model\" title=\"Shopify Data Centric Machine Learning\" target=\"_blank\">inbox classification</a>.</p>\n<p>When serving machine learning models for real time predictions, latency becomes much more important compared to batch jobs, as the latency of machine learning models can impact the performance of user facing services and impact the business. When considering online inference for a machine learning use case, it is important to consider:&nbsp;</p>\n<ul>\n<li>The cost</li>\n<li>The use-case requirements&nbsp;</li>\n<li>The skillset of the team members (e.g. being able to handle running and maintaining machine learning services in production versus running them in batch jobs).</li>\n</ul>\n<p>As we prepared to expand Merlin for online inference, we first interviewed our internal stakeholders to better understand their use-case requirements more in detail. Once we had these, we could design the system, its architecture and infrastructure in ways that optimize for cost and performance, as well as advise the machine learning teams on staffing and required support, based on the service-level objectives of their use-case.</p>\n<h2>Online Inference with Merlin</h2>\n<p>There are several requirements that we set out to provide with Merlin Online Inference:</p>\n<ul>\n<li>\n<strong>Robust and flexible serving of machine learning models</strong>, to enable us to serve and deploy different models and machine learning libraries used at Shopify (e.g. TensorFlow, PyTorch, XGBoost).</li>\n<li>\n<strong>Low latency</strong>, in order to optimize processing of an inference request to provide responses with minimal delay.</li>\n<li>\n<strong>State-of-the-art features for online inference services</strong>, like rolling deployments, autoscaling, observability, automatic model updates, etc.</li>\n<li>\n<strong>Integration with the Merlin machine learning platform</strong>, so users can utilize the different Merlin features, such as model registry and Pano online feature store.</li>\n<li>\n<strong>Seamless and streamlined service creation, management and deployment</strong> for machine learning models.</li>\n</ul>\n<p>With these requirements in mind, we aimed to build Merlin Online Inference to enable deploying and serving machine learning models for real-time predictions.</p>\n<h2>Merlin Online Inference Architecture</h2>\n<p>In our previous Merlin blog post, we described how Merlin is used for training machine learning models. With Merlin Online Inference, we can provide our data scientists and machine learning engineers with the tools to <em>deploy and <em>serve their machine learning models and use cases.</em></em></p>\n<p>Every machine learning use case that requires online inference runs as its own dedicated service. These services are deployed on Shopify’s Kubernetes clusters (<a href=\"https://cloud.google.com/kubernetes-engine\" title=\"Google Kubernetes Engine\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Google Kubernetes Engine</a>) as any other Shopify service. Each service lives in its own Kubernetes namespace and can be configured individually, for example, to autoscale based on different parameters and metrics.</p>\n<figure><img alt=\"Merlin Online Inference Architecture\" data-src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Merlin_Online_Inference_-_Architecture_5bf531f3-0a20-4ef6-b439-3f300493c58b.png?format=webp&amp;v=1678456762\" src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Merlin_Online_Inference_-_Architecture_5bf531f3-0a20-4ef6-b439-3f300493c58b.png?format=webp&amp;v=1678456762\">\n<figcaption>A high level architecture diagram of Merlin Online Inference</figcaption>\n</figure>\n<p>Each service loads its dedicated machine learning model from our model registry, Comet ML, as well as any other required artifacts. Different clients can call an inference endpoint to generate predictions in real-time. The main clients that use Merlin Online Inference services are Shopify’s core services (or any other internal service that requires real-time inference), as well as <a href=\"https://shopify.engineering/optimizing-apache-flink-applications-tips\" title=\"Shopify's Tips for Apache Flink\" target=\"_blank\">streaming pipelines on Flink</a> for near real-time predictions. Pano, our feature store, can be used to access features in low latency during inference both from the Merlin Online Inference service or from the different clients that send requests to the service.</p>\n<p>Each service has a monitoring dashboard with predefined metrics such as latency, requests per second, CPU, etc. This can be used to observe the health of the service, and can be further customized per service.</p>\n<p>Each Merlin Online Inference service has two main components:</p>\n<ul>\n<li>\n<strong>Serving layer</strong>: the API that enables an endpoint to return predictions from a model.</li>\n<li>\n<strong>Deployment</strong>: how the service will be deployed in Shopify’s infrastructure.</li>\n</ul>\n<h2>Serving Layer</h2>\n<p>The Merlin Online Inference serving layer is the API that serves the model (or models). It exposes an endpoint for processing inputs from the clients, and returns predictions from the model. The serving layer accomplishes the following:</p>\n<ol>\n<li>Starts a web server for the service</li>\n<li>Loads the model and additional artifacts to memory as part of the initialization process</li>\n<li>Exposes an endpoint for the inference function that will take features as inputs and return predictions</li>\n</ol>\n<figure><img alt=\"Merlin Online Inference Serving Layer\" data-src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Merlin_Online_Inference_Serving_Layer_4c62d0a7-84b8-436e-a8f9-ec664beb319f.png?format=webp&amp;v=1678457450\" src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Merlin_Online_Inference_Serving_Layer_4c62d0a7-84b8-436e-a8f9-ec664beb319f.png?format=webp&amp;v=1678457450\">\n<figcaption>A high level diagram of the serving layer</figcaption>\n</figure>\n<p>The serving layer is written in Python, which makes it easier for our data scientists and machine learning engineers to implement it. It’s defined in the Merlin Project of each use case. A Merlin Project is a folder in our Merlin mono-repo where the code, configuration and tests of the use case are kept. This allows the serving layer to reuse different parts of the machine learning workflow logic. The serving layer is added to the Merlin Docker image, which is created by Podman from a dedicated Dockerfile. It is then deployed in the Merlin Online Inference Kubernetes clusters.</p>\n<h3>Serving Layer Types</h3>\n<p>When analyzing the requirements of our users, we identified a need to support multiple types of serving layers. We wanted to make sure that we can abstract from our users a lot of the hassle of writing a complete API for every machine learning use case, while also enabling other more custom services.</p>\n<p>In order to do that, we currently support two types of serving libraries with Merlin Online Inference:</p>\n<ol>\n<li>\n<a href=\"https://mlserver.readthedocs.io/en/latest/\" title=\"MLServer Documentation\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>MLServer</strong></a>: An open source inference server for machine learning models built by <a href=\"https://www.seldon.io/\" title=\"Seldon Model Deployment\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Seldon</a>. MLServer supports REST and gRPC interfaces out of the box as well the ability to batch requests. It supports the <a href=\"https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html\" title=\"Seldon V2 Inference Protocol\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">V2 inference protocol</a> that standardizes communication protocols around with different inference servers and by that increases their utility and portability.</li>\n<li>\n<a href=\"https://fastapi.tiangolo.com/\" title=\"FastAPI\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>FastAPI</strong></a><strong>:</strong> A fast and high-performance web framework for building APIs with Python.</li>\n</ol>\n<p>These two libraries enable three different methods for our users to implement their online inference serving layer, starting from no code or low code with MLServer, to a fully customizable API using FastAPI. The following table describes the differences between these approaches:</p>\n\n<table>\n<tbody>\n<tr>\n<td><strong>Serving Layer Type</strong></td>\n<td><strong>Description</strong></td>\n<td><strong>What to Use</strong></td>\n</tr>\n<tr>\n<td><strong>No Code</strong></td>\n<td>\n<p>Uses MLServer’s pre-built <a href=\"https://mlserver.readthedocs.io/en/latest/index.html#inference-runtimes\" title=\"MLServer Inference Implementations\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">inference serving implementations.</a>&nbsp;Only requires configuration changes in MLServer json files.</p>\n</td>\n<td>\n<p>For this serving layer type, you have a model and all you need is to deploy it behind an end point such as Scikit-learn, XGBoost, LightGBM, ect.</p>\n</td>\n</tr>\n<tr>\n<td><strong>Low Code</strong></td>\n<td>\n<p>Uses MLServer’s <a href=\"https://mlserver.readthedocs.io/en/latest/runtimes/custom.html\" title=\"MLServer Custom Inference Implementations\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">custom serving implementation</a>.&nbsp; Requires minimal code implementation of the serving class.</p>\n</td>\n<td>\n<p>Include transformation or business logic&nbsp;in the serving layer, or a model that uses an unsupported ML library in MLServer.</p>\n</td>\n</tr>\n<tr>\n<td><strong>Full Custom</strong></td>\n<td>In this case, the user gets pre-defined boilerplate code for a FastAPI serving layer which they can fully customize.</td>\n<td>The machine learning use case needs to expose additional endpoints or a requirement that is unsupported in MLServer.&nbsp;</td>\n</tr>\n</tbody>\n</table>\n\n<h3>Creating the Serving Layer in Merlin</h3>\n<p>In order to abstract away much of the complexity of starting to write a serving layer from scratch, we wrote Merlin CLI, which uses input from the user to build a custom serving layer. This serving layer will be generated from a predefined cookiecutter, and will provide boilerplate code for the user to build on.&nbsp;</p>\n<figure><img alt=\"Creating the serving layer in Merlin\" data-src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.33.50_AM.png?format=webp&amp;v=1678458860\" src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.33.50_AM.png?format=webp&amp;v=1678458860\">\n<figcaption>Creating the serving layer in Merlin</figcaption>\n</figure>\n<p>In the image above, the user can choose the inference library that they want to implement their serving layer in, then pick the machine learning library that they will use to load and serve their model.</p>\n<h3>Example of Serving a Custom Model with MLServer</h3>\n<p>Once we’ve created the boilerplate code for the serving layer, we can start implementing the specific logic for our use-case. In the following example, we’re serving a <a href=\"https://huggingface.co/\" title=\"Hugging Face\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">🤗Hugging Face 🤗</a> model to translate English to French with online inference.</p>\n\n<p>We create a class for our model which inherits from <em>mlserver.MLModel and implements the following methods:</em></p>\n<ol>\n<li>\n<strong>Load: </strong>loads the model and any additional required artifact to memory.</li>\n<li>\n<strong>Predict:</strong> generates predictions from the model based on a payload that the method received.</li>\n</ol>\n<p>In addition, with MLServer, there’s an option to serve models as a <a href=\"https://mlserver.readthedocs.io/en/latest/runtimes/huggingface.html\" title=\"HuggingFace runtime for MLServer\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">pre-built HuggingFace runtime</a> using only configuration files:</p>\n\n<p>The above is a very basic example of how low code and no-code examples can be used with MLServer without the need to define the whole API from scratch. This allows our users to focus only on what matters most to them, which is loading the model and serving it to generate inference.</p>\n<h3>Testing the Serving Layer with Merlin Workspaces</h3>\n<p>For testing purposes, our users can take advantage of the tools we already have in Merlin. They can create Merlin Workspaces, which are dedicated environments that can be defined by code, dependencies and required resources. These dedicated environments also enable distributed computing and scalability for the machine learning tasks that run on them. They are intended to be short lived, and our users can create them, run the serving layer in them, and expose a temporary endpoint for development, debugging and stress testing. This enables fast iterations on the serving layer, as it abstracts the friction of deploying a complete service between each iteration.&nbsp;</p>\n<figure><img alt=\"Merlin Workspaces Architecture\" data-src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.32.14_AM.png?format=webp&amp;v=1678458756\" src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.32.14_AM.png?format=webp&amp;v=1678458756\">\n<figcaption>A high level architecture diagram of Merlin Workspaces</figcaption>\n</figure>\n<p>In the diagram above, we can see how different use cases run with their respective Merlin Workspaces on Merlin. Each use case can define their own infrastructure resources, machine learning libraries, Python packages, serving layer, etc. and run them in an isolated and scalable environment. This is where our users can iterate on their use cases while developing their model and serving layer, in the same way that they would for any other part of their machine learning workflow.</p>\n<p>The Merlin Workspace enables our users to access the swagger page of their API. This enables them to test out their code and make sure that it works before deploying it as a service.&nbsp;&nbsp;</p>\n<h2>Merlin Online Inference Service Deployment</h2>\n<p>Once the serving layer has been tested and validated, it’s ready to be deployed to production as a service. The deployment phase is where all of the Merlin components will be put together to form the deployed service. These components include the serving layer code, API, model, artifacts, libraries and requirements, image container that was created in the CI/CD pipelines and any additional configuration libraries and package requirements to form the Merlin Service.</p>\n<figure><img alt=\"Merlin Service Components\" data-src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.28.34_AM.png?format=webp&amp;v=1678458552\" src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.28.34_AM.png?format=webp&amp;v=1678458552\">\n<figcaption>The components that form a Merlin Service</figcaption>\n</figure>\n<h3>Creating a Merlin Service</h3>\n<p>Similarly to the serving layer creation, we leverage the same Merlin CLI to create a Merlin Service in Shopify’s ecosystem. When generating the service, we utilize as much of Shopify’s service infrastructure to deploy Merlin Services as any other Shopify service. This ensures that Merlin Services can scale to Shopify’s capabilities, as well as allowing them to integrate and benefit from the existing tooling that we have in place.</p>\n<p>When a Merlin Service is created, it is registered to Shopify’s Services DB. Services DB is an internal tool used to track all production services running at Shopify. It supports creating new services and provides comprehensive views and tools to help development teams maintain and operate their services with high quality.</p>\n<figure><img alt=\"Merlin Service Deployment Process\" data-src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.26.27_AM.png?format=webp&amp;v=1678458435\" src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.26.27_AM.png?format=webp&amp;v=1678458435\">\n<figcaption>The deployment process of a Merlin Service</figcaption>\n</figure>\n<p>Once the Merlin Service is created, the entire build and deployment workflow is automatically generated for it. When a user merges new changes to their repository, Shopify’s Buildkite pipeline is automatically triggered and among other actions, builds the image for the service. In the next step of the workflow, that image is then deployed on Shopify’s Kubernetes clusters using our <a href=\"https://github.com/Shopify/shipit-engine\" title=\"Shopify shipit-engine\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">internal Shipit pipelines</a>.</p>\n<h3>Merlin Service Configuration</h3>\n<p>Each Merlin Service is created with two configuration files, one for a production environment and one for a staging environment. These include settings for the resources, parameters and related artifacts of the service. Having a different configuration for each environment allows the user to define a different set of resources and parameters per environment. This can help optimize the resources used by the service, which in turn can reduce infrastructure cost. In addition, our users can leverage the staging environment of Merlin Services to test new model versions or configuration settings before deploying them to production.</p>\n<p>The following is an example of a Merlin Service configuration file which contains different parameters such as the project name, metadata, artifact paths, CPU, memory, GPUs, autoscaling configuration, etc.:&nbsp;</p>\n\n<p>This example shows a configuration file for the <em>classification_model_example project which uses MLServer to serve its model. It uses at least 3 replicas that can scale to 10 replicas, and each one has 2 CPUs, 32 GB of memory and <em>nvidia-tesla-t4 GPU. In addition, when the service starts, it will load a model from our model registry.</em></em></p>\n<h2>What’s Next for Merlin Online Inference</h2>\n<p>And there you have it, our path for deploying and serving machine learning models for real-time predictions with Merlin. To sum it up, our users start by creating a Merlin Project that contains everything required for their machine learning use case. An image is automatically built for their project which is then used in the training pipeline, resulting in a trained model that is saved to our model registry. If the use case requires online inference, Merlin can be used to create a serving layer and a dedicated Merlin Service. Once the service is deployed to production, Merlin users can continue iterating on their models and deploy new versions as they become available.</p>\n<figure><img alt=\"Merlin Online Inference User Journey\" data-src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.24.04_AM.png?format=webp&amp;v=1678458272\" src=\"https://cdn.shopify.com/s/files/1/0779/4361/files/Screenshot_2023-03-10_at_9.24.04_AM.png?format=webp&amp;v=1678458272\">\n<figcaption>A high level overview of the user journey</figcaption>\n</figure>\n<p>As we onboard new online inference use cases to Merlin we plan to tackle additional areas in order to enable:</p>\n<ol>\n<li>\n<strong>Ensemble models / inference graphs:&nbsp;</strong>while we have the ability to deploy and serve machine learning models, we are aware that in some cases we will need to combine multiple models in the inference process. We are looking into leveraging some open source tools that can help us achieve that with&nbsp;<a href=\"https://docs.ray.io/en/latest/serve/index.html\" title=\"Ray Serve\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Ray Serve,</a>&nbsp;<a href=\"https://www.seldon.io/solutions/open-source-projects/core\" title=\"Seldon Core\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Seldon Core</a>&nbsp;or&nbsp;<a href=\"https://www.bentoml.com/\" title=\"BentoML\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">BentoML</a>.</li>\n<li>\n<strong>Monitoring for online inference:&nbsp;</strong>with our current abilities, it is possible to create workflows, metrics and dashboards to detect different drifts. However, at the moment this step is completely manual and requires a lot of effort from our users. We want to enable a platform-based monitoring solution that will seamlessly integrate with the rest of Merlin.</li>\n<li>\n<strong>Continuous training:&nbsp;</strong>as the amount of input and data which are used for predictions increases, some of our use cases will need to start training their models more frequently and will require an automated and easier deployment process. We are looking into automating more of the service management process and lifecycle of our online inference models.</li>\n</ol>\n<p>While online inference is still a new part of Merlin, it’s already empowering our users and data science teams with the low latency, scalability and fast iterations that we had in mind when designing it. We're excited to keep building the platform and onboarding new use cases, so we can continue to unlock new possibilities to keep merchants on the cutting edge of innovation. With Merlin we help enable the millions of businesses powered by Shopify.</p>\n<p><strong>Isaac Vidas&nbsp;</strong>is a tech lead on the ML Platform team, focusing on designing and building Merlin, Shopify’s machine learning platform. Connect with Isaac on <a href=\"https://www.linkedin.com/in/isaac-vidas/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">LinkedIn</a>.</p>\n<hr>\n<p>Are you passionate about solving data problems and eager to learn more about Shopify? Check out openings on our <a href=\"https://www.shopify.com/ca/careers\" title=\"Shopify Careers\" target=\"_blank\">careers page</a>.</p>\n</div></div>","textContent":"\n  In 2022, we shipped Merlin, Shopify’s new and improved machine learning platform built on Ray. We built a flexible platform to support the varying requirements, inputs, data types, dependencies and integrations we deal with at Shopify (you can read all about Merlin’s architecture in our previous blog). Since then, we’ve enhanced the platform by onboarding more use cases and adding features to complete the end-to-end machine learning workflow, including:\n\n\nMerlin Online Inference, which provides the ability to deploy and serve machine learning models for real-time predictions.\n\nModel Registry and experiment tracking with Comet ML.\n\nMerlin Pipelines, a framework for reproducible machine learning pipelines on top of Merlin.\n\nPano Feature Store, an offline / online feature store built on top of an open source feature store, Feast.\n\nThe ability to provide real-time predictions for user-facing applications and services is an important requirement at Shopify, and will become increasingly critical as more machine learning models are integrated closer to user-facing products. But it’s a challenging requirement. As machine learning is utilized by many different teams across Shopify, each with its own use-cases and requirements, we had to ensure that Merlin’s online inference could be an effective, generalized solution. We needed to build something robust that could be used by all of our use-cases and allow low-latency, while serving machine learning models at Shopify scale.\nIn this blog post, we’ll walk through how we built Merlin’s online inference capabilities to deploy and serve machine learning models for real-time prediction at scale. We’ll cover everything from the serving layer where our users can focus solely on their specific inference logic, to service deployment where we utilized our internal service ecosystem to ensure that online inference services can scale to Shopify’s capabilities.\nWhat is online inference?\nIn machine learning workflows, once a model has been trained, evaluated and productionized, it can be applied on input data to return predictions. This is called machine learning inference. There are two main types of inference, batch and online inference.\nWhile batch inference can run periodically on a finite set of data, online inference is the ability to compute predictions in real-time as the input becomes available. With online inference, the observations that we produce predictions for can be infinite, as we’re using streams of data that are generated over time.\nPopular examples for online inference and real-time predictions can be found in many use cases. Some examples are recommender systems, where having real-time predictions can provide users with the most relevant results, or fraud detection, where the ability to detect fraud is required as soon as it happens. Other Shopify specific use cases that benefit from online inference are product categorization and inbox classification.\nWhen serving machine learning models for real time predictions, latency becomes much more important compared to batch jobs, as the latency of machine learning models can impact the performance of user facing services and impact the business. When considering online inference for a machine learning use case, it is important to consider: \n\nThe cost\nThe use-case requirements \nThe skillset of the team members (e.g. being able to handle running and maintaining machine learning services in production versus running them in batch jobs).\n\nAs we prepared to expand Merlin for online inference, we first interviewed our internal stakeholders to better understand their use-case requirements more in detail. Once we had these, we could design the system, its architecture and infrastructure in ways that optimize for cost and performance, as well as advise the machine learning teams on staffing and required support, based on the service-level objectives of their use-case.\nOnline Inference with Merlin\nThere are several requirements that we set out to provide with Merlin Online Inference:\n\n\nRobust and flexible serving of machine learning models, to enable us to serve and deploy different models and machine learning libraries used at Shopify (e.g. TensorFlow, PyTorch, XGBoost).\n\nLow latency, in order to optimize processing of an inference request to provide responses with minimal delay.\n\nState-of-the-art features for online inference services, like rolling deployments, autoscaling, observability, automatic model updates, etc.\n\nIntegration with the Merlin machine learning platform, so users can utilize the different Merlin features, such as model registry and Pano online feature store.\n\nSeamless and streamlined service creation, management and deployment for machine learning models.\n\nWith these requirements in mind, we aimed to build Merlin Online Inference to enable deploying and serving machine learning models for real-time predictions.\nMerlin Online Inference Architecture\nIn our previous Merlin blog post, we described how Merlin is used for training machine learning models. With Merlin Online Inference, we can provide our data scientists and machine learning engineers with the tools to deploy and serve their machine learning models and use cases.\nEvery machine learning use case that requires online inference runs as its own dedicated service. These services are deployed on Shopify’s Kubernetes clusters (Google Kubernetes Engine) as any other Shopify service. Each service lives in its own Kubernetes namespace and can be configured individually, for example, to autoscale based on different parameters and metrics.\n\nA high level architecture diagram of Merlin Online Inference\n\nEach service loads its dedicated machine learning model from our model registry, Comet ML, as well as any other required artifacts. Different clients can call an inference endpoint to generate predictions in real-time. The main clients that use Merlin Online Inference services are Shopify’s core services (or any other internal service that requires real-time inference), as well as streaming pipelines on Flink for near real-time predictions. Pano, our feature store, can be used to access features in low latency during inference both from the Merlin Online Inference service or from the different clients that send requests to the service.\nEach service has a monitoring dashboard with predefined metrics such as latency, requests per second, CPU, etc. This can be used to observe the health of the service, and can be further customized per service.\nEach Merlin Online Inference service has two main components:\n\n\nServing layer: the API that enables an endpoint to return predictions from a model.\n\nDeployment: how the service will be deployed in Shopify’s infrastructure.\n\nServing Layer\nThe Merlin Online Inference serving layer is the API that serves the model (or models). It exposes an endpoint for processing inputs from the clients, and returns predictions from the model. The serving layer accomplishes the following:\n\nStarts a web server for the service\nLoads the model and additional artifacts to memory as part of the initialization process\nExposes an endpoint for the inference function that will take features as inputs and return predictions\n\n\nA high level diagram of the serving layer\n\nThe serving layer is written in Python, which makes it easier for our data scientists and machine learning engineers to implement it. It’s defined in the Merlin Project of each use case. A Merlin Project is a folder in our Merlin mono-repo where the code, configuration and tests of the use case are kept. This allows the serving layer to reuse different parts of the machine learning workflow logic. The serving layer is added to the Merlin Docker image, which is created by Podman from a dedicated Dockerfile. It is then deployed in the Merlin Online Inference Kubernetes clusters.\nServing Layer Types\nWhen analyzing the requirements of our users, we identified a need to support multiple types of serving layers. We wanted to make sure that we can abstract from our users a lot of the hassle of writing a complete API for every machine learning use case, while also enabling other more custom services.\nIn order to do that, we currently support two types of serving libraries with Merlin Online Inference:\n\n\nMLServer: An open source inference server for machine learning models built by Seldon. MLServer supports REST and gRPC interfaces out of the box as well the ability to batch requests. It supports the V2 inference protocol that standardizes communication protocols around with different inference servers and by that increases their utility and portability.\n\nFastAPI: A fast and high-performance web framework for building APIs with Python.\n\nThese two libraries enable three different methods for our users to implement their online inference serving layer, starting from no code or low code with MLServer, to a fully customizable API using FastAPI. The following table describes the differences between these approaches:\n\n\n\n\nServing Layer Type\nDescription\nWhat to Use\n\n\nNo Code\n\nUses MLServer’s pre-built inference serving implementations. Only requires configuration changes in MLServer json files.\n\n\nFor this serving layer type, you have a model and all you need is to deploy it behind an end point such as Scikit-learn, XGBoost, LightGBM, ect.\n\n\n\nLow Code\n\nUses MLServer’s custom serving implementation.  Requires minimal code implementation of the serving class.\n\n\nInclude transformation or business logic in the serving layer, or a model that uses an unsupported ML library in MLServer.\n\n\n\nFull Custom\nIn this case, the user gets pre-defined boilerplate code for a FastAPI serving layer which they can fully customize.\nThe machine learning use case needs to expose additional endpoints or a requirement that is unsupported in MLServer. \n\n\n\n\nCreating the Serving Layer in Merlin\nIn order to abstract away much of the complexity of starting to write a serving layer from scratch, we wrote Merlin CLI, which uses input from the user to build a custom serving layer. This serving layer will be generated from a predefined cookiecutter, and will provide boilerplate code for the user to build on. \n\nCreating the serving layer in Merlin\n\nIn the image above, the user can choose the inference library that they want to implement their serving layer in, then pick the machine learning library that they will use to load and serve their model.\nExample of Serving a Custom Model with MLServer\nOnce we’ve created the boilerplate code for the serving layer, we can start implementing the specific logic for our use-case. In the following example, we’re serving a 🤗Hugging Face 🤗 model to translate English to French with online inference.\n\nWe create a class for our model which inherits from mlserver.MLModel and implements the following methods:\n\n\nLoad: loads the model and any additional required artifact to memory.\n\nPredict: generates predictions from the model based on a payload that the method received.\n\nIn addition, with MLServer, there’s an option to serve models as a pre-built HuggingFace runtime using only configuration files:\n\nThe above is a very basic example of how low code and no-code examples can be used with MLServer without the need to define the whole API from scratch. This allows our users to focus only on what matters most to them, which is loading the model and serving it to generate inference.\nTesting the Serving Layer with Merlin Workspaces\nFor testing purposes, our users can take advantage of the tools we already have in Merlin. They can create Merlin Workspaces, which are dedicated environments that can be defined by code, dependencies and required resources. These dedicated environments also enable distributed computing and scalability for the machine learning tasks that run on them. They are intended to be short lived, and our users can create them, run the serving layer in them, and expose a temporary endpoint for development, debugging and stress testing. This enables fast iterations on the serving layer, as it abstracts the friction of deploying a complete service between each iteration. \n\nA high level architecture diagram of Merlin Workspaces\n\nIn the diagram above, we can see how different use cases run with their respective Merlin Workspaces on Merlin. Each use case can define their own infrastructure resources, machine learning libraries, Python packages, serving layer, etc. and run them in an isolated and scalable environment. This is where our users can iterate on their use cases while developing their model and serving layer, in the same way that they would for any other part of their machine learning workflow.\nThe Merlin Workspace enables our users to access the swagger page of their API. This enables them to test out their code and make sure that it works before deploying it as a service.  \nMerlin Online Inference Service Deployment\nOnce the serving layer has been tested and validated, it’s ready to be deployed to production as a service. The deployment phase is where all of the Merlin components will be put together to form the deployed service. These components include the serving layer code, API, model, artifacts, libraries and requirements, image container that was created in the CI/CD pipelines and any additional configuration libraries and package requirements to form the Merlin Service.\n\nThe components that form a Merlin Service\n\nCreating a Merlin Service\nSimilarly to the serving layer creation, we leverage the same Merlin CLI to create a Merlin Service in Shopify’s ecosystem. When generating the service, we utilize as much of Shopify’s service infrastructure to deploy Merlin Services as any other Shopify service. This ensures that Merlin Services can scale to Shopify’s capabilities, as well as allowing them to integrate and benefit from the existing tooling that we have in place.\nWhen a Merlin Service is created, it is registered to Shopify’s Services DB. Services DB is an internal tool used to track all production services running at Shopify. It supports creating new services and provides comprehensive views and tools to help development teams maintain and operate their services with high quality.\n\nThe deployment process of a Merlin Service\n\nOnce the Merlin Service is created, the entire build and deployment workflow is automatically generated for it. When a user merges new changes to their repository, Shopify’s Buildkite pipeline is automatically triggered and among other actions, builds the image for the service. In the next step of the workflow, that image is then deployed on Shopify’s Kubernetes clusters using our internal Shipit pipelines.\nMerlin Service Configuration\nEach Merlin Service is created with two configuration files, one for a production environment and one for a staging environment. These include settings for the resources, parameters and related artifacts of the service. Having a different configuration for each environment allows the user to define a different set of resources and parameters per environment. This can help optimize the resources used by the service, which in turn can reduce infrastructure cost. In addition, our users can leverage the staging environment of Merlin Services to test new model versions or configuration settings before deploying them to production.\nThe following is an example of a Merlin Service configuration file which contains different parameters such as the project name, metadata, artifact paths, CPU, memory, GPUs, autoscaling configuration, etc.: \n\nThis example shows a configuration file for the classification_model_example project which uses MLServer to serve its model. It uses at least 3 replicas that can scale to 10 replicas, and each one has 2 CPUs, 32 GB of memory and nvidia-tesla-t4 GPU. In addition, when the service starts, it will load a model from our model registry.\nWhat’s Next for Merlin Online Inference\nAnd there you have it, our path for deploying and serving machine learning models for real-time predictions with Merlin. To sum it up, our users start by creating a Merlin Project that contains everything required for their machine learning use case. An image is automatically built for their project which is then used in the training pipeline, resulting in a trained model that is saved to our model registry. If the use case requires online inference, Merlin can be used to create a serving layer and a dedicated Merlin Service. Once the service is deployed to production, Merlin users can continue iterating on their models and deploy new versions as they become available.\n\nA high level overview of the user journey\n\nAs we onboard new online inference use cases to Merlin we plan to tackle additional areas in order to enable:\n\n\nEnsemble models / inference graphs: while we have the ability to deploy and serve machine learning models, we are aware that in some cases we will need to combine multiple models in the inference process. We are looking into leveraging some open source tools that can help us achieve that with Ray Serve, Seldon Core or BentoML.\n\nMonitoring for online inference: with our current abilities, it is possible to create workflows, metrics and dashboards to detect different drifts. However, at the moment this step is completely manual and requires a lot of effort from our users. We want to enable a platform-based monitoring solution that will seamlessly integrate with the rest of Merlin.\n\nContinuous training: as the amount of input and data which are used for predictions increases, some of our use cases will need to start training their models more frequently and will require an automated and easier deployment process. We are looking into automating more of the service management process and lifecycle of our online inference models.\n\nWhile online inference is still a new part of Merlin, it’s already empowering our users and data science teams with the low latency, scalability and fast iterations that we had in mind when designing it. We're excited to keep building the platform and onboarding new use cases, so we can continue to unlock new possibilities to keep merchants on the cutting edge of innovation. With Merlin we help enable the millions of businesses powered by Shopify.\nIsaac Vidas is a tech lead on the ML Platform team, focusing on designing and building Merlin, Shopify’s machine learning platform. Connect with Isaac on LinkedIn.\n\nAre you passionate about solving data problems and eager to learn more about Shopify? Check out openings on our careers page.\n","length":18497,"excerpt":"Learn how Shopify Data built new online inference capabilities into its Machine Learning Platform to deploy and serve models for real-time prediction at scale.","byline":null,"dir":null,"siteName":"Shopify","lang":"en"},"finalizedMeta":{"title":"Unlocking Real-time Predictions with Shopify's Machine Learning Platform","description":"Learn how Shopify Data built new online inference capabilities into its Machine Learning Platform to deploy and serve models for real-time prediction at scale.","author":false,"creator":"","publisher":false,"date":"2023-04-24T14:13:33.473Z","topics":[]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"Unlocking Real-time Predictions with Shopify's Machine Learning Platform (2023)","description":"Learn how Shopify Data built new online inference capabilities into its Machine Learning Platform to deploy and serve models for real-time prediction at scale.","canonical":"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions","keywords":[],"image":"","firstParagraph":"Shopify Engineering"},"dublinCore":{},"opengraph":{"title":"Unlocking Real-time Predictions with Shopify's Machine Learning Platform","description":"Learn how Shopify Data built new online inference capabilities into its Machine Learning Platform to deploy and serve models for real-time prediction at scale.","url":"https://shopify.engineering/shopifys-machine-learning-platform-real-time-predictions","site_name":"Shopify","locale":false,"type":"article","typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":"https://cdn.shopify.com/s/files/1/0779/4361/articles/ShopifyEng_BlogIllustrations_220216_72ppi_03_TheMagicOfMerlin-ShopifysMachineLearningPlatform_3e07a5ed-cd94-44e2-b4ca-c6db1ec81bd6.jpg?v=1678451850"},"twitter":{"site":false,"description":false,"card":false,"creator":false,"title":false,"image":false,"label1":"Reading time","data1":"12 minutes"},"archivedData":{"link":false,"wayback":false}}}