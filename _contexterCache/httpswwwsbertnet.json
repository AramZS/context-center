{"initialLink":"https://www.sbert.net/","sanitizedLink":"https://www.sbert.net/","finalLink":"https://www.sbert.netindex.html/","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://www.sbert.netindex.html/\" itemprop=\"url\">SentenceTransformers Documentation — Sentence-Transformers  documentation</a></contexter-box-head></contexter-box-head><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2022-03-23T21:52:33.658Z\">2/23/2022</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>You can install it using pip:</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a href=\"https://web.archive.org/web/20220323215237/https://www.sbert.net/\" is=\"contexter-link\" target=\"_blank\" rel=\"timemap\" class=\"read-link archive-link\" itemprop=\"archivedAt\" slot=\"archive-link\">Archived</a><a is=\"contexter-link\" href=\"https://www.sbert.netindex.html/\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"6fc339bd11011cf4387d1302ea50aa3ecc597e60","data":{"originalLink":"https://www.sbert.net/","sanitizedLink":"https://www.sbert.net/","canonical":"https://www.sbert.netindex.html/","htmlText":"\r\n\r\n<!DOCTYPE html>\r\n<html class=\"writer-html5\" lang=\"en\" >\r\n<head>\r\n  <meta charset=\"utf-8\">\r\n  \r\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n  \r\n  <title>SentenceTransformers Documentation &mdash; Sentence-Transformers  documentation</title>\r\n  \r\n\r\n  \r\n  <link rel=\"stylesheet\" href=\"_static/css/theme.css\" type=\"text/css\" />\r\n  <link rel=\"stylesheet\" href=\"_static/pygments.css\" type=\"text/css\" />\r\n  <link rel=\"stylesheet\" href=\"_static/css/custom.css\" type=\"text/css\" />\r\n\r\n  \r\n  \r\n    <link rel=\"shortcut icon\" href=\"_static/favicon.ico\"/>\r\n  \r\n  \r\n  \r\n    <link rel=\"canonical\" href=\"https://www.sbert.netindex.html\"/>\r\n  \r\n\r\n  \r\n  <!--[if lt IE 9]>\r\n    <script src=\"_static/js/html5shiv.min.js\"></script>\r\n  <![endif]-->\r\n  \r\n    \r\n      <script type=\"text/javascript\" id=\"documentation_options\" data-url_root=\"./\" src=\"_static/documentation_options.js\"></script>\r\n        <script src=\"_static/jquery.js\"></script>\r\n        <script src=\"_static/underscore.js\"></script>\r\n        <script src=\"_static/doctools.js\"></script>\r\n        <script src=\"_static/language_data.js\"></script>\r\n        <script src=\"_static/js/custom.js\"></script>\r\n    \r\n    <script type=\"text/javascript\" src=\"_static/js/theme.js\"></script>\r\n\r\n    \r\n    <link rel=\"index\" title=\"Index\" href=\"genindex.html\" />\r\n    <link rel=\"search\" title=\"Search\" href=\"search.html\" />\r\n    <link rel=\"next\" title=\"Installation\" href=\"docs/installation.html\" /> \r\n</head>\r\n\r\n<body class=\"wy-body-for-nav\">\r\n\r\n   \r\n  <div class=\"wy-grid-for-nav\">\r\n    \r\n    <nav data-toggle=\"wy-nav-shift\" class=\"wy-nav-side\">\r\n      <div class=\"wy-side-scroll\">\r\n        <div class=\"wy-side-nav-search\" >\r\n          \r\n\r\n            <a href=\"#\">\r\n              <img src=\"_static/logo.png\" class=\"logo\" alt=\"Logo\"/>\r\n              <span class=\"icon icon-home project-name\"> Sentence-Transformers</span>\r\n            </a>\r\n\r\n            <div style=\"display: flex; justify-content: center;\">\r\n              <div id=\"twitter-button\">\r\n                <a href=\"https://twitter.com/Nils_Reimers\" target=\"_blank\" title=\"Follow SBERT on Twitter\"><img src=\"/_static/Twitter_Logo_White.svg\" height=\"20\" style=\"margin: 0px 10px 0px -10px;\"> </a>\r\n              </div>\r\n              <div id=\"github-button\"></div>\r\n            </div>\r\n\r\n          \r\n            \r\n            \r\n          \r\n\r\n          \r\n<div role=\"search\">\r\n  <form id=\"rtd-search-form\" class=\"wy-form\" action=\"search.html\" method=\"get\">\r\n    <input type=\"text\" name=\"q\" placeholder=\"Search docs\" />\r\n    <input type=\"hidden\" name=\"check_keywords\" value=\"yes\" />\r\n    <input type=\"hidden\" name=\"area\" value=\"default\" />\r\n  </form>\r\n</div>\r\n\r\n          \r\n        </div>\r\n\r\n        \r\n        <div class=\"wy-menu wy-menu-vertical\" data-spy=\"affix\" role=\"navigation\" aria-label=\"main navigation\">\r\n          \r\n            \r\n            \r\n              \r\n            \r\n            \r\n              <p class=\"caption\"><span class=\"caption-text\">Overview</span></p>\r\n<ul>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/installation.html\">Installation</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/quickstart.html\">Quickstart</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/pretrained_models.html\">Pretrained Models</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/pretrained_cross-encoders.html\">Pretrained Cross-Encoders</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/publications.html\">Publications</a></li>\r\n</ul>\r\n<p class=\"caption\"><span class=\"caption-text\">Usage</span></p>\r\n<ul>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/computing-embeddings/README.html\">Computing Sentence Embeddings</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/usage/semantic_textual_similarity.html\">Semantic Textual Similarity</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/semantic-search/README.html\">Semantic Search</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/retrieve_rerank/README.html\">Retrieve &amp; Re-Rank</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/clustering/README.html\">Clustering</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/paraphrase-mining/README.html\">Paraphrase Mining</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/parallel-sentence-mining/README.html\">Translated Sentence Mining</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/cross-encoder/README.html\">Cross-Encoders</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/image-search/README.html\">Image Search</a></li>\r\n</ul>\r\n<p class=\"caption\"><span class=\"caption-text\">Training</span></p>\r\n<ul>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/training/overview.html\">Training Overview</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/multilingual/README.html\">Multilingual-Models</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/distillation/README.html\">Model Distillation</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/cross-encoder/README.html\">Cross-Encoders</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/data_augmentation/README.html\">Augmented SBERT</a></li>\r\n</ul>\r\n<p class=\"caption\"><span class=\"caption-text\">Training Examples</span></p>\r\n<ul>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/sts/README.html\">Semantic Textual Similarity</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/nli/README.html\">Natural Language Inference</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/paraphrases/README.html\">Paraphrase Data</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/quora_duplicate_questions/README.html\">Quora Duplicate Questions</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/ms_marco/README.html\">MS MARCO</a></li>\r\n</ul>\r\n<p class=\"caption\"><span class=\"caption-text\">Unsupervised Learning</span></p>\r\n<ul>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/unsupervised_learning/README.html\">Unsupervised Learning</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/domain_adaptation/README.html\">Domain Adaptation</a></li>\r\n</ul>\r\n<p class=\"caption\"><span class=\"caption-text\">Package Reference</span></p>\r\n<ul>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/package_reference/SentenceTransformer.html\">SentenceTransformer</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/package_reference/util.html\">util</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/package_reference/models.html\">Models</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/package_reference/losses.html\">Losses</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/package_reference/evaluation.html\">Evaluation</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/package_reference/datasets.html\">Datasets</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/package_reference/cross_encoder.html\">cross_encoder</a></li>\r\n</ul>\r\n\r\n            \r\n          \r\n        </div>\r\n        \r\n      </div>\r\n    </nav>\r\n\r\n    <section data-toggle=\"wy-nav-shift\" class=\"wy-nav-content-wrap\">\r\n\r\n      \r\n      <nav class=\"wy-nav-top\" aria-label=\"top navigation\">\r\n        \r\n          <i data-toggle=\"wy-nav-top\" class=\"fa fa-bars\"></i>\r\n          <a href=\"#\">Sentence-Transformers</a>\r\n        \r\n      </nav>\r\n\r\n\r\n      <div class=\"wy-nav-content\">\r\n        \r\n        <div class=\"rst-content\">\r\n        \r\n          \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n<div role=\"navigation\" aria-label=\"breadcrumbs navigation\">\r\n\r\n  <ul class=\"wy-breadcrumbs\">\r\n    \r\n      <li><a href=\"#\" class=\"icon icon-home\"></a> &raquo;</li>\r\n        \r\n      <li>SentenceTransformers Documentation</li>\r\n    \r\n    \r\n      <li class=\"wy-breadcrumbs-aside\">\r\n        \r\n          \r\n            \r\n              <a href=\"https://github.com/UKPLab/sentence-transformers/blob/master/index.rst\" class=\"fa fa-github\"> Edit on GitHub</a>\r\n            \r\n          \r\n        \r\n      </li>\r\n    \r\n  </ul>\r\n\r\n  \r\n  <hr/>\r\n</div>\r\n          <div role=\"main\" class=\"document\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\r\n           <div itemprop=\"articleBody\">\r\n            \r\n  <div class=\"section\" id=\"sentencetransformers-documentation\">\r\n<h1>SentenceTransformers Documentation<a class=\"headerlink\" href=\"#sentencetransformers-documentation\" title=\"Permalink to this headline\">¶</a></h1>\r\n<p>SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. The initial work is described in our paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1908.10084\">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a>.</p>\r\n<p>You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for <a class=\"reference external\" href=\"docs/usage/semantic_textual_similarity.html\">semantic textual similar</a>, <a class=\"reference external\" href=\"examples/applications/semantic-search/README.html\">semantic search</a>, or <a class=\"reference external\" href=\"examples/applications/paraphrase-mining/README.html\">paraphrase mining</a>.</p>\r\n<p>The framework is based on <a class=\"reference external\" href=\"https://pytorch.org/\">PyTorch</a> and <a class=\"reference external\" href=\"https://huggingface.co/transformers/\">Transformers</a> and offers a large collection of <a class=\"reference external\" href=\"docs/pretrained_models.html\">pre-trained models</a> tuned for various tasks. Further, it is easy to <a class=\"reference external\" href=\"docs/training/overview.html\">fine-tune your own models</a>.</p>\r\n</div>\r\n<div class=\"section\" id=\"installation\">\r\n<h1>Installation<a class=\"headerlink\" href=\"#installation\" title=\"Permalink to this headline\">¶</a></h1>\r\n<p>You can install it using pip:</p>\r\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"o\">-</span><span class=\"n\">U</span> <span class=\"n\">sentence</span><span class=\"o\">-</span><span class=\"n\">transformers</span>\r\n</pre></div>\r\n</div>\r\n<p>We recommend <strong>Python 3.6</strong> or higher, and at least <strong>PyTorch 1.6.0</strong>. See <a class=\"reference external\" href=\"docs/installation.html\">installation</a> for further installation options, especially if you want to use a GPU.</p>\r\n</div>\r\n<div class=\"section\" id=\"usage\">\r\n<h1>Usage<a class=\"headerlink\" href=\"#usage\" title=\"Permalink to this headline\">¶</a></h1>\r\n<p>The usage is as simple as:</p>\r\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">from</span> <span class=\"nn\">sentence_transformers</span> <span class=\"kn\">import</span> <span class=\"n\">SentenceTransformer</span>\r\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">SentenceTransformer</span><span class=\"p\">(</span><span class=\"s1\">&#39;paraphrase-MiniLM-L6-v2&#39;</span><span class=\"p\">)</span>\r\n\r\n<span class=\"c1\">#Our sentences we like to encode</span>\r\n<span class=\"n\">sentences</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;This framework generates embeddings for each input sentence&#39;</span><span class=\"p\">,</span>\r\n    <span class=\"s1\">&#39;Sentences are passed as a list of string.&#39;</span><span class=\"p\">,</span>\r\n    <span class=\"s1\">&#39;The quick brown fox jumps over the lazy dog.&#39;</span><span class=\"p\">]</span>\r\n\r\n<span class=\"c1\">#Sentences are encoded by calling model.encode()</span>\r\n<span class=\"n\">embeddings</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">sentences</span><span class=\"p\">)</span>\r\n\r\n<span class=\"c1\">#Print the embeddings</span>\r\n<span class=\"k\">for</span> <span class=\"n\">sentence</span><span class=\"p\">,</span> <span class=\"n\">embedding</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">sentences</span><span class=\"p\">,</span> <span class=\"n\">embeddings</span><span class=\"p\">):</span>\r\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Sentence:&quot;</span><span class=\"p\">,</span> <span class=\"n\">sentence</span><span class=\"p\">)</span>\r\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Embedding:&quot;</span><span class=\"p\">,</span> <span class=\"n\">embedding</span><span class=\"p\">)</span>\r\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;&quot;</span><span class=\"p\">)</span>\r\n</pre></div>\r\n</div>\r\n</div>\r\n<div class=\"section\" id=\"performance\">\r\n<h1>Performance<a class=\"headerlink\" href=\"#performance\" title=\"Permalink to this headline\">¶</a></h1>\r\n<p>Our models are evaluated extensively and achieve state-of-the-art performance on various tasks. Further, the code is tuned to provide the highest possible speed. Have a look at <a class=\"reference external\" href=\"https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/\">Pre-Trained Models</a> for an overview of available models and the respective performance on different tasks.</p>\r\n</div>\r\n<div class=\"section\" id=\"contact\">\r\n<h1>Contact<a class=\"headerlink\" href=\"#contact\" title=\"Permalink to this headline\">¶</a></h1>\r\n<p>Contact person: Nils Reimers, <a class=\"reference external\" href=\"/cdn-cgi/l/email-protection#91f8fff7feb7b2a2a6aab7b2a4a3aab7b2a5a9aafff8fde2bce3f4f8fcf4e3e2b7b2a5a7aaf5f4\">info<span>&#64;</span>nils-reimers<span>&#46;</span>de</a></p>\r\n<p><a class=\"reference external\" href=\"https://www.ukp.tu-darmstadt.de/\">https://www.ukp.tu-darmstadt.de/</a></p>\r\n<p>Don’t hesitate to send us an e-mail or report an issue, if something is broken (and it shouldn’t be) or if you have further questions.</p>\r\n<p><em>This repository contains experimental software and is published for the sole purpose of giving additional background details on the respective publication.</em></p>\r\n</div>\r\n<div class=\"section\" id=\"citing-authors\">\r\n<h1>Citing &amp; Authors<a class=\"headerlink\" href=\"#citing-authors\" title=\"Permalink to this headline\">¶</a></h1>\r\n<p>If you find this repository helpful, feel free to cite our publication <a class=\"reference external\" href=\"https://arxiv.org/abs/1908.10084\">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a>:</p>\r\n<blockquote>\r\n<div><div class=\"highlight-bibtex notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">reimers-2019-sentence-bert</span><span class=\"p\">,</span>\r\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">month</span> <span class=\"p\">=</span> <span class=\"s\">&quot;11&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">&quot;2019&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">publisher</span> <span class=\"p\">=</span> <span class=\"s\">&quot;Association for Computational Linguistics&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class=\"p\">,</span>\r\n<span class=\"p\">}</span>\r\n</pre></div>\r\n</div>\r\n</div></blockquote>\r\n<p>If you use one of the multilingual models, feel free to cite our publication <a class=\"reference external\" href=\"https://arxiv.org/abs/2004.09813\">Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation</a>:</p>\r\n<blockquote>\r\n<div><div class=\"highlight-bibtex notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">reimers-2020-multilingual-sentence-bert</span><span class=\"p\">,</span>\r\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">&quot;Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">&quot;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">month</span> <span class=\"p\">=</span> <span class=\"s\">&quot;11&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">&quot;2020&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">publisher</span> <span class=\"p\">=</span> <span class=\"s\">&quot;Association for Computational Linguistics&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">&quot;https://arxiv.org/abs/2004.09813&quot;</span><span class=\"p\">,</span>\r\n<span class=\"p\">}</span>\r\n</pre></div>\r\n</div>\r\n</div></blockquote>\r\n<p>If you use the code for <a class=\"reference external\" href=\"https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/data_augmentation\">data augmentation</a>, feel free to cite our publication <a class=\"reference external\" href=\"https://arxiv.org/abs/2010.08240\">Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks</a>:</p>\r\n<blockquote>\r\n<div><div class=\"highlight-bibtex notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">thakur-2020-AugSBERT</span><span class=\"p\">,</span>\r\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">&quot;Augmented {SBERT}: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">&quot;Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes  and Gurevych, Iryna&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">&quot;Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">month</span> <span class=\"p\">=</span> <span class=\"nv\">jun</span><span class=\"p\">,</span>\r\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">&quot;2021&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">address</span> <span class=\"p\">=</span> <span class=\"s\">&quot;Online&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">publisher</span> <span class=\"p\">=</span> <span class=\"s\">&quot;Association for Computational Linguistics&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">&quot;https://www.aclweb.org/anthology/2021.naacl-main.28&quot;</span><span class=\"p\">,</span>\r\n  <span class=\"na\">pages</span> <span class=\"p\">=</span> <span class=\"s\">&quot;296--310&quot;</span><span class=\"p\">,</span>\r\n<span class=\"p\">}</span>\r\n</pre></div>\r\n</div>\r\n</div></blockquote>\r\n<div class=\"toctree-wrapper compound\">\r\n<p class=\"caption\"><span class=\"caption-text\">Overview</span></p>\r\n<ul>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/installation.html\">Installation</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/installation.html#install-sentencetransformers\">Install SentenceTransformers</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/installation.html#install-pytorch-with-cuda-support\">Install PyTorch with CUDA-Support</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/quickstart.html\">Quickstart</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/quickstart.html#comparing-sentence-similarities\">Comparing Sentence Similarities</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/quickstart.html#pre-trained-models\">Pre-Trained Models</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/quickstart.html#training-your-own-embeddings\">Training your own Embeddings</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/pretrained_models.html\">Pretrained Models</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/pretrained_models.html#model-overview\">Model Overview</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/pretrained_models.html#semantic-search\">Semantic Search</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/pretrained_models.html#multi-lingual-models\">Multi-Lingual Models</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/pretrained_models.html#image-text-models\">Image &amp; Text-Models</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/pretrained_models.html#other-models\">Other Models</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/pretrained_cross-encoders.html\">Pretrained Cross-Encoders</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/pretrained_cross-encoders.html#ms-marco\">MS MARCO</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/pretrained_cross-encoders.html#squad-qnli\">SQuAD (QNLI)</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/pretrained_cross-encoders.html#stsbenchmark\">STSbenchmark</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/pretrained_cross-encoders.html#quora-duplicate-questions\">Quora Duplicate Questions</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/pretrained_cross-encoders.html#nli\">NLI</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/publications.html\">Publications</a></li>\r\n</ul>\r\n</div>\r\n<div class=\"toctree-wrapper compound\">\r\n<p class=\"caption\"><span class=\"caption-text\">Usage</span></p>\r\n<ul>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/computing-embeddings/README.html\">Computing Sentence Embeddings</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/computing-embeddings/README.html#input-sequence-length\">Input Sequence Length</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/computing-embeddings/README.html#storing-loading-embeddings\">Storing &amp; Loading Embeddings</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/computing-embeddings/README.html#multi-process-multi-gpu-encoding\">Multi-Process / Multi-GPU Encoding</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/computing-embeddings/README.html#sentence-embeddings-with-transformers\">Sentence Embeddings with Transformers</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/usage/semantic_textual_similarity.html\">Semantic Textual Similarity</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/semantic-search/README.html\">Semantic Search</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/semantic-search/README.html#background\">Background</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search\">Symmetric vs. Asymmetric Semantic Search</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/semantic-search/README.html#python\">Python</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/semantic-search/README.html#util-semantic-search\">util.semantic_search</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/semantic-search/README.html#speed-optimization\">Speed Optimization</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/semantic-search/README.html#elasticsearch\">ElasticSearch</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/semantic-search/README.html#approximate-nearest-neighbor\">Approximate Nearest Neighbor</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/semantic-search/README.html#retrieve-re-rank\">Retrieve &amp; Re-Rank</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/semantic-search/README.html#examples\">Examples</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/retrieve_rerank/README.html\">Retrieve &amp; Re-Rank</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/retrieve_rerank/README.html#retrieve-re-rank-pipeline\">Retrieve &amp; Re-Rank Pipeline</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/retrieve_rerank/README.html#retrieval-bi-encoder\">Retrieval: Bi-Encoder</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/retrieve_rerank/README.html#re-ranker-cross-encoder\">Re-Ranker: Cross-Encoder</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/retrieve_rerank/README.html#example-scripts\">Example Scripts</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/retrieve_rerank/README.html#pre-trained-bi-encoders-retrieval\">Pre-trained Bi-Encoders (Retrieval)</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/retrieve_rerank/README.html#pre-trained-cross-encoders-re-ranker\">Pre-trained Cross-Encoders (Re-Ranker)</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/clustering/README.html\">Clustering</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/clustering/README.html#k-means\">k-Means</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/clustering/README.html#agglomerative-clustering\">Agglomerative Clustering</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/clustering/README.html#fast-clustering\">Fast Clustering</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/clustering/README.html#topic-modeling\">Topic Modeling</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/paraphrase-mining/README.html\">Paraphrase Mining</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/parallel-sentence-mining/README.html\">Translated Sentence Mining</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/parallel-sentence-mining/README.html#marging-based-mining\">Marging Based Mining</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/parallel-sentence-mining/README.html#examples\">Examples</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/cross-encoder/README.html\">Cross-Encoders</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/cross-encoder/README.html#bi-encoder-vs-cross-encoder\">Bi-Encoder vs. Cross-Encoder</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/cross-encoder/README.html#when-to-use-cross-bi-encoders\">When to use Cross- / Bi-Encoders?</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/cross-encoder/README.html#cross-encoders-usage\">Cross-Encoders Usage</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/cross-encoder/README.html#combining-bi-and-cross-encoders\">Combining Bi- and Cross-Encoders</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/cross-encoder/README.html#training-cross-encoders\">Training Cross-Encoders</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/applications/image-search/README.html\">Image Search</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/image-search/README.html#installation\">Installation</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/image-search/README.html#usage\">Usage</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/applications/image-search/README.html#examples\">Examples</a></li>\r\n</ul>\r\n</li>\r\n</ul>\r\n</div>\r\n<div class=\"toctree-wrapper compound\">\r\n<p class=\"caption\"><span class=\"caption-text\">Training</span></p>\r\n<ul>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/training/overview.html\">Training Overview</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/training/overview.html#network-architecture\">Network Architecture</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/training/overview.html#creating-networks-from-scratch\">Creating Networks from Scratch</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/training/overview.html#training-data\">Training Data</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/training/overview.html#loss-functions\">Loss Functions</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/training/overview.html#evaluators\">Evaluators</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/training/overview.html#loading-custom-sentencetransformer-models\">Loading Custom SentenceTransformer Models</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/training/overview.html#multitask-training\">Multitask Training</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/training/overview.html#adding-special-tokens\">Adding Special Tokens</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"docs/training/overview.html#best-transformer-model\">Best Transformer Model</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/multilingual/README.html\">Multilingual-Models</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/multilingual/README.html#available-pre-trained-models\">Available Pre-trained Models</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/multilingual/README.html#usage\">Usage</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/multilingual/README.html#performance\">Performance</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/multilingual/README.html#extend-your-own-models\">Extend your own models</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/multilingual/README.html#training\">Training</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/multilingual/README.html#data-format\">Data Format</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/multilingual/README.html#loading-training-datasets\">Loading Training Datasets</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/multilingual/README.html#sources-for-training-data\">Sources for Training Data</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/multilingual/README.html#evaluation\">Evaluation</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/multilingual/README.html#citation\">Citation</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/distillation/README.html\">Model Distillation</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/distillation/README.html#knowledge-distillation\">Knowledge Distillation</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/distillation/README.html#speed-performance-trade-off\">Speed - Performance Trade-Off</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/distillation/README.html#dimensionality-reduction\">Dimensionality Reduction</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/distillation/README.html#quantization\">Quantization</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/cross-encoder/README.html\">Cross-Encoders</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/cross-encoder/README.html#examples\">Examples</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/cross-encoder/README.html#training-crossencoders\">Training CrossEncoders</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/data_augmentation/README.html\">Augmented SBERT</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/data_augmentation/README.html#motivation\">Motivation</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/data_augmentation/README.html#extend-to-your-own-datasets\">Extend to your own datasets</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/data_augmentation/README.html#methodology\">Methodology</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/data_augmentation/README.html#scenario-1-limited-or-small-annotated-datasets-few-labeled-sentence-pairs\">Scenario 1: Limited or small annotated datasets (few labeled sentence-pairs)</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/data_augmentation/README.html#scenario-2-no-annotated-datasets-only-unlabeled-sentence-pairs\">Scenario 2: No annotated datasets (Only unlabeled sentence-pairs)</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/data_augmentation/README.html#training\">Training</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/data_augmentation/README.html#citation\">Citation</a></li>\r\n</ul>\r\n</li>\r\n</ul>\r\n</div>\r\n<div class=\"toctree-wrapper compound\">\r\n<p class=\"caption\"><span class=\"caption-text\">Training Examples</span></p>\r\n<ul>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/sts/README.html\">Semantic Textual Similarity</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/sts/README.html#training-data\">Training data</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/sts/README.html#loss-function\">Loss Function</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/nli/README.html\">Natural Language Inference</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/nli/README.html#data\">Data</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/nli/README.html#softmaxloss\">SoftmaxLoss</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/nli/README.html#multiplenegativesrankingloss\">MultipleNegativesRankingLoss</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/paraphrases/README.html\">Paraphrase Data</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/paraphrases/README.html#datasets\">Datasets</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/paraphrases/README.html#training\">Training</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/paraphrases/README.html#pre-trained-models\">Pre-Trained Models</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/paraphrases/README.html#work-in-progress\">Work in Progress</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/quora_duplicate_questions/README.html\">Quora Duplicate Questions</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/quora_duplicate_questions/README.html#pretrained-models\">Pretrained Models</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/quora_duplicate_questions/README.html#dataset\">Dataset</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/quora_duplicate_questions/README.html#usage\">Usage</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/quora_duplicate_questions/README.html#training\">Training</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/quora_duplicate_questions/README.html#multiplenegativesrankingloss\">MultipleNegativesRankingLoss</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/training/ms_marco/README.html\">MS MARCO</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/ms_marco/README.html#bi-encoder\">Bi-Encoder</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/ms_marco/README.html#cross-encoder\">Cross-Encoder</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/training/ms_marco/README.html#cross-encoder-knowledge-distillation\">Cross-Encoder Knowledge Distillation</a></li>\r\n</ul>\r\n</li>\r\n</ul>\r\n</div>\r\n<div class=\"toctree-wrapper compound\">\r\n<p class=\"caption\"><span class=\"caption-text\">Unsupervised Learning</span></p>\r\n<ul>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/unsupervised_learning/README.html\">Unsupervised Learning</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/unsupervised_learning/README.html#tsdae\">TSDAE</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/unsupervised_learning/README.html#simcse\">SimCSE</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/unsupervised_learning/README.html#ct\">CT</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/unsupervised_learning/README.html#ct-in-batch-negative-sampling\">CT (In-Batch Negative Sampling)</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/unsupervised_learning/README.html#masked-language-model-mlm\">Masked Language Model (MLM)</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/unsupervised_learning/README.html#genq\">GenQ</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/unsupervised_learning/README.html#gpl\">GPL</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/unsupervised_learning/README.html#performance-comparison\">Performance Comparison</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"examples/domain_adaptation/README.html\">Domain Adaptation</a><ul>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/domain_adaptation/README.html#domain-adaptation-vs-unsupervised-learning\">Domain Adaptation vs. Unsupervised Learning</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/domain_adaptation/README.html#adaptive-pre-training\">Adaptive Pre-Training</a></li>\r\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"examples/domain_adaptation/README.html#gpl-generative-pseudo-labeling\">GPL: Generative Pseudo-Labeling</a></li>\r\n</ul>\r\n</li>\r\n</ul>\r\n</div>\r\n<div class=\"toctree-wrapper compound\">\r\n<p class=\"caption\"><span class=\"caption-text\">Package Reference</span></p>\r\n<ul>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/package_reference/SentenceTransformer.html\">SentenceTransformer</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/package_reference/util.html\">util</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/package_reference/models.html\">Models</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/package_reference/losses.html\">Losses</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/package_reference/evaluation.html\">Evaluation</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/package_reference/datasets.html\">Datasets</a></li>\r\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docs/package_reference/cross_encoder.html\">cross_encoder</a></li>\r\n</ul>\r\n</div>\r\n</div>\r\n\r\n\r\n           </div>\r\n           \r\n          </div>\r\n          <footer>\r\n  \r\n    <div class=\"rst-footer-buttons\" role=\"navigation\" aria-label=\"footer navigation\">\r\n      \r\n        <a href=\"docs/installation.html\" class=\"btn btn-neutral float-right\" title=\"Installation\" accesskey=\"n\" rel=\"next\">Next <span class=\"fa fa-arrow-circle-right\"></span></a>\r\n      \r\n      \r\n    </div>\r\n  \r\n\r\n  <hr/>\r\n\r\n  <div role=\"contentinfo\">\r\n    <p>\r\n        \r\n        &copy; Copyright 2022, Nils Reimers\r\n\r\n       &bull; <a href=\"/docs/contact.html\">Contact</a>\r\n\r\n    </p>\r\n  </div>\r\n    \r\n    \r\n    \r\n    Built with <a href=\"https://www.sphinx-doc.org/\">Sphinx</a> using a\r\n    \r\n    <a href=\"https://github.com/readthedocs/sphinx_rtd_theme\">theme</a>\r\n    \r\n    provided by <a href=\"https://readthedocs.org\">Read the Docs</a>. \r\n\r\n</footer>\r\n\r\n        </div>\r\n      </div>\r\n\r\n    </section>\r\n\r\n  </div>\r\n  \r\n\r\n  <script data-cfasync=\"false\" src=\"/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js\"></script><script type=\"text/javascript\">\r\n      jQuery(function () {\r\n          SphinxRtdTheme.Navigation.enable(true);\r\n      });\r\n  </script>\r\n\r\n  \r\n  \r\n    \r\n   \r\n\r\n</body>\r\n</html>","oembed":false,"readabilityObject":{"title":"SentenceTransformers Documentation — Sentence-Transformers documentation","content":"<div id=\"readability-page-1\" class=\"page\"><div>\n    \n    <nav data-toggle=\"wy-nav-shift\">\n      \n    </nav>\n\n    <section data-toggle=\"wy-nav-shift\">\n\n      \n      <nav aria-label=\"top navigation\">\n        \n          <i data-toggle=\"wy-nav-top\"></i>\n          <a href=\"#\">Sentence-Transformers</a>\n        \n      </nav>\n\n\n      <div itemprop=\"articleBody\" role=\"main\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Article\">\n            \n  \n<div id=\"installation\">\n<h2>Installation<a href=\"#installation\" title=\"Permalink to this headline\">¶</a></h2>\n<p>You can install it using pip:</p>\n<div><pre><span></span><span>pip</span> <span>install</span> <span>-</span><span>U</span> <span>sentence</span><span>-</span><span>transformers</span>\n</pre></div>\n<p>We recommend <strong>Python 3.6</strong> or higher, and at least <strong>PyTorch 1.6.0</strong>. See <a href=\"docs/installation.html\">installation</a> for further installation options, especially if you want to use a GPU.</p>\n</div>\n<div id=\"usage\">\n<h2>Usage<a href=\"#usage\" title=\"Permalink to this headline\">¶</a></h2>\n<p>The usage is as simple as:</p>\n<div><pre><span></span><span>from</span> <span>sentence_transformers</span> <span>import</span> <span>SentenceTransformer</span>\n<span>model</span> <span>=</span> <span>SentenceTransformer</span><span>(</span><span>'paraphrase-MiniLM-L6-v2'</span><span>)</span>\n\n<span>#Our sentences we like to encode</span>\n<span>sentences</span> <span>=</span> <span>[</span><span>'This framework generates embeddings for each input sentence'</span><span>,</span>\n    <span>'Sentences are passed as a list of string.'</span><span>,</span>\n    <span>'The quick brown fox jumps over the lazy dog.'</span><span>]</span>\n\n<span>#Sentences are encoded by calling model.encode()</span>\n<span>embeddings</span> <span>=</span> <span>model</span><span>.</span><span>encode</span><span>(</span><span>sentences</span><span>)</span>\n\n<span>#Print the embeddings</span>\n<span>for</span> <span>sentence</span><span>,</span> <span>embedding</span> <span>in</span> <span>zip</span><span>(</span><span>sentences</span><span>,</span> <span>embeddings</span><span>):</span>\n    <span>print</span><span>(</span><span>\"Sentence:\"</span><span>,</span> <span>sentence</span><span>)</span>\n    <span>print</span><span>(</span><span>\"Embedding:\"</span><span>,</span> <span>embedding</span><span>)</span>\n    <span>print</span><span>(</span><span>\"\"</span><span>)</span>\n</pre></div>\n</div>\n<div id=\"performance\">\n<h2>Performance<a href=\"#performance\" title=\"Permalink to this headline\">¶</a></h2>\n<p>Our models are evaluated extensively and achieve state-of-the-art performance on various tasks. Further, the code is tuned to provide the highest possible speed. Have a look at <a href=\"https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/\">Pre-Trained Models</a> for an overview of available models and the respective performance on different tasks.</p>\n</div>\n\n<div id=\"citing-authors\">\n<h2>Citing &amp; Authors<a href=\"#citing-authors\" title=\"Permalink to this headline\">¶</a></h2>\n<p>If you find this repository helpful, feel free to cite our publication <a href=\"https://arxiv.org/abs/1908.10084\">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a>:</p>\n<blockquote>\n<div><pre><span></span><span>@inproceedings</span><span>{</span><span>reimers-2019-sentence-bert</span><span>,</span>\n  <span>title</span> <span>=</span> <span>\"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\"</span><span>,</span>\n  <span>author</span> <span>=</span> <span>\"Reimers, Nils and Gurevych, Iryna\"</span><span>,</span>\n  <span>booktitle</span> <span>=</span> <span>\"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\"</span><span>,</span>\n  <span>month</span> <span>=</span> <span>\"11\"</span><span>,</span>\n  <span>year</span> <span>=</span> <span>\"2019\"</span><span>,</span>\n  <span>publisher</span> <span>=</span> <span>\"Association for Computational Linguistics\"</span><span>,</span>\n  <span>url</span> <span>=</span> <span>\"https://arxiv.org/abs/1908.10084\"</span><span>,</span>\n<span>}</span>\n</pre></div></blockquote>\n<p>If you use one of the multilingual models, feel free to cite our publication <a href=\"https://arxiv.org/abs/2004.09813\">Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation</a>:</p>\n<blockquote>\n<div><pre><span></span><span>@inproceedings</span><span>{</span><span>reimers-2020-multilingual-sentence-bert</span><span>,</span>\n  <span>title</span> <span>=</span> <span>\"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\"</span><span>,</span>\n  <span>author</span> <span>=</span> <span>\"Reimers, Nils and Gurevych, Iryna\"</span><span>,</span>\n  <span>booktitle</span> <span>=</span> <span>\"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\"</span><span>,</span>\n  <span>month</span> <span>=</span> <span>\"11\"</span><span>,</span>\n  <span>year</span> <span>=</span> <span>\"2020\"</span><span>,</span>\n  <span>publisher</span> <span>=</span> <span>\"Association for Computational Linguistics\"</span><span>,</span>\n  <span>url</span> <span>=</span> <span>\"https://arxiv.org/abs/2004.09813\"</span><span>,</span>\n<span>}</span>\n</pre></div></blockquote>\n<p>If you use the code for <a href=\"https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/data_augmentation\">data augmentation</a>, feel free to cite our publication <a href=\"https://arxiv.org/abs/2010.08240\">Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks</a>:</p>\n<blockquote>\n<div><pre><span></span><span>@inproceedings</span><span>{</span><span>thakur-2020-AugSBERT</span><span>,</span>\n  <span>title</span> <span>=</span> <span>\"Augmented {SBERT}: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks\"</span><span>,</span>\n  <span>author</span> <span>=</span> <span>\"Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes  and Gurevych, Iryna\"</span><span>,</span>\n  <span>booktitle</span> <span>=</span> <span>\"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\"</span><span>,</span>\n  <span>month</span> <span>=</span> <span>jun</span><span>,</span>\n  <span>year</span> <span>=</span> <span>\"2021\"</span><span>,</span>\n  <span>address</span> <span>=</span> <span>\"Online\"</span><span>,</span>\n  <span>publisher</span> <span>=</span> <span>\"Association for Computational Linguistics\"</span><span>,</span>\n  <span>url</span> <span>=</span> <span>\"https://www.aclweb.org/anthology/2021.naacl-main.28\"</span><span>,</span>\n  <span>pages</span> <span>=</span> <span>\"296--310\"</span><span>,</span>\n<span>}</span>\n</pre></div></blockquote>\n<div>\n<p><span>Overview</span></p>\n<ul>\n<li><a href=\"docs/installation.html\">Installation</a><ul>\n<li><a href=\"docs/installation.html#install-sentencetransformers\">Install SentenceTransformers</a></li>\n<li><a href=\"docs/installation.html#install-pytorch-with-cuda-support\">Install PyTorch with CUDA-Support</a></li>\n</ul>\n</li>\n<li><a href=\"docs/quickstart.html\">Quickstart</a><ul>\n<li><a href=\"docs/quickstart.html#comparing-sentence-similarities\">Comparing Sentence Similarities</a></li>\n<li><a href=\"docs/quickstart.html#pre-trained-models\">Pre-Trained Models</a></li>\n<li><a href=\"docs/quickstart.html#training-your-own-embeddings\">Training your own Embeddings</a></li>\n</ul>\n</li>\n<li><a href=\"docs/pretrained_models.html\">Pretrained Models</a><ul>\n<li><a href=\"docs/pretrained_models.html#model-overview\">Model Overview</a></li>\n<li><a href=\"docs/pretrained_models.html#semantic-search\">Semantic Search</a></li>\n<li><a href=\"docs/pretrained_models.html#multi-lingual-models\">Multi-Lingual Models</a></li>\n<li><a href=\"docs/pretrained_models.html#image-text-models\">Image &amp; Text-Models</a></li>\n<li><a href=\"docs/pretrained_models.html#other-models\">Other Models</a></li>\n</ul>\n</li>\n<li><a href=\"docs/pretrained_cross-encoders.html\">Pretrained Cross-Encoders</a><ul>\n<li><a href=\"docs/pretrained_cross-encoders.html#ms-marco\">MS MARCO</a></li>\n<li><a href=\"docs/pretrained_cross-encoders.html#squad-qnli\">SQuAD (QNLI)</a></li>\n<li><a href=\"docs/pretrained_cross-encoders.html#stsbenchmark\">STSbenchmark</a></li>\n<li><a href=\"docs/pretrained_cross-encoders.html#quora-duplicate-questions\">Quora Duplicate Questions</a></li>\n<li><a href=\"docs/pretrained_cross-encoders.html#nli\">NLI</a></li>\n</ul>\n</li>\n<li><a href=\"docs/publications.html\">Publications</a></li>\n</ul>\n</div>\n<div>\n<p><span>Usage</span></p>\n<ul>\n<li><a href=\"examples/applications/computing-embeddings/README.html\">Computing Sentence Embeddings</a><ul>\n<li><a href=\"examples/applications/computing-embeddings/README.html#input-sequence-length\">Input Sequence Length</a></li>\n<li><a href=\"examples/applications/computing-embeddings/README.html#storing-loading-embeddings\">Storing &amp; Loading Embeddings</a></li>\n<li><a href=\"examples/applications/computing-embeddings/README.html#multi-process-multi-gpu-encoding\">Multi-Process / Multi-GPU Encoding</a></li>\n<li><a href=\"examples/applications/computing-embeddings/README.html#sentence-embeddings-with-transformers\">Sentence Embeddings with Transformers</a></li>\n</ul>\n</li>\n<li><a href=\"docs/usage/semantic_textual_similarity.html\">Semantic Textual Similarity</a></li>\n<li><a href=\"examples/applications/semantic-search/README.html\">Semantic Search</a><ul>\n<li><a href=\"examples/applications/semantic-search/README.html#background\">Background</a></li>\n<li><a href=\"examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search\">Symmetric vs. Asymmetric Semantic Search</a></li>\n<li><a href=\"examples/applications/semantic-search/README.html#python\">Python</a></li>\n<li><a href=\"examples/applications/semantic-search/README.html#util-semantic-search\">util.semantic_search</a></li>\n<li><a href=\"examples/applications/semantic-search/README.html#speed-optimization\">Speed Optimization</a></li>\n<li><a href=\"examples/applications/semantic-search/README.html#elasticsearch\">ElasticSearch</a></li>\n<li><a href=\"examples/applications/semantic-search/README.html#approximate-nearest-neighbor\">Approximate Nearest Neighbor</a></li>\n<li><a href=\"examples/applications/semantic-search/README.html#retrieve-re-rank\">Retrieve &amp; Re-Rank</a></li>\n<li><a href=\"examples/applications/semantic-search/README.html#examples\">Examples</a></li>\n</ul>\n</li>\n<li><a href=\"examples/applications/retrieve_rerank/README.html\">Retrieve &amp; Re-Rank</a><ul>\n<li><a href=\"examples/applications/retrieve_rerank/README.html#retrieve-re-rank-pipeline\">Retrieve &amp; Re-Rank Pipeline</a></li>\n<li><a href=\"examples/applications/retrieve_rerank/README.html#retrieval-bi-encoder\">Retrieval: Bi-Encoder</a></li>\n<li><a href=\"examples/applications/retrieve_rerank/README.html#re-ranker-cross-encoder\">Re-Ranker: Cross-Encoder</a></li>\n<li><a href=\"examples/applications/retrieve_rerank/README.html#example-scripts\">Example Scripts</a></li>\n<li><a href=\"examples/applications/retrieve_rerank/README.html#pre-trained-bi-encoders-retrieval\">Pre-trained Bi-Encoders (Retrieval)</a></li>\n<li><a href=\"examples/applications/retrieve_rerank/README.html#pre-trained-cross-encoders-re-ranker\">Pre-trained Cross-Encoders (Re-Ranker)</a></li>\n</ul>\n</li>\n<li><a href=\"examples/applications/clustering/README.html\">Clustering</a><ul>\n<li><a href=\"examples/applications/clustering/README.html#k-means\">k-Means</a></li>\n<li><a href=\"examples/applications/clustering/README.html#agglomerative-clustering\">Agglomerative Clustering</a></li>\n<li><a href=\"examples/applications/clustering/README.html#fast-clustering\">Fast Clustering</a></li>\n<li><a href=\"examples/applications/clustering/README.html#topic-modeling\">Topic Modeling</a></li>\n</ul>\n</li>\n<li><a href=\"examples/applications/paraphrase-mining/README.html\">Paraphrase Mining</a></li>\n<li><a href=\"examples/applications/parallel-sentence-mining/README.html\">Translated Sentence Mining</a><ul>\n<li><a href=\"examples/applications/parallel-sentence-mining/README.html#marging-based-mining\">Marging Based Mining</a></li>\n<li><a href=\"examples/applications/parallel-sentence-mining/README.html#examples\">Examples</a></li>\n</ul>\n</li>\n<li><a href=\"examples/applications/cross-encoder/README.html\">Cross-Encoders</a><ul>\n<li><a href=\"examples/applications/cross-encoder/README.html#bi-encoder-vs-cross-encoder\">Bi-Encoder vs. Cross-Encoder</a></li>\n<li><a href=\"examples/applications/cross-encoder/README.html#when-to-use-cross-bi-encoders\">When to use Cross- / Bi-Encoders?</a></li>\n<li><a href=\"examples/applications/cross-encoder/README.html#cross-encoders-usage\">Cross-Encoders Usage</a></li>\n<li><a href=\"examples/applications/cross-encoder/README.html#combining-bi-and-cross-encoders\">Combining Bi- and Cross-Encoders</a></li>\n<li><a href=\"examples/applications/cross-encoder/README.html#training-cross-encoders\">Training Cross-Encoders</a></li>\n</ul>\n</li>\n<li><a href=\"examples/applications/image-search/README.html\">Image Search</a><ul>\n<li><a href=\"examples/applications/image-search/README.html#installation\">Installation</a></li>\n<li><a href=\"examples/applications/image-search/README.html#usage\">Usage</a></li>\n<li><a href=\"examples/applications/image-search/README.html#examples\">Examples</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<div>\n<p><span>Training</span></p>\n<ul>\n<li><a href=\"docs/training/overview.html\">Training Overview</a><ul>\n<li><a href=\"docs/training/overview.html#network-architecture\">Network Architecture</a></li>\n<li><a href=\"docs/training/overview.html#creating-networks-from-scratch\">Creating Networks from Scratch</a></li>\n<li><a href=\"docs/training/overview.html#training-data\">Training Data</a></li>\n<li><a href=\"docs/training/overview.html#loss-functions\">Loss Functions</a></li>\n<li><a href=\"docs/training/overview.html#evaluators\">Evaluators</a></li>\n<li><a href=\"docs/training/overview.html#loading-custom-sentencetransformer-models\">Loading Custom SentenceTransformer Models</a></li>\n<li><a href=\"docs/training/overview.html#multitask-training\">Multitask Training</a></li>\n<li><a href=\"docs/training/overview.html#adding-special-tokens\">Adding Special Tokens</a></li>\n<li><a href=\"docs/training/overview.html#best-transformer-model\">Best Transformer Model</a></li>\n</ul>\n</li>\n<li><a href=\"examples/training/multilingual/README.html\">Multilingual-Models</a><ul>\n<li><a href=\"examples/training/multilingual/README.html#available-pre-trained-models\">Available Pre-trained Models</a></li>\n<li><a href=\"examples/training/multilingual/README.html#usage\">Usage</a></li>\n<li><a href=\"examples/training/multilingual/README.html#performance\">Performance</a></li>\n<li><a href=\"examples/training/multilingual/README.html#extend-your-own-models\">Extend your own models</a></li>\n<li><a href=\"examples/training/multilingual/README.html#training\">Training</a></li>\n<li><a href=\"examples/training/multilingual/README.html#data-format\">Data Format</a></li>\n<li><a href=\"examples/training/multilingual/README.html#loading-training-datasets\">Loading Training Datasets</a></li>\n<li><a href=\"examples/training/multilingual/README.html#sources-for-training-data\">Sources for Training Data</a></li>\n<li><a href=\"examples/training/multilingual/README.html#evaluation\">Evaluation</a></li>\n<li><a href=\"examples/training/multilingual/README.html#citation\">Citation</a></li>\n</ul>\n</li>\n<li><a href=\"examples/training/distillation/README.html\">Model Distillation</a><ul>\n<li><a href=\"examples/training/distillation/README.html#knowledge-distillation\">Knowledge Distillation</a></li>\n<li><a href=\"examples/training/distillation/README.html#speed-performance-trade-off\">Speed - Performance Trade-Off</a></li>\n<li><a href=\"examples/training/distillation/README.html#dimensionality-reduction\">Dimensionality Reduction</a></li>\n<li><a href=\"examples/training/distillation/README.html#quantization\">Quantization</a></li>\n</ul>\n</li>\n<li><a href=\"examples/training/cross-encoder/README.html\">Cross-Encoders</a><ul>\n<li><a href=\"examples/training/cross-encoder/README.html#examples\">Examples</a></li>\n<li><a href=\"examples/training/cross-encoder/README.html#training-crossencoders\">Training CrossEncoders</a></li>\n</ul>\n</li>\n<li><a href=\"examples/training/data_augmentation/README.html\">Augmented SBERT</a><ul>\n<li><a href=\"examples/training/data_augmentation/README.html#motivation\">Motivation</a></li>\n<li><a href=\"examples/training/data_augmentation/README.html#extend-to-your-own-datasets\">Extend to your own datasets</a></li>\n<li><a href=\"examples/training/data_augmentation/README.html#methodology\">Methodology</a></li>\n<li><a href=\"examples/training/data_augmentation/README.html#scenario-1-limited-or-small-annotated-datasets-few-labeled-sentence-pairs\">Scenario 1: Limited or small annotated datasets (few labeled sentence-pairs)</a></li>\n<li><a href=\"examples/training/data_augmentation/README.html#scenario-2-no-annotated-datasets-only-unlabeled-sentence-pairs\">Scenario 2: No annotated datasets (Only unlabeled sentence-pairs)</a></li>\n<li><a href=\"examples/training/data_augmentation/README.html#training\">Training</a></li>\n<li><a href=\"examples/training/data_augmentation/README.html#citation\">Citation</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<div>\n<p><span>Training Examples</span></p>\n<ul>\n<li><a href=\"examples/training/sts/README.html\">Semantic Textual Similarity</a><ul>\n<li><a href=\"examples/training/sts/README.html#training-data\">Training data</a></li>\n<li><a href=\"examples/training/sts/README.html#loss-function\">Loss Function</a></li>\n</ul>\n</li>\n<li><a href=\"examples/training/nli/README.html\">Natural Language Inference</a><ul>\n<li><a href=\"examples/training/nli/README.html#data\">Data</a></li>\n<li><a href=\"examples/training/nli/README.html#softmaxloss\">SoftmaxLoss</a></li>\n<li><a href=\"examples/training/nli/README.html#multiplenegativesrankingloss\">MultipleNegativesRankingLoss</a></li>\n</ul>\n</li>\n<li><a href=\"examples/training/paraphrases/README.html\">Paraphrase Data</a><ul>\n<li><a href=\"examples/training/paraphrases/README.html#datasets\">Datasets</a></li>\n<li><a href=\"examples/training/paraphrases/README.html#training\">Training</a></li>\n<li><a href=\"examples/training/paraphrases/README.html#pre-trained-models\">Pre-Trained Models</a></li>\n<li><a href=\"examples/training/paraphrases/README.html#work-in-progress\">Work in Progress</a></li>\n</ul>\n</li>\n<li><a href=\"examples/training/quora_duplicate_questions/README.html\">Quora Duplicate Questions</a><ul>\n<li><a href=\"examples/training/quora_duplicate_questions/README.html#pretrained-models\">Pretrained Models</a></li>\n<li><a href=\"examples/training/quora_duplicate_questions/README.html#dataset\">Dataset</a></li>\n<li><a href=\"examples/training/quora_duplicate_questions/README.html#usage\">Usage</a></li>\n<li><a href=\"examples/training/quora_duplicate_questions/README.html#training\">Training</a></li>\n<li><a href=\"examples/training/quora_duplicate_questions/README.html#multiplenegativesrankingloss\">MultipleNegativesRankingLoss</a></li>\n</ul>\n</li>\n<li><a href=\"examples/training/ms_marco/README.html\">MS MARCO</a><ul>\n<li><a href=\"examples/training/ms_marco/README.html#bi-encoder\">Bi-Encoder</a></li>\n<li><a href=\"examples/training/ms_marco/README.html#cross-encoder\">Cross-Encoder</a></li>\n<li><a href=\"examples/training/ms_marco/README.html#cross-encoder-knowledge-distillation\">Cross-Encoder Knowledge Distillation</a></li>\n</ul>\n</li>\n</ul>\n</div>\n<div>\n<p><span>Unsupervised Learning</span></p>\n<ul>\n<li><a href=\"examples/unsupervised_learning/README.html\">Unsupervised Learning</a><ul>\n<li><a href=\"examples/unsupervised_learning/README.html#tsdae\">TSDAE</a></li>\n<li><a href=\"examples/unsupervised_learning/README.html#simcse\">SimCSE</a></li>\n<li><a href=\"examples/unsupervised_learning/README.html#ct\">CT</a></li>\n<li><a href=\"examples/unsupervised_learning/README.html#ct-in-batch-negative-sampling\">CT (In-Batch Negative Sampling)</a></li>\n<li><a href=\"examples/unsupervised_learning/README.html#masked-language-model-mlm\">Masked Language Model (MLM)</a></li>\n<li><a href=\"examples/unsupervised_learning/README.html#genq\">GenQ</a></li>\n<li><a href=\"examples/unsupervised_learning/README.html#gpl\">GPL</a></li>\n<li><a href=\"examples/unsupervised_learning/README.html#performance-comparison\">Performance Comparison</a></li>\n</ul>\n</li>\n<li><a href=\"examples/domain_adaptation/README.html\">Domain Adaptation</a><ul>\n<li><a href=\"examples/domain_adaptation/README.html#domain-adaptation-vs-unsupervised-learning\">Domain Adaptation vs. Unsupervised Learning</a></li>\n<li><a href=\"examples/domain_adaptation/README.html#adaptive-pre-training\">Adaptive Pre-Training</a></li>\n<li><a href=\"examples/domain_adaptation/README.html#gpl-generative-pseudo-labeling\">GPL: Generative Pseudo-Labeling</a></li>\n</ul>\n</li>\n</ul>\n</div>\n\n</div>\n\n\n           </div>\n\n    </section>\n\n  </div></div>","textContent":"\n    \n    \n      \n    \n\n    \n\n      \n      \n        \n          \n          Sentence-Transformers\n        \n      \n\n\n      \n            \n  \n\nInstallation¶\nYou can install it using pip:\npip install -U sentence-transformers\n\nWe recommend Python 3.6 or higher, and at least PyTorch 1.6.0. See installation for further installation options, especially if you want to use a GPU.\n\n\nUsage¶\nThe usage is as simple as:\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n#Our sentences we like to encode\nsentences = ['This framework generates embeddings for each input sentence',\n    'Sentences are passed as a list of string.',\n    'The quick brown fox jumps over the lazy dog.']\n\n#Sentences are encoded by calling model.encode()\nembeddings = model.encode(sentences)\n\n#Print the embeddings\nfor sentence, embedding in zip(sentences, embeddings):\n    print(\"Sentence:\", sentence)\n    print(\"Embedding:\", embedding)\n    print(\"\")\n\n\n\nPerformance¶\nOur models are evaluated extensively and achieve state-of-the-art performance on various tasks. Further, the code is tuned to provide the highest possible speed. Have a look at Pre-Trained Models for an overview of available models and the respective performance on different tasks.\n\n\n\nCiting & Authors¶\nIf you find this repository helpful, feel free to cite our publication Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks:\n\n@inproceedings{reimers-2019-sentence-bert,\n  title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n  author = \"Reimers, Nils and Gurevych, Iryna\",\n  booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n  month = \"11\",\n  year = \"2019\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://arxiv.org/abs/1908.10084\",\n}\n\nIf you use one of the multilingual models, feel free to cite our publication Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation:\n\n@inproceedings{reimers-2020-multilingual-sentence-bert,\n  title = \"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\",\n  author = \"Reimers, Nils and Gurevych, Iryna\",\n  booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\",\n  month = \"11\",\n  year = \"2020\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://arxiv.org/abs/2004.09813\",\n}\n\nIf you use the code for data augmentation, feel free to cite our publication Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks:\n\n@inproceedings{thakur-2020-AugSBERT,\n  title = \"Augmented {SBERT}: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks\",\n  author = \"Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes  and Gurevych, Iryna\",\n  booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n  month = jun,\n  year = \"2021\",\n  address = \"Online\",\n  publisher = \"Association for Computational Linguistics\",\n  url = \"https://www.aclweb.org/anthology/2021.naacl-main.28\",\n  pages = \"296--310\",\n}\n\n\nOverview\n\nInstallation\nInstall SentenceTransformers\nInstall PyTorch with CUDA-Support\n\n\nQuickstart\nComparing Sentence Similarities\nPre-Trained Models\nTraining your own Embeddings\n\n\nPretrained Models\nModel Overview\nSemantic Search\nMulti-Lingual Models\nImage & Text-Models\nOther Models\n\n\nPretrained Cross-Encoders\nMS MARCO\nSQuAD (QNLI)\nSTSbenchmark\nQuora Duplicate Questions\nNLI\n\n\nPublications\n\n\n\nUsage\n\nComputing Sentence Embeddings\nInput Sequence Length\nStoring & Loading Embeddings\nMulti-Process / Multi-GPU Encoding\nSentence Embeddings with Transformers\n\n\nSemantic Textual Similarity\nSemantic Search\nBackground\nSymmetric vs. Asymmetric Semantic Search\nPython\nutil.semantic_search\nSpeed Optimization\nElasticSearch\nApproximate Nearest Neighbor\nRetrieve & Re-Rank\nExamples\n\n\nRetrieve & Re-Rank\nRetrieve & Re-Rank Pipeline\nRetrieval: Bi-Encoder\nRe-Ranker: Cross-Encoder\nExample Scripts\nPre-trained Bi-Encoders (Retrieval)\nPre-trained Cross-Encoders (Re-Ranker)\n\n\nClustering\nk-Means\nAgglomerative Clustering\nFast Clustering\nTopic Modeling\n\n\nParaphrase Mining\nTranslated Sentence Mining\nMarging Based Mining\nExamples\n\n\nCross-Encoders\nBi-Encoder vs. Cross-Encoder\nWhen to use Cross- / Bi-Encoders?\nCross-Encoders Usage\nCombining Bi- and Cross-Encoders\nTraining Cross-Encoders\n\n\nImage Search\nInstallation\nUsage\nExamples\n\n\n\n\n\nTraining\n\nTraining Overview\nNetwork Architecture\nCreating Networks from Scratch\nTraining Data\nLoss Functions\nEvaluators\nLoading Custom SentenceTransformer Models\nMultitask Training\nAdding Special Tokens\nBest Transformer Model\n\n\nMultilingual-Models\nAvailable Pre-trained Models\nUsage\nPerformance\nExtend your own models\nTraining\nData Format\nLoading Training Datasets\nSources for Training Data\nEvaluation\nCitation\n\n\nModel Distillation\nKnowledge Distillation\nSpeed - Performance Trade-Off\nDimensionality Reduction\nQuantization\n\n\nCross-Encoders\nExamples\nTraining CrossEncoders\n\n\nAugmented SBERT\nMotivation\nExtend to your own datasets\nMethodology\nScenario 1: Limited or small annotated datasets (few labeled sentence-pairs)\nScenario 2: No annotated datasets (Only unlabeled sentence-pairs)\nTraining\nCitation\n\n\n\n\n\nTraining Examples\n\nSemantic Textual Similarity\nTraining data\nLoss Function\n\n\nNatural Language Inference\nData\nSoftmaxLoss\nMultipleNegativesRankingLoss\n\n\nParaphrase Data\nDatasets\nTraining\nPre-Trained Models\nWork in Progress\n\n\nQuora Duplicate Questions\nPretrained Models\nDataset\nUsage\nTraining\nMultipleNegativesRankingLoss\n\n\nMS MARCO\nBi-Encoder\nCross-Encoder\nCross-Encoder Knowledge Distillation\n\n\n\n\n\nUnsupervised Learning\n\nUnsupervised Learning\nTSDAE\nSimCSE\nCT\nCT (In-Batch Negative Sampling)\nMasked Language Model (MLM)\nGenQ\nGPL\nPerformance Comparison\n\n\nDomain Adaptation\nDomain Adaptation vs. Unsupervised Learning\nAdaptive Pre-Training\nGPL: Generative Pseudo-Labeling\n\n\n\n\n\n\n\n\n           \n\n    \n\n  ","length":6060,"excerpt":"You can install it using pip:","byline":null,"dir":null,"siteName":null,"lang":"en"},"finalizedMeta":{"title":"SentenceTransformers Documentation — Sentence-Transformers  documentation","description":"You can install it using pip:","author":false,"creator":"","publisher":false,"date":"2022-03-23T21:52:33.658Z","topics":[]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"SentenceTransformers Documentation — Sentence-Transformers  documentation","description":false,"canonical":"https://www.sbert.netindex.html/","keywords":[],"image":"_static/logo.png","firstParagraph":"Overview"},"dublinCore":{},"opengraph":{"title":false,"description":false,"url":false,"site_name":false,"locale":false,"type":false,"typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":false},"twitter":{"site":false,"description":false,"card":false,"creator":false,"title":false,"image":false},"archivedData":{"link":"https://web.archive.org/web/20220323215237/https://www.sbert.net/","wayback":"https://web.archive.org/web/20220323215237/https://www.sbert.net/"}}}