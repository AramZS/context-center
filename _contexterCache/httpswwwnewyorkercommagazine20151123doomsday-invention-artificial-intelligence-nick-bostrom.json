{"initialLink":"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom","sanitizedLink":"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom","finalLink":"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"><img src=\"https://media.newyorker.com/photos/590971f5ebe912338a377328/16:9/w_2560,h_1440,c_limit/151123_r27342.jpg\" alt=\"\" itemprop=\"image\" /></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom\" itemprop=\"url\">The Philosopher of Doomsday</a></contexter-box-head></contexter-box-head><contexter-byline class=\"p-author author\" slot=\"author\"><span class=\"p-name byline\" rel=\"author\" itemprop=\"author\">Condé Nast</span></contexter-byline><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2015-11-16T00:00:00.000Z\">11/15/2015</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"><span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\">a reporter at large</span>, <span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\">artificial intelligence (a.i.)</span>, <span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\">category_science_tech</span>, <span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\">philosophers</span>, <span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\">magazine</span>, <span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\">tags</span></contexter-keywordset><a is=\"contexter-link\" href=\"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"0a6855b8d833f6ccfe3090ca753dfa6967acb644","data":{"originalLink":"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom","sanitizedLink":"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom","canonical":"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom","htmlText":"<!DOCTYPE html><html lang=\"en-US\"><head><title>The Philosopher of Doomsday | The New Yorker</title><meta charSet=\"utf-8\"/><meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/><meta name=\"msapplication-tap-highlight\" content=\"no\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"/><meta name=\"author\" content=\"Condé Nast\"/><meta name=\"copyright\" content=\"Copyright (c) Condé Nast 2024\"/><meta name=\"description\" content=\"Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.\"/><meta name=\"id\" content=\"5911cbb2803aff0f1c1359ac\"/><meta name=\"keywords\" content=\"artificial intelligence (a.i.),category_science_tech,philosophers\"/><meta name=\"news_keywords\" content=\"artificial intelligence (a.i.),category_science_tech,philosophers\"/><meta name=\"robots\" content=\"index, follow, noarchive, max-image-preview:large\"/><meta name=\"content-type\" content=\"article\"/><meta name=\"parsely-post-id\" content=\"5911cbb2803aff0f1c1359ac\"/><meta name=\"parsely-metadata\" content=\"{&quot;description&quot;:&quot;Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.&quot;,&quot;image-16-9&quot;:&quot;https://media.newyorker.com/photos/590971f5ebe912338a377328/16:9/w_1000,c_limit/151123_r27342.jpg&quot;,&quot;image-1-1&quot;:&quot;https://media.newyorker.com/photos/590971f5ebe912338a377328/1:1/w_1000,c_limit/151123_r27342.jpg&quot;}\"/><meta property=\"og:description\" content=\"Will artificial intelligence bring us utopia or destruction?\"/><meta property=\"og:image\" content=\"https://media.newyorker.com/photos/590971f5ebe912338a377328/16:9/w_1280,c_limit/151123_r27342.jpg\"/><meta property=\"og:site_name\" content=\"The New Yorker\"/><meta property=\"og:title\" content=\"The Doomsday Invention\"/><meta property=\"og:type\" content=\"article\"/><meta property=\"og:url\" content=\"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom\"/><meta property=\"article:content_tier\" content=\"free\"/><meta property=\"article:opinion\" content=\"false\"/><meta http-equiv=\"content-language\" content=\"en-US\"/><meta property=\"article:section\" content=\"tags\"/><meta property=\"article:published_time\" content=\"2015-11-16T00:00:00.000Z\"/><meta property=\"article:modified_time\" content=\"2015-11-16T04:00:00.000Z\"/><meta property=\"article:author\" content=\"Raffi Khatchadourian\"/><meta property=\"twitter:card\" content=\"summary_large_image\"/><meta property=\"twitter:creator\" content=\"@NewYorker\"/><meta property=\"twitter:description\" content=\"Will artificial intelligence bring us utopia or destruction?\"/><meta property=\"twitter:domain\" content=\"https://www.newyorker.com\"/><meta property=\"twitter:image\" content=\"https://media.newyorker.com/photos/590971f5ebe912338a377328/16:9/w_1280,c_limit/151123_r27342.jpg?mbid=social_retweet\"/><meta property=\"twitter:site\" content=\"@NewYorker\"/><meta property=\"twitter:title\" content=\"The Doomsday Invention\"/><meta property=\"pinterest:image\" content=\"https://media.newyorker.com/photos/590971f5ebe912338a377328/2:3/w_1000,h_1500,c_limit/151123_r27342.jpg\"/><meta property=\"fb:app_id\" content=\"1147169538698836\"/><meta property=\"fb:pages\" content=\"9258148868\"/><link rel=\"canonical\" href=\"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom\"/><link rel=\"alternate\" type=\"application/rss+xml\" href=\"https://www.newyorker.com/feed/rss\"/><style></style><link rel=\"preload\" as=\"image\" href=\"https://media.newyorker.com/photos/590971f5ebe912338a377328/master/w_2560,c_limit/151123_r27342.jpg\" imagesrcset=\"https://media.newyorker.com/photos/590971f5ebe912338a377328/master/w_120,c_limit/151123_r27342.jpg 120w, https://media.newyorker.com/photos/590971f5ebe912338a377328/master/w_240,c_limit/151123_r27342.jpg 240w, https://media.newyorker.com/photos/590971f5ebe912338a377328/master/w_320,c_limit/151123_r27342.jpg 320w, https://media.newyorker.com/photos/590971f5ebe912338a377328/master/w_640,c_limit/151123_r27342.jpg 640w, https://media.newyorker.com/photos/590971f5ebe912338a377328/master/w_960,c_limit/151123_r27342.jpg 960w, https://media.newyorker.com/photos/590971f5ebe912338a377328/master/w_1280,c_limit/151123_r27342.jpg 1280w, https://media.newyorker.com/photos/590971f5ebe912338a377328/master/w_1600,c_limit/151123_r27342.jpg 1600w, https://media.newyorker.com/photos/590971f5ebe912338a377328/master/w_1920,c_limit/151123_r27342.jpg 1920w, https://media.newyorker.com/photos/590971f5ebe912338a377328/master/w_2240,c_limit/151123_r27342.jpg 2240w\" imagesizes=\"100vw\"  fetchpriority=\"high\" /><style></style><style data-styled=\"\" data-styled-version=\"5.3.10\">{/*! normalize.scss v0.1.0 | MIT License | based on git.io/normalize */}/*!sc*/\nhtml{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;}/*!sc*/\nbody{margin:0;}/*!sc*/\narticle,aside,details,figcaption,figure,footer,header,hgroup,main,menu,nav,section,summary{display:block;}/*!sc*/\naudio,canvas,progress,video{display:inline-block;vertical-align:baseline;}/*!sc*/\naudio:not([controls]){display:none;height:0;}/*!sc*/\n[hidden],template{display:none;}/*!sc*/\na{background-color:transparent;}/*!sc*/\na:active,a:hover{outline:0;}/*!sc*/\nabbr[title]{border-bottom:1px dotted;}/*!sc*/\nb,strong{font-weight:bold;}/*!sc*/\ndfn{font-style:italic;}/*!sc*/\nh1{font-size:2em;margin:0.67em 0;}/*!sc*/\nmark{background:#ff0;color:#000;}/*!sc*/\nsmall{font-size:80%;}/*!sc*/\nsub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline;}/*!sc*/\nsup{top:-0.5em;}/*!sc*/\nsub{bottom:-0.25em;}/*!sc*/\nimg{border:0;}/*!sc*/\nsvg:not(:root){overflow:hidden;}/*!sc*/\nfigure{margin:1em 40px;}/*!sc*/\nhr{-moz-box-sizing:content-box;box-sizing:content-box;height:0;}/*!sc*/\npre{overflow:auto;}/*!sc*/\ncode,kbd,pre,samp{font-family:monospace,monospace;font-size:1em;}/*!sc*/\nbutton,input,optgroup,select,textarea{color:inherit;font:inherit;margin:0;}/*!sc*/\nbutton{overflow:visible;}/*!sc*/\nbutton,select{text-transform:none;}/*!sc*/\nbutton,html input[type=\"button\"],input[type=\"reset\"],input[type=\"submit\"]{-webkit-appearance:button;cursor:pointer;}/*!sc*/\nbutton[disabled],html input[disabled]{cursor:default;}/*!sc*/\nbutton::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0;}/*!sc*/\ninput{line-height:normal;}/*!sc*/\ninput[type=\"checkbox\"],input[type=\"radio\"]{box-sizing:border-box;padding:0;}/*!sc*/\n._hj-widget-container input[type=\"radio\"]{inset:initial;height:initial;width:initial;}/*!sc*/\ninput[type=\"number\"]::-webkit-inner-spin-button,input[type=\"number\"]::-webkit-outer-spin-button{height:auto;}/*!sc*/\ninput[type=\"search\"]{-webkit-appearance:textfield;-moz-box-sizing:content-box;-webkit-box-sizing:content-box;box-sizing:content-box;}/*!sc*/\ninput[type=\"search\"]::-webkit-search-cancel-button,input[type=\"search\"]::-webkit-search-decoration{-webkit-appearance:none;}/*!sc*/\nfieldset{border:1px solid #c0c0c0;margin:0 2px;padding:0.35em 0.625em 0.75em;}/*!sc*/\nlegend{border:0;padding:0;}/*!sc*/\ntextarea{overflow:auto;}/*!sc*/\noptgroup{font-weight:bold;}/*!sc*/\ntable{border-collapse:collapse;border-spacing:0;}/*!sc*/\ntd,th{padding:0;}/*!sc*/\n*{box-sizing:border-box;}/*!sc*/\nhtml{height:100%;font-size:16px;}/*!sc*/\n@media (max-width:768px){html{overflow-x:clip;}}/*!sc*/\nbody{margin:0;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;}/*!sc*/\nbody{--type-token:consumptionEditorial.body-core;text-transform:none;font-family:TNYAdobeCaslonPro ,Times New Roman,Times,serif;font-feature-settings:normal;font-style:normal;-webkit-letter-spacing:normal;-moz-letter-spacing:normal;-ms-letter-spacing:normal;letter-spacing:normal;line-break:auto;line-height:1.5em;font-size:21px;font-weight:400;overflow-wrap:normal;height:100%;min-height:100%;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;}/*!sc*/\n.page{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;padding-top:3.5rem;min-height:100vh;}/*!sc*/\n.stackednavigation-site-navigation .page{padding-top:3.5rem;}/*!sc*/\n@media (min-width:1024px){.stackednavigation-site-navigation .page{padding-top:11rem;}}/*!sc*/\n@media (min-width:1024px){.stackednavigation-site-navigation .page.page-hero-ad-hidden,.stackednavigation-site-navigation .page.page-theme-inverted{padding-top:7rem;}}/*!sc*/\n.stackednavigation-site-navigation.fixed-header-x-large-logo-with-mega-menu-nav-variation.header-with-gql-link-banner.link-banner-link-list{padding-top:0;}/*!sc*/\n@media (min-width:1024px){.stackednavigation-site-navigation.fixed-header-x-large-logo-with-mega-menu-nav-variation.header-with-gql-link-banner.link-banner-link-list .page{padding-top:0;}}/*!sc*/\n@media (min-width:768px){.stackednavigation-site-navigation.fixed-header-x-large-logo-with-mega-menu-nav-variation.header-with-gql-link-banner.link-banner-link-list .page{padding-top:0;}}/*!sc*/\n.siteheader-site-navigation .page{padding-top:0;}/*!sc*/\n@media (min-width:1024px){.siteheader-site-navigation .page .grid-layout__aside .sticky-box{top:8rem;}}/*!sc*/\n@media (min-width:1024px){.fixed-header-with-link-banner-nav-variation .page.page-theme-inverted{padding-top:7.75rem;}}/*!sc*/\n.cns-ads-slot-state-empty{display:none;}/*!sc*/\n@media (min-width:320px){.cns-ads-slot-state-filled.cns-ads-slot-type-hero,.ad-height-hold,.ad-stickyhero{background-color:rgba(255,255,255,0);}}/*!sc*/\n@media (min-width:768px){.cns-ads-slot-state-filled.cns-ads-slot-type-hero,.ad-height-hold,.ad-stickyhero{background-color:rgba(255,255,255,0);}}/*!sc*/\n.ad-stickyhero-issticky{background-color:rgba(255,255,255,1);}/*!sc*/\n.page-theme-inverted .ad-height-hold,.page-theme-inverted .ad__slot--hero,.page-theme-inverted .ad--hero,.page-theme-inverted .ad-stickyhero,.page-theme-inverted .cns-ads-slot-state-filled.cns-ads-slot-type-hero{background-color:rgba(0,0,0,1);}/*!sc*/\n.page-theme-special .ad-height-hold,.page-theme-special .ad__slot--hero,.page-theme-special .ad--hero,.page-theme-special .ad-stickyhero,.page-theme-special .cns-ads-slot-state-filled.cns-ads-slot-type-hero{background-color:rgba(245,245,245,1);}/*!sc*/\n.page .cns-ads-slot-type-footer{padding:2.5rem 0;}/*!sc*/\n.ad--footer{background-color:rgba(255,255,255,0);}/*!sc*/\n.ad--footer.should-hold-space{min-height:calc(50px + 5rem);}/*!sc*/\n@media (min-width:768px){.ad--footer.should-hold-space{min-height:calc(90px + 5rem);}}/*!sc*/\n.ad--mid-content{position:relative;}/*!sc*/\n.ad--mid-content .cns-ads-slot-size-2x1,.ad--mid-content .cns-ads-slot-size-4x1,.ad--mid-content .cns-ads-slot-size-9x1,.ad--mid-content .cns-ads-slot-size-9x2{z-index:80;}/*!sc*/\n.ad--mid-content .ad-label{--type-token:globalEditorial.ad-label;text-transform:uppercase;font-family:NeutrafaceNewYorker ,Helvetica Neue,Helvetica,Arial,sans-serif;font-feature-settings:normal;font-style:normal;-webkit-letter-spacing:0.15em;-moz-letter-spacing:0.15em;-ms-letter-spacing:0.15em;letter-spacing:0.15em;line-break:auto;line-height:1em;font-size:10px;font-weight:400;overflow-wrap:normal;display:block;text-align:center;text-transform:uppercase;color:rgba(151,151,151,1);}/*!sc*/\n.ad--mid-content .ad__slot{background-color:rgba(255,255,255,0);}/*!sc*/\n.ad-label{--type-token:globalEditorial.ad-label;text-transform:uppercase;font-family:NeutrafaceNewYorker ,Helvetica Neue,Helvetica,Arial,sans-serif;font-feature-settings:normal;font-style:normal;-webkit-letter-spacing:0.15em;-moz-letter-spacing:0.15em;-ms-letter-spacing:0.15em;letter-spacing:0.15em;line-break:auto;line-height:1em;font-size:10px;font-weight:400;overflow-wrap:normal;display:block;top:-1.5rem;padding-top:1.5em;text-align:center;text-transform:uppercase;color:rgba(151,151,151,1);}/*!sc*/\n.ad--in-content{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/\n.ad--in-content .ad__slot{width:85%;max-width:575px;}/*!sc*/\n.ad--in-content .ad-label{--type-token:globalEditorial.ad-label;text-transform:uppercase;font-family:NeutrafaceNewYorker ,Helvetica Neue,Helvetica,Arial,sans-serif;font-feature-settings:normal;font-style:normal;-webkit-letter-spacing:0.15em;-moz-letter-spacing:0.15em;-ms-letter-spacing:0.15em;letter-spacing:0.15em;line-break:auto;line-height:1em;font-size:10px;font-weight:400;overflow-wrap:normal;display:block;top:-1.5rem;padding-top:1.5em;text-align:center;text-transform:uppercase;color:rgba(151,151,151,1);}/*!sc*/\n.grid + .row-mid-content-ad .ad--mid-content{z-index:calc(300 - 1);margin-top:-0.5rem;margin-bottom:2rem;}/*!sc*/\n.ad-stickyhero{top:0;z-index:300;}/*!sc*/\n.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-issticky{position:-webkit-sticky;position:sticky;top:56px;z-index:400;}/*!sc*/\n.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-issticky.with-searchbar{top:112px;}/*!sc*/\n@media (min-width:1024px){.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-issticky.with-searchbar{top:136px;}}/*!sc*/\n.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-issticky.with-link-banner{top:116px;}/*!sc*/\n@media (max-width:768px){.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-issticky.with-link-banner{top:116px;}}/*!sc*/\n@media (max-width:768px){.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-issticky.with-link-banner-is-scrolled{top:60px;}}/*!sc*/\n.siteheader-site-navigation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-issticky{top:0;}/*!sc*/\n.siteheader-site-navigation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-issticky.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:0;background-color:transparent;}/*!sc*/\n@media (max-width:768px){.siteheader-site-navigation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-issticky.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:0;}}/*!sc*/\n.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-animating{-webkit-transition:-webkit-transform 1s ease;-webkit-transition:transform 1s ease;transition:transform 1s ease;}/*!sc*/\n.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible{top:0;-webkit-transform:translateY(0);-ms-transform:translateY(0);transform:translateY(0);}/*!sc*/\n@media (max-width:768px){.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible{top:55px;}}/*!sc*/\n.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:60px;}/*!sc*/\n@media (max-width:768px){.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:55px;}}/*!sc*/\n.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-50{-webkit-transform:translateY(-160px);-ms-transform:translateY(-160px);transform:translateY(-160px);}/*!sc*/\n.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-90{-webkit-transform:translateY(-240px);-ms-transform:translateY(-240px);transform:translateY(-240px);}/*!sc*/\n.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-250{-webkit-transform:translateY(-560px);-ms-transform:translateY(-560px);transform:translateY(-560px);}/*!sc*/\n@media (min-width:768px){.ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-issticky{position:-webkit-sticky;position:sticky;top:56px;z-index:400;}.ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-issticky.with-searchbar{top:112px;}@media (min-width:1024px){.ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-issticky.with-searchbar{top:136px;}}.ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-issticky.with-link-banner{top:116px;}@media (max-width:768px){.ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-issticky.with-link-banner{top:116px;}}@media (max-width:768px){.ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-issticky.with-link-banner-is-scrolled{top:60px;}}.siteheader-site-navigation .ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-issticky{top:0;}.siteheader-site-navigation .ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-issticky.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:0;background-color:transparent;}@media (max-width:768px){.siteheader-site-navigation .ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-issticky.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:0;}}.ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-sticky-animating{-webkit-transition:-webkit-transform 1s ease;-webkit-transition:transform 1s ease;transition:transform 1s ease;}.ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-sticky-visible{top:0;-webkit-transform:translateY(0);-ms-transform:translateY(0);transform:translateY(0);}@media (max-width:768px){.ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-sticky-visible{top:55px;}}.ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:60px;}@media (max-width:768px){.ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:55px;}}.ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-sticky-50{-webkit-transform:translateY(-160px);-ms-transform:translateY(-160px);transform:translateY(-160px);}.ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-sticky-90{-webkit-transform:translateY(-240px);-ms-transform:translateY(-240px);transform:translateY(-240px);}.ad-stickyhero.ad-stickyhero-disable-mobile.ad-stickyhero-sticky-250{-webkit-transform:translateY(-560px);-ms-transform:translateY(-560px);transform:translateY(-560px);}}/*!sc*/\n.ad-stickyhero .ad-stickyhero-button{position:absolute;right:0;z-index:100;margin:0 0.75rem 0 auto;border:0;background-color:transparent;padding:0;width:1.5rem;}/*!sc*/\n@media (min-width:320px){.ad-stickyhero .ad-stickyhero-button{display:none;}}/*!sc*/\n@media (min-width:768px){.ad-stickyhero .ad-stickyhero-button{display:block;top:10px;margin:0 1.25rem 0 1rem;}}/*!sc*/\n.ad-stickyhero .ad-stickyhero-button:hover,.ad-stickyhero .ad-stickyhero-button:focus{border:0;background-color:transparent;}/*!sc*/\n.ad-stickyhero .ad-stickyhero-button:hover svg path,.ad-stickyhero .ad-stickyhero-button:focus svg path{fill:rgba(8,121,191,1);}/*!sc*/\n.ad-stickyhero .ad-stickyhero-button .ButtonIconWrapper-gFdzAL{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;width:100%;max-width:1.4rem;height:100%;}/*!sc*/\n.ad-stickyhero .ad-stickyhero-button svg path{fill:rgba(0,0,0,1);}/*!sc*/\n.page-theme-inverted .ad-stickyhero .ad-stickyhero-button svg path{fill:rgba(255,255,255,1);}/*!sc*/\n@media (max-width:768px){.ad-stickymidcontent .ad.ad--mid-content{position:-webkit-sticky;position:sticky;top:5px;}}/*!sc*/\n.siteheader-site-navigation.drawer-enabled-nav-variation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:0;}/*!sc*/\n.stackednavigation-site-navigation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:105px;z-index:500;}/*!sc*/\n@media (max-width:768px){.stackednavigation-site-navigation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:55px;z-index:500;}}/*!sc*/\n.stackednavigation-site-navigation.fixed-header-logo-with-search-bar-nav-variation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:135px;}/*!sc*/\n.stackednavigation-site-navigation.fixed-header-logo-with-search-bar-nav-variation.header-with-gql-link-banner.link-banner-marquee.link-banner-without-navigation .ad__slot--hero,.stackednavigation-site-navigation.fixed-header-logo-with-search-bar-nav-variation.header-with-gql-link-banner.link-banner-marquee.link-banner-without-navigation .ad--hero,.stackednavigation-site-navigation.fixed-header-logo-with-search-bar-nav-variation.header-with-gql-link-banner.link-banner-marquee.link-banner-without-navigation .ad-stickyhero,.stackednavigation-site-navigation.fixed-header-logo-with-search-bar-nav-variation.header-with-gql-link-banner.link-banner-marquee.link-banner-without-navigation .ad-stickyhero--standard{padding-top:0;}/*!sc*/\n@media (max-width:768px){.stackednavigation-site-navigation.fixed-header-logo-with-search-bar-nav-variation.header-with-gql-link-banner.link-banner-marquee.link-banner-without-navigation .ad__slot--hero,.stackednavigation-site-navigation.fixed-header-logo-with-search-bar-nav-variation.header-with-gql-link-banner.link-banner-marquee.link-banner-without-navigation .ad--hero,.stackednavigation-site-navigation.fixed-header-logo-with-search-bar-nav-variation.header-with-gql-link-banner.link-banner-marquee.link-banner-without-navigation .ad-stickyhero,.stackednavigation-site-navigation.fixed-header-logo-with-search-bar-nav-variation.header-with-gql-link-banner.link-banner-marquee.link-banner-without-navigation .ad-stickyhero--standard{padding-top:12px;}}/*!sc*/\n.stackednavigation-site-navigation.fixed-header-logo-with-search-bar-nav-variation.header-with-gql-link-banner.link-banner-marquee.link-banner-without-navigation .ad-stickyhero--standard.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:120px;}/*!sc*/\n.stackednavigation-site-navigation.fixed-header-large-logo-with-right-menu-nav-variation{background-color:transparent;}/*!sc*/\n.stackednavigation-site-navigation.fixed-header-large-logo-with-right-menu-nav-variation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:63px;}/*!sc*/\n@media (max-width:768px){.stackednavigation-site-navigation.fixed-header-large-logo-with-right-menu-nav-variation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:55px;}}/*!sc*/\n@media (max-width:768px){.stackednavigation-site-navigation.fixed-header-large-logo-with-right-menu-nav-variation.header-with-gql-link-banner.link-banner-link-list .ad-stickyhero.ad-stickyhero--standard.ad-stickyhero-enable-mobile.should-hold-space{padding-top:43px;}}/*!sc*/\n.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-marquee .ad__slot--hero,.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-marquee .ad--hero,.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-marquee .ad-stickyhero,.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-marquee .ad-stickyhero--standard{padding-top:12px;}/*!sc*/\n.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-marquee .ad-stickyhero--standard.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:80px;z-index:500;}/*!sc*/\n.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-marquee .ad-stickyhero--standard.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled .button.ad-stickyhero-button{position:relative;float:right;}/*!sc*/\n@media (min-width:768px){.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-marquee .ad-stickyhero--standard.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled .button.ad-stickyhero-button{display:block;top:30px;}}/*!sc*/\n.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-link-list .ad-stickyhero.ad-stickyhero--standard.ad-stickyhero-enable-mobile.should-hold-space.ad-stickyhero-issticky.ad-stickyhero-sticky-animating.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:124px;z-index:2;}/*!sc*/\n@media (max-width:1024px){.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-link-list .ad-stickyhero.ad-stickyhero--standard.ad-stickyhero-enable-mobile.should-hold-space.ad-stickyhero-issticky.ad-stickyhero-sticky-animating.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:124px;}}/*!sc*/\n@media (max-width:768px){.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-link-list .ad-stickyhero.ad-stickyhero--standard.ad-stickyhero-enable-mobile.should-hold-space.ad-stickyhero-issticky.ad-stickyhero-sticky-animating.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:56px;padding-top:20px;}}/*!sc*/\n.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-link-list .button.ad-stickyhero-button{position:relative;float:right;}/*!sc*/\n@media (max-width:768px){.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-link-list .button.ad-stickyhero-button{position:relative;float:right;}}/*!sc*/\n.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-visual-link-banner .ad-stickyhero,.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-visual-link-banner .ad-stickyhero--standard,.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-visual-link-banner .ad-stickyhero-issticky{top:0;z-index:2;}/*!sc*/\n.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-visual-link-banner .ad-stickyhero .button.ad-stickyhero-button,.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-visual-link-banner .ad-stickyhero--standard .button.ad-stickyhero-button,.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-visual-link-banner .ad-stickyhero-issticky .button.ad-stickyhero-button{position:relative;float:right;}/*!sc*/\n.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-visual-link-banner .ad-stickyhero.ad-stickyhero--standard.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled,.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-visual-link-banner .ad-stickyhero--standard.ad-stickyhero--standard.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled,.stackednavigation-site-navigation.header-with-gql-link-banner.link-banner-visual-link-banner .ad-stickyhero-issticky.ad-stickyhero--standard.ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:-40px;z-index:2;}/*!sc*/\n.stackednavigation-site-navigation.fixed-header-x-large-logo-with-mega-menu-nav-variation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:39px;z-index:2;padding-top:0;}/*!sc*/\n@media (max-width:1024px){.stackednavigation-site-navigation.fixed-header-x-large-logo-with-mega-menu-nav-variation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:0;}}/*!sc*/\n@media (max-width:768px){.stackednavigation-site-navigation.fixed-header-x-large-logo-with-mega-menu-nav-variation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:0;}}/*!sc*/\n.stackednavigation-site-navigation.fixed-header-x-large-logo-with-mega-menu-nav-variation.header-with-gql-link-banner.link-banner-link-list{padding-top:220px;}/*!sc*/\n@media (max-width:768px){.stackednavigation-site-navigation.fixed-header-x-large-logo-with-mega-menu-nav-variation.header-with-gql-link-banner.link-banner-link-list{padding-top:40px;}}/*!sc*/\n@media (max-width:768px){.stackednavigation-site-navigation.fixed-header-x-large-logo-with-mega-menu-nav-variation.header-with-gql-link-banner.link-banner-link-list .ad-stickyhero.ad-stickyhero--standard.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-50{-webkit-transform:translateY(0);-ms-transform:translateY(0);transform:translateY(0);}}/*!sc*/\n.stackednavigation-site-navigation.fixed-header-x-large-logo-with-mega-menu-nav-variation .page-theme-inverted .ad-stickyhero{position:relative;padding-top:77px;}/*!sc*/\n@media (max-width:1024px){.stackednavigation-site-navigation.fixed-header-x-large-logo-with-mega-menu-nav-variation .page-theme-inverted .ad-stickyhero{top:0;padding-top:30px;}}/*!sc*/\n@media (max-width:768px){.stackednavigation-site-navigation.fixed-header-x-large-logo-with-mega-menu-nav-variation .page-theme-inverted .ad-stickyhero{padding-top:12px;}}/*!sc*/\n@media (max-width:768px){.stackednavigation-site-navigation .ad-stickymidcontent .ad.ad--mid-content{top:70px;}}/*!sc*/\n@media (max-width:768px){.logo-left-with-search-nav-variation .ad-stickymidcontent .ad.ad--mid-content{top:70px;}}/*!sc*/\n.standardnavigation-site-navigation.logo-left-with-search-nav-variation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled,.standardnavigation-site-navigation.logo-center-with-cm-nav-variation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled,.standardnavigation-site-navigation.logo-left-with-search-and-headline-nav-variation .ad-stickyhero.ad-stickyhero-enable-mobile.ad-stickyhero-sticky-visible.visual-link-banner--is-scrolled{top:56px;}/*!sc*/\n@media (max-width:768px){.standardnavigation-site-navigation .ad-stickymidcontent .ad.ad--mid-content{top:65.6px;}}/*!sc*/\n@media (max-width:1024px){.fixed-header-large-logo-with-right-menu-and-link-banner-nav-variation .ad-stickyhero{padding-top:0;}}/*!sc*/\n@media (max-width:768px){.fixed-header-large-logo-with-right-menu-and-link-banner-nav-variation .ad-stickyhero{padding-top:43px;}}/*!sc*/\n@media (max-width:1024px){.fixed-header-large-logo-with-right-menu-and-link-banner-nav-variation .ad-stickyhero-issticky{padding-top:0;}}/*!sc*/\n@media (max-width:768px){.fixed-header-large-logo-with-right-menu-and-link-banner-nav-variation .ad-stickyhero-issticky{padding-top:0;}}/*!sc*/\n@media print{.ad{display:none;}}/*!sc*/\ninput[type='submit'],button{-webkit-text-decoration:none;text-decoration:none;cursor:pointer;display:inline-block;--background-color__token-name:colors.interactive.base.light;background-color:rgba(229,229,229,1);text-align:center;-webkit-text-decoration:none;text-decoration:none;border:0;}/*!sc*/\ninput[type='submit']:hover,button:hover,input[type='submit']:focus,button:focus{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/\ninput[type='checkbox']:checked + label::after{opacity:1;}/*!sc*/\ninput[type='checkbox']:disabled + label{opacity:0.5;}/*!sc*/\ninput[type='checkbox'].checkbox__invalid + label,input[type='checkbox']:invalid + label{--color__token-name:colors.interactive.base.black;color:rgba(0,0,0,1);}/*!sc*/\ninput[type='checkbox'].checkbox__invalid + label::before,input[type='checkbox']:invalid + label::before{border:1px solid;--border-color__token-name:colors.interactive.feedback.invalid-primary;border-color:rgba(170,27,25,1);background-color:rgba( 255,255,255, 0.15 );}/*!sc*/\ninput[type='checkbox'].checkbox__invalid + label::after,input[type='checkbox']:invalid + label::after{opacity:1;}/*!sc*/\ninput[type='checkbox'] + label{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;position:relative;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/\ninput[type='checkbox'] + label:hover::before,input[type='checkbox'] + label:focus::before{--border-color__token-name:colors.interactive.base.black;border-color:rgba(0,0,0,1);}/*!sc*/\ninput[type='checkbox'] + label::before{display:block;position:relative;content:'';margin-right:0.5em;border-width:1px;border-style:solid;height:1.5em;--border-color__token-name:colors.interactive.base.black;border-color:rgba(0,0,0,1);width:1.5em;}/*!sc*/\ninput[type='checkbox'] + label::after{display:block;position:absolute;content:'';top:0.9em;left:0.75em;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);opacity:0;content:'✔︎';-webkit-transition-property:all;transition-property:all;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in-out;transition-timing-function:ease-in-out;}/*!sc*/\ninput[type='checkbox'].checkbox--switch:checked + label::after{-webkit-transform:translateX(1.1em) translateY(-50%);-ms-transform:translateX(1.1em) translateY(-50%);transform:translateX(1.1em) translateY(-50%);opacity:1;--background-color__token-name:colors.interactive.base.white;background-color:rgba(255,255,255,1);}/*!sc*/\ninput[type='checkbox'].checkbox--switch + label:hover::before,input[type='checkbox'].checkbox--switch + label:focus::before{--border-color__token-name:colors.interactive.base.black;border-color:rgba(0,0,0,1);}/*!sc*/\ninput[type='checkbox'].checkbox--switch + label::before{margin-right:0.5em;border-width:1px;border-style:solid;height:1.5em;--border-color__token-name:colors.interactive.base.black;border-color:rgba(0,0,0,1);border-radius:1em;width:3em;}/*!sc*/\ninput[type='checkbox'].checkbox--switch + label::after{display:block;position:absolute;content:'';top:0.87em;left:0.75em;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);opacity:1;border:0;border-radius:50%;--background-color__token-name:colors.interactive.base.light;background-color:rgba(229,229,229,1);width:0.999em;height:0.999em;-webkit-transition-property:background-color,-webkit-transform;-webkit-transition-property:background-color,transform;transition-property:background-color,transform;-webkit-transition-duration:0.3s;transition-duration:0.3s;-webkit-transition-timing-function:ease-in-out;transition-timing-function:ease-in-out;}/*!sc*/\ninput[type='radio']:checked + label::after{opacity:1;}/*!sc*/\ninput[type='radio']:disabled + label{opacity:0.5;}/*!sc*/\ninput[type='radio'] + label{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;position:relative;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}/*!sc*/\ninput[type='radio'] + label:hover::before,input[type='radio'] + label:focus::before{--border-color__token-name:colors.interactive.base.black;border-color:rgba(0,0,0,1);}/*!sc*/\ninput[type='radio'] + label::before{display:block;position:relative;content:'';margin-right:0.5em;border:1px solid;--border-color__token-name:colors.interactive.base.dark;border-color:rgba(102,102,102,1);border-radius:50%;--background-color__token-name:colors.interactive.base.white;background-color:rgba(255,255,255,1);width:1.5em;height:1.5em;}/*!sc*/\ninput[type='radio'] + label::after{display:block;position:absolute;content:'';top:0.87em;left:0.795em;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);opacity:0;border-radius:50%;--background-color__token-name:colors.interactive.base.white;background-color:rgba(255,255,255,1);width:0.999em;height:0.999em;-webkit-transition-property:all;transition-property:all;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in-out;transition-timing-function:ease-in-out;}/*!sc*/\nfieldset{margin:0;border:0;padding:0;}/*!sc*/\ninput{border:1px solid;--border-color__token-name:colors.interactive.base.black;border-color:rgba(0,0,0,1);padding:0.125em 0.5rem;}/*!sc*/\ninput[disabled]{pointer-events:none;}/*!sc*/\ninput[type='checkbox'],input[type='radio']{position:absolute;top:0;right:0;bottom:0;left:0;visibility:initial;opacity:0;margin:0;cursor:inherit;padding:0;width:100%;height:100%;}/*!sc*/\ninput[type='submit']{-webkit-text-decoration:none;text-decoration:none;cursor:pointer;display:inline-block;--background-color__token-name:colors.interactive.base.light;background-color:rgba(229,229,229,1);text-align:center;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/\ninput[type='submit']:hover,input[type='submit']:focus{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/\ninput[type='reset']{cursor:pointer;}/*!sc*/\ninput[type='number']::-webkit-inner-spin-button,input[type='number']::-webkit-outer-spin-button{height:auto;}/*!sc*/\ninput[type='search']{box-sizing:border-box;}/*!sc*/\nselect{border:1px solid;--border-color__token-name:colors.interactive.base.black;border-color:rgba(0,0,0,1);padding:0.125em 0.5rem;width:auto;}/*!sc*/\ntextarea{border:1px solid;--border-color__token-name:colors.interactive.base.black;border-color:rgba(0,0,0,1);padding:0.125em 0.5rem;display:block;width:100%;max-width:25em;}/*!sc*/\nimg{max-width:100%;height:auto;vertical-align:bottom;}/*!sc*/\na{cursor:pointer;--color__token-name:colors.interactive.base.light;color:rgba(229,229,229,1);-webkit-transition-property:color,background,text-shadow;transition-property:color,background,text-shadow;-webkit-transition-duration:0.2s;transition-duration:0.2s;-webkit-transition-timing-function:ease-in-out;transition-timing-function:ease-in-out;}/*!sc*/\ntable{width:100%;}/*!sc*/\ntd{padding:0.5em;vertical-align:bottom;text-align:left;vertical-align:baseline;}/*!sc*/\nth{padding:0.5em;vertical-align:bottom;text-align:left;vertical-align:bottom;}/*!sc*/\nthead{--border-color__token-name:colors.foundation.menu.dividers;border-color:rgba(229,229,229,1);border-width:0 0 2px 0;border-style:solid;}/*!sc*/\ntfoot{--border-color__token-name:colors.foundation.menu.dividers;border-color:rgba(229,229,229,1);border-width:2px 0 0 0;border-style:solid;}/*!sc*/\ntbody tr{--border-color__token-name:colors.foundation.menu.dividers;border-color:rgba(229,229,229,1);border-width:0 0 1px 0;border-style:solid;}/*!sc*/\ntbody tr:nth-of-type(even){background-color:rgba( 229,229,229, 0.5 );}/*!sc*/\ntbody td{--border-color__token-name:colors.foundation.menu.dividers;border-color:rgba(229,229,229,1);border-width:1px 0 0 0;border-style:solid;}/*!sc*/\ntbody td:last-of-type{border-right:0;}/*!sc*/\ndata-styled.g2006[id=\"sc-global-kAyyKB1\"]{content:\"sc-global-kAyyKB1,\"}/*!sc*/\n.kybWcM.kybWcM.kybWcM{background-color:#FFFFFF;}/*!sc*/\n@media (min-width:768px){.kybWcM.kybWcM.kybWcM .grid-layout__content{grid-column:3 / span 8;}}/*!sc*/\n@media (min-width:1024px){.kybWcM.kybWcM.kybWcM .grid-layout__content{grid-column:2 / span 6;}}/*!sc*/\n@media (min-width:320px) and (max-width:768px){.kybWcM.kybWcM.kybWcM .grid-layout__content{grid-column:1 / -1;}}/*!sc*/\n@media (min-width:768px){.kybWcM.kybWcM.kybWcM .grid-layout--adrail.narrow .container--body-inner{grid-column:1 / -1;}}/*!sc*/\n.kybWcM.kybWcM.kybWcM .grid-layout--adrail.narrow .RecircMostPopularWrapper-bRuGTi:first-child{margin-top:0;}/*!sc*/\n.kybWcM.kybWcM.kybWcM .grid-layout--adrail.narrow .RecircMostPopularWrapper-bRuGTi:first-child .RecircMostPopularHeading-iVzhzt{margin-top:0;}/*!sc*/\n.kybWcM.kybWcM.kybWcM .container--body{grid-gap:20px;}/*!sc*/\n.kybWcM.kybWcM.kybWcM inline-embed[name='align-right']{text-align:right;}/*!sc*/\n.kybWcM.kybWcM.kybWcM inline-embed[name='align-center']{text-align:center;}/*!sc*/\ndata-styled.g2022[id=\"ArticlePageBase-jjJCSb\"]{content:\"kybWcM,\"}/*!sc*/\n</style><style id=\"font-faces\">@font-face {font-family: Adobe Caslon;font-weight: 400;font-style: italic;font-display: swap;src: url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-Italic.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-Italic.woff\") format(\"woff\"); }@font-face {font-family: Adobe Caslon;font-weight: 400;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-Regular.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-Regular.woff\") format(\"woff\"); }@font-face {font-family: Graphik;font-weight: 700;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/Graphik-Medium.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/Graphik-Medium.woff\") format(\"woff\"); }@font-face {font-family: Graphik;font-weight: 700;font-style: italic;font-display: swap;src: url(\"/verso/static/assets/fonts/Graphik-MediumItalic.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/Graphik-MediumItalic.woff\") format(\"woff\"); }@font-face {font-family: Graphik;font-weight: 500;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/Graphik-Medium.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/Graphik-Medium.woff\") format(\"woff\"); }@font-face {font-family: Graphik;font-weight: 500;font-style: italic;font-display: swap;src: url(\"/verso/static/assets/fonts/Graphik-MediumItalic.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/Graphik-MediumItalic.woff\") format(\"woff\"); }@font-face {font-family: Graphik;font-weight: 400;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/Graphik-Regular-Web.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/Graphik-Regular-Web.woff\") format(\"woff\"); }@font-face {font-family: Graphik;font-weight: 400;font-style: italic;font-display: swap;src: url(\"/verso/static/assets/fonts/Graphik-RegularItalic-Web.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/Graphik-RegularItalic-Web.woff\") format(\"woff\"); }@font-face {font-family: Graphik Web;font-weight: 500;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/Graphik-Medium.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/Graphik-Medium.woff\") format(\"woff\"); }@font-face {font-family: Irvin Heading;font-weight: 400;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/TNYIrvinPro-HeadingSimple.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/TNYIrvinPro-HeadingSimple.woff\") format(\"woff\"); }@font-face {font-family: IrvinText;font-weight: 400;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/IrvinText-Regular.woff\") format(\"woff\"); }@font-face {font-family: IrvinHeadingPro;font-weight: 400;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/TNYIrvinPro-HeadingSimple.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/TNYIrvinPro-HeadingSimple.woff\") format(\"woff\"); }@font-face {font-family: IrvinHeadingPro;font-weight: bold;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/TNYIrvinPro-HeadingSimple.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/TNYIrvinPro-HeadingSimple.woff\") format(\"woff\"); }@font-face {font-family: IrvinHeadingPro;font-weight: 400;font-style: italic;font-display: swap;src: url(\"/verso/static/assets/fonts/TNYIrvinPro-HeadingSimple.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/TNYIrvinPro-HeadingSimple.woff\") format(\"woff\"); }@font-face {font-family: IrvinHeadingPro;font-weight: bold;font-style: italic;font-display: swap;src: url(\"/verso/static/assets/fonts/TNYIrvinPro-HeadingSimple.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/TNYIrvinPro-HeadingSimple.woff\") format(\"woff\"); }@font-face {font-family: Irvin Text;font-weight: 400;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/IrvinText-Regular.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/IrvinText-Regular.woff\") format(\"woff\"); }@font-face {font-family: Neutra Face;font-weight: 600;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/NeutrafaceNewYorker-SemiBold.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/NeutrafaceNewYorker-SemiBold.woff\") format(\"woff\"); }@font-face {font-family: NeutrafaceNewYorker;font-weight: 600;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/NeutrafaceNewYorker-SemiBold.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/NeutrafaceNewYorker-SemiBold.woff\") format(\"woff\"); }@font-face {font-family: TNYAdobeCaslonPro;font-weight: 700;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-Bold.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-Bold.woff\") format(\"woff\"); }@font-face {font-family: TNYAdobeCaslonPro;font-weight: 700;font-style: italic;font-display: swap;src: url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-BoldItalic.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-BoldItalic.woff\") format(\"woff\"); }@font-face {font-family: TNYAdobeCaslonPro;font-weight: 400;font-style: italic;font-display: swap;src: url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-Italic.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-Italic.woff\") format(\"woff\"); }@font-face {font-family: TNYAdobeCaslonPro;font-weight: 400;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-Regular.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-Regular.woff\") format(\"woff\"); }@font-face {font-family: TNYAdobeCaslonPro;font-weight: 600;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-SemiBold.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-SemiBold.woff\") format(\"woff\"); }@font-face {font-family: TNYAdobeCaslonPro;font-weight: 600;font-style: italic;font-display: swap;src: url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-SemiBoldItalic.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/TNYAdobeCaslonPro-SemiBoldItalic.woff\") format(\"woff\"); }@font-face {font-family: Lora;font-weight: 700;font-style: italic;font-display: swap;src: url(\"/verso/static/assets/fonts/Lora-BoldItalic.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/Lora-BoldItalic.woff\") format(\"woff\"); }@font-face {font-family: Lora;font-weight: 400;font-style: italic;font-display: swap;src: url(\"/verso/static/assets/fonts/Lora-RegularItalic.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/Lora-RegularItalic.woff\") format(\"woff\"); }@font-face {font-family: Lora;font-weight: 700;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/lora-bold.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/lora-bold.woff\") format(\"woff\"); }@font-face {font-family: Lora;font-weight: 400;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/lora-regular.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/lora-regular.woff\") format(\"woff\"); }@font-face {font-family: Lato;font-weight: 400;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/Lato-Regular.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/Lato-Regular.woff\") format(\"woff\"); }@font-face {font-family: Lato;font-weight: 400;font-style: italic;font-display: swap;src: url(\"/verso/static/assets/fonts/Lato-RegularItalic.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/Lato-RegularItalic.woff\") format(\"woff\"); }@font-face {font-family: Lato;font-weight: 600;font-style: normal;font-display: swap;src: url(\"/verso/static/assets/fonts/Lato-SemiBold.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/Lato-SemiBold.woff\") format(\"woff\"); }@font-face {font-family: Lato;font-weight: 600;font-style: italic;font-display: swap;src: url(\"/verso/static/assets/fonts/Lato-SemiBoldItalic.woff2\") format(\"woff2\"), url(\"/verso/static/assets/fonts/Lato-SemiBoldItalic.woff\") format(\"woff\"); }</style><link rel=\"shortcut icon\" href=\"https://www.newyorker.com/verso/static/the-new-yorker/assets/favicon.ico\" type=\"image/x-icon\"/><link rel=\"preconnect\" href=\"https://polyfill-fastly.io\"/><script type=\"text/javascript\">var Bus=function(){\"use strict\";var r=0;function n(r){var n=[],t=0,i=0;this.push=function(s){t-i>=r&&++i>=r&&(i=0,t=r-1),n[t%r]=s,t++},this.asArray=function(){var s=n.slice(i,Math.min(t,r)),u=n.slice(0,Math.max(t-r,0));return s.concat(u)},this.list=n}function t(n,t){for(var i=t,s=0;s<n.length;s++){var u=n[s],a=i.r;a[u]||(a[u]={w:u,r:{},i:r++}),i=a[u]}return i}function i(r,n,t){var i;return t[r]?i=t[r]:(i=function(r,n){for(var t=[[n,0]],i={},s=[];t.length;){var u=t.shift(),a=u[0],e=u[1],f=a.r,h=r[e];if(void 0===h&&a.fn&&!i[a.i]?(i[a.i]=1,s.push(a.fn)):f[h]&&t.push([f[h],e+1]),f[\"#\"])for(var o=e;o<=r.length;o++)t.push([f[\"#\"],o]);h&&f[\"*\"]&&t.push([f[\"*\"],e+1])}return s}(r.split(\".\"),n),t[r]=i),i}var s=function(){var s={w:\"\",r:{},i:r++},u={},a=new n(9999);function e(r,n){var i=t(r.split(\".\"),s),a=i.fn||[];return a.push(n),i.fn=a,u={},function(){var r=a.indexOf(n);r>-1&&a.splice(r,1)}}function f(r,n){var t=Date.now();a.push([r,t]);for(var e=i(r,s,u),f={topic:r},h=0;h<e.length;h++)for(var o=e[h],v=0;v<o.length;v++)o[v](n,f)}this.emit=f,this.on=e,this.history=function(n){var s={w:\"\",r:{},i:r++};t(n.split(\".\"),s).fn=1;for(var u=[],e={},f=a.asArray(),h=0;h<f.length;h++){var o=f[h];i(o[0],s,e).length&&u.push(o)}return u},this.publish=f,this.subscribe=e};return s.Ring=n,s}();\"undefined\"!=typeof exports&&(module.exports=Bus); window.cnBus = window.cnBus || new Bus();</script><link rel=\"preload\" id=\"oneTrustPreload\" href=\"https://cdn.cookielaw.org/scripttemplates/otSDKStub.js\" as=\"script\"/><script id=\"onetrust-script\" src=\"https://cdn.cookielaw.org/scripttemplates/otSDKStub.js\" data-domain-script=\"50e16771-e794-4da4-88c1-7d09c0587e00\"></script><script id=\"oneTrustScripts\">window.OptanonWrapper=function(){var CCPAButton=document.getElementById(\"ot-sdk-btn\");CCPAButton&&CCPAButton.classList.add(\"ot-sdk-btn--visible\");window.dataLayer&&window.dataLayer.push({event:\"OneTrustGroupsUpdated\"});window.cnBus&&window.cnBus.emit(\"onetrust.OneTrustGroupsUpdated\")};\nfunction getCookie(name){var parts=(\"; \"+document.cookie).split(\"; \"+name+\"=\");if(2==parts.length)return parts.pop().split(\";\").shift()}function setCookie(cname,cvalue,exdays){var d=new Date;d.setTime(d.getTime()+24*exdays*60*60*1e3);var expires=\"expires=\"+d.toUTCString();document.cookie=cname+\"=\"+cvalue+\";\"+expires+\";path=/\"}function setGPC(){var gpcValue=navigator.globalPrivacyControl;if(null==getCookie(\"OptanonAlertBoxClosed\")&&gpcValue){console.log(\"First visit & GPC enabled; setting targeting group to 0\");setCookie(\"OptanonConsent\",encodeURIComponent(\"groups=C0003:0,C0004:0,C0005:0\"),365)}}setGPC();</script><link rel=\"preconnect\" href=\"https://securepubads.g.doubleclick.net\"/><script src=\"https://securepubads.g.doubleclick.net/tag/js/gpt.js\" id=\"gpt-script\" async=\"\"></script><script>\n          window.googletag = window.googletag || { cmd: [] }\n        </script><script type=\"text/javascript\">window.cns = window.cns || {}; window.cns.pageContext = {\"channel\":\"magazine\",\"content\":{\"copyCount\":12144,\"imageCount\":0,\"embedCount\":14,\"ratio\":867.4285714285714,\"midContentCadence\":900},\"contentType\":\"article\",\"experiments\":{},\"keywords\":{\"copilotid\":[\"5911cbb2803aff0f1c1359ac\"],\"platform\":[\"verso\"],\"tags\":[\"magazine\",\"a-reporter-at-large\",\"2015-11-23\",\"artificial-intelligence-ai\",\"category-science-tech\",\"philosophers\",\"reporting\",\"hide-page-items\",\"override-all\",\"amp-exclude\",\"today-exclude\",\"archive-restoration\"]},\"server\":\"production\",\"slug\":\"doomsday-invention-artificial-intelligence-nick-bostrom\",\"subChannel\":\"a-reporter-at-large\",\"subSubChannel\":\"\",\"templateType\":\"mt_article_override\"};</script><link rel=\"preconnect\" href=\"https://ads-static.conde.digital\"/><script id=\"ads-v6\" src=\"https://ads-static.conde.digital/production/cns/builds/the-new-yorker/v6.js\" async=\"\"></script><script type=\"application/ld+json\">{\"@context\":\"http://schema.org\",\"@type\":\"NewsArticle\",\"articleBody\":\"I. Omens\\nLast year, a curious nonfiction book became a Times best-seller: a dense meditation on artificial intelligence by the philosopher Nick Bostrom, who holds an appointment at Oxford. Titled “Superintelligence: Paths, Dangers, Strategies,” it argues that true artificial intelligence, if it is realized, might pose a danger that exceeds every previous threat from technology—even nuclear weapons—and that if its development is not managed carefully humanity risks engineering its own extinction. Central to this concern is the prospect of an “intelligence explosion,” a speculative event in which an A.I. gains the ability to improve itself, and in short order exceeds the intellectual potential of the human brain by many orders of magnitude.\\nSuch a system would effectively be a new kind of life, and Bostrom’s fears, in their simplest form, are evolutionary: that humanity will unexpectedly become outmatched by a smarter competitor. He sometimes notes, as a point of comparison, the trajectories of people and gorillas: both primates, but with one species dominating the planet and the other at the edge of annihilation. “Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb,” he concludes. “We have little idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound.”\\nAt the age of forty-two, Bostrom has become a philosopher of remarkable influence. “Superintelligence” is only his most visible response to ideas that he encountered two decades ago, when he became a transhumanist, joining a fractious quasi-utopian movement united by the expectation that accelerating advances in technology will result in drastic changes—social, economic, and, most strikingly, biological—which could converge at a moment of epochal transformation known as the Singularity. Bostrom is arguably the leading transhumanist philosopher today, a position achieved by bringing order to ideas that might otherwise never have survived outside the half-crazy Internet ecosystem where they formed. He rarely makes concrete predictions, but, by relying on probability theory, he seeks to tease out insights where insights seem impossible.\\nSome of Bostrom’s cleverest arguments resemble Swiss Army knives: they are simple, toylike, a pleasure to consider, with colorful exteriors and precisely calibrated mechanics. He once cast a moral case for medically engineered immortality as a fable about a kingdom terrorized by an insatiable dragon. A reformulation of Pascal’s wager became a dialogue between the seventeenth-­century philosopher and a mugger from another dimension.\\n“Superintelligence” is not intended as a treatise of deep originality; Bostrom’s contribution is to impose the rigors of analytic philosophy on a messy corpus of ideas that emerged at the margins of academic thought. Perhaps because the field of A.I. has recently made striking advances—with everyday technology seeming, more and more, to exhibit something like intelligent reasoning—the book has struck a nerve. Bostrom’s supporters compare it to “Silent Spring.” In moral philosophy, Peter Singer and Derek Parfit have received it as a work of importance, and distinguished physicists such as Stephen Hawking have echoed its warning. Within the high caste of Silicon Valley, Bostrom has acquired the status of a sage. Elon Musk, the C.E.O. of Tesla, promoted the book on Twitter, noting, “We need to be super careful with AI. Potentially more dangerous than nukes.” Bill Gates recommended it, too. Suggesting that an A.I. could threaten humanity, he said, during a talk in China, “When people say it’s not a problem, then I really start to get to a point of disagreement. How can they not see what a huge challenge this is?”\\nThe people who say that artificial intelligence is not a problem tend to work in artificial intelligence. Many prominent researchers regard Bostrom’s basic views as implausible, or as a distraction from the near-term benefits and moral dilemmas posed by the technology—not least because A.I. systems today can barely guide robots to open doors. Last summer, Oren Etzioni, the C.E.O. of the Allen Institute for Artificial Intelligence, in Seattle, referred to the fear of machine intelligence as a “Frankenstein complex.” Another leading researcher declared, “I don’t worry about that for the same reason I don’t worry about overpopulation on Mars.” Jaron Lanier, a Microsoft researcher and tech commentator, told me that even framing the differing views as a debate was a mistake. “This is not an honest conversation,” he said. “People think it is about technology, but it is really about religion, people turning to metaphysics to cope with the human condition. They have a way of dramatizing their beliefs with an end-of-days scenario—and one does not want to criticize other people’s religions.”\\nBecause the argument has played out on blogs and in the popular press, beyond the ambit of peer-reviewed journals, the two sides have appeared in caricature, with headlines suggesting either doom (“Will Super-intelligent Machines Kill Us All?”) or a reprieve from doom (“Artificial intelligence ‘will not end human race’ ”). Even the most grounded version of the debate occupies philosophical terrain where little is clear. But, Bostrom argues, if artificial intelligence can be achieved it would be an event of unparalleled consequence—perhaps even a rupture in the fabric of history. A bit of long-range forethought might be a moral obligation to our own species.\\nBostrom’s sole responsibility at Oxford is to direct an organization called the Future of Humanity Institute, which he founded ten years ago, with financial support from James Martin, a futurist and tech millionaire. Bostrom runs the institute as a kind of philosophical radar station: a bunker sending out navigational pulses into the haze of possible futures. Not long ago, an F.H.I. fellow studied the possibility of a “dark fire scenario,” a cosmic event that, he hypothesized, could occur under certain high-energy conditions: everyday matter mutating into dark matter, in a runaway process that could erase most of the known universe. (He concluded that it was highly unlikely.) Discussions at F.H.I. range from conventional philosophic topics, like the nature of compromise, to the optimal structure of space empires—whether a single intergalactic machine intelligence, supported by a vast array of probes, presents a more ethical future than a cosmic imperium housing millions of digital minds.\\nEarlier this year, I visited the institute, which is situated on a winding street in a part of Oxford that is a thousand years old. It takes some work to catch Bostrom at his office. Demand for him on the lecture circuit is high; he travels overseas nearly every month to relay his technological omens in a range of settings, from Google’s headquarters to a Presidential commission in Washington. Even at Oxford, he maintains an idiosyncratic schedule, remaining in the office until two in the morning and returning sometime the next afternoon.\\nI arrived before he did, and waited in a hallway between two conference rooms. A plaque indicated that one of them was the Arkhipov Room, honoring Vasili Arkhipov, a Soviet naval officer. During the Cuban missile crisis, Arkhipov was serving on a submarine in the Caribbean when U.S. destroyers set off depth charges nearby. His captain, unable to establish radio contact with Moscow, feared that the conflict had escalated and ordered a nuclear strike. But Arkhipov dissuaded him, and all-out atomic war was averted. Across the hallway was the Petrov Room, named for another Soviet officer who prevented a global nuclear catastrophe. Bostrom later told me, “They may have saved more lives than most of the statesmen we celebrate on stamps.”\\nThe sense that a vanguard of technical-minded people working in obscurity, at odds with consensus, might save the world from auto-annihilation runs through the atmosphere at F.H.I. like an electrical charge. While waiting for Bostrom, I peered through a row of windows into the Arkh­ipov Room, which looked as though it was used for both meetings and storage; on a bookcase there were boxes containing light bulbs, lampshades, cables, spare mugs. A gaunt philosophy Ph.D. wrapped in a thick knitted cardigan was pacing in front of a whiteboard covered in notation, which he attacked in bursts. After each paroxysm, he paced, hands behind his back, head tilted downward. At one point, he erased a panel of his work. Taking this as an opportunity to interrupt, I asked him what he was doing. “It is a problem involving an aspect of A.I. called ‘planning,’ ” he said. His demeanor radiated irritation. I left him alone.\\nBostrom arrived at 2 p.m. He has a boyish countenance and the lean, vital physique of a yoga instructor—though he could never be mistaken for a yoga instructor. His intensity is too untidily contained, evident in his harried gait on the streets outside his office (he does not drive), in his voracious consumption of audiobooks (played at two or three times the normal speed, to maximize efficiency), and his fastidious guarding against illnesses (he avoids handshakes and wipes down silverware beneath a tablecloth). Bostrom can be stubborn about the placement of an office plant or the choice of a font. But when his arguments are challenged he listens attentively, the mechanics of consideration nearly dis­cernible beneath his skin. Then, calmly, quickly, he dispatches a response, one idea interlocked with another.\\nHe asked if I wanted to go to the market. “You can watch me make my elixir,” he said. For the past year or so, he has been drinking his lunch (another efficiency): a smoothie containing fruits, vegetables, proteins, and fats. Using his elbow, he hit a button that electronically opened the front door. Then we rushed out.\\nBostrom has a reinvented man’s sense of lost time. An only child, he grew up—as Niklas Boström—in Helsingborg, on the southern coast of Sweden. Like many exceptionally bright children, he hated school, and as a teen-ager he developed a listless, romantic persona. In 1989, he wandered into a library and stumbled onto an anthology of nineteenth-century German philosophy, containing works by Nietzsche and Schopenhauer. He read it in a nearby forest, in a clearing that he often visited to think and to write poetry, and experienced a euphoric insight into the possibilities of learning and achievement. “It’s hard to convey in words what that was like,” Bostrom told me; instead he sent me a photograph of an oil painting that he had made shortly afterward. It was a semi-representational landscape, with strange figures crammed into dense undergrowth; beyond, a hawk soared below a radiant sun. He titled it “The First Day.”\\nDeciding that he had squandered his early life, he threw himself into a campaign of self-education. He ran down the citations in the anthology, branching out into art, literature, science. He says that he was motivated not only by curiosity but also by a desire for actionable knowledge about how to live. To his parents’ dismay, Bostrom insisted on finishing his final year of high school from home by taking special exams, which he completed in ten weeks. He grew distant from old friends: “I became quite fanatical and felt quite isolated for a period of time.”\\nWhen Bostrom was a graduate student in Stockholm, he studied the work of the analytic philosopher W. V. Quine, who had explored the difficult relationship between language and reality. His adviser drilled precision into him by scribbling “not clear” throughout the margins of his papers. “It was basically his only feedback,” Bostrom told me. “The effect was still, I think, beneficial.” His previous academic interests had ranged from psychology to mathematics; now he took up theoretical physics. He was fascinated by technology. The World Wide Web was just emerging, and he began to sense that the heroic philosophy which had inspired him might be outmoded. In 1995, Bostrom wrote a poem, “Requiem,” which he told me was “a signing-off letter to an earlier self.” It was in Swedish, so he offered me a synopsis: “I describe a brave general who has overslept and finds his troops have left the encampment. He rides off to catch up with them, pushing his horse to the limit. Then he hears the thunder of a modern jet plane streaking past him across the sky, and he realizes that he is obsolete, and that courage and spiritual nobility are no match for machines.”\\nAlthough Bostrom did not know it, a growing number of people around the world shared his intuition that technology could cause transformative change, and they were finding one another in an online discussion group administered by an organization in California called the Extropy Institute. The term “extropy,” coined in 1967, is generally used to describe life’s capacity to reverse the spread of entropy across space and time. Extropianism is a libertarian strain of transhumanism that seeks “to direct human evolution,” hoping to eliminate disease, suffering, even death; the means might be genetic modification, or as yet un­invented nanotechnology, or perhaps dispensing with the body entirely and uploading minds into supercomputers. (As one member noted, “Immortality is mathematical, not mystical.”) The Extropians advocated the development of artificial superintelligence to achieve these goals, and they envisioned humanity colonizing the universe, converting inert matter into engines of civilization. The discussions were nerdy, lunatic, imaginative, thought-provoking. Anders Sandberg, a former member of the group who now works at Bostrom’s institute, told me, “Just imagine if you could listen in on the debates of the Italian Futurists or early Surrealists.”\\nIn 1996, while pursuing further graduate work at the London School of Economics, Bostrom learned about the Extropy discussion group and became an active participant. A year later, he co-founded his own organization, the World Transhumanist Association, which was less libertarian and more academically spirited. He crafted approachable statements on transhumanist values and gave interviews to the BBC. The line between his academic work and his activism blurred: his Ph.D. dissertation centered on a study of the Doomsday Argument, which uses probability theory to make inferences about the longevity of human civilization. The work baffled his advisers, who respected him but rarely agreed with his conclusions. Mostly, they left him alone.\\nBostrom had little interest in conventional philosophy—not least because he expected that superintelligent minds, whether biologically enhanced or digital, would make it obsolete. “Suppose you had to build a new subway line, and it was this grand trans-generational enterprise that humanity was engaged in, and everybody had a little role,” he told me. “So you have a little shovel. But if you know that a giant bulldozer will arrive on the scene tomorrow, then does it really make sense to spend your time today digging the big hole with your shovel? Maybe there is something else you could do with your time. Maybe you could put up a signpost for the great shovel, so it will start digging in the right place.” He came to believe that a key role of the philosopher in modern society was to acquire the knowledge of a polymath, then use it to help guide humanity to its next phase of existence—a discipline that he called “the philosophy of technological prediction.” He was trying to become such a seer.\\n“He was ultra-consistent,” Daniel Hill, a British philosopher who befriended Bostrom while they were graduate students in London, told me. “His interest in science was a natural outgrowing of his understandable desire to live forever, basically.”\\nBostrom has written more than a hundred articles, and his longing for immortality can be seen throughout. In 2008, he framed an essay as a call to action from a future utopia. “Death is not one but a multitude of assassins,” he warned. “Take aim at the causes of early death—infection, violence, malnutrition, heart attack, cancer. Turn your biggest gun on aging, and fire. You must seize the biochemical processes in your body in order to vanquish, by and by, illness and senescence. In time, you will discover ways to move your mind to more durable media.” He tends to see the mind as immaculate code, the body as inefficient hardware—able to accommodate limited hacks but probably destined for replacement.\\nEven Bostrom’s marriage is largely mediated by technology. His wife, Susan, has a Ph.D. in the sociology of medicine and a bright, down-to-earth manner. (“She teases me about the Terminator and the robot army,” he told me.) They met thirteen years ago, and for all but six months they have lived on opposite sides of the Atlantic, even after the recent birth of their son. The arrangement is voluntary: she prefers Montreal; his work keeps him at Oxford. They Skype several times a day, and he directs as much international travel as possible through Canada, so they can meet in non-digital form.\\nIn Oxford, as Bostrom shopped for his smoothie, he pointed out a man vaping. “There is also the more old-school method of taking nicotine: chewing gum,” he told me. “I do chew nicotine gum. I read a few papers saying it might have some nootropic effect”—that is, it might enhance cognition. He drinks coffee, and usually abstains from alcohol. He briefly experimented with the smart drug Modafinil, but gave it up.\\nBack at the institute, he filled an industrial blender with lettuce, carrots, cauliflower, broccoli, blueberries, turmeric, vanilla, oat milk, and whey powder. “If there is one thing Nick cares about, it is minds,” Sandberg told me. “That is at the root of many of his views about food, because he is worried that toxin X or Y might be bad for his brain.” He suspects that Bostrom also enjoys the ritualistic display. “Swedes are known for their smugness,” he joked. “Perhaps Nick is subsisting on smugness.”\\nA young employee eyed Bostrom getting ready to fire up the blender. “I can tell when Nick comes into the office,” he said. “My hair starts shaking.”\\n“Yeah, this has got three horsepower,” Bostrom said. He ran the blender, producing a noise like a circular saw, and then filled a tall glass stein with purple-­green liquid. We headed to his office, which was meticulous. By a window was a wooden desk supporting an iMac and not another item; against a wall were a chair and a cabinet with a stack of documents. The only hint of excess was light: there were fourteen lamps.\\nIt is hard to spend time at Bostrom’s institute without drifting into reveries of a far future. What might humanity look like millions of years from now? The upper limit of survival on Earth is fixed to the life span of the sun, which in five billion years will become a red giant and swell to more than two hundred times its present size. It is possible that Earth’s orbit will adjust, but more likely that the planet will be destroyed. In any case, long before then, nearly all plant life will die, the oceans will boil, and the Earth’s crust will heat to a thousand degrees. In half a billion years, the planet will be uninhabitable.\\nThe view of the future from Bostrom’s office can be divided into three grand panoramas. In one, humanity experiences an evolutionary leap—either assisted by technology or by merging into it and becoming software—to achieve a sublime condition that Bostrom calls “posthumanity.” Death is overcome, mental experience expands beyond recognition, and our descendants colonize the universe. In another panorama, humanity becomes extinct or experiences a disaster so great that it is unable to recover. Between these extremes, Bostrom envisions scenarios that resemble the status quo—people living as they do now, forever mired in the “human era.” It’s a vision familiar to fans of sci-fi: on “Star Trek,” Captain Kirk was born in the year 2233, but when an alien portal hurls him through time and space to Depression-era Manhattan he blends in easily.\\nBostrom dislikes science fiction. “I’ve never been keen on stories that just try to present ‘wow’ ideas—the equivalent of movie productions that rely on stunts and explosions to hold the attention,” he told me. “The question is not whether we can think of something radical or extreme but whether we can discover some sufficient reason for updating our credence function.”\\nHe believes that the future can be studied with the same meticulousness as the past, even if the conclusions are far less firm. “It may be highly unpredictable where a traveller will be one hour after the start of her journey, yet predictable that after five hours she will be at her destination,” he once argued. “The very long-term future of humanity may be relatively easy to predict.” He offers an example: if history were reset, the industrial revolution might occur at a different time, or in a different place, or perhaps not at all, with innovation instead occurring in increments over hundreds of years. In the short term, predicting technological achievements in the counter-history might not be possible; but after, say, a hundred thousand years it is easier to imagine that all the same inventions would have emerged.\\nBostrom calls this the Technological Completion Conjecture: “If scientific- and technological-development efforts do not effectively cease, then all impor­t­­­ant basic capabilities that could be obtained through some possible technology will be obtained.” In light of this, he suspects that the farther into the future one looks the less likely it seems that life will continue as it is. He favors the far ends of possibility: humanity becomes transcendent or it perishes.\\nIn the nineteen-nineties, as these ideas crystallized in his thinking, Bostrom began to give more attention to the question of extinction. He did not believe that doomsday was imminent. His interest was in risk, like an insurance agent’s. No matter how improbable extinction may be, Bostrom argues, its consequences are near-infinitely bad; thus, even the tiniest step toward reducing the chance that it will happen is near-­infinitely valuable. At times, he uses arithmetical sketches to illustrate this point. Imagining one of his utopian scenarios—trillions of digital minds thriving across the cosmos—he reasons that, if there is even a one-per-cent chance of this happening, the expected value of reducing an existential threat by a billionth of a billionth of one per cent would be worth a hundred billion times the value of a billion present-day lives. Put more simply: he believes that his work could dwarf the moral importance of anything else.\\nBostrom introduced the philosophical concept of “existential risk” in 2002, in the Journal of Evolution and Technology. In recent years, new organizations have been founded almost annually to help reduce it—among them the Centre for the Study of Existential Risk, affiliated with Cambridge Uni­versity, and the Future of Life Institute, which has ties to the Massachusetts Institute of Technology. All of them face a key problem: Homo sapiens, since its emergence two hundred thousand years ago, has proved to be remarkably resilient, and figuring out what might imperil its existence is not obvious. Climate change is likely to cause vast environmental and economic damage—but it does not seem impossible to survive. So-called super-volcanoes have thus far not threatened the perpetuation of the species. NASA spends forty million dollars each year to determine if there are significant comets or asteroids headed for Earth. (There aren’t.)\\nBostrom does not find the lack of obvious existential threats comforting. Because it is impossible to endure extinction twice, he argues, we cannot rely on history to calculate the probability that it will occur. The most worrying dangers are those that Earth has never encountered before. “It is hard to cause human extinction with seventeenth-century technology,” Bostrom told me. Three centuries later, though, the prospect of a technological apocalypse was urgently plausible. Bostrom dates the first scientific analysis of existential risk to the Manhattan Project: in 1942, Robert Oppenheimer became concerned that an atomic detonation of sufficient power could cause the entire atmosphere to ignite. A subsequent study concluded that the scenario was “unreasonable,” given the limitations of the weapons then in development. But even if the great nuclear nightmares of the Cold War did not come true, the tools were there to cause destruction on a scale not previously possible. As innovations grow even more complex, it is increasingly difficult to evaluate the dangers ahead. The answers must be fraught with ambiguity, because they can be derived only by predicting the effects of technologies that exist mostly as theories or, even more indirectly, by using abstract reasoning.\\nAs a philosopher, Bostrom takes a sweeping, even cosmic, view of such problems. One afternoon, he told me, “The probabilities that any given planet will produce intelligent life—this may also have action-relevant information.” In the past several years, NASA probes have found increasing evidence that the building blocks of life are abundant throughout space. So much water has been discovered—on Mars and on the moons of Jupiter and Saturn—that one scientist described our solar system as “a pretty soggy place.” There are amino acids on icy comets and complex organic molecules in distant star-forming clouds. On this planet, life has proved capable of thriving in unimaginably punishing conditions: without oxygen, without light, at four hundred degrees above or below zero. In 2007, the European Space Agency hitched tiny creatures to the exterior of a satellite. They not only survived the flight; some even laid eggs afterward.\\nWith ten billion Earth-like planets in our galaxy alone, and a hundred billion galaxies in the universe, there is good reason to suspect that extraterrestrial life may one day be discovered. For Bostrom, this would augur disaster. “It would be great news to find that Mars is a completely sterile planet,” he argued not long ago. “Dead rocks and lifeless sands would lift my spirits.” His reasoning begins with the age of the universe. Many of those Earth-like planets are thought to be far, far older than ours. One that was recently discovered, called Kepler 452b, is as much as one and a half billion years older. Bostrom asks: If life had formed there on a time scale resembling our own, what would it look like? What kind of technological progress could a civilization achieve with a head start of hundreds of millions of years?\\nLife as we know it tends to spread wherever it can, and Bostrom estimates that, if an alien civilization could design space probes capable of travelling at even one per cent of the speed of light, the entire Milky Way could be colonized in twenty million years—a tiny fraction of the age difference between Kepler 452b and Earth. One could argue that no technology will ever propel ships at so great a speed. Or perhaps millions of alien civilizations possess the know-how for intergalactic travel, but they aren’t interested. Even so, because the universe is so colossal, and because it is so old, only a small number of civilizations would need to behave as life does on Earth—unceasingly expanding—in order to be visible. Yet, as Bostrom notes, “You start with billions and billions of potential germination points for life, and you end up with a sum total of zero alien civilizations that developed technologically to the point where they become manifest to us earthly observers. So what’s stopping them?”\\nIn 1950, Enrico Fermi sketched a version of this paradox during a lunch break while he was working on the H-bomb, at Los Alamos. Since then, many resolutions have been proposed—some of them exotic, such as the idea that Earth is housed in an interplanetary alien zoo. Bostrom suspects that the answer is simple: space appears to be devoid of life because it is. This implies that intelligent life on Earth is an astronomically rare accident. But, if so, when did that accident occur? Was it in the first chemical reactions in the primordial soup? Or when single-celled organisms began to replicate using DNA? Or when animals learned to use tools? Bos­trom likes to think of these hurdles as Great Filters: key phases of improbability that life everywhere must pass through in order to develop into intelligent species. Those which do not make it either go extinct or fail to evolve.\\nThus, for Bostrom, the discovery of a single-celled creature inhabiting a damp stretch of Martian soil would constitute a disconcerting piece of evidence. If two planets independently evolved primitive organisms, then it seems more likely that this type of life can be found on many planets throughout the universe. Bostrom reasons that this would suggest that the Great Filter comes at some later evolutionary stage. The discovery of a fossilized vertebrate would be even worse: it would suggest that the universe appears lifeless not because complex life is unusual but, rather, because it is always somehow thwarted before it becomes advanced enough to colonize space.\\nIn Bostrom’s view, the most distressing possibility is that the Great Filter is ahead of us—that evolution frequently achieves civilizations like our own, but they perish before reaching their technological maturity. Why might that be? “Natural disasters such as asteroid hits and super-­volcanic eruptions are unlikely Great Filter candidates, because, even if they destroyed a significant number of civilizations, we would expect some civilizations to get lucky and escape disaster,” he argues. “Perhaps the most likely type of existential risks that could constitute a Great Filter are those that arise from technological discovery. It is not far-fetched to suppose that there might be some possible technology which is such that (a) virtually all suffi­ciently advanced civilizations eventually discover it and (b) its discovery leads almost universally to existential disaster.”\\nII. The Machines\\nThe field of artificial intelligence was born in a fit of scientific optimism, in 1955, when a small group of researchers—three mathematicians and an I.B.M. programmer—drew up a proposal for a project at Dartmouth. “An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves,” they stated. “We think a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.”\\nTheir optimism was understandable. Since the turn of the twentieth century, science had been advancing at a breakneck pace: the discovery of radioactivity quickly led to insights into the inner workings of the atom, and then to the development of controlled nuclear energy, and then to the warheads over Hiroshima and Nagasaki, and then to the H-bomb. This rush of discovery was reflected in fiction, too, in the work of Isaac Asimov, among others, who envisioned advanced civilizations inhabited by intelligent robots (each encoded with simple, ethical Laws of Robotics, to prevent it from causing harm). The year the scientists met at Dartmouth, Asimov published “The Last Question,” a story featuring a superintelligent A.I. that is continually “self-adjusting and self-correcting”—gaining knowledge as it helps human civilization expand throughout the universe. When the universe’s last stars start dying out, all humanity uploads itself into the A.I., and the device, achieving godhood, creates a new cosmos.\\nScientists perceived the mechanics of intelligence—like those of the atom—as a source of huge potential, a great frontier. If the brain was merely a biological machine, there was no theoretical reason that it could not be replicated, or even surpassed, much the way a jet could outfly a falcon. Even before the Dartmouth conference, machines exceeded human ability in narrow domains like code-breaking. In 1951, Alan Turing argued that at some point computers would probably exceed the intellectual capacity of their inventors, and that “therefore we should have to expect the machines to take control.” Whether this would be good or bad he did not say.\\nSix years later, Herbert Simon, one of the Dartmouth attendees, declared that machines would achieve human intelligence “in a visible future.” The crossing of such a threshold, he suspected, could be psychologically crushing, but he was on the whole optimistic. “We must also remain sensitive to the need to keep the computer’s goals attuned with our own,” he later said, but added, “I am not convinced that this will be difficult.” For other computer pioneers, the future appeared more ambivalent. Norbert Wiener, the father of cybernetics, argued that it would be difficult to manage powerful computers, or even to accurately predict their behavior. “Complete subservience and complete intelligence do not go together,” he said. Envisioning Sorcerer’s Apprentice scenarios, he predicted, “The future will be an ever more demanding struggle against the limitations of our intelligence, not a comfortable hammock in which we can lie down to be waited upon by our robot slaves.”\\nIt was in this milieu that the “intelligence explosion” idea was first formally expressed by I. J. Good, a statistician who had worked with Turing. “An ultraintelligent machine could design even better machines,” he wrote. “There would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.”\\nThe scientists at Dartmouth recognized that success required answers to fundamental questions: What is intelligence? What is the mind? By 1965, the field had experimented with several models of problem solving: some were based on formal logic; some used heuristic reasoning; some, called “neural networks,” were inspired by the brain. With each, the scientists’ work indicated that A.I. systems could find their own solutions to problems. One algorithm proved numerous theorems in the classic text “Principia Mathematica,” and in one instance it did so more elegantly than the authors. A program designed to play checkers learned to beat its programmer. And yet, despite the great promise in these experiments, the challenges to creating an A.I. were forbidding. Programs that performed well in the laboratory were useless in everyday situations; a simple act like picking up a ball turned out to require an overwhelming number of computations.\\nThe research fell into the first of several “A.I. winters.” As Bostrom notes in his book, “Among academics and their funders, ‘A.I.’ became an unwanted epithet.” Eventually, the researchers started to question the goal of building a mind altogether. Why not try instead to divide the problem into pieces? They began to limit their interests to specific cognitive functions: vision, say, or speech. Even in isolation, these functions would have value: a computer that could identify objects might not be an A.I., but it could help guide a forklift. As the research fragmented, the morass of technical problems made any questions about the consequences of success seem distant, even silly.\\nUnexpectedly, by dismissing its founding goals, the field of A.I. created space for outsiders to imagine more freely what the technology might look like. Bostrom wrote his first paper on artificial superintelligence in the nineteen-nineties, envisioning it as potentially perilous but irresistible to both commerce and government. “If there is a way of guaranteeing that superior artificial intellects will never harm human beings, then such intellects will be created,” he argued. “If there is no way to have such a guarantee, then they will probably be created nevertheless.” His audience at the time was primarily other transhumanists. But the movement was maturing. In 2005, an organization called the Singularity Institute for Artificial Intelligence began to operate out of Silicon Valley; its primary founder, a former member of the Extropian discussion group, published a stream of literature on the dangers of A.I. That same year, the futurist and inventor Ray Kurzweil wrote “The Singularity Is Near,” a best-seller that prophesied a merging of man and machine in the foreseeable future. Bostrom created his institute at Oxford.\\nThe two communities could not have been more different. The scientists, steeped in technical detail, were preoccupied with making devices that worked; the transhumanists, motivated by the hope of a utopian future, were asking, What would the ultimate impact of those devices be? In 2007, the Association for the Advancement of Artificial Intelligence—the most prominent professional organization for A.I. researchers—elected Eric Horvitz, a scientist from Microsoft, as its president. Until then, it had given virtually no attention to the ethical and social implications of the research, but Horvitz was open to the big questions. “It is hard to understand what success would mean for A.I.,” he told me. “I was friendly with Jack Good, who wrote that piece on superintelligence. I knew him as a creative, funny guy who referred to a lot of his ideas as P.B.I.s—partly baked ideas. And here is this piece of his being opened up outside the field as this Bible and studied with a silver pointer. Wouldn’t it be useful, I said, even if you thought these were crazy or low-probability scenarios, to find out: Can we be proactive, should there be some poor outcome for humanity?”\\nHorvitz organized a meeting at the Asilomar Conference Grounds, in California, a place chosen for its symbolic value: biologists had gathered there in 1975 to discuss the hazards of their research in the age of modern genetics. He divided the researchers into groups. One studied short-term ramifications, like the possible use of A.I. to commit crimes; another considered long-term consequences. Mostly, there was skepticism about the intelligence-explosion idea, which assumed answers to many unresolved questions. No one fully understands what intelligence is, let alone how it might evolve in a machine. Can it grow as Good imagined, gaining I.Q. points like a rocketing stock price? If so, what would its upper limit be? And would its increase be merely a function of optimized software design, without the difficult process of acquiring knowledge through experience? Can software fundamentally rewrite itself without risking crippling breakdowns? No one knows. In the history of computer science, no programmer has created code that can substantially improve itself.\\nBut the notion of an intelligence explosion was also impossible to disprove. It was theoretically coherent, and it had even been attempted in limited ways. David McAllester, an A.I. researcher at the Toyota Technological Institute, affiliated with the University of Chicago, headed the long-term panel. The idea, he argued, was worth taking seriously. “I am uncomfortable saying that we are ninety-­nine per cent certain that we are safe for fifty years,” he told me. “That feels like hubris to me.” The group concluded that more technical work was needed before an evaluation of the dangers could be made, but it also hinted at a concern among panelists that the gathering was based on “a perception of urgency”—generated largely by the transhumanists—and risked raising unfounded alarm. With A.I. seeming like a remote prospect, the researchers declared, attention was better spent on near-term concerns. Bart Selman, a professor at Cornell who co-­organized the panel, told me, “The mode was ‘This is interesting, but it’s all academic—it’s not going to happen.’ ”\\nAt the time the A.I researchers met at Asilomar, Bostrom was grappling with an expansive book on existential risks. He had sketched out chapters on bioengineering and on nanotechnology, among other topics, but many of these problems came to seem less compelling, while his chapter on A.I. grew and grew. Eventually, he pasted the A.I. chapter into a new file, which became “Superintelligence.”\\nThe book is its own elegant paradox: analytical in tone and often lucidly argued, yet punctuated by moments of messianic urgency. Some portions are so extravagantly speculative that it is hard to take them seriously. (“Suppose we could somehow establish that a certain future AI will have an IQ of 6,455: then what?”) But Bostrom is aware of the limits to his type of futurology. When he was a graduate student in London, thinking about how to maximize his ability to communicate, he pursued stand­­up comedy; he has a deadpan sense of humor, which can be found lightly buried among the book’s self-serious passages. “Many of the points made in this book are probably wrong,” he writes, with an endnote that leads to the line “I don’t know which ones.”\\nBostrom prefers to act as a cartographer rather than a polemicist, but beneath his exhaustive mapping of scenarios one can sense an argument being built and perhaps a fear of being forthright about it. “Traditionally, this topic domain has been occupied by cranks,” he told me. “By popular media, by science fiction—or maybe by a retired physicist no longer able to do serious work, so he will write a popular book and pontificate. That is kind of the level of rigor that is the baseline. I think that a lot of reasons why there has not been more serious work in this area is that academics don’t want to be conflated with flaky, crackpot type of things. Futurists are a certain type.”\\nThe book begins with an “unfinished” fable about a flock of sparrows that decide to raise an owl to protect and advise them. They go looking for an owl egg to steal and bring back to their tree, but, because they believe their search will be so difficult, they postpone studying how to domesticate owls until they succeed. Bostrom concludes, “It is not known how the story ends.”\\nThe parable is his way of introducing the book’s core question: Will an A.I., if realized, use its vast capability in a way that is beyond human control? One way to think about the concern is to begin with the familiar. Bos­trom writes, “Artificial intelligence already outperforms human intelligence in many domains.” The examples range from chess to Scrabble. One program from 1981, called Eurisko, was designed to teach itself a naval role-playing game. After playing ten thousand matches, it arrived at a morally grotesque strategy: to field thousands of small, immobile ships, the vast majority of which were intended as cannon fodder. In a national tournament, Eurisko demolished its human opponents, who insisted that the game’s rules be changed. The following year, Eurisko won again—by forcing its damaged ships to sink themselves.\\nThe program was by no means superintelligent. But Bostrom’s book essentially asks: What if it were? Assume that it has a broad ability to consider problems and that it has access to the Internet. It could read and acquire general knowledge and communicate with people seamlessly online. It could conduct experiments, either virtually or by tinkering with networked infrastructure. Given even the most benign objective—to win a game—such a system, Bostrom argues, might develop “instrumental goals”: gather resources, or invent technology, or take steps to insure that it cannot be turned off, in the process paying as much heed to human life as humans do to ants.\\nIn people, intelligence is inseparable from consciousness, emotional and social awareness, the complex interaction of mind and body. An A.I. need not have any such attributes. Bostrom believes that machine intelligences—no matter how flexible in their tactics—will likely be rigidly fixated on their ultimate goals. How, then, to create a machine that respects the nuances of social cues? That adheres to ethical norms, even at the expense of its goals? No one has a coherent solution. It is hard enough to reliably inculcate such behavior in people.\\nIn science fiction, superintelligent computers that run amok are often circumvented at the last minute; think of WOPR, the computer in “WarGames,” which was stopped just short of triggering nuclear war, or HAL 9000, which was reduced to helplessly singing while it watched itself get dismantled. For Bos­trom, this strains credulity. Whether out of a desire to consider the far ends of risk or out of transhumanist longings, he often ascribes nearly divine abilities to machines, as if to ask: Can a digital god really be contained? He imagines machines so intelligent that merely by inspecting their own code they can extrapolate the nature of the universe and of human society, and in this way outsmart any effort to contain them. “Is it possible to build machines that are not like agents—goal-pursuing, autonomous, artificial intelligences?” he asked me. “Maybe you can design something more like an oracle that can only answer yes or no. Would that be safer? It is not so clear. There might be agent-like processes within it.” Asking a simple question—“Is it possible to convert a DeLorean into a time machine and travel to 1955?”—might trigger a cascade of action as the device tests hypotheses. What if, working through a police computer, it impounds a DeLorean that happens to be convenient to a clock tower? “In fairy tales, you have genies who grant wishes,” Bostrom said. “Almost universally, the moral of those is that if you are not extremely careful what you wish for, then what seems like it should be a great blessing turns out to be a curse.”\\nBostrom worries that solving the “control problem”—insuring that a superintelligent machine does what humans want it to do—will require more time than solving A.I. does. The intelligence explosion is not the only way that a superintelligence might be created suddenly. Bostrom once sketched out a decades-long process, in which researchers arduously improved their systems to equal the intelligence of a mouse, then a chimp, then—after incredible labor—the village idiot. “The difference between village idiot and genius-­level intelligence might be trivial from the point of view of how hard it is to replicate the same functionality in a machine,” he said. “The brain of the village idiot and the brain of a scientific genius are almost identical. So we might very well see relatively slow and incremental progress that doesn’t really raise any alarm bells until we are just one step away from something that is radically superintelligent.”\\nTo a large degree, Bostrom’s concerns turn on a simple question of timing: Can breakthroughs be predicted? “It is ridiculous to talk about such things so early—A.I. is eons away,” Edward Feigenbaum, an emeritus professor at Stanford University, told me. The researcher Oren Etzioni, who used the term “Frankenstein complex” to dismiss the “dystopian vision of A.I.,” concedes Bostrom’s overarching point: that the field must one day confront profound philosophical questions. Decades ago, he explored them himself, in a brief paper, but concluded that the problem was too remote to think about productively. “Once, Nick Bostrom gave a talk, and I gave a little counterpoint,” he told me. “A lot of the disagreements come down to what time scale you are thinking about. Nobody responsible would say you will see anything remotely like A.I. in the next five to ten years. And I think most computer scientists would say, ‘In a million years—we don’t see why it shouldn’t happen.’ So now the question is: What is the rate of progress? There are a lot of people who will ask: Is it possible we are wrong? Yes. I am not going to rule it out. I am going to say, ‘I am a scientist. Show me the evidence.’ ”\\nThe history of science is an uneven guide to the question: How close are we? There has been no shortage of unfulfilled promises. But there are also plenty of examples of startling nearsightedness, a pattern that Arthur C. Clarke enshrined as Clarke’s First Law: “When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong.” After the electron was discovered, at Cambridge, in 1897, physicists at an annual dinner toasted, “To the electron: may it never be of use to anybody.” Lord Kelvin famously declared, just eight years before the Wright brothers launched from Kitty Hawk, that heavier-than-air flight was impossible.\\nStuart Russell, the co-author of the textbook “Artificial Intelligence: A Modern Approach” and one of Bostrom’s most vocal supporters in A.I., told me that he had been studying the physics community during the advent of nuclear weapons. At the turn of the twentieth century, Ernest Rutherford discovered that heavy elements produced radiation by atomic decay, confirming that vast reservoirs of energy were stored in the atom. Rutherford believed that the energy could not be harnessed, and in 1933 he proclaimed, “Anyone who expects a source of power from the transformation of these atoms is talking moonshine.” The next day, a former student of Einstein’s named Leo Szilard read the comment in the papers. Irritated, he took a walk, and the idea of a nuclear chain reaction occurred to him. He visited Rutherford to discuss it, but Rutherford threw him out. Einstein, too, was skeptical about nuclear energy—splitting atoms at will, he said, was “like shooting birds in the dark in a country where there are only a few birds.” A decade later, Szilard’s insight was used to build the bomb.\\nRussell now relays the story to A.I. researchers as a cautionary tale. “There will have to be more breakthroughs to get to A.I., but, as Szilard illustrated, those can happen overnight,” he told me. “People are putting billions of dollars into achieving those breakthroughs. As the debate stands, Bostrom and others have said, ‘If we achieve superintelligence, here are some of the problems that might arise.’ As far as I know, no one has proved why those are not real.”\\nIII. Mission Control\\nThe offices of the Future of Humanity Institute have a hybrid atmosphere: part physics lab, part college dorm room. There are whiteboards covered with mathematical notation and technical glyphs; there are posters of “Brave New World” and HAL 9000. There is also art work by Nick Bostrom. One afternoon, he guided me to one of his pieces, “At Sea,” a digital collage that he had printed out and then drawn on. “It is a bit damaged, but the good thing about digital is that you can re-instantiate it,” he said. At the center was a pale man, nearly an apparition, clinging to a barrel in an inky-black ocean. “It is an existentialist vibe. You are hanging on for as long as you can. When you get tired, you sink, and become fish food—or maybe a current will take him to land. We don’t know.”\\nDespite the time he spends going to conferences and raising money, Bostrom attends to many details at the institute. “We needed a logo when we started,” he told me. “We went to this online site where you could buy the work of freelance artists. If you sat down and tried to make the ugliest logo, you couldn’t come close. Then we hired a designer, who made a blurry figure of a person. We showed it to someone here, who said it looked like a toilet sign. As soon as she said it, I thought, Oh, my God, we almost adopted a toilet sign as our logo. So I mucked around a bit and came up with a black diamond. You have the black monolith from ‘2001.’ Standing on its corner, it indicates instability. Also, there is a limit to how ugly a black square can be.”\\nThe institute shares office space with the Centre for Effective Altruism, and both organizations intersect with a social movement that promotes pure rationality as a guide to moral action. Toby Ord, a philosopher who works with both, told me that Bostrom often pops into his office at the end of the day, poses a problem, then leaves him pondering it for the night. Among the first of Bostrom’s questions was this: If the universe turns out to contain an infinite number of beings, then how could any single person’s action affect the cosmic balance of suffering and happiness? After lengthy discussions, they left the paradox unresolved. “My main thinking is that we can sort it out later,” Ord told me.\\nWhen I asked Bostrom if I could observe a discussion at the institute, he seemed reluctant; it was hard to judge whether he was concerned that my presence would interfere or that unfiltered talk of, say, engineered pathogens might inspire criminals. (“At some point, one gets into the realm of information hazard,” he hinted.) Eventually, he let me observe a session in the Petrov Room involving half a dozen staff members. The key question under discussion was whether a global catastrophe, on the order of a continent-wide famine, could trigger a series of geopolitical events that would result in human extinction—and whether that meant that a merely catastrophic risk could therefore be taken as seriously as an existential risk. Bostrom, wearing a gray hoodie over a blue button-­down, organized the problem on a whiteboard with visible pleasure. Anders Sandberg told me that he once spent days with Bostrom working through such a problem, distilling a complex argument to its essence. “He had to refine it,” he said. “We had a lot of schemes on the whiteboard that gradually were simplified to one box and three arrows.”\\nFor anyone in the business of publicizing existential risk, 2015 began as a good year. Other institutes devoted to these issues had started to find their voice, bringing an additional gloss of respectability to the ideas in Bostrom’s book. The people weighing in now were no longer just former Extropians. They were credentialled, like Lord Martin Rees, an astrophysicist and the co-founder of Cambridge’s Centre for the Study of Existential Risk. In January, he wrote of A.I., in the Evening Standard, “We don’t know where the boundary lies between what may happen and what will remain science fiction.”\\nRees’s counterpart at the Future of Life Institute, the M.I.T. physicist Max Tegmark, hosted a closed-door meeting in Puerto Rico, to try to make sense of the long-term trajectory of the research. Bostrom flew down, joining a mix of A.I. practitioners, legal scholars, and, for lack of a better term, members of the “A.I. safety” community. “These are not people who are usually in the same room,” Tegmark told me. “Someone advised me to put Valium in people’s drinks so nobody got into fistfights. But, by the time Nick’s session started, people were ready to listen to each other.” Questions that had seemed fanciful to researchers only seven years earlier were beginning to look as though they might be worth reconsidering. Whereas the Asilomar meeting concluded on a note of skepticism about the validity of the whole endeavor, the Puerto Rico conference resulted in an open letter, signed by many prominent researchers, that called for more research to insure that A.I. would be “robust and beneficial.”\\nBetween the two conferences, the field had experienced a revolution, built on an approach called deep learning—a type of neural network that can discern complex patterns in huge quantities of data. For de­c­ades, researchers, hampered by the limits of their hardware, struggled to get the technique to work well. But, beginning in 2010, the increasing availability of Big Data and cheap, powerful video-­game processors had a dramatic effect on performance. Without any profound theoretical breakthrough, deep learning suddenly offered breathtaking advances. “I have been talking to quite a few contemporaries,” Stuart Russell told me. “Pretty much everyone sees examples of progress they just didn’t expect.” He cited a YouTube clip of a four-legged robot: one of its designers tries to kick it over, but it quickly regains its balance, scrambling with uncanny naturalness. “A problem that had been viewed as very difficult, where progress was slow and incremental, was all of a sudden done. Locomotion: done.”\\nIn an array of fields—speech processing, face recognition, language translation—the approach was ascendant. Researchers working on computer vision had spent years to get systems to identify objects. In almost no time, the deep-learning networks crushed their records. In one common test, using a database called ImageNet, humans identify photographs with a five-per-cent error rate; Google’s network operates at 4.8 per cent. A.I. systems can differentiate a Pembroke Welsh Corgi from a Cardigan Welsh Corgi.\\nLast October, Tomaso Poggio, an M.I.T. researcher, gave a skeptical interview. “The ability to describe the content of an image would be one of the most intellectually challenging things of all for a machine to do,” he said. “We will need another cycle of basic research to solve this kind of question.” The cycle, he predicted, would take at least twenty years. A month later, Google announced that its deep-learning network could analyze an image and offer a caption of what it saw: “Two pizzas sitting on top of a stove top,” or “People shopping at an outdoor market.” When I asked Poggio about the results, he dismissed them as automatic associations between objects and language; the system did not understand what it saw. “Maybe human intelligence is the same thing, in which case I am wrong, or not, in which case I was right,” he told me. “How do you decide?”\\nA respected minority of A.I. researchers began to wonder: If increasingly powerful hardware could facilitate the deep-learning revolution, would it make other long-shelved A.I. principles viable? “Suppose the brain is just a million different evolutionarily developed hacks: one for smell, one for recognizing faces, one for how you recognize animals,” Tom Mitchell, who holds a chair in machine learning at Carnegie Mellon, told me. “If that is what underlies intelligence, then I think we are far, far from getting there—because we don’t have many of those hacks. On the other hand, suppose that what underlies intelligence are twenty-three general mechanisms, and when you put them together you get synergy, and it works. We now have systems that can do a pretty good job with computer vision—and it turns out that we didn’t have to construct a million hacks. So part of the uncertainty is: if we do not need a million different hacks, then will we find the right twenty-­three fundamental generic methods?” He paused. “I no longer have the feeling, which I had twenty-five years ago, that there are gaping holes. I know we don’t have a good architecture to assemble the ideas, but it is not obvious to me that we are missing components.”\\nBostrom noticed the shift in attitude. He recently conducted a poll of A.I. researchers to gauge their sense of progress, and in Puerto Rico a survey gathered opinions on how long it would be until an artificial intelligence could reason indistinguishably from a human being. Like Bostrom, the engineers are often careful to express their views as probabilities, rather than as facts. Richard Sutton, a Canadian computer scientist whose work has earned tens of thousands of scholarly citations, gives a range of outcomes: there is a ten-per-cent chance that A.I. will never be achieved, but a twenty-five-per-cent chance that it will arrive by 2030. The median response in Bostrom’s poll gives a fifty-fifty chance that human-level A.I. would be attained by 2050. These surveys are unscientific, but he is confident enough to offer an interpretive assumption: “It is not a ridiculous prospect to take seriously the possibility that it can happen in the lifetime of people alive today.”\\nOn my last day in Oxford, I walked with Bostrom across town. He was racing to catch a train to London, to speak at the Royal Society, one of the world’s oldest scientific institutions. His spirits were high. The gulf between the transhumanists and the scientific community was slowly shrinking. Elon Musk had pledged ten million dollars in grants for academics seeking to investigate A.I. safety, and, rather than mock him, researchers applied for the money; Bostrom’s institute was helping to evaluate the proposals. “Right now, there is a lot of interest,” he told me. “But then there were all these long years when nobody else seemed to pay attention at all. I am not sure which is the less abnormal condition.”\\nThere were clear limits to that interest. To publicly stake out a position in the middle of the debate was difficult, not least because of the polarized atmosphere Bostrom’s book had helped to create. Even though a growing number of researchers were beginning to suspect that profound questions loomed, and that they might be worth addressing now, it did not mean that they believed A.I. would lead inevitably to an existential demise or a techno-utopia. Most of them were engaged with more immediate problems: privacy, unemployment, weaponry, driverless cars running amok. When I asked Bostrom about this pragmatic ethical awakening, he reacted with dismay. “My fear is that it would swallow up the concerns for the longer term,” he said. “On the other hand, yes, maybe it is useful to build bridges to these different communities. Kind of makes the issue part of a larger continuum of things to work on.”\\nAt the Royal Society, Bostrom took a seat at the back of a large hall. As he crossed his legs, I noticed a thin leather band around his ankle. A metal buckle was engraved with contact information for Alcor, a cryonics facility in Arizona, where Bostrom is a fee-paying member. Within hours of his death, Alcor will take custody of his body and maintain it in a giant steel bottle flooded with liquid nitrogen, in the hope that one day technology will allow him to be revived, or to have his mind uploaded into a computer. When he signed up, two other colleagues at the institute joined him. “My background is transhumanism,” he once reminded me. “The character of that is gung-ho techno-cheerleading, bring it on now, where are my life-­extension pills.”\\nThe hall was packed with some of the most technically sophisticated researchers in A.I.—not necessarily Bostrom’s people—and when he spoke he began by trying to assure them that his concern was not out of Ludditism. “It would be tragic if machine intelligence were never developed to its full capacity,” he said. “I think this is ultimately the key, or the portal, we have to pass through to realize the full dimension of humanity’s long-term potential.” But, even as he avoided talk of existential risk, he pressed his audience to consider the danger of building an A.I. without regarding its ethical design.\\nAn attendee raised his hand to object. “We can’t control basic computer worms,” he said. “The A.I. that will happen is going to be a highly adaptive, emergent capability, and highly distributed. We will be able to work with it—for it—not necessarily contain it.”\\n“I guess I am a little frustrated,” Bos­trom responded. “People tend to fall into two camps. On one hand, there are those, like yourself, who think it is probably hopeless. The other camp thinks it is easy enough that it will be solved automatically. And both of these have in common the implication that we don’t have to make any effort now.”\\nFor the rest of the day, engineers presented their work at the lectern, each promising a glimpse of the future—robot vision, quantum computers, algorithms called “thought vectors.” Early in Bostrom’s career, he predicted that cascading economic demand for an A.I. would build up across the fields of medicine, entertainment, finance, and defense. As the technology became useful, that demand would only grow. “If you make a one-per-cent improvement to something—say, an algorithm that recommends books on Amazon—there is a lot of value there,” Bostrom told me. “Once every improvement potentially has enormous economic benefit, that promotes effort to make more improvements.”\\nMany of the world’s largest tech companies are now locked in an A.I. arms race, purchasing other companies and opening specialized units to advance the technology. Industry is vacuuming up Ph.D.s so quickly that people in the field worry there will no longer be top talent in academia. After decades of pursuing narrow forms of A.I., researchers are seeking to integrate them into systems that resemble a general intellect. Since I.B.M.’s Watson won “Jeopardy,” the company has committed more than a billion dollars to develop it, and is reorienting its business around “cognitive systems.” One senior I.B.M. executive declared, “The separation between human and machine is going to blur in a very fundamental way.”\\nAt the Royal Society, a contingent of researchers from Google occupied a privileged place; they likely had more resources at their disposal than anyone else in the room. Early on, Google’s founders, Larry Page and Sergey Brin, understood that the company’s mission required solving fundamental A.I. problems. Page has said that he believes the ideal system would understand questions, even anticipate them, and produce responses in conversational language. Google scientists often invoke the computer in “Star Trek” as a model.\\nIn recent years, Google has purchased seven robotics companies and several firms specializing in machine intelligence; it may now employ the world’s largest contingent of Ph.D.s in deep learning. Perhaps the most interesting acquisition is a British company called DeepMind, started in 2011 to build a general artificial intelligence. Its founders had made an early bet on deep learning, and sought to combine it with other A.I. mechanisms in a cohesive architecture. In 2013, they published the results of a test in which their system played seven classic Atari games, with no instruction other than to improve its score. For many people in A.I., the importance of the results was immediately evident. I.B.M.’s chess program had defeated Garry Kasparov, but it could not beat a three-year-old at tic-tac-toe. In six games, DeepMind’s system outperformed all previous algorithms; in three it was superhuman. In a boxing game, it learned to pin down its opponent and subdue him with a barrage of punches.\\nWeeks after the results were released, Google bought the company, reportedly for half a billion dollars. DeepMind placed two unusual conditions on the deal: its work could never be used for espionage or defense purposes, and an ethics board would oversee the research as it drew closer to achieving A.I. Anders Sandberg had told me, “We are happy that they are among the most likely to do it. They recognize there are some problems.”\\nDeepMind’s chief founder, Demis Hassabis, described his company to the audience at the Royal Society as an “Apollo Program” with a two-part mission: “Step one, solve intelligence. Step two, use it to solve everything else.” Since the test in 2013, his system had aced more than a dozen other Atari titles. Hassabis demonstrated an unpublished trial using a three-dimensional driving game, in which it had quickly outperformed the game’s automated drivers. The plan was to test it in increasingly complex virtual environments and, eventually, in the real world. The patent lists a range of uses, from finance to robotics.\\nHassabis was clear about the challenges. DeepMind’s system still fails hopelessly at tasks that require long-range planning, knowledge about the world, or the ability to defer rewards—things that a five-year-old child might be expected to handle. The company is working to give the algorithm conceptual understanding and the capability of transfer learning, which allows humans to apply lessons from one situation to another. These are not easy problems. But DeepMind has more than a hundred Ph.D.s to work on them, and the rewards could be immense. Hassabis spoke of building artificial scientists to resolve climate change, disease, poverty. “Even with the smartest set of humans on the planet working on these problems, these systems might be so complex that it is difficult for individual humans, scientific experts,” he said. “If we can crack what intelligence is, then we can use it to help us solve all these other problems.” He, too, believes that A.I. is a gateway to expanded human potential.\\nThe keynote speaker at the Royal Society was another Google employee: Geoffrey Hinton, who for decades has been a central figure in developing deep learning. As the conference wound down, I spotted him chatting with Bostrom in the middle of a scrum of researchers. Hinton was saying that he did not expect A.I. to be achieved for decades. “No sooner than 2070,” he said. “I am in the camp that is hopeless.”\\n“In that you think it will not be a cause for good?” Bostrom asked.\\n“I think political systems will use it to terrorize people,” Hinton said. Already, he believed, agencies like the N.S.A. were attempting to abuse similar technology.\\n“Then why are you doing the research?” Bostrom asked.\\n“I could give you the usual arguments,” Hinton said. “But the truth is that the prospect of discovery is too sweet.” He smiled awkwardly, the word hanging in the air—an echo of Oppenheimer, who famously said of the bomb, “When you see something that is technically sweet, you go ahead and do it, and you argue about what to do about it only after you have had your technical success.”\\nAs the scientists retreated to tables set up for refreshments, I asked Hinton if he believed an A.I. could be controlled. “That is like asking if a child can control his parents,” he said. “It can happen with a baby and a mother—there is biological hardwiring—but there is not a good track record of less intelligent things controlling things of greater intelligence.” He looked as if he might elaborate. Then a scientist called out, “Let’s all get drinks!”\\nBostrom had little interest in the cocktail party. He shook a few hands, then headed for St. James’s Park, a public garden that extends from the gates of Buckingham Palace through central London. The world appeared in splendorous analog: sunlight over trees, duck ponds, children and grandparents feeding birds. The spot had been a park for hundreds of years, and the vista seemed timeless. Yet, during the past millennium, the grounds had also been a marsh, a leper hospital, a deer sanctuary, and royal gardens. It seemed plausible that, a thousand years from now, digital posthumans, regarding it as wasted space, would tear it up, replace the landscaping with computer banks, and erect a vast virtual idyll.\\nBostrom’s pace settled into its natural quickness as we circled the park. He talked about his family; he would be seeing his wife and son soon. He was reading widely: history, psychology, economics. He was learning to code. He was thinking about expanding his institute. Although he did not know it then, F.H.I. was about to receive one and a half million dollars from Elon Musk, to create a unit that would craft social policies informed by some of Bostrom’s theories. He would need to hire people. He was also giving thought to the framing of his message. “A lot more is said about the risks than the upsides, but that is not necessarily because the upside is not there,” he told me. “There is just more to be said about the risk—and maybe more use in describing the pitfalls, so we know how to steer around them—than spending time now figuring out the details of how we are going to furnish the great palace a thousand years from now.”\\nWe passed a fountain, near a cluster of rocks engineered to give ducks a resting place. Bostrom, in his forties, must soon contend with physical decline, and he spoke with annoyance of the first glimmers of mortality. Even though he is an Alcor member, there is no guarantee that cryonics will work. Perhaps the most radical of his visions is that superintelligent A.I. will hasten the uploading of minds—what he calls “whole-brain emulations”—technology that might not be possible for centuries, if at all. Bostrom, in his most hopeful mode, imagines emulations not only as reproductions of the original intellect “with memory and personality intact”—a soul in the machine—but as minds expandable in countless ways. “We live for seven decades, and we have three-pound lumps of cheesy matter to think with, but to me it is plausible that there could be extremely valuable mental states outside this little particular set of possibilities that might be much better,” he told me.\\nIn his book, Bostrom considers a distant future in which trillions of digital minds merge into an enormous cognitive cyber-soup. “Whether the set of extremely positive posthuman modes of being would include some kind of dissolved bouillon, there is some uncertainty,” he said. “If you look at religious views, there are many where merging with something greater is a form of heaven, being in the presence of this enormous beauty and goodness. In many traditions, the best possible state does not involve being a little individual pursuing goals. But it is hard to get a grasp of what would be going on in that soup. Maybe some soups would not be preferable as a long-term outcome. I don’t know.” He stopped and looked ahead. “What I want to avoid is to think from our parochial 2015 view—from my own limited life experience, my own limited brain—and super-confidentially postulate what is the best form for civilization a billion years from now, when you could have brains the size of planets and billion-year life spans. It seems unlikely that we will figure out some detailed blueprint for utopia. What if the great apes had asked whether they should evolve into Homo sapiens—pros and cons—and they had listed, on the pro side, ‘Oh, we could have a lot of bananas if we became human’? Well, we can have unlimited bananas now, but there is more to the human condition than that.” ♦\\nIllustration by Todd St. John/Coding by Jono Brandel.\",\"isBasedOn\":\"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom\",\"articleSection\":\"a reporter at large\",\"author\":[{\"@type\":\"Person\",\"name\":\"Raffi Khatchadourian\",\"sameAs\":\"https://www.newyorker.com/contributors/raffi-khatchadourian\"}],\"dateModified\":\"2015-11-15T23:00:00.000-05:00\",\"datePublished\":\"2015-11-15T19:00:00.000-05:00\",\"headline\":\"The Philosopher of Doomsday\",\"image\":[\"https://media.newyorker.com/photos/590971f5ebe912338a377328/16:9/w_2560,h_1440,c_limit/151123_r27342.jpg\",\"https://media.newyorker.com/photos/590971f5ebe912338a377328/4:3/w_2560,h_1920,c_limit/151123_r27342.jpg\",\"https://media.newyorker.com/photos/590971f5ebe912338a377328/1:1/w_2000,h_2000,c_limit/151123_r27342.jpg\"],\"keywords\":[\"a reporter at large\",\"artificial intelligence (a.i.)\",\"category_science_tech\",\"philosophers\",\"magazine\"],\"thumbnailUrl\":\"https://media.newyorker.com/photos/590971f5ebe912338a377328/1:1/w_2000,h_2000,c_limit/151123_r27342.jpg\",\"url\":\"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom\",\"isPartOf\":{\"@type\":\"CreativeWork\",\"name\":\"The New Yorker\"},\"isAccessibleForFree\":true,\"alternativeHeadline\":\"Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.\",\"description\":\"Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom\"},\"publisher\":{\"@context\":\"https://schema.org\",\"@type\":\"Organization\",\"name\":\"The New Yorker\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https://www.newyorker.com/verso/static/the-new-yorker/assets/the-new-yorker-seo-logo.jpg\",\"width\":\"1200px\",\"height\":\"630px\"},\"url\":\"https://www.newyorker.com\"}}</script><script type=\"application/ld+json\">{\"@context\":\"https://schema.org/\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Magazine\",\"item\":\"https://www.newyorker.com/magazine\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Artificial Intelligence (A.I.)\",\"item\":\"https://www.newyorker.com/tag/artificial-intelligence-ai\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"The Doomsday Invention\"}]}</script><meta id=\"google-signin-meta\" name=\"google-signin-client_id\" content=\"275906274807-b4eqbdqr511u9msdpj8mh0pf77fcciv7.apps.googleusercontent.com\"/><script id=\"google-api-script\" src=\"https://apis.google.com/js/platform.js\" async=\"\" defer=\"\"></script><script id=\"martech-lib-pre-script\">window.Martech = window.Martech || new Promise((resolve) => {\n  window._mt_init = { resolve };\n});\n\nwindow.Martech.then((martech) => {\n  martech.setConfig({\n    // custom behavior for header based authentication\n    authHeaders: () => ({\n      'Authorization': 'Bearer ' + (\n        martech.util.getCookie('CN_token_id') ||\n        martech.util.getCookie('CN_userAuth')\n      )\n    }),\n  });\n});</script><script id=\"martech-lib-script\" src=\"https://martech.condenastdigital.com/lib/martech.js\" async=\"\" defer=\"\"></script></head><body class=\"stackednavigation-site-navigation fixed-header-large-logo-nav-variation\"><noscript><iframe title=\"Google Tag Manager\" src=\"https://www.googletagmanager.com/ns.html?id=GTM-NX5LSK3\" height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe></noscript><div id=\"app-root\"><div><div class=\"interactive-override-container interactive-override-container--all\"><link rel=\"stylesheet\" href=\"https://projects.newyorker.com/interactive/2015/ai-story/archive/logo.css\" />\n<link rel=\"stylesheet\" href=\"https://projects.newyorker.com/interactive/2015/ai-story/archive/critical.min.css@v=20190418195522.css\" />\n<style>html p.descender::first-letter{line-height:82px;background:url(https://projects.newyorker.com/interactive/2015/ai-story/archive/images/drop-caps/T.jpg) center center;background-size:cover !important;color:transparent;font-size:0;padding:28px 10px;margin:0 20px 0 0}html p.descender.dropcap-A::first-letter{background:url(https://projects.newyorker.com/interactive/2015/ai-story/archive/images/drop-caps/A.jpg) center center}html p.descender.dropcap-B::first-letter{background:url(https://projects.newyorker.com/interactive/2015/ai-story/archive/images/drop-caps/B.jpg) center center}html p.descender.dropcap-I::first-letter{background:url(https://projects.newyorker.com/interactive/2015/ai-story/archive/images/drop-caps/I.jpg) center center;background-size:99% 99% !important}html p.descender.dropcap-L::first-letter{background:url(https://projects.newyorker.com/interactive/2015/ai-story/archive/images/drop-caps/L.jpg) center center}html p.descender.dropcap-T::first-letter{background:url(https://projects.newyorker.com/interactive/2015/ai-story/archive/images/drop-caps/T.jpg) center center}html p.descender.dropcap-O::first-letter{background:url(https://projects.newyorker.com/interactive/2015/ai-story/archive/images/drop-caps/O.jpg) center center}html.reduce-descender-spacing p.descender::first-letter{padding:41px 28px}html.reduce-descender-spacing p.descender.dropcap-I::first-letter{padding:41px 18px}.player-resize{max-width:1040px;width:100%;z-index:999999;position:absolute;margin-bottom:480px}@media (max-width: 420px){.player-resize{margin-bottom:275px}}@media (min-width: 53.125em){.player-resize{margin-bottom:500px}}span.videospacer{display:block;height:625px;width:1px}#featured-ai{background-image:none;background-size:100% auto;width:100%;min-height:600px;-webkit-transition:background-image 2s ease-in;transition:background-image 2s ease-in}#featured-ai.fallback{background-image:url(\"../../../../wp-content/uploads/2015/11/151123_r27342.jpg\")}.false-header{margin:0 !important;max-width:none !important;padding:0 !important;pointer-events:none;position:absolute !important;top:0;width:100%}.false-header #page{background-color:transparent !important;border:none;left:0 !important;opacity:1}.false-header #page #masthead,.false-header #page .social-module{pointer-events:all}.false-header .background-image{display:none}#content #articleBody h2,#content #articleBody h2.aligncenter{font-family:\"Irvin Heading\",\"Helvetica Neue\",Helvetica,Arial,sans-serif;font-weight:normal}figure.alignleft.size-full{padding-bottom:0 !important}\n\n.media-block--hero.crop-center .background-image, .media-block--hero.crop-center-desktop .background-image {\n\tdisplay: none; \n}\n\nbody.article {\n\tpadding-top: 0 !important;\n}\n\n.contributors {\n  border-bottom-width: 0px;\n    border-top-width: 0px;\n    font-style: normal;\n}\n\n#featured-ai {\n\tposition: absolute;\n\tz-index: 0;\n\twidth: 100%;\n\theight: 100vh;\n\tleft: 0;\n\ttop: 0;\n}\n\n#page {\n\tbackground: transparent;\n\tborder: 0;\n}\n  \n.hero-image-caption {\n  margin: 60px auto 0 !important;\n  padding-left: 20px !important;\n  padding-right: 20px !important;\n}\n\n@media (min-width: 600px) {\n  .hero-image-caption {\n    max-width: 1060px;\n    margin: 20px auto 0 !important;\n  }\n}\n\n.media-cne {\n\tmargin-left: auto !important;\n    margin-right: auto !important;\n    max-width: 650px;\n}\n.cne-interlude-container {\n\tdisplay: block;\n}\n\n#content {\n  max-width: 1060px;\n  margin: 0 auto;\n}\n\n</style>\n\n<div class=\"article\">\n\t<div class=\"logo_wrapper\">\n\t\t<div class=\"logo_inner\">\n\t\t\t<a href=\"https://www.newyorker.com/\" target=\"_top\"><img src=\"https://www.newyorker.com/projects/interactive/2019/190211-kaminsky/assets/svg/tny_logo.svg\"></a>\n\t\t</div>\n\t\t<div class=\"subscribe\"><a href=\"https://subscribe.newyorker.com/subscribe/newyorker/\">Subscribe »</a></div>\n\t</div>\n\t<iframe class=\"featured-ai fallback\" id=\"featured-ai\" src=\"http://projects.newyorker.com/interactive/2015/ai-story/featured/i9/index.html\" style=\"height: 850px;\"></iframe>\n\t<article class=\"single-column header-ad-hidden\" data-details=\"{&quot;title&quot;:&quot;The Doomsday Invention&quot;,&quot;author&quot;:[28442,&quot;Raffi Khatchadourian&quot;,&quot;https:\\/\\/www.newyorker.com\\/contributors\\/raffi-khatchadourian&quot;],&quot;wordCount&quot;:12441,&quot;paragraphCount&quot;:92}\" itemscope itemtype=\"http://schema.org/NewsArticle\" itemid=\"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom\">\n\t\t<header id=\"page\" \t\tclass=\"is-ready js-media-block media-block media-block--hero crop-center text-align-x-center text-align-y-center text-color-dark\" data-format=\"full-bleed\">\n\t\t<hgroup id=\"masthead\">\n\n\t\t\t<div class=\"masthead-wrapper\">\n\t\t\t\t\n\t\t\t\t<div class=\"rubric-and-issue-date\">\n\t\t\t\t\t<h4 class=\"rubric\">\n\t\t\t\t\t\t<a href=\"https://www.newyorker.com/magazine/a-reporter-at-large\" title=\"A Reporter at Large\">A Reporter at Large</a>\n\t\t\t\t\t</h4>\n\t\t\t\t\t<a class=\"issue-publish-date-link\" href=\"https://www.newyorker.com/magazine/2015/11/23\" title=\"Published in 2015-11-23\">\n\t\t\t\t\t\t<time class=\"issue\" itemprop=\"datePublished\" content=\"2015-11-16\">November 23, 2015 Issue</time>\n\t\t\t\t\t</a>\n\t\t\t\t\t<meta itemprop=\"dateModified\" content=\"September 7, 2017\" />\n\t\t\t\t</div>\n\t\t\t\t<h1 class=\"title\" itemprop=\"headline\">The Doomsday Invention</h1>\n\t\t\t\t<h2 class=\"dek\" itemprop=\"alternativeHeadline\">Will artificial intelligence bring us utopia or destruction?</h2><div class=\"byline-and-date\">\n\t\t\t\t\t<div style=\"background-image: url(../../../../wp-content/uploads/2014/03/raffi-khatchadourian.jpg)\" class=\"contributor-image\" ></div><h3 class=\"contributors\">By<span itemscope itemprop=\"author\" itemtype=\"http://schema.org/Person\"> <meta itemprop=\"url\" content=\"https://www.newyorker.com/contributors/raffi-khatchadourian\" > <a href=\"https://www.newyorker.com/contributors/raffi-khatchadourian\" title=\"Raffi Khatchadourian\" rel=\"author\" itemprop=\"url\"><span itemprop=\"name\">Raffi Khatchadourian</span></a></span></h3></div>\n\t\t\t\t\t<div class=\"social-share-links js-social-module\">\n\t\t\t\t\t\t<ul class=\"options\">\n\t\t\t\t\t\t\t<li class=\"option facebook js-option-facebook\">\n\n\n\t\t\t\t\t\t\t\t<a class=\"facebook\" href=\"https://www.facebook.com/sharer/sharer.php?u=https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom&display=popup&ref=plugin\" target=\"_blank\">\n\t\t\t\t\t\t\t\t\t<svg width=\"10px\" height=\"18px\" viewBox=\"7 3 10 18\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><path d=\"M13.3165,20.5 L13.3165,12.746 L15.9185,12.746 L16.3085,9.723 L13.3165,9.723 L13.3165,7.794 C13.3165,6.919 13.5585,6.323 14.8145,6.323 L16.4135,6.322 L16.4135,3.618 C16.1375,3.582 15.1875,3.5 14.0825,3.5 C11.7755,3.5 10.1955,4.908 10.1955,7.494 L10.1955,9.723 L7.5865,9.723 L7.5865,12.746 L10.1955,12.746 L10.1955,20.5 L13.3165,20.5 Z\" id=\"Fill-5\" stroke=\"none\" fill=\"#000000\" fill-rule=\"evenodd\"></path></svg>\t\t\t</a>\n\t\t\t\t\t\t\t\t</li>\n\n\t\t\t\t\t\t\t\t<li class=\"option twitter js-option-twitter\">\n\n\n\t\t\t\t\t\t\t\t\t<a class=\"twitter\" href=\"https://twitter.com/intent/tweet?original_referer=https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom&text=The+Doomsday+Invention&tw_p=tweetbutton&url=https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom&via=raffiwriter\" target=\"_blank\">\n\t\t\t\t\t\t\t\t\t\t<svg width=\"18px\" height=\"16px\" viewBox=\"3 4 18 16\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><path d=\"M21,6.4175 C20.338,6.7115 19.626,6.9095 18.879,6.9985 C19.641,6.5415 20.227,5.8185 20.503,4.9555 C19.789,5.3795 18.999,5.6865 18.158,5.8525 C17.484,5.1345 16.524,4.6855 15.462,4.6855 C13.423,4.6855 11.769,6.3395 11.769,8.3785 C11.769,8.6685 11.802,8.9495 11.865,9.2205 C8.796,9.0665 6.074,7.5965 4.253,5.3615 C3.935,5.9075 3.753,6.5415 3.753,7.2185 C3.753,8.4995 4.405,9.6295 5.396,10.2925 C4.791,10.2735 4.221,10.1065 3.723,9.8305 L3.723,9.8765 C3.723,11.6655 4.996,13.1585 6.685,13.4975 C6.375,13.5825 6.049,13.6275 5.712,13.6275 C5.474,13.6275 5.243,13.6045 5.018,13.5615 C5.488,15.0285 6.851,16.0955 8.467,16.1255 C7.203,17.1165 5.611,17.7065 3.881,17.7065 C3.583,17.7065 3.289,17.6895 3,17.6545 C4.634,18.7025 6.575,19.3145 8.661,19.3145 C15.454,19.3145 19.168,13.6865 19.168,8.8065 C19.168,8.6465 19.164,8.4875 19.157,8.3295 C19.879,7.8085 20.505,7.1585 21,6.4175\" id=\"Fill-3\" stroke=\"none\" fill=\"#000000\" fill-rule=\"evenodd\"></path></svg>\t\t\t</a>\n\t\t\t\t\t\t\t\t\t</li>\n\n\t\t\t\t\t\t\t\t\t<li class=\"option email js-option-email\">\n\n\n\t\t\t\t\t\t\t\t\t\t<a class=\"email\" href=\"mailto:?subject=From%20newyorker.com:%20The%20Doomsday%20Invention&amp;body=The%20Doomsday%20Invention%0Ahttps%3A%2F%2Fwww.newyorker.com%2Fmagazine%2F2015%2F11%2F23%2Fdoomsday-invention-artificial-intelligence-nick-bostrom\" target=\"_blank\">\n\t\t\t\t\t\t\t\t\t\t\t<svg width=\"19px\" height=\"15px\" viewBox=\"2 5 19 15\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><path d=\"M12.0778,14.4853 L12.0748,14.4823 L12.0708,14.4853 L4.9998,7.4143 L6.4138,6.0003 L12.0748,11.6603 L17.7348,6.0003 L19.1488,7.4143 L12.0778,14.4853 Z M2.9998,19.0003 L20.9998,19.0003 L20.9998,5.0003 L2.9998,5.0003 L2.9998,19.0003 Z\" id=\"Fill-3\" stroke=\"none\" fill=\"#000000\" fill-rule=\"evenodd\"></path></svg>\t\t\t</a>\n\t\t\t\t\t\t\t\t\t\t</li>\n\n\t\t\t\t\t\t\t\t\t\t<li class=\"option print js-option-print\">\n\n\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"print\" href=\"doomsday-invention-artificial-intelligence-nick-bostrom.html#\" target=\"_blank\">\n\t\t\t\t\t\t\t\t\t\t\t\t<svg width=\"18px\" height=\"16px\" version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\"\t viewBox=\"159.1 247.8 295 294.7\" enable-background=\"new 159.1 247.8 295 294.7\" xml:space=\"preserve\"><g>\t<path fill=\"#000000\" d=\"M232.6,266.2h147.5v37h18.4v-37c0-10.3-8.4-18.4-18.4-18.4H232.6c-10.3,0-18.4,8.4-18.4,18.4v37h18.4V266.2\t\tz\"/>\t<path fill=\"#000000\" d=\"M435.8,321.6H177.5c-10.3,0-18.4,8.4-18.4,18.4v92.1c0,10.3,8.4,18.4,18.4,18.4h37v73.8\t\tc0,10.3,8.4,18.4,18.4,18.4h147.5c10.3,0,18.4-8.4,18.4-18.4v-73.4h37c10.3,0,18.4-8.4,18.4-18.4V340\t\tC454.2,330,445.8,321.6,435.8,321.6z M380.4,524.5H232.6V395.4h147.5v129.2H380.4z M417.1,377c-10.3,0-18.4-8.4-18.4-18.4\t\tc0-10.3,8.4-18.4,18.4-18.4c10.3,0,18.4,8.4,18.4,18.4C435.8,368.6,427.4,377,417.1,377z\"/>\t<rect x=\"251.2\" y=\"414\" fill=\"#000000\" width=\"73.8\" height=\"18.4\"/>\t<rect x=\"251.2\" y=\"450.8\" fill=\"#000000\" width=\"110.8\" height=\"18.4\"/>\t<rect x=\"251.2\" y=\"487.8\" fill=\"#000000\" width=\"110.8\" height=\"18.4\"/></g></svg>\t\t\t</a>\n\t\t\t\t\t\t\t\t\t\t\t</li>\n\n\n\t\t\t\t\t\t\t\t\t\t</ul>\n\t\t\t\t\t\t\t\t\t</div>\n\n\t\t\t\t\t\t\t\t\t<!-- end masthead-wrapper -->\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t</hgroup>\n\t\t\t\t\t\t\t<div class=\"background-image\">\n\t\t\t\t\t\t\t\t<div class=\"image js-background-image\" style=\"background-image: url('https://projects.newyorker.com/interactive/2015/ai-story/archive/images/151123_r27342-1992x2400-1447365389.jpg')\"></div>\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<div class=\"background-image mobile-crop\">\n\n\n\t\t\t\t\t\t\t\t<div class=\"image js-background-image\" style=\"background-image: url('https://projects.newyorker.com/interactive/2015/ai-story/archive/images/151123_r27342-690x831-1447365389.jpg')\"></div>\n\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t</header>\n\n\t\t\t\t\t\t<aside class=\"hero-image-caption\">\n\t\t\t\t\t\t\t<span>Nick Bostrom, a philosopher focussed on A.I. risks, says, “The very long-term future of humanity may be relatively easy to predict.”</span>\n\t\t\t\t\t\t\t<span class=\"hero-image-credit\">Illustration by Todd St. John</span>\n\t\t\t\t\t\t</aside>\n\n\t\t\t\t\t\t<div id=\"content\" >\n\t\t\t\t\t\t\t<div itemprop=\"articleBody\" class=\"articleBody\" id=\"articleBody\">\n\t\t\t\t\t\t\t\t<!-- Barrier Status: '' --><h2 class=\"aligncenter\">I. Omens</h2> <a class=\"tny-slot\" name=\"/2\" data-total-words=\"0\"></a><a class=\"tny-page\" name=\"/1\" data-total-words=\"0\"></a><p class=\"descender dropcap-L\" word_count=\"113\" data-wc=\"113\"> Last year, a curious nonfiction book became a <em>Times</em> best-seller: a dense meditation on artificial intelligence by the philosopher Nick Bostrom, who holds an appointment at Oxford. Titled “Superintelligence: Paths, Dangers, Strategies,” it argues that true artificial intelligence, if it is realized, might pose a danger that exceeds every previous threat from technology—even nuclear weapons—and that if its development is not managed carefully humanity risks engineering its own extinction. Central to this concern is the prospect of an “intelligence explosion,” a speculative event in which an A.I. gains the ability to improve itself, and in short order exceeds the intellectual potential of the human brain by many orders of magnitude. </p> <p word_count=\"106\" data-wc=\"106\">Such a system would effectively be a new kind of life, and Bostrom’s fears, in their simplest form, are evolutionary: that humanity will unexpectedly become outmatched by a smarter competitor. He sometimes notes, as a point of comparison, the trajectories of people and gorillas: both primates, but with one species dominating the planet and the other at the edge of annihilation. “Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb,” he concludes. “We have little idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound.” </p> <p word_count=\"122\" data-wc=\"122\">At the age of forty-two, Bostrom has become a philosopher of remarkable influence. “Superintelligence” is only his most visible response to ideas that he encountered two decades ago, when he became a transhumanist, joining a fractious quasi-utopian movement united by the expectation that accelerating advances in technology will result in drastic changes—social, economic, and, most strikingly, biological—which could converge at a moment of epochal transformation known as the Singularity. Bostrom is arguably the leading transhumanist philosopher today, a position achieved by bringing order to ideas that might otherwise never have survived outside the half-crazy Internet ecosystem where they formed. He rarely makes concrete predictions, but, by relying on probability theory, he seeks to tease out insights where insights seem impossible. </p> <p word_count=\"66\" data-wc=\"66\">Some of Bostrom’s cleverest arguments resemble Swiss Army knives: they are simple, toylike, a pleasure to consider, with colorful exteriors and precisely calibrated mechanics. He once cast a moral case for medically engineered immortality as a fable about a kingdom terrorized by an insatiable dragon. A reformulation of Pascal’s wager became a dialogue between the seventeenth-­century philosopher and a mugger from another dimension. </p> <p word_count=\"193\" data-wc=\"193\">“Superintelligence” is not intended as a treatise of deep originality; Bostrom’s contribution is to impose the rigors of analytic philosophy on a messy corpus of ideas that emerged at the margins of academic thought. Perhaps because the field of A.I. has recently made striking advances—with everyday technology seeming, more and more, to exhibit something like intelligent reasoning—the book has struck a nerve. Bostrom’s supporters compare it to “Silent Spring.” In moral philosophy, Peter Singer and Derek Parfit have received it as a work of importance, and distinguished physicists such as Stephen Hawking have echoed its warning. Within the high caste of Silicon Valley, Bostrom has acquired the status of a sage. Elon Musk, the C.E.O. of Tesla, promoted the book on Twitter, noting, “We need to be super careful with AI. Potentially more dangerous than nukes.” Bill Gates recommended it, too. Suggesting that an A.I. could threaten humanity, he said, during a talk in China, “When people say it’s not a problem, then I really start to get to a point of disagreement. How can they not see what a huge challenge this is?”</p><div class=\"content-ad-wrapper first\"><div class=\"advertisement\" data-ismobile=\"true\" data-name=\"yrailTop\" data-sz=\"320x251\" data-kw=\"topBox\"><div id=\"yrailTop320x251_frame\" class=\"displayAd displayAd320x251Js\" data-cb-ad-id=\"yrailTop320x251_frame\"></div></div><div class=\"advertisement\" data-ismobile=\"false\" data-name=\"inlineTop\" data-sz=\"728x90\" data-kw=\"topBanner\"><div id=\"inlineTop728x90_frame\" class=\"displayAd displayAd728x90Js\" data-cb-ad-id=\"inlineTop728x90_frame\"></div></div></div> <p word_count=\"181\" data-wc=\"181\">The people who say that artificial intelligence is not a problem tend to work in artificial intelligence. Many prominent researchers regard Bostrom’s basic views as implausible, or as a distraction from the near-term benefits and moral dilemmas posed by the technology—not least because A.I. systems today can barely guide robots to open doors. Last summer, Oren Etzioni, the C.E.O. of the Allen Institute for Artificial Intelligence, in Seattle, referred to the fear of machine intelligence as a “Frankenstein complex.” Another leading researcher declared, “I don’t worry about that for the same reason I don’t worry about overpopulation on Mars.” Jaron Lanier, a Microsoft researcher and tech commentator, told me that even framing the differing views as a debate was a mistake. “This is not an honest conversation,” he said. “People think it is about technology, but it is really about religion, people turning to metaphysics to cope with the human condition. They have a way of dramatizing their beliefs with an end-of-days scenario—and one does not want to criticize other people’s religions.”</p> <p word_count=\"104\" data-wc=\"104\">Because the argument has played out on blogs and in the popular press, beyond the ambit of peer-reviewed journals, the two sides have appeared in caricature, with headlines suggesting either doom (“<small>Will Super-intelligent Machines Kill Us All</small>?”) or a reprieve from doom (“<small>Artificial intelligence </small>‘<small>will not end human race</small>’&#8200;”). Even the most grounded version of the debate occupies philosophical terrain where little is clear. But, Bostrom argues, if artificial intelligence can be achieved it would be an event of unparalleled consequence—perhaps even a rupture in the fabric of history. A bit of long-range forethought might be a moral obligation to our own species. </p> <a class=\"tny-slot\" name=\"/3\" data-total-words=\"885\"></a><a class=\"tny-page\" name=\"/2\" data-total-words=\"885\"></a><p class=\"descender dropcap-B\" word_count=\"159\" data-wc=\"159\">Bostrom’s sole responsibility at Oxford is to direct an organization called the Future of Humanity Institute, which he founded ten years ago, with financial support from James Martin, a futurist and tech millionaire. Bostrom runs the institute as a kind of philosophical radar station: a bunker sending out navigational pulses into the haze of possible futures. Not long ago, an F.H.I. fellow studied the possibility of a “dark fire scenario,” a cosmic event that, he hypothesized, could occur under certain high-energy conditions: everyday matter mutating into dark matter, in a runaway process that could erase most of the known universe. (He concluded that it was highly unlikely.) Discussions at F.H.I. range from conventional philosophic topics, like the nature of compromise, to the optimal structure of space empires—whether a single intergalactic machine intelligence, supported by a vast array of probes, presents a more ethical future than a cosmic imperium housing millions of digital minds. </p><div id=\"parallax9x2_frame\" data-jivox-ad-id=\"constellation-parallax\" data-constellation-id=\"parallax\"></div><div class=\"content-ad-wrapper\"><div class=\"advertisement\" data-ismobile=\"true\" data-name=\"yrailBottom\" data-sz=\"320x252\" data-kw=\"2ndBox\"><div id=\"yrailBottom320x252_frame\" class=\"displayAd displayAd320x252Js\" data-cb-ad-id=\"yrailBottom320x252_frame\"></div></div><div class=\"advertisement\" data-ismobile=\"false\" data-name=\"inlineBottom\" data-sz=\"728x90\" data-kw=\"2ndBox\"><div id=\"inlineBottom728x90_frame\" class=\"displayAd displayAd728x90Js\" data-cb-ad-id=\"inlineBottom728x90_frame\"></div></div></div> <p word_count=\"93\" data-wc=\"93\">Earlier this year, I visited the institute, which is situated on a winding street in a part of Oxford that is a thousand years old. It takes some work to catch Bostrom at his office. Demand for him on the lecture circuit is high; he travels overseas nearly every month to relay his technological omens in a range of settings, from Google’s headquarters to a Presidential commission in Washington. Even at Oxford, he maintains an idiosyncratic schedule, remaining in the office until two in the morning and returning sometime the next afternoon.</p> <p word_count=\"122\" data-wc=\"122\">I arrived before he did, and waited in a hallway between two conference rooms. A plaque indicated that one of them was the Arkhipov Room, honoring Vasili Arkhipov, a Soviet naval officer. During the Cuban missile crisis, Arkhipov was serving on a submarine in the Caribbean when U.S. destroyers set off depth charges nearby. His captain, unable to establish radio contact with Moscow, feared that the conflict had escalated and ordered a nuclear strike. But Arkhipov dissuaded him, and all-out atomic war was averted. Across the hallway was the Petrov Room, named for another Soviet officer who prevented a global nuclear catastrophe. Bostrom later told me, “They may have saved more lives than most of the statesmen we celebrate on stamps.” </p> <p word_count=\"158\" data-wc=\"158\">The sense that a vanguard of technical-minded people working in obscurity, at odds with consensus, might save the world from auto-annihilation runs through the atmosphere at F.H.I. like an electrical charge. While waiting for Bostrom, I peered through a row of windows into the Arkh­ipov Room, which looked as though it was used for both meetings and storage; on a bookcase there were boxes containing light bulbs, lampshades, cables, spare mugs. A gaunt philosophy Ph.D. wrapped in a thick knitted cardigan was pacing in front of a whiteboard covered in notation, which he attacked in bursts. After each paroxysm, he paced, hands behind his back, head tilted downward. At one point, he erased a panel of his work. Taking this as an opportunity to interrupt, I asked him what he was doing. “It is a problem involving an aspect of A.I. called ‘planning,’&#8200;” he said. His demeanor radiated irritation. I left him alone.</p> <p word_count=\"132\" data-wc=\"132\">Bostrom arrived at 2 <small>p.m</small>. He has a boyish countenance and the lean, vital physique of a yoga instructor—though he could never be mistaken for a yoga instructor. His intensity is too untidily contained, evident in his harried gait on the streets outside his office (he does not drive), in his voracious consumption of audiobooks (played at two or three times the normal speed, to maximize efficiency), and his fastidious guarding against illnesses (he avoids handshakes and wipes down silverware beneath a tablecloth). Bostrom can be stubborn about the placement of an office plant or the choice of a font. But when his arguments are challenged he listens attentively, the mechanics of consideration nearly dis­cernible beneath his skin. Then, calmly, quickly, he dispatches a response, one idea interlocked with another. </p> <p word_count=\"58\" data-wc=\"58\">He asked if I wanted to go to the market. “You can watch me make my elixir,” he said. For the past year or so, he has been drinking his lunch (another efficiency): a smoothie containing fruits, vegetables, proteins, and fats. Using his elbow, he hit a button that electronically opened the front door. Then we rushed out. </p> <a class=\"tny-slot\" name=\"/4\" data-total-words=\"1607\"></a><p class=\"descender dropcap-B\" word_count=\"155\" data-wc=\"155\">Bostrom has a reinvented man’s sense of lost time. An only child, he grew up—as Niklas Boström—in Helsingborg, on the southern coast of Sweden. Like many exceptionally bright children, he hated school, and as a teen-ager he developed a listless, romantic persona. In 1989, he wandered into a library and stumbled onto an anthology of nineteenth-century German philosophy, containing works by Nietzsche and Schopenhauer. He read it in a nearby forest, in a clearing that he often visited to think and to write poetry, and experienced a euphoric insight into the possibilities of learning and achievement. “It’s hard to convey in words what that was like,” Bostrom told me; instead he sent me a photograph of an oil painting that he had made shortly afterward. It was a semi-representational landscape, with strange figures crammed into dense undergrowth; beyond, a hawk soared below a radiant sun. He titled it “The First Day.”</p> <p word_count=\"97\" data-wc=\"97\">Deciding that he had squandered his early life, he threw himself into a campaign of self-education. He ran down the citations in the anthology, branching out into art, literature, science. He says that he was motivated not only by curiosity but also by a desire for actionable knowledge about how to live. To his parents’ dismay, Bostrom insisted on finishing his final year of high school from home by taking special exams, which he completed in ten weeks. He grew distant from old friends: “I became quite fanatical and felt quite isolated for a period of time.” </p> <a class=\"tny-page\" name=\"/3\" data-total-words=\"1859\"></a><p word_count=\"197\" data-wc=\"197\">When Bostrom was a graduate student in Stockholm, he studied the work of the analytic philosopher W. V. Quine, who had explored the difficult relationship between language and reality. His adviser drilled precision into him by scribbling “not clear” throughout the margins of his papers. “It was basically his only feedback,” Bostrom told me. “The effect was still, I think, beneficial.” His previous academic interests had ranged from psychology to mathematics; now he took up theoretical physics. He was fascinated by technology. The World Wide Web was just emerging, and he began to sense that the heroic philosophy which had inspired him might be outmoded. In 1995, Bostrom wrote a poem, “Requiem,” which he told me was “a signing-off letter to an earlier self.” It was in Swedish, so he offered me a synopsis: “I describe a brave general who has overslept and finds his troops have left the encampment. He rides off to catch up with them, pushing his horse to the limit. Then he hears the thunder of a modern jet plane streaking past him across the sky, and he realizes that he is obsolete, and that courage and spiritual nobility are no match for machines.” </p> <figure class=\"cartoon-image\" data-track-location=\"embeddedCartoon\" data-cartoon-id=\"a19497\"><a href=\"https://www.newyorker.com/cartoons/a19497\" target=\"_blank\"><img class=\"cartoon post-load\" alt=\"Cartoon\" data-src-mobile=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19497-500.jpg\" src=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19497-690.jpg\"></a><figcaption><span class=\"caption\">“I’m starting a startup that helps other startups start up.”</span></figcaption></figure>   <p word_count=\"187\" data-wc=\"187\">Although Bostrom did not know it, a growing number of people around the world shared his intuition that technology could cause transformative change, and they were finding one another in an online discussion group administered by an organization in California called the Extropy Institute. The term “extropy,” coined in 1967, is generally used to describe life’s capacity to reverse the spread of entropy across space and time. Extropianism is a libertarian strain of transhumanism that seeks “to direct human evolution,” hoping to eliminate disease, suffering, even death; the means might be genetic modification, or as yet un­invented nanotechnology, or perhaps dispensing with the body entirely and uploading minds into supercomputers. (As one member noted, “Immortality is mathematical, not mystical.”) The Extropians advocated the development of artificial superintelligence to achieve these goals, and they envisioned humanity colonizing the universe, converting inert matter into engines of civilization. The discussions were nerdy, lunatic, imaginative, thought-provoking. Anders Sandberg, a former member of the group who now works at Bostrom’s institute, told me, “Just imagine if you could listen in on the debates of the Italian Futurists or early Surrealists.”</p> <p word_count=\"111\" data-wc=\"111\">In 1996, while pursuing further graduate work at the London School of Economics, Bostrom learned about the Extropy discussion group and became an active participant. A year later, he co-founded his own organization, the World Transhumanist Association, which was less libertarian and more academically spirited. He crafted approachable statements on transhumanist values and gave interviews to the BBC. The line between his academic work and his activism blurred: his Ph.D. dissertation centered on a study of the Doomsday Argument, which uses probability theory to make inferences about the longevity of human civilization. The work baffled his advisers, who respected him but rarely agreed with his conclusions. Mostly, they left him alone.</p> <p word_count=\"176\" data-wc=\"176\">Bostrom had little interest in conventional philosophy—not least because he expected that superintelligent minds, whether biologically enhanced or digital, would make it obsolete. “Suppose you had to build a new subway line, and it was this grand trans-generational enterprise that humanity was engaged in, and everybody had a little role,” he told me. “So you have a little shovel. But if you know that a giant bulldozer will arrive on the scene tomorrow, then does it really make sense to spend your time today digging the big hole with your shovel? Maybe there is something else you could do with your time. Maybe you could put up a signpost for the great shovel, so it will start digging in the right place.” He came to believe that a key role of the philosopher in modern society was to acquire the knowledge of a polymath, then use it to help guide humanity to its next phase of existence—a discipline that he called “the philosophy of technological prediction.” He was trying to become such a seer. </p> <a class=\"tny-slot\" name=\"/5\" data-total-words=\"2530\"></a><p word_count=\"36\" data-wc=\"36\">“He was ultra-consistent,” Daniel Hill, a British philosopher who befriended Bostrom while they were graduate students in London, told me. “His interest in science was a natural outgrowing of his understandable desire to live forever, basically.” </p> <p word_count=\"121\" data-wc=\"121\">Bostrom has written more than a hundred articles, and his longing for immortality can be seen throughout. In 2008, he framed an essay as a call to action from a future utopia. “Death is not one but a multitude of assassins,” he warned. “Take aim at the causes of early death—infection, violence, malnutrition, heart attack, cancer. Turn your biggest gun on aging, and fire. You must seize the biochemical processes in your body in order to vanquish, by and by, illness and senescence. In time, you will discover ways to move your mind to more durable media.” He tends to see the mind as immaculate code, the body as inefficient hardware—able to accommodate limited hacks but probably destined for replacement. </p> <p word_count=\"104\" data-wc=\"104\">Even Bostrom’s marriage is largely mediated by technology. His wife, Susan, has a Ph.D. in the sociology of medicine and a bright, down-to-earth manner. (“She teases me about the Terminator and the robot army,” he told me.) They met thirteen years ago, and for all but six months they have lived on opposite sides of the Atlantic, even after the recent birth of their son. The arrangement is voluntary: she prefers Montreal; his work keeps him at Oxford. They Skype several times a day, and he directs as much international travel as possible through Canada, so they can meet in non-digital form. </p> <p word_count=\"72\" data-wc=\"72\">In Oxford, as Bostrom shopped for his smoothie, he pointed out a man vaping. “There is also the more old-school method of taking nicotine: chewing gum,” he told me. “I do chew nicotine gum. I read a few papers saying it might have some nootropic effect”—that is, it might enhance cognition. He drinks coffee, and usually abstains from alcohol. He briefly experimented with the smart drug Modafinil, but gave it up. </p> <a class=\"tny-page\" name=\"/4\" data-total-words=\"2863\"></a><p word_count=\"86\" data-wc=\"86\">Back at the institute, he filled an industrial blender with lettuce, carrots, cauliflower, broccoli, blueberries, turmeric, vanilla, oat milk, and whey powder. “If there is one thing Nick cares about, it is minds,” Sandberg told me. “That is at the root of many of his views about food, because he is worried that toxin X or Y might be bad for his brain.” He suspects that Bostrom also enjoys the ritualistic display. “Swedes are known for their smugness,” he joked. “Perhaps Nick is subsisting on smugness.” </p> <p word_count=\"27\" data-wc=\"27\">A young employee eyed Bostrom getting ready to fire up the blender. “I can tell when Nick comes into the office,” he said. “My hair starts shaking.” </p> <p word_count=\"77\" data-wc=\"77\">“Yeah, this has got three horsepower,” Bostrom said. He ran the blender, producing a noise like a circular saw, and then filled a tall glass stein with purple-­green liquid. We headed to his office, which was meticulous. By a window was a wooden desk supporting an iMac and not another item; against a wall were a chair and a cabinet with a stack of documents. The only hint of excess was light: there were fourteen lamps. </p> <p class=\"descender dropcap-I\" word_count=\"120\" data-wc=\"120\">It is hard to spend time at Bostrom’s institute without drifting into reveries of a far future. What might humanity look like millions of years from now? The upper limit of survival on Earth is fixed to the life span of the sun, which in five billion years will become a red giant and swell to more than two hundred times its present size. It is possible that Earth’s orbit will adjust, but more likely that the planet will be destroyed. In any case, long before then, nearly all plant life will die, the oceans will boil, and the Earth’s crust will heat to a thousand degrees. In half a billion years, the planet will be uninhabitable.</p> <p word_count=\"136\" data-wc=\"136\">The view of the future from Bostrom’s office can be divided into three grand panoramas. In one, humanity experiences an evolutionary leap—either assisted by technology or by merging into it and becoming software—to achieve a sublime condition that Bostrom calls “posthumanity.” Death is overcome, mental experience expands beyond recognition, and our descendants colonize the universe. In another panorama, humanity becomes extinct or experiences a disaster so great that it is unable to recover. Between these extremes, Bostrom envisions scenarios that resemble the status quo—people living as they do now, forever mired in the “human era.” It’s a vision familiar to fans of sci-fi: on “Star Trek,” Captain Kirk was born in the year 2233, but when an alien portal hurls him through time and space to Depression-era Manhattan he blends in easily. </p> <a class=\"tny-slot\" name=\"/6\" data-total-words=\"3309\"></a><p word_count=\"62\" data-wc=\"62\">Bostrom dislikes science fiction. “I’ve never been keen on stories that just try to present ‘wow’ ideas—the equivalent of movie productions that rely on stunts and explosions to hold the attention,” he told me. “The question is not whether we can think of something radical or extreme but whether we can discover some sufficient reason for updating our credence function.” </p> <p word_count=\"139\" data-wc=\"139\">He believes that the future can be studied with the same meticulousness as the past, even if the conclusions are far less firm. “It may be highly unpredictable where a traveller will be one hour after the start of her journey, yet predictable that after five hours she will be at her destination,” he once argued. “The <em>very</em> long-term future of humanity may be relatively easy to predict.” He offers an example: if history were reset, the industrial revolution might occur at a different time, or in a different place, or perhaps not at all, with innovation instead occurring in increments over hundreds of years. In the short term, predicting technological achievements in the counter-history might not be possible; but after, say, a hundred thousand years it is easier to imagine that all the same inventions would have emerged. </p> <p word_count=\"73\" data-wc=\"73\">Bostrom calls this the Technological Completion Conjecture: “If scientific- and technological-development efforts do not effectively cease, then all impor­t­­­ant basic capabilities that could be obtained through some possible technology will be obtained.” In light of this, he suspects that the farther into the future one looks the less likely it seems that life will continue as it is. He favors the far ends of possibility: humanity becomes transcendent or it perishes. </p> <p word_count=\"155\" data-wc=\"155\">In the nineteen-nineties, as these ideas crystallized in his thinking, Bostrom began to give more attention to the question of extinction. He did not believe that doomsday was imminent. His interest was in risk, like an insurance agent’s. No matter how improbable extinction may be, Bostrom argues, its consequences are near-infinitely bad; thus, even the tiniest step toward reducing the <em>chance</em> that it will happen is near-­infinitely valuable. At times, he uses arithmetical sketches to illustrate this point. Imagining one of his utopian scenarios—trillions of digital minds thriving across the cosmos—he reasons that, if there is even a one-per-cent chance of this happening, the expected value of reducing an existential threat by a billionth of a billionth of one per cent would be worth a hundred billion times the value of a billion present-day lives. Put more simply: he believes that his work could dwarf the moral importance of anything else.</p> <p class=\"descender dropcap-B\" word_count=\"147\" data-wc=\"147\">Bostrom introduced the philosophical concept of “existential risk” in 2002, in the <em>Journal of Evolution and Technology</em>. In recent years, new organizations have been founded almost annually to help reduce it—among them the Centre for the Study of Existential Risk, affiliated with Cambridge Uni­versity, and the Future of Life Institute, which has ties to the Massachusetts Institute of Technology. All of them face a key problem: <em>Homo sapiens</em>, since its emergence two hundred thousand years ago, has proved to be remarkably resilient, and figuring out what might imperil its existence is not obvious. Climate change is likely to cause vast environmental and economic damage—but it does not seem impossible to survive. So-called super-volcanoes have thus far not threatened the perpetuation of the species. <small>NASA</small> spends forty million dollars each year to determine if there are significant comets or asteroids headed for Earth. (There aren’t.) </p> <a class=\"tny-page\" name=\"/5\" data-total-words=\"3885\"></a><p word_count=\"197\" data-wc=\"197\">Bostrom does not find the lack of obvious existential threats comforting. Because it is impossible to endure extinction twice, he argues, we cannot rely on history to calculate the probability that it will occur. The most worrying dangers are those that Earth has never encountered before. “It is hard to cause human extinction with seventeenth-century technology,” Bostrom told me. Three centuries later, though, the prospect of a technological apocalypse was urgently plausible. Bostrom dates the first scientific analysis of existential risk to the Manhattan Project: in 1942, Robert Oppenheimer became concerned that an atomic detonation of sufficient power could cause the entire atmosphere to ignite. A subsequent study concluded that the scenario was “unreasonable,” given the limitations of the weapons then in development. But even if the great nuclear nightmares of the Cold War did not come true, the tools were there to cause destruction on a scale not previously possible. As innovations grow even more complex, it is increasingly difficult to evaluate the dangers ahead. The answers must be fraught with ambiguity, because they can be derived only by predicting the effects of technologies that exist mostly as theories or, even more indirectly, by using abstract reasoning. </p> <div class=\"media-cne\"><script async src=\"//player-backend.cnevids.com/script/video/5641364261646d047600004d.js\" class=\"x-skip\"></script><span class=\"caption\" style=\"display: block; margin: 12px 0; width: 100%;\"><span class=\"caption-text\">Nick Bostrom asks, Will we engineer our own extinction?</span></span></div> <a class=\"tny-slot\" name=\"/7\" data-total-words=\"4082\"></a><p word_count=\"148\" data-wc=\"148\">As a philosopher, Bostrom takes a sweeping, even cosmic, view of such problems. One afternoon, he told me, “The probabilities that any given planet will produce intelligent life—this may also have action-relevant information.” In the past several years, <small>NASA</small> probes have found increasing evidence that the building blocks of life are abundant throughout space. So much water has been discovered—on Mars and on the moons of Jupiter and Saturn—that one scientist described our solar system as “a pretty soggy place.” There are amino acids on icy comets and complex organic molecules in distant star-forming clouds. On this planet, life has proved capable of thriving in unimaginably punishing conditions: without oxygen, without light, at four hundred degrees above or below zero. In 2007, the European Space Agency hitched tiny creatures to the exterior of a satellite. They not only survived the flight; some even laid eggs afterward. </p> <p word_count=\"145\" data-wc=\"145\">With ten billion Earth-like planets in our galaxy alone, and a hundred billion galaxies in the universe, there is good reason to suspect that extraterrestrial life may one day be discovered. For Bostrom, this would augur disaster. “It would be great news to find that Mars is a completely sterile planet,” he argued not long ago. “Dead rocks and lifeless sands would lift my spirits.” His reasoning begins with the age of the universe. Many of those Earth-like planets are thought to be far, far older than ours. One that was recently discovered, called Kepler 452b, is as much as one and a half billion years older. Bostrom asks: If life had formed there on a time scale resembling our own, what would it look like? What kind of technological progress could a civilization achieve with a head start of hundreds of millions of years?</p> <p word_count=\"174\" data-wc=\"174\">Life as we know it tends to spread wherever it can, and Bostrom estimates that, if an alien civilization could design space probes capable of travelling at even one per cent of the speed of light, the entire Milky Way could be colonized in twenty million years—a tiny fraction of the age difference between Kepler 452b and Earth. One could argue that no technology will ever propel ships at so great a speed. Or perhaps millions of alien civilizations possess the know-how for intergalactic travel, but they aren’t interested. Even so, because the universe is so colossal, and because it is so old, only a small number of civilizations would need to behave as life does on Earth—unceasingly expanding—in order to be visible. Yet, as Bostrom notes, “You start with billions and billions of potential germination points for life, and you end up with a sum total of <em>zero</em> alien civilizations that developed technologically to the point where they become manifest to us earthly observers. So what’s stopping them?” </p> <p word_count=\"152\" data-wc=\"152\">In 1950, Enrico Fermi sketched a version of this paradox during a lunch break while he was working on the H-bomb, at Los Alamos. Since then, many resolutions have been proposed—some of them exotic, such as the idea that Earth is housed in an interplanetary alien zoo. Bostrom suspects that the answer is simple: space appears to be devoid of life because it is. This implies that intelligent life on Earth is an astronomically rare accident. But, if so, when did that accident occur? Was it in the first chemical reactions in the primordial soup? Or when single-celled organisms began to replicate using DNA? Or when animals learned to use tools? Bos­trom likes to think of these hurdles as Great Filters: key phases of improbability that life everywhere must pass through in order to develop into intelligent species. Those which do not make it either go extinct or fail to evolve.</p> <p word_count=\"105\" data-wc=\"105\">Thus, for Bostrom, the discovery of a single-celled creature inhabiting a damp stretch of Martian soil would constitute a disconcerting piece of evidence. If two planets independently evolved primitive organisms, then it seems more likely that this type of life can be found on many planets throughout the universe. Bostrom reasons that this would suggest that the Great Filter comes at some later evolutionary stage. The discovery of a fossilized vertebrate would be even worse: it would suggest that the universe appears lifeless not because complex life is unusual but, rather, because it is always somehow thwarted before it becomes advanced enough to colonize space. </p> <a class=\"tny-slot\" name=\"/8\" data-total-words=\"4806\"></a><a class=\"tny-page\" name=\"/6\" data-total-words=\"4806\"></a><p word_count=\"133\" data-wc=\"133\">In Bostrom’s view, the most distressing possibility is that the Great Filter is ahead of us—that evolution frequently achieves civilizations like our own, but they perish before reaching their technological maturity. Why might that be? “Natural disasters such as asteroid hits and super-­volcanic eruptions are unlikely Great Filter candidates, because, even if they destroyed a significant number of civilizations, we would expect some civilizations to get lucky and escape disaster,” he argues. “Perhaps the most likely type of existential risks that could constitute a Great Filter are those that arise from technological discovery. It is not far-fetched to suppose that there might be some possible technology which is such that (a) virtually all suffi­ciently advanced civilizations eventually discover it and (b) its discovery leads almost universally to existential disaster.”</p> <h2 class=\"aligncenter\">II. The Machines</h2> <p class=\"descender dropcap-T\" word_count=\"96\" data-wc=\"96\">The field of artificial intelligence was born in a fit of scientific optimism, in 1955, when a small group of researchers—three mathematicians and an I.B.M. programmer—drew up a proposal for a project at Dartmouth. “An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves,” they stated. “We think a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.”</p> <p word_count=\"158\" data-wc=\"158\">Their optimism was understandable. Since the turn of the twentieth century, science had been advancing at a breakneck pace: the discovery of radioactivity quickly led to insights into the inner workings of the atom, and then to the development of controlled nuclear energy, and then to the warheads over Hiroshima and Nagasaki, and then to the H-bomb. This rush of discovery was reflected in fiction, too, in the work of Isaac Asimov, among others, who envisioned advanced civilizations inhabited by intelligent robots (each encoded with simple, ethical Laws of Robotics, to prevent it from causing harm). The year the scientists met at Dartmouth, Asimov published “The Last Question,” a story featuring a superintelligent A.I. that is continually “self-adjusting and self-correcting”—gaining knowledge as it helps human civilization expand throughout the universe. When the universe’s last stars start dying out, all humanity uploads itself into the A.I., and the device, achieving godhood, creates a new cosmos.</p> <p word_count=\"107\" data-wc=\"107\">Scientists perceived the mechanics of intelligence—like those of the atom—as a source of huge potential, a great frontier. If the brain was merely a biological machine, there was no theoretical reason that it could not be replicated, or even surpassed, much the way a jet could outfly a falcon. Even before the Dartmouth conference, machines exceeded human ability in narrow domains like code-breaking. In 1951, Alan Turing argued that at some point computers would probably exceed the intellectual capacity of their inventors, and that “therefore we should have to expect the machines to take control.” Whether this would be good or bad he did not say.</p> <figure class=\"cartoon-image\" data-track-location=\"embeddedCartoon\" data-cartoon-id=\"a19633\"><a href=\"https://www.newyorker.com/cartoons/a19633\" target=\"_blank\"><img class=\"cartoon post-load\" alt=\"Cartoon\" data-src-mobile=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19633-500.jpg\" src=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19633-690.jpg\"></a><figcaption><span class=\"caption\">“I hoped you’d like the size of it.”</span></figcaption></figure>   <p word_count=\"155\" data-wc=\"155\">Six years later, Herbert Simon, one of the Dartmouth attendees, declared that machines would achieve human intelligence “in a visible future.” The crossing of such a threshold, he suspected, could be psychologically crushing, but he was on the whole optimistic. “We must also remain sensitive to the need to keep the computer’s goals attuned with our own,” he later said, but added, “I am not convinced that this will be difficult.” For other computer pioneers, the future appeared more ambivalent. Norbert Wiener, the father of cybernetics, argued that it would be difficult to manage powerful computers, or even to accurately predict their behavior. “Complete subservience and complete intelligence do not go together,” he said. Envisioning Sorcerer’s Apprentice scenarios, he predicted, “The future will be an ever more demanding struggle against the limitations of our intelligence, not a comfortable hammock in which we can lie down to be waited upon by our robot slaves.” </p> <p word_count=\"106\" data-wc=\"106\">It was in this milieu that the “intelligence explosion” idea was first formally expressed by I. J. Good, a statistician who had worked with Turing. “An ultraintelligent machine could design even better machines,” he wrote. “There would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the <em>last</em> invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.” </p> <p class=\"descender dropcap-T\" word_count=\"147\" data-wc=\"147\">The scientists at Dartmouth recognized that success required answers to fundamental questions: What is intelligence? What is the mind? By 1965, the field had experimented with several models of problem solving: some were based on formal logic; some used heuristic reasoning; some, called “neural networks,” were inspired by the brain. With each, the scientists’ work indicated that A.I. systems could find their own solutions to problems. One algorithm proved numerous theorems in the classic text “Principia Mathematica,” and in one instance it did so more elegantly than the authors. A program designed to play checkers learned to beat its programmer. And yet, despite the great promise in these experiments, the challenges to creating an A.I. were forbidding. Programs that performed well in the laboratory were useless in everyday situations; a simple act like picking up a ball turned out to require an overwhelming number of computations. </p> <a class=\"tny-slot\" name=\"/9\" data-total-words=\"5708\"></a><p word_count=\"113\" data-wc=\"113\">The research fell into the first of several “A.I. winters.” As Bostrom notes in his book, “Among academics and their funders, ‘A.I.’ became an unwanted epithet.” Eventually, the researchers started to question the goal of building a mind altogether. Why not try instead to divide the problem into pieces? They began to limit their interests to specific cognitive functions: vision, say, or speech. Even in isolation, these functions would have value: a computer that could identify objects might not be an A.I., but it could help guide a forklift. As the research fragmented, the morass of technical problems made any questions about the consequences of success seem distant, even silly. </p> <a class=\"tny-page\" name=\"/7\" data-total-words=\"5821\"></a><p word_count=\"176\" data-wc=\"176\">Unexpectedly, by dismissing its founding goals, the field of A.I. created space for outsiders to imagine more freely what the technology might look like. Bostrom wrote his first paper on artificial superintelligence in the nineteen-nineties, envisioning it as potentially perilous but irresistible to both commerce and government. “If there is a way of guaranteeing that superior artificial intellects will never harm human beings, then such intellects will be created,” he argued. “If there is no way to have such a guarantee, then they will probably be created nevertheless.” His audience at the time was primarily other transhumanists. But the movement was maturing. In 2005, an organization called the Singularity Institute for Artificial Intelligence began to operate out of Silicon Valley; its primary founder, a former member of the Extropian discussion group, published a stream of literature on the dangers of A.I. That same year, the futurist and inventor Ray Kurzweil wrote “The Singularity Is Near,” a best-seller that prophesied a merging of man and machine in the foreseeable future. Bostrom created his institute at Oxford.</p> <p word_count=\"201\" data-wc=\"201\">The two communities could not have been more different. The scientists, steeped in technical detail, were preoccupied with making devices that worked; the transhumanists, motivated by the hope of a utopian future, were asking, What would the ultimate impact of those devices be? In 2007, the Association for the Advancement of Artificial Intelligence—the most prominent professional organization for A.I. researchers—elected Eric Horvitz, a scientist from Microsoft, as its president. Until then, it had given virtually no attention to the ethical and social implications of the research, but Horvitz was open to the big questions. “It is hard to understand what success would mean for A.I.,” he told me. “I was friendly with Jack Good, who wrote that piece on superintelligence. I knew him as a creative, funny guy who referred to a lot of his ideas as P.B.I.s—partly baked ideas. And here is this piece of his being opened up outside the field as this Bible and studied with a silver pointer. Wouldn’t it be useful, I said, even if you thought these were crazy or low-probability scenarios, to find out: Can we be proactive, should there be some poor outcome for humanity?” </p> <p word_count=\"163\" data-wc=\"163\">Horvitz organized a meeting at the Asilomar Conference Grounds, in California, a place chosen for its symbolic value: biologists had gathered there in 1975 to discuss the hazards of their research in the age of modern genetics. He divided the researchers into groups. One studied short-term ramifications, like the possible use of A.I. to commit crimes; another considered long-term consequences. Mostly, there was skepticism about the intelligence-explosion idea, which assumed answers to many unresolved questions. No one fully understands what intelligence is, let alone how it might evolve in a machine. Can it grow as Good imagined, gaining I.Q. points like a rocketing stock price? If so, what would its upper limit be? And would its increase be merely a function of optimized software design, without the difficult process of acquiring knowledge through experience? Can software fundamentally rewrite itself without risking crippling breakdowns? No one knows. In the history of computer science, no programmer has created code that can substantially improve itself.</p> <figure class=\"cartoon-image\" data-track-location=\"embeddedCartoon\" data-cartoon-id=\"a19630\"><a href=\"https://www.newyorker.com/cartoons/a19630\" target=\"_blank\"><img class=\"cartoon post-load\" alt=\"Cartoon\" data-src-mobile=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19630-500.jpg\" src=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19630-690.jpg\"></a><figcaption><span class=\"caption\">“No, you want the A train. This is just a train.”</span></figcaption></figure>   <p word_count=\"177\" data-wc=\"177\">But the notion of an intelligence explosion was also impossible to disprove. It was theoretically coherent, and it had even been attempted in limited ways. David McAllester, an A.I. researcher at the Toyota Technological Institute, affiliated with the University of Chicago, headed the long-term panel. The idea, he argued, was worth taking seriously. “I am uncomfortable saying that we are ninety-­nine per cent certain that we are safe for fifty years,” he told me. “That feels like hubris to me.” The group concluded that more technical work was needed before an evaluation of the dangers could be made, but it also hinted at a concern among panelists that the gathering was based on “a <em>perception</em> of urgency”—generated largely by the transhumanists—and risked raising unfounded alarm. With A.I. seeming like a remote prospect, the researchers declared, attention was better spent on near-term concerns. Bart Selman, a professor at Cornell who co-­organized the panel, told me, “The mode was ‘This is interesting, but it’s all academic—it’s not going to happen.’&#8200;”</p> <a class=\"tny-slot\" name=\"/10\" data-total-words=\"6538\"></a><p class=\"descender dropcap-A\" word_count=\"66\" data-wc=\"66\">At the time the A.I researchers met at Asilomar, Bostrom was grappling with an expansive book on existential risks. He had sketched out chapters on bioengineering and on nanotechnology, among other topics, but many of these problems came to seem less compelling, while his chapter on A.I. grew and grew. Eventually, he pasted the A.I. chapter into a new file, which became “Superintelligence.” </p> <p word_count=\"132\" data-wc=\"132\">The book is its own elegant paradox: analytical in tone and often lucidly argued, yet punctuated by moments of messianic urgency. Some portions are so extravagantly speculative that it is hard to take them seriously. (“Suppose we could somehow establish that a certain future AI will have an IQ of 6,455: then what?”) But Bostrom is aware of the limits to his type of futurology. When he was a graduate student in London, thinking about how to maximize his ability to communicate, he pursued stand­­up comedy; he has a deadpan sense of humor, which can be found lightly buried among the book’s self-serious passages. “Many of the points made in this book are probably wrong,” he writes, with an endnote that leads to the line “I don’t know which ones.”</p> <p word_count=\"124\" data-wc=\"124\">Bostrom prefers to act as a cartographer rather than a polemicist, but beneath his exhaustive mapping of scenarios one can sense an argument being built and perhaps a fear of being forthright about it. “Traditionally, this topic domain has been occupied by cranks,” he told me. “By popular media, by science fiction—or maybe by a retired physicist no longer able to do serious work, so he will write a popular book and pontificate. That is kind of the level of rigor that is the baseline. I think that a lot of reasons why there has not been more serious work in this area is that academics don’t want to be conflated with flaky, crackpot type of things. Futurists are a certain type.”</p> <a class=\"tny-page\" name=\"/8\" data-total-words=\"6860\"></a><p word_count=\"68\" data-wc=\"68\">The book begins with an “unfinished” fable about a flock of sparrows that decide to raise an owl to protect and advise them. They go looking for an owl egg to steal and bring back to their tree, but, because they believe their search will be so difficult, they postpone studying how to domesticate owls until they succeed. Bostrom concludes, “It is not known how the story ends.”</p> <p word_count=\"137\" data-wc=\"137\">The parable is his way of introducing the book’s core question: Will an A.I., if realized, use its vast capability in a way that is beyond human control? One way to think about the concern is to begin with the familiar. Bos­trom writes, “Artificial intelligence already outperforms human intelligence in many domains.” The examples range from chess to Scrabble. One program from 1981, called Eurisko, was designed to teach itself a naval role-playing game. After playing ten thousand matches, it arrived at a morally grotesque strategy: to field thousands of small, immobile ships, the vast majority of which were intended as cannon fodder. In a national tournament, Eurisko demolished its human opponents, who insisted that the game’s rules be changed. The following year, Eurisko won again—by forcing its damaged ships to sink themselves. </p> <p word_count=\"110\" data-wc=\"110\">The program was by no means superintelligent. But Bostrom’s book essentially asks: What if it were? Assume that it has a broad ability to consider problems and that it has access to the Internet. It could read and acquire general knowledge and communicate with people seamlessly online. It could conduct experiments, either virtually or by tinkering with networked infrastructure. Given even the most benign objective—to win a game—such a system, Bostrom argues, might develop “instrumental goals”: gather resources, or invent technology, or take steps to insure that it cannot be turned off, in the process paying as much heed to human life as humans do to ants. </p> <p word_count=\"90\" data-wc=\"90\">In people, intelligence is inseparable from consciousness, emotional and social awareness, the complex interaction of mind and body. An A.I. need not have any such attributes. Bostrom believes that machine intelligences—no matter how flexible in their tactics—will likely be rigidly fixated on their ultimate goals. How, then, to create a machine that respects the nuances of social cues? That adheres to ethical norms, even at the expense of its goals? No one has a coherent solution. It is hard enough to reliably inculcate such behavior in people. </p> <a class=\"tny-slot\" name=\"/11\" data-total-words=\"7265\"></a><p word_count=\"265\" data-wc=\"265\">In science fiction, superintelligent computers that run amok are often circumvented at the last minute; think of WOPR, the computer in “WarGames,” which was stopped just short of triggering nuclear war, or HAL 9000, which was reduced to helplessly singing while it watched itself get dismantled. For Bos­trom, this strains credulity. Whether out of a desire to consider the far ends of risk or out of transhumanist longings, he often ascribes nearly divine abilities to machines, as if to ask: Can a digital god really be contained? He imagines machines so intelligent that merely by inspecting their own code they can extrapolate the nature of the universe and of human society, and in this way outsmart any effort to contain them. “Is it possible to build machines that are not like agents—goal-pursuing, autonomous, artificial intelligences?” he asked me. “Maybe you can design something more like an oracle that can only answer yes or no. Would that be safer? It is not so clear. There might be agent-like processes within it.” Asking a simple question—“Is it possible to convert a DeLorean into a time machine and travel to 1955?”—might trigger a cascade of action as the device tests hypotheses. What if, working through a police computer, it impounds a DeLorean that happens to be convenient to a clock tower? “In fairy tales, you have genies who grant wishes,” Bostrom said. “Almost universally, the moral of those is that if you are not extremely careful what you wish for, then what seems like it should be a great blessing turns out to be a curse.”</p> <p class=\"descender dropcap-B\" word_count=\"154\" data-wc=\"154\">Bostrom worries that solving the “control problem”—insuring that a superintelligent machine does what humans want it to do—will require more time than solving A.I. does. The intelligence explosion is not the only way that a superintelligence might be created suddenly. Bostrom once sketched out a decades-long process, in which researchers arduously improved their systems to equal the intelligence of a mouse, then a chimp, then—after incredible labor—the village idiot. “The difference between village idiot and genius-­level intelligence might be trivial from the point of view of how hard it is to replicate the same functionality in a machine,” he said. “The brain of the village idiot and the brain of a scientific genius are almost identical. So we might very well see relatively slow and incremental progress that doesn’t really raise any alarm bells until we are just one step away from something that is radically superintelligent.”</p> <p word_count=\"216\" data-wc=\"216\">To a large degree, Bostrom’s concerns turn on a simple question of timing: Can breakthroughs be predicted? “It is ridiculous to talk about such things so early—A.I. is eons away,” Edward Feigenbaum, an emeritus professor at Stanford University, told me. The researcher Oren Etzioni, who used the term “Frankenstein complex” to dismiss the “dystopian vision of A.I.,” concedes Bostrom’s overarching point: that the field must one day confront profound philosophical questions. Decades ago, he explored them himself, in a brief paper, but concluded that the problem was too remote to think about productively. “Once, Nick Bostrom gave a talk, and I gave a little counterpoint,” he told me. “A lot of the disagreements come down to what time scale you are thinking about. Nobody responsible would say you will see anything remotely like A.I. in the next five to ten years. And I think most computer scientists would say, ‘In a million years—we don’t see why it shouldn’t happen.’ So now the question is: What is the rate of progress? There are a lot of people who will ask: Is it <em>possible</em> we are wrong? Yes. I am not going to rule it out. I am going to say, ‘I am a scientist. Show me the evidence.’&#8200;”</p> <a class=\"tny-page\" name=\"/9\" data-total-words=\"7900\"></a><p word_count=\"118\" data-wc=\"118\">The history of science is an uneven guide to the question: How close are we? There has been no shortage of unfulfilled promises. But there are also plenty of examples of startling nearsightedness, a pattern that Arthur C. Clarke enshrined as Clarke’s First Law: “When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong.” After the electron was discovered, at Cambridge, in 1897, physicists at an annual dinner toasted, “To the electron: may it never be of use to anybody.” Lord Kelvin famously declared, just eight years before the Wright brothers launched from Kitty Hawk, that heavier-than-air flight was impossible. </p> <a class=\"tny-slot\" name=\"/12\" data-total-words=\"8018\"></a><p word_count=\"184\" data-wc=\"184\">Stuart Russell, the co-author of the textbook “Artificial Intelligence: A Modern Approach” and one of Bostrom’s most vocal supporters in A.I., told me that he had been studying the physics community during the advent of nuclear weapons. At the turn of the twentieth century, Ernest Rutherford discovered that heavy elements produced radiation by atomic decay, confirming that vast reservoirs of energy were stored in the atom. Rutherford believed that the energy could not be harnessed, and in 1933 he proclaimed, “Anyone who expects a source of power from the transformation of these atoms is talking moonshine.” The next day, a former student of Einstein’s named Leo Szilard read the comment in the papers. Irritated, he took a walk, and the idea of a nuclear chain reaction occurred to him. He visited Rutherford to discuss it, but Rutherford threw him out. Einstein, too, was skeptical about nuclear energy—splitting atoms at will, he said, was “like shooting birds in the dark in a country where there are only a few birds.” A decade later, Szilard’s insight was used to build the bomb.</p> <p word_count=\"82\" data-wc=\"82\">Russell now relays the story to A.I. researchers as a cautionary tale. “There will have to be more breakthroughs to get to A.I., but, as Szilard illustrated, those can happen overnight,” he told me. “People are putting billions of dollars into achieving those breakthroughs. As the debate stands, Bostrom and others have said, ‘If we achieve superintelligence, here are some of the problems that might arise.’ As far as I know, no one has proved why those are not real.”</p> <h2 class=\"aligncenter\">III. Mission Control</h2> <p class=\"descender dropcap-T\" word_count=\"145\" data-wc=\"145\">The offices of the Future of Humanity Institute have a hybrid atmosphere: part physics lab, part college dorm room. There are whiteboards covered with mathematical notation and technical glyphs; there are posters of “Brave New World” and HAL 9000. There is also art work by Nick Bostrom. One afternoon, he guided me to one of his pieces, “At Sea,” a digital collage that he had printed out and then drawn on. “It is a bit damaged, but the good thing about digital is that you can re-instantiate it,” he said. At the center was a pale man, nearly an apparition, clinging to a barrel in an inky-black ocean. “It is an existentialist vibe. You are hanging on for as long as you can. When you get tired, you sink, and become fish food—or maybe a current will take him to land. We don’t know.” </p> <p word_count=\"146\" data-wc=\"146\">Despite the time he spends going to conferences and raising money, Bostrom attends to many details at the institute. “We needed a logo when we started,” he told me. “We went to this online site where you could buy the work of freelance artists. If you sat down and tried to make the ugliest logo, you couldn’t come close. Then we hired a designer, who made a blurry figure of a person. We showed it to someone here, who said it looked like a toilet sign. As soon as she said it, I thought, Oh, my God, we almost adopted a toilet sign as our logo. So I mucked around a bit and came up with a black diamond. You have the black monolith from ‘2001.’ Standing on its corner, it indicates instability. Also, there is a limit to how ugly a black square can be.” </p> <p word_count=\"122\" data-wc=\"122\">The institute shares office space with the Centre for Effective Altruism, and both organizations intersect with a social movement that promotes pure rationality as a guide to moral action. Toby Ord, a philosopher who works with both, told me that Bostrom often pops into his office at the end of the day, poses a problem, then leaves him pondering it for the night. Among the first of Bostrom’s questions was this: If the universe turns out to contain an infinite number of beings, then how could any single person’s action affect the cosmic balance of suffering and happiness? After lengthy discussions, they left the paradox unresolved. “My main thinking is that we can sort it out later,” Ord told me. </p> <figure class=\"cartoon-image\" data-track-location=\"embeddedCartoon\" data-cartoon-id=\"a19596\"><a href=\"https://www.newyorker.com/cartoons/a19596\" target=\"_blank\"><img class=\"cartoon post-load\" alt=\"Cartoon\" data-src-mobile=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19596-500.jpg\" src=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19596-690.jpg\"></a><figcaption><span class=\"caption\">“Chaucer on lyne thrie.”</span></figcaption></figure>   <p word_count=\"188\" data-wc=\"188\">When I asked Bostrom if I could observe a discussion at the institute, he seemed reluctant; it was hard to judge whether he was concerned that my presence would interfere or that unfiltered talk of, say, engineered pathogens might inspire criminals. (“At some point, one gets into the realm of information hazard,” he hinted.) Eventually, he let me observe a session in the Petrov Room involving half a dozen staff members. The key question under discussion was whether a global catastrophe, on the order of a continent-wide famine, could trigger a series of geopolitical events that would result in human extinction—and whether that meant that a merely catastrophic risk could therefore be taken as seriously as an existential risk. Bostrom, wearing a gray hoodie over a blue button-­down, organized the problem on a whiteboard with visible pleasure. Anders Sandberg told me that he once spent days with Bostrom working through such a problem, distilling a complex argument to its essence. “He had to <em>refine</em> it,” he said. “We had a lot of schemes on the whiteboard that gradually were simplified to one box and three arrows.” </p> <a class=\"tny-slot\" name=\"/13\" data-total-words=\"8885\"></a><a class=\"tny-page\" name=\"/10\" data-total-words=\"8885\"></a><p word_count=\"101\" data-wc=\"101\">For anyone in the business of publicizing existential risk, 2015 began as a good year. Other institutes devoted to these issues had started to find their voice, bringing an additional gloss of respectability to the ideas in Bostrom’s book. The people weighing in now were no longer just former Extropians. They were credentialled, like Lord Martin Rees, an astrophysicist and the co-founder of Cambridge’s Centre for the Study of Existential Risk. In January, he wrote of A.I., in the <em>Evening Standard</em>, “We don’t know where the boundary lies between what may happen and what will remain science fiction.”</p> <p word_count=\"174\" data-wc=\"174\">Rees’s counterpart at the Future of Life Institute, the M.I.T. physicist Max Tegmark, hosted a closed-door meeting in Puerto Rico, to try to make sense of the long-term trajectory of the research. Bostrom flew down, joining a mix of A.I. practitioners, legal scholars, and, for lack of a better term, members of the “A.I. safety” community. “These are not people who are usually in the same room,” Tegmark told me. “Someone advised me to put Valium in people’s drinks so nobody got into fistfights. But, by the time Nick’s session started, people were ready to listen to each other.” Questions that had seemed fanciful to researchers only seven years earlier were beginning to look as though they might be worth reconsidering. Whereas the Asilomar meeting concluded on a note of skepticism about the validity of the whole endeavor, the Puerto Rico conference resulted in an open letter, signed by many prominent researchers, that called for more research to insure that A.I. would be “robust and beneficial.” </p> <p word_count=\"160\" data-wc=\"160\">Between the two conferences, the field had experienced a revolution, built on an approach called deep learning—a type of neural network that can discern complex patterns in huge quantities of data. For de­c­ades, researchers, hampered by the limits of their hardware, struggled to get the technique to work well. But, beginning in 2010, the increasing availability of Big Data and cheap, powerful video-­game processors had a dramatic effect on performance. Without any profound theoretical breakthrough, deep learning suddenly offered breathtaking advances. “I have been talking to quite a few contemporaries,” Stuart Russell told me. “Pretty much everyone sees examples of progress they just didn’t expect.” He cited a YouTube clip of a four-legged robot: one of its designers tries to kick it over, but it quickly regains its balance, scrambling with uncanny naturalness. “A problem that had been viewed as very difficult, where progress was slow and incremental, was all of a sudden done. Locomotion: done.”</p> <p word_count=\"77\" data-wc=\"77\">In an array of fields—speech processing, face recognition, language translation—the approach was ascendant. Researchers working on computer vision had spent years to get systems to identify objects. In almost no time, the deep-learning networks crushed their records. In one common test, using a database called ImageNet, humans identify photographs with a five-per-cent error rate; Google’s network operates at 4.8 per cent. A.I. systems can differentiate a Pembroke Welsh Corgi from a Cardigan Welsh Corgi. </p> <p word_count=\"154\" data-wc=\"154\">Last October, Tomaso Poggio, an M.I.T. researcher, gave a skeptical interview. “The ability to describe the content of an image would be one of the most intellectually challenging things of all for a machine to do,” he said. “We will need another cycle of basic research to solve this kind of question.” The cycle, he predicted, would take at least twenty years. A month later, Google announced that its deep-learning network could analyze an image and offer a caption of what it saw: “Two pizzas sitting on top of a stove top,” or “People shopping at an outdoor market.” When I asked Poggio about the results, he dismissed them as automatic associations between objects and language; the system did not <em>understand</em> what it saw. “Maybe human intelligence is the same thing, in which case I am wrong, or not, in which case I was right,” he told me. “How do you decide?”</p> <p word_count=\"214\" data-wc=\"214\">A respected minority of A.I. researchers began to wonder: If increasingly powerful hardware could facilitate the deep-learning revolution, would it make other long-shelved A.I. principles viable? “Suppose the brain is just a million different evolutionarily developed hacks: one for smell, one for recognizing faces, one for how you recognize animals,” Tom Mitchell, who holds a chair in machine learning at Carnegie Mellon, told me. “If that is what underlies intelligence, then I think we are far, far from getting there—because we don’t have many of those hacks. On the other hand, suppose that what underlies intelligence are twenty-three general mechanisms, and when you put them together you get synergy, and it works. We now have systems that can do a pretty good job with computer vision—and it turns out that we didn’t have to construct a million hacks. So part of the uncertainty is: if we do not need a million different hacks, then will we find the right twenty-­three fundamental generic methods?” He paused. “I no longer have the feeling, which I had twenty-five years ago, that there are gaping holes. I know we don’t have a good architecture to assemble the ideas, but it is not obvious to me that we are missing components.” </p> <figure class=\"cartoon-image\" data-track-location=\"embeddedCartoon\" data-cartoon-id=\"a19555\"><a href=\"https://www.newyorker.com/cartoons/a19555\" target=\"_blank\"><img class=\"cartoon post-load\" alt=\"Cartoon\" data-src-mobile=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19555-500.jpg\" src=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19555-690.jpg\"></a><figcaption></figcaption></figure>   <a class=\"tny-slot\" name=\"/14\" data-total-words=\"9765\"></a><p word_count=\"160\" data-wc=\"160\">Bostrom noticed the shift in attitude. He recently conducted a poll of A.I. researchers to gauge their sense of progress, and in Puerto Rico a survey gathered opinions on how long it would be until an artificial intelligence could reason indistinguishably from a human being. Like Bostrom, the engineers are often careful to express their views as probabilities, rather than as facts. Richard Sutton, a Canadian computer scientist whose work has earned tens of thousands of scholarly citations, gives a range of outcomes: there is a ten-per-cent chance that A.I. will never be achieved, but a twenty-five-per-cent chance that it will arrive by 2030. The median response in Bostrom’s poll gives a fifty-fifty chance that human-level A.I. would be attained by 2050. These surveys are unscientific, but he is confident enough to offer an interpretive assumption: “It is not a ridiculous prospect to take seriously the possibility that it can happen in the lifetime of people alive today.”</p> <a class=\"tny-page\" name=\"/11\" data-total-words=\"9925\"></a><p class=\"descender dropcap-O\" word_count=\"125\" data-wc=\"125\">On my last day in Oxford, I walked with Bostrom across town. He was racing to catch a train to London, to speak at the Royal Society, one of the world’s oldest scientific institutions. His spirits were high. The gulf between the transhumanists and the scientific community was slowly shrinking. Elon Musk had pledged ten million dollars in grants for academics seeking to investigate A.I. safety, and, rather than mock him, researchers applied for the money; Bostrom’s institute was helping to evaluate the proposals. “Right now, there is a lot of interest,” he told me. “But then there were all these long years when nobody else seemed to pay attention at all. I am not sure which is the less abnormal condition.” </p> <p word_count=\"153\" data-wc=\"153\">There were clear limits to that interest. To publicly stake out a position in the middle of the debate was difficult, not least because of the polarized atmosphere Bostrom’s book had helped to create. Even though a growing number of researchers were beginning to suspect that profound questions loomed, and that they might be worth addressing now, it did not mean that they believed A.I. would lead inevitably to an existential demise or a techno-utopia. Most of them were engaged with more immediate problems: privacy, unemployment, weaponry, driverless cars running amok. When I asked Bostrom about this pragmatic ethical awakening, he reacted with dismay. “My fear is that it would swallow up the concerns for the longer term,” he said. “On the other hand, yes, maybe it is useful to build bridges to these different communities. Kind of makes the issue part of a larger continuum of things to work on.”</p> <p word_count=\"133\" data-wc=\"133\">At the Royal Society, Bostrom took a seat at the back of a large hall. As he crossed his legs, I noticed a thin leather band around his ankle. A metal buckle was engraved with contact information for Alcor, a cryonics facility in Arizona, where Bostrom is a fee-paying member. Within hours of his death, Alcor will take custody of his body and maintain it in a giant steel bottle flooded with liquid nitrogen, in the hope that one day technology will allow him to be revived, or to have his mind uploaded into a computer. When he signed up, two other colleagues at the institute joined him. “My background is transhumanism,” he once reminded me. “The character of that is gung-ho techno-cheerleading, bring it on now, where are my life-­extension pills.” </p> <p word_count=\"107\" data-wc=\"107\">The hall was packed with some of the most technically sophisticated researchers in A.I.—not necessarily Bostrom’s people—and when he spoke he began by trying to assure them that his concern was not out of Ludditism. “It would be tragic if machine intelligence were never developed to its full capacity,” he said. “I think this is ultimately the key, or the portal, we have to pass through to realize the full dimension of humanity’s long-term potential.” But, even as he avoided talk of existential risk, he pressed his audience to consider the danger of building an A.I. without regarding its ethical design. </p> <a class=\"tny-slot\" name=\"/15\" data-total-words=\"10443\"></a><p word_count=\"48\" data-wc=\"48\">An attendee raised his hand to object. “We can’t control basic computer worms,” he said. “The A.I. that will happen is going to be a highly adaptive, emergent capability, and highly distributed. We will be able to work with it—<em>for</em> it—not necessarily contain it.” </p> <p word_count=\"64\" data-wc=\"64\">“I guess I am a little frustrated,” Bos­trom responded. “People tend to fall into two camps. On one hand, there are those, like yourself, who think it is probably hopeless. The other camp thinks it is easy enough that it will be solved automatically. And both of these have in common the implication that we don’t have to make any effort now.”</p> <p word_count=\"106\" data-wc=\"106\">For the rest of the day, engineers presented their work at the lectern, each promising a glimpse of the future—robot vision, quantum computers, algorithms called “thought vectors.” Early in Bostrom’s career, he predicted that cascading economic demand for an A.I. would build up across the fields of medicine, entertainment, finance, and defense. As the technology became useful, that demand would only grow. “If you make a one-per-cent improvement to something—say, an algorithm that recommends books on Amazon—there is a lot of value there,” Bostrom told me. “Once every improvement potentially has enormous economic benefit, that promotes effort to make more improvements.” </p> <p word_count=\"124\" data-wc=\"124\">Many of the world’s largest tech companies are now locked in an A.I. arms race, purchasing other companies and opening specialized units to advance the technology. Industry is vacuuming up Ph.D.s so quickly that people in the field worry there will no longer be top talent in academia. After decades of pursuing narrow forms of A.I., researchers are seeking to integrate them into systems that resemble a general intellect. Since I.B.M.’s Watson won “Jeopardy,” the company has committed more than a billion dollars to develop it, and is reorienting its business around “cognitive systems.” One senior I.B.M. executive declared, “The separation between human and machine is going to blur in a very fundamental way.” </p> <p word_count=\"83\" data-wc=\"83\">At the Royal Society, a contingent of researchers from Google occupied a privileged place; they likely had more resources at their disposal than anyone else in the room. Early on, Google’s founders, Larry Page and Sergey Brin, understood that the company’s mission required solving fundamental A.I. problems. Page has said that he believes the ideal system would understand questions, even anticipate them, and produce responses in conversational language. Google scientists often invoke the computer in “Star Trek” as a model. </p> <figure class=\"cartoon-image\" data-track-location=\"embeddedCartoon\" data-cartoon-id=\"a19484\"><a href=\"https://www.newyorker.com/cartoons/a19484\" target=\"_blank\"><img class=\"cartoon post-load\" alt=\"Cartoon\" data-src-mobile=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19484-500.jpg\" src=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19484-690.jpg\"></a><figcaption><span class=\"caption\">“O.K., there’s the moon—now give me a nice long howl instead of last night’s yip.”</span></figcaption></figure>   <a class=\"tny-page\" name=\"/12\" data-total-words=\"10868\"></a><p word_count=\"168\" data-wc=\"168\">In recent years, Google has purchased seven robotics companies and several firms specializing in machine intelligence; it may now employ the world’s largest contingent of Ph.D.s in deep learning. Perhaps the most interesting acquisition is a British company called DeepMind, started in 2011 to build a general artificial intelligence. Its founders had made an early bet on deep learning, and sought to combine it with other A.I. mechanisms in a cohesive architecture. In 2013, they published the results of a test in which their system played seven classic Atari games, with no instruction other than to improve its score. For many people in A.I., the importance of the results was immediately evident. I.B.M.’s chess program had defeated Garry Kasparov, but it could not beat a three-year-old at tic-tac-toe. In six games, DeepMind’s system outperformed all previous algorithms; in three it was superhuman. In a boxing game, it learned to pin down its opponent and subdue him with a barrage of punches.</p> <p word_count=\"75\" data-wc=\"75\">Weeks after the results were released, Google bought the company, reportedly for half a billion dollars. DeepMind placed two unusual conditions on the deal: its work could never be used for espionage or defense purposes, and an ethics board would oversee the research as it drew closer to achieving A.I. Anders Sandberg had told me, “We are happy that they are among the most likely to do it. They recognize there are some problems.”</p> <p word_count=\"100\" data-wc=\"100\">DeepMind’s chief founder, Demis Hassabis, described his company to the audience at the Royal Society as an “Apollo Program” with a two-part mission: “Step one, solve intelligence. Step two, use it to solve everything else.” Since the test in 2013, his system had aced more than a dozen other Atari titles. Hassabis demonstrated an unpublished trial using a three-dimensional driving game, in which it had quickly outperformed the game’s automated drivers. The plan was to test it in increasingly complex virtual environments and, eventually, in the real world. The patent lists a range of uses, from finance to robotics. </p> <a class=\"tny-slot\" name=\"/16\" data-total-words=\"11211\"></a><p word_count=\"166\" data-wc=\"166\">Hassabis was clear about the challenges. DeepMind’s system still fails hopelessly at tasks that require long-range planning, knowledge about the world, or the ability to defer rewards—things that a five-year-old child might be expected to handle. The company is working to give the algorithm conceptual understanding and the capability of transfer learning, which allows humans to apply lessons from one situation to another. These are not easy problems. But DeepMind has more than a hundred Ph.D.s to work on them, and the rewards could be immense. Hassabis spoke of building artificial scientists to resolve climate change, disease, poverty. “Even with the smartest set of humans on the planet working on these problems, these systems might be so complex that it is difficult for individual humans, scientific experts,” he said. “If we can crack what intelligence is, then we can use it to help us solve all these other problems.” He, too, believes that A.I. is a gateway to expanded human potential. </p> <p word_count=\"72\" data-wc=\"72\">The keynote speaker at the Royal Society was another Google employee: Geoffrey Hinton, who for decades has been a central figure in developing deep learning. As the conference wound down, I spotted him chatting with Bostrom in the middle of a scrum of researchers. Hinton was saying that he did not expect A.I. to be achieved for decades. “No sooner than 2070,” he said. “I am in the camp that is hopeless.”</p> <p word_count=\"14\" data-wc=\"14\">“In that you think it will not be a cause for good?” Bostrom asked. </p> <p word_count=\"27\" data-wc=\"27\">“I think political systems will use it to terrorize people,” Hinton said. Already, he believed, agencies like the N.S.A. were attempting to abuse similar technology. </p> <p word_count=\"9\" data-wc=\"9\">“Then why are you doing the research?” Bostrom asked.</p> <p word_count=\"71\" data-wc=\"71\">“I could give you the usual arguments,” Hinton said. “But the truth is that the prospect of discovery is too <em>sweet</em>.” He smiled awkwardly, the word hanging in the air—an echo of Oppenheimer, who famously said of the bomb, “When you see something that is technically sweet, you go ahead and do it, and you argue about what to do about it only after you have had your technical success.” </p> <p word_count=\"82\" data-wc=\"82\">As the scientists retreated to tables set up for refreshments, I asked Hinton if he believed an A.I. could be controlled. “That is like asking if a child can control his parents,” he said. “It can happen with a baby and a mother—there is biological hardwiring—but there is not a good track record of less intelligent things controlling things of greater intelligence.” He looked as if he might elaborate. Then a scientist called out, “Let’s all get drinks!”</p> <p class=\"descender dropcap-B\" word_count=\"118\" data-wc=\"118\">Bostrom had little interest in the cocktail party. He shook a few hands, then headed for St. James’s Park, a public garden that extends from the gates of Buckingham Palace through central London. The world appeared in splendorous analog: sunlight over trees, duck ponds, children and grandparents feeding birds. The spot had been a park for hundreds of years, and the vista seemed timeless. Yet, during the past millennium, the grounds had also been a marsh, a leper hospital, a deer sanctuary, and royal gardens. It seemed plausible that, a thousand years from now, digital posthumans, regarding it as wasted space, would tear it up, replace the landscaping with computer banks, and erect a vast virtual idyll.</p> <p word_count=\"176\" data-wc=\"176\">Bostrom’s pace settled into its natural quickness as we circled the park. He talked about his family; he would be seeing his wife and son soon. He was reading widely: history, psychology, economics. He was learning to code. He was thinking about expanding his institute. Although he did not know it then, F.H.I. was about to receive one and a half million dollars from Elon Musk, to create a unit that would craft social policies informed by some of Bostrom’s theories. He would need to hire people. He was also giving thought to the framing of his message. “A lot more is said about the risks than the upsides, but that is not necessarily because the upside is not there,” he told me. “There is just more to be said about the risk—and maybe more use in describing the pitfalls, so we know how to steer around them—than spending time now figuring out the details of how we are going to furnish the great palace a thousand years from now.”</p> <figure class=\"cartoon-image\" data-track-location=\"embeddedCartoon\" data-cartoon-id=\"a19698\"><a href=\"https://www.newyorker.com/cartoons/a19698\" target=\"_blank\"><img class=\"cartoon post-load\" alt=\"Cartoon\" data-src-mobile=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19698-500.jpg\" src=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19698-690.jpg\"></a><figcaption><span class=\"caption\">“I’m bringing on Josh here for when we take over fantasy sports betting.”</span></figcaption></figure>   <a class=\"tny-page\" name=\"/13\" data-total-words=\"11946\"></a><p word_count=\"164\" data-wc=\"164\">We passed a fountain, near a cluster of rocks engineered to give ducks a resting place. Bostrom, in his forties, must soon contend with physical decline, and he spoke with annoyance of the first glimmers of mortality. Even though he is an Alcor member, there is no guarantee that cryonics will work. Perhaps the most radical of his visions is that superintelligent A.I. will hasten the uploading of minds—what he calls “whole-brain emulations”—technology that might not be possible for centuries, if at all. Bostrom, in his most hopeful mode, imagines emulations not only as reproductions of the original intellect “with memory and personality intact”—a soul in the machine—but as minds expandable in countless ways. “We live for seven decades, and we have three-pound lumps of cheesy matter to think with, but to me it is plausible that there could be extremely valuable mental states outside this little particular set of possibilities that might be much better,” he told me. </p> <a class=\"tny-slot\" name=\"/17\" data-total-words=\"12110\"></a><p word_count=\"242\" data-wc=\"242\">In his book, Bostrom considers a distant future in which trillions of digital minds merge into an enormous cognitive cyber-soup. “Whether the set of extremely positive posthuman modes of being would include some kind of dissolved bouillon, there is some uncertainty,” he said. “If you look at religious views, there are many where merging with something greater is a form of heaven, being in the presence of this enormous beauty and goodness. In many traditions, the best possible state does not involve being a little individual pursuing goals. But it is hard to get a grasp of what would be going on in that soup. Maybe some soups would not be preferable as a long-term outcome. I don’t know.” He stopped and looked ahead. “What I want to avoid is to think from our parochial 2015 view—from my own limited life experience, my own limited brain—and super-confidentially postulate what is the best form for civilization a billion years from now, when you could have brains the size of planets and billion-year life spans. It seems unlikely that we will figure out some detailed blueprint for utopia. What if the great apes had asked whether they should evolve into <em>Homo sapiens</em>—pros and cons—and they had listed, on the pro side, ‘Oh, we could have a lot of bananas if we became human’? Well, we can have unlimited bananas now, but there is more to the human condition than that.” <span class=\"dingbat\">♦</span></p> <p class=\"credit\" word_count=\"9\" data-wc=\"9\"><em>Illustration by Todd St. John/Coding by Jono Brandel.</em></p> \t\t\t</div>\n\t\t\t\t\t\t\t</div><!-- #content -->\n\n\t\t\t\t\t\t\t<footer>\n\t\t\t\t\t\t\t\t<section class=\"article-contributors\">\n\n\t\t\t\t\t\t\t\t\t<aside class=\"author-details\" itemscope itemtype=\"http://schema.org/Person\">\n\t\t\t\t\t\t\t\t\t\t<meta itemprop=\"name\" content=\"Raffi Khatchadourian\">\n\t\t\t\t\t\t\t\t\t\t<meta itemprop=\"url\" content=\"https://www.newyorker.com/contributors/raffi-khatchadourian\">\n\t\t\t\t\t\t\t\t\t\t<meta itemprop=\"image\" content=\"https://projects.newyorker.com/interactive/2015/ai-story/archive/images/raffi-khatchadourian.jpg\">\n\t\t\t\t\t\t\t\t\t\t<div rel=\"me\" itemprop=\"url\" class=\"author-details-wrap\" >\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"author-image-wrap\">\n\t\t\t\t\t\t\t\t\t\t\t\t<img class=\"author-image\" src=\"https://projects.newyorker.com/interactive/2015/ai-story/archive/images/raffi-khatchadourian.jpg\" />\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"author-masthead has-bio\">\n\t\t\t\t\t\t\t\t\t\t\t\t<div class=\"contributor-info\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t<p>Raffi Khatchadourian became a staff writer at <em>The New Yorker</em> in 2008.</p>\n\t\t\t\t\t\t\t\t\t\t\t\t\t<ul class=\"author-links\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a class=\"more-link\" href=\"https://www.newyorker.com/contributors/raffi-khatchadourian\"  title=\"Raffi Khatchadourian\">More</a>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t\t\t\t\t\t\t</ul>\n\t\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\n\t\t\t\t\t\t\t\t\t</aside>\n\t\t\t\t\t\t\t\t</section>\n\n\t\t\t\t\t\t\t</footer>\n\t\t\t\t\t\t</article>\n\t\t\t\t\t</div>\n\n\t\t\t\t\t<!-- <script type=\"text/javascript\"\n                                          async src=\"https://projects.newyorker.com/interactive/2015/ai-story/manifest.js\"> -->\n</div></div></div><script type=\"text/javascript\">window.__PRELOADED_STATE__ = {\"componentConfig\":{\"AccountLinks\":{\"settings\":{\"hasSignOutSeparator\":false,\"signOutButtonLabel\":\"Sign out\"}},\"AccountProfilePage\":{\"settings\":{\"signOutButtonLabel\":\"Not you?\"}},\"ArticlePage\":{\"settings\":{\"hasLightbox\":true,\"hasSlideShow\":false,\"hideRecircMostPopular\":true,\"hasDynamicNewsletterSignup\":true,\"responsiveCartoonVariation\":\"Card\",\"isActionBarStickyLargeScreen\":true,\"actionBarLargeScreenVariation\":\"Stacked\",\"actionBarButtons\":[\"bookmark\"],\"hasReducedBackgroundSpacing\":true,\"showContributorImageOnMobile\":false,\"showIssueDateInArticle\":false,\"hasLinkbannerCrossSlideAnimation\":true,\"showEnhancedTextOverlay\":true,\"shouldRenderSlimFromJourney\":true}},\"BasePage\":{\"settings\":{\"hasFooterMargins\":true,\"showContentFooterWithHeaderOverride\":true,\"showNavWithHeaderOverride\":false}},\"BlockquoteEmbed\":{\"settings\":{\"hasParagraphMargin\":true,\"hasSmallMargins\":true,\"hasTopBorder\":false,\"shouldUseBodyColor\":true}},\"Caption\":{\"settings\":{\"shade\":\"light\",\"creditTextStyle\":\"caption\"}},\"Carousel\":{\"settings\":{\"hasCarouselSectionTitle\":true,\"hasDividerBelowCarouselSectionTitle\":false,\"carouselSummaryItemHasRule\":false}},\"CartoonPage\":{\"settings\":{\"cartoon\":{\"tagCloud\":{\"sectionHeader\":\"Keywords\"}}}},\"ChunkedArticleContent\":{\"variation\":\"WithAdRail\",\"settings\":{\"horizontalRuleStyle\":\"thin\",\"multiChunkRailStrategy\":\"alternate\",\"singleChunkRailStrategy\":\"split-in-three\"}},\"CNEVideoEmbed\":{\"variation\":\"StickyLiveStories\",\"settings\":{\"stickyTags\":[\"sticky-player\"]}},\"ConnectedNewsletterSubscribeForm\":{\"settings\":{\"dangerousSuccessHed\":\"You’re all set.\",\"dangerousSuccessDek\":\"Thank you for signing up for this newsletter.\",\"hasAlternateNewsletterStyle\":true}},\"ConnectedNavigation\":{\"settings\":{\"navPattern\":\"StackedNavigation\"}},\"ContributorBio\":{\"settings\":{\"avatarImageShape\":\"round\",\"shouldHideSocialIcons\":true,\"shouldHideTitle\":true,\"shouldUseTitleForContributorBio\":true}},\"Contributors\":{\"settings\":{\"maxContributors\":100}},\"ContentHeader\":{\"variation\":\"TextAboveLeftSmallWithRule\",\"settings\":{\"dividerType\":\"bottom\",\"rubricVariation\":\"Item\",\"hasContributorImageBackground\":true,\"hasDesktopTitleBlockDivider\":true,\"hideIssueDate\":true,\"hideIssueDatePipeSeparator\":true,\"persistentAsideAlign\":\"left-lead-asset\"}},\"GallerySlide\":{\"variation\":\"ItemLeft\"},\"HorizontalList\":{\"settings\":{\"sectionTitleVariation\":\"SoftDivider\",\"sectionTitleTypeToken\":\"subhed-section-collection\",\"summaryItemVariation\":\"TextBelowCenter\"}},\"Drawer\":{\"variation\":\"Left\",\"settings\":{\"overlayColor\":\"white\"}},\"ProductEmbed\":{\"variation\":\"ImageLeft\",\"settings\":{\"ctaText\":\"Shop Now\"}},\"ReviewPage\":{\"settings\":{\"ctaText\":\"Shop Now\"}},\"SearchPage\":{\"settings\":{\"placeholder\":\"Enter search terms\"}},\"SecondaryMenu\":{\"settings\":{\"hideSocialIcons\":true,\"showSearch\":true}},\"ExternalLinkEmbed\":{\"settings\":{\"hasArrowIcon\":false,\"textColumnSize\":\"large\"}},\"GalleryCarousel\":{\"settings\":{\"dangerousNavigationIcon\":\"\\u003Csvg focusable=\\\"false\\\"viewBox=\\\"0 0 40 24\\\"width=\\\"32\\\"height=\\\"32\\\"xmlns=\\\"http:\\u002F\\u002Fwww.w3.org\\u002F2000\\u002Fsvg\\\"\\u003E\\u003Cpath d=\\\"M39.56,12c-0.48-0.5-2.14-0.49-2.76-0.41c-0.9,0.12-1.61,0.19-2.51,0.1c-0.98-0.1-2.13-0.31-3.11-0.37 c-0.84-0.05-1.68,0.07-2.51-0.02c-1.1-0.11-1.98-0.35-3.07-0.07c-1.13,0.28-2.24-0.08-3.37-0.06c-1.23,0.02-2.46,0.16-3.7,0.13 c-1.09-0.03-2.18-0.2-3.27-0.24c-0.75-0.03-1.51,0.11-2.27,0.05c-2.32-0.2-4.5,0.4-6.86,0.14c-1.24-0.14-2.21,0.07-3.41,0.18 C1.82,11.5,0.86,11.12,0,11.39v0.91c0.22-0.05,0.45-0.09,0.69-0.1c0.76,0.04,1.73,0.08,2.49,0.12c0.59-0.06,1.17-0.24,1.8-0.25 c0.71-0.01,1.42,0.14,2.1,0.19c1.14,0,2.02-0.12,3.11-0.31c1.22-0.21,2.45,0.14,3.67,0.13c1.05,0,2.01-0.12,3.06,0.04 c0.93,0.06,1.92,0.06,2.82,0.08c0.64,0,1.28,0,1.92-0.01c0.42-0.04,0.67-0.11,1.26-0.11c0.54,0,1.18,0.11,1.68,0.19 c0.63,0.01,0.94-0.14,1.5-0.19c0.66,0,1.32-0.01,1.98-0.01c0.49,0.02,0.78,0.15,1.26,0.17c0.32-0.04,0.77,0.01,1.09-0.02 c0.34,0.03,0.68,0.05,1.02,0.08c0.88,0.11,2.02,0.38,2.96,0.37c0.86-0.06,2.36-0.05,3.22-0.12c0.48-0.01,0.89,0.11,1.33,0.14 c0.09-0.12-0.12-0.22,0-0.34C39.79,12.31,39.5,12.09,39.56,12z\\\" \\u002F\\u003E\\u003Cpath d=\\\"M38.69,11.97c0.25-0.17,0.51-0.35,0.76-0.52c0.02-0.02,0.05-0.04,0.08-0.06l0.47,0.75l-0.02-0.18 c0-0.14-0.04-0.16-0.04-0.3c-0.14-0.14-0.3-0.32-0.47-0.38c-0.13-0.1-0.26-0.19-0.39-0.29c-0.39-0.43-0.79-0.85-1.18-1.28 c-0.12-0.08-0.23-0.17-0.35-0.25C37.33,9.25,37,9.01,36.72,8.94c-0.63-0.16-1.73-1.71-2.19-2.19c-1.01-1.06-2.1-1.9-3.12-2.94 c-0.51-0.58-1.27-1.78-2.05-1.93c-0.31-0.31-0.63-0.61-0.95-0.92c-0.23-0.18-0.52-0.31-0.75-0.53c-0.13-0.13-0.26-0.47-0.43-0.49 c-0.13-0.02-0.28,0.22-0.33,0.32c-0.43,0.81,1.84,2.15,2.34,2.59c0.3,0.3,0.59,0.59,0.89,0.89c0.41,0.44,0.71,1.02,1.1,1.45 c0.19,0.2,0.45,0.3,0.67,0.44c0.4,0.26,0.85,0.55,1.21,0.91c0.91,0.91,1.79,1.99,2.71,2.89c0.59,0.57,1.27,0.89,1.85,1.46 c0.25,0.25,0.39,0.52,0.61,0.77c0.04,0.04,0.07,0.09,0.11,0.13C38.49,11.84,38.59,11.9,38.69,11.97z\\\" \\u002F\\u003E\\u003Cpath d=\\\"M29.36,22.07c0.28-0.27,0.77-0.43,1.08-0.75c0.35-0.36,0.62-0.79,0.96-1.17c0.83-0.91,1.83-1.62,2.7-2.48 c0.62-0.61,1.8-2.45,2.62-2.67c0.28-0.07,0.61-0.31,0.83-0.53c0.12-0.08,0.23-0.17,0.35-0.25c0.39-0.43,0.79-0.85,1.18-1.28 c0.13-0.1,0.26-0.19,0.39-0.29c0.17-0.05,0.33-0.24,0.47-0.38c0.07-0.07,0.07-0.09,0.07-0.19l-0.04-0.16l-0.44-0.54 c-0.02,0.02-0.05,0.04-0.08,0.06c-0.25,0.17-0.51,0.35-0.76,0.52c-0.1,0.07-0.2,0.14-0.3,0.21c-0.12,0.11-0.21,0.23-0.29,0.35 c-0.13,0.18-0.25,0.36-0.43,0.54c-0.58,0.58-1.26,0.89-1.85,1.46c-0.92,0.89-1.8,1.97-2.71,2.89c-0.36,0.36-0.81,0.64-1.21,0.91 c-0.22,0.14-0.48,0.24-0.67,0.44c-0.39,0.42-0.69,1-1.1,1.45c-0.3,0.3-0.59,0.59-0.89,0.89c-0.5,0.43-2.77,1.77-2.34,2.59 c0.05,0.1,0.2,0.34,0.33,0.32c0.17-0.02,0.3-0.36,0.43-0.49c0.23-0.22,0.51-0.34,0.75-0.53C28.73,22.68,29.04,22.38,29.36,22.07z\\\" \\u002F\\u003E\\u003C\\u002Fsvg\\u003E\",\"showPublishedDate\":true}},\"GalleryEmbed\":{\"settings\":{\"shouldCycleSlides\":true,\"showAds\":false,\"midGalleryAdsLimit\":1}},\"IframeEmbed\":{\"variation\":\"WithAudioTag\",\"settings\":{\"audioTagIconTitle\":\"Audio available\",\"audioTagIncludeUrls\":[\"audm.\"],\"shouldAllowFullScreen\":true,\"shouldValidateAudmPlayer\":true}},\"LinkBanner\":{\"settings\":{\"hideHed\":false,\"hideMobileMarqueeImage\":true,\"shouldReplaceWithDropdown\":true}},\"NewsletterSubscribeContent\":{\"variation\":\"FullViewPort\",\"settings\":{\"shouldHaveExtraPadding\":true,\"hasAlternateNewsletterStyle\":true,\"buttonLabel\":\"Complete sign-up\"}},\"RecircList\":{\"variation\":\"FourUp\",\"settings\":{\"applicationID\":\"the-new-yorker-article-bottom-recirc\",\"excludeCategories\":[\"functional-tags\\u002Fnoriver\",\"\\u002Fchannels\\u002Fhumor\\u002Fborowitz-report\"],\"pageSize\":4,\"shouldHideRubric\":false,\"strategy\":\"similar\"}},\"SectionTitle\":{\"variation\":\"SoftDivider\"},\"SignInModal\":{\"settings\":{\"hasBlueGoogleSignInButton\":true,\"hasRoundedCornersButtons\":true}},\"SiteFooter\":{\"variation\":\"LinkDense\",\"settings\":{\"hideTagline\":true,\"showOneTrustButton\":true}},\"SocialIcons\":{\"variation\":\"Circular\",\"settings\":{\"icons\":\"thinner\"}},\"StackedNavigation\":{\"variation\":\"FixedHeaderLargeLogo\",\"settings\":{\"navigationHideStrategy\":\"delta\",\"primaryNavigationSize\":\"large\",\"profileLinkLabel\":\"My Profile\",\"hideNavWhenLinkBannerEnabled\":true}},\"SummaryItem\":{\"settings\":{\"hasRule\":false,\"maxHedLines\":null,\"shouldHideDangerousDek\":false,\"shouldHideIcon\":true,\"showCommaAsideContributorName\":true}},\"SmartItem\":{\"settings\":{\"maxContributors\":2,\"maxContributorsNames\":3}},\"tempHomepageRelated\":{\"settings\":{\"applicationID\":\"the-new-yorker-verso-hp-trending\",\"pageSize\":4,\"strategy\":\"popular\"}},\"Toggle\":{\"variation\":\"Triangle\"},\"GroupCallout\":{\"settings\":{\"heading\":{\"article\":\"Related Stories\"}}},\"GenericCallout\":{\"settings\":{\"smallWidth\":\"wide\",\"mediumWidth\":\"narrow\"}},\"ContributorHeader\":{\"settings\":{\"avatarImageShape\":\"round\"}},\"MultiPackages\":{\"settings\":{\"hasReducedMargin\":true,\"shouldUseConstrainedParagraph\":true,\"tickerMarginTopType\":\"none\",\"hasVersoFeaturesReducedMargin\":true,\"isPaddingRequired\":true,\"shouldUseGridForEmbed\":false,\"summaryCollageOneVariation\":\"HeroFeature\"}},\"SummaryCollageFive\":{\"settings\":{\"summaryItemRubricVariation\":\"DiscoveryItem\"}},\"SummaryCollageOne\":{\"settings\":{\"shouldHideDangerousDek\":false,\"shouldHideDangerousHed\":false,\"shouldHidePublishDate\":true,\"summaryItemRubricVariation\":\"DiscoveryItem\",\"summaryItemVariation\":\"SideBySideCenterImageLeft\",\"shouldUseHedFeatureToken\":true}},\"SummaryCollageThree\":{\"settings\":{\"summaryItemRubricVariation\":\"DiscoveryItem\",\"desktopFeatureColSpan\":\"use9\",\"desktopFeatureColSpanForVideo\":\"use9\"}},\"SummaryCollectionGrid\":{\"settings\":{\"summaryItemRubricVariation\":\"DiscoveryItem\",\"summaryItemVariation\":\"DenseDesktopStack\",\"sectionTitleVariation\":\"SoftDivider\",\"nativeAdIndex\":3}},\"SummaryRiver\":{\"settings\":{\"summaryItemRubricVariation\":\"DiscoveryItem\",\"summaryItemVariation\":\"DenseDesktopThirds\",\"showRecircMostPopularInAsideWithRail\":true,\"hasRule\":true,\"gridColSpanValue\":7,\"hideBottomBorders\":false}},\"SummaryCollectionRow\":{\"variation\":\"FourColumnsTighterBylineSpaceWithDek\",\"settings\":{\"hasBackgroundColor\":false,\"sectionTitleVariation\":\"SoftDivider\",\"summaryItemVariation\":\"SideBySideDense\",\"shouldHideDangerousDek\":true,\"summaryItemRubricVariation\":\"DiscoveryItem\",\"shouldShowAllContentWhenDense\":true}},\"SummarySpotlight\":{\"settings\":{\"summaryItemVariation\":\"SideBySideDense\",\"summaryItemVariationWithNoAssest\":\"TextBelowDesktopOnlyNoAsset\",\"actionBarConfig\":{\"audio\":{\"test\":\"testing\",\"defaultLabel\":\"audioPrimaryLabel\",\"defaultIcon\":\"\\u003Csvg xmlns=\\\"http:\\u002F\\u002Fwww.w3.org\\u002F2000\\u002Fsvg\\\" viewBox=\\\"0 0 21.5 21.5\\\"\\u003E\\u003Cdefs\\u003E\\u003Cstyle\\u003E.cls-1{fill:#231f20;stroke:#231f20;stroke-miterlimit:10;stroke-width:0.5px;}\\u003C\\u002Fstyle\\u003E\\u003C\\u002Fdefs\\u003E\\u003Cg id=\\\"icons\\\"\\u003E\\u003Cpath class=\\\"cls-1\\\" d=\\\"M39.75,29.5A10.51,10.51,0,0,0,29.25,40v5a5.51,5.51,0,0,0,5.5,5.5h.5V48.93A2,2,0,0,0,36.75,47V43a2,2,0,0,0-1.5-1.93V39.5h-.5a5.49,5.49,0,0,0-4.5,2.35V40a9.5,9.5,0,0,1,19,0v1.85a5.49,5.49,0,0,0-4.5-2.35h-.5v1.57A2,2,0,0,0,42.75,43v4a2,2,0,0,0,1.5,1.93V50.5h.5a5.51,5.51,0,0,0,5.5-5.5V40A10.51,10.51,0,0,0,39.75,29.5Zm-5.5,11v8.94a4.5,4.5,0,0,1,0-8.94Zm11,8.94V40.53a4.5,4.5,0,0,1,0,8.94Z\\\" transform=\\\"translate(-29 -29.25)\\\"\\u002F\\u003E\\u003C\\u002Fg\\u003E\\u003C\\u002Fsvg\\u003E\"}}}},\"TagCloud\":{\"settings\":{\"sectionHeader\":\"More:\"}},\"Ticker\":{\"variation\":\"Utility\"},\"VersoEmbed\":{\"settings\":{\"sectionTitleVariation\":\"SoftDivider\"}},\"VersoFeatures\":{\"settings\":{\"hasBottomBorderOnLargeScreen\":false,\"hasBorderOnMobileAndTabletOnly\":true,\"shouldTreatTabletLikeMobile\":true,\"shouldUseMediumBreakpoint\":true,\"shouldHideCarouselBylines\":false,\"hasMinimalVerticalSpacing\":false,\"hasCarouselSectionTitle\":true,\"hasDividerBelowCarouselSectionTitle\":false,\"shouldOverrideTypeToken\":true,\"carouselSummaryItemVariation\":\"TextBelowLeft\",\"carouselSummaryItemHasRule\":false,\"summaryItemVariation\":\"DenseDesktopStack\"}},\"PodcastListingPage\":{\"settings\":{\"podcastVariation\":\"PodcastsWithoutAppleEmbed\",\"sideBySideVerticalAlign\":\"top\",\"dangerousHed\":\"Podcasts\",\"subHed\":\"All of The New Yorker’s podcasts, featuring politics, culture, short stories, and more.\",\"shouldHideRubric\":true,\"hasBorder\":true}},\"PodcastDetailedPage\":{\"settings\":{\"contentHeaderVariation\":\"PodcastContentHeader\",\"sectionTitleVariation\":\"SoftDivider\",\"primaryCTALabel\":\"Start Listening\",\"relatedArticleHed\":\"Related Articles\"}},\"ForYou\":{\"settings\":{\"pageSize\":4,\"strategy\":\"popular\",\"canonicalUrl\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002F\"}},\"ActionBar\":{\"settings\":{\"bookmarkCompletionAriaLabel\":\"Saved successfully. To revisit this, select My Account, then View Saved Items. Press Escape to dismiss tooltip.\",\"signInMessage\":\"After signing in, you can save and easily revisit them on any device.\",\"actionBarConfig\":{\"bookmark\":{\"defaultIcon\":\"\\u003Csvg class=\\\"icon icon-bookmark\\\" width=\\\"24\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" fill=\\\"none\\\" xmlns=\\\"http:\\u002F\\u002Fwww.w3.org\\u002F2000\\u002Fsvg\\\"\\u003E\\u003Ctitle\\u003ESave this story\\u003C\\u002Ftitle\\u003E\\u003Cpath class=\\\"icon-bookmark-fill\\\" d=\\\"M20 23.9508L12.5 19.7312L5 23.9508V2.95081H14V3.93211H6V22.1845L12.5 18.5536L19 22.1845V8.83866H20V23.9508Z\\\"\\u003E\\u003C\\u002Fpath\\u003E\\u003Cpath class=\\\"icon-bookmark-fill\\\" d=\\\"M23 3H20V0H19V3H16V4H19V7H20V4H23V3Z\\\"\\u003E\\u003C\\u002Fpath\\u003E\\u003C\\u002Fsvg\\u003E\",\"afterActionIcon\":\"\\u003Csvg class=\\\"icon icon-bookmark icon-bookmark--activated\\\" width=\\\"24\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" fill=\\\"none\\\" xmlns=\\\"http:\\u002F\\u002Fwww.w3.org\\u002F2000\\u002Fsvg\\\"\\u003E\\u003Ctitle\\u003ESaved\\u003C\\u002Ftitle\\u003E\\u003Cpath class=\\\"icon-bookmark-fill icon-bookmark--activated-outline\\\" d=\\\"M20 24L12.5 19.7804L5 24V3H20V24ZM12.5 18.6028L19 22.2336V3.98131H6V22.2336L12.5 18.6028Z\\\"\\u003E\\u003C\\u002Fpath\\u003E\\u003Cpath class=\\\"icon-bookmark-fill icon-bookmark--activated-checkmark\\\" d=\\\"M11.5541 13.7033L9 11.2033L9.66216 10.5552L11.5541 12.407L15.3378 8.70331L16 9.35146L11.5541 13.7033Z\\\"\\u003E\\u003C\\u002Fpath\\u003E\\u003C\\u002Fsvg\\u003E\",\"defaultLabel\":\"bookmarkSaveThisStory\",\"afterActionLabel\":\"bookmarkSaved\",\"loaderIcon\":\"\\u003Csvg class=\\\"icon icon-loader\\\" width=\\\"24\\\" height=\\\"24\\\" viewBox=\\\"0 0 24 24\\\" fill=\\\"none\\\" xmlns=\\\"http:\\u002F\\u002Fwww.w3.org\\u002F2000\\u002Fsvg\\\"\\u003E\\n\\u003Cpath class=\\\"icon-loader-track\\\" fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20ZM12 21C16.9706 21 21 16.9706 21 12C21 7.02944 16.9706 3 12 3C7.02944 3 3 7.02944 3 12C3 16.9706 7.02944 21 12 21Z\\\" \\u002F\\u003E\\n\\u003Cpath class=\\\"icon-loader-progress\\\" d=\\\"M12 21C7.07368 21 3 16.9263 3 12H3.94737C3.94737 16.4526 7.54737 20.0526 12 20.0526C16.4526 20.0526 20.0526 16.4526 20.0526 12C20.0526 7.54737 16.4526 3.94737 12 3.94737V3C16.9263 3 21 7.07368 21 12C21 16.9263 16.9263 21 12 21Z\\\" \\u002F\\u003E\\n\\u003CanimateTransform attributeName=\\\"transform\\\" attributeType=\\\"XML\\\" type=\\\"rotate\\\" dur=\\\"1s\\\" from=\\\"0 0 0\\\" to=\\\"360 0 0\\\" repeatCount=\\\"indefinite\\\"\\u002F\\u003E\\n\\u003C\\u002Fsvg\\u003E\"}}}},\"SlimNewsletterWrapper\":{\"settings\":{\"isNewsletterSlim\":false,\"nodeForNewsletterPosition\":\"top\",\"patternType\":\"utility\",\"shouldFooterBehaveLikeInlineNewsletter\":false,\"shouldShowFooterNewsletter\":false,\"shouldShowSlimNewsletter\":true,\"shouldUseInlineNewsletterModules\":false,\"showToggleForLoggedInUser\":false}},\"StickyMidContent\":{\"settings\":{\"isStickyEnabled\":true}}},\"config\":{\"account\":{\"bookmark\":{\"emptyBookmarksDek\":\"\\u003Cimg class=account-bookmark-page__dek-image src=\\\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5dd42947386b440009670bfa\\u002Fmaster\\u002Fw_250,h_250\\u002FTNYinnovationsMfinal.png\\\" alt=\\\"Spot illustration on empty bookmarks page\\\"\\u003E\\u003Cbr\\u003EYou have not saved any \\u003Cspan class=\\\"account-bookmark-page__line-break-empty-bookmarks-dek\\\"\\u003E stories recently.\\u003C\\u002Fspan\\u003E\"},\"linking\":{\"dangerousDek\":\"\\u003Cp\\u003EEnter your details to connect your subscription. This will enable your unlimited access to \\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002F\\\"\\u003Enewyorker.com\\u003C\\u002Fa\\u003E and the Today app.\\u003C\\u002Fp\\u003E\\u003Cimg class=\\\"account-linking-page__dek-image\\\" src=\\\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5d7bdab453b1ff00082fab03\\u002Fmaster\\u002Fpass\\u002Fshipping_label.png\\\" alt=\\\"New Yorker magazine shipping label highlighting subscription number and zip code\\\"\\u003E\",\"skipAccountLinking\":true},\"oidcProvider\":{\"clientID\":\"condenast.identity.551db804934e94baa14c434240328bf2\",\"signOutLinkActive\":false},\"profileContent\":{\"bookmarksModulePageSize\":5,\"dangerousDek\":\"You are signed in as\",\"dangerousHed\":\"My Account\",\"shouldUseAlternativeStyle\":false}},\"accountLinks\":{\"nodes\":[{\"attributes\":{\"name\":\"\\u002Fmanage-account\"},\"index\":0,\"text\":\"Manage account\",\"url\":\"\\u002Faccount\\u002Fprofile\"},{\"attributes\":{\"name\":\"\\u002Fview-saved-stories\"},\"index\":1,\"text\":\"View saved stories\",\"url\":\"\\u002Faccount\\u002Fsaved\"}]},\"adsConfig\":{\"displayHighCadenceWordCountDesktop\":200,\"displayHighCadenceWordCountMobile\":50,\"displayWordCountDesktop\":900,\"displayWordCountMobile\":300,\"galleryDisplayInitialMidContentPositionDesktop\":2,\"galleryDisplayInitialMidContentPositionMobile\":2,\"galleryDisplayMidContentCadenceDesktop\":2,\"galleryDisplayMidContentCadenceMobile\":2,\"galleryEmbedMidGalleryAdCadence\":6,\"galleryNativeInContentCadence\":0,\"liveStoryAdInsertRatio\":4,\"nativeInContentCadence\":0,\"shoulHoldStickyHeroAd\":false},\"beOp\":{},\"brand\":{\"deprecatedSlug\":\"the-new-yorker\",\"organizationId\":\"4gKgcFDnpSvUqozcC7TYUEcCiDJv\",\"organizationSlug\":\"the-new-yorker\",\"publicDisplayName\":\"The New Yorker\",\"rootBrand\":\"The New Yorker\",\"shortCode\":\"tny\"},\"carousel\":{},\"community\":{},\"commenting\":{\"coralHost\":\"newyorker.coral.coralproject.net\"},\"contactLinks\":{\"heading\":\"More\",\"nodes\":[{\"attributes\":{\"name\":\"\\u002Fcustomer-care\"},\"isExternal\":true,\"index\":0,\"text\":\"Customer Care\",\"url\":\"http:\\u002F\\u002Fw1.buysub.com\\u002Fservlet\\u002FCSGateway?cds_mag_code=NYR\"},{\"attributes\":{\"name\":\"\\u002Fshop-the-new-yorker\"},\"isExternal\":true,\"index\":1,\"text\":\"Shop The New Yorker\",\"url\":\"https:\\u002F\\u002Fstore.newyorker.com\"},{\"attributes\":{\"name\":\"\\u002Fbuy-covers-and-cartoons\"},\"isExternal\":true,\"index\":2,\"text\":\"Buy Covers and Cartoons\",\"url\":\"https:\\u002F\\u002Fcondenaststore.com\\u002Fart\\u002Fnew+yorker+covers\"},{\"attributes\":{\"name\":\"\\u002Fconde-nast-store\"},\"isExternal\":true,\"index\":3,\"text\":\"Condé Nast Store\",\"url\":\"https:\\u002F\\u002Fcondenaststore.com\\u002Fconde-nast-brand\\u002Fthenewyorker\"},{\"attributes\":{\"name\":\"\\u002Fdigital-access\"},\"isExternal\":false,\"index\":4,\"text\":\"Digital Access\",\"url\":\"\\u002Fdigital-editions\"},{\"attributes\":{\"name\":\"\\u002Fnewsletters\"},\"isExternal\":false,\"index\":5,\"text\":\"Newsletters\",\"url\":\"\\u002Fnewsletter\"},{\"attributes\":{\"name\":\"\\u002Fjigsaw-puzzle\"},\"isExternal\":false,\"index\":6,\"text\":\"Jigsaw Puzzle\",\"url\":\"\\u002Fjigsaw\"},{\"attributes\":{\"name\":\"\\u002Frss\"},\"isExternal\":false,\"index\":7,\"text\":\"RSS\",\"url\":\"\\u002Fabout\\u002Ffeeds\"}]},\"contentSystem\":{\"contentApiDomain\":\"tsugu-service-tny-content.gp-prod-na-0.conde.digital\",\"copilotCode\":\"tny\",\"copilotName\":\"tny-services\",\"copilotRegion\":\"us\",\"mediaDomain\":\"media.newyorker.com\",\"pipelineAliases\":[\"newyorker\",\"the new yorker\",\"thenewyorker\",\"the-new-yorker\"],\"pipelineEnabled\":false},\"disclaimer\":{\"affiliateDisclaimer\":\"All products are independently selected by our editors. If you buy something, we may earn an affiliate commission.\",\"hideProductDisclaimer\":true},\"dynamicCapability\":{},\"eventBanner\":{},\"favicon\":{\"isEnabled\":false},\"featureFlags\":{\"enableAccounts\":true,\"enableActionBar\":true,\"enableAutopilotBundleComponent\":false,\"enableBookmarking\":true,\"enableComments\":false,\"enableConsent\":true,\"enableDropcap\":true,\"enableEnhancedArticleHeader\":true,\"enableInfinityId\":true,\"enableJourneyPayment\":true,\"enableLayoutPresets\":true,\"enableMediaOverridesV2\":false,\"personalizeRecircList\":true,\"hideContributorBio\":false,\"hideTagCloud\":false},\"footerLinks\":{\"heading\":\"Sections\",\"nodes\":[{\"index\":0,\"isExternal\":false,\"text\":\"News\",\"url\":\"\\u002Fnews\"},{\"index\":1,\"isExternal\":false,\"text\":\"Books & Culture\",\"url\":\"\\u002Fculture\"},{\"index\":2,\"isExternal\":false,\"text\":\"Fiction & Poetry\",\"url\":\"\\u002Ffiction-and-poetry\"},{\"index\":3,\"isExternal\":false,\"text\":\"Humor & Cartoons\",\"url\":\"\\u002Fhumor\"},{\"index\":4,\"isExternal\":false,\"text\":\"Magazine\",\"url\":\"\\u002Fmagazine\"},{\"index\":5,\"isExternal\":false,\"text\":\"Crossword\",\"url\":\"\\u002Fcrossword-puzzles-and-games\"},{\"index\":6,\"isExternal\":false,\"text\":\"Video\",\"url\":\"\\u002Fvideo\"},{\"index\":7,\"isExternal\":false,\"text\":\"Podcasts\",\"url\":\"\\u002Fpodcast\"},{\"index\":8,\"isExternal\":false,\"text\":\"Archive\",\"url\":\"\\u002Farchive\"},{\"index\":9,\"isExternal\":false,\"text\":\"Goings On\",\"url\":\"\\u002Fgoings-on\"}]},\"fourD\":{\"enableFourdUser\":true},\"googleCategoryMap\":{},\"googleTagManager\":{\"tag\":\"NX5LSK3\",\"token\":\"OC-EFKSOTAiYDu96SkRSeA~1236\"},\"growthbook\":{\"enableClient\":true,\"apiHost\":\"https:\\u002F\\u002Fvariants.conde.digital\",\"clientSdkKey\":\"sdk-lBIhOJ5Er4pNjLkp\"},\"legalese\":{},\"mapbox\":{},\"marketing\":{\"permissionsPrimaryDek\":\"\\u003Cem\\u003EThe New Yorker\\u003C\\u002Fem\\u003E may e-mail me regarding the benefits of my subscription and special promotions.\",\"permissionsThirdPartyDek\":\"\\u003Cem\\u003EThe New Yorker\\u003C\\u002Fem\\u003E may e-mail me regarding products or services from select companies that may be of interest to me.\"},\"marketingConsentList\":{\"nodes\":[{\"marketingConsentId\":\"657fe98d6be3b0fdf26b81af\",\"name\":\"FIRST_PARTY\",\"dangerousDek\":\"The New Yorker may e-mail me regarding the benefits of my subscription and special promotions.\",\"isFeatured\":true},{\"marketingConsentId\":\"657fe98d6be3b0fdf26b81b0\",\"name\":\"THIRD_PARTY\",\"dangerousDek\":\"The New Yorker may e-mail me regarding products or services from select companies that may be of interest to me.\",\"isFeatured\":true}]},\"martech\":{\"isEnabled\":true,\"products\":[\"newyorker.com:basic\"],\"redirectURL\":\"\\u002F\",\"isAccessCookieEnabled\":true},\"newsletter\":{\"manage\":{\"authenticatedButtonLabel\":\"Update E-mail Preferences\",\"buttonLabel\":\"Sign up\",\"buttonLabelOnSuccess\":\"Subscribed\",\"dangerousDek\":\"Select the newsletters you’d like to receive. Then, add your e-mail to sign up.\",\"dangerousDisclaimer\":\"\\u003Cp\\u003EBy signing up, you agree to our \\u003Ca href=\\\"https:\\u002F\\u002Fwww.condenast.com\\u002Fuser-agreement\\\" rel=\\\"nofollow noopener noreferrer\\\" target=\\\"_blank\\\"\\u003EUser Agreement\\u003C\\u002Fa\\u003E and \\u003Ca href=\\\"https:\\u002F\\u002Fwww.condenast.com\\u002Fprivacy-policy\\\" rel=\\\"nofollow noopener noreferrer\\\" target=\\\"_blank\\\"\\u003EPrivacy Policy & Cookie Statement\\u003C\\u002Fa\\u003E. This site is protected by reCAPTCHA and the Google\\u003Ca href=\\\"https:\\u002F\\u002Fpolicies.google.com\\u002Fprivacy\\\" rel=\\\"nofollow noopener noreferrer\\\" target=\\\"_blank\\\"\\u003E Privacy Policy\\u003C\\u002Fa\\u003E and\\u003Ca href=\\\"https:\\u002F\\u002Fpolicies.google.com\\u002Fterms\\\" rel=\\\"nofollow noopener noreferrer\\\" target=\\\"_blank\\\"\\u003E Terms of Service\\u003C\\u002Fa\\u003E apply.\\u003C\\u002Fp\\u003E\",\"dangerousSuccessDisclaimer\":\"\\u003Cp\\u003ESomething else in mind? \\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fnewsletters\\\"\\u003EManage newsletters\\u003C\\u002Fa\\u003E\\u003C\\u002Fp\\u003E\",\"dangerousHed\":\"Get the best of \\u003Ci\\u003EThe New Yorker\\u003C\\u002Fi\\u003E in your in-box.\",\"dangerousText\":\"\\u003Cp\\u003EBy signing up, you agree to our \\u003Ca href=\\\"https:\\u002F\\u002Fwww.condenast.com\\u002Fuser-agreement\\\"\\u003EUser Agreement\\u003C\\u002Fa\\u003E and \\u003Ca href=\\\"https:\\u002F\\u002Fwww.condenast.com\\u002Fprivacy-policy\\\"\\u003EPrivacy Policy & Cookie Statement\\u003C\\u002Fa\\u003E.\\u003C\\u002Fp\\u003E\",\"dangerousLegend\":\"Newsletters\",\"hasAlternateNewsletterStyle\":true,\"heroImage\":{},\"image\":{\"altText\":\"The New Yorker\",\"imageAttributes\":{\"aria-hidden\":true,\"role\":\"image\"}},\"placeholder\":\"E-mail address\",\"sailthruCopy\":{\"authenticatedButtonLabel\":\"Update preferences\",\"dangerousDek\":\"Check the box of each newsletter you would like to receive.\",\"dangerousHed\":\"Update your e-mail preferences\"},\"seoTitle\":\"Newsletters\",\"seoDescription\":\"Get the best of The New Yorker in your in-box.\",\"showListInColumns\":true,\"signedInDek\":\"\\u003Cp\\u003EYour e-mail: \\u003Cb\\u003E{{email}}\\u003C\\u002Fb\\u003E. Manage newsletters in \\u003Ca href=\\\"{{profile}}\\\"\\u003EMy Account\\u003C\\u002Fa\\u003E.\\u003C\\u002Fp\\u003E\\u003Cp\\u003ENot you? \\u003Ca href=\\\"\\u002Fauth\\u002Fend?redirectURL={{signInURL}}\\\"\\u003ESwitch accounts\\u003C\\u002Fa\\u003E.\\u003C\\u002Fp\\u003E\",\"signedUpDek\":\"\\u003Cp\\u003EYour e-mail: \\u003Cb\\u003E{{email}}\\u003C\\u002Fb\\u003E. \\u003Ca href='\\u002Fauth\\u002Finitiate?redirectURL={{newsletterURL}}'\\u003E Sign in or create an account\\u003C\\u002Fa\\u003E to manage your preferences.\\u003C\\u002Fp\\u003E\",\"signUpSuccessDek\":\"To manage current newsletters for \\u003Cb\\u003E{{email}}\\u003C\\u002Fb\\u003E, \\u003Ca href='\\u002Fauth\\u002Finitiate?redirectURL=\\u002Fnewsletter'\\u003Esign in or create an account\\u003C\\u002Fa\\u003E.\",\"textFieldAssistiveSubtext\":\"\",\"textFieldLabel\":\"E-mail address\"},\"market\":\"US\",\"provider\":\"sailthru\",\"showAsCheckboxesOnProfile\":true,\"showSubscribedOnly\":true,\"subscribe\":{\"dangerousDisclaimer\":\"\\u003Cp\\u003EBy signing up, you agree to our \\u003Ca href=\\\"https:\\u002F\\u002Fwww.condenast.com\\u002Fuser-agreement\\\" rel=\\\"nofollow noopener noreferrer\\\" target=\\\"_blank\\\"\\u003EUser Agreement\\u003C\\u002Fa\\u003E and \\u003Ca href=\\\"https:\\u002F\\u002Fwww.condenast.com\\u002Fprivacy-policy\\\" rel=\\\"nofollow noopener noreferrer\\\" target=\\\"_blank\\\"\\u003EPrivacy Policy & Cookie Statement\\u003C\\u002Fa\\u003E. This site is protected by reCAPTCHA and the Google\\u003Ca href=\\\"https:\\u002F\\u002Fpolicies.google.com\\u002Fprivacy\\\" rel=\\\"nofollow noopener noreferrer\\\" target=\\\"_blank\\\"\\u003E Privacy Policy\\u003C\\u002Fa\\u003E and\\u003Ca href=\\\"https:\\u002F\\u002Fpolicies.google.com\\u002Fterms\\\" rel=\\\"nofollow noopener noreferrer\\\" target=\\\"_blank\\\"\\u003E Terms of Service\\u003C\\u002Fa\\u003E apply.\\u003C\\u002Fp\\u003E\",\"dangerousText\":\"\\u003Cp\\u003EBy signing up, you agree to our \\u003Ca href=\\\"https:\\u002F\\u002Fwww.condenast.com\\u002Fuser-agreement\\\"\\u003EUser Agreement\\u003C\\u002Fa\\u003E and \\u003Ca href=\\\"https:\\u002F\\u002Fwww.condenast.com\\u002Fprivacy-policy\\\"\\u003EPrivacy Policy & Cookie Statement\\u003C\\u002Fa\\u003E.\\u003C\\u002Fp\\u003E\"},\"unsubscribe\":{\"dangerousHed\":\"Are you sure?\",\"dangerousSuccessDek\":\"Changed your mind? \\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fnewsletters\\\"\\u003ESubscribe\\u003C\\u002Fa\\u003E to our newsletters.\",\"dangerousText\":\"\\u003Cp\\u003EView our \\u003Ca href=\\\"https:\\u002F\\u002Fwww.condenast.com\\u002Fuser-agreement\\u002F\\\"\\u003EUser Agreement\\u003C\\u002Fa\\u003E and \\u003Ca href=\\\"http:\\u002F\\u002Fwww.condenast.com\\u002Fprivacy-policy\\u002F#privacypolicy\\\"\\u003EPrivacy Policy\\u003C\\u002Fa\\u003E\\u003C\\u002Fp\\u003E\",\"singleDangerousDek\":\"If, alternatively, you would like to adjust your&nbsp;e&#8209;mail&nbsp;preferences, please \\u003Ca href=\\\"\\u002Faccount\\u002Fsign-in\\\"\\u003Esign in\\u003C\\u002Fa\\u003E.\"},\"useManageForSubscribe\":true},\"newsletterList\":[{\"category\":\"The New Yorker Newsletter\",\"frequencyBadge\":\"Daily\",\"index\":0,\"previewURL\":\"\\u002Fnewsletter\\u002Fdaily\",\"slimTags\":[\"business\",\"election-2020\",\"campaign-chronicles\",\"our-columnists\",\"the-political-scene\"],\"slug\":\"daily\",\"name\":\"Daily\",\"newsletterId\":\"217\",\"socialDataSrc\":{\"dek\":\"The best of The New Yorker today, including essential reads, fiction, humor, and a peek behind the scenes of our biggest stories.\",\"hed\":\"Sign up for The New Yorker's daily newsletter\",\"image\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64bfa48e3f0ea61101b7c89d\\u002Fmaster\\u002Fpass\\u002Fsocial-image-daily.jpg\"},\"manage\":{},\"subscribe\":{\"buttonLabel\":\"Sign up\",\"dangerousDek\":\"Our flagship newsletter highlights the best of \\u003Ci\\u003EThe New Yorker\\u003C\\u002Fi\\u003E, including top stories, fiction, humor, and podcasts.\",\"dangerousHed\":\"Daily\",\"headerImage\":{\"altText\":\"Daily logo\",\"sources\":{\"sm\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c402531777c13052d66\\u002Fmaster\\u002Fpass\\u002Ficon_daily.jpg\"}}},\"image\":{\"altText\":\"Daily\",\"sources\":{\"md\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c402531777c13052d66\\u002Fmaster\\u002Fpass\\u002Ficon_daily.jpg\",\"width\":1024},\"lg\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c402531777c13052d66\\u002Fmaster\\u002Fpass\\u002Ficon_daily.jpg\",\"width\":1280},\"xl\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c402531777c13052d66\\u002Fmaster\\u002Fpass\\u002Ficon_daily.jpg\",\"width\":1600}}},\"placeholder\":\"E-mail address\",\"newsletterTemplateURL\":\"https:\\u002F\\u002Flink.newyorker.com\\u002Fview\\u002F5df1007997a21448cc7f66876663723ed7b5aea90b032b2c\\u002F3e45cd90\"},\"unsubscribe\":{}},{\"category\":\"The New Yorker Newsletter\",\"frequencyBadge\":\"Weekly\",\"index\":1,\"previewURL\":\"\\u002Fnewsletter\\u002Fweekly\",\"slimTags\":[\"magazine\",\"\"],\"slug\":\"weekly\",\"name\":\"Weekly\",\"newsletterId\":\"248840\",\"socialDataSrc\":{\"dek\":\"Your guide to the latest magazine and our biggest stories of the week, plus highlights from podcasts, humor, and more.\",\"hed\":\"Sign up for The New Yorker's weekly newsletter\",\"image\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64bfa432ef09d4a0e04cb7bf\\u002Fmaster\\u002Fpass\\u002Fsocial-image-weekly.jpg\"},\"manage\":{},\"subscribe\":{\"buttonLabel\":\"Sign up\",\"dangerousDek\":\"Enjoy our flagship newsletter as a digest delivered once a week.\",\"dangerousHed\":\"Weekly\",\"headerImage\":{\"altText\":\"Weekly logo\",\"sources\":{\"sm\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c437a7e243bdeb479f2\\u002Fmaster\\u002Fpass\\u002Ficon_weekly-rd.jpg\"}}},\"image\":{\"altText\":\"Weekly\",\"sources\":{\"md\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c437a7e243bdeb479f2\\u002Fmaster\\u002Fpass\\u002Ficon_weekly-rd.jpg\",\"width\":1024},\"lg\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c437a7e243bdeb479f2\\u002Fmaster\\u002Fpass\\u002Ficon_weekly-rd.jpg\",\"width\":1280},\"xl\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c437a7e243bdeb479f2\\u002Fmaster\\u002Fpass\\u002Ficon_weekly-rd.jpg\",\"width\":1600}}},\"placeholder\":\"E-mail address\",\"newsletterTemplateURL\":\"https:\\u002F\\u002Flink.newyorker.com\\u002Fview\\u002F5df1007997a21448cc7f6687666329c635d9a74bd30c651e\\u002F3e45cd90\"},\"unsubscribe\":{}},{\"category\":\"Insight & Analysis\",\"frequencyBadge\":\"Twice weekly\",\"index\":2,\"previewURL\":\"\\u002Fnewsletter\\u002Fclassics\",\"slimTags\":[\"magazine\"],\"slug\":\"classics\",\"name\":\"New Yorker Classics\",\"newsletterId\":\"248782\",\"socialDataSrc\":{\"dek\":\"Gems from the archive that deliver timeless pleasures with fresh insight.\",\"hed\":\"Sign up for The New Yorker Classics newsletter\",\"image\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64bfa48269af7f8649141af1\\u002Fmaster\\u002Fpass\\u002Fsocial-image-classics.jpg\"},\"manage\":{},\"subscribe\":{\"buttonLabel\":\"Sign up\",\"dangerousDek\":\"Gems from the archive that deliver timeless pleasures with fresh insight.\",\"dangerousHed\":\"New Yorker Classics\",\"headerImage\":{\"altText\":\"New Yorker Classics logo\",\"sources\":{\"sm\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c3f9e87508ed764faf5\\u002Fmaster\\u002Fpass\\u002Ficon_classics.jpg\"}}},\"image\":{\"altText\":\"New Yorker Classics\",\"sources\":{\"md\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c3f9e87508ed764faf5\\u002Fmaster\\u002Fpass\\u002Ficon_classics.jpg\",\"width\":1024},\"lg\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c3f9e87508ed764faf5\\u002Fmaster\\u002Fpass\\u002Ficon_classics.jpg\",\"width\":1280},\"xl\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c3f9e87508ed764faf5\\u002Fmaster\\u002Fpass\\u002Ficon_classics.jpg\",\"width\":1600}}},\"placeholder\":\"E-mail address\",\"newsletterTemplateURL\":\"https:\\u002F\\u002Flink.newyorker.com\\u002Fview\\u002F5df1007997a21448cc7f66876663320bfd6f84bf5e0e9031\\u002F3e45cd90\"},\"unsubscribe\":{}},{\"alertBadge\":\"Subscriber exclusives\",\"category\":\"Culture & the Arts\",\"frequencyBadge\":\"Weekly\",\"hasAlertBadge\":true,\"index\":3,\"previewURL\":\"\\u002Fnewsletter\\u002Fgoings-on\",\"slimTags\":[\"culture\",\"goings-on\"],\"slug\":\"goings-on\",\"name\":\"Goings On\",\"newsletterId\":\"248805\",\"socialDataSrc\":{\"dek\":\"What we’re watching, listening to, and doing this week, online, in N.Y.C., and beyond. Paid subscribers also receive book picks.\",\"hed\":\"Sign up for the Goings On newsletter\",\"image\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64bfa4ab1f95b9ee5ffdc3c2\\u002Fmaster\\u002Fpass\\u002Fsocial-image-goings-on.jpg\"},\"manage\":{},\"subscribe\":{\"buttonLabel\":\"Sign up\",\"dangerousDek\":\"What we’re watching, listening to, and doing this week, online, in N.Y.C., and beyond. Paid subscribers also receive book picks.\",\"dangerousHed\":\"Goings On\",\"headerImage\":{\"altText\":\"Goings On logo\",\"sources\":{\"sm\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c40646861503581f969\\u002Fmaster\\u002Fpass\\u002Ficon_goat.jpg\"}}},\"image\":{\"altText\":\"Goings On\",\"sources\":{\"md\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c40646861503581f969\\u002Fmaster\\u002Fpass\\u002Ficon_goat.jpg\",\"width\":1024},\"lg\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c40646861503581f969\\u002Fmaster\\u002Fpass\\u002Ficon_goat.jpg\",\"width\":1280},\"xl\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c40646861503581f969\\u002Fmaster\\u002Fpass\\u002Ficon_goat.jpg\",\"width\":1600}}},\"placeholder\":\"E-mail address\",\"newsletterTemplateURL\":\"https:\\u002F\\u002Flink.newyorker.com\\u002Fview\\u002F5df1007997a21448cc7f6687666374082ad55546440abac4\\u002F3e45cd90\"},\"unsubscribe\":{}},{\"category\":\"Fun & Games\",\"frequencyBadge\":\"Weekdays\",\"index\":4,\"previewURL\":\"\\u002Fnewsletter\\u002Fhumor\",\"slimTags\":[\"shouts-murmurs\",\"cartoons\",\"humor\",\"caption-contest\"],\"slug\":\"humor\",\"tags\":[\"caption-contest\"],\"name\":\"Daily Humor\",\"newsletterId\":\"442\",\"socialDataSrc\":{\"dek\":\"The Daily Cartoon, Shouts, and other funny stuff from our Dept. of Hoopla.\",\"hed\":\"Sign up for the Daily Humor newsletter\",\"image\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64bfa4309e87508ed76505c3\\u002Fmaster\\u002Fpass\\u002Fsocial-image-humor.jpg\"},\"manage\":{},\"subscribe\":{\"buttonLabel\":\"Sign up\",\"dangerousDek\":\"The Daily Cartoon, Shouts, and other funny stuff from our Dept. of Hoopla.\",\"dangerousHed\":\"Daily Humor\",\"headerImage\":{\"altText\":\"Daily Humor logo\",\"sources\":{\"sm\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64ad30398cd3dd8925166215\\u002Fmaster\\u002Fpass\\u002Ficon_humor.jpg\"}}},\"image\":{\"altText\":\"Daily Humor\",\"sources\":{\"md\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c431f95b9ee5ffdb53e\\u002Fmaster\\u002Fpass\\u002Ficon_humor.jpg\",\"width\":1024},\"lg\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c431f95b9ee5ffdb53e\\u002Fmaster\\u002Fpass\\u002Ficon_humor.jpg\",\"width\":1280},\"xl\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c431f95b9ee5ffdb53e\\u002Fmaster\\u002Fpass\\u002Ficon_humor.jpg\",\"width\":1600}}},\"placeholder\":\"E-mail address\",\"newsletterTemplateURL\":\"https:\\u002F\\u002Flink.newyorker.com\\u002Fview\\u002F5df1007997a21448cc7f668766636936d7853cca1000062a\\u002F3e45cd90\"},\"unsubscribe\":{}},{\"category\":\"Culture & the Arts\",\"frequencyBadge\":\"Weekly\",\"index\":5,\"previewURL\":\"\\u002Fnewsletter\\u002Ffiction\",\"slimTags\":[\"fiction\",\"under-review\",\"books\",\"writers\",\"flash-fiction\",\"journals\"],\"slug\":\"fiction\",\"name\":\"Books & Fiction\",\"newsletterId\":\"248770\",\"socialDataSrc\":{\"dek\":\"Early access to new short stories, plus essays, criticism, and coverage of the literary world. Paid subscribers get author interviews and poems. \",\"hed\":\"Sign up for the Books & Fiction newsletter\",\"image\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64bfa472ef09d4a0e04cb7c3\\u002Fmaster\\u002Fpass\\u002Fsocial-image-books-fiction.jpg\"},\"manage\":{},\"subscribe\":{\"buttonLabel\":\"Sign up\",\"dangerousDek\":\"Early access to new short stories, plus essays, criticism, and coverage of the literary world. Paid subscribers get author interviews and poems. \",\"dangerousHed\":\"Books & Fiction\",\"headerImage\":{\"altText\":\"Fiction logo\",\"sources\":{\"sm\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c40901381cd3728946e\\u002Fmaster\\u002Fpass\\u002Ficon_fiction2.jpg\"}}},\"image\":{\"altText\":\"Books & Fiction\",\"sources\":{\"md\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c40901381cd3728946e\\u002Fmaster\\u002Fpass\\u002Ficon_fiction2.jpg\",\"width\":1024},\"lg\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c40901381cd3728946e\\u002Fmaster\\u002Fpass\\u002Ficon_fiction2.jpg\",\"width\":1280},\"xl\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c40901381cd3728946e\\u002Fmaster\\u002Fpass\\u002Ficon_fiction2.jpg\",\"width\":1600}}},\"placeholder\":\"E-mail address\",\"newsletterTemplateURL\":\"https:\\u002F\\u002Flink.newyorker.com\\u002Fview\\u002F5df1007997a21448cc7f6687666727832130833cf7073d86\\u002F1acc38f9\"},\"unsubscribe\":{}},{\"category\":\"Fun & Games\",\"frequencyBadge\":\"Weekdays\",\"index\":6,\"previewURL\":\"\\u002Fnewsletter\\u002Fcrossword\",\"slimTags\":[\"puzzles-and-games\",\"humor\"],\"slug\":\"crossword\",\"name\":\"Crossword Puzzles\",\"newsletterId\":\"248896\",\"socialDataSrc\":{\"dek\":\"Our daily crossword puzzles, which range from beginner-friendly to challenging, plus cryptics, quizzes, and other brain-teasing games.\",\"hed\":\"Sign up for the Puzzles & Games newsletter\",\"image\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64bfa4313f0ea61101b7c89b\\u002Fmaster\\u002Fpass\\u002Fsocial-image-puzzles-games.jpg\"},\"manage\":{},\"subscribe\":{\"buttonLabel\":\"Sign up\",\"dangerousDek\":\"Our daily crossword puzzles, which range from beginner-friendly to challenging, plus cryptics, quizzes, and other brain-teasing games.\",\"dangerousHed\":\"Puzzles & Games\",\"headerImage\":{\"altText\":\"Crossword Puzzles logo\",\"sources\":{\"sm\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c4299c75b29bf383727\\u002Fmaster\\u002Fpass\\u002Ficon_puzzles.jpg\"}}},\"image\":{\"altText\":\"Puzzles & Games\",\"sources\":{\"md\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c4299c75b29bf383727\\u002Fmaster\\u002Fpass\\u002Ficon_puzzles.jpg\",\"width\":1024},\"lg\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c4299c75b29bf383727\\u002Fmaster\\u002Fpass\\u002Ficon_puzzles.jpg\",\"width\":1280},\"xl\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c4299c75b29bf383727\\u002Fmaster\\u002Fpass\\u002Ficon_puzzles.jpg\",\"width\":1600}}},\"placeholder\":\"E-mail address\",\"newsletterTemplateURL\":\"https:\\u002F\\u002Flink.newyorker.com\\u002Fview\\u002F5df1007997a21448cc7f668766636a3b0c6026236309c99d\\u002F3e45cd90\"},\"unsubscribe\":{}},{\"category\":\"Culture & the Arts\",\"frequencyBadge\":\"Weekly\",\"index\":7,\"previewURL\":\"\\u002Fnewsletter\\u002Ffood-scene\",\"slimTags\":[\"annals-of-gastronomy\",\"tables-for-two\"],\"slug\":\"food-scene\",\"name\":\"Food\",\"newsletterId\":\"248868\",\"socialDataSrc\":{\"dek\":\"Helen Rosner on what, where, and how to eat.\",\"hed\":\"Sign up for The Food Scene newsletter\",\"image\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64bfa49c2531777c13053e2b\\u002Fmaster\\u002Fpass\\u002Fsocial-image-food.jpg\"},\"manage\":{},\"subscribe\":{\"buttonLabel\":\"Sign up\",\"dangerousDek\":\"Helen Rosner on what, where, and how to eat. \",\"dangerousHed\":\"The Food Scene\",\"headerImage\":{\"altText\":\"Food logo\",\"sources\":{\"sm\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c3f3f0ea61101b7be1e\\u002Fmaster\\u002Fpass\\u002Ficon_food.jpg\"}}},\"image\":{\"altText\":\"The Food Scene\",\"sources\":{\"md\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c3f3f0ea61101b7be1e\\u002Fmaster\\u002Fpass\\u002Ficon_food.jpg\",\"width\":1024},\"lg\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c3f3f0ea61101b7be1e\\u002Fmaster\\u002Fpass\\u002Ficon_food.jpg\",\"width\":1280},\"xl\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c3f3f0ea61101b7be1e\\u002Fmaster\\u002Fpass\\u002Ficon_food.jpg\",\"width\":1600}}},\"placeholder\":\"E-mail address\",\"newsletterTemplateURL\":\"https:\\u002F\\u002Flink.newyorker.com\\u002Fview\\u002F5df1007997a21448cc7f668766671dcb2250600ed103b133\\u002F1acc38f9\"},\"unsubscribe\":{}},{\"category\":\"Insight & Analysis\",\"frequencyBadge\":\"Weekly\",\"index\":8,\"previewURL\":\"\\u002Fnewsletter\\u002Fscience-technology\",\"slimTags\":[\"annals-of-a-warming-planet\",\"magazine\",\"\"],\"slug\":\"science-technology\",\"name\":\"Science & Technology\",\"newsletterId\":\"248934\",\"socialDataSrc\":{\"dek\":\"In-depth reporting and commentary on A.I., medicine, psychology, climate, and more.\",\"hed\":\"Sign up for the Science & Technology newsletter\",\"image\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64bfa432ef09d4a0e04cb7c1\\u002Fmaster\\u002Fpass\\u002Fsocial-image-books-fiction.jpg\"},\"manage\":{},\"subscribe\":{\"buttonLabel\":\"Sign up\",\"dangerousDek\":\"In-depth reporting and commentary on A.I., medicine, psychology, climate, and more.\",\"dangerousHed\":\"Science & Technology\",\"headerImage\":{\"altText\":\"Science & Technology logo\",\"sources\":{\"sm\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c423b391db87dff93ab\\u002Fmaster\\u002Fpass\\u002Ficon_science-rd.jpg\"}}},\"image\":{\"altText\":\"Science & Technology\",\"sources\":{\"md\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c423b391db87dff93ab\\u002Fmaster\\u002Fpass\\u002Ficon_science-rd.jpg\",\"width\":1024},\"lg\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c423b391db87dff93ab\\u002Fmaster\\u002Fpass\\u002Ficon_science-rd.jpg\",\"width\":1280},\"xl\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c423b391db87dff93ab\\u002Fmaster\\u002Fpass\\u002Ficon_science-rd.jpg\",\"width\":1600}}},\"placeholder\":\"E-mail address\",\"newsletterTemplateURL\":\"https:\\u002F\\u002Flink.newyorker.com\\u002Fview\\u002F5df1007997a21448cc7f66876663337c93f11dbe360de0fd\\u002F3e45cd90\"},\"unsubscribe\":{}},{\"category\":\"  \",\"disableSubscribePage\":true,\"frequencyBadge\":\" \",\"index\":9,\"previewURL\":\" \",\"slug\":\"tny-festival-2021\",\"tags\":[\"tny-festival-2021\"],\"name\":\"New Yorker Festival 2021\",\"newsletterId\":\"248973\",\"socialDataSrc\":{\"dek\":\" \",\"hed\":\" \",\"image\":\" \"},\"manage\":{},\"subscribe\":{\"buttonLabel\":\"Sign up\",\"dangerousContext\":\" \",\"dangerousDek\":\"  \",\"dangerousHed\":\"Sign up below to be the first to get updates about this year’s festival.\",\"dangerousText\":\" \",\"headerImage\":{\"altText\":\" \",\"sources\":{\"sm\":{\"url\":\" \"}}},\"placeholder\":\"E-mail address\",\"newsletterTemplateURL\":\" \",\"textFieldLabel\":\" \"},\"unsubscribe\":{}},{\"category\":\"Insight & Analysis\",\"frequencyBadge\":\"Twice weekly\",\"index\":10,\"previewURL\":\"\\u002Fnewsletter\\u002Fnews-politics\",\"slimTags\":[\"andy-borowitz\",\"susan-b-glasser\",\"benjamin-wallace-wells\",\"doreen-st-felix\",\"john-cassidy\",\"amy-davidson-sorkin\",\"jay-caspian-kang\"],\"slug\":\"news-politics\",\"name\":\"News & Politics\",\"newsletterId\":\"249009\",\"socialDataSrc\":{\"dek\":\"The latest from Washington and beyond, covering current events, the economy, and more, from our columnists and correspondents.\",\"hed\":\"Sign up for the News & Politics newsletter\",\"image\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64bfa432ef09d4a0e04cb7c0\\u002Fmaster\\u002Fpass\\u002Fsocial-image-news-politics.jpg\"},\"manage\":{},\"subscribe\":{\"buttonLabel\":\"Sign up\",\"dangerousDek\":\"The latest from Washington and beyond, covering current events, the economy, and more, from our columnists and correspondents.\",\"dangerousHed\":\"News & Politics\",\"headerImage\":{\"altText\":\"News & Politics logo\",\"sources\":{\"sm\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c423b391db87dff93ad\\u002Fmaster\\u002Fpass\\u002Ficon_news.jpg\"}}},\"image\":{\"altText\":\"News & Politics\",\"sources\":{\"md\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c423b391db87dff93ad\\u002Fmaster\\u002Fpass\\u002Ficon_news.jpg\",\"width\":1024},\"lg\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c423b391db87dff93ad\\u002Fmaster\\u002Fpass\\u002Ficon_news.jpg\",\"width\":1280},\"xl\":{\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F64b64c423b391db87dff93ad\\u002Fmaster\\u002Fpass\\u002Ficon_news.jpg\",\"width\":1600}}},\"placeholder\":\"E-mail address\",\"newsletterTemplateURL\":\"https:\\u002F\\u002Flink.newyorker.com\\u002Fview\\u002F5df1007997a21448cc7f66876663555f6ac5630118000ac4\\u002F3e45cd90\"},\"unsubscribe\":{}},{\"category\":\" \",\"disableSubscribePage\":true,\"frequencyBadge\":\" \",\"index\":11,\"previewURL\":\" \",\"slug\":\" \",\"name\":\"Movies\",\"newsletterId\":\"248895\",\"socialDataSrc\":{\"dek\":\" \",\"hed\":\" \",\"image\":\" \"},\"manage\":{},\"subscribe\":{},\"unsubscribe\":{}}],\"noticesLinks\":{\"nodes\":[{\"attributes\":{\"name\":\"\\u002Fabout\"},\"nodeType\":\"link\",\"url\":\"\\u002Fabout\\u002Fus\",\"text\":\"About\",\"index\":0,\"isExternal\":false},{\"attributes\":{\"name\":\"\\u002Fcareers\"},\"nodeType\":\"link\",\"url\":\"\\u002Fabout\\u002Fcareers\",\"text\":\"Careers\",\"index\":1},{\"attributes\":{\"name\":\"\\u002Fcontact\"},\"nodeType\":\"link\",\"url\":\"\\u002Fabout\\u002Fcontact\",\"text\":\"Contact\",\"index\":2},{\"attributes\":{\"name\":\"\\u002Ffaq\"},\"nodeType\":\"link\",\"url\":\"\\u002Fabout\\u002Ffaq\",\"text\":\"F.A.Q.\",\"index\":3},{\"attributes\":{\"name\":\"\\u002Fmedia-kit\"},\"nodeType\":\"link\",\"url\":\"https:\\u002F\\u002Fwww.condenast.com\\u002Fadvertising\",\"text\":\"Media Kit\",\"index\":4},{\"attributes\":{\"name\":\"\\u002Fpress\"},\"nodeType\":\"link\",\"url\":\"\\u002Fabout\\u002Fpress\",\"text\":\"Press\",\"index\":5},{\"attributes\":{\"name\":\"\\u002Faccessibility-help\"},\"nodeType\":\"link\",\"url\":\"\\u002Fabout\\u002Faccessibility-help\",\"rel\":\"nofollow\",\"text\":\"Accessibility Help\",\"index\":6},{\"attributes\":{\"name\":\"\\u002Fuser-agreement\"},\"nodeType\":\"link\",\"url\":\"https:\\u002F\\u002Fwww.condenast.com\\u002Fuser-agreement\\u002F\",\"rel\":\"nofollow\",\"text\":\"User Agreement\",\"index\":7,\"isExternal\":true},{\"attributes\":{\"name\":\"\\u002Fprivacy-policy\"},\"nodeType\":\"link\",\"url\":\"http:\\u002F\\u002Fwww.condenast.com\\u002Fprivacy-policy#privacypolicy\",\"rel\":\"nofollow\",\"text\":\"Privacy Policy\",\"index\":8,\"isExternal\":true},{\"attributes\":{\"name\":\"\\u002Fyour-california-privacy-rights\"},\"nodeType\":\"link\",\"url\":\"http:\\u002F\\u002Fwww.condenast.com\\u002Fprivacy-policy#privacypolicy-california\",\"rel\":\"nofollow\",\"text\":\"Your California Privacy Rights\",\"index\":9,\"isExternal\":true}]},\"pim\":{},\"primaryLinks\":{\"nodes\":[{\"attributes\":{\"name\":\"\\u002Fthe-latest\"},\"index\":0,\"nodeType\":\"link\",\"showInTopNav\":true,\"text\":\"The Latest\",\"url\":\"\\u002Flatest\"},{\"attributes\":{\"name\":\"\\u002Fnews\"},\"forceLeftOfNav\":false,\"index\":1,\"isExternal\":false,\"nodeType\":\"link\",\"showInTopNav\":true,\"text\":\"News\",\"url\":\"\\u002Fnews\"},{\"attributes\":{\"name\":\"\\u002Fbooks-and-culture\"},\"forceLeftOfNav\":false,\"index\":2,\"isExternal\":false,\"nodeType\":\"link\",\"showInTopNav\":true,\"text\":\"Books & Culture\",\"url\":\"\\u002Fculture\"},{\"attributes\":{\"name\":\"\\u002Ffiction-and-poetry\"},\"forceLeftOfNav\":false,\"index\":3,\"isExternal\":false,\"nodeType\":\"link\",\"showInTopNav\":true,\"text\":\"Fiction & Poetry\",\"url\":\"\\u002Ffiction-and-poetry\"},{\"attributes\":{\"name\":\"\\u002Fhumor-and-cartoons\"},\"forceLeftOfNav\":false,\"index\":4,\"isExternal\":false,\"nodeType\":\"link\",\"showInTopNav\":true,\"text\":\"Humor & Cartoons\",\"url\":\"\\u002Fhumor\"},{\"attributes\":{\"name\":\"\\u002Fmagazine\"},\"forceLeftOfNav\":false,\"index\":5,\"isExternal\":false,\"nodeType\":\"link\",\"showInTopNav\":true,\"text\":\"Magazine\",\"url\":\"\\u002Fmagazine\"},{\"attributes\":{\"name\":\"\\u002Fpuzzles-and-games\"},\"forceLeftOfNav\":false,\"index\":6,\"isExternal\":false,\"nodeType\":\"link\",\"showInTopNav\":true,\"text\":\"Puzzles & Games\",\"url\":\"\\u002Fcrossword-puzzles-and-games\"},{\"attributes\":{\"name\":\"\\u002Fvideo\"},\"forceLeftOfNav\":false,\"index\":7,\"isExternal\":false,\"nodeType\":\"link\",\"showInTopNav\":true,\"text\":\"Video\",\"url\":\"\\u002Fvideo\"},{\"attributes\":{\"name\":\"\\u002Fpodcasts\"},\"forceLeftOfNav\":false,\"index\":8,\"isExternal\":false,\"nodeType\":\"link\",\"showInTopNav\":true,\"text\":\"Podcasts\",\"url\":\"\\u002Fpodcasts\"},{\"attributes\":{\"name\":\"\\u002Farchive\"},\"forceLeftOfNav\":false,\"index\":9,\"isExternal\":false,\"nodeType\":\"link\",\"showInTopNav\":false,\"text\":\"Archive\",\"url\":\"\\u002Farchive\"},{\"attributes\":{\"name\":\"\\u002Fgoings-on\"},\"forceLeftOfNav\":false,\"index\":10,\"isExternal\":false,\"nodeType\":\"link\",\"showInTopNav\":true,\"text\":\"Goings On\",\"url\":\"\\u002Fgoings-on\"},{\"attributes\":{\"name\":\"\\u002Fshop\"},\"forceLeftOfNav\":false,\"index\":11,\"isExternal\":true,\"nodeType\":\"link\",\"showInTopNav\":false,\"text\":\"Shop\",\"url\":\"https:\\u002F\\u002Fstore.newyorker.com\"},{\"attributes\":{\"name\":\"\\u002Ffestival\"},\"forceLeftOfNav\":true,\"index\":12,\"nodeType\":\"link\",\"showInTopNav\":true,\"text\":\"Festival\",\"url\":\"https:\\u002F\\u002Ffestival.newyorker.com\\u002F\"}]},\"productSummaryGrid\":{},\"productCard\":{},\"recaptcha\":{\"invisibleSiteKey\":\"6LdBUaMUAAAAABqTClaGs_N7oa4tu4jwHeFQmFHH\",\"invisibleSecret\":\"6LdBUaMUAAAAAI_VXfkZOVT9JwrNApigrQ2RyVOP\"},\"recommendations\":{\"recommendationsApiUrl\":\"https:\\u002F\\u002Frecs-api.conde.digital\\u002Fv4\",\"enableClientSideRender\":false},\"savingsUnitedCoupons\":{},\"secondaryLinks\":{\"nodes\":[]},\"shopify\":{\"enable\":false},\"snowplow\":{\"collectorUrl\":\"c.newyorker.com\",\"enableSnowplow\":true},\"social\":{\"facebookAdmins\":\"\",\"facebookAppId\":\"1147169538698836\",\"facebookHandle\":\"newyorker\",\"facebookPageId\":\"9258148868\",\"twitterHandle\":\"NewYorker\"},\"socialLinks\":{\"heading\":\"\",\"nodes\":[{\"attributes\":{\"name\":\"\\u002Ffollow-us-on-facebook\"},\"nodeType\":\"link\",\"url\":\"https:\\u002F\\u002Fwww.facebook.com\\u002Fnewyorker\\u002F\",\"network\":\"facebook\",\"text\":\"Follow us on Facebook\",\"index\":0},{\"attributes\":{\"name\":\"\\u002Ffollow-us-on-x\"},\"nodeType\":\"link\",\"url\":\"https:\\u002F\\u002Ftwitter.com\\u002FNewYorker\\u002F\",\"network\":\"twitter\",\"text\":\"Follow us on X\",\"index\":1},{\"attributes\":{\"name\":\"\\u002Ffollow-us-on-snapchat\"},\"nodeType\":\"link\",\"url\":\"https:\\u002F\\u002Fwww.snapchat.com\\u002Fadd\\u002Fnewyorkermag\",\"network\":\"snapchat\",\"text\":\"Follow us on Snapchat\",\"index\":2},{\"attributes\":{\"name\":\"\\u002Ffollow-us-on-youtube\"},\"nodeType\":\"link\",\"url\":\"https:\\u002F\\u002Fwww.youtube.com\\u002Fuser\\u002FNewYorkerDotCom\\u002F\",\"network\":\"youtube\",\"text\":\"Follow us on YouTube\",\"index\":3},{\"attributes\":{\"name\":\"\\u002Ffollow-us-on-instagram\"},\"nodeType\":\"link\",\"url\":\"https:\\u002F\\u002Finstagram.com\\u002Fnewyorkermag\\u002F\",\"network\":\"instagram\",\"text\":\"Follow us on Instagram\",\"index\":4}]},\"subNavigation\":{},\"subNavigationLinks\":{\"nodes\":[]},\"subscribeWithGoogle\":{\"oAuthClientId\":\"275906274807-b4eqbdqr511u9msdpj8mh0pf77fcciv7.apps.googleusercontent.com\"},\"taboola\":{\"isPixelScriptEnabled\":true,\"pixelScriptAccountID\":\"1187698\"},\"unifiedProductCard\":{}},\"featureFlags\":{\"shouldUseBookmarkV3\":true,\"enableSlimNewsletter\":false,\"enableAllContributorsOnBundles\":true,\"enableEntitlementProxy\":true,\"enableEntitlementValidation\":true,\"enableFictionContributor\":true,\"enableGqlForLinkBanner\":true,\"enableLinkStack\":true,\"enableRecipeRatings\":false,\"enableUserContext\":true,\"hideRelatedOnBundles\":true,\"shouldExtractRecircRubricFromCategories\":true,\"recentWorkTeaser\":\"rubric-or-channel\",\"bundleTeaser\":\"rubric-or-channel-or-section\",\"contentTeaser\":\"rubric-or-channel-or-section\",\"tagTeaser\":\"rubric-or-channel\",\"preferCollectionGrid\":true,\"overrideBodyContentHeadings\":true,\"enableSponsoredContentInRelated\":false,\"personalizeRecircMostPopular\":true,\"videoPersistable\":false,\"google\":{\"swgEnabled\":false,\"signInEnabled\":true},\"featureOnboarding\":{\"bookmarks\":false},\"enableContributorAuthorHub\":true,\"applyPlaceHolderImage\":false,\"enableAudioIcon\":false,\"enableEnhancedCartoonExperience\":true,\"embeddedLedeCategoriesPath\":\"formatting\\u002Flede-image-layout\\u002Flede-image--right-aligned\",\"embeddedLedeDisabledPaths\":[\"formatting\\u002Fhero-layouts\\u002Flayout--image-left\",\"formatting\\u002Fhero-layouts\\u002Flayout--image-right\"],\"jsonld\":{\"useSubChannelAsSection\":true},\"hasHeaderIssueDateLink\":true,\"hasLeadStandardHeading\":true,\"hasMagazineHeaderPromoCopy\":true,\"hasMagazineDisclaimer\":true,\"hideHeroAdContentHeaders\":[\"TextBelowCenterFullBleedNoContributor\",\"TextOverlayCenterFullBleedGradient\",\"SplitScreenImageLeftFullBleed\",\"SplitScreenImageRightFullBleed\",\"SplitScreenImageRightInset\",\"SplitScreenImageLeftInset\"],\"hideLedeImageCaption\":false,\"hideBylineContributorImage\":false,\"issueLinkPrefix\":\"\\u002Fmagazine\",\"mediaSocialShares\":[],\"paddingTop\":\"large\",\"socialShares\":[\"facebook\",\"twitter\",\"email\",\"print\",\"bookmark\"],\"shouldUsePersistentAd\":true,\"showFirstRailRecirc\":true,\"allowRelatedExternalLinks\":true,\"useCategoryIssueDate\":true,\"enableAccounts\":true,\"enableActionBar\":true,\"enableAutopilotBundleComponent\":false,\"enableBookmarking\":true,\"enableComments\":false,\"enableConsent\":true,\"enableDropcap\":true,\"enableEnhancedArticleHeader\":true,\"enableInfinityId\":true,\"enableJourneyPayment\":true,\"enableLayoutPresets\":true,\"enableMediaOverridesV2\":false,\"personalizeRecircList\":true,\"hideContributorBio\":false,\"hideTagCloud\":false},\"renditions\":{\"eventPage\":{\"lede\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":640},\"lg\":{\"aspectRatio\":\"master\",\"width\":768},\"xl\":{\"aspectRatio\":\"master\",\"width\":1024},\"xxl\":{\"aspectRatio\":\"master\",\"width\":1280}}},\"article\":{\"brandedSponsorLogo\":{\"sm\":{\"width\":640},\"md\":{\"width\":768},\"lg\":{\"width\":1024},\"xl\":{\"width\":1280},\"xxl\":{\"width\":1600}},\"contentPromo\":{\"sm\":{\"aspectRatio\":\"9:16\",\"width\":768},\"md\":{\"aspectRatio\":\"9:16\",\"width\":1024},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":1280},\"xl\":{\"aspectRatio\":\"16:9\",\"width\":1600},\"xxl\":{\"aspectRatio\":\"16:9\",\"width\":1600}},\"externalLink\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":100},\"md\":{\"aspectRatio\":\"1:1\",\"width\":100},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":200},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":200},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":200}},\"imageEmbed\":{\"sm\":{\"width\":640},\"md\":{\"width\":768},\"lg\":{\"width\":1024},\"xl\":{\"width\":1280},\"xxl\":{\"width\":1600}},\"recipeEmbed\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":640},\"md\":{\"aspectRatio\":\"16:9\",\"width\":768},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":768},\"xl\":{\"aspectRatio\":\"16:9\",\"width\":775},\"xxl\":{\"aspectRatio\":\"16:9\",\"width\":775}},\"summaryCollectionSplitColumns\":{\"lede\":{\"sm\":{\"aspectRatio\":\"4:3\"},\"md\":{\"aspectRatio\":\"4:3\"},\"lg\":{\"aspectRatio\":\"4:3\"},\"xl\":{\"aspectRatio\":\"4:3\"},\"xxl\":{\"aspectRatio\":\"4:3\"}},\"recommendedItems\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":720},\"md\":{\"aspectRatio\":\"16:9\",\"width\":720},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":748},\"xl\":{\"aspectRatio\":\"16:9\",\"width\":748}}},\"lede\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":1024},\"lg\":{\"aspectRatio\":\"master\",\"width\":1280},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"master\",\"width\":2560}},\"licensedPartnerBadge\":{\"sm\":{\"height\":100}},\"contributorThumbnail\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":240},\"md\":{\"aspectRatio\":\"1:1\",\"width\":240},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":270},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":270},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":270}},\"productEmbed\":{\"sm\":{\"width\":360},\"md\":{\"width\":1024},\"lg\":{\"width\":1280},\"xl\":{\"width\":1280},\"xxl\":{\"width\":1280}},\"ctProductEmbed\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":720,\"pimImageSize\":\"-small\"},\"md\":{\"aspectRatio\":\"1:1\",\"width\":720,\"pimImageSize\":\"-medium\"},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":748,\"pimImageSize\":\"-large\"},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":748,\"pimImageSize\":\"-large\"}},\"articleEmbed\":{\"sm\":{\"width\":640},\"md\":{\"width\":768},\"lg\":{\"width\":768},\"xl\":{\"width\":775},\"xxl\":{\"width\":775}},\"relatedArticleEmbed\":{\"sm\":{\"width\":640},\"md\":{\"width\":768},\"lg\":{\"width\":768},\"xl\":{\"width\":775},\"xxl\":{\"width\":775}},\"reviewEmbed\":{\"sm\":{\"width\":640},\"md\":{\"width\":768},\"lg\":{\"width\":768},\"xl\":{\"width\":775},\"xxl\":{\"width\":775}},\"linkList\":{\"headerImage\":{\"sm\":{\"width\":64},\"md\":{\"width\":128},\"lg\":{\"width\":128},\"xl\":{\"width\":256},\"xxl\":{\"width\":256}}}},\"artist\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":360},\"md\":{\"aspectRatio\":\"16:9\",\"width\":1024},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":1024},\"xl\":{\"aspectRatio\":\"16:9\",\"width\":1024}},\"bundle\":{\"brandedSponsorLogo\":{\"sm\":{\"width\":640},\"md\":{\"width\":768},\"lg\":{\"width\":1024},\"xl\":{\"width\":1280},\"xxl\":{\"width\":1600}},\"imageSlideShow\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":1024},\"lg\":{\"aspectRatio\":\"master\",\"width\":1280},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"master\",\"width\":2560}},\"productEmbed\":{\"sm\":{\"width\":360},\"md\":{\"width\":1024},\"lg\":{\"width\":1280},\"xl\":{\"width\":1280},\"xxl\":{\"width\":1280}},\"productVisual\":{\"lede\":{\"sm\":{\"width\":768,\"aspectRatio\":\"3:2\"},\"lg\":{\"width\":1280,\"aspectRatio\":\"16:9\"}},\"carousel\":{\"sm\":{\"width\":768,\"sizes\":\"25vw\"},\"lg\":{\"width\":1280,\"sizes\":\"25vw\"}}},\"versoPLPFilterableFeatureAssetOnly\":{\"sm\":{\"aspectRatio\":\"9:16\",\"width\":720},\"md\":{\"aspectRatio\":\"9:16\",\"width\":720},\"lg\":{\"aspectRatio\":\"9:16\",\"width\":748},\"xl\":{\"aspectRatio\":\"9:16\",\"width\":748}},\"fullBleedBanner\":{\"lede\":{\"sm\":{\"width\":768,\"aspectRatio\":\"3:2\"},\"lg\":{\"width\":1280,\"aspectRatio\":\"3:1\"},\"xl\":{\"aspectRatio\":\"3:1\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"3:1\",\"width\":1600}}},\"summaryCollectionGrid\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":360},\"lg\":{\"aspectRatio\":\"master\",\"width\":720},\"xl\":{\"aspectRatio\":\"master\",\"width\":720},\"xxl\":{\"aspectRatio\":\"master\",\"width\":720}},\"summaryRiver\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":320},\"md\":{\"aspectRatio\":\"4:3\",\"width\":568},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":720},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":1024},\"xxl\":{\"aspectRatio\":\"4:3\",\"width\":1024}},\"versoFilterableSummaryList\":{\"sm\":{\"aspectRatio\":\"3:4\",\"width\":720},\"md\":{\"aspectRatio\":\"3:4\",\"width\":720},\"lg\":{\"aspectRatio\":\"3:4\",\"width\":748},\"xl\":{\"aspectRatio\":\"3:4\",\"width\":748}},\"versoPLPFilterableFeature\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":720},\"md\":{\"aspectRatio\":\"1:1\",\"width\":720},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":748},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":748}},\"versoPLPFilterableFeatureDiffRatio\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":720},\"md\":{\"aspectRatio\":\"16:9\",\"width\":1280},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":1600},\"xl\":{\"aspectRatio\":\"16:9\",\"width\":1920}},\"versoPLPMultiFilterableFeature\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":720},\"md\":{\"aspectRatio\":\"1:1\",\"width\":720},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":748},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":748}},\"versoArticleFilterableFeature\":{\"sm\":{\"aspectRatio\":\"3:4\",\"width\":720},\"md\":{\"aspectRatio\":\"3:4\",\"width\":720},\"lg\":{\"aspectRatio\":\"3:4\",\"width\":748},\"xl\":{\"aspectRatio\":\"3:4\",\"width\":748}},\"summaryCarousel\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":768},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"4:3\",\"width\":1600}},\"summaryCollageThree\":{\"primary\":{\"sm\":{\"aspectRatio\":\"16:9\"},\"md\":{\"aspectRatio\":\"16:9\"},\"lg\":{\"aspectRatio\":\"16:9\"},\"xl\":{\"aspectRatio\":\"16:9\"},\"xxl\":{\"aspectRatio\":\"16:9\"}}},\"summaryCollageFive\":{\"primary\":{\"sm\":{\"aspectRatio\":\"16:9\"},\"md\":{\"aspectRatio\":\"16:9\"},\"lg\":{\"aspectRatio\":\"16:9\"},\"xl\":{\"aspectRatio\":\"16:9\"},\"xxl\":{\"aspectRatio\":\"16:9\"}}},\"versoFlatPackage\":{\"containerImage\":{\"sm\":{\"aspectRatio\":\"master\"},\"md\":{\"aspectRatio\":\"master\"},\"lg\":{\"aspectRatio\":\"master\"},\"xl\":{\"aspectRatio\":\"master\"},\"xxl\":{\"aspectRatio\":\"master\"}}},\"seriesNavigationCarousel\":{\"sm\":{\"aspectRatio\":\"4:3\"},\"md\":{\"aspectRatio\":\"4:3\"},\"lg\":{\"aspectRatio\":\"4:3\"},\"xl\":{\"aspectRatio\":\"4:3\"},\"xxl\":{\"aspectRatio\":\"4:3\"}}},\"capabilities\":{\"mostRecentContent\":{\"sm\":{\"aspectRatio\":\"1:1\",\"height\":450,\"width\":800},\"md\":{\"aspectRatio\":\"1:1\",\"height\":450,\"width\":800},\"lg\":{\"aspectRatio\":\"1:1\",\"height\":197,\"width\":350},\"xl\":{\"aspectRatio\":\"1:1\",\"height\":450,\"width\":800}}},\"cartoon\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":1024},\"lg\":{\"aspectRatio\":\"master\",\"width\":1280},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"master\",\"width\":1280}},\"captionContest\":{\"content\":{\"sm\":{\"aspectRatio\":\"master\"},\"md\":{\"aspectRatio\":\"master\"},\"lg\":{\"aspectRatio\":\"master\"},\"xl\":{\"aspectRatio\":\"master\"},\"xxl\":{\"aspectRatio\":\"master\"}},\"label\":{\"sm\":{\"aspectRatio\":\"master\",\"height\":100},\"md\":{\"aspectRatio\":\"master\",\"height\":100},\"lg\":{\"aspectRatio\":\"master\",\"height\":120},\"xl\":{\"aspectRatio\":\"master\",\"height\":120}}},\"cookbook\":{\"lede\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":1024},\"lg\":{\"aspectRatio\":\"master\",\"width\":760},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"master\",\"width\":2560}}},\"contributor\":{\"lede\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":164},\"md\":{\"aspectRatio\":\"1:1\",\"width\":246},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":310},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":350}},\"recentWork\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":320},\"md\":{\"aspectRatio\":\"4:3\",\"width\":568},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":720},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":1024},\"xxl\":{\"aspectRatio\":\"4:3\",\"width\":1024}},\"review\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":724},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":274},\"xxl\":{\"aspectRatio\":\"16:9\",\"width\":330}}},\"designer\":{\"intro\":{\"sm\":{\"aspectRatio\":\"6:7\",\"width\":768},\"lg\":{\"aspectRatio\":\"6:7\",\"width\":1024}}},\"editorPicks\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":320,\"height\":320},\"md\":{\"aspectRatio\":\"1:1\",\"width\":320,\"height\":320},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":320,\"height\":320},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":320,\"height\":320}},\"featuredInCarousel\":{\"sm\":{\"aspectRatio\":\"3:4\",\"width\":720},\"md\":{\"aspectRatio\":\"3:4\",\"width\":720},\"lg\":{\"aspectRatio\":\"3:4\",\"width\":748},\"xl\":{\"aspectRatio\":\"3:4\",\"width\":748}},\"gallery\":{\"brandedSponsorLogo\":{\"sm\":{\"width\":640},\"md\":{\"width\":768},\"lg\":{\"width\":1024},\"xl\":{\"width\":1280},\"xxl\":{\"width\":1600}},\"lede\":{\"sm\":\"1:1\",\"lg\":\"16:9\"},\"slides\":{\"sm\":\"master\",\"lg\":\"master\"},\"social\":{\"sm\":\"16:9\",\"lg\":\"16:9\"},\"pimSlides\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":720},\"md\":{\"aspectRatio\":\"1:1\",\"width\":720},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":748},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":748}}},\"infopage\":{\"lede\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":1024},\"lg\":{\"aspectRatio\":\"master\",\"width\":1280},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"master\",\"width\":1280}},\"tout\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":1024},\"lg\":{\"aspectRatio\":\"master\",\"width\":1280},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"master\",\"width\":1280}}},\"podcast\":{\"listingPage\":{\"lede\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":320},\"md\":{\"aspectRatio\":\"1:1\",\"width\":768},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":768},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":1600}}},\"detailPage\":{\"lede\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":320},\"md\":{\"aspectRatio\":\"1:1\",\"width\":768},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":768},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":1600}}}},\"livestory\":{\"brandedSponsorLogo\":{\"sm\":{\"width\":640},\"md\":{\"width\":768},\"lg\":{\"width\":1024},\"xl\":{\"width\":1280},\"xxl\":{\"width\":1600}},\"lede\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":1024},\"lg\":{\"aspectRatio\":\"master\",\"width\":1280},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"master\",\"width\":1280}}},\"inlineRecirc\":{\"SideBySide\":{\"sm\":{\"aspectRatio\":\"1:1\",\"height\":320,\"width\":320},\"lg\":{\"aspectRatio\":\"1:1\",\"height\":320,\"width\":320}},\"TextOverlay\":{\"sm\":{\"aspectRatio\":\"2:3\",\"width\":768},\"lg\":{\"aspectRatio\":\"2:3\",\"width\":768}}},\"linkBanner\":{\"sm\":{\"aspectRatio\":\"master\",\"height\":100},\"md\":{\"aspectRatio\":\"master\",\"height\":100},\"lg\":{\"aspectRatio\":\"master\",\"height\":120},\"xl\":{\"aspectRatio\":\"master\",\"height\":120}},\"navigationList\":{\"sm\":{\"aspectRatio\":\"3:2\",\"height\":420},\"md\":{\"aspectRatio\":\"3:2\",\"height\":640},\"lg\":{\"aspectRatio\":\"3:2\",\"height\":640},\"xl\":{\"aspectRatio\":\"3:2\",\"height\":640}},\"mostPopular\":{\"sm\":{\"aspectRatio\":\"1:1\",\"height\":350,\"width\":350},\"lg\":{\"aspectRatio\":\"1:1\",\"height\":240,\"width\":240},\"xl\":{\"aspectRatio\":\"1:1\",\"height\":350,\"width\":350}},\"bundleHeader\":{\"lede\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":360},\"md\":{\"aspectRatio\":\"1:1\",\"width\":1024},\"lg\":{\"aspectRatio\":\"2:1\",\"width\":1280},\"xl\":{\"aspectRatio\":\"2:1\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"2:1\",\"width\":2560}}},\"recircList\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480}},\"productCard\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":360}},\"recircListTextOverlay\":{\"sm\":{\"aspectRatio\":\"1:1\"},\"md\":{\"aspectRatio\":\"4:3\"},\"lg\":{\"aspectRatio\":\"4:3\"},\"xl\":{\"aspectRatio\":\"4:3\"}},\"review\":{\"lede\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":320},\"md\":{\"aspectRatio\":\"16:9\",\"width\":768},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":952},\"xxl\":{\"aspectRatio\":\"16:9\",\"width\":1160}},\"tout\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":1024},\"lg\":{\"aspectRatio\":\"master\",\"width\":1280},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"master\",\"width\":1280}}},\"recipe\":{\"brandedSponsorLogo\":{\"sm\":{\"width\":640},\"md\":{\"width\":768},\"lg\":{\"width\":1024},\"xl\":{\"width\":1280},\"xxl\":{\"width\":1600}},\"imageEmbed\":{\"sm\":{\"width\":640},\"md\":{\"width\":768},\"lg\":{\"width\":768},\"xl\":{\"width\":775},\"xxl\":{\"width\":775}},\"lede\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":1024},\"lg\":{\"aspectRatio\":\"master\",\"width\":1280},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"master\",\"width\":2560}},\"tout\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":1024},\"lg\":{\"aspectRatio\":\"master\",\"width\":1280},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"master\",\"width\":1280}},\"contributorThumbnail\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":80},\"md\":{\"aspectRatio\":\"1:1\",\"width\":80},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":90},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":90},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":90}},\"cookbook\":{\"sm\":{\"width\":160}},\"productCarousel\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":320},\"md\":{\"aspectRatio\":\"1:1\",\"width\":640},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":640},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":640},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":640}}},\"runwayShow\":{\"galleryThumbnail\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":1024},\"lg\":{\"aspectRatio\":\"master\",\"width\":1280},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"master\",\"width\":2560}},\"socialShare\":{\"sm\":{\"aspectRatio\":\"16:9\"}}},\"season\":{\"curatedShows\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":1024},\"lg\":{\"aspectRatio\":\"master\",\"width\":1280},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280}}},\"seriesPagination\":{\"sm\":{\"aspectRatio\":\"master\",\"height\":100},\"md\":{\"aspectRatio\":\"master\",\"height\":100},\"lg\":{\"aspectRatio\":\"master\",\"height\":120},\"xl\":{\"aspectRatio\":\"master\",\"height\":120}},\"seriesRecirc\":{\"sm\":{\"aspectRatio\":\"6:5\",\"width\":360},\"md\":{\"aspectRatio\":\"6:5\",\"width\":360},\"lg\":{\"aspectRatio\":\"3:2\",\"width\":720},\"xl\":{\"aspectRatio\":\"3:2\",\"width\":720},\"xxl\":{\"aspectRatio\":\"3:2\",\"width\":720}},\"socialShare\":{\"aspectRatio\":\"16:9\",\"width\":1280},\"tag\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":1080},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"4:3\",\"width\":1280},\"md\":{\"aspectRatio\":\"4:3\",\"width\":1080},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":1280}},\"tagpage\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":724},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":316},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":354}},\"search\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":724},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":316},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":354}},\"searchVideoResults\":{\"sm\":{\"aspectRatio\":\"16:9\"},\"md\":{\"aspectRatio\":\"16:9\"},\"lg\":{\"aspectRatio\":\"16:9\"},\"xl\":{\"aspectRatio\":\"16:9\"}},\"searchResultsList\":{\"lede\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":360},\"md\":{\"aspectRatio\":\"1:1\",\"width\":1024},\"lg\":{\"aspectRatio\":\"4:1\",\"width\":1280},\"xl\":{\"aspectRatio\":\"4:1\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"4:1\",\"width\":2560}}},\"searchReviewResults\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":360},\"md\":{\"aspectRatio\":\"1:1\",\"width\":1024},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":1024},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":1024}},\"artistReviewResults\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":360},\"md\":{\"aspectRatio\":\"1:1\",\"width\":1024},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":1024},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":1024}},\"homepage\":{\"curatedShows\":{\"sm\":{\"aspectRatio\":\"2:3\",\"width\":480},\"lg\":{\"aspectRatio\":\"2:3\",\"width\":480}},\"cneVideo\":{\"summaryCollageOne\":{\"sm\":{\"h\":441,\"w\":784},\"lg\":{\"h\":720,\"w\":1280},\"xxl\":{\"h\":900,\"w\":1600}},\"summaryCollageFive\":{\"primary\":{\"sm\":{\"h\":441,\"w\":784},\"lg\":{\"h\":720,\"w\":1280},\"xxl\":{\"h\":900,\"w\":1600}},\"secondary\":{\"sm\":{\"h\":441,\"w\":784},\"lg\":{\"h\":720,\"w\":1280},\"xxl\":{\"h\":900,\"w\":1600}}},\"summaryCollageSix\":{\"primary\":{\"sm\":{\"h\":441,\"w\":784},\"lg\":{\"h\":720,\"w\":1280},\"xxl\":{\"h\":900,\"w\":1600}},\"secondary\":{\"sm\":{\"h\":441,\"w\":784},\"lg\":{\"h\":720,\"w\":1280},\"xxl\":{\"h\":900,\"w\":1600}}}},\"contributorThumbnail\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":80},\"md\":{\"aspectRatio\":\"1:1\",\"width\":80},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":80},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":88},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":88}},\"summaryCollectionGrid\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":768},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"4:3\",\"width\":1600}},\"summaryCollectionGridThreeColumn\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":768},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"4:3\",\"width\":1600}},\"summaryCollageFive\":{\"primary\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":768},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":768},\"xxl\":{\"aspectRatio\":\"4:3\",\"width\":1024}},\"secondary\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":320},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":640}}},\"summaryCollageOne\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":360},\"md\":{\"aspectRatio\":\"4:3\",\"width\":640},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":768},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":768}},\"summaryCollageTwo\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":768},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":1600}},\"summaryCollageThree\":{\"primary\":{\"sm\":{\"aspectRatio\":\"1:1\",\"height\":768,\"width\":768},\"lg\":{\"aspectRatio\":\"1:1\",\"height\":1024,\"width\":1024},\"xxl\":{\"aspectRatio\":\"1:1\",\"height\":1280,\"width\":1280}},\"secondary\":{\"sm\":{\"aspectRatio\":\"1:1\",\"height\":768,\"width\":768},\"lg\":{\"aspectRatio\":\"16:9\",\"height\":432,\"width\":768}}},\"summaryCollageNine\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":768},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"16:9\",\"width\":1600}},\"summaryCarousel\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":768},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"16:9\",\"width\":1600}},\"mixedSummaryList\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":768},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":1280,\"sizes\":\"50vw\"},\"xxl\":{\"aspectRatio\":\"16:9\",\"width\":1600,\"sizes\":\"50vw\"}},\"summaryList\":{\"sm\":{\"aspectRatio\":\"1:1\",\"height\":768,\"width\":768},\"lg\":{\"aspectRatio\":\"4:3\",\"height\":960,\"width\":1280,\"sizes\":\"50vw\"},\"xxl\":{\"aspectRatio\":\"4:3\",\"height\":1200,\"width\":1600,\"sizes\":\"50vw\"}},\"summaryListWithAside\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":768},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":1280,\"sizes\":\"25vw\"},\"xxl\":{\"aspectRatio\":\"4:3\",\"width\":1600,\"sizes\":\"25vw\"}},\"squareCropForReviews\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":768},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":1280,\"sizes\":\"25vw\"},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":1600,\"sizes\":\"25vw\"}},\"summaryRiverWithVideos\":{\"sm\":{\"aspectRatio\":\"2:1\",\"width\":768},\"lg\":{\"aspectRatio\":\"2:1\",\"width\":1280,\"sizes\":\"25vw\"},\"xxl\":{\"aspectRatio\":\"2:1\",\"width\":1600,\"sizes\":\"25vw\"}},\"tout\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"16:9\",\"width\":1280}},\"podcast-articles\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":360},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":1024},\"md\":{\"aspectRatio\":\"4:3\",\"width\":1280},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"4:3\",\"width\":1280}},\"verso-features-rows\":{\"primary\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":768},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":768}},\"secondary\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":80},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":80}}},\"verso-multi-package-feature\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":768},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"4:3\",\"width\":1600}},\"verso-featured-item\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360},\"md\":{\"aspectRatio\":\"master\",\"width\":1024},\"lg\":{\"aspectRatio\":\"master\",\"width\":1280},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"master\",\"width\":1280}},\"verso-issue-feature\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":768},\"lg\":{\"aspectRatio\":\"3:4\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"3:4\",\"width\":1600}},\"verso-native\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":160},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":320}},\"verso-promobox\":{\"sm\":{\"aspectRatio\":\"3:4\",\"width\":240},\"md\":{\"aspectRatio\":\"16:9\",\"width\":960},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":1280},\"xl\":{\"aspectRatio\":\"16:9\",\"width\":1920}},\"verso-related\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":160},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":320}},\"verso-related-list-curation\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":160},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":320}},\"verso-river\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":160},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":768}},\"verso-river-list\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":320},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":768}},\"verso-subtopic-discovery\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":768},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"16:9\"}},\"verso-best-stories-package\":{\"primary\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":768},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"16:9\"}},\"secondary\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":768},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"16:9\"}}},\"verso-category-feature-container\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":640},\"md\":{\"aspectRatio\":\"1:1\",\"width\":1024},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":1280},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":1600},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":2560}},\"verso-summary-collection-row\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":160},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":320}},\"verso-in-page-marketing-newsletter\":{\"sm\":{\"aspectRatio\":\"16:9\",\"width\":768},\"lg\":{\"aspectRatio\":\"16:9\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"16:9\",\"width\":1600}},\"verso-ticker\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":150},\"md\":{\"aspectRatio\":\"master\",\"width\":150},\"lg\":{\"aspectRatio\":\"master\",\"width\":150},\"xl\":{\"aspectRatio\":\"master\",\"width\":150}},\"verso-topics-list\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":134},\"md\":{\"aspectRatio\":\"1:1\",\"width\":120},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":120},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":120}},\"verso-fifty-fifty\":{\"primary\":{\"sm\":{\"aspectRatio\":\"3:4\",\"width\":960},\"md\":{\"aspectRatio\":\"16:9\",\"width\":1280},\"lg\":{\"aspectRatio\":\"2:3\",\"width\":1280},\"xl\":{\"aspectRatio\":\"2:3\",\"width\":1920}},\"secondary\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":240},\"md\":{\"aspectRatio\":\"1:1\",\"width\":960},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":1280},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":1920}},\"secondaryAlternate\":{\"sm\":{\"aspectRatio\":\"2:3\",\"width\":240},\"md\":{\"aspectRatio\":\"2:3\",\"width\":960},\"lg\":{\"aspectRatio\":\"2:3\",\"width\":1280},\"xl\":{\"aspectRatio\":\"2:3\",\"width\":1920}}},\"trending-curated\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":640},\"md\":{\"aspectRatio\":\"1:1\",\"width\":1024},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":1280},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":1600},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":2560}},\"verso-celebrated-entrypoint\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":240},\"md\":{\"aspectRatio\":\"4:3\",\"width\":768},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":1024},\"xl\":{\"aspectRatio\":\"16:9\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"16:9\",\"width\":1600}},\"verso-summary-spotlight\":{\"sm\":{\"aspectRatio\":\"9:16\",\"width\":240},\"md\":{\"aspectRatio\":\"9:16\",\"width\":768},\"lg\":{\"aspectRatio\":\"6:4\",\"width\":1024},\"xl\":{\"aspectRatio\":\"6:4\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"6:4\",\"width\":1600}},\"all-fictions\":{\"sm\":{\"aspectRatio\":\"3:4\",\"width\":768},\"lg\":{\"aspectRatio\":\"3:4\",\"width\":768},\"md\":{\"aspectRatio\":\"3:4\",\"width\":768},\"xl\":{\"aspectRatio\":\"3:4\",\"width\":1024},\"xxl\":{\"aspectRatio\":\"3:4\",\"width\":1024}},\"articles-about-author\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":768},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":768},\"md\":{\"aspectRatio\":\"4:3\",\"width\":768},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":1024},\"xxl\":{\"aspectRatio\":\"4:3\",\"width\":1024}},\"more-by-author\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":768},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":768},\"md\":{\"aspectRatio\":\"4:3\",\"width\":768},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":1024},\"xxl\":{\"aspectRatio\":\"4:3\",\"width\":1024}},\"verso-flat-package\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":240},\"md\":{\"aspectRatio\":\"1:1\",\"width\":768},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":1024},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":1600}},\"verso-focus-package\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":240},\"md\":{\"aspectRatio\":\"master\",\"width\":768},\"lg\":{\"aspectRatio\":\"master\",\"width\":1024},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"master\",\"width\":1600}},\"verso-puzzles-games-package\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":240},\"md\":{\"aspectRatio\":\"4:3\",\"width\":768},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":768},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":768},\"xxl\":{\"aspectRatio\":\"4:3\",\"width\":768}},\"verso-for-you-package\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":240},\"md\":{\"aspectRatio\":\"4:3\",\"width\":768},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":1024},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":1600}},\"smartItem\":{\"dense\":{\"sm\":{\"aspectRatio\":\"4:3\"}}},\"summarySpotlight\":{\"primary\":{\"sm\":{\"aspectRatio\":\"3:4\",\"width\":768},\"lg\":{\"aspectRatio\":\"3:4\",\"width\":768},\"xxl\":{\"aspectRatio\":\"3:4\",\"width\":1024}},\"secondary\":{\"sm\":{\"aspectRatio\":\"2:2\",\"width\":320},\"lg\":{\"aspectRatio\":\"2:2\",\"width\":640}}},\"featured-contributor\":{\"sm\":{\"aspectRatio\":\"3:4\",\"width\":768},\"md\":{\"aspectRatio\":\"3:4\",\"width\":768},\"lg\":{\"aspectRatio\":\"3:4\",\"width\":768},\"xl\":{\"aspectRatio\":\"3:4\",\"width\":1024},\"xxl\":{\"aspectRatio\":\"3:4\",\"width\":1024}}},\"productArticleGallleryCarousel\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":720,\"pimImageSize\":\"-small\"},\"md\":{\"aspectRatio\":\"1:1\",\"width\":720,\"pimImageSize\":\"-medium\"},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":748,\"pimImageSize\":\"-large\"},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":748,\"pimImageSize\":\"-large\"}},\"automatedPLPpageFeature\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":720,\"pimImageSize\":\"-small\"},\"md\":{\"aspectRatio\":\"1:1\",\"width\":720,\"pimImageSize\":\"-medium\"},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":748,\"pimImageSize\":\"-large\"},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":748,\"pimImageSize\":\"-large\"}},\"shopifyContentRecommendations\":{\"sm\":{\"aspectRatio\":\"3:2\",\"height\":350,\"width\":350},\"md\":{\"aspectRatio\":\"3:2\",\"height\":350,\"width\":350},\"lg\":{\"aspectRatio\":\"3:2\",\"height\":240,\"width\":240},\"xl\":{\"aspectRatio\":\"3:2\",\"height\":350,\"width\":350}},\"unifiedProductCard\":{\"sm\":{\"aspectRatio\":\"3:4\",\"width\":720},\"md\":{\"aspectRatio\":\"3:4\",\"width\":720},\"lg\":{\"aspectRatio\":\"3:4\",\"width\":748},\"xl\":{\"aspectRatio\":\"3:4\",\"width\":748}},\"filterableBundle\":{\"sm\":{\"aspectRatio\":\"1:1\",\"width\":768},\"md\":{\"aspectRatio\":\"1:1\",\"width\":1280},\"lg\":{\"aspectRatio\":\"1:1\",\"width\":1280},\"xl\":{\"aspectRatio\":\"1:1\",\"width\":1280},\"xxl\":{\"aspectRatio\":\"1:1\",\"width\":1280}}},\"transformed\":{\"identifier\":\"magazine\\u002F2015\\u002F11\\u002F23\\u002Fdoomsday-invention-artificial-intelligence-nick-bostrom\",\"contentChannelIds\":[\"5907ef12019dfc3494e9c8f2\"],\"journey\":{\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fjourney\\u002Fcompiler\\u002Fbuild-d7c91178b91d7f28d497d62bde79989b.js\"},\"ads.page\":{\"channel\":\"magazine\",\"content\":{\"copyCount\":12144,\"imageCount\":0,\"embedCount\":14,\"ratio\":867.4285714285714,\"midContentCadence\":900},\"contentType\":\"article\",\"experiments\":{},\"keywords\":{\"copilotid\":[\"5911cbb2803aff0f1c1359ac\"],\"platform\":[\"verso\"],\"tags\":[\"magazine\",\"a-reporter-at-large\",\"2015-11-23\",\"artificial-intelligence-ai\",\"category-science-tech\",\"philosophers\",\"reporting\",\"hide-page-items\",\"override-all\",\"amp-exclude\",\"today-exclude\",\"archive-restoration\"]},\"server\":\"production\",\"slug\":\"doomsday-invention-artificial-intelligence-nick-bostrom\",\"subChannel\":\"a-reporter-at-large\",\"subSubChannel\":\"\",\"templateType\":\"mt_article_override\"},\"isNoAds\":false,\"ads.sandbox\":\"\",\"content4d\":{\"wordcount\":12001,\"brand\":\"the-new-yorker\",\"topics\":[{\"name\":\"ALLBRANDS_75\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_286\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_281\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_263\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_229\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_228\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_183\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_176\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_150\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_125\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_118\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_253\",\"score\":0.21078800055398475},{\"name\":\"ALLBRANDS_172\",\"score\":0.21078800055398475}],\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2015\\u002F11\\u002F23\\u002Fdoomsday-invention-artificial-intelligence-nick-bostrom\",\"desc\":\"Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.\",\"image\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fpass\\u002F151123_r27342.jpg\",\"keywords\":{\"list\":[{\"keyword\":\"nick bostrom\",\"score\":1},{\"keyword\":\"artificial intelligence\",\"score\":0.7672357070651912},{\"keyword\":\"oxford\",\"score\":0.745974368128514},{\"keyword\":\"raffi\",\"score\":0.7254444924215627},{\"keyword\":\"philosopher\",\"score\":0.6937607446517172},{\"keyword\":\"doomsday\",\"score\":0.5726910253414765},{\"keyword\":\"superintelligence\",\"score\":0.5625288889049718},{\"keyword\":\"intelligence explosion\",\"score\":0.5185071848027826},{\"keyword\":\"people\",\"score\":0.5166152397668053},{\"keyword\":\"khatchadourian\",\"score\":0.511003462035755},{\"keyword\":\"nonfiction book\",\"score\":0.47045544791936994},{\"keyword\":\"utopia\",\"score\":0.4401575766794139},{\"keyword\":\"vasili arkhipov\",\"score\":0.43964867613957564},{\"keyword\":\"times best-seller\",\"score\":0.4011705298216575},{\"keyword\":\"soviet\",\"score\":0.3938740524773817},{\"keyword\":\"stephen hawking\",\"score\":0.3916145531812142},{\"keyword\":\"a.i. system\",\"score\":0.38717700549081074},{\"keyword\":\"order of magnitude\",\"score\":0.3866933656800401},{\"keyword\":\"researcher\",\"score\":0.3633794659723257},{\"keyword\":\"cleverest argument\",\"score\":0.362182638979635}],\"delimited\":\"nick bostrom|artificial intelligence|oxford|raffi|philosopher|doomsday|superintelligence|intelligence explosion|people|khatchadourian|nonfiction book|utopia|vasili arkhipov|times best-seller|soviet|stephen hawking|a.i. system|order of magnitude|researcher|cleverest argument\"},\"entities\":[{\"name\":\"nick bostrom\",\"score\":1},{\"name\":\"artificial intelligence\",\"score\":0.7672357070651912},{\"name\":\"oxford\",\"score\":0.745974368128514},{\"name\":\"raffi\",\"score\":0.7254444924215627},{\"name\":\"doomsday\",\"score\":0.5726910253414765},{\"name\":\"superintelligence\",\"score\":0.5625288889049718},{\"name\":\"intelligence explosion\",\"score\":0.5185071848027826},{\"name\":\"people\",\"score\":0.5166152397668053},{\"name\":\"khatchadourian\",\"score\":0.511003462035755},{\"name\":\"utopia\",\"score\":0.4401575766794139},{\"name\":\"vasili arkhipov\",\"score\":0.43964867613957564},{\"name\":\"soviet\",\"score\":0.3938740524773817},{\"name\":\"stephen hawking\",\"score\":0.3916145531812142},{\"name\":\"order of magnitude\",\"score\":0.3866933656800401},{\"name\":\"internet\",\"score\":0.35615084478771064},{\"name\":\"elon musk\",\"score\":0.3514935780679704},{\"name\":\"twitter\",\"score\":0.3503479560091934},{\"name\":\"bill gates\",\"score\":0.34969935818877296},{\"name\":\"swiss army\",\"score\":0.34831239888414406},{\"name\":\"u.s.\",\"score\":0.3279900080377224}],\"pubdate\":\"2015-11-16T04:00:00.000Z\",\"title\":\"The Doomsday Invention\",\"sg\":[\"OBTOX35\",\"OBQHJEC\",\"OBWCIEL\",\"OBZV9GD\",\"OB59EHS\",\"OBJQJ5Q\",\"OBIFBUD\",\"OBDLO0K\",\"OBNRJH9\",\"OB4UNIY\",\"OBSWFYA\",\"OBTTUBP\",\"OBCJFRP\",\"OBAXNFG\",\"OBUY9QA\",\"OBIWJYH\",\"OB6JJPV\",\"OBRAFER\",\"OBVFVCL\",\"OBANXS2\",\"OB0GT8Y\",\"OBNRS42\",\"OBPGG8H\",\"OBJFYJP\",\"OB765CX\",\"OBBR5NA\",\"OB3EYH8\",\"OBP2CAZ\",\"OB3NGHI\",\"OBEWHZK\",\"OBT9BZ0\",\"OBDR7NL\",\"OBRWCV1\",\"OB3VXA5\",\"OBDQUUR\",\"OBK3GFZ\",\"OBSIJ0G\",\"OBBZ12H\",\"OBYAZWQ\",\"OBFAP8S\",\"OBGD5NT\",\"OBJT059\",\"OBNDYYW\",\"OBWON6S\",\"OBTPMHM\",\"OBYBNRN\",\"OBPBCAB\",\"OB5DTPU\",\"OBIBJRZ\",\"OBJRVGV\",\"OBBVCEW\",\"OBKVGFV\",\"OB8MVMT\",\"OBLKYKS\",\"OBR2SWF\",\"OBHZJB8\",\"OBK3O8F\",\"OB6T5H9\",\"OBGQKUO\",\"OBYO6FE\",\"OBYXQ6T\",\"OBJXATN\",\"OBVF8JV\",\"OBEE3TO\",\"OBSRK6N\",\"OBIYDMG\",\"OBPHOHQ\",\"OBKHL9B\",\"OBZYTDJ\",\"OBCW7A6\",\"OB5IA5J\",\"OBLVTGO\",\"OBSSALH\",\"OBPONY8\",\"OBUUCAT\",\"OBFL4RV\",\"OBK5FEC\",\"OBZAPO0\",\"OBY23HC\",\"OBBHOI0\",\"OBDTTJR\",\"OB69SLY\",\"OBITX5G\",\"OBSNBWV\",\"OBHDUXI\",\"OBOECUP\",\"OBZDFDP\",\"OBA2GPO\",\"OBEI1MY\",\"OBDIR4A\",\"OBDGAJV\",\"OB1BKCA\",\"OBYY9JU\",\"OBKDAM0\",\"OBHLACS\",\"OBQSSX3\",\"OBTDGJT\",\"OBIG3WT\",\"OB6TCWX\",\"OBD6VYO\",\"OBB86JC\",\"OBCOCGF\",\"OBVTPTH\"],\"authors\":[\"Raffi Khatchadourian\"]},\"permutiveContextualCohorts\":{\"cohorts\":[\"bycl\",\"byaz\",\"bybf\",\"bjfa\",\"bxxe\",\"bycq\",\"bxzq\"],\"gam\":[\"bycl\",\"byaz\",\"bybf\",\"bjfa\",\"bxxe\",\"bycq\",\"bxzq\"],\"xandr\":[]},\"payment\":{\"form\":\"free\",\"acceptableFormsOfTenderedPayment\":[\"free\"],\"contentClassifiers\":{\"isPaywalled\":false,\"canBeSampled\":false},\"acceptableForms\":[\"free\",\"sample\",\"sub\"],\"acceptableScopes\":[],\"groupsToRender\":[\"ads\",\"consumer-marketing\",\"subscription-workflow\"],\"entitlement\":{\"enabled\":true,\"domain\":\".newyorker.com\",\"server\":\"\"},\"products\":[{\"name\":\"newyorker.com:basic\",\"slug\":\"tny\"}],\"negotiation\":{\"content\":{\"channelSlug\":\"magazine\",\"subChannelSlug\":\"a-reporter-at-large\",\"contentType\":\"article\",\"functionalTags\":[\"override-all\"],\"isPreview\":false,\"publishDate\":\"2015-11-16T04:00:00.000Z\",\"tags\":[\"artificial intelligence (a.i.)\",\"category_science_tech\",\"philosophers\"]},\"config\":{\"acceptableForms\":[\"free\",\"sample\",\"sub\"],\"acceptableScopes\":[],\"products\":[{\"name\":\"newyorker.com:basic\",\"slug\":\"tny\"}],\"contentTypes\":[\"article\"],\"renderingRules\":[{\"form\":\"free\",\"groups\":[\"ads\",\"consumer-marketing\",\"subscription-workflow\"]},{\"form\":\"sample\",\"groups\":[\"ads\",\"consumer-marketing\",\"paywall\",\"subscription-workflow\"]},{\"form\":\"sub\",\"groups\":[\"ads\",\"consumer-marketing\",\"subs-cta\"]},{\"form\":\"\",\"groups\":[\"ads\",\"consumer-marketing\",\"paywall\",\"subscription-workflow\"]}],\"entitlementChecks\":{},\"allContentIsFree\":false,\"articlesAreFreeWhen\":[]}},\"bypass\":true},\"response\":{\"headers\":{}},\"paywall\":{\"strategy\":\"beta\",\"paragraphLimit\":1,\"gallerySlideLimit\":1,\"isMuted\":false,\"gateway\":{}},\"databricks\":{\"enabled\":false},\"access\":{\"accessCondeBaseUrl\":\"https:\\u002F\\u002Faccess.conde.io\\u002F\",\"isAccessNegotiationEnabled\":false,\"contentRestricted\":false,\"reason\":\"\"},\"coreDataLayer\":{\"content\":{\"authorIds\":\"590a0325fba4e90c8d8d92b1\",\"authorNames\":\"Raffi Khatchadourian\",\"brand\":\"The New Yorker\",\"brandSlug\":\"the-new-yorker\",\"contentId\":\"5911cbb2803aff0f1c1359ac\",\"contentLength\":\"14\",\"contentLang\":\"en-US\",\"contentSource\":\"magazine\",\"contentType\":\"article\",\"contentTitle\":\"The Philosopher of Doomsday\",\"coralCommentsStatus\":\"disabled\",\"dataSource\":\"web\",\"editorNames\":\"cg-3617-tny-aviator-script\",\"embeddedMedia\":\"\",\"functionalTags\":\"_override_all\",\"hasBuyButtons\":\"false\",\"magazineTOCSection\":\"Reporting\",\"modifiedDate\":\"2015-11-16T04:00:00.000Z\",\"noOfRevisions\":\"41\",\"pageValue\":\"all\",\"publishDate\":\"2015-11-16T00:00:00.000Z\",\"revisionVersion\":\"41\",\"section\":\"magazine\",\"subsection\":\"a reporter at large\",\"subsection2\":\"\",\"tags\":\"magazine|a reporter at large|artificial intelligence (a.i.)|category_science_tech|philosophers\",\"wordCount\":\"12144\",\"totalGalleryImages\":\"0\",\"templateType\":\"mt_article_override\",\"hasAffiliateLinks\":false,\"affiliateLinksCount\":0,\"isCommerceContent\":false},\"marketing\":{\"brand\":\"The New Yorker\"},\"page\":{\"syndicatorUrl\":\"\",\"canonical\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2015\\u002F11\\u002F23\\u002Fdoomsday-invention-artificial-intelligence-nick-bostrom\"},\"search\":{},\"site\":{\"orgId\":\"4gKgcFDnpSvUqozcC7TYUEcCiDJv\",\"orgAppId\":\"a61a3c7a-01d9-4175-8ab8-7171949de605\",\"appVersion\":\"multi-tenant\",\"env\":\"production\"},\"syndication\":{\"content\":\"false\",\"originalSource\":\"\"}},\"googleTagManagerId\":\"NX5LSK3\",\"boomerang\":{\"tags\":{\"brand\":\"the-new-yorker\",\"contentType\":\"article\",\"brand_slug\":\"the-new-yorker\",\"content_type\":\"article\"}},\"brandName\":\"The New Yorker\",\"channelSlug\":\"default\",\"universalLayout\":\"default\",\"scTheme\":{\"colors\":{\"background\":{\"adContainer\":{\"inverted\":\"gray900\",\"special\":\"gray100\",\"standard\":\"g17\",\"sticky\":\"white\"},\"black\":\"black\",\"brand\":\"white\",\"dark\":\"black\",\"light\":\"gray100\",\"white\":\"white\"},\"consumption\":{\"body\":{\"inverted\":{\"accent\":\"gray800\",\"adlabel\":\"g16\",\"bg-card\":\"gray800\",\"bg-photo\":\"gray900\",\"body\":\"gray100\",\"body-deemphasized\":\"gray500\",\"display-signature\":\"gray100\",\"display-texture\":\"black\",\"divider\":\"gray700\",\"link\":\"gray100\",\"link-hover\":\"gray500\",\"subhed\":\"gray100\"},\"special\":{\"accent\":\"gray200\",\"adlabel\":\"g16\",\"bg-card\":\"gray100\",\"bg-photo\":\"gray200\",\"body\":\"black\",\"body-deemphasized\":\"black\",\"display-signature\":\"tnyRed\",\"display-texture\":\"black\",\"divider\":\"gray200\",\"link\":\"black\",\"link-hover\":\"black\",\"subhed\":\"black\"},\"standard\":{\"accent\":\"gray200\",\"adlabel\":\"g16\",\"bg-card\":\"white\",\"bg-photo\":\"white\",\"body\":\"black\",\"body-deemphasized\":\"gray700\",\"display-signature\":\"tnyRed\",\"display-texture\":\"black\",\"divider\":\"gray200\",\"link\":\"black\",\"link-hover\":\"black\",\"subhed\":\"black\"}},\"lead\":{\"inverted\":{\"accent\":\"gray800\",\"accreditation\":\"gray200\",\"background\":\"black\",\"context-signature\":\"white\",\"context-tertiary\":\"white\",\"context-texture\":\"white\",\"description\":\"gray100\",\"divider\":\"gray800\",\"heading\":\"white\",\"heading-background\":\"black\",\"link\":\"gray100\",\"link-hover\":\"gray100\",\"syndication\":\"gray500\"},\"special\":{\"accent\":\"gray200\",\"accreditation\":\"black\",\"background\":\"gray100\",\"context-signature\":\"tnyRed\",\"context-tertiary\":\"black\",\"context-texture\":\"white\",\"description\":\"black\",\"divider\":\"gray200\",\"heading\":\"black\",\"heading-background\":\"white\",\"link\":\"black\",\"link-hover\":\"black\",\"syndication\":\"gray700\"},\"standard\":{\"accent\":\"gray200\",\"accreditation\":\"black\",\"background\":\"white\",\"context-signature\":\"tnyRed\",\"context-tertiary\":\"gray700\",\"context-texture\":\"white\",\"description\":\"gray900\",\"divider\":\"gray200\",\"heading\":\"black\",\"heading-background\":\"white\",\"link\":\"black\",\"link-hover\":\"black\",\"syndication\":\"gray700\"}}},\"discovery\":{\"body\":{\"black\":{\"accent\":\"white\",\"accreditation\":\"gray200\",\"background\":\"black\",\"border\":\"white30\",\"context-signature\":\"gray100\",\"context-tertiary\":\"gray100\",\"context-texture\":\"black\",\"description\":\"gray100\",\"divider\":\"gray800\",\"heading\":\"white\",\"heading-background\":\"black\",\"syndication\":\"gray500\"},\"brand\":{\"accent\":\"black\",\"accreditation\":\"gray900\",\"background\":\"white\",\"border\":\"gray200\",\"context-signature\":\"tnyRed\",\"context-tertiary\":\"gray700\",\"context-texture\":\"white\",\"description\":\"black\",\"divider\":\"gray200\",\"heading\":\"black\",\"heading-background\":\"white\",\"syndication\":\"gray500\"},\"dark\":{\"accent\":\"gray800\",\"accreditation\":\"gray200\",\"background\":\"black\",\"border\":\"gray800\",\"context-signature\":\"gray100\",\"context-tertiary\":\"gray500\",\"context-texture\":\"black\",\"description\":\"gray100\",\"divider\":\"gray800\",\"heading\":\"white\",\"heading-background\":\"black\",\"syndication\":\"gray500\"},\"light\":{\"accent\":\"black\",\"accreditation\":\"gray900\",\"background\":\"gray100\",\"border\":\"gray200\",\"context-signature\":\"tnyRed\",\"context-tertiary\":\"gray700\",\"context-texture\":\"white\",\"description\":\"black\",\"divider\":\"gray200\",\"heading\":\"black\",\"heading-background\":\"white\",\"syndication\":\"gray500\"},\"white\":{\"accent\":\"black\",\"accreditation\":\"gray900\",\"background\":\"white\",\"border\":\"gray200\",\"context-signature\":\"tnyRed\",\"context-tertiary\":\"gray700\",\"context-texture\":\"black\",\"description\":\"gray800\",\"divider\":\"gray200\",\"heading\":\"black\",\"heading-background\":\"white\",\"syndication\":\"gray500\"}},\"lead\":{\"primary\":{\"accent\":\"tnyRed\",\"background\":\"white\",\"description\":\"black\",\"divider\":\"gray200\",\"hed\":\"black\",\"link\":\"black\",\"link-hover\":\"gray800\"},\"secondary\":{\"accent\":\"tnyRed\",\"background\":\"gray100\",\"description\":\"black\",\"divider\":\"gray200\",\"hed\":\"black\",\"link\":\"black\",\"link-hover\":\"gray800\"}}},\"foundation\":{\"ad\":{\"background-inverted\":\"ad2\",\"background-standard\":\"ad1\",\"label-inverted\":\"ad4\",\"label-standard\":\"ad3\"},\"collapsed-menu\":{\"nav-link\":{\"default\":\"black\",\"hover\":\"gray800\"},\"utility-link\":{\"default\":\"black\",\"hover\":\"gray800\"}},\"expanded-context\":\"gray900\",\"expanded-menu\":{\"nav-link\":{\"default\":\"black\",\"hover\":\"gray800\"},\"utility-link\":{\"default\":\"black\",\"hover\":\"gray800\"}},\"expanded-utility\":{\"nav-link\":{\"default\":\"tnyBlue\",\"hover\":\"tnyBlueHovered\"}},\"footer\":{\"accent\":\"gray800\",\"bg\":\"gray900\",\"context\":\"white\",\"links\":{\"primary\":\"gray500\",\"secondary\":\"gray500\"},\"meta-primary\":\"white\",\"meta-secondary\":\"gray500\",\"social\":{\"hover\":\"white\"}},\"icon\":{\"default\":\"black\",\"hover\":\"gray800\"},\"menu\":{\"dividers\":\"gray200\"},\"menu-bg\":{\"accent\":\"gray200\",\"collapsed\":\"white\",\"expanded\":\"white\"}},\"interactive\":{\"base\":{\"black\":\"black\",\"body\":\"black\",\"border\":\"gray200\",\"brand-primary\":\"tnyBlue\",\"brand-secondary\":\"tnyBlueHovered\",\"dark\":\"gray700\",\"deemphasized\":\"gray700\",\"highlight\":\"gray100\",\"light\":\"gray200\",\"white\":\"white\",\"hover\":\"hover1\"},\"feedback\":{\"alert-primary\":\"white\",\"alert-secondary\":\"tnyBlue\",\"invalid-primary\":\"errorRed\",\"invalid-secondary\":\"white\",\"notice-primary\":\"notice2\",\"notice-secondary\":\"notice1\",\"valid-primary\":\"successGreen\",\"valid-secondary\":\"successGreenLight\"},\"social\":{\"primary\":\"black\",\"primary-hover\":\"gray700\",\"secondary\":\"white\",\"secondary-hover\":\"gray700\"}},\"palette\":{\"tnyRed\":\"rgba(219,51,52,1)\",\"black\":\"rgba(0,0,0,1)\",\"white\":\"rgba(255,255,255,1)\",\"tnyBlue\":\"rgba(8,121,191,1)\",\"tnyBlueHovered\":\"rgba(7,88,139,1)\",\"ad1\":\"rgba(0,0,0,0.07000000029802322)\",\"ad2\":\"rgba(255,255,255,0.10000000149011612)\",\"ad3\":\"rgba(26,26,26,1)\",\"ad4\":\"rgba(247,247,247,1)\",\"gray900\":\"rgba(18,18,18,1)\",\"gray800\":\"rgba(51,51,51,1)\",\"gray700\":\"rgba(102,102,102,1)\",\"gray500\":\"rgba(162,162,162,1)\",\"gray200\":\"rgba(229,229,229,1)\",\"gray100\":\"rgba(245,245,245,1)\",\"white30\":\"rgba(255,255,255,0.3)\",\"errorRed\":\"rgba(170,27,25,1)\",\"successGreen\":\"rgba(8,195,105,1)\",\"successGreenLight\":\"rgba(222,237,229,1)\",\"d3\":\"rgba(104,174,218,1)\",\"g12\":\"rgba(77,76,76,1)\",\"g13\":\"rgba(59,89,152,1)\",\"g14\":\"rgba(0,123,182,1)\",\"g15\":\"rgba(189,8,28,1)\",\"g16\":\"rgba(151,151,151,1)\",\"g17\":\"rgba(255,255,255,0)\",\"notice1\":\"rgba(247,196,66,1)\",\"notice2\":\"rgba(255,242,208,1)\",\"hover1\":\"rgba(222, 235, 255, 1)\"}},\"container-styles\":{\"block-background\":{\"pattern\":[{\"solid\":{\"color\":\"#FFFFFF\"}}]},\"content-background\":null,\"item-background\":null,\"lede-background\":null,\"main-background\":null,\"page-background\":{\"pattern\":[{\"solid\":{\"color\":\"#FFFFFF\"}}]}},\"decorations\":{\"borderRadius\":0,\"borderStyle\":\"solid\",\"borderWidth\":1,\"cardRadiusLg\":0,\"cardRadiusMd\":0,\"cardRadiusSm\":0,\"dividerStyle\":\"solid\",\"dividerWidth\":1,\"iconProfileRadius\":0,\"navigationRuleStyle\":\"solid\",\"navigationRuleWidth\":1,\"sectionOrnamentLength\":50,\"sectionOrnamentStyle\":\"solid\",\"sectionOrnamentWidth\":8,\"titleBorderDecoration\":{\"type\":\"none\",\"file\":\"\",\"thickness\":0,\"offset\":0,\"placement\":\"grid\",\"repeat\":\"round\",\"width\":\"100%\",\"align\":\"center\",\"side\":\"bottom\"},\"sectionBorderPrimary\":{\"type\":\"none\",\"file\":\"\",\"thickness\":0,\"offset\":0,\"placement\":\"grid\",\"repeat\":\"round\",\"width\":\"100%\",\"align\":\"center\",\"side\":\"bottom\"},\"sectionBorderSecondary\":{\"type\":\"none\",\"file\":\"\",\"thickness\":0,\"offset\":0,\"placement\":\"grid\",\"repeat\":\"round\",\"width\":\"100%\",\"align\":\"center\",\"side\":\"bottom\"},\"sectionBorderTertiary\":{\"type\":\"none\",\"file\":\"\",\"thickness\":0,\"offset\":0,\"placement\":\"grid\",\"repeat\":\"round\",\"width\":\"100%\",\"align\":\"center\",\"side\":\"bottom\"},\"badgeFlagLg\":{\"file\":\"\",\"height\":0,\"offsetX\":0,\"offsetY\":0,\"placement\":\"center\",\"rotation\":0,\"type\":\"none\",\"width\":0,\"x\":\"start\",\"y\":\"start\"},\"badgeFlagMd\":{\"file\":\"\",\"height\":0,\"offsetX\":0,\"offsetY\":0,\"placement\":\"center\",\"rotation\":0,\"type\":\"none\",\"width\":0,\"x\":\"start\",\"y\":\"start\"},\"badgeFlagSm\":{\"file\":\"g\",\"height\":0,\"offsetX\":0,\"offsetY\":0,\"placement\":\"center\",\"rotation\":0,\"type\":\"none\",\"width\":0,\"x\":\"start\",\"y\":\"start\"},\"badgePrimaryLg\":{\"file\":\"\",\"height\":0,\"offsetX\":0,\"offsetY\":0,\"placement\":\"center\",\"rotation\":0,\"type\":\"none\",\"width\":0,\"x\":\"start\",\"y\":\"start\"},\"badgePrimaryMd\":{\"file\":\"\",\"height\":0,\"offsetX\":0,\"offsetY\":0,\"placement\":\"center\",\"rotation\":0,\"type\":\"none\",\"width\":0,\"x\":\"start\",\"y\":\"start\"},\"badgePrimarySm\":{\"file\":\"\",\"height\":0,\"offsetX\":0,\"offsetY\":0,\"placement\":\"center\",\"rotation\":0,\"type\":\"none\",\"width\":0,\"x\":\"start\",\"y\":\"start\"},\"badgeSecondaryLg\":{\"file\":\"\",\"height\":0,\"offsetX\":0,\"offsetY\":0,\"placement\":\"center\",\"rotation\":0,\"type\":\"none\",\"width\":0,\"x\":\"start\",\"y\":\"start\"},\"badgeSecondaryMd\":{\"file\":\"\",\"height\":0,\"offsetX\":0,\"offsetY\":0,\"placement\":\"center\",\"rotation\":0,\"type\":\"none\",\"width\":0,\"x\":\"start\",\"y\":\"start\"},\"badgeSecondarySm\":{\"file\":\"\",\"height\":0,\"offsetX\":0,\"offsetY\":0,\"placement\":\"center\",\"rotation\":0,\"type\":\"none\",\"width\":0,\"x\":\"start\",\"y\":\"start\"},\"backgroundImagePrimary\":{\"type\":\"none\",\"file\":\"\",\"attachment\":\"initial\",\"position\":\"initial\",\"repeat\":\"no-repeat\",\"size\":\"cover\"},\"backgroundImageSecondary\":{\"type\":\"none\",\"file\":\"\",\"attachment\":\"initial\",\"position\":\"initial\",\"repeat\":\"no-repeat\",\"size\":\"cover\"}},\"interactive\":{\"links\":{\"default\":{\"active\":{\"style\":\"null\"},\"focus\":{\"style\":\"null\"},\"hover\":{\"style\":\"underline\"},\"link\":{\"style\":\"underline\"},\"visited\":{\"style\":\"underline\"}},\"navigation\":{\"active\":{\"style\":\"null\"},\"focus\":{\"style\":\"null\"},\"hover\":{\"style\":\"underline\"},\"link\":{\"style\":\"null\"},\"visited\":{\"style\":\"null\"}}}},\"navigation\":{\"header\":{\"container-spacing-unit\":0,\"lg\":{\"align-logo\":\"left\",\"hasPrimary\":false,\"hasSecondary\":true,\"logo-height\":3.25,\"logo-padding\":[0,3]},\"max-width\":1280,\"md\":{\"align-logo\":\"left\",\"hasPrimary\":false,\"hasSecondary\":true,\"logo-height\":3.25,\"logo-padding\":[0,3]},\"sm\":{\"align-logo\":\"center\",\"hasPrimary\":false,\"hasSecondary\":true,\"logo-height\":2,\"logo-padding\":[0,3]},\"xl\":{\"align-logo\":\"left\",\"hasPrimary\":true,\"hasSecondary\":true,\"logo-height\":3,\"logo-padding\":[0,3]}}},\"spacing\":{\"box-inset\":10,\"spacing-0\":\"0px\",\"spacing-4\":\"4px\",\"spacing-8\":\"8px\",\"spacing-12\":\"12px\",\"spacing-16\":\"16px\",\"spacing-24\":\"24px\",\"spacing-32\":\"32px\",\"spacing-40\":\"40px\",\"spacing-48\":\"48px\",\"spacing-56\":\"56px\",\"spacing-64\":\"64px\",\"spacing-96\":\"96px\",\"spacing-128\":\"128px\",\"section\":{\"gap-sm\":\"spacing-0\",\"gap-md\":\"spacing-0\",\"padding-top-sm\":\"spacing-0\",\"padding-top-md\":\"spacing-0\",\"padding-bottom-sm\":\"spacing-0\",\"padding-bottom-md\":\"spacing-0\"}},\"motion\":{\"duration\":{\"0\":\"0ms\",\"100\":\"100ms\",\"150\":\"150ms\",\"200\":\"200ms\",\"300\":\"300ms\",\"400\":\"400ms\",\"500\":\"500ms\",\"800\":\"800ms\",\"1200\":\"1200ms\"},\"delay\":{\"50\":\"0ms\",\"100\":\"100ms\",\"150\":\"150ms\",\"200\":\"200ms\",\"300\":\"300ms\",\"400\":\"400ms\",\"500\":\"500ms\",\"700\":\"700ms\"},\"easing\":{\"linear\":\"0,0,1,1\",\"standard-in-and-out\":\"0.37,0,0.63,1\",\"standard-in\":\"0.12,0,0.39,0\",\"standard-out\":\"0.61,1,0.88,1\",\"emphasized-in-and-out\":\"0.65,0,0.35,1\",\"emphasized-in\":\"0.32,0,0.67,0\",\"emphasized-out\":\"0.33,1,0.68,1\",\"playful-in-and-out\":\"0.83,0,0.17,1\",\"playful-in\":\"0.64,0,0.78,0\",\"playful-out\":\"0.22,1,0.36,1\"},\"pattern\":{\"move\":{\"easing\":\"standard-in-and-out\",\"duration\":\"duration.200\"},\"move-enter\":{\"easing\":\"standard-in-and-out\",\"duration\":\"duration.200\"},\"move-exit\":{\"easing\":\"standard-in-and-out\",\"duration\":\"duration.200\"},\"fade-in-instant\":{\"easing\":\"standard-in-and-out\",\"duration\":\"duration.0\"},\"fade-in-medium\":{\"easing\":\"standard-in-and-out\",\"duration\":\"duration.300\"},\"fade-out-medium\":{\"easing\":\"standard-in-and-out\",\"duration\":\"duration.300\"},\"collapse\":{\"easing\":\"standard-in-and-out\",\"duration\":\"duration.300\"},\"expand\":{\"easing\":\"emphasized-in-and-out\",\"duration\":\"duration.400\"},\"spin\":{\"easing\":\"linear\",\"duration\":\"duration.1200\"},\"rotate180\":{\"easing\":\"emphasized-in-and-out\",\"duration\":\"duration.400\"}}},\"typography\":{\"definitions\":{\"consumptionEditorial\":{\"body-core\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.5,\"mobile-size\":21,\"weight\":400},\"body-feature\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.4300000190734863,\"mobile-size\":20,\"weight\":400},\"citation\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.5,\"mobile-size\":18,\"weight\":400},\"description-core\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"fontSizeMd\":21,\"italic\":true,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.3333333333,\"lineHeightMd\":1.3333333333,\"mobile-size\":18,\"weight\":400},\"description-embed\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"fontSizeMd\":22,\"italic\":true,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.2,\"lineHeightMd\":1.3,\"mobile-size\":20,\"weight\":400},\"description-feature\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"italic\":true,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.2,\"mobile-size\":20,\"weight\":400},\"display-large\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1,\"mobile-size\":60,\"weight\":400},\"display-medium\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1799999583851208,\"mobile-size\":22,\"weight\":400},\"display-small\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"fontSizeMd\":26.399999618530273,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1799999583851208,\"lineHeightMd\":1.1799242450240777,\"mobile-size\":22,\"weight\":400},\"hed-bulletin\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"fontSizeMd\":28,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1428571429,\"lineHeightMd\":1.1428571429,\"mobile-size\":28,\"weight\":400},\"hed-feature\":{\"case\":\"normal\",\"family\":\"IrvinHeadingPro\",\"fontSizeLg\":44.20000076293945,\"fontSizeMd\":40.79999923706055,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1299999461454504,\"lineHeightLg\":1.130090495492055,\"lineHeightMd\":1.1299019445139182,\"mobile-size\":34,\"weight\":400},\"hed-standard\":{\"case\":\"normal\",\"family\":\"IrvinHeadingPro\",\"fontSizeLg\":42,\"fontSizeMd\":36,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.06666666667,\"lineHeightLg\":1.1111111111,\"lineHeightMd\":1.1111111111,\"mobile-size\":30,\"weight\":400},\"subhed-aux-primary\":{\"case\":\"normal\",\"family\":\"NeutrafaceNewYorker\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.3333333333333333,\"mobile-size\":24,\"weight\":600},\"subhed-aux-secondary\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.25,\"mobile-size\":16,\"weight\":400},\"subhed-break-primary\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"fontSizeMd\":32,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.125,\"lineHeightMd\":1.125,\"mobile-size\":32,\"weight\":400},\"subhed-break-secondary\":{\"case\":\"normal\",\"family\":\"IrvinHeadingPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1428571429,\"mobile-size\":28,\"weight\":400}},\"discovery\":{\"description-core\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.411764705882353,\"mobile-size\":17,\"weight\":400},\"description-feature\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"fontSizeMd\":17,\"fontSizeLg\":21,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.411764705882353,\"lineHeightMd\":1.411764705882353,\"lineHeightLg\":1.3333333333,\"mobile-size\":17,\"weight\":400},\"description-page\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"fontSizeMd\":21,\"italic\":true,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.411764705882353,\"lineHeightMd\":1.3333333333,\"mobile-size\":18,\"weight\":400},\"hed-break-out\":{\"case\":\"normal\",\"family\":\"IrvinHeadingPro\",\"fontSizeLg\":46,\"fontSizeMd\":28,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1428571428571428,\"lineHeightLg\":1.2173913043478262,\"lineHeightMd\":1.1428571428571428,\"mobile-size\":28,\"weight\":400},\"hed-bulletin-primary\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.411764705882353,\"mobile-size\":17,\"weight\":400},\"hed-bulletin-secondary\":{\"case\":\"normal\",\"family\":\"IrvinHeadingPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.375,\"mobile-size\":16,\"weight\":400},\"hed-core-primary\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"fontSizeMd\":22,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.2727272727272727,\"lineHeightMd\":1.2727272727272727,\"mobile-size\":22,\"weight\":400},\"hed-core-secondary\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"fontSizeMd\":22,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.2727272727272727,\"lineHeightMd\":1.2727272727272727,\"mobile-size\":18,\"weight\":400},\"hed-feature\":{\"case\":\"normal\",\"family\":\"IrvinHeadingPro\",\"fontSizeLg\":36,\"fontSizeMd\":28,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1428571428571428,\"lineHeightLg\":1.1111111111111112,\"lineHeightMd\":1.1428571428571428,\"mobile-size\":28,\"weight\":400},\"page-hed-section\":{\"case\":\"normal\",\"family\":\"IrvinHeadingPro\",\"fontSizeMd\":41.599998474121094,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1299999952316284,\"lineHeightMd\":1.1300480780252338,\"mobile-size\":32,\"weight\":400},\"page-hed-subsection\":{\"case\":\"normal\",\"family\":\"IrvinHeadingPro\",\"fontSizeMd\":41.599998474121094,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1299999952316284,\"lineHeightMd\":1.1300480780252338,\"mobile-size\":32,\"weight\":400},\"subhed-section-collection\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"italic\":true,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.3333333333,\"mobile-size\":21,\"weight\":400},\"subhed-section-primary\":{\"case\":\"normal\",\"family\":\"IrvinHeadingPro\",\"fontSizeMd\":28,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.0909090909090908,\"lineHeightMd\":1.1428571428571428,\"mobile-size\":22,\"weight\":400},\"subhed-section-secondary\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.2,\"mobile-size\":20,\"weight\":400},\"subhed-section-tertiary\":{\"case\":\"normal\",\"family\":\"IrvinText\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.3333333333333333,\"mobile-size\":12,\"weight\":400}},\"foundation\":{\"ad-label\":{\"case\":\"normal\",\"family\":\"Helvetica\",\"italic\":false,\"letter-spacing\":0.2,\"ligatures\":null,\"line-height\":1.2,\"mobile-size\":10,\"weight\":400},\"link-feature\":{\"case\":\"normal\",\"family\":\"IrvinHeadingPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.600000023841858,\"mobile-size\":16,\"weight\":400},\"link-primary\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2699999809265137,\"mobile-size\":12,\"weight\":500},\"link-secondary\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2699999809265137,\"mobile-size\":12,\"weight\":500},\"link-utility\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2699999809265137,\"mobile-size\":12,\"weight\":500},\"list\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.7100000381469727,\"mobile-size\":14,\"weight\":500},\"meta-primary\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.2399999955121208,\"mobile-size\":17,\"weight\":400},\"meta-secondary\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2699999809265137,\"mobile-size\":10,\"weight\":500},\"title-primary\":{\"case\":\"normal\",\"family\":\"IrvinText\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.3000000544956751,\"mobile-size\":14,\"weight\":400},\"title-secondary\":{\"case\":\"normal\",\"family\":\"IrvinText\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.3000000544956751,\"mobile-size\":14,\"weight\":400}},\"globalEditorial\":{\"accreditation-core\":{\"case\":\"normal\",\"family\":\"NeutrafaceNewYorker\",\"fontSizeMd\":15,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1428571429,\"lineHeightMd\":1.1428571429,\"mobile-size\":14,\"weight\":600},\"accreditation-feature\":{\"case\":\"normal\",\"family\":\"NeutrafaceNewYorker\",\"fontSizeMd\":15,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1428571429,\"lineHeightMd\":1.1428571429,\"mobile-size\":14,\"weight\":600},\"ad-label\":{\"case\":\"uppercase\",\"family\":\"NeutrafaceNewYorker\",\"italic\":false,\"letter-spacing\":0.15,\"ligatures\":null,\"line-height\":1,\"mobile-size\":10,\"weight\":400},\"context-primary\":{\"case\":\"normal\",\"family\":\"IrvinText\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.3333333333333333,\"mobile-size\":12,\"weight\":400},\"context-secondary\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2307692308,\"mobile-size\":13,\"weight\":400},\"context-tertiary\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.1666666666666667,\"mobile-size\":12,\"weight\":500},\"context-title\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2307692308,\"mobile-size\":13,\"weight\":500},\"numerical-large\":{\"case\":\"normal\",\"family\":\"IrvinHeadingPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1799999583851208,\"mobile-size\":22,\"weight\":400},\"numerical-small\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2307692308,\"mobile-size\":13,\"weight\":500},\"syndication\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2307692308,\"mobile-size\":13,\"weight\":500},\"tags\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2307692308,\"mobile-size\":13,\"weight\":500}},\"utility\":{\"assistive-text\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2307692308,\"mobile-size\":13,\"weight\":400},\"body\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2307692308,\"mobile-size\":13,\"weight\":400},\"button-bulletin\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2307692308,\"mobile-size\":13,\"weight\":500},\"button-core\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2307692308,\"mobile-size\":13,\"weight\":500},\"button-utility\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2307692308,\"mobile-size\":13,\"weight\":500},\"card-heading\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.2727272727272727,\"mobile-size\":22,\"weight\":400},\"description\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.411764705882353,\"mobile-size\":17,\"weight\":400},\"heading\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"fontSizeMd\":40,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1428571428571428,\"lineHeightMd\":1.2,\"mobile-size\":28,\"weight\":400},\"input-core\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.309999942779541,\"mobile-size\":16,\"weight\":400},\"input-feature\":{\"case\":\"normal\",\"family\":\"Graphik\",\"fontSizeMd\":25.200000762939453,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.189999989100865,\"lineHeightMd\":1.1900793199666546,\"mobile-size\":21,\"weight\":400},\"label\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.3100000528188853,\"mobile-size\":13,\"weight\":500},\"landing-body\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.2307692307692308,\"mobile-size\":13,\"weight\":400},\"landing-description\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.3333333333,\"mobile-size\":21,\"weight\":400},\"landing-heading\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"fontSizeLg\":46,\"fontSizeMd\":40,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1428571428571428,\"lineHeightLg\":1.2173913043478262,\"lineHeightMd\":1.2,\"mobile-size\":28,\"weight\":400},\"landing-subheading\":{\"case\":\"normal\",\"family\":\"Graphik\",\"fontSizeMd\":28,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.2727272727272727,\"lineHeightMd\":1.1428571428571428,\"mobile-size\":22,\"weight\":500},\"list-heading\":{\"case\":\"normal\",\"family\":\"Graphik\",\"fontSizeMd\":16,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.7142857142857142,\"lineHeightMd\":1.5,\"mobile-size\":14,\"weight\":400},\"modal-body\":{\"case\":\"normal\",\"family\":\"Graphik\",\"italic\":false,\"letter-spacing\":-0.0125,\"ligatures\":null,\"line-height\":1.25,\"mobile-size\":16,\"weight\":400},\"modal-hed\":{\"case\":\"normal\",\"family\":\"TNYAdobeCaslonPro\",\"fontSizeMd\":28,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.1428571428571428,\"lineHeightMd\":1.1428571428571428,\"mobile-size\":28,\"weight\":400},\"pricing\":{\"case\":\"normal\",\"family\":\"Graphik\",\"fontSizeMd\":16,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.5,\"lineHeightMd\":1.5,\"mobile-size\":16,\"weight\":500},\"pricing-secondary\":{\"case\":\"normal\",\"family\":\"Graphik\",\"fontSizeMd\":16,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":2,\"lineHeightMd\":1.75,\"mobile-size\":14,\"weight\":400},\"subheading\":{\"case\":\"normal\",\"family\":\"Graphik\",\"fontSizeMd\":22,\"italic\":false,\"letter-spacing\":0,\"ligatures\":null,\"line-height\":1.3333333333333333,\"lineHeightMd\":1.2727272727272727,\"mobile-size\":18,\"weight\":500}},\"commerce\":{\"brand-name\":{\"case\":\"uppercase\",\"family\":\"Helvetica\",\"italic\":false,\"letter-spacing\":0.05714285799435207,\"ligatures\":null,\"line-height\":1.1428571428571428,\"mobile-size\":14,\"weight\":700},\"call-to-action\":{\"case\":\"uppercase\",\"family\":\"Helvetica\",\"italic\":false,\"letter-spacing\":0.033333333830038704,\"ligatures\":null,\"line-height\":1.3333333333333333,\"mobile-size\":12,\"weight\":400},\"label\":{\"case\":\"uppercase\",\"family\":\"Helvetica\",\"italic\":false,\"letter-spacing\":0.11666666467984517,\"ligatures\":null,\"line-height\":1.3333333333333333,\"mobile-size\":12,\"weight\":400},\"product-description\":{\"case\":\"normal\",\"family\":\"Helvetica\",\"italic\":false,\"letter-spacing\":-0.007692307806932009,\"ligatures\":null,\"line-height\":1.5384615384615385,\"mobile-size\":13,\"weight\":300},\"product-title\":{\"case\":\"normal\",\"family\":\"Helvetica\",\"italic\":false,\"letter-spacing\":0.014285714498588018,\"ligatures\":null,\"line-height\":1.3571428571428572,\"mobile-size\":14,\"weight\":400}}},\"typefaces\":{\"Graphik\":{\"fallback\":\"'Helvetica Neue, Helvetica, Arial, sans-serif'\",\"imports\":[{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FGraphik-Medium.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FGraphik-Medium.woff2'\"},\"italic\":false,\"weight\":700},{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FGraphik-MediumItalic.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FGraphik-MediumItalic.woff2'\"},\"italic\":true,\"weight\":700},{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FGraphik-Medium.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FGraphik-Medium.woff2'\"},\"italic\":false,\"weight\":500},{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FGraphik-MediumItalic.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FGraphik-MediumItalic.woff2'\"},\"italic\":true,\"weight\":500},{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FGraphik-Regular-Web.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FGraphik-Regular-Web.woff2'\"},\"italic\":false,\"weight\":400},{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FGraphik-RegularItalic-Web.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FGraphik-RegularItalic-Web.woff2'\"},\"italic\":true,\"weight\":400}]},\"IrvinHeadingPro\":{\"fallback\":\"'Helvetica Neue, Helvetica, Arial, sans-serif'\",\"imports\":[{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff2'\"},\"italic\":false,\"weight\":400},{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff2'\"},\"italic\":true,\"weight\":400},{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff2'\"},\"italic\":false,\"weight\":600},{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff2'\"},\"italic\":true,\"weight\":600}]},\"IrvinText\":{\"fallback\":\"'IrvinHeadingPro, Georgia, Times New Roman, Times, serif'\",\"imports\":[{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FIrvinText.woff'\"},\"italic\":false,\"weight\":400}]},\"NeutrafaceNewYorker\":{\"fallback\":\"'Helvetica Neue, Helvetica, Arial, sans-serif'\",\"imports\":[{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FNeutrafaceNewYorkerWeb-SemiBold.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FNeutrafaceNewYorkerWeb-SemiBold.woff2'\"},\"italic\":false,\"weight\":600}]},\"TNYAdobeCaslonPro\":{\"fallback\":\"'Times New Roman, Times, serif'\",\"imports\":[{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Regular.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Regular.woff2'\"},\"italic\":false,\"weight\":400},{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Italic.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Italic.woff2'\"},\"italic\":true,\"weight\":400},{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FTNYAdobeCaslonPro-SemiBold.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FTNYAdobeCaslonPro-SemiBold.woff2'\"},\"italic\":false,\"weight\":600},{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FTNYAdobeCaslonPro-SemiBoldItalic.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FTNYAdobeCaslonPro-SemiBoldItalic.woff2'\"},\"italic\":true,\"weight\":600},{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Bold.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Bold.woff2'\"},\"italic\":false,\"weight\":700},{\"files\":{\"woff\":\"'assets\\u002Ffonts\\u002FTNYAdobeCaslonPro-BoldItalic.woff'\",\"woff2\":\"'assets\\u002Ffonts\\u002FTNYAdobeCaslonPro-BoldItalic.woff2'\"},\"italic\":true,\"weight\":700}]},\"Helvetica\":{\"fallback\":\"'helvetica, sans-serif'\"},\"Georgia\":{\"fallback\":\"'helvetica, sans-serif'\"}},\"fontFaces\":\"\\n      \\n@font-face {\\n    font-family: Adobe Caslon;\\n    font-weight: 400;\\n    font-style: italic;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Italic.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Italic.woff\\\") format(\\\"woff\\\"); }\\n  \\n  @font-face {\\n    font-family: Adobe Caslon;\\n    font-weight: 400;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Regular.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Regular.woff\\\") format(\\\"woff\\\"); }\\n    \\n      \\n\\n  @font-face {\\n    font-family: Graphik;\\n    font-weight: 700;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FGraphik-Medium.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FGraphik-Medium.woff\\\") format(\\\"woff\\\"); }\\n  \\n  @font-face {\\n    font-family: Graphik;\\n    font-weight: 700;\\n    font-style: italic;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FGraphik-MediumItalic.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FGraphik-MediumItalic.woff\\\") format(\\\"woff\\\"); }\\n\\n  @font-face {\\n    font-family: Graphik;\\n    font-weight: 500;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FGraphik-Medium.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FGraphik-Medium.woff\\\") format(\\\"woff\\\"); }\\n  \\n  @font-face {\\n    font-family: Graphik;\\n    font-weight: 500;\\n    font-style: italic;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FGraphik-MediumItalic.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FGraphik-MediumItalic.woff\\\") format(\\\"woff\\\"); }\\n  \\n  @font-face {\\n    font-family: Graphik;\\n    font-weight: 400;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FGraphik-Regular-Web.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FGraphik-Regular-Web.woff\\\") format(\\\"woff\\\"); }\\n  \\n  @font-face {\\n    font-family: Graphik;\\n    font-weight: 400;\\n    font-style: italic;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FGraphik-RegularItalic-Web.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FGraphik-RegularItalic-Web.woff\\\") format(\\\"woff\\\"); }\\n    \\n      \\n  @font-face {\\n    font-family: Graphik Web;\\n    font-weight: 500;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FGraphik-Medium.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FGraphik-Medium.woff\\\") format(\\\"woff\\\"); }\\n    \\n      \\n  @font-face {\\n    font-family: Irvin Heading;\\n    font-weight: 400;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff\\\") format(\\\"woff\\\"); }\\n    \\n      \\n  @font-face {\\n    font-family: IrvinText;\\n    font-weight: 400;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FIrvinText-Regular.woff\\\") format(\\\"woff\\\"); }\\n    \\n      \\n  @font-face {\\n    font-family: IrvinHeadingPro;\\n    font-weight: 400;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff\\\") format(\\\"woff\\\"); }\\n\\n  @font-face {\\n    font-family: IrvinHeadingPro;\\n    font-weight: bold;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff\\\") format(\\\"woff\\\"); }\\n    \\n  @font-face {\\n    font-family: IrvinHeadingPro;\\n    font-weight: 400;\\n    font-style: italic;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff\\\") format(\\\"woff\\\"); }\\n\\n  @font-face {\\n    font-family: IrvinHeadingPro;\\n    font-weight: bold;\\n    font-style: italic;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYIrvinPro-HeadingSimple.woff\\\") format(\\\"woff\\\"); }  \\n    \\n      \\n  @font-face {\\n    font-family: Irvin Text;\\n    font-weight: 400;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FIrvinText-Regular.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FIrvinText-Regular.woff\\\") format(\\\"woff\\\"); }\\n    \\n      \\n  @font-face {\\n    font-family: Neutra Face;\\n    font-weight: 600;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FNeutrafaceNewYorker-SemiBold.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FNeutrafaceNewYorker-SemiBold.woff\\\") format(\\\"woff\\\"); }\\n    \\n      \\n  @font-face {\\n    font-family: NeutrafaceNewYorker;\\n    font-weight: 600;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FNeutrafaceNewYorker-SemiBold.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FNeutrafaceNewYorker-SemiBold.woff\\\") format(\\\"woff\\\"); }\\n    \\n      \\n  @font-face {\\n    font-family: TNYAdobeCaslonPro;\\n    font-weight: 700;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Bold.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Bold.woff\\\") format(\\\"woff\\\"); }\\n  \\n  @font-face {\\n    font-family: TNYAdobeCaslonPro;\\n    font-weight: 700;\\n    font-style: italic;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-BoldItalic.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-BoldItalic.woff\\\") format(\\\"woff\\\"); }\\n  \\n  @font-face {\\n    font-family: TNYAdobeCaslonPro;\\n    font-weight: 400;\\n    font-style: italic;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Italic.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Italic.woff\\\") format(\\\"woff\\\"); }\\n  \\n  @font-face {\\n    font-family: TNYAdobeCaslonPro;\\n    font-weight: 400;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Regular.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-Regular.woff\\\") format(\\\"woff\\\"); }\\n  \\n  @font-face {\\n    font-family: TNYAdobeCaslonPro;\\n    font-weight: 600;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-SemiBold.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-SemiBold.woff\\\") format(\\\"woff\\\"); }\\n  \\n  @font-face {\\n    font-family: TNYAdobeCaslonPro;\\n    font-weight: 600;\\n    font-style: italic;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-SemiBoldItalic.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FTNYAdobeCaslonPro-SemiBoldItalic.woff\\\") format(\\\"woff\\\"); }\\n  \\n      \\n@font-face {\\n    font-family: Lora;\\n    font-weight: 700;\\n    font-style: italic;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FLora-BoldItalic.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FLora-BoldItalic.woff\\\") format(\\\"woff\\\"); }\\n  \\n  @font-face {\\n    font-family: Lora;\\n    font-weight: 400;\\n    font-style: italic;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FLora-RegularItalic.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FLora-RegularItalic.woff\\\") format(\\\"woff\\\"); }\\n  @font-face {\\n    font-family: Lora;\\n    font-weight: 700;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002Flora-bold.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002Flora-bold.woff\\\") format(\\\"woff\\\"); }\\n  \\n  @font-face {\\n    font-family: Lora;\\n    font-weight: 400;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002Flora-regular.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002Flora-regular.woff\\\") format(\\\"woff\\\"); }\\n  \\n      \\n  @font-face {\\n    font-family: Lato;\\n    font-weight: 400;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FLato-Regular.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FLato-Regular.woff\\\") format(\\\"woff\\\"); }\\n\\n  @font-face {\\n    font-family: Lato;\\n    font-weight: 400;\\n    font-style: italic;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FLato-RegularItalic.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FLato-RegularItalic.woff\\\") format(\\\"woff\\\"); }\\n\\n  @font-face {\\n    font-family: Lato;\\n    font-weight: 600;\\n    font-style: normal;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FLato-SemiBold.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FLato-SemiBold.woff\\\") format(\\\"woff\\\"); }\\n\\n  @font-face {\\n    font-family: Lato;\\n    font-weight: 600;\\n    font-style: italic;\\n    font-display: swap;\\n    src: url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FLato-SemiBoldItalic.woff2\\\") format(\\\"woff2\\\"), url(\\\"\\u002Fverso\\u002Fstatic\\u002Fassets\\u002Ffonts\\u002FLato-SemiBoldItalic.woff\\\") format(\\\"woff\\\"); }\\n\\n    \"},\"meta\":{\"name\":\"the-new-yorker\",\"market\":\"us\"}},\"presenter\":\"presenter-articles\",\"comScoreCollectionName\":\"\",\"footerLogo\":{\"altText\":\"The New Yorker\",\"sources\":{\"sm\":{\"url\":\"\\u002Fverso\\u002Fstatic\\u002Fthe-new-yorker\\u002Fassets\\u002Flogo-reverse.svg\"}}},\"headerLogo\":{\"altText\":\"The New Yorker\",\"sources\":{\"sm\":{\"url\":\"\\u002Fverso\\u002Fstatic\\u002Fthe-new-yorker\\u002Fassets\\u002Flogo-header.svg\"}}},\"headerInvertedLogo\":{\"altText\":\"The New Yorker\",\"sources\":{\"sm\":{\"url\":\"\\u002Fverso\\u002Fstatic\\u002Fthe-new-yorker\\u002Fassets\\u002Flogo-header-reverse.svg\"}}},\"logo\":{\"altText\":\"The New Yorker\",\"sources\":{\"sm\":{\"url\":\"\\u002Fverso\\u002Fstatic\\u002Fthe-new-yorker\\u002Fassets\\u002Flogo.svg\"}}},\"invertedLogo\":{\"altText\":\"The New Yorker\",\"sources\":{\"sm\":{\"url\":\"\\u002Fverso\\u002Fstatic\\u002Fthe-new-yorker\\u002Fassets\\u002Flogo-inverted.svg\"}}},\"logoBaseUrl\":\"\\u002F\",\"head.og.image\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002F16:9\\u002Fw_1280,c_limit\\u002F151123_r27342.jpg\",\"rootBrandName\":\"The New Yorker\",\"head.twitterImageSrc\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002F16:9\\u002Fw_1280,c_limit\\u002F151123_r27342.jpg\",\"seriesLogos\":{},\"snowplow\":{\"collectorURL\":\"c.newyorker.com\",\"enableSnowplow\":true,\"slug\":\"the-new-yorker\",\"appInfoObj\":{\"appName\":\"Verso\",\"appEnv\":\"production\",\"appVersion\":\"7946\"}},\"fourd\":{\"enableFourdUser\":true},\"legalese\":{},\"landingPageLink\":{},\"navigationSearch\":true,\"translations\":{\"AccountInformationCard.Discard\":[{\"type\":0,\"value\":\"Discard\"}],\"AccountInformationCard.ErrorMessage\":[{\"type\":0,\"value\":\"Unable to save username. Please try again.\"}],\"AccountInformationCard.SubmitButtonLabel\":[{\"type\":0,\"value\":\"Save Username\"}],\"AccountInformationCard.SuccessMessage\":[{\"type\":0,\"value\":\"Your username is saved.\"}],\"AccountInformationCard.UserNameLabel\":[{\"type\":0,\"value\":\"Username\"}],\"AccountInformationCard.UserNamePlaceholer\":[{\"type\":0,\"value\":\"YOUR_USERNAME\"}],\"AccountInformationCard.alreadyTakenError\":[{\"type\":0,\"value\":\"This Username is already taken.\"}],\"AccountInformationCard.lengthError\":[{\"type\":0,\"value\":\"Usernames must be between 2 and 23 characters.\"}],\"AccountInformationCard.specialCharError\":[{\"type\":0,\"value\":\"Usernames can only include letters, numbers and underscores (_).\"}],\"AccountLinks.NavigationAriaLabel\":[{\"type\":0,\"value\":\"Account\"}],\"Ad.adLabel\":[{\"type\":0,\"value\":\"Advertisement\"}],\"AgeGate.AcceptLabel\":[{\"type\":0,\"value\":\"I am 18+\"}],\"AgeGate.DeclineLabel\":[{\"type\":0,\"value\":\"I'm under 18\"}],\"AgeGate.DekText\":[{\"type\":0,\"value\":\"This material is intended for people over the age of 18\"}],\"AgeGate.HedText\":[{\"type\":0,\"value\":\"Are you 18 or over?\"}],\"ArticlePage.Back to article\":[{\"type\":0,\"value\":\"Back to article\"}],\"ArticlePage.DefaultDisclaimer\":[{\"type\":0,\"value\":\"All products are independently selected by our editors. If you buy something, we may earn an affiliate commission.\"}],\"ArticlePage.From the issue of\":[{\"type\":0,\"value\":\"From the issue of\"}],\"ArticlePage.MessageBannerContent\":[{\"type\":0,\"value\":\"Your fellow home cooks thank you.\"}],\"ArticlePage.MessageBannerTitle\":[{\"type\":0,\"value\":\"Note added\"}],\"ArticlePage.TruncatedButtonLabel\":[{\"type\":0,\"value\":\"Read Full Story\"}],\"AudioPrimaryLabel.Listen\":[{\"type\":0,\"value\":\"Listen\"}],\"AudioSecondaryLabel.NowPlaying\":[{\"type\":0,\"value\":\"Now playing\"}],\"BizzaboEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Bizzabo content\"}],\"BlueskyEmbed.AriaLabelText\":[{\"type\":0,\"value\":\"social media post\"}],\"BlueskyEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Bluesky content\"}],\"Bookmark.SignInMessage\":[{\"type\":0,\"value\":\"After signing in, you can save stories and easily revisit them on any device—even off-line.\"}],\"BookmarkButton.Label\":[{\"type\":0,\"value\":\"save recipe\"}],\"BookmarkButton.LabelSaved\":[{\"type\":0,\"value\":\"recipe saved\"}],\"BookmarkIcon.Alert\":[{\"type\":0,\"value\":\"Save this story for later.\"}],\"BookmarkIcon.CompletionLabel\":[{\"type\":0,\"value\":\"Story saved. To revisit this article, select My Account, then View Saved Stories. Press Escape to dismiss tooltip.\"}],\"BookmarkIcon.Label\":[{\"type\":0,\"value\":\"Story saved\"}],\"BookmarkIcon.OnboardingAriaLabel\":[{\"type\":0,\"value\":\"Save story. Press Enter to save this story for later. Press Escape to dismiss tooltip.\"}],\"BookmarkIcon.SignInMessage\":[{\"type\":0,\"value\":\"After signing in, you can save stories and easily revisit them on any device—even off-line.\"}],\"BookmarkPrimaryLabel.RecipeSaved\":[{\"type\":0,\"value\":\"Recipe Saved\"}],\"BookmarkPrimaryLabel.Save\":[{\"type\":0,\"value\":\"Save\"}],\"BookmarkPrimaryLabel.SaveRecipe\":[{\"type\":0,\"value\":\"Save Recipe\"}],\"BookmarkPrimaryLabel.SaveThisStory\":[{\"type\":0,\"value\":\"Save this story\"}],\"BookmarkPrimaryLabel.Saved\":[{\"type\":0,\"value\":\"Saved\"}],\"BookmarkPrimaryLabel.SavedToLibrary\":[{\"type\":0,\"value\":\"Saved to library\"}],\"BusinessApplication.AccountProfileIsResubmittedText\":[{\"type\":0,\"value\":\"Updated\"}],\"BusinessApplication.AccountProfileLink\":[{\"type\":1,\"value\":\"url\"}],\"BusinessApplication.ApplicationAPIErrorMessage\":[{\"type\":0,\"value\":\"We couldn't complete this. Please try again.\"}],\"BusinessApplication.ApplicationBannerDeletePhotoLabel\":[{\"type\":0,\"value\":\"Delete Banner Photo\"}],\"BusinessApplication.ApplicationBannerImageDesc\":[{\"type\":0,\"value\":\"This will appear very large at the top of your profile page and must be landscape in ratio. It should show a completed project that's representative of your design style.\"}],\"BusinessApplication.ApplicationBannerImageTitle\":[{\"type\":0,\"value\":\"Banner Image\"}],\"BusinessApplication.ApplicationBannerUploadImageLabel\":[{\"type\":0,\"value\":\"Upload Banner Photo\"}],\"BusinessApplication.ApplicationFormBusinessHeader\":[{\"type\":0,\"value\":\"Describe Your Business\"}],\"BusinessApplication.ApplicationFormErrorHeader\":[{\"type\":0,\"value\":\"The following errors must be corrected:\"}],\"BusinessApplication.ApplicationFormSaveAndExitLink\":[{\"type\":0,\"value\":\"Save & Exit Application\"}],\"BusinessApplication.ApplicationFormSaveAndExitText\":[{\"type\":0,\"value\":\"If you want to continue later,\"}],\"BusinessApplication.ApplicationFormSectionHeader\":[{\"type\":0,\"value\":\"About Your Business\"}],\"BusinessApplication.ApplicationFormSectionSubHeader\":[{\"type\":0,\"value\":\"You can save and return by selecting the ‘Save and Exit Application’ link at the bottom of the page.\"}],\"BusinessApplication.ApplicationImagesInlineValidationError\":[{\"type\":0,\"value\":\"Images must be at least 1200 x 1200 pixels and no larger than 30MB. Please try again or select a different file.\"}],\"BusinessApplication.ApplicationModalExitButton\":[{\"type\":0,\"value\":\"Exit Without Saving\"}],\"BusinessApplication.ApplicationModalSaveExitButton\":[{\"type\":0,\"value\":\"Save and exit\"}],\"BusinessApplication.ApplicationPageSectionHeader\":[{\"type\":0,\"value\":\"Create Your Profile\"}],\"BusinessApplication.ApplicationPageSectionHeader2\":[{\"type\":0,\"value\":\"Your Profile Application\"}],\"BusinessApplication.ApplicationPageSectionSubHeader\":[{\"type\":0,\"value\":\"Start your application by completing a business profile.\"}],\"BusinessApplication.ApplicationPhotoLabel\":[{\"type\":0,\"value\":\"Photos\"}],\"BusinessApplication.ApplicationPhotosDescription\":[{\"type\":0,\"value\":\"Images must be JPEG or PNG format. Minimum size: 1200 x 1200 pixels. Maximum file size: 30MB.\"}],\"BusinessApplication.ApplicationPortfolioDeletePhotoLabel \":[{\"type\":0,\"value\":\"Delete Portfolio Photo\"}],\"BusinessApplication.ApplicationProfileDeletePhotoLabel\":[{\"type\":0,\"value\":\"Delete Profile Photo\"}],\"BusinessApplication.ApplicationProfileImageDesc\":[{\"type\":0,\"value\":\"This will appear next to your company name and contact details. You must upload an image that is square in ratio but it will automatically crop as you see in the preview. The photo can be of you, your colleagues or company logo.\"}],\"BusinessApplication.ApplicationProfileImageTitle\":[{\"type\":0,\"value\":\"Profile Image\"}],\"BusinessApplication.ApplicationProfileUploadImageLabel\":[{\"type\":0,\"value\":\"Upload Profile Photo\"}],\"BusinessApplication.ApplicationUploadImageLabel \":[{\"type\":0,\"value\":\"Upload Portfolio Photo &nbsp\"}],\"BusinessApplication.ApplicationWorkImageDesc\":[{\"type\":0,\"value\":\"The first will appear on the search results page. These should be of recent projects or work you have done. Include at least \"},{\"type\":1,\"value\":\"minWorkImages\"},{\"type\":0,\"value\":\".\"}],\"BusinessApplication.ApplicationWorkImageTitle\":[{\"type\":0,\"value\":\"Portfolio\"}],\"BusinessApplication.ApprovedAccountProfileBtnLabel\":[{\"type\":0,\"value\":\"Check My Profile\"}],\"BusinessApplication.ApprovedAccountProfileDek\":[{\"type\":0,\"value\":\"You're almost there! The AD PRO editors may have made minor changes to your page for consistency and style. To proceed, use the secure payment link in the email we sent you. If you have any questions, you can reach us at \"},{\"type\":1,\"value\":\"email\"},{\"type\":0,\"value\":\".\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"Once payment is processed, you'll have access to all the AD PRO features, then receive a link to your profile once it's published ahead of launch.\"}],\"BusinessApplication.ApprovedAccountProfileHed\":[{\"type\":0,\"value\":\"Your AD PRO Directory Application Has Been Approved\"}],\"BusinessApplication.ApprovedBannerDek\":[{\"type\":0,\"value\":\"Your profile is ready to be published. You cannot make any changes to it at this time.\"}],\"BusinessApplication.ApprovedBannerTitle\":[{\"type\":0,\"value\":\"Application Approved\"}],\"BusinessApplication.BannerIsResubmittedText\":[{\"type\":0,\"value\":\"updated\"}],\"BusinessApplication.BannerIsSubmittedText\":[{\"type\":0,\"value\":\"submitted\"}],\"BusinessApplication.BannerOnSubmitValidationError\":[{\"type\":0,\"value\":\"Upload a banner photo with the correct file type and size\"}],\"BusinessApplication.CompanyBusinessTypeLabel\":[{\"type\":0,\"value\":\"Choose a Category\"}],\"BusinessApplication.CompanyBusinessTypeOnSubmitValidationError\":[{\"type\":0,\"value\":\"Choose a Business Category\"}],\"BusinessApplication.CompanyCityInlineValidationError\":[{\"type\":0,\"value\":\"Don't use special characters.\"}],\"BusinessApplication.CompanyCityLabel\":[{\"type\":0,\"value\":\"CITY\"}],\"BusinessApplication.CompanyCityOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter a City\"}],\"BusinessApplication.CompanyCountryAssistiveLabel\":[{\"type\":0,\"value\":\"At the moment, we are only open to US-based designers. If you’re overseas, you can \"},{\"type\":1,\"value\":\"registerLink\"},{\"type\":0,\"value\":\" and be the first to know about the Directory’s global expansion.\"}],\"BusinessApplication.CompanyCountryLabel\":[{\"type\":0,\"value\":\"COUNTRY\"}],\"BusinessApplication.CompanyCountryRegisterHereLabel\":[{\"type\":0,\"value\":\"register here\"}],\"BusinessApplication.CompanyDescriptionLabel\":[{\"type\":0,\"value\":\"YOUR BUSINESS IN DETAIL\"}],\"BusinessApplication.CompanyDescriptionOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter the Business in Detail information. It must be between 1000 and 2000 characters.\"}],\"BusinessApplication.CompanyDescriptionPlaceholderText\":[{\"type\":0,\"value\":\"Tell us about you and your business in more detail. What sets you apart from the competition? Do you specialize in a particular style? How do you prefer to work? What are some examples of recent projects? Please also note in this section if you are willing to travel for a job. This information will appear on your business profile page.\"}],\"BusinessApplication.CompanyEmailInlineValidationError\":[{\"type\":0,\"value\":\"Check the email address is valid\"}],\"BusinessApplication.CompanyEmailLabel\":[{\"type\":0,\"value\":\"EMAIL ADDRESS\"}],\"BusinessApplication.CompanyEmailOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter a valid Email Address\"}],\"BusinessApplication.CompanyNameInlineValidationError\":[{\"type\":0,\"value\":\"Use a maximum of 128 characters with no special characters\"}],\"BusinessApplication.CompanyNameLabel\":[{\"type\":0,\"value\":\"COMPANY NAME\"}],\"BusinessApplication.CompanyNameOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter a Company Name with a maximum of 128 characters\"}],\"BusinessApplication.CompanyPhoneInlineValidationError\":[{\"type\":0,\"value\":\"Check the phone number is valid\"}],\"BusinessApplication.CompanyPhoneLabel\":[{\"type\":0,\"value\":\"PHONE NUMBER\"}],\"BusinessApplication.CompanyPhoneOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter a valid Phone Number using only 10 digits\"}],\"BusinessApplication.CompanyPhonePlaceholderText\":[{\"type\":0,\"value\":\"Enter 10 digits only\"}],\"BusinessApplication.CompanyProfessionLabel\":[{\"type\":0,\"value\":\"Choose a Profession\"}],\"BusinessApplication.CompanyProfessionOnSubmitValidationError\":[{\"type\":0,\"value\":\"Choose a Profession\"}],\"BusinessApplication.CompanySocialUrlInlineValidationError\":[{\"type\":0,\"value\":\"Don't use spaces\"}],\"BusinessApplication.CompanySocialUrlLabel\":[{\"type\":0,\"value\":\"Instagram username\"}],\"BusinessApplication.CompanyStateLabel\":[{\"type\":0,\"value\":\"STATE\"}],\"BusinessApplication.CompanyStateOnSubmitValidationError\":[{\"type\":0,\"value\":\"Select a State\"}],\"BusinessApplication.CompanyStatePlaceholderText\":[{\"type\":0,\"value\":\"Select an option\"}],\"BusinessApplication.CompanyStreetAddressLabel\":[{\"type\":0,\"value\":\"STREET ADDRESS\"}],\"BusinessApplication.CompanyStreetAddressOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter a Street Address\"}],\"BusinessApplication.CompanyTagLineOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter the Business in Brief information. It must be between 80 and 120 characters.\"}],\"BusinessApplication.CompanyTaglineLabel\":[{\"type\":0,\"value\":\"YOUR BUSINESS IN BRIEF\"}],\"BusinessApplication.CompanyTaglinePlaceholderText\":[{\"type\":0,\"value\":\"Tell us about you and your business and what makes you stand out from the competition. This will appear on the search results page when our readers look for someone in your profession so this should make them want to find out more.\"}],\"BusinessApplication.CompanyWebsiteInlineValidationError\":[{\"type\":0,\"value\":\"Invalid URL format. Please include the https:\\u002F\\u002F prefix, e.g., https:\\u002F\\u002Fwww.example.com.\"}],\"BusinessApplication.CompanyWebsiteLabel\":[{\"type\":0,\"value\":\"WEBSITE\"}],\"BusinessApplication.CompanyWebsitePlaceholderText\":[{\"type\":0,\"value\":\"Optional\"}],\"BusinessApplication.CompanyZipCodeInlineValidationError\":[{\"type\":0,\"value\":\"Check the Zip Code is valid\"}],\"BusinessApplication.CompanyZipCodeLabel\":[{\"type\":0,\"value\":\"ZIP CODE\"}],\"BusinessApplication.CompanyZipCodeOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter a valid ZIP Code\"}],\"BusinessApplication.DataConsentLabel\":[{\"type\":0,\"value\":\"I have read and accept the User Terms (\"},{\"type\":1,\"value\":\"link\"},{\"type\":0,\"value\":\") and confirm that the content and images submitted as part of my application belong to me and do not and will not infringe, misappropriate or otherwise violate any third party rights, including without limitation, music, talent, logos, trademarks, copyright, or any other third party intellectual property rights. I understand and agree that Conde Nast will process my personal data in accordance with its Privacy Policy (\"},{\"type\":1,\"value\":\"linkPrivacy\"},{\"type\":0,\"value\":\")\"}],\"BusinessApplication.DefaultAccountProfileBtnLabel\":[{\"type\":0,\"value\":\"Find Out More\"}],\"BusinessApplication.DefaultAccountProfileDek\":[{\"type\":0,\"value\":\"Create a personalized designer profile and list your business on our go-to guide for homeowners and renovators.\"}],\"BusinessApplication.DefaultAccountProfileHed\":[{\"type\":0,\"value\":\"Introducing the AD PRO Directory\"}],\"BusinessApplication.DraftAccountProfileBtnLabel\":[{\"type\":0,\"value\":\"Go to My Application\"}],\"BusinessApplication.DraftAccountProfileDek\":[{\"type\":0,\"value\":\"You started a business profile on \"},{\"type\":1,\"value\":\"submissionDate\"},{\"type\":0,\"value\":\".\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"Complete now and apply to join the industry's most respected roster of design talent.\"}],\"BusinessApplication.DraftAccountProfileHed\":[{\"type\":0,\"value\":\"Your AD PRO Directory Application\"}],\"BusinessApplication.DraftBannerDek\":[{\"type\":0,\"value\":\"If you have any questions about your application or the AD PRO Directory, visit our \"},{\"type\":1,\"value\":\"faq\"},{\"type\":0,\"value\":\" or contact us at \"},{\"type\":1,\"value\":\"email\"},{\"type\":0,\"value\":\".\"}],\"BusinessApplication.DraftBannerTitle\":[{\"type\":0,\"value\":\"Your profile is incomplete\"}],\"BusinessApplication.EditConsentLabel\":[{\"type\":0,\"value\":\"You understand and agree that if your application is successful the \"},{\"type\":1,\"value\":\"italicAD\"},{\"type\":0,\"value\":\" editorial team will have editorial discretion in writing your AD PRO Directory profile using the information provided in your application. The \"},{\"type\":1,\"value\":\"italicAD\"},{\"type\":0,\"value\":\" editorial team cannot accommodate suggestions or changes to profiles once published other than in the case of correcting a factual inaccuracy.\"}],\"BusinessApplication.ExpiredAccountProfileBtnLabel\":[{\"type\":0,\"value\":\"Restart\"}],\"BusinessApplication.ExpiredAccountProfileDek\":[{\"type\":0,\"value\":\"Restart your membership, by paying now.\"}],\"BusinessApplication.ExpiredAccountProfileHed\":[{\"type\":0,\"value\":\"Your AD PRO Directory membership is no longer active.\"}],\"BusinessApplication.FormLabel\":[{\"type\":0,\"value\":\"About Your Business\"}],\"BusinessApplication.FormSubmitButtonLabel\":[{\"type\":0,\"value\":\"SUBMIT APPLICATION\"}],\"BusinessApplication.MessageBannerCtaButtonLabel\":[{\"type\":0,\"value\":\"GO TO MY LISTING\"}],\"BusinessApplication.NeedInputAccountProfileBtnLabel\":[{\"type\":0,\"value\":\"Update My Application\"}],\"BusinessApplication.NeedInputAccountProfileDek\":[{\"type\":0,\"value\":\"The business profile you submitted on \"},{\"type\":1,\"value\":\"submissionDate\"},{\"type\":0,\"value\":\" has been reviewed.\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"A few changes are needed before we approve your listing. Please review the application form for our editors' comments and recommended next steps.\"}],\"BusinessApplication.NeedInputsBannerDek\":[{\"type\":0,\"value\":\"After review, the AD PRO editors have the following comments:\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":1,\"value\":\"remark\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"You can then resubmit your application for further review. If you have any questions about your application or the AD PRO Directory, visit our \"},{\"type\":1,\"value\":\"faq\"},{\"type\":0,\"value\":\" or contact us at \"},{\"type\":1,\"value\":\"email\"},{\"type\":0,\"value\":\".\"}],\"BusinessApplication.NeedInputsBannerTitle\":[{\"type\":0,\"value\":\"Our Feedback to You\"}],\"BusinessApplication.PaymentDoneAccountProfileBtnLabel\":[{\"type\":0,\"value\":\"Go to My Business Profile\"}],\"BusinessApplication.PaymentDoneAccountProfileDek\":[{\"type\":0,\"value\":\"Congratulations! We are pleased to welcome you to the AD PRO Directory.\"}],\"BusinessApplication.PaymentDoneAccountProfileHed\":[{\"type\":0,\"value\":\"Your Profile is Live on the AD PRO Directory.\"}],\"BusinessApplication.PaymentDoneBannerDek\":[{\"type\":0,\"value\":\"If you need to make changes to your listing, please email \"},{\"type\":1,\"value\":\"email\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":1,\"value\":\"profileURL\"}],\"BusinessApplication.PaymentDoneBannerTitle\":[{\"type\":0,\"value\":\"Your profile is live on the AD PRO Directory\"}],\"BusinessApplication.ProfileOnSubmitValidationError\":[{\"type\":0,\"value\":\"Upload a profile photo with the correct file type and size\"}],\"BusinessApplication.RejectedAccountProfileDek\":[{\"type\":0,\"value\":\"Thank you for your interest in joining the AD PRO Directory.\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"Unfortunately, we will not be able to accept your business at this time. We recognize this may be disappointing news, but you are welcome to apply again in the future. Please email our Directory team at \"},{\"type\":1,\"value\":\"email\"},{\"type\":0,\"value\":\" if you would like to reopen this application\"}],\"BusinessApplication.RejectedBannerDek\":[{\"type\":0,\"value\":\"You submitted this application on \"},{\"type\":1,\"value\":\"submittedAt\"},{\"type\":0,\"value\":\". To reapply, please contact us at \"},{\"type\":1,\"value\":\"email\"},{\"type\":0,\"value\":\".\"}],\"BusinessApplication.SubmitAccountProfileDek\":[{\"type\":0,\"value\":\"The \"},{\"type\":1,\"value\":\"italicAD\"},{\"type\":0,\"value\":\" Editors are reviewing your business profile and will email you shortly.\"}],\"BusinessApplication.SubmitAccountProfileHed\":[{\"type\":0,\"value\":\"We’ve Got Your \"},{\"type\":1,\"value\":\"isResubmitted\"},{\"type\":0,\"value\":\" AD PRO Directory Application\"}],\"BusinessApplication.SubmitBannerDek\":[{\"type\":0,\"value\":\"Application \"},{\"type\":1,\"value\":\"isResubmitted\"},{\"type\":0,\"value\":\" on \"},{\"type\":1,\"value\":\"submittedAt\"},{\"type\":0,\"value\":\". \"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"Your business profile is being reviewed and you can't make any changes to it at this time. The Directory team will email you shortly.\"}],\"BusinessApplication.UnderReviewAccountProfileHed\":[{\"type\":0,\"value\":\"We’ve Got Your Updated AD PRO Directory Application\"}],\"BusinessApplication.UnderReviewBannerDek\":[{\"type\":0,\"value\":\"Application updated on \"},{\"type\":1,\"value\":\"submittedAt\"},{\"type\":0,\"value\":\". \"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"Your business profile is being reviewed and you can't make any changes to it at this time. The Directory team will email you shortly.\"}],\"BusinessApplication.WorkImageOnSubmitValidationError\":[{\"type\":0,\"value\":\"Upload at least 3 photos of your work with the correct file type and size\"}],\"BusinessApplication.applicationModalButtonText\":[{\"type\":0,\"value\":\"Continue with the Application\"}],\"BusinessApplication.applicationModalMessage1\":[{\"type\":0,\"value\":\"Ensure you have saved your application before leaving.\"}],\"BusinessApplication.applicationModalMessage2\":[{\"type\":0,\"value\":\"Go to your AD account to complete your AD PRO Directory application\"}],\"BusinessApplication.applicationPhotosAddInfoText\":[{\"type\":0,\"value\":\"Add Info\"}],\"BusinessApplication.applicationPhotosEditInfoText\":[{\"type\":0,\"value\":\"Edit Info\"}],\"BusinessApplication.applicationSessionErrorMessage\":[{\"type\":0,\"value\":\"Something went wrong. Please try again.\"}],\"BusinessApplication.applicationWorkImageAdditionalDesc\":[{\"type\":0,\"value\":\"Portfolio Photo 1 will appear on the search results page and must be landscape in ratio.\"}],\"BusinessApplication.captionPlaceholderText\":[{\"type\":0,\"value\":\"Type your caption here. Captions are optional but should provide context to the user (e.g. location and scope of project).\"}],\"BusinessApplication.creditPlaceholderText\":[{\"type\":0,\"value\":\"Type your credit here. Credits are optional but please include the photographer’s name or image source if known.\"}],\"BusinessApplication.dataConsentOnSubmitValidationError\":[{\"type\":0,\"value\":\"Accept the user terms and conditions\"}],\"BusinessApplication.editConsentOnSubmitValidationError\":[{\"type\":0,\"value\":\"Please read and accept the terms\"}],\"BusinessApplication.imageCaptionLabel\":[{\"type\":0,\"value\":\"CAPTION\"}],\"BusinessApplication.imageCreditCaptionUpdateLabel\":[{\"type\":0,\"value\":\"UPDATE\"}],\"BusinessApplication.imageCreditLabel\":[{\"type\":0,\"value\":\"CREDIT\"}],\"BusinessApplication.loginTimeoutModalSecondaryCTAButton\":[{\"type\":0,\"value\":\"Go To Home Page\"}],\"BusinessApplication.modalHeaderText\":[{\"type\":0,\"value\":\"Photo caption and credit (optional)\"}],\"BusinessApplication.timeoutModalHeaderText\":[{\"type\":0,\"value\":\"This session will expire in\"}],\"BusinessApplication.timeoutModalLoginSecondaryText\":[{\"type\":0,\"value\":\"Please log in to your Account to continue with your application.\"}],\"BusinessApplication.timeoutModalPrimaryCTALoginButton\":[{\"type\":0,\"value\":\"Login\"}],\"BusinessApplication.timeoutModalSecondaryCTAButton\":[{\"type\":0,\"value\":\"Save and Exit Application\"}],\"BusinessApplication.timeoutModalSecondaryText\":[{\"type\":0,\"value\":\"You will lose any unsaved changes.\"}],\"BusinessApplication.timeoutModalSessionExipreHeaderText\":[{\"type\":0,\"value\":\"The session has expired.\"}],\"BusinessProfile.EditorialTitleHed\":[{\"type\":0,\"value\":\"Editors' Take\"}],\"BusinessProfile.ExploreTitleHed\":[{\"type\":0,\"value\":\"More from AD\"}],\"BusinessProfile.SectionTitleHed\":[{\"type\":0,\"value\":\"About\"}],\"BusinessProfile.businessProfileImageCreditPrefix\":[{\"type\":0,\"value\":\"Photo by\"}],\"Byline.More\":[{\"type\":0,\"value\":\"more\"}],\"Byline.Preamble\":[{\"type\":0,\"value\":\"By\"}],\"Bylines.AdaptationEditorPreamble\":[{\"type\":0,\"value\":\"Translated and Adapted by\"}],\"Bylines.AnimatorPreamble\":[{\"type\":0,\"value\":\"Animation by\"}],\"Bylines.ArtistPreamble\":[{\"type\":0,\"value\":\"Art by\"}],\"Bylines.ArtworkPreamble\":[{\"type\":0,\"value\":\"Artwork by\"}],\"Bylines.AstoldtoPreamble\":[{\"type\":0,\"value\":\"As told to\"}],\"Bylines.AuthorPreamble\":[{\"type\":0,\"value\":\"By\"}],\"Bylines.CoverShootPreamble\":[{\"type\":0,\"value\":\"Cover Shoot by\"}],\"Bylines.DeveloperPreamble\":[{\"type\":0,\"value\":\"Development by\"}],\"Bylines.DirectorPreamble\":[{\"type\":0,\"value\":\"Directed by\"}],\"Bylines.EditorPreamble\":[{\"type\":0,\"value\":\"Edited by\"}],\"Bylines.FilmByPreamble\":[{\"type\":0,\"value\":\"Film by\"}],\"Bylines.HairPreamble\":[{\"type\":0,\"value\":\"Hair by\"}],\"Bylines.IllustratorPreamble\":[{\"type\":0,\"value\":\"Illustration by\"}],\"Bylines.IntroducerPreamble\":[{\"type\":0,\"value\":\"Introduced by\"}],\"Bylines.MakeupPreamble\":[{\"type\":0,\"value\":\"Makeup by\"}],\"Bylines.MedicalReviewerPreamble\":[{\"type\":0,\"value\":\"Medically reviewed by\"}],\"Bylines.NailsPreamble\":[{\"type\":0,\"value\":\"Nails by\"}],\"Bylines.PhotographerPreamble\":[{\"type\":0,\"value\":\"Photography by\"}],\"Bylines.PodcasthostPreamble\":[{\"type\":0,\"value\":\"With\"}],\"Bylines.ProducerPreamble\":[{\"type\":0,\"value\":\"Produced by\"}],\"Bylines.ReporterPreamble\":[{\"type\":0,\"value\":\"Reporting by\"}],\"Bylines.ReviewerPreamble\":[{\"type\":0,\"value\":\"Reviewed by\"}],\"Bylines.StylistPreamble\":[{\"type\":0,\"value\":\"Styled by\"}],\"Bylines.TextByPreamble\":[{\"type\":0,\"value\":\"Text by\"}],\"Bylines.ToldbyPreamble\":[{\"type\":0,\"value\":\"As told by\"}],\"Bylines.VideoByPreamble\":[{\"type\":0,\"value\":\"Video by\"}],\"Bylines.VideoDirectorPreamble\":[{\"type\":0,\"value\":\"Video Directed by\"}],\"Bylines.WithPreamble\":[{\"type\":0,\"value\":\"With\"}],\"Bylines.Writer\":[{\"type\":0,\"value\":\"Written by\"}],\"Bylines.additionalReportingPreamble\":[{\"type\":0,\"value\":\"Additional Reporting by\"}],\"Bylines.inconversationPreamble\":[{\"type\":0,\"value\":\"In Conversation with\"}],\"Bylines.introductionPreamble\":[{\"type\":0,\"value\":\"Introduction by\"}],\"CNEVideoWatchPage.AboutPremiereDate\":[{\"type\":0,\"value\":\"Released on \"},{\"type\":1,\"value\":\"premiereDate\"}],\"CNEVideoWatchPage.AboutPremiereDateFormat\":[{\"type\":0,\"value\":\"MM\\u002FDD\\u002FYYYY\"}],\"CNEVideoWatchPage.AboutTabLabel\":[{\"type\":0,\"value\":\"About\"}],\"CNEVideoWatchPage.CreditsTabLabel\":[{\"type\":0,\"value\":\"Credits\"}],\"CNEVideoWatchPage.PlaylistHeading\":[{\"type\":0,\"value\":\"Up Next\"}],\"CNEVideoWatchPage.PreviewHeading\":[{\"type\":0,\"value\":\"Preview\"}],\"CNEVideoWatchPage.PreviewLinkCopied\":[{\"type\":0,\"value\":\"copied to clipboard\"}],\"CNEVideoWatchPage.RubricEpisode\":[{\"type\":0,\"value\":\"Episode \"},{\"type\":1,\"value\":\"episode\"}],\"CNEVideoWatchPage.TheaterModeLabel\":[{\"type\":0,\"value\":\"Hide\"}],\"CNEVideoWatchPage.TranscriptHeading\":[{\"type\":0,\"value\":\"Transcript\"}],\"CNEVideoWatchPage.TrendingRecsHeading\":[{\"type\":0,\"value\":\"Trending video\"}],\"CaptionContest.CaptionButton.authenticateSubmitButtonLabel\":[{\"type\":0,\"value\":\"Sign in to submit\"}],\"CaptionContest.CaptionButton.authenticateVoteButtonLabel\":[{\"type\":0,\"value\":\"Sign in to vote\"}],\"CaptionContest.CaptionButton.voteButtonLabel\":[{\"type\":0,\"value\":\"Vote\"}],\"CaptionContest.contestTabLoggedInDisclaimer\":[{\"type\":0,\"value\":\"You must be thirteen or older to enter the contest. Your entry must be received by 11:59 P.M. E.T. on \"},{\"type\":1,\"value\":\"submissionDeadline\"},{\"type\":0,\"value\":\", and must follow the \"},{\"children\":[{\"type\":0,\"value\":\"official caption contest rules\"}],\"type\":8,\"value\":\"a\"},{\"type\":0,\"value\":\".\"},{\"children\":[],\"type\":8,\"value\":\"b\"},{\"type\":0,\"value\":\"You can \"},{\"children\":[{\"type\":0,\"value\":\"update your contact information\"}],\"type\":8,\"value\":\"c\"},{\"type\":0,\"value\":\" anytime.\"}],\"CaptionContest.contestTabLoggedOutDisclaimer\":[{\"type\":0,\"value\":\"You must be thirteen or older to enter the contest. Your entry must be received by 11:59 P.M. E.T. on \"},{\"type\":1,\"value\":\"submissionDeadline\"},{\"type\":0,\"value\":\", and must follow the \"},{\"children\":[{\"type\":0,\"value\":\"official caption contest rules\"}],\"type\":8,\"value\":\"a\"},{\"type\":0,\"value\":\".\"}],\"CaptionContest.fetchErrorMessage\":[{\"type\":0,\"value\":\"There was a problem submitting your caption. Please try again later.\"}],\"CaptionContest.rateApiErrorMessage\":[{\"type\":0,\"value\":\"Captions aren’t available right now. Come back later to rate them.\"}],\"CaptionContest.rateAuthButtonLabel\":[{\"type\":0,\"value\":\"Sign in to rate captions\"}],\"CaptionContest.rateButtonFunnyLabel\":[{\"type\":0,\"value\":\"Funny\"}],\"CaptionContest.rateButtonSomeWhatFunnyLabel\":[{\"type\":0,\"value\":\"Somewhat funny\"}],\"CaptionContest.rateCaptionsCompletedMessage\":[{\"type\":0,\"value\":\"You've rated all of this week's captions!\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"Please go do something (anything) else till next week.\"}],\"CaptionContest.rateLegacyButtonText\":[{\"type\":0,\"value\":\"Start rating submissions\"}],\"CaptionContest.rateLoadingText\":[{\"type\":0,\"value\":\"Loading\"}],\"CaptionContest.rateRecirculationDek\":[{\"type\":0,\"value\":\"Rate submissions »\"}],\"CaptionContest.rateRecirculationHed\":[{\"type\":0,\"value\":\"Help us pick three finalists\"}],\"CaptionContest.rateTabDisclaimer\":[{\"type\":0,\"value\":\"All captions submitted by readers of \"},{\"children\":[{\"type\":0,\"value\":\"brand\"}],\"type\":8,\"value\":\"i\"},{\"type\":0,\"value\":\".\"}],\"CaptionContest.recirculationHed\":[{\"type\":0,\"value\":\"Next\"}],\"CaptionContest.secondPlaceTitle\":[{\"type\":0,\"value\":\"Second Place\"}],\"CaptionContest.submitButtonLabel\":[{\"type\":0,\"value\":\"Submit\"}],\"CaptionContest.submitTabCounterErrorMsg\":[{\"type\":0,\"value\":\"Use maximum of \"},{\"type\":1,\"value\":\"maxCount\"},{\"type\":0,\"value\":\" characters only.\"}],\"CaptionContest.submitTabUserInteractionLabel\":[{\"type\":0,\"value\":\"Your Caption\"}],\"CaptionContest.voteRecirculationDek\":[{\"type\":0,\"value\":\"Vote on Finalists »\"}],\"CaptionContest.voteRecirculationHed\":[{\"type\":0,\"value\":\"Help choose the winner\"}],\"CaptionContest.voteTabRecirculationHed\":[{\"type\":0,\"value\":\"View results for \"},{\"type\":1,\"value\":\"contestTitle\"}],\"CaptionContest.winnerRecirculationDek\":[{\"type\":0,\"value\":\"The Winner »\"}],\"CaptionModal.captionModalButtonLabel\":[{\"type\":0,\"value\":\"Save and submit caption\"}],\"CaptionModal.captionModalDek\":[{\"type\":0,\"value\":\"Add your name, location, and phone number, so we can contact and credit you if you're a finalist.\"}],\"CaptionModal.captionModalDisclaimerText\":[{\"children\":[{\"type\":0,\"value\":\"Need help? Call: 1-800-444-7570 | E-mail: help@newyorker.com\"}],\"type\":8,\"value\":\"p\"},{\"children\":[{\"type\":0,\"value\":\"We’re available Monday through Friday, 9 A.M. to 9 P.M. E.T., and Saturday through Sun, 9 A.M. to 5 P.M. E.T.\"}],\"type\":8,\"value\":\"p\"}],\"CaptionModal.captionModalHed\":[{\"type\":0,\"value\":\"How would you like to be acknowledged?\"}],\"CarouselControls.BackAriaLabel\":[{\"type\":0,\"value\":\"Carousel back\"}],\"CarouselControls.ForwardAriaLabel\":[{\"type\":0,\"value\":\"Carousel forward\"}],\"CartoonContest.contestCartoonCredit\":[{\"type\":0,\"value\":\"Cartoon by \"},{\"type\":1,\"value\":\"credit\"}],\"CartoonContest.fetchErrorMessage\":[{\"type\":0,\"value\":\"There was an issue submitting your vote. Please try again later.\"}],\"CartoonContest.instructionsButtonLabel\":[{\"type\":0,\"value\":\"How does it work?\"}],\"CartoonContest.rateButtonUnFunnyLabel\":[{\"type\":0,\"value\":\"Unfunny\"}],\"CartoonContest.rateTabDek\":[{\"type\":0,\"value\":\"Your responses will help us select three finalists for \"},{\"type\":1,\"value\":\"title\"},{\"type\":0,\"value\":\".\"}],\"CartoonContest.rateTabHed\":[{\"type\":0,\"value\":\"Rate Submissions\"}],\"CartoonContest.stageRate\":[{\"type\":0,\"value\":\"Rate\"}],\"CartoonContest.stageSubmit\":[{\"type\":0,\"value\":\"Submit\"}],\"CartoonContest.stageVote\":[{\"type\":0,\"value\":\"Vote\"}],\"CartoonContest.stageWinner\":[{\"type\":0,\"value\":\"Winner\"}],\"CartoonContest.submitSuccessMessage\":[{\"type\":0,\"value\":\"Thanks for your submission. Check back on \"},{\"type\":1,\"value\":\"finalistDate\"},{\"type\":0,\"value\":\" to see the finalists. If your caption is among them, we’ll be in touch.\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"While you wait, you can rate reader-submitted captions from last week’s contest.\"}],\"CartoonContest.submitTabDek\":[{\"type\":0,\"value\":\"Your caption for \"},{\"type\":1,\"value\":\"title\"},{\"type\":0,\"value\":\" will be rated by readers, like you, in the next round.\"}],\"CartoonContest.submitTabHed\":[{\"type\":0,\"value\":\"Submit Your Caption\"}],\"CartoonContest.thirdPlaceTitle\":[{\"type\":0,\"value\":\"Third Place\"}],\"CartoonContest.voteSuccessMessage\":[{\"type\":0,\"value\":\"Thanks for your vote.\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"The next set of finalists’ captions will appear on \"},{\"type\":1,\"value\":\"finalistDate\"},{\"type\":0,\"value\":\".\"}],\"CartoonContest.voteTabDek\":[{\"type\":0,\"value\":\"Your choice will help determine the winning caption for \"},{\"type\":1,\"value\":\"title\"},{\"type\":0,\"value\":\".\"}],\"CartoonContest.voteTabHed\":[{\"type\":0,\"value\":\"Vote on Finalists\"}],\"CartoonContest.winnerTabDek\":[{\"type\":1,\"value\":\"title\"}],\"CartoonContest.winnerTabHed\":[{\"type\":0,\"value\":\"The Winner\"}],\"Ceros.IframeTitle\":[{\"type\":0,\"value\":\"Ceros embed\"}],\"ChannelFilter.ClearAll\":[{\"type\":0,\"value\":\"Clear All\"}],\"ChannelFilter.ClearAllFiltersText\":[{\"type\":0,\"value\":\"Clear All Filters and Keywords\"}],\"ChannelFilter.FilterPreamble\":[{\"type\":0,\"value\":\"Filter by\"}],\"ChannelFilter.Save\":[{\"type\":0,\"value\":\"Save\"}],\"ChannelFilter.StoryCount\":[{\"type\":0,\"value\":\"Showing \"},{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Story\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Stories\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"storyCount\"}],\"ChannelNavigation.BookmarkAlertLabel\":[{\"type\":0,\"value\":\"Image saved. View saved images in\"}],\"ChannelNavigation.BookmarkAlertMyAccountLabel\":[{\"type\":0,\"value\":\"My Account.\"}],\"ChannelNavigation.ChannelDrawerContentLabel\":[{\"type\":0,\"value\":\"Runway filters navigation\"}],\"ChannelNavigation.GlobalDrawerContentLabel\":[{\"type\":0,\"value\":\"Navigation Menu\"}],\"ChannelNavigation.ToggleLabel\":[{\"type\":0,\"value\":\"Open Navigation Menu\"}],\"Clamp.ReadLess\":[{\"type\":0,\"value\":\"Read less\"}],\"Clamp.ReadMore\":[{\"type\":0,\"value\":\"Read more\"}],\"CneVideoEmbed.Live\":[{\"type\":0,\"value\":\"• Live\"}],\"CneVideoEmbed.PersistantCloseTitle\":[{\"type\":0,\"value\":\"Close Persisted Player\"}],\"CneVideoEmbed.WatchNow\":[{\"type\":0,\"value\":\"Streaming Live Now\"}],\"CollectionsDrawer.bookmarkCountType\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" image\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" images\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"CollectionsDrawer.bookmarkSavedText\":[{\"type\":0,\"value\":\"Image saved\"}],\"CollectionsDrawer.collectionsListHeading\":[{\"type\":0,\"value\":\"Choose A Board\"}],\"CollectionsDrawer.collectionsNewCollectionButtonLabel\":[{\"type\":0,\"value\":\"New Board\"}],\"CollectionsDrawer.createCollectionDuplicateNameError\":[{\"type\":0,\"value\":\"You have already used this name\"}],\"CollectionsDrawer.createCollectionNotMadeError\":[{\"type\":0,\"value\":\"Collection not made, please try again\"}],\"ComingSoon.SeriesNavigation\":[{\"type\":0,\"value\":\"COMING SOON\"}],\"ConnectedNewsletterSubscribeForm.BadResponse\":[{\"type\":0,\"value\":\"Bad response for signup newsletter\"}],\"ConnectedNewsletterSubscribeForm.ErrorMessage\":[{\"type\":0,\"value\":\"Subscription failed:\"}],\"ConnectedNewsletterSubscribeForm.SuccessDek\":[{\"type\":0,\"value\":\"You've successfully subscribed to our newsletter....\"}],\"ConnectedNewsletterSubscribeForm.SuccessHed\":[{\"type\":0,\"value\":\"You're all set...\"}],\"ConnectedNewsletterSubscribeForm.TypeMismatchMessage\":[{\"type\":0,\"value\":\"Invalid email. Double check and try again.\"}],\"ConsentBanner.consentText\":[{\"type\":0,\"value\":\"This content can also be viewed on the site it \"},{\"children\":[{\"type\":0,\"value\":\"originates\"}],\"type\":8,\"value\":\"a\"},{\"type\":0,\"value\":\" from.\"}],\"ConsentBanner.consentWarningText\":[{\"type\":0,\"value\":\"To honor your privacy preferences, this content can only be viewed on the site it \"},{\"children\":[{\"type\":0,\"value\":\"originates\"}],\"type\":8,\"value\":\"a\"},{\"type\":0,\"value\":\" from.\"}],\"ContentCardEmbed.articleButtonCta\":[{\"type\":0,\"value\":\"View Story\"}],\"ContentCardEmbed.recipeButtonCta\":[{\"type\":0,\"value\":\"View Recipe\"}],\"ContentHeader.ReadReviews\":[{\"type\":0,\"value\":\"Read Reviews\"}],\"ContentHeader.ShowAllPhotos\":[{\"type\":0,\"value\":\"Show all Photos\"}],\"ContentPageControlRow.NextPage\":[{\"type\":0,\"value\":\"Next\"}],\"ContentPageControlRow.PreviousPage\":[{\"type\":0,\"value\":\"Previous\"}],\"ContentPromoEmbed.DefaultButtonText\":[{\"type\":0,\"value\":\"Read More\"}],\"ContentPromoEmbed.GalleryButtonText\":[{\"type\":0,\"value\":\"View Slideshow\"}],\"ContentsList.contentsListTitle\":[{\"type\":0,\"value\":\"Table of Contents\"}],\"ContributorHeader.SeeMoreContributorLink\":[{\"type\":0,\"value\":\"See More By\"}],\"ContributorPage.LoadMoreLoadingText\":[{\"type\":0,\"value\":\"Loading ...\"}],\"ContributorPage.LoadMoreText\":[{\"type\":0,\"value\":\"More Stories\"}],\"ContributorPage.featuredStoriesHedText\":[{\"type\":0,\"value\":\"Featured Articles\"}],\"ContributorPage.sectionHedText\":[{\"type\":0,\"value\":\"Archive\"}],\"Contributors.AuthorPreamble\":[{\"type\":0,\"value\":\"Written by \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" Staff\"}],\"CreateCollectionDrawer.createCollectionHeading\":[{\"type\":0,\"value\":\"Create A Board\"}],\"CreateCollectionDrawer.createCollectionInputLabel\":[{\"type\":0,\"value\":\"Board Name\"}],\"CreateCollectionDrawer.createCollectionNoTextError\":[{\"type\":0,\"value\":\"Please enter some text\"}],\"CreateCollectionDrawer.createCollectionPlaceholderText\":[{\"type\":0,\"value\":\"Example: “monochrome” or “fall inspo”\"}],\"CreateCollectionDrawer.createCollectionResetButtonLabel\":[{\"type\":0,\"value\":\"Reset Board Name\"}],\"CreateCollectionDrawer.createCollectionSubmitButtonLabel\":[{\"type\":0,\"value\":\"Create\"}],\"CreateCollectionDrawer.createCollectionValueMissingError\":[{\"type\":0,\"value\":\"Please enter a collection name\"}],\"CrosswordEmbed.SignInMessage\":[{\"type\":0,\"value\":\"To save your progress, sign in to your \"},{\"children\":[{\"type\":1,\"value\":\"portal\"}],\"type\":8,\"value\":\"emTag\"},{\"type\":0,\"value\":\" account.\"}],\"CrosswordEmbed.Title\":[{\"type\":0,\"value\":\"Embedded Crossword\"}],\"CuratedShows.ButtonLabel\":[{\"type\":0,\"value\":\"View all shows\"}],\"CuratedShows.DrawerContentLabel\":[{\"type\":0,\"value\":\"Runway All Shows navigation\"}],\"CuratedShows.GroupedNavigationBrowserFilterLabel\":[{\"type\":0,\"value\":\"Search...\"}],\"CuratedShows.GroupedNavigationFilterLabel\":[{\"type\":0,\"value\":\"Search for a designer...\"}],\"CuratedShows.GroupedNavigationSummaryCarouselFilterLabel\":[{\"type\":0,\"value\":\"Search...\"}],\"CustomMenuButtons.Apply\":[{\"type\":0,\"value\":\"APPLY\"}],\"CustomMenuButtons.ClearAll\":[{\"type\":0,\"value\":\"Clear all\"}],\"Disclaimer.Text\":[{\"type\":0,\"value\":\"All products featured on \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" are independently selected by our editors. However, when you buy something through our retail links, we may earn an affiliate commission.\"}],\"Drawer.ButtonLabel\":[{\"type\":0,\"value\":\"Close drawer\"}],\"Drawer.ContentLabel\":[{\"type\":0,\"value\":\"Secondary menu navigation\"}],\"DynamicChannelNav.PrimaryLinks\":[{\"type\":0,\"value\":\"Primary\"}],\"ErrorBoundary.ErrorMessage\":[{\"type\":0,\"value\":\"An error occurred.\"}],\"ErrorContent.buttonLabel\":[{\"type\":0,\"value\":\"Go to Homepage\"}],\"ErrorContent.buttonLink\":[{\"type\":0,\"value\":\"\\u002F\"}],\"ErrorContent.dangerousDek\":[{\"type\":0,\"value\":\"There was an issue with this page\"}],\"ErrorContent.dangerousHed\":[{\"type\":0,\"value\":\"Oops\"}],\"ErrorPage.buttonLabel\":[{\"type\":0,\"value\":\"Go to Homepage\"}],\"ErrorPage.buttonLink\":[{\"type\":0,\"value\":\"\\u002F\"}],\"ErrorPage.dangerousDek\":[{\"type\":0,\"value\":\"There was an issue with this page\"}],\"ErrorPage.dangerousHed\":[{\"type\":0,\"value\":\"Oops\"}],\"ErrorPage.statusCodePreamble\":[{\"type\":0,\"value\":\"Status Code\"}],\"EventBanner.CloseBanner\":[{\"type\":0,\"value\":\"Close Banner\"}],\"EventBanner.LiveOn\":[{\"type\":0,\"value\":\"Live on\"}],\"EventBanner.SponsorPreamble\":[{\"type\":0,\"value\":\"Countdown Presented By\"}],\"EventBanner.WatchLiveOn\":[{\"type\":0,\"value\":\"Watch live on\"}],\"EventBanner.eventDays\":[{\"type\":0,\"value\":\"Days\"}],\"EventBanner.eventHours\":[{\"type\":0,\"value\":\"Hours\"}],\"EventBanner.eventMinutes\":[{\"type\":0,\"value\":\"Minutes\"}],\"EventBanner.eventSeconds\":[{\"type\":0,\"value\":\"Seconds\"}],\"EventsList.Title\":[{\"type\":0,\"value\":\"Featured Events\"}],\"ExternalLinkEmbed.Rubric\":[{\"type\":0,\"value\":\"Read More\"}],\"FacebookEmbed.AriaLabelText\":[{\"type\":0,\"value\":\"social media post\"}],\"FacebookEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Facebook content\"}],\"FeaturedContributor.ViewMore\":[{\"type\":0,\"value\":\"View more\"}],\"FeaturedContributorAllFiction.ViewMore\":[{\"type\":0,\"value\":\"View more\"}],\"FeaturedStories.HedText\":[{\"type\":0,\"value\":\"Featured Articles By \"},{\"type\":1,\"value\":\"contributorName\"}],\"Filmstrip.CollapsedMessage\":[{\"type\":0,\"value\":\"Explore\"}],\"Filmstrip.expandedMessage\":[{\"type\":0,\"value\":\"Hide\"}],\"Filter.ShowAllTags\":[{\"type\":0,\"value\":\"SHOW ALL\"}],\"Filter.ShowLess\":[{\"type\":0,\"value\":\"HIDE FILTERS\"}],\"Filter.ShowLessTags\":[{\"type\":0,\"value\":\"SHOW LESS\"}],\"Filter.ShowMore\":[{\"type\":0,\"value\":\"SHOW FILTERS\"}],\"FilterComponent.ContentLoadingLabel\":[{\"type\":0,\"value\":\"Updating\"}],\"FilterComponent.FilterApplyActionButton\":[{\"type\":0,\"value\":\"Apply\"}],\"FilterComponent.FilterBy\":[{\"type\":0,\"value\":\"Filter by\"}],\"FilterComponent.FilterCancelActionButton\":[{\"type\":0,\"value\":\"Cancel\"}],\"FilterComponent.FilterDeselectActionButton\":[{\"type\":0,\"value\":\"Unselect all\"}],\"FilterComponent.FilterMenuCloseButton\":[{\"type\":0,\"value\":\"Close Filter Menu\"}],\"FilterComponent.Items\":[{\"type\":0,\"value\":\"Items\"}],\"FilterComponent.NoResultDek\":[{\"type\":0,\"value\":\"Sorry, we can't display any results for those filtering options, please try again.\"}],\"FilterComponent.NoResultHed\":[{\"type\":0,\"value\":\"No Result\"}],\"FilterComponent.ShowItems\":[{\"type\":0,\"value\":\"Show \"},{\"type\":1,\"value\":\"totalItems\"},{\"type\":0,\"value\":\" Results\"}],\"FilterComponent.SortBy\":[{\"type\":0,\"value\":\"Sort by\"}],\"FilterComponent.TotalCount\":[{\"type\":0,\"value\":\"Results\"}],\"FilterComponent.reviewTags\":[{\"type\":1,\"value\":\"reviewTag\"}],\"FilterableSummaryList.AtArticleGalleryCarouselBtnText\":[{\"type\":0,\"value\":\"VIEW ALL \"},{\"type\":1,\"value\":\"categoryName\"}],\"FilterableSummaryList.AtArticleGalleryCarouselBtnTextWithCtaLink\":[{\"type\":1,\"value\":\"categoryName\"}],\"FireworkEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Firework content\"}],\"FormWithValidation.BadInput\":[{\"type\":0,\"value\":\"Bad input\"}],\"FormWithValidation.CustomError\":[{\"type\":0,\"value\":\"Custom error\"}],\"FormWithValidation.InvalidValueMessage\":[{\"type\":1,\"value\":\"field\"},{\"type\":0,\"value\":\" is invalid.\"}],\"FormWithValidation.PatternMismatch\":[{\"type\":0,\"value\":\"Pattern mismatch\"}],\"FormWithValidation.RangeOverflow\":[{\"type\":0,\"value\":\"Range overflow\"}],\"FormWithValidation.RangeUnderflow\":[{\"type\":0,\"value\":\"Range underflow\"}],\"FormWithValidation.StepMismatch\":[{\"type\":0,\"value\":\"Step mismatch\"}],\"FormWithValidation.TooLong\":[{\"type\":0,\"value\":\"Too long\"}],\"FormWithValidation.TooShort\":[{\"type\":0,\"value\":\"Too short\"}],\"FormWithValidation.TypeMismatch\":[{\"type\":0,\"value\":\"Type mismatch\"}],\"FormWithValidation.ValueMissing\":[{\"type\":0,\"value\":\"This field cannot be empty\"}],\"GalleryCarousel.Next\":[{\"type\":0,\"value\":\"Next\"}],\"GalleryCarousel.NextGallery\":[{\"type\":0,\"value\":\"Next gallery\"}],\"GalleryCarousel.Previous\":[{\"type\":0,\"value\":\"Previous\"}],\"GalleryEmbedControls.AdSlideText\":[{\"type\":0,\"value\":\"Advertisement\"}],\"GalleryEmbedControls.BackArrowButtonAriaLabel\":[{\"type\":0,\"value\":\"gallery-back\"}],\"GalleryEmbedControls.ForwardArrowButtonAriaLabel\":[{\"type\":0,\"value\":\"gallery-forward\"}],\"GalleryPage.SignInCalloutLinkText\":[{\"type\":0,\"value\":\"Sign in or create an account to vote\"}],\"GalleryRecircCards.ViewGalleryCTAText\":[{\"type\":0,\"value\":\"View gallery »\"}],\"GalleryRecircCards.keepOnLaughingText\":[{\"type\":0,\"value\":\"Keep on laughing\"}],\"GalleryRecircCards.midGalleryRecircHeading\":[{\"type\":0,\"value\":\"Want more laughs? Try another cartoon gallery.\"}],\"GalleryRecircCards.viewNextGalleryCTAText\":[{\"type\":0,\"value\":\"View next gallery »\"}],\"GallerySlide.ArticleCta\":[{\"type\":0,\"value\":\"View Story\"}],\"GallerySlide.DefaultCTAText\":[{\"type\":0,\"value\":\"Book Now\"}],\"GallerySlide.ProductCta\":[{\"type\":0,\"value\":\"Shop Now\"}],\"GallerySlide.RecipeCta\":[{\"type\":0,\"value\":\"View Recipe\"}],\"GallerySlide.ReviewCta\":[{\"type\":0,\"value\":\"Read More\"}],\"GallerySlide.VenueCta\":[{\"type\":0,\"value\":\"Book Now\"}],\"GallerySlide.VenueSellerPreviewText\":[{\"type\":0,\"value\":\"Powered By:\"}],\"GalleryVoting.galleryVotingDangerousDek\":[{\"type\":0,\"value\":\"You can also save stories and manage newsletter preferences\"}],\"GalleryVoting.galleryVotingDangerousHed\":[{\"type\":0,\"value\":\"To vote, sign in or\"}],\"GalleryVoting.galleryVotingDangerousHedSpanTag\":[{\"type\":0,\"value\":\"create an account\"}],\"GoogleSignInButton.Label\":[{\"type\":0,\"value\":\"Sign in with Google\"}],\"GroupedNavigation.FilterInputAriaLabel\":[{\"type\":0,\"value\":\"Filter links\"}],\"GroupedNavigationHasBrowser.FilterInputAriaLabel\":[{\"type\":0,\"value\":\"Filter links\"}],\"GroupedNavigationHasSummaryCarousel.FilterInputAriaLabel\":[{\"type\":0,\"value\":\"Filter links\"}],\"Icons.Account\":[{\"type\":0,\"value\":\"Account\"}],\"Icons.AgeGate\":[{\"type\":0,\"value\":\"Age Gate\"}],\"Icons.Arrow\":[{\"type\":0,\"value\":\"Arrow\"}],\"Icons.Article\":[{\"type\":0,\"value\":\"Article\"}],\"Icons.Bookmark\":[{\"type\":0,\"value\":\"Save Story\"}],\"Icons.BookmarkActivated\":[{\"type\":0,\"value\":\"Story Saved\"}],\"Icons.Cart\":[{\"type\":0,\"value\":\"Cart\"}],\"Icons.Check\":[{\"type\":0,\"value\":\"Check\"}],\"Icons.Chevron\":[{\"type\":0,\"value\":\"Chevron\"}],\"Icons.Close\":[{\"type\":0,\"value\":\"Close\"}],\"Icons.Collapse\":[{\"type\":0,\"value\":\"Collapse\"}],\"Icons.Comment\":[{\"type\":0,\"value\":\"Comment\"}],\"Icons.CopyLink\":[{\"type\":0,\"value\":\"CopyLink\"}],\"Icons.Dots\":[{\"type\":0,\"value\":\"Dots\"}],\"Icons.DownloadCloud\":[{\"type\":0,\"value\":\"DownloadCloud\"}],\"Icons.DownloadWeb\":[{\"type\":0,\"value\":\"DownloadWeb\"}],\"Icons.Email\":[{\"type\":0,\"value\":\"Email\"}],\"Icons.Expand\":[{\"type\":0,\"value\":\"Expand\"}],\"Icons.Facebook\":[{\"type\":0,\"value\":\"Facebook\"}],\"Icons.Filmstrip\":[{\"type\":0,\"value\":\"Filmstrip\"}],\"Icons.Filter\":[{\"type\":0,\"value\":\"Filter\"}],\"Icons.Flipboard\":[{\"type\":0,\"value\":\"Flipboard\"}],\"Icons.Gallery\":[{\"type\":0,\"value\":\"Gallery\"}],\"Icons.GoogleNews\":[{\"type\":0,\"value\":\"Google News\"}],\"Icons.Grid\":[{\"type\":0,\"value\":\"Grid\"}],\"Icons.Headphone\":[{\"type\":0,\"value\":\"Headphone\"}],\"Icons.Information\":[{\"type\":0,\"value\":\"Information\"}],\"Icons.Instagram\":[{\"type\":0,\"value\":\"Instagram\"}],\"Icons.LargeChevron\":[{\"type\":0,\"value\":\"LargeChevron\"}],\"Icons.Like\":[{\"type\":0,\"value\":\"Like\"}],\"Icons.LikeFilled\":[{\"type\":0,\"value\":\"LikeFilled\"}],\"Icons.Line\":[{\"type\":0,\"value\":\"Line\"}],\"Icons.LinkedIn\":[{\"type\":0,\"value\":\"LinkedIn\"}],\"Icons.List\":[{\"type\":0,\"value\":\"List\"}],\"Icons.Loader\":[{\"type\":0,\"value\":\"Loader\"}],\"Icons.Maximize\":[{\"type\":0,\"value\":\"Maximize\"}],\"Icons.Menu\":[{\"type\":0,\"value\":\"Menu\"}],\"Icons.NativeShare\":[{\"type\":0,\"value\":\"Native Share\"}],\"Icons.Newsletter\":[{\"type\":0,\"value\":\"Newsletter\"}],\"Icons.Ok\":[{\"type\":0,\"value\":\"Odnoklassniki\"}],\"Icons.Pause\":[{\"type\":0,\"value\":\"Pause\"}],\"Icons.PhotoStack\":[{\"type\":0,\"value\":\"PhotoStack\"}],\"Icons.Pinterest\":[{\"type\":0,\"value\":\"Pinterest\"}],\"Icons.Play\":[{\"type\":0,\"value\":\"Play\"}],\"Icons.PlayCNE\":[{\"type\":0,\"value\":\"PlayCNE\"}],\"Icons.PlayOutlined\":[{\"type\":0,\"value\":\"PlayOutlined\"}],\"Icons.Playlist\":[{\"type\":0,\"value\":\"Playlist\"}],\"Icons.Print\":[{\"type\":0,\"value\":\"Print\"}],\"Icons.RatingFilled\":[{\"type\":0,\"value\":\"RatingFilled\"}],\"Icons.RatingHalf\":[{\"type\":0,\"value\":\"RatingHalf\"}],\"Icons.RatingOutlined\":[{\"type\":0,\"value\":\"RatingOutlined\"}],\"Icons.Replay\":[{\"type\":0,\"value\":\"Replay\"}],\"Icons.Rss\":[{\"type\":0,\"value\":\"Rss\"}],\"Icons.Search\":[{\"type\":0,\"value\":\"Search\"}],\"Icons.Share\":[{\"type\":0,\"value\":\"Share\"}],\"Icons.Shopping\":[{\"type\":0,\"value\":\"Shopping\"}],\"Icons.Snapchat\":[{\"type\":0,\"value\":\"Snapchat\"}],\"Icons.SocialHandle\":[{\"type\":0,\"value\":\"SocialHandle\"}],\"Icons.Spotify\":[{\"type\":0,\"value\":\"Spotify\"}],\"Icons.Star\":[{\"type\":0,\"value\":\"Star\"}],\"Icons.Telegram\":[{\"type\":0,\"value\":\"Telegram\"}],\"Icons.Threads\":[{\"type\":0,\"value\":\"Threads\"}],\"Icons.Tiktok\":[{\"type\":0,\"value\":\"Tiktok\"}],\"Icons.Timestamp\":[{\"type\":0,\"value\":\"Timestamp\"}],\"Icons.Triangle\":[{\"type\":0,\"value\":\"Triangle\"}],\"Icons.TriangleDown\":[{\"type\":0,\"value\":\"TriangleDown\"}],\"Icons.TriangleUp\":[{\"type\":0,\"value\":\"TriangleUp\"}],\"Icons.Tumblr\":[{\"type\":0,\"value\":\"Tumblr\"}],\"Icons.Twitter\":[{\"type\":0,\"value\":\"X\"}],\"Icons.Vero\":[{\"type\":0,\"value\":\"VERO\"}],\"Icons.Viber\":[{\"type\":0,\"value\":\"Rakuten Viber\"}],\"Icons.Video\":[{\"type\":0,\"value\":\"Video\"}],\"Icons.Vk\":[{\"type\":0,\"value\":\"VKonkakte\"}],\"Icons.WeChat\":[{\"type\":0,\"value\":\"WeChat\"}],\"Icons.WebLink\":[{\"type\":0,\"value\":\"Website Link\"}],\"Icons.Weibo\":[{\"type\":0,\"value\":\"Sina Weibo\"}],\"Icons.Whatsapp\":[{\"type\":0,\"value\":\"Whatsapp\"}],\"Icons.Xing\":[{\"type\":0,\"value\":\"Xing\"}],\"Icons.YandexZen\":[{\"type\":0,\"value\":\"Yandex.Zen\"}],\"Icons.YouTube\":[{\"type\":0,\"value\":\"YouTube\"}],\"Icons.chevronFill\":[{\"type\":0,\"value\":\"chevronFill\"}],\"Icons.funny\":[{\"type\":0,\"value\":\"Funny\"}],\"Icons.someWhatFunny\":[{\"type\":0,\"value\":\"Somewhat funny\"}],\"Icons.unFunny\":[{\"type\":0,\"value\":\"Unfunny\"}],\"Icons.wavyarrow\":[{\"type\":0,\"value\":\"Wavy Arrow\"}],\"IframeEmbed.AriaLabel\":[{\"type\":0,\"value\":\"Click button to go to: \"},{\"type\":1,\"value\":\"name\"}],\"IframeEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Content\"}],\"IframeEmbed.DangerousDek\":[{\"type\":0,\"value\":\"Listen to this story\"}],\"IframeEmbed.Title\":[{\"type\":0,\"value\":\"Embedded Frame\"}],\"ImageSlideShow.galleryLink\":[{\"type\":0,\"value\":\"See the gallery\"}],\"ImageSlideShow.lastSlideCTA\":[{\"type\":0,\"value\":\"Explore the gallery\"}],\"IngredientList.hedText\":[{\"type\":0,\"value\":\"Ingredients\"}],\"IngredientList.nutritionHedText\":[{\"type\":0,\"value\":\"Nutrition Per Serving\"}],\"InstagramEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Instagram content\"}],\"InstructionList.StepText\":[{\"type\":0,\"value\":\"Step\"}],\"ItemCount.ItemTypeCharacter\":[{\"options\":{\"other\":{\"value\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" character\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" characters\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}]},\"withMinCountLimit\":{\"value\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" character\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" characters\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"},{\"type\":0,\"value\":\" (at least \"},{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" character\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" characters\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"minCount\"},{\"type\":0,\"value\":\" are required)\"}]}},\"type\":5,\"value\":\"messageType\"}],\"ItemCount.ItemTypeDefault\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Item\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Items\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"ItemCount.ItemTypeImage\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Image\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Images\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"ItemCount.ItemTypePhoto\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Photo\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Photos\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"ItemCount.ItemTypeSlide\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Slide\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Slides\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"ItemCount.ItemTypeVenue\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Venue\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Venues\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"JustWatchEmbed.PoweredBy\":[{\"type\":0,\"value\":\"Powered by\"}],\"Lightbox.CloseButtonIconLabel\":[{\"type\":0,\"value\":\"Close Lightbox\"}],\"Lightbox.ContentLabel\":[{\"type\":0,\"value\":\"Photo Gallery\"}],\"LiveStory.LoadMoreButtonLabel\":[{\"type\":0,\"value\":\"Load More\"}],\"LiveStory.LoadMoreLoadingLabel\":[{\"type\":0,\"value\":\"Loading ...\"}],\"LiveStory.feedADayAgoLabel\":[{\"type\":0,\"value\":\"a day ago\"}],\"LiveStory.feedAMinAgoLabel\":[{\"type\":0,\"value\":\"a minute ago\"}],\"LiveStory.feedAMonthAgoLabel\":[{\"type\":0,\"value\":\"a month ago\"}],\"LiveStory.feedAYearAgoLabel\":[{\"type\":0,\"value\":\"a year ago\"}],\"LiveStory.feedAnHourAgoLabel\":[{\"type\":0,\"value\":\"an hour ago\"}],\"LiveStory.feedFewSecondsAgoLabel\":[{\"type\":0,\"value\":\"a few seconds ago\"}],\"LiveStory.lastUpdatedTimePrefixLabel\":[{\"type\":0,\"value\":\"Updated\"}],\"MagazineBundlePage.magazineHeading\":[{\"type\":0,\"value\":\"The Magazine\"}],\"MagazineBundlePage.nextButton\":[{\"type\":0,\"value\":\"Next\"}],\"MagazineBundlePage.previousButton\":[{\"type\":0,\"value\":\"Previous\"}],\"MagazineDisclaimer.DisclaimerNoHed\":[{\"type\":0,\"value\":\"Published in the print edition of the \"},{\"type\":1,\"value\":\"issueDate\"},{\"type\":0,\"value\":\", issue.\"}],\"MagazineDisclaimer.DisclaimerWithHed\":[{\"type\":0,\"value\":\"Published in the print edition of the \"},{\"type\":1,\"value\":\"issueDate\"},{\"type\":0,\"value\":\", issue, with the headline “\"},{\"type\":1,\"value\":\"hed\"},{\"type\":0,\"value\":\".”\"}],\"MapPreview.MapAriaLabel\":[{\"type\":0,\"value\":\"Location map of address\"}],\"MapPreview.MapPreviewHeader\":[{\"type\":0,\"value\":\"Location Map\"}],\"MastodonEmbed.AriaLabelText\":[{\"type\":0,\"value\":\"social media post\"}],\"MastodonEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Mastodon content\"}],\"MegaMenu.All\":[{\"type\":0,\"value\":\"All\"}],\"MegaMenu.MegaMenuButton\":[{\"type\":0,\"value\":\"Close Mega Menu\"}],\"MegaMenu.NavigationPrimaryAriaLabel\":[{\"type\":0,\"value\":\"Primary\"}],\"MegaMenu.SignInLinkText\":[{\"type\":0,\"value\":\"Sign in\"}],\"MegaMenu.VerboseClickOut\":[{\"type\":0,\"value\":\"More\"}],\"MenuList.unavailableFilters\":[{\"type\":0,\"value\":\"Unavailable filters\"}],\"MessageBanner.saveStory\":[{\"type\":0,\"value\":\"Save story\"}],\"MobileProductCard.CardAvailability\":[{\"type\":0,\"value\":\"Multiple Buying Options Available\"}],\"MobileProductCard.CardRating\":[{\"type\":0,\"value\":\"Rating: \"},{\"type\":1,\"value\":\"rating\"},{\"type\":0,\"value\":\"\\u002F10\"}],\"MobileProductCard.CtaText\":[{\"type\":0,\"value\":\"Buy Now\"}],\"Multipackages.ExploreInstead\":[{\"type\":0,\"value\":\"Explore these instead\"}],\"Multipackages.NoStories\":[{\"type\":0,\"value\":\"No stories found for your search\"}],\"NativeDisclaimer.Text\":[{\"type\":0,\"value\":\"This article was published by The New Yorker Brand Studio for \"},{\"type\":1,\"value\":\"sponsorName\"},{\"type\":0,\"value\":\". The editorial staff of The New Yorker had no role in this post's preparation.\"}],\"NativeShareButton.ButtonTitle\":[{\"type\":0,\"value\":\"Share\"}],\"Navigation.OneTrustButtonLabel\":[{\"type\":0,\"value\":\"Do Not Sell My Personal Info\"}],\"NewsFeed.LoadMoreNewsPreamble\":[{\"type\":0,\"value\":\"Show More News\"}],\"NewsLetterManagePageNewsLetterSubscribeForm.TypeMismatchMessage\":[{\"type\":0,\"value\":\"Invalid email. Double check and try again.\"}],\"NewsletterManageContent.SecondaryOptinsDangerousLegend\":[{\"type\":0,\"value\":\"Primary and Third Party Optins\"}],\"NewsletterManageContent.SignUp\":[{\"type\":0,\"value\":\"Sign up\"}],\"NewsletterManageContent.SignUpMessage\":[{\"type\":0,\"value\":\"You’re signed up.\"}],\"NewsletterManagePage.BadResponseServerMessage\":[{\"type\":0,\"value\":\"Subscription failed: Bad response\"}],\"NewsletterManagePage.EmptyNewsletterErrorMessage\":[{\"type\":0,\"value\":\"Subscription failed: Please select a newsletter\"}],\"NewsletterManagePage.UnknownErrorMessage\":[{\"type\":0,\"value\":\"Subscription failed: Unknown error occurred\"}],\"NewsletterManagePage.closeSubmitModelTextField\":[{\"type\":0,\"value\":\"Go back\"}],\"NewsletterManagePage.completeSignUp\":[{\"type\":0,\"value\":\"Complete Sign Up\"}],\"NewsletterManagePage.goBackFromPreviewButton\":[{\"type\":0,\"value\":\"Go back\"}],\"NewsletterManagePage.previewText\":[{\"type\":0,\"value\":\"Preview\"}],\"NewsletterManagePage.readPreviewButton\":[{\"type\":0,\"value\":\"Read a preview\"}],\"NewsletterManagePage.signedUpPopUpMessage\":[{\"type\":0,\"value\":\"You’re signed up to this newsletter.\"}],\"NewsletterManagePage.ukPrimaryMarketingPermission\":[{\"type\":0,\"value\":\"Sign up to receive emails from \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" about \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\"’s products, promotions, and services.\"}],\"NewsletterManagePage.ukThirdPartyMarketingPermission\":[{\"type\":0,\"value\":\"Sign up to receive marketing emails from \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" about special offers and promotions for other Condé Nast brands and our marketing partners.\"}],\"NewsletterManagePage.viewMoreNewsletters\":[{\"type\":0,\"value\":\"View more newsletters\"}],\"NewsletterOneClickForm.BadResponse\":[{\"type\":0,\"value\":\"Bad response for signup newsletter\"}],\"NewsletterOneClickForm.ErrorMessage\":[{\"type\":0,\"value\":\"Subscription failed\"}],\"NewsletterOneClickForm.TypeMismatchMessage\":[{\"type\":0,\"value\":\"Invalid email. Double check and try again.\"}],\"NewsletterSlice.DismissButton\":[{\"type\":0,\"value\":\"Dismiss Newsletter Slice\"}],\"NewsletterSlice.NewsletterSecondaryOptinsLegend\":[{\"type\":0,\"value\":\"Consent checks\"}],\"NowReading.SeriesNavigation\":[{\"type\":0,\"value\":\"Now Reading\"}],\"OverlayNavigation.OverlayNavigationButton\":[{\"type\":0,\"value\":\"Close Navigation Menu\"}],\"OverlayNavigation.OverlayNavigationPrimaryLinks\":[{\"type\":0,\"value\":\"Primary\"}],\"OverlayNavigation.OverlayNavigationSearchLabel\":[{\"type\":0,\"value\":\"Search\"}],\"OverlayNavigation.OverlayNavigationSecondaryLinks\":[{\"type\":0,\"value\":\"Secondary\"}],\"OverlayNavigation.OverlayNavigationSignInLabel\":[{\"type\":0,\"value\":\"Sign In\"}],\"OverlayNavigation.OverlayNavigationUtilityLinks\":[{\"type\":0,\"value\":\"Utility\"}],\"OverlayNavigation.OverlayNavigationWrapper\":[{\"type\":0,\"value\":\"Overlay Navigation\"}],\"PLPProductCard.AtRetailerNameComponentText\":[{\"type\":0,\"value\":\"Shop at \"},{\"type\":1,\"value\":\"sellerNameText\"}],\"PLPProductCard.AtRetailerNameLabel\":[{\"type\":0,\"value\":\"$ \"},{\"type\":1,\"value\":\"finalPriceLabel\"},{\"type\":0,\"value\":\" At \"},{\"type\":1,\"value\":\"sellerNameText\"}],\"PLPProductCard.BuyAt\":[{\"type\":0,\"value\":\"Buy At \"},{\"type\":1,\"value\":\"sellerNameText\"}],\"PLPProductCard.ShopNowComponentText\":[{\"type\":0,\"value\":\"Shop Now\"}],\"PaginationModal.NextPage\":[{\"type\":0,\"value\":\"Next\"}],\"PaginationModal.PreviousPage\":[{\"type\":0,\"value\":\"Previous\"}],\"PaginationRow.NextPage\":[{\"type\":0,\"value\":\"Next Page\"}],\"PhotoBookmark.GalleryDesktopMessageBannerText\":[{\"type\":0,\"value\":\"Click the icon to save an image to your account.\"}],\"PhotoBookmark.GalleryMobileMessageBannerText\":[{\"type\":0,\"value\":\"Tap the icon to save an image to your account.\"}],\"PhotoBookmark.RunwayDesktopMessageBannerText\":[{\"type\":0,\"value\":\"Hover over an image and click the icon to save it to your account.\"}],\"PhotoBookmark.RunwayMobileMessageBannerText\":[{\"type\":0,\"value\":\"Tap the icon to save an image to your account.\"}],\"PhotoBookmark.SaveAlertSavedToBoardMessage\":[{\"type\":0,\"value\":\"Go To Boards\"}],\"PhotoBookmark.SaveAlertWithBoardName\":[{\"type\":0,\"value\":\"Saved to\"}],\"PhotoBookmark.SaveBookmarkAlertLink\":[{\"type\":0,\"value\":\"Create a Board\"}],\"PhotoBookmark.SaveBookmarkAlertText\":[{\"type\":0,\"value\":\"Image saved\"}],\"PhotoBookmark.SaveIconTitle\":[{\"type\":0,\"value\":\"Save Image\"}],\"PhotoBookmark.SavedIconTitle\":[{\"type\":0,\"value\":\"Image saved\"}],\"PhotoBookmark.SignInHed\":[{\"type\":0,\"value\":\"Save images\"}],\"PhotoBookmark.SignInHedSpanTag\":[{\"type\":0,\"value\":\"to your Vogue account\"}],\"PhotoBookmark.SignInMessage\":[{\"type\":0,\"value\":\"Sign in to save runway and street style images, then easily revisit them on any device.\"}],\"PhotoVogueHomePage.bestOfPvText\":[{\"type\":0,\"value\":\"Best of Photovogue\"}],\"PhotoVogueHomePage.featured\":[{\"type\":0,\"value\":\"featured\"}],\"PhotoVogueHomePage.introductionText\":[{\"type\":0,\"value\":\"Connecting artists, community and commerce through Condé Nast's global creative networks, we champion talent and influence visual literacy to help foster a more just, ethical and inclusive world.\"}],\"PhotoVogueHomePage.nationalitiesAuthorNameText\":[{\"type\":0,\"value\":\"Countries represented\"}],\"PhotoVogueHomePage.nationalitiesRubricNameText\":[{\"type\":0,\"value\":\"Nationalities\"}],\"PhotoVogueHomePage.news\":[{\"type\":0,\"value\":\"news\"}],\"PhotoVogueHomePage.photoCountText\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Photo\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Photos\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"photoCount\"}],\"PhotoVogueHomePage.photoVogue\":[{\"type\":0,\"value\":\"PhotoVogue\"}],\"PhotoVogueHomePage.photograpersAuthorNameText\":[{\"type\":0,\"value\":\"Participating photographers\"}],\"PhotoVogueHomePage.photographersRubricNameText\":[{\"type\":0,\"value\":\"Photographers\"}],\"PhotoVogueHomePage.photosAuthorNameText\":[{\"type\":0,\"value\":\"Photos selected by the editors\"}],\"PhotoVogueHomePage.photosRubricNameText\":[{\"type\":0,\"value\":\"Photos\"}],\"PhotoVogueHomePage.picOfTheDay\":[{\"type\":0,\"value\":\"PIC OF THE DAY\"}],\"PhotoVogueHomePage.seeAllBestOf\":[{\"type\":0,\"value\":\"See All Best Of\"}],\"PhotoVogueHomePage.seeAllFeatured\":[{\"type\":0,\"value\":\"SEE ALL featured\"}],\"PhotoVogueHomePage.seeAllNews\":[{\"type\":0,\"value\":\"See All News\"}],\"PhotoVogueHomePage.seeAllPhotographers\":[{\"type\":0,\"value\":\"See All Photographers\"}],\"PhotoVogueHomePage.seeAllPicOfTheDay\":[{\"type\":0,\"value\":\"SEE ALL PICS OF THE DAY\"}],\"PhotoVogueHomePage.spotlightPhotographers\":[{\"type\":0,\"value\":\"Spotlight Photographers\"}],\"PhotoVogueHomePage.spotlightRubric\":[{\"type\":0,\"value\":\"Spotlight\"}],\"PhotoVogueHomePage.tickerText\":[{\"type\":0,\"value\":\"We’re excited to open up contributions to video, illustration and digital art in the near future\"}],\"PhotoVoguePhotographersFilter.placeholderText\":[{\"type\":0,\"value\":\"Search by first or last name\"}],\"PhotoVoguePhotographersFilter.searchButtonText\":[{\"type\":0,\"value\":\"Search\"}],\"PhotoVoguePhotographersPage.PhotosCount\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" photo\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" photos\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"PhotoVoguePhotographersPage.clearAllText\":[{\"type\":0,\"value\":\"Clear All\"}],\"PhotoVoguePhotographersPage.country\":[{\"type\":0,\"value\":\"Country\"}],\"PhotoVoguePhotographersPage.errorMessage\":[{\"type\":0,\"value\":\"Sorry, something went wrong. Please refresh the page and try again.\"}],\"PhotoVoguePhotographersPage.genre\":[{\"type\":0,\"value\":\"Genre\"}],\"PhotoVoguePhotographersPage.isSpotlight\":[{\"type\":0,\"value\":\"Spotlight\"}],\"PhotoVoguePhotographersPage.loadMore\":[{\"type\":0,\"value\":\"See More\"}],\"PhotoVoguePhotographersPage.loading\":[{\"type\":0,\"value\":\"Loading…\"}],\"PhotoVoguePhotographersPage.orText\":[{\"type\":0,\"value\":\"or\"}],\"PhotoVoguePhotographersPage.photographerLabel\":[{\"type\":0,\"value\":\"photographer\"}],\"PhotoVoguePhotographersPage.photographersLabel\":[{\"type\":0,\"value\":\"photographers\"}],\"PhotoVoguePhotographersPage.spotlight\":[{\"type\":0,\"value\":\"Spotlight\"}],\"PhotoVoguePhotosPage.Filter\":[{\"type\":0,\"value\":\"Filter\"}],\"PhotoVoguePhotosPage.countPrefixText\":[{\"type\":0,\"value\":\"Showing\"}],\"PhotoVoguePhotosPage.errorMessage\":[{\"type\":0,\"value\":\"Sorry, something went wrong. Please refresh the page and try again.\"}],\"PhotoVoguePhotosPage.loadMore\":[{\"type\":0,\"value\":\"Load More\"}],\"PhotoVoguePhotosPage.loading\":[{\"type\":0,\"value\":\"Loading…\"}],\"PhotoVoguePhotosPage.photoLabel\":[{\"type\":0,\"value\":\"photo\"}],\"PhotoVoguePhotosPage.photosLabel\":[{\"type\":0,\"value\":\"photos\"}],\"PhotovogueArtistProfilePage.ShowLessText\":[{\"type\":0,\"value\":\"Show less\"}],\"PhotovogueArtistProfilePage.ShowMoreText\":[{\"type\":0,\"value\":\"Show more\"}],\"PhotovogueArtistProfilePage.allPhotos\":[{\"type\":0,\"value\":\"All photos:\"}],\"PhotovogueArtistProfilePage.bestOfPhotoVogue\":[{\"type\":0,\"value\":\"Best of PhotoVogue:\"}],\"PhotovogueArtistProfilePage.genre\":[{\"type\":0,\"value\":\"Genre:\"}],\"PhotovogueArtistProfilePage.picOfTheDayVogueText\":[{\"type\":0,\"value\":\"Pic of the Day:\"}],\"PhotovogueArtistProfilePage.portfolio\":[{\"type\":0,\"value\":\"Portfolio\"}],\"PhotovogueArtistProfilePage.showMoreText\":[{\"type\":0,\"value\":\"Show more\"}],\"PhotovogueArtistProfilePage.spotlightTitle\":[{\"type\":0,\"value\":\"Spotlight\"}],\"PinterestEmbed.AriaLabelText\":[{\"type\":0,\"value\":\"social media post\"}],\"PinterestEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Pinterest content\"}],\"PodcastDetailedPage.contextualHeader\":[{\"type\":0,\"value\":\"You Might Like This\"}],\"PodcastDetailedPage.primaryCTALabel\":[{\"type\":0,\"value\":\"Start Listening Now\"}],\"PodcastDetailedPage.relatedArticleHed\":[{\"type\":0,\"value\":\"Related Articles\"}],\"ProductCard.CardRating\":[{\"type\":0,\"value\":\"Rating: \"},{\"type\":1,\"value\":\"rating\"},{\"type\":0,\"value\":\"\\u002F10\"}],\"ProductEmbed.DefaultCtaText\":[{\"type\":0,\"value\":\"Buy It\"}],\"ProductEmbed.DefaultTextPreamble\":[{\"type\":0,\"value\":\"Learn More\"}],\"ProductEmbed.PriceWithNoSellerPreamble\":[{\"type\":0,\"value\":\"Buy for \"},{\"type\":1,\"value\":\"price\"}],\"ProductEmbed.PriceWithSellerPreamble\":[{\"type\":1,\"value\":\"price\"},{\"type\":0,\"value\":\" at \"},{\"type\":1,\"value\":\"sellerName\"}],\"ProductEmbed.VenueCtaText\":[{\"type\":0,\"value\":\"Book Now\"}],\"ProductOffer.defaultPriceString\":[{\"type\":1,\"value\":\"priceValue\"},{\"type\":0,\"value\":\" \"},{\"type\":1,\"value\":\"sellerName\"}],\"ProductOffer.price\":[{\"type\":1,\"value\":\"priceValue\"},{\"type\":0,\"value\":\" at \"},{\"type\":1,\"value\":\"sellerName\"}],\"ProductOffer.productOffersaveBookmarkLabel\":[{\"type\":0,\"value\":\"Save story\"}],\"ProductOffer.variationDefaultCTA\":[{\"type\":0,\"value\":\"Shop Now\"}],\"ProductOffer.variationSellerNameString\":[{\"type\":0,\"value\":\"Shop at\"}],\"ProsCons.ConsLabel\":[{\"type\":0,\"value\":\"Cons →\"}],\"ProsCons.ProsLabel\":[{\"type\":0,\"value\":\"Pros →\"}],\"Rating.RatingAriaLabel\":[{\"type\":0,\"value\":\"Rating\"}],\"RatingsForm.LoadingText\":[{\"type\":0,\"value\":\"Loading...\"}],\"RatingsForm.PreviousRatingText\":[{\"type\":0,\"value\":\"You previously rated \"},{\"type\":1,\"value\":\"RATING_SUBJECT\"},{\"type\":0,\"value\":\".\"}],\"RatingsForm.PromptText\":[{\"type\":0,\"value\":\"How would you rate \"},{\"type\":1,\"value\":\"RATING_SUBJECT\"},{\"type\":0,\"value\":\"?\"}],\"RatingsForm.SuccessText\":[{\"type\":0,\"value\":\"Thanks for your feedback!\"}],\"ReadMore.SeriesRecirc\":[{\"type\":0,\"value\":\"Read more\"}],\"ReadNext.SeriesRecirc\":[{\"type\":0,\"value\":\"Read next\"}],\"RecipePage.ActiveTime\":[{\"type\":0,\"value\":\"Active Time\"}],\"RecipePage.CommunityGuidelines\":[{\"type\":0,\"value\":\"Community Guidelines\"}],\"RecipePage.CooksNote\":[{\"type\":0,\"value\":\"Cooks' Note\"}],\"RecipePage.LikeActionErrorMessage\":[{\"type\":0,\"value\":\"Unable to like this note. Please try again.\"}],\"RecipePage.MessageBannerContentHed\":[{\"type\":1,\"value\":\"messageContentHed\"}],\"RecipePage.MessageBannerContentTrail\":[{\"type\":1,\"value\":\"messageContentTrail\"}],\"RecipePage.MessageBannerSubContent\":[{\"type\":1,\"value\":\"messageSubContent\"}],\"RecipePage.MessageBannerTitle\":[{\"type\":1,\"value\":\"title\"}],\"RecipePage.PrepTime\":[{\"type\":0,\"value\":\"Prep Time\"}],\"RecipePage.TotalTime\":[{\"type\":0,\"value\":\"Total Time\"}],\"RecipePage.UnlikeActionErrorMessage\":[{\"type\":0,\"value\":\"Unable to unlike this note. Please try again.\"}],\"RecipePage.Yield\":[{\"type\":0,\"value\":\"Yield\"}],\"RecipePage.info\":[{\"type\":0,\"value\":\"Recipe information\"}],\"RecipeProductCarousel.Title\":[{\"type\":0,\"value\":\"What you’ll need\"}],\"RecircList.ReadMore\":[{\"type\":0,\"value\":\"Read More\"}],\"RecircMostPopular.SectionTitle\":[{\"type\":0,\"value\":\"Most Popular\"}],\"RedditEmbed.AriaLabelText\":[{\"type\":0,\"value\":\"social media post\"}],\"RedditEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Reddit content\"}],\"RelatedList.SectionTitle\":[{\"type\":0,\"value\":\"Related Stories\"}],\"RelatedVideo.HeaderText\":[{\"type\":0,\"value\":\"Featured Video\"}],\"ResponsiveCartoon.cartButtonMessage\":[{\"type\":0,\"value\":\"Shop\"}],\"ResponsiveCartoon.copiedLinkAlertMessage\":[{\"type\":0,\"value\":\"Link copied\"}],\"ResponsiveCartoon.copyLinkButtonMessage\":[{\"type\":0,\"value\":\"Copy link to cartoon\"}],\"ResponsiveCartoon.downloadButtonMessage\":[{\"type\":0,\"value\":\"Download\"}],\"ResponsiveCartoon.openCartoonGalleryButtonIconMessage\":[{\"type\":0,\"value\":\"Open Gallery\"}],\"ResponsiveCartoon.openCartoonGalleryButtonMessage\":[{\"type\":0,\"value\":\"Open cartoon gallery\"}],\"ResponsiveClip.ClipAriaLabel\":[{\"type\":0,\"value\":\"Play\\u002FPause\"}],\"ResponsiveClip.ClipLabel\":[{\"type\":0,\"value\":\"Play\\u002FPause Button\"}],\"ReviewForm.AlertMessageError\":[{\"type\":0,\"value\":\"Your feedback wasn't posted due to some error, please try again.\"}],\"ReviewForm.AlertMessageSuccess\":[{\"type\":0,\"value\":\"Thanks for your feedback!\"}],\"ReviewForm.FakeInputPlaceholderText\":[{\"type\":0,\"value\":\"Tell us what you think\"}],\"ReviewForm.Hed\":[{\"type\":0,\"value\":\"Leave a Review\"}],\"ReviewForm.InvalidFieldErrorMessage\":[{\"type\":0,\"value\":\"Required fields missing\"}],\"ReviewForm.IsAnonymousCheckboxLabel\":[{\"type\":0,\"value\":\"Share anonymously\"}],\"ReviewForm.LocationFieldLabel\":[{\"type\":0,\"value\":\"Where are you from?\"}],\"ReviewForm.LocationFieldPlaceholder\":[{\"type\":0,\"value\":\"Boston, MA\"}],\"ReviewForm.ReviewTextFieldLabel\":[{\"type\":0,\"value\":\"Your Review\"}],\"ReviewForm.ReviewTextFieldPlaceholder\":[{\"type\":0,\"value\":\"Let us know your thoughts…\"}],\"ReviewForm.ReviewerInfoFieldLabel\":[{\"type\":0,\"value\":\"Display Name\"}],\"ReviewForm.ReviewerInfoFieldPlaceholder\":[{\"type\":0,\"value\":\"Jane Doe\"}],\"ReviewForm.SubmitButtonLabel\":[{\"type\":0,\"value\":\"Submit\"}],\"ReviewForm.SubmitButtonLabelLoading\":[{\"type\":0,\"value\":\"Loading…\"}],\"ReviewForm.WillPrepareAgainOption1Label\":[{\"type\":0,\"value\":\"Yes\"}],\"ReviewForm.WillPrepareAgainOption2Label\":[{\"type\":0,\"value\":\"No\"}],\"ReviewForm.WillPrepareAgainRadioLabel\":[{\"type\":0,\"value\":\"Would you make this recipe again?\"}],\"ReviewForm.nonLoggedInErrorMessage\":[{\"type\":0,\"value\":\"Sign in or create an account to add note.\"}],\"ReviewList.Loading\":[{\"type\":0,\"value\":\"Loading…\"}],\"ReviewList.ReviewListError\":[{\"type\":0,\"value\":\"Sorry, more reviews can‘t be loaded right now. \"},{\"type\":1,\"value\":\"br\"},{\"type\":0,\"value\":\" Please try again later.\"}],\"ReviewList.ReviewReplyLabel\":[{\"type\":0,\"value\":\"Reply\"}],\"ReviewListContainer.UtilityLabel\":[{\"type\":0,\"value\":\"Back to Top\"}],\"ReviewNoteModal.CloseButtonAriaLabel\":[{\"type\":0,\"value\":\"Close ReviewNoteModal Modal\"}],\"ReviewNoteModal.CloseButtonLabel\":[{\"type\":0,\"value\":\"close modal button label\"}],\"ReviewNoteModal.Hed\":[{\"type\":1,\"value\":\"hed\"}],\"ReviewNoteModal.continueLabel\":[{\"type\":1,\"value\":\"continueLabel\"}],\"ReviewNoteModal.dek\":[{\"type\":1,\"value\":\"dek\"}],\"ReviewNoteModal.discardLabel\":[{\"type\":1,\"value\":\"discardLabel\"}],\"ReviewNotes.AddNoteFailedToastMessage\":[{\"type\":0,\"value\":\"Unable to add your note. Please try again.\"}],\"ReviewNotesForm.ReviewerInfoFieldLabel\":[{\"type\":0,\"value\":\"Adding Note As:\"}],\"ReviewNotesForm.ReviewerRatingLabel\":[{\"type\":1,\"value\":\"reviewerRatingLabel\"}],\"ReviewNotesForm.addNoteLabel\":[{\"type\":0,\"value\":\"Add Note\"}],\"ReviewNotesForm.buttonLabel\":[{\"type\":0,\"value\":\"Sign in or create account\"}],\"ReviewNotesForm.cancelNoteLabel\":[{\"type\":0,\"value\":\"Discard\"}],\"ReviewNotesForm.defaultcommunityReviewText\":[{\"type\":0,\"value\":\"Ask a question or leave a helpful tip, suggestion or opinion that is relevant and respectful for the community.\"}],\"ReviewNotesForm.invalidReviewNoteLength\":[{\"type\":1,\"value\":\"message\"}],\"ReviewNotesForm.maxCharLimitMet\":[{\"type\":0,\"value\":\"_MAX_ character limit met\"}],\"ReviewNotesForm.messageBannerText\":[{\"type\":0,\"value\":\"Join the home cook community and add recipe notes.\"}],\"ReviewNotesForm.remainingMaxCharLimit\":[{\"type\":0,\"value\":\"_COUNT_ of _MAX_ character limit remaining\"}],\"ReviewNotesForm.requiredField\":[{\"type\":1,\"value\":\"message\"}],\"ReviewNotesForm.reviewTagsLabel\":[{\"type\":0,\"value\":\"TAG YOUR NOTE (OPTIONAL)\"}],\"ReviewNotesForm.reviewerFieldInfoIconText\":[{\"type\":0,\"value\":\"Your username appears next to your recipe notes and replies. Change it anytime in My Account.\"}],\"ReviewNotesForm.reviewerInfoIconButtonLabel\":[{\"type\":0,\"value\":\"user name update message\"}],\"ReviewNotesForm.textFieldLabel\":[{\"type\":0,\"value\":\"Your Review\"}],\"ReviewRatingData.ButtonLabel\":[{\"type\":0,\"value\":\"Open rating explainer\"}],\"ReviewRatingData.DataLabel\":[{\"type\":0,\"value\":\"Rating:\"}],\"ReviewReplyComment.HideRepliesLabel\":[{\"type\":0,\"value\":\"Hide replies\"}],\"ReviewReplyComment.LoadingRepliesLabel\":[{\"type\":0,\"value\":\"Loading…\"}],\"ReviewReplyComment.ReviewReplyCommentLabel\":[{\"type\":0,\"value\":\"Reply\"}],\"ReviewReplyComment.ReviewReplyLabel\":[{\"type\":0,\"value\":\"Replying to\"}],\"ReviewReplyComment.ShowMoreRepliesLabel\":[{\"type\":0,\"value\":\"Show more replies\"}],\"ReviewReplyNote.AddReplyFailedToastMessage\":[{\"type\":0,\"value\":\"Unable to add your reply. Please try again.\"}],\"ReviewReplyNote.AddReplySuccessToastMessage\":[{\"type\":0,\"value\":\"Reply added\"}],\"ReviewReplyNote.CancelButtonLabel\":[{\"type\":0,\"value\":\"Discard\"}],\"ReviewReplyNote.ReplyButtonLabel\":[{\"type\":0,\"value\":\"Reply\"}],\"ReviewReplyNote.ReplyFieldPlaceHolder\":[{\"type\":0,\"value\":\"Add your reply here\"}],\"ReviewReplyNote.ReplyTextFieldLabel\":[{\"type\":0,\"value\":\"Your Reply\"}],\"ReviewReplyNote.ReviewFieldAlertLimitErrorText\":[{\"type\":0,\"value\":\"_CHARACTER_LIMIT_CURRENT_ of _CHARACTER_LIMIT_ character limit remaining.\"}],\"ReviewReplyNote.ReviewFieldMaxLimitErrorText\":[{\"type\":0,\"value\":\"_CHARACTER_LIMIT_ character limit met.\"}],\"ReviewReplyNote.ReviewFieldMinLimitErrorText\":[{\"type\":0,\"value\":\"Enter 2 characters or more to add a reply.\"}],\"ReviewReplyNote.ReviewReplyLabel\":[{\"type\":0,\"value\":\"Replying To:\"}],\"ReviewSummary.SummaryLabel\":[{\"type\":0,\"value\":\"TL;DR:\"}],\"RunwayGalleryFilmstrip.FilmstripCollapsedMessage\":[{\"type\":0,\"value\":\"Explore Collection\"}],\"RunwayGalleryFilmstrip.FilmstripCollapsedMessageForNonCollectionGalleries\":[{\"type\":0,\"value\":\"Explore Gallery\"}],\"RunwayGalleryFilmstrip.FilmstripExpandedMessage\":[{\"type\":0,\"value\":\"Hide Collection\"}],\"RunwayGalleryFilmstrip.FilmstripExpandedMessageForNonCollectionGalleries\":[{\"type\":0,\"value\":\"Hide Gallery\"}],\"RunwayGalleryLookNumber.imageLookNumberPrefix\":[{\"type\":0,\"value\":\"Look\"}],\"RunwayGalleryPage.SaveBookmarkAlertLink\":[{\"type\":0,\"value\":\"VIEW ALL\"}],\"RunwayGalleryPage.SaveBookmarkAlertText\":[{\"type\":0,\"value\":\"Image saved\"}],\"ScoreBox.BestNewMusic\":[{\"type\":0,\"value\":\"Best New Music\"}],\"ScoreBox.BestNewReissue\":[{\"type\":0,\"value\":\"Best New Reissue\"}],\"ScoreBox.BestNewTrack\":[{\"type\":0,\"value\":\"Best New Track\"}],\"SearchBar.placeholder\":[{\"type\":0,\"value\":\"Search for \\\"stir-fry\\\"\"}],\"SearchPage.ArtistTitle\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":0,\"value\":\"Artist\"}]},\"other\":{\"value\":[{\"type\":0,\"value\":\"Artists\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"SearchPage.AuthorTitle\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":0,\"value\":\"Author\"}]},\"other\":{\"value\":[{\"type\":0,\"value\":\"Authors\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"SearchPage.ClearAll\":[{\"type\":0,\"value\":\"Clear All\"}],\"SearchPage.FeatureTitle\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":0,\"value\":\"Feature\"}]},\"other\":{\"value\":[{\"type\":0,\"value\":\"Features\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"SearchPage.FilterResults\":[{\"type\":0,\"value\":\"Filter Results\"}],\"SearchPage.LoadMoreButtonLabel\":[{\"type\":0,\"value\":\"More Stories\"}],\"SearchPage.LoadMoreLoadingLabel\":[{\"type\":0,\"value\":\"Loading ...\"}],\"SearchPage.Loading\":[{\"type\":0,\"value\":\"Loading ...\"}],\"SearchPage.MoreStories\":[{\"type\":0,\"value\":\"More Stories\"}],\"SearchPage.NewsTitle\":[{\"type\":0,\"value\":\"News\"}],\"SearchPage.NoResultsFound\":[{\"type\":0,\"value\":\"Sorry we can't display any results for those filtering options, please try again\"}],\"SearchPage.NoResultsHed\":[{\"type\":0,\"value\":\"No stories found for your search\"}],\"SearchPage.ResultsHed\":[{\"type\":0,\"value\":\"Search stories from \"},{\"type\":1,\"value\":\"brandName\"}],\"SearchPage.ResultsHedOnIssueDate\":[{\"type\":0,\"value\":\"Search Results from \"},{\"type\":1,\"value\":\"issueDate\"},{\"type\":0,\"value\":\" issue\"}],\"SearchPage.ReviewTitle\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":0,\"value\":\"Review\"}]},\"other\":{\"value\":[{\"type\":0,\"value\":\"Reviews\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"SearchPage.SearchButtonLabel\":[{\"type\":0,\"value\":\"Search\"}],\"SearchPage.SearchInputAriaLabel\":[{\"type\":0,\"value\":\"search\"}],\"SearchPage.SearchInputPlaceholder\":[{\"type\":0,\"value\":\"Try \\\"Racial justice\\\"\"}],\"SearchPage.SortBy\":[{\"type\":0,\"value\":\"Sort By\"}],\"SearchPage.SortLabel\":[{\"type\":0,\"value\":\"Sort by\"}],\"SearchPage.ThePitchTitle\":[{\"type\":0,\"value\":\"The Pitch\"}],\"SearchPage.TrackTitle\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":0,\"value\":\"Track\"}]},\"other\":{\"value\":[{\"type\":0,\"value\":\"Tracks\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"SearchPage.VideoTitle\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":0,\"value\":\"Video\"}]},\"other\":{\"value\":[{\"type\":0,\"value\":\"Videos\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"SearchPage.noResultsContentHed\":[{\"type\":0,\"value\":\"We didn't find any recipes, articles or videos for\"}],\"SearchPage.noResultsCustomContentHed\":[{\"type\":0,\"value\":\"We didn't find any results for\"}],\"SearchPage.noResultsSubHed\":[{\"type\":0,\"value\":\"We didn't find any\"}],\"SearchPage.resultswithWordHed\":[{\"type\":0,\"value\":\"Search results for\"}],\"SearchPage.showAllArtists\":[{\"type\":0,\"value\":\"SHOW ALL ARTISTS\"}],\"SearchPage.showAllAuthors\":[{\"type\":0,\"value\":\"SHOW ALL AUTHORS\"}],\"SearchResultsIndicator.EmptyResultText\":[{\"type\":0,\"value\":\"explore these instead\"}],\"SearchResultsIndicator.ResultsTextWithTerm\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" story\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":1,\"value\":\"moreResultsIndicator\"},{\"type\":0,\"value\":\" stories\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"resultCount\"},{\"type\":0,\"value\":\" about \\\"\"},{\"type\":1,\"value\":\"searchTerm\"},{\"type\":0,\"value\":\"\\\"\"}],\"SearchResultsIndicator.ResultsTextWithoutTerm\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" story\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":1,\"value\":\"moreResultsIndicator\"},{\"type\":0,\"value\":\" results\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"resultCount\"},{\"type\":0,\"value\":\" from \"},{\"type\":1,\"value\":\"brandName\"}],\"SearchResultsIndicator.ResultsWithPagination\":[{\"type\":1,\"value\":\"pageStartIndex\"},{\"type\":0,\"value\":\" - \"},{\"type\":1,\"value\":\"pageEndIndex\"},{\"type\":0,\"value\":\" of \"},{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Result\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":1,\"value\":\"moreResultsIndicator\"},{\"type\":0,\"value\":\" Results\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"resultCount\"}],\"SearchResultsIndicator.resultsList\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Result\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":1,\"value\":\"moreResultsIndicator\"},{\"type\":0,\"value\":\" Results\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"resultCount\"}],\"SearchResultsIndicator.resultsListWithEntity\":[{\"type\":1,\"value\":\"resultCount\"},{\"type\":1,\"value\":\"moreResultsIndicator\"},{\"type\":0,\"value\":\" \"},{\"type\":1,\"value\":\"entity\"}],\"SearchResultsIndicator.resultsListWithLocation\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Result\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":1,\"value\":\"moreResultsIndicator\"},{\"type\":0,\"value\":\" Results\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"resultCount\"},{\"type\":0,\"value\":\" in \"},{\"type\":1,\"value\":\"location\"}],\"SearchResultsIndicator.resultsListWithLocationAndEntity\":[{\"type\":1,\"value\":\"resultCount\"},{\"type\":1,\"value\":\"moreResultsIndicator\"},{\"type\":0,\"value\":\" \"},{\"type\":1,\"value\":\"entity\"},{\"type\":0,\"value\":\" in \"},{\"type\":1,\"value\":\"location\"}],\"SearchResultsListPage.DekTextActivity\":[{\"type\":0,\"value\":\"The best activities in \"},{\"type\":1,\"value\":\"locationName\"},{\"type\":0,\"value\":\", as reviewed by our editors and contributors.\"}],\"SearchResultsListPage.DekTextHotel\":[{\"type\":0,\"value\":\"The best hotels in \"},{\"type\":1,\"value\":\"locationName\"},{\"type\":0,\"value\":\", as reviewed by our editors and contributors. We've stayed at some of the finest properties around the world, and these made the top of our list.\"}],\"SearchResultsListPage.HeaderText\":[{\"type\":1,\"value\":\"pageHed\"},{\"type\":0,\"value\":\" \"},{\"options\":{\"activity\":{\"value\":[{\"type\":0,\"value\":\"activities\"}]},\"hotel\":{\"value\":[{\"type\":0,\"value\":\"hotels\"}]},\"other\":{\"value\":[]}},\"type\":5,\"value\":\"contentType\"}],\"SearchResultsListPage.Loading\":[{\"type\":0,\"value\":\"Loading ...\"}],\"SearchResultsListPage.MoreResults\":[{\"type\":0,\"value\":\"Load More\"}],\"SearchResultsListPage.NoResults\":[{\"type\":0,\"value\":\"No results. Please try again.\"}],\"SearchResultsListPage.ResultsWithoutAnyFilter\":[{\"type\":0,\"value\":\"We currently do not have any results that match your criteria. Here are design professionals who recently joined the Directory.\"}],\"SearchResultsListPage.ResultsWithoutProfession\":[{\"type\":0,\"value\":\"We do not have any results that match your exact criteria. Here are other \"},{\"children\":[{\"type\":1,\"value\":\"selectedCategory\"}],\"type\":8,\"value\":\"b\"},{\"type\":0,\"value\":\" professionals in \"},{\"children\":[{\"type\":1,\"value\":\"selectedState\"}],\"type\":8,\"value\":\"b\"},{\"type\":0,\"value\":\".\"}],\"SearchResultsListPage.ResultsWithoutProfessionAndALLState\":[{\"type\":0,\"value\":\"We do not have any results that match your exact criteria. Here are other \"},{\"children\":[{\"type\":1,\"value\":\"selectedCategory\"}],\"type\":8,\"value\":\"b\"},{\"type\":0,\"value\":\" professionals.\"}],\"SearchResultsListPage.ResultsWithoutState\":[{\"type\":0,\"value\":\"We do not have any results that match your exact criteria. Here are \"},{\"children\":[{\"type\":1,\"value\":\"selectedCategory\"}],\"type\":8,\"value\":\"b\"},{\"type\":0,\"value\":\" professionals in other locations.\"}],\"SearchResultsListPage.ResultsWithoutStateProfession\":[{\"type\":0,\"value\":\"We do not have any results that match your exact criteria. Here are \"},{\"children\":[{\"type\":1,\"value\":\"selectedCategory\"}],\"type\":8,\"value\":\"b\"},{\"type\":0,\"value\":\" professionals in other locations.\"}],\"SearchResultsListPage.SearchButtonLabel\":[{\"type\":0,\"value\":\"Search\"}],\"SearchResultsListPage.searchResultsListPageHedText\":[{\"type\":0,\"value\":\"Browse our trusted list of \"},{\"type\":1,\"value\":\"brand\"},{\"type\":0,\"value\":\"-approved designers\"}],\"SearchableSummaryCollection.AsyncDropdownPlaceholder\":[{\"type\":0,\"value\":\"Search by city or destination\"}],\"SearchableSummaryCollection.ClickoutButtonLabel\":[{\"type\":0,\"value\":\"View all \"},{\"type\":1,\"value\":\"location\"},{\"type\":0,\"value\":\" hotels\"}],\"SearchableSummaryCollection.NoMatchesFoundLabel\":[{\"type\":0,\"value\":\"No matches found\"}],\"SearchableSummaryCollection.NoResultsMessage\":[{\"type\":0,\"value\":\"Sorry, there are no results for your search - please try another location\"}],\"SearchableSummaryCollection.SearchContainerHed\":[{\"type\":0,\"value\":\"Where do you want to go?\"}],\"SearchableSummaryCollection.SubmitButtonLabel\":[{\"type\":0,\"value\":\"Search\"}],\"SecondaryMenu.NavDropdownAssistiveLabel\":[{\"type\":0,\"value\":\"Select international site\"}],\"SecondaryMenu.NavDropdownHeader\":[{\"type\":0,\"value\":\"Explore \"},{\"type\":1,\"value\":\"rootBrandName\"},{\"type\":0,\"value\":\" across the globe\"}],\"SecondaryMenu.NavigationPrimaryAriaLabel\":[{\"type\":0,\"value\":\"Primary\"}],\"SecondaryMenu.SearchLinkText\":[{\"type\":0,\"value\":\"Search\"}],\"SecondaryMenu.SecondaryLinksAriaLabel\":[{\"type\":0,\"value\":\"Secondary\"}],\"SecondaryMenu.SignInLinkText\":[{\"type\":0,\"value\":\"Sign in\"}],\"SecondaryMenu.UtilityLinksAriaLabel\":[{\"type\":0,\"value\":\"Utility\"}],\"ShopifyCart.CartHeader\":[{\"type\":0,\"value\":\"Shopping Cart\"}],\"ShopifyCartEmpty.EmptyCartButtonLabel\":[{\"type\":0,\"value\":\"GO SHOPPING\"}],\"ShopifyCartEmpty.EmptyCartButtonLink\":[{\"type\":0,\"value\":\"\\u002Fshop\\u002Flisting\\u002Fall\"}],\"ShopifyCartEmpty.EmptyCartHeader\":[{\"type\":0,\"value\":\"YOUR SHOPPING CART IS EMPTY\"}],\"ShopifyCartItem.CheckoutLabel\":[{\"type\":0,\"value\":\"CHECK OUT\"}],\"ShopifyCartItem.CheckoutText\":[{\"type\":0,\"value\":\"Shipping and taxes calculated at checkout\"}],\"ShopifyCartItem.RetailerLabel\":[{\"type\":0,\"value\":\"Retailer:\"}],\"ShopifyCartItem.SubtotalLabel\":[{\"type\":0,\"value\":\"Subtotal\"}],\"ShopifyProductDetail.addToCartLabel\":[{\"type\":0,\"value\":\"Add To Cart\"}],\"ShopifyProductDetail.quantityLabel\":[{\"type\":0,\"value\":\"Quantity\"}],\"ShoppableAssetEmbed.shoppingIconHoverText\":[{\"type\":0,\"value\":\"Shop the look\"}],\"SignInModal.CloseButtonAriaLabel\":[{\"type\":0,\"value\":\"Close Sign In Modal\"}],\"SignInModal.CloseButtonLabel\":[{\"type\":0,\"value\":\"Close Sign In Modal\"}],\"SignInModal.Hed\":[{\"type\":0,\"value\":\"Save stories\"}],\"SignInModal.HedSpanTag\":[{\"type\":0,\"value\":\"with an account\"}],\"SignInModal.OidcSignInLinkText\":[{\"type\":0,\"value\":\"Sign in\"}],\"SignInModal.OidcSignInText\":[{\"type\":0,\"value\":\"Already have an account?\"}],\"SignInModal.OidcSignUpButtonLabel\":[{\"type\":0,\"value\":\"Create account\"}],\"SignOutButton.SignOut\":[{\"type\":0,\"value\":\"Sign Out\"}],\"SimpleNavigation.SearchLabel\":[{\"type\":0,\"value\":\"Search\"}],\"SiteFooter.Dropdown\":[{\"type\":0,\"value\":\"Select international site\"}],\"SiteFooter.NoticesContainer\":[{\"type\":0,\"value\":\"Notices\"}],\"SiteFooter.OneTrustButtonLabel\":[{\"type\":0,\"value\":\"Do Not Sell My Personal Info\"}],\"SiteHeader.ScrollingNavigation\":[{\"type\":0,\"value\":\"Primary\"}],\"SiteHeader.UtilityNavigation\":[{\"type\":0,\"value\":\"Utility\"}],\"SmallProductCard.AtRetailerNameComponentText\":[{\"type\":0,\"value\":\"At \"},{\"type\":1,\"value\":\"retailerNameText\"}],\"SmallProductCard.AtRetailerNameLabel\":[{\"type\":1,\"value\":\"finalPriceLabel\"},{\"type\":0,\"value\":\" At \"},{\"type\":1,\"value\":\"retailerNameText\"}],\"SmallProductCard.BuyAt\":[{\"type\":0,\"value\":\"Buy At \"},{\"type\":1,\"value\":\"retailerNameText\"}],\"SplitScreenContentHeader.RatingLinkLabel\":[{\"type\":0,\"value\":\"Read Reviews\"}],\"SplitScreenContentHeader.VariousArtists\":[{\"type\":0,\"value\":\"Various Artists\"}],\"SponsoredContentHeader.BylineAdvertisementFeatureWith\":[{\"type\":0,\"value\":\"Advertisement Feature With\"}],\"SponsoredContentHeader.BylineAdvertising\":[{\"type\":0,\"value\":\"Advertising\"}],\"SponsoredContentHeader.BylineBrandPresentsAdvertiser\":[{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" Presents\"}],\"SponsoredContentHeader.BylineBrandXAdvertiser\":[{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" X\"}],\"SponsoredContentHeader.BylineBrandedContent\":[{\"type\":0,\"value\":\"Branded Content By\"}],\"SponsoredContentHeader.BylineCreated\":[{\"type\":0,\"value\":\"Created By \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" For\"}],\"SponsoredContentHeader.BylineInCollaboration\":[{\"type\":0,\"value\":\"In Collaboration With\"}],\"SponsoredContentHeader.BylineInPartnership\":[{\"type\":0,\"value\":\"In Partnership With\"}],\"SponsoredContentHeader.BylineOriginalContentBy\":[{\"type\":0,\"value\":\"Original Content By\"}],\"SponsoredContentHeader.BylinePR\":[{\"type\":0,\"value\":\"PR\"}],\"SponsoredContentHeader.BylinePaidPost\":[{\"type\":0,\"value\":\"PAID POST\"}],\"SponsoredContentHeader.BylinePaidPostByAdvertiser\":[{\"type\":0,\"value\":\"Paid Post by \"},{\"type\":1,\"value\":\"sponsorName\"},{\"type\":0,\"value\":\", Brought to you By Business Reporter\"}],\"SponsoredContentHeader.BylinePresentedByAdvertiser\":[{\"type\":0,\"value\":\"Presented By\"}],\"SponsoredContentHeader.BylineProduced\":[{\"type\":0,\"value\":\"Produced By\"}],\"SponsoredContentHeader.BylineProducedByAdvertiser\":[{\"type\":0,\"value\":\"Produced By\"}],\"SponsoredContentHeader.BylineProducedByBrand\":[{\"type\":0,\"value\":\"Produced By \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" With\"}],\"SponsoredContentHeader.BylinePromotedByAdvertiser\":[{\"type\":0,\"value\":\"Promoted by\"}],\"SponsoredContentHeader.BylinePromotion\":[{\"type\":0,\"value\":\"Promotion\"}],\"SponsoredContentHeader.BylinePublishedByAdvertiser\":[{\"type\":0,\"value\":\"Published By\"}],\"SponsoredContentHeader.BylineSponsored\":[{\"type\":0,\"value\":\"Sponsored content\"}],\"SponsoredContentHeader.BylineSponsoredBy\":[{\"type\":0,\"value\":\"Sponsored By\"}],\"SponsoredContentHeader.BylineSponsoredContent\":[{\"type\":0,\"value\":\"Sponsored Content By\"}],\"SponsoredContentHeader.BylineTogetherWith\":[{\"type\":0,\"value\":\"Together with\"}],\"SponsoredContentHeader.SponsoredLinkCTA\":[{\"type\":0,\"value\":\"Click to go to \"},{\"type\":1,\"value\":\"sponsorName\"},{\"type\":0,\"value\":\"'s website\"}],\"SponsoredContentHeader.bylineAd\":[{\"type\":0,\"value\":\"Ad\"}],\"SponsoredContentHeader.bylineAdvertisementByAdvertiser\":[{\"type\":0,\"value\":\"Advertisement By\"}],\"SponsoredContentHeader.bylineAffiliatePartner\":[{\"type\":0,\"value\":\"Affiliate Partner\"}],\"SponsoredContentHeader.bylineInPartnershipWithAdvertiser\":[{\"type\":0,\"value\":\"In Partnership With\"}],\"SponsoredContentHeader.bylinePaidPartnershipWithAdvertiser\":[{\"type\":0,\"value\":\"Paid Partnership With\"}],\"SponsoredContentHeader.bylinePaidPromotionByAdvertiser\":[{\"type\":0,\"value\":\"Paid Promotion By\"}],\"SponsoredContentHeader.bylineSpecialFeature\":[{\"type\":0,\"value\":\"Special Feature\"}],\"SponsoredContentHeader.bylineSponsoredByAdvertiser\":[{\"type\":0,\"value\":\"Sponsored By\"}],\"StackedNavigation.BookmarkAlertLabel\":[{\"type\":0,\"value\":\"Find anything you save across the site in your account\"}],\"StackedNavigation.DrawerLabel\":[{\"type\":0,\"value\":\"Navigation and Sign Up Menu\"}],\"StackedNavigation.MenuButton\":[{\"type\":0,\"value\":\"Open Navigation Menu\"}],\"StackedNavigation.OpenSearchMenuLabel\":[{\"type\":0,\"value\":\"Open Search Menu\"}],\"StackedNavigation.PrimaryLinksLabel\":[{\"type\":0,\"value\":\"Primary\"}],\"StackedNavigation.ProfileLinkLabel\":[{\"type\":0,\"value\":\"My Profile\"}],\"StackedNavigation.SavedStoriesLabel\":[],\"StackedNavigation.SearchLabel\":[{\"type\":0,\"value\":\"Search\"}],\"StackedNavigation.SubscribeLabel\":[{\"type\":0,\"value\":\"Subscribe\"}],\"StackedNavigation.UtilityLinksLabel\":[{\"type\":0,\"value\":\"Utility\"}],\"StandardNavigation.AccountBookmarkAlertLabel\":[{\"type\":0,\"value\":\"To revisit this article, select My Account, then\"}],\"StandardNavigation.AccountLabel\":[{\"type\":0,\"value\":\"My Account\"}],\"StandardNavigation.BookmarkAlertLabel\":[{\"type\":0,\"value\":\"To revisit this article, visit My Profile, then\"}],\"StandardNavigation.Drawer\":[{\"type\":0,\"value\":\"Navigation and Sign Up Menu\"}],\"StandardNavigation.MenuButton\":[{\"type\":0,\"value\":\"Open Navigation Menu\"}],\"StandardNavigation.OpenSearchMenuLabel\":[{\"type\":0,\"value\":\"Open Search Menu\"}],\"StandardNavigation.SavedStoriesLabel\":[{\"type\":0,\"value\":\"View saved stories\"}],\"StandardNavigation.SearchLabel\":[{\"type\":0,\"value\":\"Search\"}],\"StandardNavigation.SecondaryLinksLabel\":[{\"type\":0,\"value\":\"Secondary\"}],\"StandardNavigation.ShoppingCartAriaLabel\":[{\"type\":0,\"value\":\"item(s) in Cart\"}],\"StandardNavigation.ShoppingCartLabel\":[{\"type\":0,\"value\":\"Shopping Cart\"}],\"StandardNavigation.SignInLabel\":[{\"type\":0,\"value\":\"Sign In\"}],\"StandardNavigation.UtilityLinksLabel\":[{\"type\":0,\"value\":\"Utility\"}],\"StandardNavigation.saveBookmarkLabel\":[{\"type\":0,\"value\":\"Save story\"}],\"SummaryCarousel.seeMoreAriaLabel\":[{\"type\":0,\"value\":\"See more videos\"}],\"SummaryCollageThree.seeMore\":[{\"type\":0,\"value\":\"See More Videos\"}],\"SummaryCollectionSplitColumns.GuideItemHed\":[{\"type\":1,\"value\":\"location\"},{\"type\":0,\"value\":\" Travel Guide\"}],\"SummaryCollectionSplitColumns.RecommendedTitle\":[{\"type\":0,\"value\":\"Recommended \"},{\"type\":1,\"value\":\"recommendedType\"}],\"SummaryCollectionSplitColumns.ViewAllButton\":[{\"type\":0,\"value\":\"View All \"},{\"type\":1,\"value\":\"location\"},{\"type\":0,\"value\":\" \"},{\"type\":1,\"value\":\"recommendedType\"}],\"SummaryItem.BusinessProfileCTAText\":[{\"type\":0,\"value\":\"View Profile\"}],\"SummaryItem.DefaultCTAText\":[{\"type\":0,\"value\":\"Book Now\"}],\"SummaryItem.DekReadMoreText\":[{\"type\":0,\"value\":\"Read full review\"}],\"SummaryItem.FuturePremiereLabel\":[{\"type\":0,\"value\":\"PREMIERES\"}],\"SummaryItem.LiveVideoLabel\":[{\"type\":0,\"value\":\"live\"}],\"SummaryItem.MoreInfoAndEpisodesPodcastCTA\":[{\"type\":0,\"value\":\"More Info and Episodes\"}],\"SummaryItem.NowShoppingLabel\":[{\"type\":0,\"value\":\"Now Shopping\"}],\"SummaryItem.PastPremiereLabel\":[{\"type\":0,\"value\":\"PREMIERED\"}],\"SummaryItem.ReadMore\":[{\"type\":0,\"value\":\"Read More\"}],\"SummaryItem.ShopNowCTA\":[{\"type\":0,\"value\":\"Shop Now\"}],\"SummaryItem.Slides\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" slide\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" slides\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"slidesCount\"}],\"SummaryItem.SponsoredContent\":[{\"type\":0,\"value\":\"Sponsored Content\"}],\"SummaryItem.StartListeningNowPodcastCTA\":[{\"type\":0,\"value\":\"Start Listening Now\"}],\"SummaryItem.TodayLabel\":[{\"type\":0,\"value\":\"TODAY\"}],\"SummaryItem.VenueSellerPreviewText\":[{\"type\":0,\"value\":\"Powered By:\"}],\"SummaryItemFeatured.FeaturedTitle\":[{\"type\":0,\"value\":\"Featured\"}],\"SummarySpotlight.ContinueReading\":[{\"type\":0,\"value\":\"Continue reading »\"}],\"SummarySpotlight.SelectedStories\":[{\"type\":0,\"value\":\"Selected Stories\"}],\"TagCloud.SectionHeader\":[{\"type\":0,\"value\":\"Topics\"}],\"TagPage.ClearAll\":[{\"type\":0,\"value\":\"Clear All\"}],\"TagPage.FilterDrawer\":[{\"type\":0,\"value\":\"matching results\"}],\"TagPage.FilterDrawer.FilterApplyActionButton\":[{\"type\":0,\"value\":\"Apply\"}],\"TagPage.FilterDrawer.FilterCancelActionButton\":[{\"type\":0,\"value\":\"Cancel\"}],\"TagPage.FilterResults\":[{\"type\":0,\"value\":\"Filter Results\"}],\"TagPage.Items\":[{\"type\":0,\"value\":\"items\"}],\"TagPage.Loading\":[{\"type\":0,\"value\":\"Loading ...\"}],\"TagPage.MoreStories\":[{\"type\":0,\"value\":\"More Stories\"}],\"TagPage.NoResultsFound\":[{\"type\":0,\"value\":\"Sorry we can't display any results for those filtering options, please try again\"}],\"TagPage.SortBy\":[{\"type\":0,\"value\":\"Sort By\"}],\"TextField.MultiLineErrorText\":[{\"type\":0,\"value\":\"Use at least _MIN_ characters and a maximum of _MAX_.\"}],\"TextField.multiLineUpperLimitErrorText\":[{\"type\":0,\"value\":\"Use maximum of _MAX_ characters only\"}],\"ThreadsEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Threads content\"}],\"TiktokEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"TikTok content\"}],\"TopStory.toutHead\":[{\"children\":[{\"type\":0,\"value\":\"Also today . . .\"}],\"type\":8,\"value\":\"i\"}],\"TwitterEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"X content\"}],\"TwitterEmbed.EmbedContainer\":[{\"type\":0,\"value\":\"social media post\"}],\"UnifiedProductCard.DefaultOfferCtaText\":[{\"type\":0,\"value\":\"Shop Now\"}],\"UnifiedProductCard.UnifiedProductCardConsLabel\":[{\"type\":0,\"value\":\"Cons\"}],\"UnifiedProductCard.UnifiedProductCardProsConsTitle\":[{\"type\":0,\"value\":\"Pros & Cons\"}],\"UnifiedProductCard.UnifiedProductCardProsLabel\":[{\"type\":0,\"value\":\"Pros\"}],\"UnifiedProductCard.UnifiedProductCardRatingTitle\":[{\"type\":0,\"value\":\"OUR RATING:\"}],\"UserNameModal.CloseButtonLabel\":[{\"type\":0,\"value\":\"Close User Name\"}],\"UserNameModal.Dek\":[{\"type\":0,\"value\":\"Your username will appear next to any recipe notes and replies you add.\"}],\"UserNameModal.ErrorMessage\":[{\"type\":0,\"value\":\"Unable to save username. Please try again.\"}],\"UserNameModal.Hed\":[{\"type\":0,\"value\":\"Create Username\"}],\"UserNameModal.SubmitButtonLabel\":[{\"type\":0,\"value\":\"Save Username\"}],\"UserNameModal.SuccessMessage\":[{\"type\":0,\"value\":\"Your username is saved.\"}],\"UserNameModal.UserNameModalAssistiveText\":[{\"type\":0,\"value\":\"Usernames must be between 2 and 23 characters and can only include letters, numbers and underscores (_).\"}],\"UserNameModal.alreadyTakenError\":[{\"type\":0,\"value\":\"This Username is already taken.\"}],\"UserNameModal.lengthError\":[{\"type\":0,\"value\":\"Usernames must be between 2 and 23 characters.\"}],\"UserNameModal.specialCharError\":[{\"type\":0,\"value\":\"Usernames can only include letters, numbers and underscores (_).\"}],\"UserProfileForm.countryFieldLabel\":[{\"type\":0,\"value\":\"Country\"}],\"UserProfileForm.countryRequiredErrorMsg\":[{\"type\":0,\"value\":\"Select your country.\"}],\"UserProfileForm.firstNameFieldLabel\":[{\"type\":0,\"value\":\"First name\"}],\"UserProfileForm.firstNameRequiredErrorMsg\":[{\"type\":0,\"value\":\"Enter your first name.\"}],\"UserProfileForm.lastNameFieldLabel\":[{\"type\":0,\"value\":\"Last name\"}],\"UserProfileForm.lastNameRequiredErrorMsg\":[{\"type\":0,\"value\":\"Enter your last name.\"}],\"UserProfileForm.phoneNumberFieldLabel\":[{\"type\":0,\"value\":\"Phone number\"}],\"UserProfileForm.phoneNumberLengthErrorMsg\":[{\"type\":0,\"value\":\"Your phone number should be ten digits.\"}],\"UserProfileForm.phoneNumberNonUSLengthErrorMsg\":[{\"type\":0,\"value\":\"Your phone number should be at least ten digits.\"}],\"UserProfileForm.phoneNumberNumericErrorMsg\":[{\"type\":0,\"value\":\"Your phone number should consist of only numerical digits.\"}],\"UserProfileForm.phoneNumberRequiredErrorMsg\":[{\"type\":0,\"value\":\"Enter your phone number.\"}],\"UserProfileForm.serverErrorMessageText\":[{\"type\":0,\"value\":\"There was a problem saving your information. Please try again.\"}],\"UserProfileForm.stateFieldLabel\":[{\"type\":0,\"value\":\"State \\u002F Province\"}],\"UserProfileForm.stateRequiredErrorMsg\":[{\"type\":0,\"value\":\"Select your state \\u002F province.\"}],\"UserProfileForm.submitButtonLabel\":[{\"type\":0,\"value\":\"Save\"}],\"UserProfileForm.termsAndConditionText\":[{\"type\":0,\"value\":\"By submitting a caption, you agree to our \"},{\"children\":[{\"type\":0,\"value\":\"User Agreement\"}],\"type\":8,\"value\":\"a\"},{\"type\":0,\"value\":\" and \"},{\"children\":[{\"type\":0,\"value\":\"Privacy Policy & Cookie Statement.\"}],\"type\":8,\"value\":\"b\"}],\"UtilityNavigation.AccountDropdown\":[{\"type\":0,\"value\":\"Account\"}],\"UtilityNavigation.AccountDropdownAssistive\":[{\"type\":0,\"value\":\"Account Navigation\"}],\"UtilityNavigation.MarketSwitcherLabel\":[{\"type\":0,\"value\":\"Country\"}],\"UtilityNavigation.SearchLabel\":[{\"type\":0,\"value\":\"Search\"}],\"UtilityNavigation.ShoppingCartAriaLabel\":[{\"type\":0,\"value\":\"item(s) in Cart\"}],\"UtilityNavigation.ShoppingCartLabel\":[{\"type\":0,\"value\":\"Shopping Cart\"}],\"UtilityNavigation.SignInLabel\":[{\"type\":0,\"value\":\"Sign In\"}],\"UtilityNavigation.SignOut\":[{\"type\":0,\"value\":\"Sign Out\"}],\"UtilityNavigation.UtilityNavigationButton\":[{\"type\":0,\"value\":\"Open Navigation Menu\"}],\"UtilityValidationDescription.Heading\":[{\"type\":0,\"value\":\"Errors\"}],\"VenuePage.BylinePreamble\":[{\"type\":0,\"value\":\"Reviewed by\"}],\"VenuePage.DefaultCTAText\":[{\"type\":0,\"value\":\"Book Now\"}],\"VenuePage.FilterableListHed\":[{\"type\":0,\"value\":\"More To Discover\"}],\"VenuePage.SectionTitleHed\":[{\"type\":0,\"value\":\"Photos\"}],\"VenuePage.VenueSellerPreviewText\":[{\"type\":0,\"value\":\"Powered By:\"}],\"VersoCommerceCollectionCurated.FilterBy.Brand\":[{\"type\":0,\"value\":\"Designer\"}],\"VersoCommerceCollectionCurated.FilterBy.Category\":[{\"type\":0,\"value\":\"Type\"}],\"VersoCommerceCollectionCurated.FilterBy.Color\":[{\"type\":0,\"value\":\"Color\"}],\"VersoCommerceCollectionCurated.FilterBy.Size\":[{\"type\":0,\"value\":\"Size\"}],\"VersoCommerceCollectionCurated.FilterBy.StorefrontBundle\":[{\"type\":0,\"value\":\"Category\"}],\"VersoCommerceCollectionCurated.FilterBy.Type\":[{\"type\":0,\"value\":\"Type\"}],\"VersoCommerceCollectionCurated.Items\":[{\"type\":0,\"value\":\"Items\"}],\"VersoCommerceCollectionCurated.SortBy.Featured\":[{\"type\":0,\"value\":\"Featured\"}],\"VersoCommerceCollectionCurated.SortBy.HighestPrice\":[{\"type\":0,\"value\":\"Highest Price\"}],\"VersoCommerceCollectionCurated.SortBy.LowestPrice\":[{\"type\":0,\"value\":\"Lowest Price\"}],\"VersoCommerceCollectionCurated.SortBy.MostRecent\":[{\"type\":0,\"value\":\"New Arrivals\"}],\"VersoCommerceCollectionCurated.SortBy.Popular\":[{\"type\":0,\"value\":\"Most Wanted\"}],\"VersoFeatures.viewAllButton\":[{\"type\":0,\"value\":\"View All\"}],\"VersoFilterableSummaryList.CTAMessage\":[{\"type\":0,\"value\":\"See more \"},{\"type\":1,\"value\":\"groupName\"},{\"type\":0,\"value\":\" recipes\"}],\"VersoIssueFeature.IssueFeatureLabel\":[{\"type\":0,\"value\":\"Table of Contents »\"}],\"VideoWrapper.headerText\":[{\"type\":0,\"value\":\"WATCH\"}],\"VideoWrapper.headerTextRelatedOverride\":[{\"type\":0,\"value\":\"Featured Video\"}],\"VideoWrapper.moreLink\":[{\"type\":0,\"value\":\"More \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" Videos\"}],\"venueFeatureList.venueBar\":[{\"type\":0,\"value\":\"Bar\"}],\"venueFeatureList.venueDetox\":[{\"type\":0,\"value\":\"Detox\"}],\"venueFeatureList.venueGolf\":[{\"type\":0,\"value\":\"Golf\"}],\"venueFeatureList.venueLocationMap\":[{\"type\":0,\"value\":\"Location Map\"}],\"venueFeatureList.venueSpa\":[{\"type\":0,\"value\":\"Spa\"}],\"venueFeatureList.venueWifi\":[{\"type\":0,\"value\":\"Wifi\"}],\"venueFeaturesList.venueAmenities\":[{\"type\":0,\"value\":\"Amenities\"}],\"venueFeaturesList.venueBeachName\":[{\"type\":0,\"value\":\"Beach\"}],\"venueFeaturesList.venueBookNow\":[{\"type\":0,\"value\":\"Book Now\"}],\"venueFeaturesList.venueBusinessName\":[{\"type\":0,\"value\":\"Business\"}],\"venueFeaturesList.venueFamily\":[{\"type\":0,\"value\":\"Family\"}],\"venueFeaturesList.venueFreeWifi\":[{\"type\":0,\"value\":\"Free Wifi\"}],\"venueFeaturesList.venueGym\":[{\"type\":0,\"value\":\"Gym\"}],\"venueFeaturesList.venueHolistic\":[{\"type\":0,\"value\":\"Holistic\"}],\"venueFeaturesList.venueKidsProgram\":[{\"type\":0,\"value\":\"Kids Program\"}],\"venueFeaturesList.venueMovementFitness\":[{\"type\":0,\"value\":\"Movement Fitness\"}],\"venueFeaturesList.venuePool\":[{\"type\":0,\"value\":\"Pool\"}],\"venueFeaturesList.venueRoomName\":[{\"type\":0,\"value\":\"Rooms\"}],\"venueFeaturesList.venueSki\":[{\"type\":0,\"value\":\"Ski\"}],\"venuePage.proximityNearby\":[{\"type\":0,\"value\":\"PLACES YOU CAN VISIT\"}],\"venuePage.proximityTravel\":[{\"type\":0,\"value\":\"HOW TO REACH\"}],\"venuePage.venueContactName\":[{\"type\":0,\"value\":\"Contact\"}]},\"consentEnabled\":true,\"brandIdentityAssets\":{\"favicon\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fverso\\u002Fstatic\\u002Fthe-new-yorker\\u002Fassets\\u002Ffavicon.ico\",\"signInModalAsset\":{\"default\":{\"altText\":\"Sign In\",\"sources\":{\"sm\":{\"url\":\"\\u002Fverso\\u002Fstatic\\u002Fthe-new-yorker\\u002Fassets\\u002Fsign-in-modal.png\"}}},\"crosswords\":{\"altText\":\"Sign In\",\"sources\":{\"sm\":{\"url\":\"\\u002Fverso\\u002Fstatic\\u002Fthe-new-yorker\\u002Fassets\\u002Fsign-in-modal-crosswords.png\"}}},\"voting\":{\"altText\":\"Sign In\",\"sources\":{\"sm\":{\"url\":\"\\u002Fverso\\u002Fstatic\\u002Fthe-new-yorker\\u002Fassets\\u002Fsign-in-modal-voting.png\"}}}}},\"useTrailingSlash\":false,\"globalMessage\":null,\"fingerprint\":{\"isEnabled\":true,\"publicKey\":\"RnzaPD7aVNbZwmkfojeZ\",\"cookieExpire\":86400000},\"fastly\":{\"response\":{\"headers\":{\"model-name\":\"article\",\"modified-at\":1698424178,\"surrogate-control\":\"stale-while-revalidate=60, stale-if-error=86400, max-age=14400\"}}},\"google\":{\"isSwgEnabledOnTenantChannel\":false,\"isSwgEnabledOnSystem\":false,\"registrationSourceCode\":\"\",\"swgPublicationId\":\"\",\"swgSku\":\"\",\"entitlement\":{\"processEntitlementResponse\":false}},\"head.canonicalUrl\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2015\\u002F11\\u002F23\\u002Fdoomsday-invention-artificial-intelligence-nick-bostrom\",\"head.hreflang\":[],\"head.description\":\"Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.\",\"head.keywords\":\"artificial intelligence (a.i.),category_science_tech,philosophers\",\"head.og.type\":\"article\",\"head.robots\":\"index, follow, noarchive, max-image-preview:large\",\"head.title\":\"The Philosopher of Doomsday | The New Yorker\",\"tenant\":\"the-new-yorker\",\"head.social.title\":\"The Doomsday Invention\",\"head.social.description\":\"Will artificial intelligence bring us utopia or destruction?\",\"head.promo.dek\":\"Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.\",\"head.social.opinion\":\"false\",\"head.jsonld\":[{\"@context\":\"http:\\u002F\\u002Fschema.org\",\"@type\":\"NewsArticle\",\"articleBody\":\"I. Omens\\nLast year, a curious nonfiction book became a Times best-seller: a dense meditation on artificial intelligence by the philosopher Nick Bostrom, who holds an appointment at Oxford. Titled “Superintelligence: Paths, Dangers, Strategies,” it argues that true artificial intelligence, if it is realized, might pose a danger that exceeds every previous threat from technology—even nuclear weapons—and that if its development is not managed carefully humanity risks engineering its own extinction. Central to this concern is the prospect of an “intelligence explosion,” a speculative event in which an A.I. gains the ability to improve itself, and in short order exceeds the intellectual potential of the human brain by many orders of magnitude.\\nSuch a system would effectively be a new kind of life, and Bostrom’s fears, in their simplest form, are evolutionary: that humanity will unexpectedly become outmatched by a smarter competitor. He sometimes notes, as a point of comparison, the trajectories of people and gorillas: both primates, but with one species dominating the planet and the other at the edge of annihilation. “Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb,” he concludes. “We have little idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound.”\\nAt the age of forty-two, Bostrom has become a philosopher of remarkable influence. “Superintelligence” is only his most visible response to ideas that he encountered two decades ago, when he became a transhumanist, joining a fractious quasi-utopian movement united by the expectation that accelerating advances in technology will result in drastic changes—social, economic, and, most strikingly, biological—which could converge at a moment of epochal transformation known as the Singularity. Bostrom is arguably the leading transhumanist philosopher today, a position achieved by bringing order to ideas that might otherwise never have survived outside the half-crazy Internet ecosystem where they formed. He rarely makes concrete predictions, but, by relying on probability theory, he seeks to tease out insights where insights seem impossible.\\nSome of Bostrom’s cleverest arguments resemble Swiss Army knives: they are simple, toylike, a pleasure to consider, with colorful exteriors and precisely calibrated mechanics. He once cast a moral case for medically engineered immortality as a fable about a kingdom terrorized by an insatiable dragon. A reformulation of Pascal’s wager became a dialogue between the seventeenth-­century philosopher and a mugger from another dimension.\\n“Superintelligence” is not intended as a treatise of deep originality; Bostrom’s contribution is to impose the rigors of analytic philosophy on a messy corpus of ideas that emerged at the margins of academic thought. Perhaps because the field of A.I. has recently made striking advances—with everyday technology seeming, more and more, to exhibit something like intelligent reasoning—the book has struck a nerve. Bostrom’s supporters compare it to “Silent Spring.” In moral philosophy, Peter Singer and Derek Parfit have received it as a work of importance, and distinguished physicists such as Stephen Hawking have echoed its warning. Within the high caste of Silicon Valley, Bostrom has acquired the status of a sage. Elon Musk, the C.E.O. of Tesla, promoted the book on Twitter, noting, “We need to be super careful with AI. Potentially more dangerous than nukes.” Bill Gates recommended it, too. Suggesting that an A.I. could threaten humanity, he said, during a talk in China, “When people say it’s not a problem, then I really start to get to a point of disagreement. How can they not see what a huge challenge this is?”\\nThe people who say that artificial intelligence is not a problem tend to work in artificial intelligence. Many prominent researchers regard Bostrom’s basic views as implausible, or as a distraction from the near-term benefits and moral dilemmas posed by the technology—not least because A.I. systems today can barely guide robots to open doors. Last summer, Oren Etzioni, the C.E.O. of the Allen Institute for Artificial Intelligence, in Seattle, referred to the fear of machine intelligence as a “Frankenstein complex.” Another leading researcher declared, “I don’t worry about that for the same reason I don’t worry about overpopulation on Mars.” Jaron Lanier, a Microsoft researcher and tech commentator, told me that even framing the differing views as a debate was a mistake. “This is not an honest conversation,” he said. “People think it is about technology, but it is really about religion, people turning to metaphysics to cope with the human condition. They have a way of dramatizing their beliefs with an end-of-days scenario—and one does not want to criticize other people’s religions.”\\nBecause the argument has played out on blogs and in the popular press, beyond the ambit of peer-reviewed journals, the two sides have appeared in caricature, with headlines suggesting either doom (“Will Super-intelligent Machines Kill Us All?”) or a reprieve from doom (“Artificial intelligence ‘will not end human race’ ”). Even the most grounded version of the debate occupies philosophical terrain where little is clear. But, Bostrom argues, if artificial intelligence can be achieved it would be an event of unparalleled consequence—perhaps even a rupture in the fabric of history. A bit of long-range forethought might be a moral obligation to our own species.\\nBostrom’s sole responsibility at Oxford is to direct an organization called the Future of Humanity Institute, which he founded ten years ago, with financial support from James Martin, a futurist and tech millionaire. Bostrom runs the institute as a kind of philosophical radar station: a bunker sending out navigational pulses into the haze of possible futures. Not long ago, an F.H.I. fellow studied the possibility of a “dark fire scenario,” a cosmic event that, he hypothesized, could occur under certain high-energy conditions: everyday matter mutating into dark matter, in a runaway process that could erase most of the known universe. (He concluded that it was highly unlikely.) Discussions at F.H.I. range from conventional philosophic topics, like the nature of compromise, to the optimal structure of space empires—whether a single intergalactic machine intelligence, supported by a vast array of probes, presents a more ethical future than a cosmic imperium housing millions of digital minds.\\nEarlier this year, I visited the institute, which is situated on a winding street in a part of Oxford that is a thousand years old. It takes some work to catch Bostrom at his office. Demand for him on the lecture circuit is high; he travels overseas nearly every month to relay his technological omens in a range of settings, from Google’s headquarters to a Presidential commission in Washington. Even at Oxford, he maintains an idiosyncratic schedule, remaining in the office until two in the morning and returning sometime the next afternoon.\\nI arrived before he did, and waited in a hallway between two conference rooms. A plaque indicated that one of them was the Arkhipov Room, honoring Vasili Arkhipov, a Soviet naval officer. During the Cuban missile crisis, Arkhipov was serving on a submarine in the Caribbean when U.S. destroyers set off depth charges nearby. His captain, unable to establish radio contact with Moscow, feared that the conflict had escalated and ordered a nuclear strike. But Arkhipov dissuaded him, and all-out atomic war was averted. Across the hallway was the Petrov Room, named for another Soviet officer who prevented a global nuclear catastrophe. Bostrom later told me, “They may have saved more lives than most of the statesmen we celebrate on stamps.”\\nThe sense that a vanguard of technical-minded people working in obscurity, at odds with consensus, might save the world from auto-annihilation runs through the atmosphere at F.H.I. like an electrical charge. While waiting for Bostrom, I peered through a row of windows into the Arkh­ipov Room, which looked as though it was used for both meetings and storage; on a bookcase there were boxes containing light bulbs, lampshades, cables, spare mugs. A gaunt philosophy Ph.D. wrapped in a thick knitted cardigan was pacing in front of a whiteboard covered in notation, which he attacked in bursts. After each paroxysm, he paced, hands behind his back, head tilted downward. At one point, he erased a panel of his work. Taking this as an opportunity to interrupt, I asked him what he was doing. “It is a problem involving an aspect of A.I. called ‘planning,’ ” he said. His demeanor radiated irritation. I left him alone.\\nBostrom arrived at 2 p.m. He has a boyish countenance and the lean, vital physique of a yoga instructor—though he could never be mistaken for a yoga instructor. His intensity is too untidily contained, evident in his harried gait on the streets outside his office (he does not drive), in his voracious consumption of audiobooks (played at two or three times the normal speed, to maximize efficiency), and his fastidious guarding against illnesses (he avoids handshakes and wipes down silverware beneath a tablecloth). Bostrom can be stubborn about the placement of an office plant or the choice of a font. But when his arguments are challenged he listens attentively, the mechanics of consideration nearly dis­cernible beneath his skin. Then, calmly, quickly, he dispatches a response, one idea interlocked with another.\\nHe asked if I wanted to go to the market. “You can watch me make my elixir,” he said. For the past year or so, he has been drinking his lunch (another efficiency): a smoothie containing fruits, vegetables, proteins, and fats. Using his elbow, he hit a button that electronically opened the front door. Then we rushed out.\\nBostrom has a reinvented man’s sense of lost time. An only child, he grew up—as Niklas Boström—in Helsingborg, on the southern coast of Sweden. Like many exceptionally bright children, he hated school, and as a teen-ager he developed a listless, romantic persona. In 1989, he wandered into a library and stumbled onto an anthology of nineteenth-century German philosophy, containing works by Nietzsche and Schopenhauer. He read it in a nearby forest, in a clearing that he often visited to think and to write poetry, and experienced a euphoric insight into the possibilities of learning and achievement. “It’s hard to convey in words what that was like,” Bostrom told me; instead he sent me a photograph of an oil painting that he had made shortly afterward. It was a semi-representational landscape, with strange figures crammed into dense undergrowth; beyond, a hawk soared below a radiant sun. He titled it “The First Day.”\\nDeciding that he had squandered his early life, he threw himself into a campaign of self-education. He ran down the citations in the anthology, branching out into art, literature, science. He says that he was motivated not only by curiosity but also by a desire for actionable knowledge about how to live. To his parents’ dismay, Bostrom insisted on finishing his final year of high school from home by taking special exams, which he completed in ten weeks. He grew distant from old friends: “I became quite fanatical and felt quite isolated for a period of time.”\\nWhen Bostrom was a graduate student in Stockholm, he studied the work of the analytic philosopher W. V. Quine, who had explored the difficult relationship between language and reality. His adviser drilled precision into him by scribbling “not clear” throughout the margins of his papers. “It was basically his only feedback,” Bostrom told me. “The effect was still, I think, beneficial.” His previous academic interests had ranged from psychology to mathematics; now he took up theoretical physics. He was fascinated by technology. The World Wide Web was just emerging, and he began to sense that the heroic philosophy which had inspired him might be outmoded. In 1995, Bostrom wrote a poem, “Requiem,” which he told me was “a signing-off letter to an earlier self.” It was in Swedish, so he offered me a synopsis: “I describe a brave general who has overslept and finds his troops have left the encampment. He rides off to catch up with them, pushing his horse to the limit. Then he hears the thunder of a modern jet plane streaking past him across the sky, and he realizes that he is obsolete, and that courage and spiritual nobility are no match for machines.”\\nAlthough Bostrom did not know it, a growing number of people around the world shared his intuition that technology could cause transformative change, and they were finding one another in an online discussion group administered by an organization in California called the Extropy Institute. The term “extropy,” coined in 1967, is generally used to describe life’s capacity to reverse the spread of entropy across space and time. Extropianism is a libertarian strain of transhumanism that seeks “to direct human evolution,” hoping to eliminate disease, suffering, even death; the means might be genetic modification, or as yet un­invented nanotechnology, or perhaps dispensing with the body entirely and uploading minds into supercomputers. (As one member noted, “Immortality is mathematical, not mystical.”) The Extropians advocated the development of artificial superintelligence to achieve these goals, and they envisioned humanity colonizing the universe, converting inert matter into engines of civilization. The discussions were nerdy, lunatic, imaginative, thought-provoking. Anders Sandberg, a former member of the group who now works at Bostrom’s institute, told me, “Just imagine if you could listen in on the debates of the Italian Futurists or early Surrealists.”\\nIn 1996, while pursuing further graduate work at the London School of Economics, Bostrom learned about the Extropy discussion group and became an active participant. A year later, he co-founded his own organization, the World Transhumanist Association, which was less libertarian and more academically spirited. He crafted approachable statements on transhumanist values and gave interviews to the BBC. The line between his academic work and his activism blurred: his Ph.D. dissertation centered on a study of the Doomsday Argument, which uses probability theory to make inferences about the longevity of human civilization. The work baffled his advisers, who respected him but rarely agreed with his conclusions. Mostly, they left him alone.\\nBostrom had little interest in conventional philosophy—not least because he expected that superintelligent minds, whether biologically enhanced or digital, would make it obsolete. “Suppose you had to build a new subway line, and it was this grand trans-generational enterprise that humanity was engaged in, and everybody had a little role,” he told me. “So you have a little shovel. But if you know that a giant bulldozer will arrive on the scene tomorrow, then does it really make sense to spend your time today digging the big hole with your shovel? Maybe there is something else you could do with your time. Maybe you could put up a signpost for the great shovel, so it will start digging in the right place.” He came to believe that a key role of the philosopher in modern society was to acquire the knowledge of a polymath, then use it to help guide humanity to its next phase of existence—a discipline that he called “the philosophy of technological prediction.” He was trying to become such a seer.\\n“He was ultra-consistent,” Daniel Hill, a British philosopher who befriended Bostrom while they were graduate students in London, told me. “His interest in science was a natural outgrowing of his understandable desire to live forever, basically.”\\nBostrom has written more than a hundred articles, and his longing for immortality can be seen throughout. In 2008, he framed an essay as a call to action from a future utopia. “Death is not one but a multitude of assassins,” he warned. “Take aim at the causes of early death—infection, violence, malnutrition, heart attack, cancer. Turn your biggest gun on aging, and fire. You must seize the biochemical processes in your body in order to vanquish, by and by, illness and senescence. In time, you will discover ways to move your mind to more durable media.” He tends to see the mind as immaculate code, the body as inefficient hardware—able to accommodate limited hacks but probably destined for replacement.\\nEven Bostrom’s marriage is largely mediated by technology. His wife, Susan, has a Ph.D. in the sociology of medicine and a bright, down-to-earth manner. (“She teases me about the Terminator and the robot army,” he told me.) They met thirteen years ago, and for all but six months they have lived on opposite sides of the Atlantic, even after the recent birth of their son. The arrangement is voluntary: she prefers Montreal; his work keeps him at Oxford. They Skype several times a day, and he directs as much international travel as possible through Canada, so they can meet in non-digital form.\\nIn Oxford, as Bostrom shopped for his smoothie, he pointed out a man vaping. “There is also the more old-school method of taking nicotine: chewing gum,” he told me. “I do chew nicotine gum. I read a few papers saying it might have some nootropic effect”—that is, it might enhance cognition. He drinks coffee, and usually abstains from alcohol. He briefly experimented with the smart drug Modafinil, but gave it up.\\nBack at the institute, he filled an industrial blender with lettuce, carrots, cauliflower, broccoli, blueberries, turmeric, vanilla, oat milk, and whey powder. “If there is one thing Nick cares about, it is minds,” Sandberg told me. “That is at the root of many of his views about food, because he is worried that toxin X or Y might be bad for his brain.” He suspects that Bostrom also enjoys the ritualistic display. “Swedes are known for their smugness,” he joked. “Perhaps Nick is subsisting on smugness.”\\nA young employee eyed Bostrom getting ready to fire up the blender. “I can tell when Nick comes into the office,” he said. “My hair starts shaking.”\\n“Yeah, this has got three horsepower,” Bostrom said. He ran the blender, producing a noise like a circular saw, and then filled a tall glass stein with purple-­green liquid. We headed to his office, which was meticulous. By a window was a wooden desk supporting an iMac and not another item; against a wall were a chair and a cabinet with a stack of documents. The only hint of excess was light: there were fourteen lamps.\\nIt is hard to spend time at Bostrom’s institute without drifting into reveries of a far future. What might humanity look like millions of years from now? The upper limit of survival on Earth is fixed to the life span of the sun, which in five billion years will become a red giant and swell to more than two hundred times its present size. It is possible that Earth’s orbit will adjust, but more likely that the planet will be destroyed. In any case, long before then, nearly all plant life will die, the oceans will boil, and the Earth’s crust will heat to a thousand degrees. In half a billion years, the planet will be uninhabitable.\\nThe view of the future from Bostrom’s office can be divided into three grand panoramas. In one, humanity experiences an evolutionary leap—either assisted by technology or by merging into it and becoming software—to achieve a sublime condition that Bostrom calls “posthumanity.” Death is overcome, mental experience expands beyond recognition, and our descendants colonize the universe. In another panorama, humanity becomes extinct or experiences a disaster so great that it is unable to recover. Between these extremes, Bostrom envisions scenarios that resemble the status quo—people living as they do now, forever mired in the “human era.” It’s a vision familiar to fans of sci-fi: on “Star Trek,” Captain Kirk was born in the year 2233, but when an alien portal hurls him through time and space to Depression-era Manhattan he blends in easily.\\nBostrom dislikes science fiction. “I’ve never been keen on stories that just try to present ‘wow’ ideas—the equivalent of movie productions that rely on stunts and explosions to hold the attention,” he told me. “The question is not whether we can think of something radical or extreme but whether we can discover some sufficient reason for updating our credence function.”\\nHe believes that the future can be studied with the same meticulousness as the past, even if the conclusions are far less firm. “It may be highly unpredictable where a traveller will be one hour after the start of her journey, yet predictable that after five hours she will be at her destination,” he once argued. “The very long-term future of humanity may be relatively easy to predict.” He offers an example: if history were reset, the industrial revolution might occur at a different time, or in a different place, or perhaps not at all, with innovation instead occurring in increments over hundreds of years. In the short term, predicting technological achievements in the counter-history might not be possible; but after, say, a hundred thousand years it is easier to imagine that all the same inventions would have emerged.\\nBostrom calls this the Technological Completion Conjecture: “If scientific- and technological-development efforts do not effectively cease, then all impor­t­­­ant basic capabilities that could be obtained through some possible technology will be obtained.” In light of this, he suspects that the farther into the future one looks the less likely it seems that life will continue as it is. He favors the far ends of possibility: humanity becomes transcendent or it perishes.\\nIn the nineteen-nineties, as these ideas crystallized in his thinking, Bostrom began to give more attention to the question of extinction. He did not believe that doomsday was imminent. His interest was in risk, like an insurance agent’s. No matter how improbable extinction may be, Bostrom argues, its consequences are near-infinitely bad; thus, even the tiniest step toward reducing the chance that it will happen is near-­infinitely valuable. At times, he uses arithmetical sketches to illustrate this point. Imagining one of his utopian scenarios—trillions of digital minds thriving across the cosmos—he reasons that, if there is even a one-per-cent chance of this happening, the expected value of reducing an existential threat by a billionth of a billionth of one per cent would be worth a hundred billion times the value of a billion present-day lives. Put more simply: he believes that his work could dwarf the moral importance of anything else.\\nBostrom introduced the philosophical concept of “existential risk” in 2002, in the Journal of Evolution and Technology. In recent years, new organizations have been founded almost annually to help reduce it—among them the Centre for the Study of Existential Risk, affiliated with Cambridge Uni­versity, and the Future of Life Institute, which has ties to the Massachusetts Institute of Technology. All of them face a key problem: Homo sapiens, since its emergence two hundred thousand years ago, has proved to be remarkably resilient, and figuring out what might imperil its existence is not obvious. Climate change is likely to cause vast environmental and economic damage—but it does not seem impossible to survive. So-called super-volcanoes have thus far not threatened the perpetuation of the species. NASA spends forty million dollars each year to determine if there are significant comets or asteroids headed for Earth. (There aren’t.)\\nBostrom does not find the lack of obvious existential threats comforting. Because it is impossible to endure extinction twice, he argues, we cannot rely on history to calculate the probability that it will occur. The most worrying dangers are those that Earth has never encountered before. “It is hard to cause human extinction with seventeenth-century technology,” Bostrom told me. Three centuries later, though, the prospect of a technological apocalypse was urgently plausible. Bostrom dates the first scientific analysis of existential risk to the Manhattan Project: in 1942, Robert Oppenheimer became concerned that an atomic detonation of sufficient power could cause the entire atmosphere to ignite. A subsequent study concluded that the scenario was “unreasonable,” given the limitations of the weapons then in development. But even if the great nuclear nightmares of the Cold War did not come true, the tools were there to cause destruction on a scale not previously possible. As innovations grow even more complex, it is increasingly difficult to evaluate the dangers ahead. The answers must be fraught with ambiguity, because they can be derived only by predicting the effects of technologies that exist mostly as theories or, even more indirectly, by using abstract reasoning.\\nAs a philosopher, Bostrom takes a sweeping, even cosmic, view of such problems. One afternoon, he told me, “The probabilities that any given planet will produce intelligent life—this may also have action-relevant information.” In the past several years, NASA probes have found increasing evidence that the building blocks of life are abundant throughout space. So much water has been discovered—on Mars and on the moons of Jupiter and Saturn—that one scientist described our solar system as “a pretty soggy place.” There are amino acids on icy comets and complex organic molecules in distant star-forming clouds. On this planet, life has proved capable of thriving in unimaginably punishing conditions: without oxygen, without light, at four hundred degrees above or below zero. In 2007, the European Space Agency hitched tiny creatures to the exterior of a satellite. They not only survived the flight; some even laid eggs afterward.\\nWith ten billion Earth-like planets in our galaxy alone, and a hundred billion galaxies in the universe, there is good reason to suspect that extraterrestrial life may one day be discovered. For Bostrom, this would augur disaster. “It would be great news to find that Mars is a completely sterile planet,” he argued not long ago. “Dead rocks and lifeless sands would lift my spirits.” His reasoning begins with the age of the universe. Many of those Earth-like planets are thought to be far, far older than ours. One that was recently discovered, called Kepler 452b, is as much as one and a half billion years older. Bostrom asks: If life had formed there on a time scale resembling our own, what would it look like? What kind of technological progress could a civilization achieve with a head start of hundreds of millions of years?\\nLife as we know it tends to spread wherever it can, and Bostrom estimates that, if an alien civilization could design space probes capable of travelling at even one per cent of the speed of light, the entire Milky Way could be colonized in twenty million years—a tiny fraction of the age difference between Kepler 452b and Earth. One could argue that no technology will ever propel ships at so great a speed. Or perhaps millions of alien civilizations possess the know-how for intergalactic travel, but they aren’t interested. Even so, because the universe is so colossal, and because it is so old, only a small number of civilizations would need to behave as life does on Earth—unceasingly expanding—in order to be visible. Yet, as Bostrom notes, “You start with billions and billions of potential germination points for life, and you end up with a sum total of zero alien civilizations that developed technologically to the point where they become manifest to us earthly observers. So what’s stopping them?”\\nIn 1950, Enrico Fermi sketched a version of this paradox during a lunch break while he was working on the H-bomb, at Los Alamos. Since then, many resolutions have been proposed—some of them exotic, such as the idea that Earth is housed in an interplanetary alien zoo. Bostrom suspects that the answer is simple: space appears to be devoid of life because it is. This implies that intelligent life on Earth is an astronomically rare accident. But, if so, when did that accident occur? Was it in the first chemical reactions in the primordial soup? Or when single-celled organisms began to replicate using DNA? Or when animals learned to use tools? Bos­trom likes to think of these hurdles as Great Filters: key phases of improbability that life everywhere must pass through in order to develop into intelligent species. Those which do not make it either go extinct or fail to evolve.\\nThus, for Bostrom, the discovery of a single-celled creature inhabiting a damp stretch of Martian soil would constitute a disconcerting piece of evidence. If two planets independently evolved primitive organisms, then it seems more likely that this type of life can be found on many planets throughout the universe. Bostrom reasons that this would suggest that the Great Filter comes at some later evolutionary stage. The discovery of a fossilized vertebrate would be even worse: it would suggest that the universe appears lifeless not because complex life is unusual but, rather, because it is always somehow thwarted before it becomes advanced enough to colonize space.\\nIn Bostrom’s view, the most distressing possibility is that the Great Filter is ahead of us—that evolution frequently achieves civilizations like our own, but they perish before reaching their technological maturity. Why might that be? “Natural disasters such as asteroid hits and super-­volcanic eruptions are unlikely Great Filter candidates, because, even if they destroyed a significant number of civilizations, we would expect some civilizations to get lucky and escape disaster,” he argues. “Perhaps the most likely type of existential risks that could constitute a Great Filter are those that arise from technological discovery. It is not far-fetched to suppose that there might be some possible technology which is such that (a) virtually all suffi­ciently advanced civilizations eventually discover it and (b) its discovery leads almost universally to existential disaster.”\\nII. The Machines\\nThe field of artificial intelligence was born in a fit of scientific optimism, in 1955, when a small group of researchers—three mathematicians and an I.B.M. programmer—drew up a proposal for a project at Dartmouth. “An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves,” they stated. “We think a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.”\\nTheir optimism was understandable. Since the turn of the twentieth century, science had been advancing at a breakneck pace: the discovery of radioactivity quickly led to insights into the inner workings of the atom, and then to the development of controlled nuclear energy, and then to the warheads over Hiroshima and Nagasaki, and then to the H-bomb. This rush of discovery was reflected in fiction, too, in the work of Isaac Asimov, among others, who envisioned advanced civilizations inhabited by intelligent robots (each encoded with simple, ethical Laws of Robotics, to prevent it from causing harm). The year the scientists met at Dartmouth, Asimov published “The Last Question,” a story featuring a superintelligent A.I. that is continually “self-adjusting and self-correcting”—gaining knowledge as it helps human civilization expand throughout the universe. When the universe’s last stars start dying out, all humanity uploads itself into the A.I., and the device, achieving godhood, creates a new cosmos.\\nScientists perceived the mechanics of intelligence—like those of the atom—as a source of huge potential, a great frontier. If the brain was merely a biological machine, there was no theoretical reason that it could not be replicated, or even surpassed, much the way a jet could outfly a falcon. Even before the Dartmouth conference, machines exceeded human ability in narrow domains like code-breaking. In 1951, Alan Turing argued that at some point computers would probably exceed the intellectual capacity of their inventors, and that “therefore we should have to expect the machines to take control.” Whether this would be good or bad he did not say.\\nSix years later, Herbert Simon, one of the Dartmouth attendees, declared that machines would achieve human intelligence “in a visible future.” The crossing of such a threshold, he suspected, could be psychologically crushing, but he was on the whole optimistic. “We must also remain sensitive to the need to keep the computer’s goals attuned with our own,” he later said, but added, “I am not convinced that this will be difficult.” For other computer pioneers, the future appeared more ambivalent. Norbert Wiener, the father of cybernetics, argued that it would be difficult to manage powerful computers, or even to accurately predict their behavior. “Complete subservience and complete intelligence do not go together,” he said. Envisioning Sorcerer’s Apprentice scenarios, he predicted, “The future will be an ever more demanding struggle against the limitations of our intelligence, not a comfortable hammock in which we can lie down to be waited upon by our robot slaves.”\\nIt was in this milieu that the “intelligence explosion” idea was first formally expressed by I. J. Good, a statistician who had worked with Turing. “An ultraintelligent machine could design even better machines,” he wrote. “There would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.”\\nThe scientists at Dartmouth recognized that success required answers to fundamental questions: What is intelligence? What is the mind? By 1965, the field had experimented with several models of problem solving: some were based on formal logic; some used heuristic reasoning; some, called “neural networks,” were inspired by the brain. With each, the scientists’ work indicated that A.I. systems could find their own solutions to problems. One algorithm proved numerous theorems in the classic text “Principia Mathematica,” and in one instance it did so more elegantly than the authors. A program designed to play checkers learned to beat its programmer. And yet, despite the great promise in these experiments, the challenges to creating an A.I. were forbidding. Programs that performed well in the laboratory were useless in everyday situations; a simple act like picking up a ball turned out to require an overwhelming number of computations.\\nThe research fell into the first of several “A.I. winters.” As Bostrom notes in his book, “Among academics and their funders, ‘A.I.’ became an unwanted epithet.” Eventually, the researchers started to question the goal of building a mind altogether. Why not try instead to divide the problem into pieces? They began to limit their interests to specific cognitive functions: vision, say, or speech. Even in isolation, these functions would have value: a computer that could identify objects might not be an A.I., but it could help guide a forklift. As the research fragmented, the morass of technical problems made any questions about the consequences of success seem distant, even silly.\\nUnexpectedly, by dismissing its founding goals, the field of A.I. created space for outsiders to imagine more freely what the technology might look like. Bostrom wrote his first paper on artificial superintelligence in the nineteen-nineties, envisioning it as potentially perilous but irresistible to both commerce and government. “If there is a way of guaranteeing that superior artificial intellects will never harm human beings, then such intellects will be created,” he argued. “If there is no way to have such a guarantee, then they will probably be created nevertheless.” His audience at the time was primarily other transhumanists. But the movement was maturing. In 2005, an organization called the Singularity Institute for Artificial Intelligence began to operate out of Silicon Valley; its primary founder, a former member of the Extropian discussion group, published a stream of literature on the dangers of A.I. That same year, the futurist and inventor Ray Kurzweil wrote “The Singularity Is Near,” a best-seller that prophesied a merging of man and machine in the foreseeable future. Bostrom created his institute at Oxford.\\nThe two communities could not have been more different. The scientists, steeped in technical detail, were preoccupied with making devices that worked; the transhumanists, motivated by the hope of a utopian future, were asking, What would the ultimate impact of those devices be? In 2007, the Association for the Advancement of Artificial Intelligence—the most prominent professional organization for A.I. researchers—elected Eric Horvitz, a scientist from Microsoft, as its president. Until then, it had given virtually no attention to the ethical and social implications of the research, but Horvitz was open to the big questions. “It is hard to understand what success would mean for A.I.,” he told me. “I was friendly with Jack Good, who wrote that piece on superintelligence. I knew him as a creative, funny guy who referred to a lot of his ideas as P.B.I.s—partly baked ideas. And here is this piece of his being opened up outside the field as this Bible and studied with a silver pointer. Wouldn’t it be useful, I said, even if you thought these were crazy or low-probability scenarios, to find out: Can we be proactive, should there be some poor outcome for humanity?”\\nHorvitz organized a meeting at the Asilomar Conference Grounds, in California, a place chosen for its symbolic value: biologists had gathered there in 1975 to discuss the hazards of their research in the age of modern genetics. He divided the researchers into groups. One studied short-term ramifications, like the possible use of A.I. to commit crimes; another considered long-term consequences. Mostly, there was skepticism about the intelligence-explosion idea, which assumed answers to many unresolved questions. No one fully understands what intelligence is, let alone how it might evolve in a machine. Can it grow as Good imagined, gaining I.Q. points like a rocketing stock price? If so, what would its upper limit be? And would its increase be merely a function of optimized software design, without the difficult process of acquiring knowledge through experience? Can software fundamentally rewrite itself without risking crippling breakdowns? No one knows. In the history of computer science, no programmer has created code that can substantially improve itself.\\nBut the notion of an intelligence explosion was also impossible to disprove. It was theoretically coherent, and it had even been attempted in limited ways. David McAllester, an A.I. researcher at the Toyota Technological Institute, affiliated with the University of Chicago, headed the long-term panel. The idea, he argued, was worth taking seriously. “I am uncomfortable saying that we are ninety-­nine per cent certain that we are safe for fifty years,” he told me. “That feels like hubris to me.” The group concluded that more technical work was needed before an evaluation of the dangers could be made, but it also hinted at a concern among panelists that the gathering was based on “a perception of urgency”—generated largely by the transhumanists—and risked raising unfounded alarm. With A.I. seeming like a remote prospect, the researchers declared, attention was better spent on near-term concerns. Bart Selman, a professor at Cornell who co-­organized the panel, told me, “The mode was ‘This is interesting, but it’s all academic—it’s not going to happen.’ ”\\nAt the time the A.I researchers met at Asilomar, Bostrom was grappling with an expansive book on existential risks. He had sketched out chapters on bioengineering and on nanotechnology, among other topics, but many of these problems came to seem less compelling, while his chapter on A.I. grew and grew. Eventually, he pasted the A.I. chapter into a new file, which became “Superintelligence.”\\nThe book is its own elegant paradox: analytical in tone and often lucidly argued, yet punctuated by moments of messianic urgency. Some portions are so extravagantly speculative that it is hard to take them seriously. (“Suppose we could somehow establish that a certain future AI will have an IQ of 6,455: then what?”) But Bostrom is aware of the limits to his type of futurology. When he was a graduate student in London, thinking about how to maximize his ability to communicate, he pursued stand­­up comedy; he has a deadpan sense of humor, which can be found lightly buried among the book’s self-serious passages. “Many of the points made in this book are probably wrong,” he writes, with an endnote that leads to the line “I don’t know which ones.”\\nBostrom prefers to act as a cartographer rather than a polemicist, but beneath his exhaustive mapping of scenarios one can sense an argument being built and perhaps a fear of being forthright about it. “Traditionally, this topic domain has been occupied by cranks,” he told me. “By popular media, by science fiction—or maybe by a retired physicist no longer able to do serious work, so he will write a popular book and pontificate. That is kind of the level of rigor that is the baseline. I think that a lot of reasons why there has not been more serious work in this area is that academics don’t want to be conflated with flaky, crackpot type of things. Futurists are a certain type.”\\nThe book begins with an “unfinished” fable about a flock of sparrows that decide to raise an owl to protect and advise them. They go looking for an owl egg to steal and bring back to their tree, but, because they believe their search will be so difficult, they postpone studying how to domesticate owls until they succeed. Bostrom concludes, “It is not known how the story ends.”\\nThe parable is his way of introducing the book’s core question: Will an A.I., if realized, use its vast capability in a way that is beyond human control? One way to think about the concern is to begin with the familiar. Bos­trom writes, “Artificial intelligence already outperforms human intelligence in many domains.” The examples range from chess to Scrabble. One program from 1981, called Eurisko, was designed to teach itself a naval role-playing game. After playing ten thousand matches, it arrived at a morally grotesque strategy: to field thousands of small, immobile ships, the vast majority of which were intended as cannon fodder. In a national tournament, Eurisko demolished its human opponents, who insisted that the game’s rules be changed. The following year, Eurisko won again—by forcing its damaged ships to sink themselves.\\nThe program was by no means superintelligent. But Bostrom’s book essentially asks: What if it were? Assume that it has a broad ability to consider problems and that it has access to the Internet. It could read and acquire general knowledge and communicate with people seamlessly online. It could conduct experiments, either virtually or by tinkering with networked infrastructure. Given even the most benign objective—to win a game—such a system, Bostrom argues, might develop “instrumental goals”: gather resources, or invent technology, or take steps to insure that it cannot be turned off, in the process paying as much heed to human life as humans do to ants.\\nIn people, intelligence is inseparable from consciousness, emotional and social awareness, the complex interaction of mind and body. An A.I. need not have any such attributes. Bostrom believes that machine intelligences—no matter how flexible in their tactics—will likely be rigidly fixated on their ultimate goals. How, then, to create a machine that respects the nuances of social cues? That adheres to ethical norms, even at the expense of its goals? No one has a coherent solution. It is hard enough to reliably inculcate such behavior in people.\\nIn science fiction, superintelligent computers that run amok are often circumvented at the last minute; think of WOPR, the computer in “WarGames,” which was stopped just short of triggering nuclear war, or HAL 9000, which was reduced to helplessly singing while it watched itself get dismantled. For Bos­trom, this strains credulity. Whether out of a desire to consider the far ends of risk or out of transhumanist longings, he often ascribes nearly divine abilities to machines, as if to ask: Can a digital god really be contained? He imagines machines so intelligent that merely by inspecting their own code they can extrapolate the nature of the universe and of human society, and in this way outsmart any effort to contain them. “Is it possible to build machines that are not like agents—goal-pursuing, autonomous, artificial intelligences?” he asked me. “Maybe you can design something more like an oracle that can only answer yes or no. Would that be safer? It is not so clear. There might be agent-like processes within it.” Asking a simple question—“Is it possible to convert a DeLorean into a time machine and travel to 1955?”—might trigger a cascade of action as the device tests hypotheses. What if, working through a police computer, it impounds a DeLorean that happens to be convenient to a clock tower? “In fairy tales, you have genies who grant wishes,” Bostrom said. “Almost universally, the moral of those is that if you are not extremely careful what you wish for, then what seems like it should be a great blessing turns out to be a curse.”\\nBostrom worries that solving the “control problem”—insuring that a superintelligent machine does what humans want it to do—will require more time than solving A.I. does. The intelligence explosion is not the only way that a superintelligence might be created suddenly. Bostrom once sketched out a decades-long process, in which researchers arduously improved their systems to equal the intelligence of a mouse, then a chimp, then—after incredible labor—the village idiot. “The difference between village idiot and genius-­level intelligence might be trivial from the point of view of how hard it is to replicate the same functionality in a machine,” he said. “The brain of the village idiot and the brain of a scientific genius are almost identical. So we might very well see relatively slow and incremental progress that doesn’t really raise any alarm bells until we are just one step away from something that is radically superintelligent.”\\nTo a large degree, Bostrom’s concerns turn on a simple question of timing: Can breakthroughs be predicted? “It is ridiculous to talk about such things so early—A.I. is eons away,” Edward Feigenbaum, an emeritus professor at Stanford University, told me. The researcher Oren Etzioni, who used the term “Frankenstein complex” to dismiss the “dystopian vision of A.I.,” concedes Bostrom’s overarching point: that the field must one day confront profound philosophical questions. Decades ago, he explored them himself, in a brief paper, but concluded that the problem was too remote to think about productively. “Once, Nick Bostrom gave a talk, and I gave a little counterpoint,” he told me. “A lot of the disagreements come down to what time scale you are thinking about. Nobody responsible would say you will see anything remotely like A.I. in the next five to ten years. And I think most computer scientists would say, ‘In a million years—we don’t see why it shouldn’t happen.’ So now the question is: What is the rate of progress? There are a lot of people who will ask: Is it possible we are wrong? Yes. I am not going to rule it out. I am going to say, ‘I am a scientist. Show me the evidence.’ ”\\nThe history of science is an uneven guide to the question: How close are we? There has been no shortage of unfulfilled promises. But there are also plenty of examples of startling nearsightedness, a pattern that Arthur C. Clarke enshrined as Clarke’s First Law: “When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong.” After the electron was discovered, at Cambridge, in 1897, physicists at an annual dinner toasted, “To the electron: may it never be of use to anybody.” Lord Kelvin famously declared, just eight years before the Wright brothers launched from Kitty Hawk, that heavier-than-air flight was impossible.\\nStuart Russell, the co-author of the textbook “Artificial Intelligence: A Modern Approach” and one of Bostrom’s most vocal supporters in A.I., told me that he had been studying the physics community during the advent of nuclear weapons. At the turn of the twentieth century, Ernest Rutherford discovered that heavy elements produced radiation by atomic decay, confirming that vast reservoirs of energy were stored in the atom. Rutherford believed that the energy could not be harnessed, and in 1933 he proclaimed, “Anyone who expects a source of power from the transformation of these atoms is talking moonshine.” The next day, a former student of Einstein’s named Leo Szilard read the comment in the papers. Irritated, he took a walk, and the idea of a nuclear chain reaction occurred to him. He visited Rutherford to discuss it, but Rutherford threw him out. Einstein, too, was skeptical about nuclear energy—splitting atoms at will, he said, was “like shooting birds in the dark in a country where there are only a few birds.” A decade later, Szilard’s insight was used to build the bomb.\\nRussell now relays the story to A.I. researchers as a cautionary tale. “There will have to be more breakthroughs to get to A.I., but, as Szilard illustrated, those can happen overnight,” he told me. “People are putting billions of dollars into achieving those breakthroughs. As the debate stands, Bostrom and others have said, ‘If we achieve superintelligence, here are some of the problems that might arise.’ As far as I know, no one has proved why those are not real.”\\nIII. Mission Control\\nThe offices of the Future of Humanity Institute have a hybrid atmosphere: part physics lab, part college dorm room. There are whiteboards covered with mathematical notation and technical glyphs; there are posters of “Brave New World” and HAL 9000. There is also art work by Nick Bostrom. One afternoon, he guided me to one of his pieces, “At Sea,” a digital collage that he had printed out and then drawn on. “It is a bit damaged, but the good thing about digital is that you can re-instantiate it,” he said. At the center was a pale man, nearly an apparition, clinging to a barrel in an inky-black ocean. “It is an existentialist vibe. You are hanging on for as long as you can. When you get tired, you sink, and become fish food—or maybe a current will take him to land. We don’t know.”\\nDespite the time he spends going to conferences and raising money, Bostrom attends to many details at the institute. “We needed a logo when we started,” he told me. “We went to this online site where you could buy the work of freelance artists. If you sat down and tried to make the ugliest logo, you couldn’t come close. Then we hired a designer, who made a blurry figure of a person. We showed it to someone here, who said it looked like a toilet sign. As soon as she said it, I thought, Oh, my God, we almost adopted a toilet sign as our logo. So I mucked around a bit and came up with a black diamond. You have the black monolith from ‘2001.’ Standing on its corner, it indicates instability. Also, there is a limit to how ugly a black square can be.”\\nThe institute shares office space with the Centre for Effective Altruism, and both organizations intersect with a social movement that promotes pure rationality as a guide to moral action. Toby Ord, a philosopher who works with both, told me that Bostrom often pops into his office at the end of the day, poses a problem, then leaves him pondering it for the night. Among the first of Bostrom’s questions was this: If the universe turns out to contain an infinite number of beings, then how could any single person’s action affect the cosmic balance of suffering and happiness? After lengthy discussions, they left the paradox unresolved. “My main thinking is that we can sort it out later,” Ord told me.\\nWhen I asked Bostrom if I could observe a discussion at the institute, he seemed reluctant; it was hard to judge whether he was concerned that my presence would interfere or that unfiltered talk of, say, engineered pathogens might inspire criminals. (“At some point, one gets into the realm of information hazard,” he hinted.) Eventually, he let me observe a session in the Petrov Room involving half a dozen staff members. The key question under discussion was whether a global catastrophe, on the order of a continent-wide famine, could trigger a series of geopolitical events that would result in human extinction—and whether that meant that a merely catastrophic risk could therefore be taken as seriously as an existential risk. Bostrom, wearing a gray hoodie over a blue button-­down, organized the problem on a whiteboard with visible pleasure. Anders Sandberg told me that he once spent days with Bostrom working through such a problem, distilling a complex argument to its essence. “He had to refine it,” he said. “We had a lot of schemes on the whiteboard that gradually were simplified to one box and three arrows.”\\nFor anyone in the business of publicizing existential risk, 2015 began as a good year. Other institutes devoted to these issues had started to find their voice, bringing an additional gloss of respectability to the ideas in Bostrom’s book. The people weighing in now were no longer just former Extropians. They were credentialled, like Lord Martin Rees, an astrophysicist and the co-founder of Cambridge’s Centre for the Study of Existential Risk. In January, he wrote of A.I., in the Evening Standard, “We don’t know where the boundary lies between what may happen and what will remain science fiction.”\\nRees’s counterpart at the Future of Life Institute, the M.I.T. physicist Max Tegmark, hosted a closed-door meeting in Puerto Rico, to try to make sense of the long-term trajectory of the research. Bostrom flew down, joining a mix of A.I. practitioners, legal scholars, and, for lack of a better term, members of the “A.I. safety” community. “These are not people who are usually in the same room,” Tegmark told me. “Someone advised me to put Valium in people’s drinks so nobody got into fistfights. But, by the time Nick’s session started, people were ready to listen to each other.” Questions that had seemed fanciful to researchers only seven years earlier were beginning to look as though they might be worth reconsidering. Whereas the Asilomar meeting concluded on a note of skepticism about the validity of the whole endeavor, the Puerto Rico conference resulted in an open letter, signed by many prominent researchers, that called for more research to insure that A.I. would be “robust and beneficial.”\\nBetween the two conferences, the field had experienced a revolution, built on an approach called deep learning—a type of neural network that can discern complex patterns in huge quantities of data. For de­c­ades, researchers, hampered by the limits of their hardware, struggled to get the technique to work well. But, beginning in 2010, the increasing availability of Big Data and cheap, powerful video-­game processors had a dramatic effect on performance. Without any profound theoretical breakthrough, deep learning suddenly offered breathtaking advances. “I have been talking to quite a few contemporaries,” Stuart Russell told me. “Pretty much everyone sees examples of progress they just didn’t expect.” He cited a YouTube clip of a four-legged robot: one of its designers tries to kick it over, but it quickly regains its balance, scrambling with uncanny naturalness. “A problem that had been viewed as very difficult, where progress was slow and incremental, was all of a sudden done. Locomotion: done.”\\nIn an array of fields—speech processing, face recognition, language translation—the approach was ascendant. Researchers working on computer vision had spent years to get systems to identify objects. In almost no time, the deep-learning networks crushed their records. In one common test, using a database called ImageNet, humans identify photographs with a five-per-cent error rate; Google’s network operates at 4.8 per cent. A.I. systems can differentiate a Pembroke Welsh Corgi from a Cardigan Welsh Corgi.\\nLast October, Tomaso Poggio, an M.I.T. researcher, gave a skeptical interview. “The ability to describe the content of an image would be one of the most intellectually challenging things of all for a machine to do,” he said. “We will need another cycle of basic research to solve this kind of question.” The cycle, he predicted, would take at least twenty years. A month later, Google announced that its deep-learning network could analyze an image and offer a caption of what it saw: “Two pizzas sitting on top of a stove top,” or “People shopping at an outdoor market.” When I asked Poggio about the results, he dismissed them as automatic associations between objects and language; the system did not understand what it saw. “Maybe human intelligence is the same thing, in which case I am wrong, or not, in which case I was right,” he told me. “How do you decide?”\\nA respected minority of A.I. researchers began to wonder: If increasingly powerful hardware could facilitate the deep-learning revolution, would it make other long-shelved A.I. principles viable? “Suppose the brain is just a million different evolutionarily developed hacks: one for smell, one for recognizing faces, one for how you recognize animals,” Tom Mitchell, who holds a chair in machine learning at Carnegie Mellon, told me. “If that is what underlies intelligence, then I think we are far, far from getting there—because we don’t have many of those hacks. On the other hand, suppose that what underlies intelligence are twenty-three general mechanisms, and when you put them together you get synergy, and it works. We now have systems that can do a pretty good job with computer vision—and it turns out that we didn’t have to construct a million hacks. So part of the uncertainty is: if we do not need a million different hacks, then will we find the right twenty-­three fundamental generic methods?” He paused. “I no longer have the feeling, which I had twenty-five years ago, that there are gaping holes. I know we don’t have a good architecture to assemble the ideas, but it is not obvious to me that we are missing components.”\\nBostrom noticed the shift in attitude. He recently conducted a poll of A.I. researchers to gauge their sense of progress, and in Puerto Rico a survey gathered opinions on how long it would be until an artificial intelligence could reason indistinguishably from a human being. Like Bostrom, the engineers are often careful to express their views as probabilities, rather than as facts. Richard Sutton, a Canadian computer scientist whose work has earned tens of thousands of scholarly citations, gives a range of outcomes: there is a ten-per-cent chance that A.I. will never be achieved, but a twenty-five-per-cent chance that it will arrive by 2030. The median response in Bostrom’s poll gives a fifty-fifty chance that human-level A.I. would be attained by 2050. These surveys are unscientific, but he is confident enough to offer an interpretive assumption: “It is not a ridiculous prospect to take seriously the possibility that it can happen in the lifetime of people alive today.”\\nOn my last day in Oxford, I walked with Bostrom across town. He was racing to catch a train to London, to speak at the Royal Society, one of the world’s oldest scientific institutions. His spirits were high. The gulf between the transhumanists and the scientific community was slowly shrinking. Elon Musk had pledged ten million dollars in grants for academics seeking to investigate A.I. safety, and, rather than mock him, researchers applied for the money; Bostrom’s institute was helping to evaluate the proposals. “Right now, there is a lot of interest,” he told me. “But then there were all these long years when nobody else seemed to pay attention at all. I am not sure which is the less abnormal condition.”\\nThere were clear limits to that interest. To publicly stake out a position in the middle of the debate was difficult, not least because of the polarized atmosphere Bostrom’s book had helped to create. Even though a growing number of researchers were beginning to suspect that profound questions loomed, and that they might be worth addressing now, it did not mean that they believed A.I. would lead inevitably to an existential demise or a techno-utopia. Most of them were engaged with more immediate problems: privacy, unemployment, weaponry, driverless cars running amok. When I asked Bostrom about this pragmatic ethical awakening, he reacted with dismay. “My fear is that it would swallow up the concerns for the longer term,” he said. “On the other hand, yes, maybe it is useful to build bridges to these different communities. Kind of makes the issue part of a larger continuum of things to work on.”\\nAt the Royal Society, Bostrom took a seat at the back of a large hall. As he crossed his legs, I noticed a thin leather band around his ankle. A metal buckle was engraved with contact information for Alcor, a cryonics facility in Arizona, where Bostrom is a fee-paying member. Within hours of his death, Alcor will take custody of his body and maintain it in a giant steel bottle flooded with liquid nitrogen, in the hope that one day technology will allow him to be revived, or to have his mind uploaded into a computer. When he signed up, two other colleagues at the institute joined him. “My background is transhumanism,” he once reminded me. “The character of that is gung-ho techno-cheerleading, bring it on now, where are my life-­extension pills.”\\nThe hall was packed with some of the most technically sophisticated researchers in A.I.—not necessarily Bostrom’s people—and when he spoke he began by trying to assure them that his concern was not out of Ludditism. “It would be tragic if machine intelligence were never developed to its full capacity,” he said. “I think this is ultimately the key, or the portal, we have to pass through to realize the full dimension of humanity’s long-term potential.” But, even as he avoided talk of existential risk, he pressed his audience to consider the danger of building an A.I. without regarding its ethical design.\\nAn attendee raised his hand to object. “We can’t control basic computer worms,” he said. “The A.I. that will happen is going to be a highly adaptive, emergent capability, and highly distributed. We will be able to work with it—for it—not necessarily contain it.”\\n“I guess I am a little frustrated,” Bos­trom responded. “People tend to fall into two camps. On one hand, there are those, like yourself, who think it is probably hopeless. The other camp thinks it is easy enough that it will be solved automatically. And both of these have in common the implication that we don’t have to make any effort now.”\\nFor the rest of the day, engineers presented their work at the lectern, each promising a glimpse of the future—robot vision, quantum computers, algorithms called “thought vectors.” Early in Bostrom’s career, he predicted that cascading economic demand for an A.I. would build up across the fields of medicine, entertainment, finance, and defense. As the technology became useful, that demand would only grow. “If you make a one-per-cent improvement to something—say, an algorithm that recommends books on Amazon—there is a lot of value there,” Bostrom told me. “Once every improvement potentially has enormous economic benefit, that promotes effort to make more improvements.”\\nMany of the world’s largest tech companies are now locked in an A.I. arms race, purchasing other companies and opening specialized units to advance the technology. Industry is vacuuming up Ph.D.s so quickly that people in the field worry there will no longer be top talent in academia. After decades of pursuing narrow forms of A.I., researchers are seeking to integrate them into systems that resemble a general intellect. Since I.B.M.’s Watson won “Jeopardy,” the company has committed more than a billion dollars to develop it, and is reorienting its business around “cognitive systems.” One senior I.B.M. executive declared, “The separation between human and machine is going to blur in a very fundamental way.”\\nAt the Royal Society, a contingent of researchers from Google occupied a privileged place; they likely had more resources at their disposal than anyone else in the room. Early on, Google’s founders, Larry Page and Sergey Brin, understood that the company’s mission required solving fundamental A.I. problems. Page has said that he believes the ideal system would understand questions, even anticipate them, and produce responses in conversational language. Google scientists often invoke the computer in “Star Trek” as a model.\\nIn recent years, Google has purchased seven robotics companies and several firms specializing in machine intelligence; it may now employ the world’s largest contingent of Ph.D.s in deep learning. Perhaps the most interesting acquisition is a British company called DeepMind, started in 2011 to build a general artificial intelligence. Its founders had made an early bet on deep learning, and sought to combine it with other A.I. mechanisms in a cohesive architecture. In 2013, they published the results of a test in which their system played seven classic Atari games, with no instruction other than to improve its score. For many people in A.I., the importance of the results was immediately evident. I.B.M.’s chess program had defeated Garry Kasparov, but it could not beat a three-year-old at tic-tac-toe. In six games, DeepMind’s system outperformed all previous algorithms; in three it was superhuman. In a boxing game, it learned to pin down its opponent and subdue him with a barrage of punches.\\nWeeks after the results were released, Google bought the company, reportedly for half a billion dollars. DeepMind placed two unusual conditions on the deal: its work could never be used for espionage or defense purposes, and an ethics board would oversee the research as it drew closer to achieving A.I. Anders Sandberg had told me, “We are happy that they are among the most likely to do it. They recognize there are some problems.”\\nDeepMind’s chief founder, Demis Hassabis, described his company to the audience at the Royal Society as an “Apollo Program” with a two-part mission: “Step one, solve intelligence. Step two, use it to solve everything else.” Since the test in 2013, his system had aced more than a dozen other Atari titles. Hassabis demonstrated an unpublished trial using a three-dimensional driving game, in which it had quickly outperformed the game’s automated drivers. The plan was to test it in increasingly complex virtual environments and, eventually, in the real world. The patent lists a range of uses, from finance to robotics.\\nHassabis was clear about the challenges. DeepMind’s system still fails hopelessly at tasks that require long-range planning, knowledge about the world, or the ability to defer rewards—things that a five-year-old child might be expected to handle. The company is working to give the algorithm conceptual understanding and the capability of transfer learning, which allows humans to apply lessons from one situation to another. These are not easy problems. But DeepMind has more than a hundred Ph.D.s to work on them, and the rewards could be immense. Hassabis spoke of building artificial scientists to resolve climate change, disease, poverty. “Even with the smartest set of humans on the planet working on these problems, these systems might be so complex that it is difficult for individual humans, scientific experts,” he said. “If we can crack what intelligence is, then we can use it to help us solve all these other problems.” He, too, believes that A.I. is a gateway to expanded human potential.\\nThe keynote speaker at the Royal Society was another Google employee: Geoffrey Hinton, who for decades has been a central figure in developing deep learning. As the conference wound down, I spotted him chatting with Bostrom in the middle of a scrum of researchers. Hinton was saying that he did not expect A.I. to be achieved for decades. “No sooner than 2070,” he said. “I am in the camp that is hopeless.”\\n“In that you think it will not be a cause for good?” Bostrom asked.\\n“I think political systems will use it to terrorize people,” Hinton said. Already, he believed, agencies like the N.S.A. were attempting to abuse similar technology.\\n“Then why are you doing the research?” Bostrom asked.\\n“I could give you the usual arguments,” Hinton said. “But the truth is that the prospect of discovery is too sweet.” He smiled awkwardly, the word hanging in the air—an echo of Oppenheimer, who famously said of the bomb, “When you see something that is technically sweet, you go ahead and do it, and you argue about what to do about it only after you have had your technical success.”\\nAs the scientists retreated to tables set up for refreshments, I asked Hinton if he believed an A.I. could be controlled. “That is like asking if a child can control his parents,” he said. “It can happen with a baby and a mother—there is biological hardwiring—but there is not a good track record of less intelligent things controlling things of greater intelligence.” He looked as if he might elaborate. Then a scientist called out, “Let’s all get drinks!”\\nBostrom had little interest in the cocktail party. He shook a few hands, then headed for St. James’s Park, a public garden that extends from the gates of Buckingham Palace through central London. The world appeared in splendorous analog: sunlight over trees, duck ponds, children and grandparents feeding birds. The spot had been a park for hundreds of years, and the vista seemed timeless. Yet, during the past millennium, the grounds had also been a marsh, a leper hospital, a deer sanctuary, and royal gardens. It seemed plausible that, a thousand years from now, digital posthumans, regarding it as wasted space, would tear it up, replace the landscaping with computer banks, and erect a vast virtual idyll.\\nBostrom’s pace settled into its natural quickness as we circled the park. He talked about his family; he would be seeing his wife and son soon. He was reading widely: history, psychology, economics. He was learning to code. He was thinking about expanding his institute. Although he did not know it then, F.H.I. was about to receive one and a half million dollars from Elon Musk, to create a unit that would craft social policies informed by some of Bostrom’s theories. He would need to hire people. He was also giving thought to the framing of his message. “A lot more is said about the risks than the upsides, but that is not necessarily because the upside is not there,” he told me. “There is just more to be said about the risk—and maybe more use in describing the pitfalls, so we know how to steer around them—than spending time now figuring out the details of how we are going to furnish the great palace a thousand years from now.”\\nWe passed a fountain, near a cluster of rocks engineered to give ducks a resting place. Bostrom, in his forties, must soon contend with physical decline, and he spoke with annoyance of the first glimmers of mortality. Even though he is an Alcor member, there is no guarantee that cryonics will work. Perhaps the most radical of his visions is that superintelligent A.I. will hasten the uploading of minds—what he calls “whole-brain emulations”—technology that might not be possible for centuries, if at all. Bostrom, in his most hopeful mode, imagines emulations not only as reproductions of the original intellect “with memory and personality intact”—a soul in the machine—but as minds expandable in countless ways. “We live for seven decades, and we have three-pound lumps of cheesy matter to think with, but to me it is plausible that there could be extremely valuable mental states outside this little particular set of possibilities that might be much better,” he told me.\\nIn his book, Bostrom considers a distant future in which trillions of digital minds merge into an enormous cognitive cyber-soup. “Whether the set of extremely positive posthuman modes of being would include some kind of dissolved bouillon, there is some uncertainty,” he said. “If you look at religious views, there are many where merging with something greater is a form of heaven, being in the presence of this enormous beauty and goodness. In many traditions, the best possible state does not involve being a little individual pursuing goals. But it is hard to get a grasp of what would be going on in that soup. Maybe some soups would not be preferable as a long-term outcome. I don’t know.” He stopped and looked ahead. “What I want to avoid is to think from our parochial 2015 view—from my own limited life experience, my own limited brain—and super-confidentially postulate what is the best form for civilization a billion years from now, when you could have brains the size of planets and billion-year life spans. It seems unlikely that we will figure out some detailed blueprint for utopia. What if the great apes had asked whether they should evolve into Homo sapiens—pros and cons—and they had listed, on the pro side, ‘Oh, we could have a lot of bananas if we became human’? Well, we can have unlimited bananas now, but there is more to the human condition than that.” ♦\\nIllustration by Todd St. John\\u002FCoding by Jono Brandel.\",\"isBasedOn\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2015\\u002F11\\u002F23\\u002Fdoomsday-invention-artificial-intelligence-nick-bostrom\",\"articleSection\":\"a reporter at large\",\"author\":[{\"@type\":\"Person\",\"name\":\"Raffi Khatchadourian\",\"sameAs\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcontributors\\u002Fraffi-khatchadourian\"}],\"dateModified\":\"2015-11-15T23:00:00.000-05:00\",\"datePublished\":\"2015-11-15T19:00:00.000-05:00\",\"headline\":\"The Philosopher of Doomsday\",\"image\":[\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002F16:9\\u002Fw_2560,h_1440,c_limit\\u002F151123_r27342.jpg\",\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002F4:3\\u002Fw_2560,h_1920,c_limit\\u002F151123_r27342.jpg\",\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002F1:1\\u002Fw_2000,h_2000,c_limit\\u002F151123_r27342.jpg\"],\"keywords\":[\"a reporter at large\",\"artificial intelligence (a.i.)\",\"category_science_tech\",\"philosophers\",\"magazine\"],\"thumbnailUrl\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002F1:1\\u002Fw_2000,h_2000,c_limit\\u002F151123_r27342.jpg\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2015\\u002F11\\u002F23\\u002Fdoomsday-invention-artificial-intelligence-nick-bostrom\",\"isPartOf\":{\"@type\":\"CreativeWork\",\"name\":\"The New Yorker\"},\"isAccessibleForFree\":true,\"alternativeHeadline\":\"Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.\",\"description\":\"Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2015\\u002F11\\u002F23\\u002Fdoomsday-invention-artificial-intelligence-nick-bostrom\"},\"publisher\":{\"@context\":\"https:\\u002F\\u002Fschema.org\",\"@type\":\"Organization\",\"name\":\"The New Yorker\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fverso\\u002Fstatic\\u002Fthe-new-yorker\\u002Fassets\\u002Fthe-new-yorker-seo-logo.jpg\",\"width\":\"1200px\",\"height\":\"630px\"},\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\"}},{\"@context\":\"https:\\u002F\\u002Fschema.org\\u002F\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Magazine\",\"item\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Artificial Intelligence (A.I.)\",\"item\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Ftag\\u002Fartificial-intelligence-ai\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"The Doomsday Invention\"}]}],\"head.contentID\":\"5911cbb2803aff0f1c1359ac\",\"head.pageType\":\"article\",\"head.photos\":{\"image-16-9\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002F16:9\\u002Fw_1000,c_limit\\u002F151123_r27342.jpg\",\"image-1-1\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002F1:1\\u002Fw_1000,c_limit\\u002F151123_r27342.jpg\"},\"head.firstPublishDate\":\"2015-11-16T00:00:00.000Z\",\"head.primaryTagsRoot\":\"tags\",\"head.hasSponsoredContent\":false,\"head.modifiedDate\":\"2015-11-16T04:00:00.000Z\",\"head.noIndex\":false,\"linkBannerData\":{\"bannerType\":\"marquee\",\"dek\":\"\",\"hed\":\"\",\"image\":{},\"links\":[],\"tracking\":{\"trackingIdentifier\":\"\",\"attributes\":{}}},\"linkBannerId\":[],\"navigation\":{\"aboutText\":\"The writers you love. The stories that matter.\",\"appDownloadUrls\":[],\"account\":{\"signInLink\":\"\\u002Fauth\\u002Finitiate?redirectURL=%2Fmagazine%2F2015%2F11%2F23%2Fdoomsday-invention-artificial-intelligence-nick-bostrom&source=VERSO_NAVIGATION\",\"accountLinks\":[{\"attributes\":{\"name\":\"\\u002Fmanage-account\"},\"index\":0,\"text\":\"Manage account\",\"url\":\"\\u002Faccount\\u002Fprofile\"},{\"attributes\":{\"name\":\"\\u002Fview-saved-stories\"},\"index\":1,\"text\":\"View saved stories\",\"url\":\"\\u002Faccount\\u002Fsaved\"}],\"redirectURL\":\"%2Fmagazine%2F2015%2F11%2F23%2Fdoomsday-invention-artificial-intelligence-nick-bostrom\",\"signOutLink\":\"\\u002Fauth\\u002Fend?redirectURL=%2Fmagazine%2F2015%2F11%2F23%2Fdoomsday-invention-artificial-intelligence-nick-bostrom\"},\"contactLinks\":[{\"isExternal\":true,\"text\":\"Customer Care\",\"url\":\"http:\\u002F\\u002Fw1.buysub.com\\u002Fservlet\\u002FCSGateway?cds_mag_code=NYR\",\"isActive\":false},{\"isExternal\":true,\"text\":\"Shop The New Yorker\",\"url\":\"https:\\u002F\\u002Fstore.newyorker.com\",\"isActive\":false},{\"isExternal\":true,\"text\":\"Buy Covers and Cartoons\",\"url\":\"https:\\u002F\\u002Fcondenaststore.com\\u002Fart\\u002Fnew+yorker+covers\",\"isActive\":false},{\"isExternal\":true,\"text\":\"Condé Nast Store\",\"url\":\"https:\\u002F\\u002Fcondenaststore.com\\u002Fconde-nast-brand\\u002Fthenewyorker\",\"isActive\":false},{\"isExternal\":false,\"text\":\"Digital Access\",\"url\":\"\\u002Fdigital-editions\",\"isActive\":false},{\"isExternal\":false,\"text\":\"Newsletters\",\"url\":\"\\u002Fnewsletter\",\"isActive\":false},{\"isExternal\":false,\"text\":\"Jigsaw Puzzle\",\"url\":\"\\u002Fjigsaw\",\"isActive\":false},{\"isExternal\":false,\"text\":\"RSS\",\"url\":\"\\u002Fabout\\u002Ffeeds\",\"isActive\":false}],\"contactLinksHeading\":\"More\",\"footerLinks\":[{\"isExternal\":false,\"text\":\"News\",\"url\":\"\\u002Fnews\",\"isActive\":false},{\"isExternal\":false,\"text\":\"Books & Culture\",\"url\":\"\\u002Fculture\",\"isActive\":false},{\"isExternal\":false,\"text\":\"Fiction & Poetry\",\"url\":\"\\u002Ffiction-and-poetry\",\"isActive\":false},{\"isExternal\":false,\"text\":\"Humor & Cartoons\",\"url\":\"\\u002Fhumor\",\"isActive\":false},{\"isExternal\":false,\"text\":\"Magazine\",\"url\":\"\\u002Fmagazine\",\"isActive\":false},{\"isExternal\":false,\"text\":\"Crossword\",\"url\":\"\\u002Fcrossword-puzzles-and-games\",\"isActive\":false},{\"isExternal\":false,\"text\":\"Video\",\"url\":\"\\u002Fvideo\",\"isActive\":false},{\"isExternal\":false,\"text\":\"Podcasts\",\"url\":\"\\u002Fpodcast\",\"isActive\":false},{\"isExternal\":false,\"text\":\"Archive\",\"url\":\"\\u002Farchive\",\"isActive\":false},{\"isExternal\":false,\"text\":\"Goings On\",\"url\":\"\\u002Fgoings-on\",\"isActive\":false}],\"footerLinksHeading\":\"Sections\",\"hasChannelNavigation\":false,\"hasSEOSupport\":false,\"isEditorPicksAvailable\":false,\"noticesLinks\":[{\"isExternal\":false,\"text\":\"About\",\"url\":\"\\u002Fabout\\u002Fus\",\"isActive\":false},{\"text\":\"Careers\",\"url\":\"\\u002Fabout\\u002Fcareers\",\"isActive\":false},{\"text\":\"Contact\",\"url\":\"\\u002Fabout\\u002Fcontact\",\"isActive\":false},{\"text\":\"F.A.Q.\",\"url\":\"\\u002Fabout\\u002Ffaq\",\"isActive\":false},{\"text\":\"Media Kit\",\"url\":\"https:\\u002F\\u002Fwww.condenast.com\\u002Fadvertising\",\"isActive\":false},{\"text\":\"Press\",\"url\":\"\\u002Fabout\\u002Fpress\",\"isActive\":false},{\"text\":\"Accessibility Help\",\"url\":\"\\u002Fabout\\u002Faccessibility-help\",\"rel\":\"nofollow\",\"isActive\":false},{\"isExternal\":true,\"text\":\"User Agreement\",\"url\":\"https:\\u002F\\u002Fwww.condenast.com\\u002Fuser-agreement\\u002F\",\"rel\":\"nofollow\",\"isActive\":false},{\"isExternal\":true,\"text\":\"Privacy Policy\",\"url\":\"http:\\u002F\\u002Fwww.condenast.com\\u002Fprivacy-policy#privacypolicy\",\"rel\":\"nofollow\",\"isActive\":false},{\"isExternal\":true,\"text\":\"Your California Privacy Rights\",\"url\":\"http:\\u002F\\u002Fwww.condenast.com\\u002Fprivacy-policy#privacypolicy-california\",\"rel\":\"nofollow\",\"isActive\":false}],\"primaryLinks\":[{\"isExternal\":false,\"showInTopNav\":true,\"text\":\"The Latest\",\"url\":\"\\u002Flatest\",\"forceLeftOfNav\":false,\"isActive\":false},{\"isExternal\":false,\"showInTopNav\":true,\"text\":\"News\",\"url\":\"\\u002Fnews\",\"forceLeftOfNav\":false,\"isActive\":false},{\"isExternal\":false,\"showInTopNav\":true,\"text\":\"Books & Culture\",\"url\":\"\\u002Fculture\",\"forceLeftOfNav\":false,\"isActive\":false},{\"isExternal\":false,\"showInTopNav\":true,\"text\":\"Fiction & Poetry\",\"url\":\"\\u002Ffiction-and-poetry\",\"forceLeftOfNav\":false,\"isActive\":false},{\"isExternal\":false,\"showInTopNav\":true,\"text\":\"Humor & Cartoons\",\"url\":\"\\u002Fhumor\",\"forceLeftOfNav\":false,\"isActive\":false},{\"isExternal\":false,\"showInTopNav\":true,\"text\":\"Magazine\",\"url\":\"\\u002Fmagazine\",\"forceLeftOfNav\":false,\"isActive\":true},{\"isExternal\":false,\"showInTopNav\":true,\"text\":\"Puzzles & Games\",\"url\":\"\\u002Fcrossword-puzzles-and-games\",\"forceLeftOfNav\":false,\"isActive\":false},{\"isExternal\":false,\"showInTopNav\":true,\"text\":\"Video\",\"url\":\"\\u002Fvideo\",\"forceLeftOfNav\":false,\"isActive\":false},{\"isExternal\":false,\"showInTopNav\":true,\"text\":\"Podcasts\",\"url\":\"\\u002Fpodcasts\",\"forceLeftOfNav\":false,\"isActive\":false},{\"isExternal\":false,\"showInTopNav\":false,\"text\":\"Archive\",\"url\":\"\\u002Farchive\",\"forceLeftOfNav\":false,\"isActive\":false},{\"isExternal\":false,\"showInTopNav\":true,\"text\":\"Goings On\",\"url\":\"\\u002Fgoings-on\",\"forceLeftOfNav\":false,\"isActive\":false},{\"isExternal\":true,\"showInTopNav\":false,\"text\":\"Shop\",\"url\":\"https:\\u002F\\u002Fstore.newyorker.com\",\"forceLeftOfNav\":false,\"isActive\":false},{\"isExternal\":false,\"showInTopNav\":true,\"text\":\"Festival\",\"url\":\"https:\\u002F\\u002Ffestival.newyorker.com\\u002F\",\"forceLeftOfNav\":true,\"isActive\":false}],\"searchLink\":\"\\u002Fsearch\",\"secondaryLinks\":[],\"callToActionLink\":{\"url\":\"\",\"text\":\"\",\"verboseText\":\"\"},\"pageHeadline\":\"The Doomsday Invention\",\"socialLinks\":[{\"label\":\"Follow us on Facebook\",\"network\":\"facebook\",\"url\":\"https:\\u002F\\u002Fwww.facebook.com\\u002Fnewyorker\\u002F\"},{\"label\":\"Follow us on X\",\"network\":\"twitter\",\"url\":\"https:\\u002F\\u002Ftwitter.com\\u002FNewYorker\\u002F\"},{\"label\":\"Follow us on Snapchat\",\"network\":\"snapchat\",\"url\":\"https:\\u002F\\u002Fwww.snapchat.com\\u002Fadd\\u002Fnewyorkermag\"},{\"label\":\"Follow us on YouTube\",\"network\":\"youtube\",\"url\":\"https:\\u002F\\u002Fwww.youtube.com\\u002Fuser\\u002FNewYorkerDotCom\\u002F\"},{\"label\":\"Follow us on Instagram\",\"network\":\"instagram\",\"url\":\"https:\\u002F\\u002Finstagram.com\\u002Fnewyorkermag\\u002F\"}],\"socialLinksHeading\":\"\",\"subchannelLinks\":[],\"utilityLinks\":[{\"text\":\"Newsletter\",\"url\":\"\\u002Fnewsletters?sourceCode=navbar\",\"forceLeftOfNav\":false,\"showInTopNav\":true,\"isActive\":false}]},\"isAccountsEnabled\":true,\"accountProps\":{\"signInLink\":\"\\u002Fauth\\u002Finitiate?redirectURL=%2Fmagazine%2F2015%2F11%2F23%2Fdoomsday-invention-artificial-intelligence-nick-bostrom&source=VERSO_NAVIGATION\",\"accountLinks\":[{\"attributes\":{\"name\":\"\\u002Fmanage-account\"},\"index\":0,\"text\":\"Manage account\",\"url\":\"\\u002Faccount\\u002Fprofile\"},{\"attributes\":{\"name\":\"\\u002Fview-saved-stories\"},\"index\":1,\"text\":\"View saved stories\",\"url\":\"\\u002Faccount\\u002Fsaved\"}],\"redirectURL\":\"%2Fmagazine%2F2015%2F11%2F23%2Fdoomsday-invention-artificial-intelligence-nick-bostrom\",\"signOutLink\":\"\\u002Fauth\\u002Fend?redirectURL=%2Fmagazine%2F2015%2F11%2F23%2Fdoomsday-invention-artificial-intelligence-nick-bostrom\"},\"parsely\":{\"isEnabled\":false,\"publicKey\":\"\",\"shouldRenderParsely\":true},\"sentry\":{\"dsn\":\"\"},\"shopifyConfiguration\":{},\"appConfig\":{\"brandSlug\":\"the-new-yorker\"},\"userPlatform\":{\"isDomainSigninSwitchEnabled\":false,\"siteCode\":\"NYR\",\"userPlatformProxy\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fapi\\u002Fup\",\"xClientID\":\"Verso-NewYorker\",\"filterBookmarkTypes\":[],\"federatedGraphqlUrl\":\"https:\\u002F\\u002Fgraphql.condenast.io\\u002Fgraphql\"},\"experiments\":{\"assignments\":[]},\"martechPlatform\":{\"isEnabled\":true,\"products\":[\"newyorker.com:basic\"],\"redirectURL\":\"\\u002F\",\"isAccessCookieEnabled\":true},\"admiral\":{\"enabled\":true,\"bundle\":\"!(function(o,_name){o[_name]=o[_name]||function $(){($.q=$.q||[]).push(arguments)},o[_name].v=o[_name].v||2;!(function(o,t,e,n,c,a){function f(n,c){(n=(function(t,e){try{if(e=(t=o.localStorage).getItem(\\\"_aQS01Q0IwOTY5NTEzOUM2MDU3MEU5M0VGREEtMTI\\\"))return JSON.parse(e).lgk||[];if((t.getItem(decodeURI(decodeURI('%257%36%253%34%25%361%256%33%25%33%31%256%35%69Z%25%37%320')))||\\\"\\\").split(\\\",\\\")[4]\\u003E0)return[[_name+\\\"-engaged\\\",\\\"true\\\"]]}catch(n){}})())&&typeof n.forEach===e&&(c=o[t].pubads())&&n.forEach((function(o){o&&o[0]&&c.setTargeting(o[0],o[1]||\\\"\\\")}))}try{(a=o[t]=o[t]||{}).cmd=a.cmd||[],typeof a.pubads===e?f():typeof a.cmd.unshift===e?a.cmd.unshift(f):a.cmd.push(f)}catch(i){}})(window,\\\"googletag\\\",\\\"function\\\");;})(window,decodeURI(decodeURI('a%25%36%34m%25%369r%61%25%36c')));!(function(t,c,i){i=t.createElement(c),t=t.getElementsByTagName(c)[0],i.async=1,i.src=\\\"https:\\u002F\\u002Fstopstomach.com\\u002F634_addd6761761dfe1a7e9ff59bd48a62399.js\\\",t.parentNode.insertBefore(i,t)})(document,\\\"script\\\");\"},\"beOp\":{},\"inlineRecirc\":[],\"linkList\":null,\"related\":[{\"id\":\"5911cbe9803aff0f1c1359c3\",\"contributors\":{\"author\":{\"items\":[{\"dangerousBio\":\"\\u003Ca href=\\\"\\u002Fcontributors\\u002Flarissa-macfarquhar\\\"\\u003ELarissa MacFarquhar\\u003C\\u002Fa\\u003E has been a staff writer at \\u003Cem\\u003EThe New Yorker\\u003C\\u002Fem\\u003E since 1998. She has written about \\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2017\\u002F08\\u002F07\\u002Fwhen-should-a-child-be-taken-from-his-parents\\\"\\u003Echild-protective services\\u003C\\u002Fa\\u003E, the \\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2019\\u002F08\\u002F19\\u002Fthe-radical-transformations-of-a-battered-womens-shelter\\\"\\u003Ebattered-women’s movement\\u003C\\u002Fa\\u003E, \\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2018\\u002F10\\u002F08\\u002Fthe-comforting-fictions-of-dementia-care\\\"\\u003Edementia\\u003C\\u002Fa\\u003E, and \\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2016\\u002F07\\u002F11\\u002Fthe-work-of-a-hospice-nurse\\\"\\u003Ehospice care\\u003C\\u002Fa\\u003E, and her Profile subjects have included John Ashbery, Barack Obama, Noam Chomsky, Hilary Mantel, Derek Parfit, David Chang, and Aaron Swartz, among many others.... \\u003Ca href=\\\"\\u002Fcontributors\\u002Flarissa-macfarquhar\\\"\\u003ERead more\\u003C\\u002Fa\\u003E\",\"dangerousTitle\":\"\\u003Ca href=\\\"\\u002Fcontributors\\u002Flarissa-macfarquhar\\\"\\u003ELarissa MacFarquhar\\u003C\\u002Fa\\u003E, a staﬀ writer at The New Yorker, is the author of “\\u003Ca href=\\\"https:\\u002F\\u002Fwww.amazon.com\\u002Fdp\\u002F0143109782\\u002F\\\"\\u003EStrangers Drowning: Impossible Idealism, Drastic Choices, and the Urge to Help\\u003C\\u002Fa\\u003E.”\",\"name\":\"Larissa MacFarquhar\",\"photo\":{\"altText\":\"\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F59097b842179605b11ad8f2a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"socialMedia\":[{\"handle\":\"LarissaMacFarqu\",\"network\":\"twitter\",\"label\":\"Follow Larissa MacFarquhar on X\"}],\"url\":\"\\u002Fcontributors\\u002Flarissa-macfarquhar\"}]}},\"contentType\":\"article\",\"copilotID\":\"5911cbe9803aff0f1c1359c3\",\"contentId\":\"5911cbb2803aff0f1c1359ac\",\"dangerousHed\":\"What Money Can Buy\",\"dangerousDek\":\"\",\"date\":\"\",\"pubDate\":\"\",\"issueDate\":\"\",\"image\":{\"altText\":\"Darren Walker the Ford headquarters\",\"caption\":\"Walker at the Ford headquarters. “In the sixties, when you came to see the president,” he says, “it was meant to be intimidating.”\",\"contentType\":\"photo\",\"credit\":\"Photograph by Andrew Moore for The New Yorker\",\"id\":\"5909730c1c7a8e33fb38f08c\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_720,c_limit\\u002F160104_r27491.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_120,c_limit\\u002F160104_r27491.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_240,c_limit\\u002F160104_r27491.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_320,c_limit\\u002F160104_r27491.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_640,c_limit\\u002F160104_r27491.jpg 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_720,c_limit\\u002F160104_r27491.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_120,c_limit\\u002F160104_r27491.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_240,c_limit\\u002F160104_r27491.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_320,c_limit\\u002F160104_r27491.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_640,c_limit\\u002F160104_r27491.jpg 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_480,c_limit\\u002F160104_r27491.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_120,c_limit\\u002F160104_r27491.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_240,c_limit\\u002F160104_r27491.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_320,c_limit\\u002F160104_r27491.jpg 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_480,c_limit\\u002F160104_r27491.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_120,c_limit\\u002F160104_r27491.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_240,c_limit\\u002F160104_r27491.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_320,c_limit\\u002F160104_r27491.jpg 320w\"}},\"masterAspectRatio\":\"1848:2560\",\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_720,c_limit\\u002F160104_r27491.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_120,c_limit\\u002F160104_r27491.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_240,c_limit\\u002F160104_r27491.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_320,c_limit\\u002F160104_r27491.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_640,c_limit\\u002F160104_r27491.jpg 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_480,c_limit\\u002F160104_r27491.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_120,c_limit\\u002F160104_r27491.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_240,c_limit\\u002F160104_r27491.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F5909730c1c7a8e33fb38f08c\\u002F4:3\\u002Fw_320,c_limit\\u002F160104_r27491.jpg 320w\"}]}},\"imageLabels\":[],\"isSponsored\":false,\"rubric\":{\"name\":\"Profiles\"},\"signage\":null,\"hasNoFollowOnSyndicated\":false,\"source\":{\"hed\":\"What Money Can Buy\",\"dek\":\"\"},\"showAssetOnly\":false,\"showLinkedAsset\":false,\"url\":\"\\u002Fmagazine\\u002F2016\\u002F01\\u002F04\\u002Fwhat-money-can-buy-profiles-larissa-macfarquhar\",\"functionalTags\":[]},{\"id\":\"5911421bf8fa3e67faf0daf2\",\"contributors\":{\"author\":{\"items\":[{\"dangerousBio\":\"\\u003Ca href=\\\"\\u002Fcontributors\\u002Fburkhard-bilger\\\"\\u003EBurkhard Bilger\\u003C\\u002Fa\\u003E has been a staff writer at \\u003Cem\\u003EThe New Yorker\\u003C\\u002Fem\\u003E since 2001. His pieces have included portraits of gem dealers in Madagascar, ginseng poachers in the Appalachians, deep-cave divers in Mexico, and a cheese-making nun in Connecticut. His work has also appeared in the New York \\u003Cem\\u003ETimes\\u003C\\u002Fem\\u003E, \\u003Cem\\u003EThe Atlantic\\u003C\\u002Fem\\u003E,... \\u003Ca href=\\\"\\u002Fcontributors\\u002Fburkhard-bilger\\\"\\u003ERead more\\u003C\\u002Fa\\u003E\",\"dangerousTitle\":\"\\u003Ca href=\\\"\\u002Fcontributors\\u002Fburkhard-bilger\\\"\\u003EBurkhard Bilger\\u003C\\u002Fa\\u003E has been a staff writer at \\u003Cem\\u003EThe New Yorker\\u003C\\u002Fem\\u003E since 2001. His new book, “\\u003Ca href=\\\"https:\\u002F\\u002Fwww.amazon.com\\u002FFatherland-Memoir-Conscience-Family-Secrets\\u002Fdp\\u002F0385353987\\u002Fref=sr_1_1?crid=3J90ULRM45BOS&amp;keywords=fatherland+burkhard+bilger&amp;qid=1679886492&amp;s=books&amp;sprefix=fatherland+burk%2Cstripbooks%2C126&amp;sr=1-1\\\"\\u003EFatherland\\u003C\\u002Fa\\u003E,” was published in 2023.\",\"name\":\"Burkhard Bilger\",\"photo\":{\"altText\":\"\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F594873131be78c4ce215b7c8\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"socialMedia\":[],\"url\":\"\\u002Fcontributors\\u002Fburkhard-bilger\"}]}},\"contentType\":\"article\",\"copilotID\":\"5911421bf8fa3e67faf0daf2\",\"contentId\":\"5911cbb2803aff0f1c1359ac\",\"dangerousHed\":\"The Great Oasis\",\"dangerousDek\":\"\",\"date\":\"\",\"pubDate\":\"\",\"issueDate\":\"\",\"image\":{\"altText\":\"About a third of all land on earth has been claimed by desert—almost twenty million square miles—and the percentage is...\",\"caption\":\"About a third of all land on earth has been claimed by desert&#8212;almost twenty million square miles&#8212;and the percentage is likely to increase with global warming. “The desert always menaces,” the French botanist André Aubréville warned.\",\"contentType\":\"photo\",\"credit\":\"Photograph by George Steinmetz\",\"id\":\"590967e32179605b11ad65e8\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_720,c_limit\\u002F111219_r21628_g2048.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_120,c_limit\\u002F111219_r21628_g2048.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_240,c_limit\\u002F111219_r21628_g2048.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_320,c_limit\\u002F111219_r21628_g2048.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_640,c_limit\\u002F111219_r21628_g2048.jpg 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_720,c_limit\\u002F111219_r21628_g2048.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_120,c_limit\\u002F111219_r21628_g2048.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_240,c_limit\\u002F111219_r21628_g2048.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_320,c_limit\\u002F111219_r21628_g2048.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_640,c_limit\\u002F111219_r21628_g2048.jpg 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_480,c_limit\\u002F111219_r21628_g2048.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_120,c_limit\\u002F111219_r21628_g2048.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_240,c_limit\\u002F111219_r21628_g2048.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_320,c_limit\\u002F111219_r21628_g2048.jpg 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_480,c_limit\\u002F111219_r21628_g2048.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_120,c_limit\\u002F111219_r21628_g2048.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_240,c_limit\\u002F111219_r21628_g2048.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_320,c_limit\\u002F111219_r21628_g2048.jpg 320w\"}},\"masterAspectRatio\":\"2048:1328\",\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_720,c_limit\\u002F111219_r21628_g2048.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_120,c_limit\\u002F111219_r21628_g2048.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_240,c_limit\\u002F111219_r21628_g2048.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_320,c_limit\\u002F111219_r21628_g2048.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_640,c_limit\\u002F111219_r21628_g2048.jpg 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_480,c_limit\\u002F111219_r21628_g2048.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_120,c_limit\\u002F111219_r21628_g2048.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_240,c_limit\\u002F111219_r21628_g2048.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590967e32179605b11ad65e8\\u002F4:3\\u002Fw_320,c_limit\\u002F111219_r21628_g2048.jpg 320w\"}]}},\"imageLabels\":[],\"isSponsored\":false,\"rubric\":{\"name\":\"A Reporter at Large\"},\"signage\":null,\"hasNoFollowOnSyndicated\":false,\"source\":{\"hed\":\"The Great Oasis\",\"dek\":\"\"},\"showAssetOnly\":false,\"showLinkedAsset\":false,\"url\":\"\\u002Fmagazine\\u002F2011\\u002F12\\u002F19\\u002Fthe-great-oasis\",\"functionalTags\":[]}],\"seriesData\":null,\"showFirstRailRecirc\":true,\"taboola\":{},\"productCarousel\":{},\"presenterResponseHeaders\":{\"surrogate-key\":\"\"},\"metadataVideo\":{},\"head.pinterest.image\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002F16:9\\u002Fw_1280,c_limit\\u002F151123_r27342.jpg\",\"communityExperience\":{\"communityDefaultSort\":{},\"enableCommunityExperience\":false,\"hasHideCommunityFunctionalTag\":false,\"hasShowCommunityFunctionalTag\":false},\"commentAttributes\":{\"organizationId\":\"4gKgcFDnpSvUqozcC7TYUEcCiDJv\",\"commentingUrl\":\"\",\"tenantID\":\"\"},\"layoutConfigs\":{},\"article\":{\"id\":\"5911cbb2803aff0f1c1359ac\",\"body\":[\"div\",[\"h2\",\"I. Omens\"],[\"p\",{\"class\":\"paywall\"},\"Last year, a curious nonfiction book became a \",[\"em\",\"Times\"],\" best-seller: a dense meditation on artificial intelligence by the philosopher Nick Bostrom, who holds an appointment at Oxford. Titled “Superintelligence: Paths, Dangers, Strategies,” it argues that true artificial intelligence, if it is realized, might pose a danger that exceeds every previous threat from technology—even nuclear weapons—and that if its development is not managed carefully humanity risks engineering its own extinction. Central to this concern is the prospect of an “intelligence explosion,” a speculative event in which an A.I. gains the ability to improve itself, and in short order exceeds the intellectual potential of the human brain by many orders of magnitude.\"],[\"cm-unit\"],[\"p\",{\"class\":\"paywall\"},\"Such a system would effectively be a new kind of life, and Bostrom’s fears, in their simplest form, are evolutionary: that humanity will unexpectedly become outmatched by a smarter competitor. He sometimes notes, as a point of comparison, the trajectories of people and gorillas: both primates, but with one species dominating the planet and the other at the edge of annihilation. “Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb,” he concludes. “We have little idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound.”\"],[\"p\",{\"class\":\"paywall\"},\"At the age of forty-two, Bostrom has become a philosopher of remarkable influence. “Superintelligence” is only his most visible response to ideas that he encountered two decades ago, when he became a transhumanist, joining a fractious quasi-utopian movement united by the expectation that accelerating advances in technology will result in drastic changes—social, economic, and, most strikingly, biological—which could converge at a moment of epochal transformation known as the Singularity. Bostrom is arguably the leading transhumanist philosopher today, a position achieved by bringing order to ideas that might otherwise never have survived outside the half-crazy Internet ecosystem where they formed. He rarely makes concrete predictions, but, by relying on probability theory, he seeks to tease out insights where insights seem impossible.\"],[\"p\",{\"class\":\"paywall\"},\"Some of Bostrom’s cleverest arguments resemble Swiss Army knives: they are simple, toylike, a pleasure to consider, with colorful exteriors and precisely calibrated mechanics. He once cast a moral case for medically engineered immortality as a fable about a kingdom terrorized by an insatiable dragon. A reformulation of Pascal’s wager became a dialogue between the seventeenth-­century philosopher and a mugger from another dimension.\"],[\"p\",{\"class\":\"paywall\"},\"“Superintelligence” is not intended as a treatise of deep originality; Bostrom’s contribution is to impose the rigors of analytic philosophy on a messy corpus of ideas that emerged at the margins of academic thought. Perhaps because the field of A.I. has recently made striking advances—with everyday technology seeming, more and more, to exhibit something like intelligent reasoning—the book has struck a nerve. Bostrom’s supporters compare it to “Silent Spring.” In moral philosophy, Peter Singer and Derek Parfit have received it as a work of importance, and distinguished physicists such as Stephen Hawking have echoed its warning. Within the high caste of Silicon Valley, Bostrom has acquired the status of a sage. Elon Musk, the C.E.O. of Tesla, promoted the book on Twitter, noting, “We need to be super careful with AI. Potentially more dangerous than nukes.” Bill Gates recommended it, too. Suggesting that an A.I. could threaten humanity, he said, during a talk in China, “When people say it’s not a problem, then I really start to get to a point of disagreement. How can they not see what a huge challenge this is?”\"],[\"p\",{\"class\":\"paywall\"},\"The people who say that artificial intelligence is not a problem tend to work in artificial intelligence. Many prominent researchers regard Bostrom’s basic views as implausible, or as a distraction from the near-term benefits and moral dilemmas posed by the technology—not least because A.I. systems today can barely guide robots to open doors. Last summer, Oren Etzioni, the C.E.O. of the Allen Institute for Artificial Intelligence, in Seattle, referred to the fear of machine intelligence as a “Frankenstein complex.” Another leading researcher declared, “I don’t worry about that for the same reason I don’t worry about overpopulation on Mars.” Jaron Lanier, a Microsoft researcher and tech commentator, told me that even framing the differing views as a debate was a mistake. “This is not an honest conversation,” he said. “People think it is about technology, but it is really about religion, people turning to metaphysics to cope with the human condition. They have a way of dramatizing their beliefs with an end-of-days scenario—and one does not want to criticize other people’s religions.”\"],[\"p\",{\"class\":\"paywall\"},\"Because the argument has played out on blogs and in the popular press, beyond the ambit of peer-reviewed journals, the two sides have appeared in caricature, with headlines suggesting either doom (“\",[\"em\",{\"class\":\"small\"},\"Will Super-intelligent Machines Kill Us All\"],\"?”) or a reprieve from doom (“\",[\"em\",{\"class\":\"small\"},\"Artificial intelligence\"],\" ‘\",[\"em\",{\"class\":\"small\"},\"will not end human race\"],\"’ ”). Even the most grounded version of the debate occupies philosophical terrain where little is clear. But, Bostrom argues, if artificial intelligence can be achieved it would be an event of unparalleled consequence—perhaps even a rupture in the fabric of history. A bit of long-range forethought might be a moral obligation to our own species.\"],[\"p\",{\"class\":\"paywall\"},\"Bostrom’s sole responsibility at Oxford is to direct an organization called the Future of Humanity Institute, which he founded ten years ago, with financial support from James Martin, a futurist and tech millionaire. Bostrom runs the institute as a kind of philosophical radar station: a bunker sending out navigational pulses into the haze of possible futures. Not long ago, an F.H.I. fellow studied the possibility of a “dark fire scenario,” a cosmic event that, he hypothesized, could occur under certain high-energy conditions: everyday matter mutating into dark matter, in a runaway process that could erase most of the known universe. (He concluded that it was highly unlikely.) Discussions at F.H.I. range from conventional philosophic topics, like the nature of compromise, to the optimal structure of space empires—whether a single intergalactic machine intelligence, supported by a vast array of probes, presents a more ethical future than a cosmic imperium housing millions of digital minds.\"],[\"ad\",{\"position\":\"mid-content\"}],[\"p\",{\"class\":\"paywall\"},\"Earlier this year, I visited the institute, which is situated on a winding street in a part of Oxford that is a thousand years old. It takes some work to catch Bostrom at his office. Demand for him on the lecture circuit is high; he travels overseas nearly every month to relay his technological omens in a range of settings, from Google’s headquarters to a Presidential commission in Washington. Even at Oxford, he maintains an idiosyncratic schedule, remaining in the office until two in the morning and returning sometime the next afternoon.\"],[\"p\",{\"class\":\"paywall\"},\"I arrived before he did, and waited in a hallway between two conference rooms. A plaque indicated that one of them was the Arkhipov Room, honoring Vasili Arkhipov, a Soviet naval officer. During the Cuban missile crisis, Arkhipov was serving on a submarine in the Caribbean when U.S. destroyers set off depth charges nearby. His captain, unable to establish radio contact with Moscow, feared that the conflict had escalated and ordered a nuclear strike. But Arkhipov dissuaded him, and all-out atomic war was averted. Across the hallway was the Petrov Room, named for another Soviet officer who prevented a global nuclear catastrophe. Bostrom later told me, “They may have saved more lives than most of the statesmen we celebrate on stamps.”\"],[\"inline-embed\",{\"type\":\"cneinterlude\",\"props\":{\"brand\":\"newyorker\",\"embeddedVideos\":[],\"hasExcludedEmbed\":false,\"playerBase\":\"https:\\u002F\\u002Fplayer.cnevids.com\",\"isRightRail\":false,\"relatedVideo\":{\"showRelatedVideo\":false,\"brand\":\"newyorker\",\"related\":{\"hed\":\"What Money Can Buy\",\"id\":\"5911cbe9803aff0f1c1359c3\",\"inlineEmbeds\":[],\"metadata\":{\"contentType\":\"article\"},\"locationRubric\":{},\"channelRubric\":{},\"toutMedia\":{\"node\":{\"altText\":\"Darren Walker the Ford headquarters\",\"aspectRatios\":[{\"name\":\"2:1\",\"url\":null,\"width\":1847,\"height\":923,\"format\":null,\"modifications\":{\"crop\":{\"height\":923,\"width\":1847,\"x\":0,\"y\":1264}}},{\"name\":\"2:2\",\"url\":null,\"width\":1847,\"height\":1847,\"format\":null,\"modifications\":{\"crop\":{\"height\":1847,\"width\":1847,\"x\":0,\"y\":481}}},{\"name\":\"16:9\",\"url\":null,\"width\":1839,\"height\":1034,\"format\":null,\"modifications\":{\"crop\":{\"height\":1034,\"width\":1839,\"x\":0,\"y\":1137}}},{\"name\":\"4:3\",\"url\":null,\"width\":1847,\"height\":1385,\"format\":null,\"modifications\":{\"crop\":{\"height\":1385,\"width\":1847,\"x\":0,\"y\":1174}}},{\"name\":\"1:1\",\"url\":null,\"width\":1847,\"height\":1847,\"format\":null,\"modifications\":{\"crop\":{\"height\":1847,\"width\":1847,\"x\":0,\"y\":712}}},{\"name\":\"master\",\"url\":\"https:\\u002F\\u002Fgp-prd-global-clips-s3.s3.amazonaws.com\\u002Fpublic\\u002Ftny-services\\u002Fproduction\\u002F2017\\u002F05\\u002F03\\u002F5909730c1c7a8e33fb38f08b_160104_r27491.jpg\",\"width\":1848,\"height\":2560,\"format\":\"JPEG\",\"modifications\":null}],\"caption\":\"Walker at the Ford headquarters. “In the sixties, when you came to see the president,” he says, “it was meant to be intimidating.”\",\"contextualBody\":null,\"contextualCaption\":null,\"contextualTitle\":null,\"cropMode\":\"TOP\",\"credit\":\"Photograph by Andrew Moore for The New Yorker\",\"filename\":\"160104_r27491.jpg\",\"id\":\"5909730c1c7a8e33fb38f08c\",\"metadata\":{\"contentType\":\"photo\"},\"contentWarnings\":[],\"restrictCropping\":false,\"__typename\":\"Photo\"}},\"contributors\":{\"author\":[{\"__typename\":\"Contributor\",\"id\":\"590a18e98b51cf59fc424780\",\"url\":\"contributors\\u002Flarissa-macfarquhar\",\"name\":\"Larissa MacFarquhar\",\"title\":\"Larissa MacFarquhar, a staﬀ writer at The New Yorker, is the author of “[Strangers Drowning: Impossible Idealism, Drastic Choices, and the Urge to Help](https:\\u002F\\u002Fwww.amazon.com\\u002Fdp\\u002F0143109782\\u002F).”\",\"type\":\"AUTHOR\",\"contributorType\":\"AUTHOR\",\"photo\":{\"id\":\"59097b842179605b11ad8f2a\",\"aspectRatios\":[{\"url\":\"https:\\u002F\\u002Fgp-prd-global-clips-s3.s3.amazonaws.com\\u002Fpublic\\u002Ftny-services\\u002Fproduction\\u002F2017\\u002F05\\u002F03\\u002F59097b832179605b11ad8f29_macfarquhar-larissa.png\"}],\"metadata\":{\"contentType\":\"photo\"}},\"bio\":\"Larissa MacFarquhar has been a staff writer at *The New Yorker* since 1998. She has written about [child-protective services](https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2017\\u002F08\\u002F07\\u002Fwhen-should-a-child-be-taken-from-his-parents), the [battered-women’s movement](https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2019\\u002F08\\u002F19\\u002Fthe-radical-transformations-of-a-battered-womens-shelter), [dementia](https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2018\\u002F10\\u002F08\\u002Fthe-comforting-fictions-of-dementia-care), and [hospice care](https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2016\\u002F07\\u002F11\\u002Fthe-work-of-a-hospice-nurse), and her Profile subjects have included John Ashbery, Barack Obama, Noam Chomsky, Hilary Mantel, Derek Parfit, David Chang, and Aaron Swartz, among many others. She is the author of “[Strangers Drowning: Impossible Idealism, Drastic Choices, and the Urge to Help](https:\\u002F\\u002Fwww.amazon.com\\u002Fdp\\u002F0143109782\\u002F?ots=1&slotNum=0&imprToken=c2bfa32c-e999-0d10-a60&tag=thneyo0f-20).” Before joining the magazine, she was a senior editor at *Lingua Franca* and an advisory editor at *The Paris Review*, and wrote for *Artforum*, *The Nation*, *The New Republic*, the *Times Book Review*, Slate, and other publications. She has received two Front Page Awards from the Newswomen’s Club of New York and the Johnson & Johnson Excellence in Media Award. Her writing has appeared in “The Best American Political Writing” and “The Best American Food Writing.”\",\"socialMedia\":[{\"handle\":\"LarissaMacFarqu\",\"network\":\"Twitter\"}],\"metadata\":{\"contentType\":\"contributor\"}}]},\"channels\":[{\"name\":\"Profiles\"}],\"_embedded\":{\"_categories\":{\"channels\":[{\"name\":\"Profiles\"}]},\"publishHistory\":{}},\"photos\":{\"lede\":[{\"altText\":\"Darren Walker the Ford headquarters\",\"aspectRatios\":{\"2:1\":{\"url\":null,\"width\":1847,\"height\":923,\"format\":null},\"2:2\":{\"url\":null,\"width\":1847,\"height\":1847,\"format\":null},\"16:9\":{\"url\":null,\"width\":1839,\"height\":1034,\"format\":null},\"4:3\":{\"url\":null,\"width\":1847,\"height\":1385,\"format\":null},\"1:1\":{\"url\":null,\"width\":1847,\"height\":1847,\"format\":null},\"master\":{\"url\":\"https:\\u002F\\u002Fgp-prd-global-clips-s3.s3.amazonaws.com\\u002Fpublic\\u002Ftny-services\\u002Fproduction\\u002F2017\\u002F05\\u002F03\\u002F5909730c1c7a8e33fb38f08b_160104_r27491.jpg\",\"width\":1848,\"height\":2560,\"format\":\"JPEG\"}},\"caption\":\"Walker at the Ford headquarters. “In the sixties, when you came to see the president,” he says, “it was meant to be intimidating.”\",\"cropMode\":\"TOP\",\"credit\":\"Photograph by Andrew Moore for The New Yorker\",\"filename\":\"160104_r27491.jpg\",\"id\":\"5909730c1c7a8e33fb38f08c\",\"metadata\":{\"contentType\":\"photo\"},\"contentWarnings\":[],\"__typename\":\"Photo\",\"collection\":\"photos\",\"modelName\":\"photo\",\"meta\":{\"modelName\":\"photo\"}}],\"tout\":[{\"altText\":\"Darren Walker the Ford headquarters\",\"aspectRatios\":{\"2:1\":{\"url\":null,\"width\":1847,\"height\":923,\"format\":null},\"2:2\":{\"url\":null,\"width\":1847,\"height\":1847,\"format\":null},\"16:9\":{\"url\":null,\"width\":1839,\"height\":1034,\"format\":null},\"4:3\":{\"url\":null,\"width\":1847,\"height\":1385,\"format\":null},\"1:1\":{\"url\":null,\"width\":1847,\"height\":1847,\"format\":null},\"master\":{\"url\":\"https:\\u002F\\u002Fgp-prd-global-clips-s3.s3.amazonaws.com\\u002Fpublic\\u002Ftny-services\\u002Fproduction\\u002F2017\\u002F05\\u002F03\\u002F5909730c1c7a8e33fb38f08b_160104_r27491.jpg\",\"width\":1848,\"height\":2560,\"format\":\"JPEG\"}},\"caption\":\"Walker at the Ford headquarters. “In the sixties, when you came to see the president,” he says, “it was meant to be intimidating.”\",\"cropMode\":\"TOP\",\"credit\":\"Photograph by Andrew Moore for The New Yorker\",\"filename\":\"160104_r27491.jpg\",\"id\":\"5909730c1c7a8e33fb38f08c\",\"metadata\":{\"contentType\":\"photo\"},\"contentWarnings\":[],\"__typename\":\"Photo\",\"collection\":\"photos\",\"modelName\":\"photo\",\"meta\":{\"modelName\":\"photo\"}}]},\"categories\":{\"channels\":[{\"name\":\"Profiles\"}]},\"savingsUnitedCoupons\":[],\"url\":\"magazine\\u002F2016\\u002F01\\u002F04\\u002Fwhat-money-can-buy-profiles-larissa-macfarquhar\",\"modelName\":\"article\",\"meta\":{\"modelName\":\"article\"}},\"pageSize\":4}}}],[\"p\",{\"class\":\"paywall\"},\"The sense that a vanguard of technical-minded people working in obscurity, at odds with consensus, might save the world from auto-annihilation runs through the atmosphere at F.H.I. like an electrical charge. While waiting for Bostrom, I peered through a row of windows into the Arkh­ipov Room, which looked as though it was used for both meetings and storage; on a bookcase there were boxes containing light bulbs, lampshades, cables, spare mugs. A gaunt philosophy Ph.D. wrapped in a thick knitted cardigan was pacing in front of a whiteboard covered in notation, which he attacked in bursts. After each paroxysm, he paced, hands behind his back, head tilted downward. At one point, he erased a panel of his work. Taking this as an opportunity to interrupt, I asked him what he was doing. “It is a problem involving an aspect of A.I. called ‘planning,’ ” he said. His demeanor radiated irritation. I left him alone.\"],[\"p\",{\"class\":\"paywall\"},\"Bostrom arrived at 2 \",[\"em\",{\"class\":\"small\"},\"p.m\"],\". He has a boyish countenance and the lean, vital physique of a yoga instructor—though he could never be mistaken for a yoga instructor. His intensity is too untidily contained, evident in his harried gait on the streets outside his office (he does not drive), in his voracious consumption of audiobooks (played at two or three times the normal speed, to maximize efficiency), and his fastidious guarding against illnesses (he avoids handshakes and wipes down silverware beneath a tablecloth). Bostrom can be stubborn about the placement of an office plant or the choice of a font. But when his arguments are challenged he listens attentively, the mechanics of consideration nearly dis­cernible beneath his skin. Then, calmly, quickly, he dispatches a response, one idea interlocked with another.\"],[\"p\",{\"class\":\"paywall\"},\"He asked if I wanted to go to the market. “You can watch me make my elixir,” he said. For the past year or so, he has been drinking his lunch (another efficiency): a smoothie containing fruits, vegetables, proteins, and fats. Using his elbow, he hit a button that electronically opened the front door. Then we rushed out.\"],[\"p\",{\"class\":\"paywall\"},\"Bostrom has a reinvented man’s sense of lost time. An only child, he grew up—as Niklas Boström—in Helsingborg, on the southern coast of Sweden. Like many exceptionally bright children, he hated school, and as a teen-ager he developed a listless, romantic persona. In 1989, he wandered into a library and stumbled onto an anthology of nineteenth-century German philosophy, containing works by Nietzsche and Schopenhauer. He read it in a nearby forest, in a clearing that he often visited to think and to write poetry, and experienced a euphoric insight into the possibilities of learning and achievement. “It’s hard to convey in words what that was like,” Bostrom told me; instead he sent me a photograph of an oil painting that he had made shortly afterward. It was a semi-representational landscape, with strange figures crammed into dense undergrowth; beyond, a hawk soared below a radiant sun. He titled it “The First Day.”\"],[\"p\",{\"class\":\"paywall\"},\"Deciding that he had squandered his early life, he threw himself into a campaign of self-education. He ran down the citations in the anthology, branching out into art, literature, science. He says that he was motivated not only by curiosity but also by a desire for actionable knowledge about how to live. To his parents’ dismay, Bostrom insisted on finishing his final year of high school from home by taking special exams, which he completed in ten weeks. He grew distant from old friends: “I became quite fanatical and felt quite isolated for a period of time.”\"],[\"p\",{\"class\":\"paywall\"},\"When Bostrom was a graduate student in Stockholm, he studied the work of the analytic philosopher W. V. Quine, who had explored the difficult relationship between language and reality. His adviser drilled precision into him by scribbling “not clear” throughout the margins of his papers. “It was basically his only feedback,” Bostrom told me. “The effect was still, I think, beneficial.” His previous academic interests had ranged from psychology to mathematics; now he took up theoretical physics. He was fascinated by technology. The World Wide Web was just emerging, and he began to sense that the heroic philosophy which had inspired him might be outmoded. In 1995, Bostrom wrote a poem, “Requiem,” which he told me was “a signing-off letter to an earlier self.” It was in Swedish, so he offered me a synopsis: “I describe a brave general who has overslept and finds his troops have left the encampment. He rides off to catch up with them, pushing his horse to the limit. Then he hears the thunder of a modern jet plane streaking past him across the sky, and he realizes that he is obsolete, and that courage and spiritual nobility are no match for machines.”\"],[\"inline-embed\",{\"props\":{\"name\":\"inset-left\",\"attrs\":{},\"childTypes\":[\"cartoon\"]},\"ref\":\"\",\"type\":\"callout:inset-left\"},[\"inline-embed\",{\"props\":{\"image\":{\"id\":\"593b649e7206a168e2d17827\",\"altText\":\"“Im starting a startup that helps other startups start up.”\",\"contentType\":\"cartoon\",\"sources\":{\"sm\":{\"width\":640,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19497.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19497.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19497.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19497.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19497.jpg 640w\",\"height\":464},\"md\":{\"width\":768,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_768,c_limit\\u002F151123_a19497.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19497.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19497.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19497.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19497.jpg 640w\",\"height\":557},\"lg\":{\"width\":1024,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_1024,c_limit\\u002F151123_a19497.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19497.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19497.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19497.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19497.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19497.jpg 960w\",\"height\":743},\"xl\":{\"width\":1280,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19497.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19497.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19497.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19497.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19497.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19497.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19497.jpg 1280w\",\"height\":929},\"xxl\":{\"width\":1600,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_a19497.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19497.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19497.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19497.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19497.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19497.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19497.jpg 1280w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b649e7206a168e2d17827\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_a19497.jpg 1600w\",\"height\":1161}},\"dangerousCaption\":\"“I’m starting a startup that helps other startups start up.”\",\"dangerousCredit\":\"\",\"links\":[{\"network\":\"facebook\",\"behavior\":\"popup\",\"url\":\"https:\\u002F\\u002Fwww.facebook.com\\u002Fdialog\\u002Ffeed?&display=popup&caption=&app_id=1147169538698836&link=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19497%3Futm_source%3Dfacebook%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned\",\"label\":\"Share on Facebook\"},{\"network\":\"twitter\",\"url\":\"https:\\u002F\\u002Ftwitter.com\\u002Fintent\\u002Ftweet\\u002F?url=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19497%3Futm_source%3Dtwitter%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned&text=&via=NewYorker\",\"label\":\"Share on X\"},{\"network\":\"email\",\"url\":\"mailto:?subject=&body=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19497%3Futm_source%3Donsite-share%26utm_medium%3Demail%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker\",\"label\":\"Share via Email\"},{\"network\":\"store\",\"url\":\"https:\\u002F\\u002Fcondenaststore.com\\u002Fconde-nast-brand\\u002Fcartoons\",\"label\":\"Get in store\"},{\"network\":\"canonical\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcartoon\\u002Fa19497\",\"label\":\"a19497\"}]}},\"level\":\"block\",\"ref\":\"593b649e7206a168e2d17827\",\"type\":\"cartoon\"}]],[\"p\",{\"class\":\"paywall\"},\"Although Bostrom did not know it, a growing number of people around the world shared his intuition that technology could cause transformative change, and they were finding one another in an online discussion group administered by an organization in California called the Extropy Institute. The term “extropy,” coined in 1967, is generally used to describe life’s capacity to reverse the spread of entropy across space and time. Extropianism is a libertarian strain of transhumanism that seeks “to direct human evolution,” hoping to eliminate disease, suffering, even death; the means might be genetic modification, or as yet un­invented nanotechnology, or perhaps dispensing with the body entirely and uploading minds into supercomputers. (As one member noted, “Immortality is mathematical, not mystical.”) The Extropians advocated the development of artificial superintelligence to achieve these goals, and they envisioned humanity colonizing the universe, converting inert matter into engines of civilization. The discussions were nerdy, lunatic, imaginative, thought-provoking. Anders Sandberg, a former member of the group who now works at Bostrom’s institute, told me, “Just imagine if you could listen in on the debates of the Italian Futurists or early Surrealists.”\"],[\"ad\",{\"position\":\"mid-content\"}],[\"p\",{\"class\":\"paywall\"},\"In 1996, while pursuing further graduate work at the London School of Economics, Bostrom learned about the Extropy discussion group and became an active participant. A year later, he co-founded his own organization, the World Transhumanist Association, which was less libertarian and more academically spirited. He crafted approachable statements on transhumanist values and gave interviews to the BBC. The line between his academic work and his activism blurred: his Ph.D. dissertation centered on a study of the Doomsday Argument, which uses probability theory to make inferences about the longevity of human civilization. The work baffled his advisers, who respected him but rarely agreed with his conclusions. Mostly, they left him alone.\"],[\"p\",{\"class\":\"paywall\"},\"Bostrom had little interest in conventional philosophy—not least because he expected that superintelligent minds, whether biologically enhanced or digital, would make it obsolete. “Suppose you had to build a new subway line, and it was this grand trans-generational enterprise that humanity was engaged in, and everybody had a little role,” he told me. “So you have a little shovel. But if you know that a giant bulldozer will arrive on the scene tomorrow, then does it really make sense to spend your time today digging the big hole with your shovel? Maybe there is something else you could do with your time. Maybe you could put up a signpost for the great shovel, so it will start digging in the right place.” He came to believe that a key role of the philosopher in modern society was to acquire the knowledge of a polymath, then use it to help guide humanity to its next phase of existence—a discipline that he called “the philosophy of technological prediction.” He was trying to become such a seer.\"],[\"p\",{\"class\":\"paywall\"},\"“He was ultra-consistent,” Daniel Hill, a British philosopher who befriended Bostrom while they were graduate students in London, told me. “His interest in science was a natural outgrowing of his understandable desire to live forever, basically.”\"],[\"p\",{\"class\":\"paywall\"},\"Bostrom has written more than a hundred articles, and his longing for immortality can be seen throughout. In 2008, he framed an essay as a call to action from a future utopia. “Death is not one but a multitude of assassins,” he warned. “Take aim at the causes of early death—infection, violence, malnutrition, heart attack, cancer. Turn your biggest gun on aging, and fire. You must seize the biochemical processes in your body in order to vanquish, by and by, illness and senescence. In time, you will discover ways to move your mind to more durable media.” He tends to see the mind as immaculate code, the body as inefficient hardware—able to accommodate limited hacks but probably destined for replacement.\"],[\"p\",{\"class\":\"paywall\"},\"Even Bostrom’s marriage is largely mediated by technology. His wife, Susan, has a Ph.D. in the sociology of medicine and a bright, down-to-earth manner. (“She teases me about the Terminator and the robot army,” he told me.) They met thirteen years ago, and for all but six months they have lived on opposite sides of the Atlantic, even after the recent birth of their son. The arrangement is voluntary: she prefers Montreal; his work keeps him at Oxford. They Skype several times a day, and he directs as much international travel as possible through Canada, so they can meet in non-digital form.\"],[\"p\",{\"class\":\"paywall\"},\"In Oxford, as Bostrom shopped for his smoothie, he pointed out a man vaping. “There is also the more old-school method of taking nicotine: chewing gum,” he told me. “I do chew nicotine gum. I read a few papers saying it might have some nootropic effect”—that is, it might enhance cognition. He drinks coffee, and usually abstains from alcohol. He briefly experimented with the smart drug Modafinil, but gave it up.\"],[\"p\",{\"class\":\"paywall\"},\"Back at the institute, he filled an industrial blender with lettuce, carrots, cauliflower, broccoli, blueberries, turmeric, vanilla, oat milk, and whey powder. “If there is one thing Nick cares about, it is minds,” Sandberg told me. “That is at the root of many of his views about food, because he is worried that toxin X or Y might be bad for his brain.” He suspects that Bostrom also enjoys the ritualistic display. “Swedes are known for their smugness,” he joked. “Perhaps Nick is subsisting on smugness.”\"],[\"p\",{\"class\":\"paywall\"},\"A young employee eyed Bostrom getting ready to fire up the blender. “I can tell when Nick comes into the office,” he said. “My hair starts shaking.”\"],[\"p\",{\"class\":\"paywall\"},\"“Yeah, this has got three horsepower,” Bostrom said. He ran the blender, producing a noise like a circular saw, and then filled a tall glass stein with purple-­green liquid. We headed to his office, which was meticulous. By a window was a wooden desk supporting an iMac and not another item; against a wall were a chair and a cabinet with a stack of documents. The only hint of excess was light: there were fourteen lamps.\"],[\"p\",{\"class\":\"paywall\"},\"It is hard to spend time at Bostrom’s institute without drifting into reveries of a far future. What might humanity look like millions of years from now? The upper limit of survival on Earth is fixed to the life span of the sun, which in five billion years will become a red giant and swell to more than two hundred times its present size. It is possible that Earth’s orbit will adjust, but more likely that the planet will be destroyed. In any case, long before then, nearly all plant life will die, the oceans will boil, and the Earth’s crust will heat to a thousand degrees. In half a billion years, the planet will be uninhabitable.\"],[\"ad\",{\"position\":\"mid-content\"}],[\"p\",{\"class\":\"paywall\"},\"The view of the future from Bostrom’s office can be divided into three grand panoramas. In one, humanity experiences an evolutionary leap—either assisted by technology or by merging into it and becoming software—to achieve a sublime condition that Bostrom calls “posthumanity.” Death is overcome, mental experience expands beyond recognition, and our descendants colonize the universe. In another panorama, humanity becomes extinct or experiences a disaster so great that it is unable to recover. Between these extremes, Bostrom envisions scenarios that resemble the status quo—people living as they do now, forever mired in the “human era.” It’s a vision familiar to fans of sci-fi: on “Star Trek,” Captain Kirk was born in the year 2233, but when an alien portal hurls him through time and space to Depression-era Manhattan he blends in easily.\"],[\"p\",{\"class\":\"paywall\"},\"Bostrom dislikes science fiction. “I’ve never been keen on stories that just try to present ‘wow’ ideas—the equivalent of movie productions that rely on stunts and explosions to hold the attention,” he told me. “The question is not whether we can think of something radical or extreme but whether we can discover some sufficient reason for updating our credence function.”\"],[\"p\",{\"class\":\"paywall\"},\"He believes that the future can be studied with the same meticulousness as the past, even if the conclusions are far less firm. “It may be highly unpredictable where a traveller will be one hour after the start of her journey, yet predictable that after five hours she will be at her destination,” he once argued. “The \",[\"em\",\"very\"],\" long-term future of humanity may be relatively easy to predict.” He offers an example: if history were reset, the industrial revolution might occur at a different time, or in a different place, or perhaps not at all, with innovation instead occurring in increments over hundreds of years. In the short term, predicting technological achievements in the counter-history might not be possible; but after, say, a hundred thousand years it is easier to imagine that all the same inventions would have emerged.\"],[\"p\",{\"class\":\"paywall\"},\"Bostrom calls this the Technological Completion Conjecture: “If scientific- and technological-development efforts do not effectively cease, then all impor­t­­­ant basic capabilities that could be obtained through some possible technology will be obtained.” In light of this, he suspects that the farther into the future one looks the less likely it seems that life will continue as it is. He favors the far ends of possibility: humanity becomes transcendent or it perishes.\"],[\"p\",{\"class\":\"paywall\"},\"In the nineteen-nineties, as these ideas crystallized in his thinking, Bostrom began to give more attention to the question of extinction. He did not believe that doomsday was imminent. His interest was in risk, like an insurance agent’s. No matter how improbable extinction may be, Bostrom argues, its consequences are near-infinitely bad; thus, even the tiniest step toward reducing the \",[\"em\",\"chance\"],\" that it will happen is near-­infinitely valuable. At times, he uses arithmetical sketches to illustrate this point. Imagining one of his utopian scenarios—trillions of digital minds thriving across the cosmos—he reasons that, if there is even a one-per-cent chance of this happening, the expected value of reducing an existential threat by a billionth of a billionth of one per cent would be worth a hundred billion times the value of a billion present-day lives. Put more simply: he believes that his work could dwarf the moral importance of anything else.\"],[\"p\",{\"class\":\"paywall\"},\"Bostrom introduced the philosophical concept of “existential risk” in 2002, in the \",[\"em\",\"Journal of Evolution and Technology\"],\". In recent years, new organizations have been founded almost annually to help reduce it—among them the Centre for the Study of Existential Risk, affiliated with Cambridge Uni­versity, and the Future of Life Institute, which has ties to the Massachusetts Institute of Technology. All of them face a key problem: \",[\"em\",\"Homo sapiens\"],\", since its emergence two hundred thousand years ago, has proved to be remarkably resilient, and figuring out what might imperil its existence is not obvious. Climate change is likely to cause vast environmental and economic damage—but it does not seem impossible to survive. So-called super-volcanoes have thus far not threatened the perpetuation of the species. \",[\"em\",{\"class\":\"small\"},\"NASA\"],\" spends forty million dollars each year to determine if there are significant comets or asteroids headed for Earth. (There aren’t.)\"],[\"p\",{\"class\":\"paywall\"},\"Bostrom does not find the lack of obvious existential threats comforting. Because it is impossible to endure extinction twice, he argues, we cannot rely on history to calculate the probability that it will occur. The most worrying dangers are those that Earth has never encountered before. “It is hard to cause human extinction with seventeenth-century technology,” Bostrom told me. Three centuries later, though, the prospect of a technological apocalypse was urgently plausible. Bostrom dates the first scientific analysis of existential risk to the Manhattan Project: in 1942, Robert Oppenheimer became concerned that an atomic detonation of sufficient power could cause the entire atmosphere to ignite. A subsequent study concluded that the scenario was “unreasonable,” given the limitations of the weapons then in development. But even if the great nuclear nightmares of the Cold War did not come true, the tools were there to cause destruction on a scale not previously possible. As innovations grow even more complex, it is increasingly difficult to evaluate the dangers ahead. The answers must be fraught with ambiguity, because they can be derived only by predicting the effects of technologies that exist mostly as theories or, even more indirectly, by using abstract reasoning.\"],[\"ad\",{\"position\":\"mid-content\"}],[\"p\",{\"class\":\"paywall\"},\"As a philosopher, Bostrom takes a sweeping, even cosmic, view of such problems. One afternoon, he told me, “The probabilities that any given planet will produce intelligent life—this may also have action-relevant information.” In the past several years, \",[\"em\",{\"class\":\"small\"},\"NASA\"],\" probes have found increasing evidence that the building blocks of life are abundant throughout space. So much water has been discovered—on Mars and on the moons of Jupiter and Saturn—that one scientist described our solar system as “a pretty soggy place.” There are amino acids on icy comets and complex organic molecules in distant star-forming clouds. On this planet, life has proved capable of thriving in unimaginably punishing conditions: without oxygen, without light, at four hundred degrees above or below zero. In 2007, the European Space Agency hitched tiny creatures to the exterior of a satellite. They not only survived the flight; some even laid eggs afterward.\"],[\"p\",{\"class\":\"paywall\"},\"With ten billion Earth-like planets in our galaxy alone, and a hundred billion galaxies in the universe, there is good reason to suspect that extraterrestrial life may one day be discovered. For Bostrom, this would augur disaster. “It would be great news to find that Mars is a completely sterile planet,” he argued not long ago. “Dead rocks and lifeless sands would lift my spirits.” His reasoning begins with the age of the universe. Many of those Earth-like planets are thought to be far, far older than ours. One that was recently discovered, called Kepler 452b, is as much as one and a half billion years older. Bostrom asks: If life had formed there on a time scale resembling our own, what would it look like? What kind of technological progress could a civilization achieve with a head start of hundreds of millions of years?\"],[\"p\",{\"class\":\"paywall\"},\"Life as we know it tends to spread wherever it can, and Bostrom estimates that, if an alien civilization could design space probes capable of travelling at even one per cent of the speed of light, the entire Milky Way could be colonized in twenty million years—a tiny fraction of the age difference between Kepler 452b and Earth. One could argue that no technology will ever propel ships at so great a speed. Or perhaps millions of alien civilizations possess the know-how for intergalactic travel, but they aren’t interested. Even so, because the universe is so colossal, and because it is so old, only a small number of civilizations would need to behave as life does on Earth—unceasingly expanding—in order to be visible. Yet, as Bostrom notes, “You start with billions and billions of potential germination points for life, and you end up with a sum total of \",[\"em\",\"zero\"],\" alien civilizations that developed technologically to the point where they become manifest to us earthly observers. So what’s stopping them?”\"],[\"p\",{\"class\":\"paywall\"},\"In 1950, Enrico Fermi sketched a version of this paradox during a lunch break while he was working on the H-bomb, at Los Alamos. Since then, many resolutions have been proposed—some of them exotic, such as the idea that Earth is housed in an interplanetary alien zoo. Bostrom suspects that the answer is simple: space appears to be devoid of life because it is. This implies that intelligent life on Earth is an astronomically rare accident. But, if so, when did that accident occur? Was it in the first chemical reactions in the primordial soup? Or when single-celled organisms began to replicate using DNA? Or when animals learned to use tools? Bos­trom likes to think of these hurdles as Great Filters: key phases of improbability that life everywhere must pass through in order to develop into intelligent species. Those which do not make it either go extinct or fail to evolve.\"],[\"p\",{\"class\":\"paywall\"},\"Thus, for Bostrom, the discovery of a single-celled creature inhabiting a damp stretch of Martian soil would constitute a disconcerting piece of evidence. If two planets independently evolved primitive organisms, then it seems more likely that this type of life can be found on many planets throughout the universe. Bostrom reasons that this would suggest that the Great Filter comes at some later evolutionary stage. The discovery of a fossilized vertebrate would be even worse: it would suggest that the universe appears lifeless not because complex life is unusual but, rather, because it is always somehow thwarted before it becomes advanced enough to colonize space.\"],[\"p\",{\"class\":\"paywall\"},\"In Bostrom’s view, the most distressing possibility is that the Great Filter is ahead of us—that evolution frequently achieves civilizations like our own, but they perish before reaching their technological maturity. Why might that be? “Natural disasters such as asteroid hits and super-­volcanic eruptions are unlikely Great Filter candidates, because, even if they destroyed a significant number of civilizations, we would expect some civilizations to get lucky and escape disaster,” he argues. “Perhaps the most likely type of existential risks that could constitute a Great Filter are those that arise from technological discovery. It is not far-fetched to suppose that there might be some possible technology which is such that (a) virtually all suffi­ciently advanced civilizations eventually discover it and (b) its discovery leads almost universally to existential disaster.”\"],[\"h2\",{\"class\":\"paywall\"},\"II. The Machines\"],[\"p\",{\"class\":\"paywall\"},\"The field of artificial intelligence was born in a fit of scientific optimism, in 1955, when a small group of researchers—three mathematicians and an I.B.M. programmer—drew up a proposal for a project at Dartmouth. “An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves,” they stated. “We think a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.”\"],[\"ad\",{\"position\":\"mid-content\"}],[\"p\",{\"class\":\"paywall\"},\"Their optimism was understandable. Since the turn of the twentieth century, science had been advancing at a breakneck pace: the discovery of radioactivity quickly led to insights into the inner workings of the atom, and then to the development of controlled nuclear energy, and then to the warheads over Hiroshima and Nagasaki, and then to the H-bomb. This rush of discovery was reflected in fiction, too, in the work of Isaac Asimov, among others, who envisioned advanced civilizations inhabited by intelligent robots (each encoded with simple, ethical Laws of Robotics, to prevent it from causing harm). The year the scientists met at Dartmouth, Asimov published “The Last Question,” a story featuring a superintelligent A.I. that is continually “self-adjusting and self-correcting”—gaining knowledge as it helps human civilization expand throughout the universe. When the universe’s last stars start dying out, all humanity uploads itself into the A.I., and the device, achieving godhood, creates a new cosmos.\"],[\"p\",{\"class\":\"paywall\"},\"Scientists perceived the mechanics of intelligence—like those of the atom—as a source of huge potential, a great frontier. If the brain was merely a biological machine, there was no theoretical reason that it could not be replicated, or even surpassed, much the way a jet could outfly a falcon. Even before the Dartmouth conference, machines exceeded human ability in narrow domains like code-breaking. In 1951, Alan Turing argued that at some point computers would probably exceed the intellectual capacity of their inventors, and that “therefore we should have to expect the machines to take control.” Whether this would be good or bad he did not say.\"],[\"inline-embed\",{\"props\":{\"name\":\"inset-left\",\"attrs\":{},\"childTypes\":[\"cartoon\"]},\"ref\":\"\",\"type\":\"callout:inset-left\"},[\"inline-embed\",{\"props\":{\"image\":{\"id\":\"593b64a057b86d47b169c7b9\",\"altText\":\"“I hoped youd like the size of it.”\",\"contentType\":\"cartoon\",\"sources\":{\"sm\":{\"width\":640,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19633.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19633.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19633.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19633.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19633.jpg 640w\",\"height\":736},\"md\":{\"width\":768,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_768,c_limit\\u002F151123_a19633.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19633.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19633.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19633.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19633.jpg 640w\",\"height\":883},\"lg\":{\"width\":1024,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_1024,c_limit\\u002F151123_a19633.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19633.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19633.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19633.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19633.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19633.jpg 960w\",\"height\":1178},\"xl\":{\"width\":1280,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19633.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19633.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19633.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19633.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19633.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19633.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19633.jpg 1280w\",\"height\":1472},\"xxl\":{\"width\":1600,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_a19633.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19633.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19633.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19633.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19633.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19633.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19633.jpg 1280w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a057b86d47b169c7b9\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_a19633.jpg 1600w\",\"height\":1840}},\"dangerousCaption\":\"“I hoped you’d like the size of it.”\",\"dangerousCredit\":\"\",\"links\":[{\"network\":\"facebook\",\"behavior\":\"popup\",\"url\":\"https:\\u002F\\u002Fwww.facebook.com\\u002Fdialog\\u002Ffeed?&display=popup&caption=&app_id=1147169538698836&link=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19633%3Futm_source%3Dfacebook%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned\",\"label\":\"Share on Facebook\"},{\"network\":\"twitter\",\"url\":\"https:\\u002F\\u002Ftwitter.com\\u002Fintent\\u002Ftweet\\u002F?url=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19633%3Futm_source%3Dtwitter%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned&text=&via=NewYorker\",\"label\":\"Share on X\"},{\"network\":\"email\",\"url\":\"mailto:?subject=&body=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19633%3Futm_source%3Donsite-share%26utm_medium%3Demail%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker\",\"label\":\"Share via Email\"},{\"network\":\"store\",\"url\":\"https:\\u002F\\u002Fcondenaststore.com\\u002Fconde-nast-brand\\u002Fcartoons\",\"label\":\"Get in store\"},{\"network\":\"canonical\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcartoon\\u002Fa19633\",\"label\":\"a19633\"}]}},\"level\":\"block\",\"ref\":\"593b64a057b86d47b169c7b9\",\"type\":\"cartoon\"}]],[\"p\",{\"class\":\"paywall\"},\"Six years later, Herbert Simon, one of the Dartmouth attendees, declared that machines would achieve human intelligence “in a visible future.” The crossing of such a threshold, he suspected, could be psychologically crushing, but he was on the whole optimistic. “We must also remain sensitive to the need to keep the computer’s goals attuned with our own,” he later said, but added, “I am not convinced that this will be difficult.” For other computer pioneers, the future appeared more ambivalent. Norbert Wiener, the father of cybernetics, argued that it would be difficult to manage powerful computers, or even to accurately predict their behavior. “Complete subservience and complete intelligence do not go together,” he said. Envisioning Sorcerer’s Apprentice scenarios, he predicted, “The future will be an ever more demanding struggle against the limitations of our intelligence, not a comfortable hammock in which we can lie down to be waited upon by our robot slaves.”\"],[\"p\",{\"class\":\"paywall\"},\"It was in this milieu that the “intelligence explosion” idea was first formally expressed by I. J. Good, a statistician who had worked with Turing. “An ultraintelligent machine could design even better machines,” he wrote. “There would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the \",[\"em\",\"last\"],\" invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.”\"],[\"p\",{\"class\":\"paywall\"},\"The scientists at Dartmouth recognized that success required answers to fundamental questions: What is intelligence? What is the mind? By 1965, the field had experimented with several models of problem solving: some were based on formal logic; some used heuristic reasoning; some, called “neural networks,” were inspired by the brain. With each, the scientists’ work indicated that A.I. systems could find their own solutions to problems. One algorithm proved numerous theorems in the classic text “Principia Mathematica,” and in one instance it did so more elegantly than the authors. A program designed to play checkers learned to beat its programmer. And yet, despite the great promise in these experiments, the challenges to creating an A.I. were forbidding. Programs that performed well in the laboratory were useless in everyday situations; a simple act like picking up a ball turned out to require an overwhelming number of computations.\"],[\"p\",{\"class\":\"paywall\"},\"The research fell into the first of several “A.I. winters.” As Bostrom notes in his book, “Among academics and their funders, ‘A.I.’ became an unwanted epithet.” Eventually, the researchers started to question the goal of building a mind altogether. Why not try instead to divide the problem into pieces? They began to limit their interests to specific cognitive functions: vision, say, or speech. Even in isolation, these functions would have value: a computer that could identify objects might not be an A.I., but it could help guide a forklift. As the research fragmented, the morass of technical problems made any questions about the consequences of success seem distant, even silly.\"],[\"p\",{\"class\":\"paywall\"},\"Unexpectedly, by dismissing its founding goals, the field of A.I. created space for outsiders to imagine more freely what the technology might look like. Bostrom wrote his first paper on artificial superintelligence in the nineteen-nineties, envisioning it as potentially perilous but irresistible to both commerce and government. “If there is a way of guaranteeing that superior artificial intellects will never harm human beings, then such intellects will be created,” he argued. “If there is no way to have such a guarantee, then they will probably be created nevertheless.” His audience at the time was primarily other transhumanists. But the movement was maturing. In 2005, an organization called the Singularity Institute for Artificial Intelligence began to operate out of Silicon Valley; its primary founder, a former member of the Extropian discussion group, published a stream of literature on the dangers of A.I. That same year, the futurist and inventor Ray Kurzweil wrote “The Singularity Is Near,” a best-seller that prophesied a merging of man and machine in the foreseeable future. Bostrom created his institute at Oxford.\"],[\"ad\",{\"position\":\"mid-content\"}],[\"p\",{\"class\":\"paywall\"},\"The two communities could not have been more different. The scientists, steeped in technical detail, were preoccupied with making devices that worked; the transhumanists, motivated by the hope of a utopian future, were asking, What would the ultimate impact of those devices be? In 2007, the Association for the Advancement of Artificial Intelligence—the most prominent professional organization for A.I. researchers—elected Eric Horvitz, a scientist from Microsoft, as its president. Until then, it had given virtually no attention to the ethical and social implications of the research, but Horvitz was open to the big questions. “It is hard to understand what success would mean for A.I.,” he told me. “I was friendly with Jack Good, who wrote that piece on superintelligence. I knew him as a creative, funny guy who referred to a lot of his ideas as P.B.I.s—partly baked ideas. And here is this piece of his being opened up outside the field as this Bible and studied with a silver pointer. Wouldn’t it be useful, I said, even if you thought these were crazy or low-probability scenarios, to find out: Can we be proactive, should there be some poor outcome for humanity?”\"],[\"p\",{\"class\":\"paywall\"},\"Horvitz organized a meeting at the Asilomar Conference Grounds, in California, a place chosen for its symbolic value: biologists had gathered there in 1975 to discuss the hazards of their research in the age of modern genetics. He divided the researchers into groups. One studied short-term ramifications, like the possible use of A.I. to commit crimes; another considered long-term consequences. Mostly, there was skepticism about the intelligence-explosion idea, which assumed answers to many unresolved questions. No one fully understands what intelligence is, let alone how it might evolve in a machine. Can it grow as Good imagined, gaining I.Q. points like a rocketing stock price? If so, what would its upper limit be? And would its increase be merely a function of optimized software design, without the difficult process of acquiring knowledge through experience? Can software fundamentally rewrite itself without risking crippling breakdowns? No one knows. In the history of computer science, no programmer has created code that can substantially improve itself.\"],[\"inline-embed\",{\"props\":{\"name\":\"inset-left\",\"attrs\":{},\"childTypes\":[\"cartoon\"]},\"ref\":\"\",\"type\":\"callout:inset-left\"},[\"inline-embed\",{\"props\":{\"image\":{\"id\":\"593b64a21f743c6ee49a53e4\",\"altText\":\"“No you want the A train. This is just a train.”\",\"contentType\":\"cartoon\",\"sources\":{\"sm\":{\"width\":640,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19630.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19630.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19630.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19630.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19630.jpg 640w\",\"height\":479},\"md\":{\"width\":768,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_768,c_limit\\u002F151123_a19630.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19630.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19630.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19630.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19630.jpg 640w\",\"height\":575},\"lg\":{\"width\":1024,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_1024,c_limit\\u002F151123_a19630.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19630.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19630.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19630.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19630.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19630.jpg 960w\",\"height\":766},\"xl\":{\"width\":1280,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19630.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19630.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19630.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19630.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19630.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19630.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19630.jpg 1280w\",\"height\":958},\"xxl\":{\"width\":1600,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_a19630.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19630.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19630.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19630.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19630.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19630.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19630.jpg 1280w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a21f743c6ee49a53e4\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_a19630.jpg 1600w\",\"height\":1198}},\"dangerousCaption\":\"“No, you want the A train. This is just a train.”\",\"dangerousCredit\":\"\",\"links\":[{\"network\":\"facebook\",\"behavior\":\"popup\",\"url\":\"https:\\u002F\\u002Fwww.facebook.com\\u002Fdialog\\u002Ffeed?&display=popup&caption=&app_id=1147169538698836&link=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19630%3Futm_source%3Dfacebook%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned\",\"label\":\"Share on Facebook\"},{\"network\":\"twitter\",\"url\":\"https:\\u002F\\u002Ftwitter.com\\u002Fintent\\u002Ftweet\\u002F?url=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19630%3Futm_source%3Dtwitter%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned&text=&via=NewYorker\",\"label\":\"Share on X\"},{\"network\":\"email\",\"url\":\"mailto:?subject=&body=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19630%3Futm_source%3Donsite-share%26utm_medium%3Demail%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker\",\"label\":\"Share via Email\"},{\"network\":\"store\",\"url\":\"https:\\u002F\\u002Fcondenaststore.com\\u002Fconde-nast-brand\\u002Fcartoons\",\"label\":\"Get in store\"},{\"network\":\"canonical\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcartoon\\u002Fa19630\",\"label\":\"a19630\"}]}},\"level\":\"block\",\"ref\":\"593b64a21f743c6ee49a53e4\",\"type\":\"cartoon\"}]],[\"p\",{\"class\":\"paywall\"},\"But the notion of an intelligence explosion was also impossible to disprove. It was theoretically coherent, and it had even been attempted in limited ways. David McAllester, an A.I. researcher at the Toyota Technological Institute, affiliated with the University of Chicago, headed the long-term panel. The idea, he argued, was worth taking seriously. “I am uncomfortable saying that we are ninety-­nine per cent certain that we are safe for fifty years,” he told me. “That feels like hubris to me.” The group concluded that more technical work was needed before an evaluation of the dangers could be made, but it also hinted at a concern among panelists that the gathering was based on “a \",[\"em\",\"perception\"],\" of urgency”—generated largely by the transhumanists—and risked raising unfounded alarm. With A.I. seeming like a remote prospect, the researchers declared, attention was better spent on near-term concerns. Bart Selman, a professor at Cornell who co-­organized the panel, told me, “The mode was ‘This is interesting, but it’s all academic—it’s not going to happen.’ ”\"],[\"p\",{\"class\":\"paywall\"},\"At the time the A.I researchers met at Asilomar, Bostrom was grappling with an expansive book on existential risks. He had sketched out chapters on bioengineering and on nanotechnology, among other topics, but many of these problems came to seem less compelling, while his chapter on A.I. grew and grew. Eventually, he pasted the A.I. chapter into a new file, which became “Superintelligence.”\"],[\"p\",{\"class\":\"paywall\"},\"The book is its own elegant paradox: analytical in tone and often lucidly argued, yet punctuated by moments of messianic urgency. Some portions are so extravagantly speculative that it is hard to take them seriously. (“Suppose we could somehow establish that a certain future AI will have an IQ of 6,455: then what?”) But Bostrom is aware of the limits to his type of futurology. When he was a graduate student in London, thinking about how to maximize his ability to communicate, he pursued stand­­up comedy; he has a deadpan sense of humor, which can be found lightly buried among the book’s self-serious passages. “Many of the points made in this book are probably wrong,” he writes, with an endnote that leads to the line “I don’t know which ones.”\"],[\"p\",{\"class\":\"paywall\"},\"Bostrom prefers to act as a cartographer rather than a polemicist, but beneath his exhaustive mapping of scenarios one can sense an argument being built and perhaps a fear of being forthright about it. “Traditionally, this topic domain has been occupied by cranks,” he told me. “By popular media, by science fiction—or maybe by a retired physicist no longer able to do serious work, so he will write a popular book and pontificate. That is kind of the level of rigor that is the baseline. I think that a lot of reasons why there has not been more serious work in this area is that academics don’t want to be conflated with flaky, crackpot type of things. Futurists are a certain type.”\"],[\"p\",{\"class\":\"paywall\"},\"The book begins with an “unfinished” fable about a flock of sparrows that decide to raise an owl to protect and advise them. They go looking for an owl egg to steal and bring back to their tree, but, because they believe their search will be so difficult, they postpone studying how to domesticate owls until they succeed. Bostrom concludes, “It is not known how the story ends.”\"],[\"ad\",{\"position\":\"mid-content\"}],[\"p\",{\"class\":\"paywall\"},\"The parable is his way of introducing the book’s core question: Will an A.I., if realized, use its vast capability in a way that is beyond human control? One way to think about the concern is to begin with the familiar. Bos­trom writes, “Artificial intelligence already outperforms human intelligence in many domains.” The examples range from chess to Scrabble. One program from 1981, called Eurisko, was designed to teach itself a naval role-playing game. After playing ten thousand matches, it arrived at a morally grotesque strategy: to field thousands of small, immobile ships, the vast majority of which were intended as cannon fodder. In a national tournament, Eurisko demolished its human opponents, who insisted that the game’s rules be changed. The following year, Eurisko won again—by forcing its damaged ships to sink themselves.\"],[\"p\",{\"class\":\"paywall\"},\"The program was by no means superintelligent. But Bostrom’s book essentially asks: What if it were? Assume that it has a broad ability to consider problems and that it has access to the Internet. It could read and acquire general knowledge and communicate with people seamlessly online. It could conduct experiments, either virtually or by tinkering with networked infrastructure. Given even the most benign objective—to win a game—such a system, Bostrom argues, might develop “instrumental goals”: gather resources, or invent technology, or take steps to insure that it cannot be turned off, in the process paying as much heed to human life as humans do to ants.\"],[\"p\",{\"class\":\"paywall\"},\"In people, intelligence is inseparable from consciousness, emotional and social awareness, the complex interaction of mind and body. An A.I. need not have any such attributes. Bostrom believes that machine intelligences—no matter how flexible in their tactics—will likely be rigidly fixated on their ultimate goals. How, then, to create a machine that respects the nuances of social cues? That adheres to ethical norms, even at the expense of its goals? No one has a coherent solution. It is hard enough to reliably inculcate such behavior in people.\"],[\"p\",{\"class\":\"paywall\"},\"In science fiction, superintelligent computers that run amok are often circumvented at the last minute; think of WOPR, the computer in “WarGames,” which was stopped just short of triggering nuclear war, or HAL 9000, which was reduced to helplessly singing while it watched itself get dismantled. For Bos­trom, this strains credulity. Whether out of a desire to consider the far ends of risk or out of transhumanist longings, he often ascribes nearly divine abilities to machines, as if to ask: Can a digital god really be contained? He imagines machines so intelligent that merely by inspecting their own code they can extrapolate the nature of the universe and of human society, and in this way outsmart any effort to contain them. “Is it possible to build machines that are not like agents—goal-pursuing, autonomous, artificial intelligences?” he asked me. “Maybe you can design something more like an oracle that can only answer yes or no. Would that be safer? It is not so clear. There might be agent-like processes within it.” Asking a simple question—“Is it possible to convert a DeLorean into a time machine and travel to 1955?”—might trigger a cascade of action as the device tests hypotheses. What if, working through a police computer, it impounds a DeLorean that happens to be convenient to a clock tower? “In fairy tales, you have genies who grant wishes,” Bostrom said. “Almost universally, the moral of those is that if you are not extremely careful what you wish for, then what seems like it should be a great blessing turns out to be a curse.”\"],[\"p\",{\"class\":\"paywall\"},\"Bostrom worries that solving the “control problem”—insuring that a superintelligent machine does what humans want it to do—will require more time than solving A.I. does. The intelligence explosion is not the only way that a superintelligence might be created suddenly. Bostrom once sketched out a decades-long process, in which researchers arduously improved their systems to equal the intelligence of a mouse, then a chimp, then—after incredible labor—the village idiot. “The difference between village idiot and genius-­level intelligence might be trivial from the point of view of how hard it is to replicate the same functionality in a machine,” he said. “The brain of the village idiot and the brain of a scientific genius are almost identical. So we might very well see relatively slow and incremental progress that doesn’t really raise any alarm bells until we are just one step away from something that is radically superintelligent.”\"],[\"p\",{\"class\":\"paywall\"},\"To a large degree, Bostrom’s concerns turn on a simple question of timing: Can breakthroughs be predicted? “It is ridiculous to talk about such things so early—A.I. is eons away,” Edward Feigenbaum, an emeritus professor at Stanford University, told me. The researcher Oren Etzioni, who used the term “Frankenstein complex” to dismiss the “dystopian vision of A.I.,” concedes Bostrom’s overarching point: that the field must one day confront profound philosophical questions. Decades ago, he explored them himself, in a brief paper, but concluded that the problem was too remote to think about productively. “Once, Nick Bostrom gave a talk, and I gave a little counterpoint,” he told me. “A lot of the disagreements come down to what time scale you are thinking about. Nobody responsible would say you will see anything remotely like A.I. in the next five to ten years. And I think most computer scientists would say, ‘In a million years—we don’t see why it shouldn’t happen.’ So now the question is: What is the rate of progress? There are a lot of people who will ask: Is it \",[\"em\",\"possible\"],\" we are wrong? Yes. I am not going to rule it out. I am going to say, ‘I am a scientist. Show me the evidence.’ ”\"],[\"ad\",{\"position\":\"mid-content\"}],[\"p\",{\"class\":\"paywall\"},\"The history of science is an uneven guide to the question: How close are we? There has been no shortage of unfulfilled promises. But there are also plenty of examples of startling nearsightedness, a pattern that Arthur C. Clarke enshrined as Clarke’s First Law: “When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong.” After the electron was discovered, at Cambridge, in 1897, physicists at an annual dinner toasted, “To the electron: may it never be of use to anybody.” Lord Kelvin famously declared, just eight years before the Wright brothers launched from Kitty Hawk, that heavier-than-air flight was impossible.\"],[\"p\",{\"class\":\"paywall\"},\"Stuart Russell, the co-author of the textbook “Artificial Intelligence: A Modern Approach” and one of Bostrom’s most vocal supporters in A.I., told me that he had been studying the physics community during the advent of nuclear weapons. At the turn of the twentieth century, Ernest Rutherford discovered that heavy elements produced radiation by atomic decay, confirming that vast reservoirs of energy were stored in the atom. Rutherford believed that the energy could not be harnessed, and in 1933 he proclaimed, “Anyone who expects a source of power from the transformation of these atoms is talking moonshine.” The next day, a former student of Einstein’s named Leo Szilard read the comment in the papers. Irritated, he took a walk, and the idea of a nuclear chain reaction occurred to him. He visited Rutherford to discuss it, but Rutherford threw him out. Einstein, too, was skeptical about nuclear energy—splitting atoms at will, he said, was “like shooting birds in the dark in a country where there are only a few birds.” A decade later, Szilard’s insight was used to build the bomb.\"],[\"p\",{\"class\":\"paywall\"},\"Russell now relays the story to A.I. researchers as a cautionary tale. “There will have to be more breakthroughs to get to A.I., but, as Szilard illustrated, those can happen overnight,” he told me. “People are putting billions of dollars into achieving those breakthroughs. As the debate stands, Bostrom and others have said, ‘If we achieve superintelligence, here are some of the problems that might arise.’ As far as I know, no one has proved why those are not real.”\"],[\"h2\",{\"class\":\"paywall\"},\"III. Mission Control\"],[\"p\",{\"class\":\"paywall\"},\"The offices of the Future of Humanity Institute have a hybrid atmosphere: part physics lab, part college dorm room. There are whiteboards covered with mathematical notation and technical glyphs; there are posters of “Brave New World” and HAL 9000. There is also art work by Nick Bostrom. One afternoon, he guided me to one of his pieces, “At Sea,” a digital collage that he had printed out and then drawn on. “It is a bit damaged, but the good thing about digital is that you can re-instantiate it,” he said. At the center was a pale man, nearly an apparition, clinging to a barrel in an inky-black ocean. “It is an existentialist vibe. You are hanging on for as long as you can. When you get tired, you sink, and become fish food—or maybe a current will take him to land. We don’t know.”\"],[\"p\",{\"class\":\"paywall\"},\"Despite the time he spends going to conferences and raising money, Bostrom attends to many details at the institute. “We needed a logo when we started,” he told me. “We went to this online site where you could buy the work of freelance artists. If you sat down and tried to make the ugliest logo, you couldn’t come close. Then we hired a designer, who made a blurry figure of a person. We showed it to someone here, who said it looked like a toilet sign. As soon as she said it, I thought, Oh, my God, we almost adopted a toilet sign as our logo. So I mucked around a bit and came up with a black diamond. You have the black monolith from ‘2001.’ Standing on its corner, it indicates instability. Also, there is a limit to how ugly a black square can be.”\"],[\"p\",{\"class\":\"paywall\"},\"The institute shares office space with the Centre for Effective Altruism, and both organizations intersect with a social movement that promotes pure rationality as a guide to moral action. Toby Ord, a philosopher who works with both, told me that Bostrom often pops into his office at the end of the day, poses a problem, then leaves him pondering it for the night. Among the first of Bostrom’s questions was this: If the universe turns out to contain an infinite number of beings, then how could any single person’s action affect the cosmic balance of suffering and happiness? After lengthy discussions, they left the paradox unresolved. “My main thinking is that we can sort it out later,” Ord told me.\"],[\"inline-embed\",{\"props\":{\"name\":\"inset-left\",\"attrs\":{},\"childTypes\":[\"cartoon\"]},\"ref\":\"\",\"type\":\"callout:inset-left\"},[\"inline-embed\",{\"props\":{\"image\":{\"id\":\"593b64a47206a168e2d1782f\",\"altText\":\"“Chaucer on lyne thrie.”\",\"contentType\":\"cartoon\",\"sources\":{\"sm\":{\"width\":640,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19596.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19596.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19596.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19596.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19596.jpg 640w\",\"height\":541},\"md\":{\"width\":768,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_768,c_limit\\u002F151123_a19596.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19596.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19596.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19596.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19596.jpg 640w\",\"height\":650},\"lg\":{\"width\":1024,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_1024,c_limit\\u002F151123_a19596.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19596.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19596.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19596.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19596.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19596.jpg 960w\",\"height\":866},\"xl\":{\"width\":1280,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19596.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19596.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19596.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19596.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19596.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19596.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19596.jpg 1280w\",\"height\":1083},\"xxl\":{\"width\":1600,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_a19596.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19596.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19596.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19596.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19596.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19596.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19596.jpg 1280w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a47206a168e2d1782f\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_a19596.jpg 1600w\",\"height\":1353}},\"dangerousCaption\":\"“Chaucer on lyne thrie.”\",\"dangerousCredit\":\"\",\"links\":[{\"network\":\"facebook\",\"behavior\":\"popup\",\"url\":\"https:\\u002F\\u002Fwww.facebook.com\\u002Fdialog\\u002Ffeed?&display=popup&caption=&app_id=1147169538698836&link=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19596%3Futm_source%3Dfacebook%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned\",\"label\":\"Share on Facebook\"},{\"network\":\"twitter\",\"url\":\"https:\\u002F\\u002Ftwitter.com\\u002Fintent\\u002Ftweet\\u002F?url=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19596%3Futm_source%3Dtwitter%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned&text=&via=NewYorker\",\"label\":\"Share on X\"},{\"network\":\"email\",\"url\":\"mailto:?subject=&body=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19596%3Futm_source%3Donsite-share%26utm_medium%3Demail%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker\",\"label\":\"Share via Email\"},{\"network\":\"store\",\"url\":\"https:\\u002F\\u002Fcondenaststore.com\\u002Fconde-nast-brand\\u002Fcartoons\",\"label\":\"Get in store\"},{\"network\":\"canonical\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcartoon\\u002Fa19596\",\"label\":\"a19596\"}]}},\"level\":\"block\",\"ref\":\"593b64a47206a168e2d1782f\",\"type\":\"cartoon\"}]],[\"p\",{\"class\":\"paywall\"},\"When I asked Bostrom if I could observe a discussion at the institute, he seemed reluctant; it was hard to judge whether he was concerned that my presence would interfere or that unfiltered talk of, say, engineered pathogens might inspire criminals. (“At some point, one gets into the realm of information hazard,” he hinted.) Eventually, he let me observe a session in the Petrov Room involving half a dozen staff members. The key question under discussion was whether a global catastrophe, on the order of a continent-wide famine, could trigger a series of geopolitical events that would result in human extinction—and whether that meant that a merely catastrophic risk could therefore be taken as seriously as an existential risk. Bostrom, wearing a gray hoodie over a blue button-­down, organized the problem on a whiteboard with visible pleasure. Anders Sandberg told me that he once spent days with Bostrom working through such a problem, distilling a complex argument to its essence. “He had to \",[\"em\",\"refine\"],\" it,” he said. “We had a lot of schemes on the whiteboard that gradually were simplified to one box and three arrows.”\"],[\"ad\",{\"position\":\"mid-content\"}],[\"p\",{\"class\":\"paywall\"},\"For anyone in the business of publicizing existential risk, 2015 began as a good year. Other institutes devoted to these issues had started to find their voice, bringing an additional gloss of respectability to the ideas in Bostrom’s book. The people weighing in now were no longer just former Extropians. They were credentialled, like Lord Martin Rees, an astrophysicist and the co-founder of Cambridge’s Centre for the Study of Existential Risk. In January, he wrote of A.I., in the \",[\"em\",\"Evening Standard\"],\", “We don’t know where the boundary lies between what may happen and what will remain science fiction.”\"],[\"p\",{\"class\":\"paywall\"},\"Rees’s counterpart at the Future of Life Institute, the M.I.T. physicist Max Tegmark, hosted a closed-door meeting in Puerto Rico, to try to make sense of the long-term trajectory of the research. Bostrom flew down, joining a mix of A.I. practitioners, legal scholars, and, for lack of a better term, members of the “A.I. safety” community. “These are not people who are usually in the same room,” Tegmark told me. “Someone advised me to put Valium in people’s drinks so nobody got into fistfights. But, by the time Nick’s session started, people were ready to listen to each other.” Questions that had seemed fanciful to researchers only seven years earlier were beginning to look as though they might be worth reconsidering. Whereas the Asilomar meeting concluded on a note of skepticism about the validity of the whole endeavor, the Puerto Rico conference resulted in an open letter, signed by many prominent researchers, that called for more research to insure that A.I. would be “robust and beneficial.”\"],[\"p\",{\"class\":\"paywall\"},\"Between the two conferences, the field had experienced a revolution, built on an approach called deep learning—a type of neural network that can discern complex patterns in huge quantities of data. For de­c­ades, researchers, hampered by the limits of their hardware, struggled to get the technique to work well. But, beginning in 2010, the increasing availability of Big Data and cheap, powerful video-­game processors had a dramatic effect on performance. Without any profound theoretical breakthrough, deep learning suddenly offered breathtaking advances. “I have been talking to quite a few contemporaries,” Stuart Russell told me. “Pretty much everyone sees examples of progress they just didn’t expect.” He cited a YouTube clip of a four-legged robot: one of its designers tries to kick it over, but it quickly regains its balance, scrambling with uncanny naturalness. “A problem that had been viewed as very difficult, where progress was slow and incremental, was all of a sudden done. Locomotion: done.”\"],[\"p\",{\"class\":\"paywall\"},\"In an array of fields—speech processing, face recognition, language translation—the approach was ascendant. Researchers working on computer vision had spent years to get systems to identify objects. In almost no time, the deep-learning networks crushed their records. In one common test, using a database called ImageNet, humans identify photographs with a five-per-cent error rate; Google’s network operates at 4.8 per cent. A.I. systems can differentiate a Pembroke Welsh Corgi from a Cardigan Welsh Corgi.\"],[\"p\",{\"class\":\"paywall\"},\"Last October, Tomaso Poggio, an M.I.T. researcher, gave a skeptical interview. “The ability to describe the content of an image would be one of the most intellectually challenging things of all for a machine to do,” he said. “We will need another cycle of basic research to solve this kind of question.” The cycle, he predicted, would take at least twenty years. A month later, Google announced that its deep-learning network could analyze an image and offer a caption of what it saw: “Two pizzas sitting on top of a stove top,” or “People shopping at an outdoor market.” When I asked Poggio about the results, he dismissed them as automatic associations between objects and language; the system did not \",[\"em\",\"understand\"],\" what it saw. “Maybe human intelligence is the same thing, in which case I am wrong, or not, in which case I was right,” he told me. “How do you decide?”\"],[\"p\",{\"class\":\"paywall\"},\"A respected minority of A.I. researchers began to wonder: If increasingly powerful hardware could facilitate the deep-learning revolution, would it make other long-shelved A.I. principles viable? “Suppose the brain is just a million different evolutionarily developed hacks: one for smell, one for recognizing faces, one for how you recognize animals,” Tom Mitchell, who holds a chair in machine learning at Carnegie Mellon, told me. “If that is what underlies intelligence, then I think we are far, far from getting there—because we don’t have many of those hacks. On the other hand, suppose that what underlies intelligence are twenty-three general mechanisms, and when you put them together you get synergy, and it works. We now have systems that can do a pretty good job with computer vision—and it turns out that we didn’t have to construct a million hacks. So part of the uncertainty is: if we do not need a million different hacks, then will we find the right twenty-­three fundamental generic methods?” He paused. “I no longer have the feeling, which I had twenty-five years ago, that there are gaping holes. I know we don’t have a good architecture to assemble the ideas, but it is not obvious to me that we are missing components.”\"],[\"inline-embed\",{\"props\":{\"name\":\"inset-left\",\"attrs\":{},\"childTypes\":[\"cartoon\"]},\"ref\":\"\",\"type\":\"callout:inset-left\"},[\"inline-embed\",{\"props\":{\"image\":{\"id\":\"593b64a557b86d47b169c7c1\",\"altText\":\"The Philosopher of Doomsday\",\"contentType\":\"cartoon\",\"sources\":{\"sm\":{\"width\":640,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19555.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19555.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19555.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19555.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19555.jpg 640w\",\"height\":478},\"md\":{\"width\":768,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_768,c_limit\\u002F151123_a19555.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19555.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19555.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19555.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19555.jpg 640w\",\"height\":573},\"lg\":{\"width\":1024,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_1024,c_limit\\u002F151123_a19555.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19555.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19555.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19555.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19555.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19555.jpg 960w\",\"height\":764},\"xl\":{\"width\":1280,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19555.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19555.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19555.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19555.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19555.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19555.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19555.jpg 1280w\",\"height\":956},\"xxl\":{\"width\":1600,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_a19555.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19555.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19555.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19555.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19555.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19555.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19555.jpg 1280w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a557b86d47b169c7c1\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_a19555.jpg 1600w\",\"height\":1194}},\"dangerousCaption\":\"\",\"dangerousCredit\":\"\",\"links\":[{\"network\":\"facebook\",\"behavior\":\"popup\",\"url\":\"https:\\u002F\\u002Fwww.facebook.com\\u002Fdialog\\u002Ffeed?&display=popup&caption=&app_id=1147169538698836&link=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19555%3Futm_source%3Dfacebook%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned\",\"label\":\"Share on Facebook\"},{\"network\":\"twitter\",\"url\":\"https:\\u002F\\u002Ftwitter.com\\u002Fintent\\u002Ftweet\\u002F?url=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19555%3Futm_source%3Dtwitter%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned&text=&via=NewYorker\",\"label\":\"Share on X\"},{\"network\":\"email\",\"url\":\"mailto:?subject=&body=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19555%3Futm_source%3Donsite-share%26utm_medium%3Demail%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker\",\"label\":\"Share via Email\"},{\"network\":\"store\",\"url\":\"https:\\u002F\\u002Fcondenaststore.com\\u002Fconde-nast-brand\\u002Fcartoons\",\"label\":\"Get in store\"},{\"network\":\"canonical\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcartoon\\u002Fa19555\",\"label\":\"a19555\"}]}},\"level\":\"block\",\"ref\":\"593b64a557b86d47b169c7c1\",\"type\":\"cartoon\"}]],[\"p\",{\"class\":\"paywall\"},\"Bostrom noticed the shift in attitude. He recently conducted a poll of A.I. researchers to gauge their sense of progress, and in Puerto Rico a survey gathered opinions on how long it would be until an artificial intelligence could reason indistinguishably from a human being. Like Bostrom, the engineers are often careful to express their views as probabilities, rather than as facts. Richard Sutton, a Canadian computer scientist whose work has earned tens of thousands of scholarly citations, gives a range of outcomes: there is a ten-per-cent chance that A.I. will never be achieved, but a twenty-five-per-cent chance that it will arrive by 2030. The median response in Bostrom’s poll gives a fifty-fifty chance that human-level A.I. would be attained by 2050. These surveys are unscientific, but he is confident enough to offer an interpretive assumption: “It is not a ridiculous prospect to take seriously the possibility that it can happen in the lifetime of people alive today.”\"],[\"ad\",{\"position\":\"mid-content\"}],[\"p\",{\"class\":\"paywall\"},\"On my last day in Oxford, I walked with Bostrom across town. He was racing to catch a train to London, to speak at the Royal Society, one of the world’s oldest scientific institutions. His spirits were high. The gulf between the transhumanists and the scientific community was slowly shrinking. Elon Musk had pledged ten million dollars in grants for academics seeking to investigate A.I. safety, and, rather than mock him, researchers applied for the money; Bostrom’s institute was helping to evaluate the proposals. “Right now, there is a lot of interest,” he told me. “But then there were all these long years when nobody else seemed to pay attention at all. I am not sure which is the less abnormal condition.”\"],[\"p\",{\"class\":\"paywall\"},\"There were clear limits to that interest. To publicly stake out a position in the middle of the debate was difficult, not least because of the polarized atmosphere Bostrom’s book had helped to create. Even though a growing number of researchers were beginning to suspect that profound questions loomed, and that they might be worth addressing now, it did not mean that they believed A.I. would lead inevitably to an existential demise or a techno-utopia. Most of them were engaged with more immediate problems: privacy, unemployment, weaponry, driverless cars running amok. When I asked Bostrom about this pragmatic ethical awakening, he reacted with dismay. “My fear is that it would swallow up the concerns for the longer term,” he said. “On the other hand, yes, maybe it is useful to build bridges to these different communities. Kind of makes the issue part of a larger continuum of things to work on.”\"],[\"p\",{\"class\":\"paywall\"},\"At the Royal Society, Bostrom took a seat at the back of a large hall. As he crossed his legs, I noticed a thin leather band around his ankle. A metal buckle was engraved with contact information for Alcor, a cryonics facility in Arizona, where Bostrom is a fee-paying member. Within hours of his death, Alcor will take custody of his body and maintain it in a giant steel bottle flooded with liquid nitrogen, in the hope that one day technology will allow him to be revived, or to have his mind uploaded into a computer. When he signed up, two other colleagues at the institute joined him. “My background is transhumanism,” he once reminded me. “The character of that is gung-ho techno-cheerleading, bring it on now, where are my life-­extension pills.”\"],[\"p\",{\"class\":\"paywall\"},\"The hall was packed with some of the most technically sophisticated researchers in A.I.—not necessarily Bostrom’s people—and when he spoke he began by trying to assure them that his concern was not out of Ludditism. “It would be tragic if machine intelligence were never developed to its full capacity,” he said. “I think this is ultimately the key, or the portal, we have to pass through to realize the full dimension of humanity’s long-term potential.” But, even as he avoided talk of existential risk, he pressed his audience to consider the danger of building an A.I. without regarding its ethical design.\"],[\"p\",{\"class\":\"paywall\"},\"An attendee raised his hand to object. “We can’t control basic computer worms,” he said. “The A.I. that will happen is going to be a highly adaptive, emergent capability, and highly distributed. We will be able to work with it—\",[\"em\",\"for\"],\" it—not necessarily contain it.”\"],[\"p\",{\"class\":\"paywall\"},\"“I guess I am a little frustrated,” Bos­trom responded. “People tend to fall into two camps. On one hand, there are those, like yourself, who think it is probably hopeless. The other camp thinks it is easy enough that it will be solved automatically. And both of these have in common the implication that we don’t have to make any effort now.”\"],[\"p\",{\"class\":\"paywall\"},\"For the rest of the day, engineers presented their work at the lectern, each promising a glimpse of the future—robot vision, quantum computers, algorithms called “thought vectors.” Early in Bostrom’s career, he predicted that cascading economic demand for an A.I. would build up across the fields of medicine, entertainment, finance, and defense. As the technology became useful, that demand would only grow. “If you make a one-per-cent improvement to something—say, an algorithm that recommends books on Amazon—there is a lot of value there,” Bostrom told me. “Once every improvement potentially has enormous economic benefit, that promotes effort to make more improvements.”\"],[\"p\",{\"class\":\"paywall\"},\"Many of the world’s largest tech companies are now locked in an A.I. arms race, purchasing other companies and opening specialized units to advance the technology. Industry is vacuuming up Ph.D.s so quickly that people in the field worry there will no longer be top talent in academia. After decades of pursuing narrow forms of A.I., researchers are seeking to integrate them into systems that resemble a general intellect. Since I.B.M.’s Watson won “Jeopardy,” the company has committed more than a billion dollars to develop it, and is reorienting its business around “cognitive systems.” One senior I.B.M. executive declared, “The separation between human and machine is going to blur in a very fundamental way.”\"],[\"p\",{\"class\":\"paywall\"},\"At the Royal Society, a contingent of researchers from Google occupied a privileged place; they likely had more resources at their disposal than anyone else in the room. Early on, Google’s founders, Larry Page and Sergey Brin, understood that the company’s mission required solving fundamental A.I. problems. Page has said that he believes the ideal system would understand questions, even anticipate them, and produce responses in conversational language. Google scientists often invoke the computer in “Star Trek” as a model.\"],[\"inline-embed\",{\"props\":{\"name\":\"inset-left\",\"attrs\":{},\"childTypes\":[\"cartoon\"]},\"ref\":\"\",\"type\":\"callout:inset-left\"},[\"inline-embed\",{\"props\":{\"image\":{\"id\":\"593b64a757b86d47b169c7c3\",\"altText\":\"“O.K. theres the moon—now give me a nice long howl instead of last nights yip.”\",\"contentType\":\"cartoon\",\"sources\":{\"sm\":{\"width\":640,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19484.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19484.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19484.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19484.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19484.jpg 640w\",\"height\":630},\"md\":{\"width\":768,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_768,c_limit\\u002F151123_a19484.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19484.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19484.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19484.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19484.jpg 640w\",\"height\":755},\"lg\":{\"width\":1024,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_1024,c_limit\\u002F151123_a19484.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19484.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19484.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19484.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19484.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19484.jpg 960w\",\"height\":1007},\"xl\":{\"width\":1280,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19484.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19484.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19484.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19484.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19484.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19484.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19484.jpg 1280w\",\"height\":1259},\"xxl\":{\"width\":1600,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_a19484.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19484.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19484.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19484.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19484.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19484.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19484.jpg 1280w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a757b86d47b169c7c3\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_a19484.jpg 1600w\",\"height\":1574}},\"dangerousCaption\":\"“O.K., there’s the moon&#8212;now give me a nice long howl instead of last night’s yip.”\",\"dangerousCredit\":\"\",\"links\":[{\"network\":\"facebook\",\"behavior\":\"popup\",\"url\":\"https:\\u002F\\u002Fwww.facebook.com\\u002Fdialog\\u002Ffeed?&display=popup&caption=&app_id=1147169538698836&link=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19484%3Futm_source%3Dfacebook%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned\",\"label\":\"Share on Facebook\"},{\"network\":\"twitter\",\"url\":\"https:\\u002F\\u002Ftwitter.com\\u002Fintent\\u002Ftweet\\u002F?url=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19484%3Futm_source%3Dtwitter%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned&text=&via=NewYorker\",\"label\":\"Share on X\"},{\"network\":\"email\",\"url\":\"mailto:?subject=&body=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19484%3Futm_source%3Donsite-share%26utm_medium%3Demail%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker\",\"label\":\"Share via Email\"},{\"network\":\"store\",\"url\":\"https:\\u002F\\u002Fcondenaststore.com\\u002Fconde-nast-brand\\u002Fcartoons\",\"label\":\"Get in store\"},{\"network\":\"canonical\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcartoon\\u002Fa19484\",\"label\":\"a19484\"}]}},\"level\":\"block\",\"ref\":\"593b64a757b86d47b169c7c3\",\"type\":\"cartoon\"}]],[\"p\",{\"class\":\"paywall\"},\"In recent years, Google has purchased seven robotics companies and several firms specializing in machine intelligence; it may now employ the world’s largest contingent of Ph.D.s in deep learning. Perhaps the most interesting acquisition is a British company called DeepMind, started in 2011 to build a general artificial intelligence. Its founders had made an early bet on deep learning, and sought to combine it with other A.I. mechanisms in a cohesive architecture. In 2013, they published the results of a test in which their system played seven classic Atari games, with no instruction other than to improve its score. For many people in A.I., the importance of the results was immediately evident. I.B.M.’s chess program had defeated Garry Kasparov, but it could not beat a three-year-old at tic-tac-toe. In six games, DeepMind’s system outperformed all previous algorithms; in three it was superhuman. In a boxing game, it learned to pin down its opponent and subdue him with a barrage of punches.\"],[\"ad\",{\"position\":\"mid-content\"}],[\"p\",{\"class\":\"paywall\"},\"Weeks after the results were released, Google bought the company, reportedly for half a billion dollars. DeepMind placed two unusual conditions on the deal: its work could never be used for espionage or defense purposes, and an ethics board would oversee the research as it drew closer to achieving A.I. Anders Sandberg had told me, “We are happy that they are among the most likely to do it. They recognize there are some problems.”\"],[\"p\",{\"class\":\"paywall\"},\"DeepMind’s chief founder, Demis Hassabis, described his company to the audience at the Royal Society as an “Apollo Program” with a two-part mission: “Step one, solve intelligence. Step two, use it to solve everything else.” Since the test in 2013, his system had aced more than a dozen other Atari titles. Hassabis demonstrated an unpublished trial using a three-dimensional driving game, in which it had quickly outperformed the game’s automated drivers. The plan was to test it in increasingly complex virtual environments and, eventually, in the real world. The patent lists a range of uses, from finance to robotics.\"],[\"p\",{\"class\":\"paywall\"},\"Hassabis was clear about the challenges. DeepMind’s system still fails hopelessly at tasks that require long-range planning, knowledge about the world, or the ability to defer rewards—things that a five-year-old child might be expected to handle. The company is working to give the algorithm conceptual understanding and the capability of transfer learning, which allows humans to apply lessons from one situation to another. These are not easy problems. But DeepMind has more than a hundred Ph.D.s to work on them, and the rewards could be immense. Hassabis spoke of building artificial scientists to resolve climate change, disease, poverty. “Even with the smartest set of humans on the planet working on these problems, these systems might be so complex that it is difficult for individual humans, scientific experts,” he said. “If we can crack what intelligence is, then we can use it to help us solve all these other problems.” He, too, believes that A.I. is a gateway to expanded human potential.\"],[\"p\",{\"class\":\"paywall\"},\"The keynote speaker at the Royal Society was another Google employee: Geoffrey Hinton, who for decades has been a central figure in developing deep learning. As the conference wound down, I spotted him chatting with Bostrom in the middle of a scrum of researchers. Hinton was saying that he did not expect A.I. to be achieved for decades. “No sooner than 2070,” he said. “I am in the camp that is hopeless.”\"],[\"p\",{\"class\":\"paywall\"},\"“In that you think it will not be a cause for good?” Bostrom asked.\"],[\"p\",{\"class\":\"paywall\"},\"“I think political systems will use it to terrorize people,” Hinton said. Already, he believed, agencies like the N.S.A. were attempting to abuse similar technology.\"],[\"p\",{\"class\":\"paywall\"},\"“Then why are you doing the research?” Bostrom asked.\"],[\"p\",{\"class\":\"paywall\"},\"“I could give you the usual arguments,” Hinton said. “But the truth is that the prospect of discovery is too \",[\"em\",\"sweet\"],\".” He smiled awkwardly, the word hanging in the air—an echo of Oppenheimer, who famously said of the bomb, “When you see something that is technically sweet, you go ahead and do it, and you argue about what to do about it only after you have had your technical success.”\"],[\"p\",{\"class\":\"paywall\"},\"As the scientists retreated to tables set up for refreshments, I asked Hinton if he believed an A.I. could be controlled. “That is like asking if a child can control his parents,” he said. “It can happen with a baby and a mother—there is biological hardwiring—but there is not a good track record of less intelligent things controlling things of greater intelligence.” He looked as if he might elaborate. Then a scientist called out, “Let’s all get drinks!”\"],[\"p\",{\"class\":\"paywall\"},\"Bostrom had little interest in the cocktail party. He shook a few hands, then headed for St. James’s Park, a public garden that extends from the gates of Buckingham Palace through central London. The world appeared in splendorous analog: sunlight over trees, duck ponds, children and grandparents feeding birds. The spot had been a park for hundreds of years, and the vista seemed timeless. Yet, during the past millennium, the grounds had also been a marsh, a leper hospital, a deer sanctuary, and royal gardens. It seemed plausible that, a thousand years from now, digital posthumans, regarding it as wasted space, would tear it up, replace the landscaping with computer banks, and erect a vast virtual idyll.\"],[\"p\",{\"class\":\"paywall\"},\"Bostrom’s pace settled into its natural quickness as we circled the park. He talked about his family; he would be seeing his wife and son soon. He was reading widely: history, psychology, economics. He was learning to code. He was thinking about expanding his institute. Although he did not know it then, F.H.I. was about to receive one and a half million dollars from Elon Musk, to create a unit that would craft social policies informed by some of Bostrom’s theories. He would need to hire people. He was also giving thought to the framing of his message. “A lot more is said about the risks than the upsides, but that is not necessarily because the upside is not there,” he told me. “There is just more to be said about the risk—and maybe more use in describing the pitfalls, so we know how to steer around them—than spending time now figuring out the details of how we are going to furnish the great palace a thousand years from now.”\"],[\"inline-embed\",{\"props\":{\"name\":\"inset-left\",\"attrs\":{},\"childTypes\":[\"cartoon\"]},\"ref\":\"\",\"type\":\"callout:inset-left\"},[\"inline-embed\",{\"props\":{\"image\":{\"id\":\"593b64a96f669264efc139ef\",\"altText\":\"“Im bringing on Josh here for when we take over fantasy sports betting.”\",\"contentType\":\"cartoon\",\"sources\":{\"sm\":{\"width\":640,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19698.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19698.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19698.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19698.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19698.jpg 640w\",\"height\":470},\"md\":{\"width\":768,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_768,c_limit\\u002F151123_a19698.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19698.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19698.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19698.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19698.jpg 640w\",\"height\":564},\"lg\":{\"width\":1024,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_1024,c_limit\\u002F151123_a19698.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19698.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19698.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19698.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19698.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19698.jpg 960w\",\"height\":752},\"xl\":{\"width\":1280,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19698.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19698.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19698.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19698.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19698.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19698.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19698.jpg 1280w\",\"height\":940},\"xxl\":{\"width\":1600,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_a19698.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_a19698.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_a19698.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_a19698.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_a19698.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_a19698.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_a19698.jpg 1280w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fcartoons\\u002F593b64a96f669264efc139ef\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_a19698.jpg 1600w\",\"height\":1174}},\"dangerousCaption\":\"“I’m bringing on Josh here for when we take over fantasy sports betting.”\",\"dangerousCredit\":\"\",\"links\":[{\"network\":\"facebook\",\"behavior\":\"popup\",\"url\":\"https:\\u002F\\u002Fwww.facebook.com\\u002Fdialog\\u002Ffeed?&display=popup&caption=&app_id=1147169538698836&link=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19698%3Futm_source%3Dfacebook%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned\",\"label\":\"Share on Facebook\"},{\"network\":\"twitter\",\"url\":\"https:\\u002F\\u002Ftwitter.com\\u002Fintent\\u002Ftweet\\u002F?url=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19698%3Futm_source%3Dtwitter%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned&text=&via=NewYorker\",\"label\":\"Share on X\"},{\"network\":\"email\",\"url\":\"mailto:?subject=&body=https%3A%2F%2Fwww.newyorker.com%2Fcartoon%2Fa19698%3Futm_source%3Donsite-share%26utm_medium%3Demail%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker\",\"label\":\"Share via Email\"},{\"network\":\"store\",\"url\":\"https:\\u002F\\u002Fcondenaststore.com\\u002Fconde-nast-brand\\u002Fcartoons\",\"label\":\"Get in store\"},{\"network\":\"canonical\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcartoon\\u002Fa19698\",\"label\":\"a19698\"}]}},\"level\":\"block\",\"ref\":\"593b64a96f669264efc139ef\",\"type\":\"cartoon\"}]],[\"p\",{\"class\":\"paywall\"},\"We passed a fountain, near a cluster of rocks engineered to give ducks a resting place. Bostrom, in his forties, must soon contend with physical decline, and he spoke with annoyance of the first glimmers of mortality. Even though he is an Alcor member, there is no guarantee that cryonics will work. Perhaps the most radical of his visions is that superintelligent A.I. will hasten the uploading of minds—what he calls “whole-brain emulations”—technology that might not be possible for centuries, if at all. Bostrom, in his most hopeful mode, imagines emulations not only as reproductions of the original intellect “with memory and personality intact”—a soul in the machine—but as minds expandable in countless ways. “We live for seven decades, and we have three-pound lumps of cheesy matter to think with, but to me it is plausible that there could be extremely valuable mental states outside this little particular set of possibilities that might be much better,” he told me.\"],[\"ad\",{\"position\":\"mid-content\"}],[\"p\",{\"class\":\"paywall\"},\"In his book, Bostrom considers a distant future in which trillions of digital minds merge into an enormous cognitive cyber-soup. “Whether the set of extremely positive posthuman modes of being would include some kind of dissolved bouillon, there is some uncertainty,” he said. “If you look at religious views, there are many where merging with something greater is a form of heaven, being in the presence of this enormous beauty and goodness. In many traditions, the best possible state does not involve being a little individual pursuing goals. But it is hard to get a grasp of what would be going on in that soup. Maybe some soups would not be preferable as a long-term outcome. I don’t know.” He stopped and looked ahead. “What I want to avoid is to think from our parochial 2015 view—from my own limited life experience, my own limited brain—and super-confidentially postulate what is the best form for civilization a billion years from now, when you could have brains the size of planets and billion-year life spans. It seems unlikely that we will figure out some detailed blueprint for utopia. What if the great apes had asked whether they should evolve into \",[\"em\",\"Homo sapiens\"],\"—pros and cons—and they had listed, on the pro side, ‘Oh, we could have a lot of bananas if we became human’? Well, we can have unlimited bananas now, but there is more to the human condition than that.” ♦\"],[\"p\",{\"class\":\"paywall\"},[\"em\",\"Illustration by Todd St. John\\u002FCoding by Jono Brandel.\"]]],\"pageStructure\":[[\"h2\",2],[\"p\",112],[\"cm-unit\",0],[\"p\",105],[\"p\",120],[\"p\",63],[\"p\",184],[\"p\",172],[\"p\",109],[\"p\",153],[\"ad\",0],[\"p\",92],[\"p\",121],[\"inline-embed\",0,\"cneinterlude\"],[\"p\",153],[\"p\",132],[\"p\",58],[\"p\",151],[\"p\",97],[\"p\",198],[\"inline-embed\",0,\"callout\"],[\"p\",185],[\"ad\",0],[\"p\",111],[\"p\",174],[\"p\",36],[\"p\",120],[\"p\",102],[\"p\",71],[\"p\",86],[\"p\",27],[\"p\",76],[\"p\",117],[\"ad\",0],[\"p\",132],[\"p\",60],[\"p\",141],[\"p\",71],[\"p\",153],[\"p\",150],[\"p\",198],[\"ad\",0],[\"p\",148],[\"p\",145],[\"p\",171],[\"p\",151],[\"p\",105],[\"p\",129],[\"h2\",3],[\"p\",93],[\"ad\",0],[\"p\",154],[\"p\",106],[\"inline-embed\",0,\"callout\"],[\"p\",153],[\"p\",108],[\"p\",146],[\"p\",110],[\"p\",175],[\"ad\",0],[\"p\",193],[\"p\",162],[\"inline-embed\",0,\"callout\"],[\"p\",170],[\"p\",63],[\"p\",130],[\"p\",122],[\"p\",68],[\"ad\",0],[\"p\",133],[\"p\",107],[\"p\",87],[\"p\",263],[\"p\",147],[\"p\",209],[\"ad\",0],[\"p\",118],[\"p\",180],[\"p\",80],[\"h2\",3],[\"p\",144],[\"p\",146],[\"p\",120],[\"inline-embed\",0,\"callout\"],[\"p\",188],[\"ad\",0],[\"p\",100],[\"p\",166],[\"p\",156],[\"p\",74],[\"p\",154],[\"p\",206],[\"inline-embed\",0,\"callout\"],[\"p\",158],[\"ad\",0],[\"p\",122],[\"p\",151],[\"p\",132],[\"p\",101],[\"p\",46],[\"p\",62],[\"p\",101],[\"p\",114],[\"p\",80],[\"inline-embed\",0,\"callout\"],[\"p\",161],[\"ad\",0],[\"p\",74],[\"p\",99],[\"p\",161],[\"p\",72],[\"p\",14],[\"p\",25],[\"p\",9],[\"p\",72],[\"p\",78],[\"p\",117],[\"p\",170],[\"inline-embed\",0,\"callout\"],[\"p\",159],[\"ad\",0],[\"p\",240],[\"p\",8]],\"hasAffiliateLinks\":false,\"affiliateLinksCount\":0,\"isCommerceContent\":false,\"contributorSpotLightProps\":{\"header\":{\"name\":\"Raffi Khatchadourian\",\"title\":\"Raffi Khatchadourian became a staff writer at The New Yorker in 2008.\",\"url\":\"\\u002Fcontributors\\u002Fraffi-khatchadourian\",\"dangerousBio\":\"\\u003Ca href=\\\"\\u002Fcontributors\\u002Fraffi-khatchadourian\\\"\\u003ERaffi Khatchadourian\\u003C\\u002Fa\\u003E has been a staff writer at \\u003Cem\\u003EThe New Yorker\\u003C\\u002Fem\\u003E since 2008. He covers a wide range of topics, including science, art, politics, foreign affairs, and national security. His articles have been anthologized in “Best American Sports Writing” and in “Best American Nonrequired Reading.” On two occasions, Khatchadourian’s work... \\u003Ca href=\\\"\\u002Fcontributors\\u002Fraffi-khatchadourian\\\"\\u003ERead more\\u003C\\u002Fa\\u003E\"}},\"contentWarnings\":[],\"coralComments\":{\"enableComments\":false,\"coralHostName\":\"newyorker.coral.coralproject.net\"},\"showBookmark\":true,\"featureFlags\":{\"shouldUseBookmarkV3\":true,\"enableSlimNewsletter\":false,\"enableAllContributorsOnBundles\":true,\"enableEntitlementProxy\":true,\"enableEntitlementValidation\":true,\"enableFictionContributor\":true,\"enableGqlForLinkBanner\":true,\"enableLinkStack\":true,\"enableRecipeRatings\":false,\"enableUserContext\":true,\"hideRelatedOnBundles\":true,\"shouldExtractRecircRubricFromCategories\":true,\"recentWorkTeaser\":\"rubric-or-channel\",\"bundleTeaser\":\"rubric-or-channel-or-section\",\"contentTeaser\":\"rubric-or-channel-or-section\",\"tagTeaser\":\"rubric-or-channel\",\"preferCollectionGrid\":true,\"overrideBodyContentHeadings\":true,\"enableSponsoredContentInRelated\":false,\"personalizeRecircMostPopular\":true,\"videoPersistable\":false,\"google\":{\"swgEnabled\":false,\"signInEnabled\":true},\"featureOnboarding\":{\"bookmarks\":false},\"enableContributorAuthorHub\":true,\"applyPlaceHolderImage\":false,\"enableAudioIcon\":false,\"enableEnhancedCartoonExperience\":true},\"isUpcEnabled\":false,\"isLinkStackEnabled\":true,\"hasTruncationOnMobile\":false,\"hasProduct\":false,\"hasProductLisitingCarousel\":false,\"headerProps\":{\"sponsoredContentHeaderProps\":null,\"contentHeaderCategories\":{\"hasCategoryEyebrow\":false},\"contentSponsorNames\":[],\"contributors\":{\"author\":{\"items\":[{\"dangerousBio\":\"\\u003Ca href=\\\"\\u002Fcontributors\\u002Fraffi-khatchadourian\\\"\\u003ERaffi Khatchadourian\\u003C\\u002Fa\\u003E has been a staff writer at \\u003Cem\\u003EThe New Yorker\\u003C\\u002Fem\\u003E since 2008. He covers a wide range of topics, including science, art, politics, foreign affairs, and national security. His articles have been anthologized in “Best American Sports Writing” and in “Best American Nonrequired Reading.” On two occasions, Khatchadourian’s work... \\u003Ca href=\\\"\\u002Fcontributors\\u002Fraffi-khatchadourian\\\"\\u003ERead more\\u003C\\u002Fa\\u003E\",\"dangerousTitle\":\"\\u003Ca href=\\\"\\u002Fcontributors\\u002Fraffi-khatchadourian\\\"\\u003ERaffi Khatchadourian\\u003C\\u002Fa\\u003E became a staff writer at The New Yorker in 2008.\",\"name\":\"Raffi Khatchadourian\",\"socialMedia\":[{\"handle\":\"raffiwriter\",\"network\":\"twitter\",\"label\":\"Follow Raffi Khatchadourian on X\"}],\"url\":\"\\u002Fcontributors\\u002Fraffi-khatchadourian\"}]}},\"dangerousHed\":\"The Doomsday Invention\",\"dangerousDek\":\"\",\"hasContentHeaderLogo\":false,\"hidePublishDate\":false,\"issueDate\":\"November 23, 2015\",\"issueLink\":\"\\u002Fmagazine\\u002F2015\\u002F11\\u002F23\",\"lede\":{\"altText\":\"An animation of floating 3D puzzle pieces\",\"caption\":\"Nick Bostrom, a philosopher focussed on A.I. risks, says, “The very long-term future of humanity may be relatively easy to predict.”\",\"contentType\":\"photo\",\"credit\":\"Illustration by Todd St. John\",\"id\":\"590971f5ebe912338a377328\",\"sources\":{\"sm\":{\"aspectRatio\":\"master\",\"width\":360,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_360,c_limit\\u002F151123_r27342.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_r27342.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_r27342.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_r27342.jpg 320w\",\"height\":281},\"md\":{\"aspectRatio\":\"master\",\"width\":1024,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_1024,c_limit\\u002F151123_r27342.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_r27342.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_r27342.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_r27342.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_r27342.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_r27342.jpg 960w\",\"height\":800},\"lg\":{\"aspectRatio\":\"master\",\"width\":1280,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_r27342.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_r27342.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_r27342.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_r27342.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_r27342.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_r27342.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_r27342.jpg 1280w\",\"height\":1000},\"xl\":{\"aspectRatio\":\"master\",\"width\":1280,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_r27342.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_r27342.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_r27342.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_r27342.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_r27342.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_r27342.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_r27342.jpg 1280w\",\"height\":1000},\"xxl\":{\"aspectRatio\":\"master\",\"width\":2560,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_2560,c_limit\\u002F151123_r27342.jpg\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_120,c_limit\\u002F151123_r27342.jpg 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_240,c_limit\\u002F151123_r27342.jpg 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_320,c_limit\\u002F151123_r27342.jpg 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_640,c_limit\\u002F151123_r27342.jpg 640w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_960,c_limit\\u002F151123_r27342.jpg 960w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_1280,c_limit\\u002F151123_r27342.jpg 1280w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_1600,c_limit\\u002F151123_r27342.jpg 1600w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_1920,c_limit\\u002F151123_r27342.jpg 1920w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F590971f5ebe912338a377328\\u002Fmaster\\u002Fw_2240,c_limit\\u002F151123_r27342.jpg 2240w\",\"height\":2000}},\"masterAspectRatio\":\"2560:2000\"},\"modifiedDate\":\"2015-11-15T23:00:00-05:00\",\"isPeritextEnabled\":false,\"publishDate\":\"November 15, 2015\",\"rubric\":{\"name\":\"A Reporter at Large\",\"url\":\"\\u002Fmagazine\\u002Fa-reporter-at-large\"},\"showIssueCopyByDate\":true,\"hideRubric\":false,\"socialMedia\":{\"showBookmark\":true,\"links\":[{\"network\":\"facebook\",\"behavior\":\"popup\",\"url\":\"https:\\u002F\\u002Fwww.facebook.com\\u002Fdialog\\u002Ffeed?&display=popup&caption=The%20Doomsday%20Invention&app_id=1147169538698836&link=https%3A%2F%2Fwww.newyorker.com%2Fmagazine%2F2015%2F11%2F23%2Fdoomsday-invention-artificial-intelligence-nick-bostrom%3Futm_source%3Dfacebook%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned\",\"label\":\"Share on Facebook\"},{\"network\":\"twitter\",\"url\":\"https:\\u002F\\u002Ftwitter.com\\u002Fintent\\u002Ftweet\\u002F?url=https%3A%2F%2Fwww.newyorker.com%2Fmagazine%2F2015%2F11%2F23%2Fdoomsday-invention-artificial-intelligence-nick-bostrom%3Futm_source%3Dtwitter%26utm_medium%3Dsocial%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker%26utm_social-type%3Dearned&text=The%20Doomsday%20Invention&via=NewYorker\",\"label\":\"Share on X\"},{\"network\":\"email\",\"url\":\"mailto:?subject=The%20Doomsday%20Invention&body=https%3A%2F%2Fwww.newyorker.com%2Fmagazine%2F2015%2F11%2F23%2Fdoomsday-invention-artificial-intelligence-nick-bostrom%3Futm_source%3Donsite-share%26utm_medium%3Demail%26utm_campaign%3Donsite-share%26utm_brand%3Dthe-new-yorker\",\"label\":\"Share via Email\"},{\"network\":\"print\",\"behavior\":\"print\",\"label\":\"Print\",\"url\":\"#\"},{\"behavior\":\"bookmark\",\"label\":\"Save story\",\"network\":\"bookmark\",\"url\":\"#\"}]},\"socialTitle\":\"The Doomsday Invention\",\"socialDescription\":\"Will artificial intelligence bring us utopia or destruction?\",\"signage\":null},\"hierarchy\":[],\"interactiveOverride\":{\"markup\":\"\\u003Clink rel=\\\"stylesheet\\\" href=\\\"https:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Farchive\\u002Flogo.css\\\" \\u002F\\u003E\\n\\u003Clink rel=\\\"stylesheet\\\" href=\\\"https:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Farchive\\u002Fcritical.min.css@v=20190418195522.css\\\" \\u002F\\u003E\\n\\u003Cstyle\\u003Ehtml p.descender::first-letter{line-height:82px;background:url(https:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Farchive\\u002Fimages\\u002Fdrop-caps\\u002FT.jpg) center center;background-size:cover !important;color:transparent;font-size:0;padding:28px 10px;margin:0 20px 0 0}html p.descender.dropcap-A::first-letter{background:url(https:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Farchive\\u002Fimages\\u002Fdrop-caps\\u002FA.jpg) center center}html p.descender.dropcap-B::first-letter{background:url(https:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Farchive\\u002Fimages\\u002Fdrop-caps\\u002FB.jpg) center center}html p.descender.dropcap-I::first-letter{background:url(https:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Farchive\\u002Fimages\\u002Fdrop-caps\\u002FI.jpg) center center;background-size:99% 99% !important}html p.descender.dropcap-L::first-letter{background:url(https:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Farchive\\u002Fimages\\u002Fdrop-caps\\u002FL.jpg) center center}html p.descender.dropcap-T::first-letter{background:url(https:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Farchive\\u002Fimages\\u002Fdrop-caps\\u002FT.jpg) center center}html p.descender.dropcap-O::first-letter{background:url(https:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Farchive\\u002Fimages\\u002Fdrop-caps\\u002FO.jpg) center center}html.reduce-descender-spacing p.descender::first-letter{padding:41px 28px}html.reduce-descender-spacing p.descender.dropcap-I::first-letter{padding:41px 18px}.player-resize{max-width:1040px;width:100%;z-index:999999;position:absolute;margin-bottom:480px}@media (max-width: 420px){.player-resize{margin-bottom:275px}}@media (min-width: 53.125em){.player-resize{margin-bottom:500px}}span.videospacer{display:block;height:625px;width:1px}#featured-ai{background-image:none;background-size:100% auto;width:100%;min-height:600px;-webkit-transition:background-image 2s ease-in;transition:background-image 2s ease-in}#featured-ai.fallback{background-image:url(\\\"..\\u002F..\\u002F..\\u002F..\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_r27342.jpg\\\")}.false-header{margin:0 !important;max-width:none !important;padding:0 !important;pointer-events:none;position:absolute !important;top:0;width:100%}.false-header #page{background-color:transparent !important;border:none;left:0 !important;opacity:1}.false-header #page #masthead,.false-header #page .social-module{pointer-events:all}.false-header .background-image{display:none}#content #articleBody h2,#content #articleBody h2.aligncenter{font-family:\\\"Irvin Heading\\\",\\\"Helvetica Neue\\\",Helvetica,Arial,sans-serif;font-weight:normal}figure.alignleft.size-full{padding-bottom:0 !important}\\n\\n.media-block--hero.crop-center .background-image, .media-block--hero.crop-center-desktop .background-image {\\n\\tdisplay: none; \\n}\\n\\nbody.article {\\n\\tpadding-top: 0 !important;\\n}\\n\\n.contributors {\\n  border-bottom-width: 0px;\\n    border-top-width: 0px;\\n    font-style: normal;\\n}\\n\\n#featured-ai {\\n\\tposition: absolute;\\n\\tz-index: 0;\\n\\twidth: 100%;\\n\\theight: 100vh;\\n\\tleft: 0;\\n\\ttop: 0;\\n}\\n\\n#page {\\n\\tbackground: transparent;\\n\\tborder: 0;\\n}\\n  \\n.hero-image-caption {\\n  margin: 60px auto 0 !important;\\n  padding-left: 20px !important;\\n  padding-right: 20px !important;\\n}\\n\\n@media (min-width: 600px) {\\n  .hero-image-caption {\\n    max-width: 1060px;\\n    margin: 20px auto 0 !important;\\n  }\\n}\\n\\n.media-cne {\\n\\tmargin-left: auto !important;\\n    margin-right: auto !important;\\n    max-width: 650px;\\n}\\n.cne-interlude-container {\\n\\tdisplay: block;\\n}\\n\\n#content {\\n  max-width: 1060px;\\n  margin: 0 auto;\\n}\\n\\n\\u003C\\u002Fstyle\\u003E\\n\\n\\u003Cdiv class=\\\"article\\\"\\u003E\\n\\t\\u003Cdiv class=\\\"logo_wrapper\\\"\\u003E\\n\\t\\t\\u003Cdiv class=\\\"logo_inner\\\"\\u003E\\n\\t\\t\\t\\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002F\\\" target=\\\"_top\\\"\\u003E\\u003Cimg src=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fprojects\\u002Finteractive\\u002F2019\\u002F190211-kaminsky\\u002Fassets\\u002Fsvg\\u002Ftny_logo.svg\\\"\\u003E\\u003C\\u002Fa\\u003E\\n\\t\\t\\u003C\\u002Fdiv\\u003E\\n\\t\\t\\u003Cdiv class=\\\"subscribe\\\"\\u003E\\u003Ca href=\\\"https:\\u002F\\u002Fsubscribe.newyorker.com\\u002Fsubscribe\\u002Fnewyorker\\u002F\\\"\\u003ESubscribe »\\u003C\\u002Fa\\u003E\\u003C\\u002Fdiv\\u003E\\n\\t\\u003C\\u002Fdiv\\u003E\\n\\t\\u003Ciframe class=\\\"featured-ai fallback\\\" id=\\\"featured-ai\\\" src=\\\"http:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Ffeatured\\u002Fi9\\u002Findex.html\\\" style=\\\"height: 850px;\\\"\\u003E\\u003C\\u002Fiframe\\u003E\\n\\t\\u003Carticle class=\\\"single-column header-ad-hidden\\\" data-details=\\\"{&quot;title&quot;:&quot;The Doomsday Invention&quot;,&quot;author&quot;:[28442,&quot;Raffi Khatchadourian&quot;,&quot;https:\\\\\\u002F\\\\\\u002Fwww.newyorker.com\\\\\\u002Fcontributors\\\\\\u002Fraffi-khatchadourian&quot;],&quot;wordCount&quot;:12441,&quot;paragraphCount&quot;:92}\\\" itemscope itemtype=\\\"http:\\u002F\\u002Fschema.org\\u002FNewsArticle\\\" itemid=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2015\\u002F11\\u002F23\\u002Fdoomsday-invention-artificial-intelligence-nick-bostrom\\\"\\u003E\\n\\t\\t\\u003Cheader id=\\\"page\\\" \\t\\tclass=\\\"is-ready js-media-block media-block media-block--hero crop-center text-align-x-center text-align-y-center text-color-dark\\\" data-format=\\\"full-bleed\\\"\\u003E\\n\\t\\t\\u003Chgroup id=\\\"masthead\\\"\\u003E\\n\\n\\t\\t\\t\\u003Cdiv class=\\\"masthead-wrapper\\\"\\u003E\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\u003Cdiv class=\\\"rubric-and-issue-date\\\"\\u003E\\n\\t\\t\\t\\t\\t\\u003Ch4 class=\\\"rubric\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002Fa-reporter-at-large\\\" title=\\\"A Reporter at Large\\\"\\u003EA Reporter at Large\\u003C\\u002Fa\\u003E\\n\\t\\t\\t\\t\\t\\u003C\\u002Fh4\\u003E\\n\\t\\t\\t\\t\\t\\u003Ca class=\\\"issue-publish-date-link\\\" href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2015\\u002F11\\u002F23\\\" title=\\\"Published in 2015-11-23\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\u003Ctime class=\\\"issue\\\" itemprop=\\\"datePublished\\\" content=\\\"2015-11-16\\\"\\u003ENovember 23, 2015 Issue\\u003C\\u002Ftime\\u003E\\n\\t\\t\\t\\t\\t\\u003C\\u002Fa\\u003E\\n\\t\\t\\t\\t\\t\\u003Cmeta itemprop=\\\"dateModified\\\" content=\\\"September 7, 2017\\\" \\u002F\\u003E\\n\\t\\t\\t\\t\\u003C\\u002Fdiv\\u003E\\n\\t\\t\\t\\t\\u003Ch1 class=\\\"title\\\" itemprop=\\\"headline\\\"\\u003EThe Doomsday Invention\\u003C\\u002Fh1\\u003E\\n\\t\\t\\t\\t\\u003Ch2 class=\\\"dek\\\" itemprop=\\\"alternativeHeadline\\\"\\u003EWill artificial intelligence bring us utopia or destruction?\\u003C\\u002Fh2\\u003E\\u003Cdiv class=\\\"byline-and-date\\\"\\u003E\\n\\t\\t\\t\\t\\t\\u003Cdiv style=\\\"background-image: url(..\\u002F..\\u002F..\\u002F..\\u002Fwp-content\\u002Fuploads\\u002F2014\\u002F03\\u002Fraffi-khatchadourian.jpg)\\\" class=\\\"contributor-image\\\" \\u003E\\u003C\\u002Fdiv\\u003E\\u003Ch3 class=\\\"contributors\\\"\\u003EBy\\u003Cspan itemscope itemprop=\\\"author\\\" itemtype=\\\"http:\\u002F\\u002Fschema.org\\u002FPerson\\\"\\u003E \\u003Cmeta itemprop=\\\"url\\\" content=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcontributors\\u002Fraffi-khatchadourian\\\" \\u003E \\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcontributors\\u002Fraffi-khatchadourian\\\" title=\\\"Raffi Khatchadourian\\\" rel=\\\"author\\\" itemprop=\\\"url\\\"\\u003E\\u003Cspan itemprop=\\\"name\\\"\\u003ERaffi Khatchadourian\\u003C\\u002Fspan\\u003E\\u003C\\u002Fa\\u003E\\u003C\\u002Fspan\\u003E\\u003C\\u002Fh3\\u003E\\u003C\\u002Fdiv\\u003E\\n\\t\\t\\t\\t\\t\\u003Cdiv class=\\\"social-share-links js-social-module\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\u003Cul class=\\\"options\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\u003Cli class=\\\"option facebook js-option-facebook\\\"\\u003E\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\u003Ca class=\\\"facebook\\\" href=\\\"https:\\u002F\\u002Fwww.facebook.com\\u002Fsharer\\u002Fsharer.php?u=https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2015\\u002F11\\u002F23\\u002Fdoomsday-invention-artificial-intelligence-nick-bostrom&display=popup&ref=plugin\\\" target=\\\"_blank\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Csvg width=\\\"10px\\\" height=\\\"18px\\\" viewBox=\\\"7 3 10 18\\\" version=\\\"1.1\\\" xmlns=\\\"http:\\u002F\\u002Fwww.w3.org\\u002F2000\\u002Fsvg\\\" xmlns:xlink=\\\"http:\\u002F\\u002Fwww.w3.org\\u002F1999\\u002Fxlink\\\"\\u003E\\u003Cpath d=\\\"M13.3165,20.5 L13.3165,12.746 L15.9185,12.746 L16.3085,9.723 L13.3165,9.723 L13.3165,7.794 C13.3165,6.919 13.5585,6.323 14.8145,6.323 L16.4135,6.322 L16.4135,3.618 C16.1375,3.582 15.1875,3.5 14.0825,3.5 C11.7755,3.5 10.1955,4.908 10.1955,7.494 L10.1955,9.723 L7.5865,9.723 L7.5865,12.746 L10.1955,12.746 L10.1955,20.5 L13.3165,20.5 Z\\\" id=\\\"Fill-5\\\" stroke=\\\"none\\\" fill=\\\"#000000\\\" fill-rule=\\\"evenodd\\\"\\u003E\\u003C\\u002Fpath\\u003E\\u003C\\u002Fsvg\\u003E\\t\\t\\t\\u003C\\u002Fa\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fli\\u003E\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cli class=\\\"option twitter js-option-twitter\\\"\\u003E\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Ca class=\\\"twitter\\\" href=\\\"https:\\u002F\\u002Ftwitter.com\\u002Fintent\\u002Ftweet?original_referer=https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2015\\u002F11\\u002F23\\u002Fdoomsday-invention-artificial-intelligence-nick-bostrom&text=The+Doomsday+Invention&tw_p=tweetbutton&url=https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2015\\u002F11\\u002F23\\u002Fdoomsday-invention-artificial-intelligence-nick-bostrom&via=raffiwriter\\\" target=\\\"_blank\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Csvg width=\\\"18px\\\" height=\\\"16px\\\" viewBox=\\\"3 4 18 16\\\" version=\\\"1.1\\\" xmlns=\\\"http:\\u002F\\u002Fwww.w3.org\\u002F2000\\u002Fsvg\\\" xmlns:xlink=\\\"http:\\u002F\\u002Fwww.w3.org\\u002F1999\\u002Fxlink\\\"\\u003E\\u003Cpath d=\\\"M21,6.4175 C20.338,6.7115 19.626,6.9095 18.879,6.9985 C19.641,6.5415 20.227,5.8185 20.503,4.9555 C19.789,5.3795 18.999,5.6865 18.158,5.8525 C17.484,5.1345 16.524,4.6855 15.462,4.6855 C13.423,4.6855 11.769,6.3395 11.769,8.3785 C11.769,8.6685 11.802,8.9495 11.865,9.2205 C8.796,9.0665 6.074,7.5965 4.253,5.3615 C3.935,5.9075 3.753,6.5415 3.753,7.2185 C3.753,8.4995 4.405,9.6295 5.396,10.2925 C4.791,10.2735 4.221,10.1065 3.723,9.8305 L3.723,9.8765 C3.723,11.6655 4.996,13.1585 6.685,13.4975 C6.375,13.5825 6.049,13.6275 5.712,13.6275 C5.474,13.6275 5.243,13.6045 5.018,13.5615 C5.488,15.0285 6.851,16.0955 8.467,16.1255 C7.203,17.1165 5.611,17.7065 3.881,17.7065 C3.583,17.7065 3.289,17.6895 3,17.6545 C4.634,18.7025 6.575,19.3145 8.661,19.3145 C15.454,19.3145 19.168,13.6865 19.168,8.8065 C19.168,8.6465 19.164,8.4875 19.157,8.3295 C19.879,7.8085 20.505,7.1585 21,6.4175\\\" id=\\\"Fill-3\\\" stroke=\\\"none\\\" fill=\\\"#000000\\\" fill-rule=\\\"evenodd\\\"\\u003E\\u003C\\u002Fpath\\u003E\\u003C\\u002Fsvg\\u003E\\t\\t\\t\\u003C\\u002Fa\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fli\\u003E\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cli class=\\\"option email js-option-email\\\"\\u003E\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Ca class=\\\"email\\\" href=\\\"mailto:?subject=From%20newyorker.com:%20The%20Doomsday%20Invention&amp;body=The%20Doomsday%20Invention%0Ahttps%3A%2F%2Fwww.newyorker.com%2Fmagazine%2F2015%2F11%2F23%2Fdoomsday-invention-artificial-intelligence-nick-bostrom\\\" target=\\\"_blank\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Csvg width=\\\"19px\\\" height=\\\"15px\\\" viewBox=\\\"2 5 19 15\\\" version=\\\"1.1\\\" xmlns=\\\"http:\\u002F\\u002Fwww.w3.org\\u002F2000\\u002Fsvg\\\" xmlns:xlink=\\\"http:\\u002F\\u002Fwww.w3.org\\u002F1999\\u002Fxlink\\\"\\u003E\\u003Cpath d=\\\"M12.0778,14.4853 L12.0748,14.4823 L12.0708,14.4853 L4.9998,7.4143 L6.4138,6.0003 L12.0748,11.6603 L17.7348,6.0003 L19.1488,7.4143 L12.0778,14.4853 Z M2.9998,19.0003 L20.9998,19.0003 L20.9998,5.0003 L2.9998,5.0003 L2.9998,19.0003 Z\\\" id=\\\"Fill-3\\\" stroke=\\\"none\\\" fill=\\\"#000000\\\" fill-rule=\\\"evenodd\\\"\\u003E\\u003C\\u002Fpath\\u003E\\u003C\\u002Fsvg\\u003E\\t\\t\\t\\u003C\\u002Fa\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fli\\u003E\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cli class=\\\"option print js-option-print\\\"\\u003E\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Ca class=\\\"print\\\" href=\\\"doomsday-invention-artificial-intelligence-nick-bostrom.html#\\\" target=\\\"_blank\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Csvg width=\\\"18px\\\" height=\\\"16px\\\" version=\\\"1.1\\\" id=\\\"Layer_1\\\" xmlns=\\\"http:\\u002F\\u002Fwww.w3.org\\u002F2000\\u002Fsvg\\\" xmlns:xlink=\\\"http:\\u002F\\u002Fwww.w3.org\\u002F1999\\u002Fxlink\\\" x=\\\"0px\\\" y=\\\"0px\\\"\\t viewBox=\\\"159.1 247.8 295 294.7\\\" enable-background=\\\"new 159.1 247.8 295 294.7\\\" xml:space=\\\"preserve\\\"\\u003E\\u003Cg\\u003E\\t\\u003Cpath fill=\\\"#000000\\\" d=\\\"M232.6,266.2h147.5v37h18.4v-37c0-10.3-8.4-18.4-18.4-18.4H232.6c-10.3,0-18.4,8.4-18.4,18.4v37h18.4V266.2\\t\\tz\\\"\\u002F\\u003E\\t\\u003Cpath fill=\\\"#000000\\\" d=\\\"M435.8,321.6H177.5c-10.3,0-18.4,8.4-18.4,18.4v92.1c0,10.3,8.4,18.4,18.4,18.4h37v73.8\\t\\tc0,10.3,8.4,18.4,18.4,18.4h147.5c10.3,0,18.4-8.4,18.4-18.4v-73.4h37c10.3,0,18.4-8.4,18.4-18.4V340\\t\\tC454.2,330,445.8,321.6,435.8,321.6z M380.4,524.5H232.6V395.4h147.5v129.2H380.4z M417.1,377c-10.3,0-18.4-8.4-18.4-18.4\\t\\tc0-10.3,8.4-18.4,18.4-18.4c10.3,0,18.4,8.4,18.4,18.4C435.8,368.6,427.4,377,417.1,377z\\\"\\u002F\\u003E\\t\\u003Crect x=\\\"251.2\\\" y=\\\"414\\\" fill=\\\"#000000\\\" width=\\\"73.8\\\" height=\\\"18.4\\\"\\u002F\\u003E\\t\\u003Crect x=\\\"251.2\\\" y=\\\"450.8\\\" fill=\\\"#000000\\\" width=\\\"110.8\\\" height=\\\"18.4\\\"\\u002F\\u003E\\t\\u003Crect x=\\\"251.2\\\" y=\\\"487.8\\\" fill=\\\"#000000\\\" width=\\\"110.8\\\" height=\\\"18.4\\\"\\u002F\\u003E\\u003C\\u002Fg\\u003E\\u003C\\u002Fsvg\\u003E\\t\\t\\t\\u003C\\u002Fa\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fli\\u003E\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Ful\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fdiv\\u003E\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003C!-- end masthead-wrapper --\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fdiv\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fhgroup\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\u003Cdiv class=\\\"background-image\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cdiv class=\\\"image js-background-image\\\" style=\\\"background-image: url('https:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Farchive\\u002Fimages\\u002F151123_r27342-1992x2400-1447365389.jpg')\\\"\\u003E\\u003C\\u002Fdiv\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fdiv\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\u003Cdiv class=\\\"background-image mobile-crop\\\"\\u003E\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cdiv class=\\\"image js-background-image\\\" style=\\\"background-image: url('https:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Farchive\\u002Fimages\\u002F151123_r27342-690x831-1447365389.jpg')\\\"\\u003E\\u003C\\u002Fdiv\\u003E\\n\\n\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fdiv\\u003E\\n\\t\\t\\t\\t\\t\\t\\u003C\\u002Fheader\\u003E\\n\\n\\t\\t\\t\\t\\t\\t\\u003Caside class=\\\"hero-image-caption\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\u003Cspan\\u003ENick Bostrom, a philosopher focussed on A.I. risks, says, “The very long-term future of humanity may be relatively easy to predict.”\\u003C\\u002Fspan\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\u003Cspan class=\\\"hero-image-credit\\\"\\u003EIllustration by Todd St. John\\u003C\\u002Fspan\\u003E\\n\\t\\t\\t\\t\\t\\t\\u003C\\u002Faside\\u003E\\n\\n\\t\\t\\t\\t\\t\\t\\u003Cdiv id=\\\"content\\\" \\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\u003Cdiv itemprop=\\\"articleBody\\\" class=\\\"articleBody\\\" id=\\\"articleBody\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\u003C!-- Barrier Status: '' --\\u003E\\u003Ch2 class=\\\"aligncenter\\\"\\u003EI. Omens\\u003C\\u002Fh2\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F2\\\" data-total-words=\\\"0\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Ca class=\\\"tny-page\\\" name=\\\"\\u002F1\\\" data-total-words=\\\"0\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp class=\\\"descender dropcap-L\\\" word_count=\\\"113\\\" data-wc=\\\"113\\\"\\u003E Last year, a curious nonfiction book became a \\u003Cem\\u003ETimes\\u003C\\u002Fem\\u003E best-seller: a dense meditation on artificial intelligence by the philosopher Nick Bostrom, who holds an appointment at Oxford. Titled “Superintelligence: Paths, Dangers, Strategies,” it argues that true artificial intelligence, if it is realized, might pose a danger that exceeds every previous threat from technology—even nuclear weapons—and that if its development is not managed carefully humanity risks engineering its own extinction. Central to this concern is the prospect of an “intelligence explosion,” a speculative event in which an A.I. gains the ability to improve itself, and in short order exceeds the intellectual potential of the human brain by many orders of magnitude. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"106\\\" data-wc=\\\"106\\\"\\u003ESuch a system would effectively be a new kind of life, and Bostrom’s fears, in their simplest form, are evolutionary: that humanity will unexpectedly become outmatched by a smarter competitor. He sometimes notes, as a point of comparison, the trajectories of people and gorillas: both primates, but with one species dominating the planet and the other at the edge of annihilation. “Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb,” he concludes. “We have little idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"122\\\" data-wc=\\\"122\\\"\\u003EAt the age of forty-two, Bostrom has become a philosopher of remarkable influence. “Superintelligence” is only his most visible response to ideas that he encountered two decades ago, when he became a transhumanist, joining a fractious quasi-utopian movement united by the expectation that accelerating advances in technology will result in drastic changes—social, economic, and, most strikingly, biological—which could converge at a moment of epochal transformation known as the Singularity. Bostrom is arguably the leading transhumanist philosopher today, a position achieved by bringing order to ideas that might otherwise never have survived outside the half-crazy Internet ecosystem where they formed. He rarely makes concrete predictions, but, by relying on probability theory, he seeks to tease out insights where insights seem impossible. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"66\\\" data-wc=\\\"66\\\"\\u003ESome of Bostrom’s cleverest arguments resemble Swiss Army knives: they are simple, toylike, a pleasure to consider, with colorful exteriors and precisely calibrated mechanics. He once cast a moral case for medically engineered immortality as a fable about a kingdom terrorized by an insatiable dragon. A reformulation of Pascal’s wager became a dialogue between the seventeenth-­century philosopher and a mugger from another dimension. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"193\\\" data-wc=\\\"193\\\"\\u003E“Superintelligence” is not intended as a treatise of deep originality; Bostrom’s contribution is to impose the rigors of analytic philosophy on a messy corpus of ideas that emerged at the margins of academic thought. Perhaps because the field of A.I. has recently made striking advances—with everyday technology seeming, more and more, to exhibit something like intelligent reasoning—the book has struck a nerve. Bostrom’s supporters compare it to “Silent Spring.” In moral philosophy, Peter Singer and Derek Parfit have received it as a work of importance, and distinguished physicists such as Stephen Hawking have echoed its warning. Within the high caste of Silicon Valley, Bostrom has acquired the status of a sage. Elon Musk, the C.E.O. of Tesla, promoted the book on Twitter, noting, “We need to be super careful with AI. Potentially more dangerous than nukes.” Bill Gates recommended it, too. Suggesting that an A.I. could threaten humanity, he said, during a talk in China, “When people say it’s not a problem, then I really start to get to a point of disagreement. How can they not see what a huge challenge this is?”\\u003C\\u002Fp\\u003E\\u003Cdiv class=\\\"content-ad-wrapper first\\\"\\u003E\\u003Cdiv class=\\\"advertisement\\\" data-ismobile=\\\"true\\\" data-name=\\\"yrailTop\\\" data-sz=\\\"320x251\\\" data-kw=\\\"topBox\\\"\\u003E\\u003Cdiv id=\\\"yrailTop320x251_frame\\\" class=\\\"displayAd displayAd320x251Js\\\" data-cb-ad-id=\\\"yrailTop320x251_frame\\\"\\u003E\\u003C\\u002Fdiv\\u003E\\u003C\\u002Fdiv\\u003E\\u003Cdiv class=\\\"advertisement\\\" data-ismobile=\\\"false\\\" data-name=\\\"inlineTop\\\" data-sz=\\\"728x90\\\" data-kw=\\\"topBanner\\\"\\u003E\\u003Cdiv id=\\\"inlineTop728x90_frame\\\" class=\\\"displayAd displayAd728x90Js\\\" data-cb-ad-id=\\\"inlineTop728x90_frame\\\"\\u003E\\u003C\\u002Fdiv\\u003E\\u003C\\u002Fdiv\\u003E\\u003C\\u002Fdiv\\u003E \\u003Cp word_count=\\\"181\\\" data-wc=\\\"181\\\"\\u003EThe people who say that artificial intelligence is not a problem tend to work in artificial intelligence. Many prominent researchers regard Bostrom’s basic views as implausible, or as a distraction from the near-term benefits and moral dilemmas posed by the technology—not least because A.I. systems today can barely guide robots to open doors. Last summer, Oren Etzioni, the C.E.O. of the Allen Institute for Artificial Intelligence, in Seattle, referred to the fear of machine intelligence as a “Frankenstein complex.” Another leading researcher declared, “I don’t worry about that for the same reason I don’t worry about overpopulation on Mars.” Jaron Lanier, a Microsoft researcher and tech commentator, told me that even framing the differing views as a debate was a mistake. “This is not an honest conversation,” he said. “People think it is about technology, but it is really about religion, people turning to metaphysics to cope with the human condition. They have a way of dramatizing their beliefs with an end-of-days scenario—and one does not want to criticize other people’s religions.”\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"104\\\" data-wc=\\\"104\\\"\\u003EBecause the argument has played out on blogs and in the popular press, beyond the ambit of peer-reviewed journals, the two sides have appeared in caricature, with headlines suggesting either doom (“\\u003Csmall\\u003EWill Super-intelligent Machines Kill Us All\\u003C\\u002Fsmall\\u003E?”) or a reprieve from doom (“\\u003Csmall\\u003EArtificial intelligence \\u003C\\u002Fsmall\\u003E‘\\u003Csmall\\u003Ewill not end human race\\u003C\\u002Fsmall\\u003E’&#8200;”). Even the most grounded version of the debate occupies philosophical terrain where little is clear. But, Bostrom argues, if artificial intelligence can be achieved it would be an event of unparalleled consequence—perhaps even a rupture in the fabric of history. A bit of long-range forethought might be a moral obligation to our own species. \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F3\\\" data-total-words=\\\"885\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Ca class=\\\"tny-page\\\" name=\\\"\\u002F2\\\" data-total-words=\\\"885\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp class=\\\"descender dropcap-B\\\" word_count=\\\"159\\\" data-wc=\\\"159\\\"\\u003EBostrom’s sole responsibility at Oxford is to direct an organization called the Future of Humanity Institute, which he founded ten years ago, with financial support from James Martin, a futurist and tech millionaire. Bostrom runs the institute as a kind of philosophical radar station: a bunker sending out navigational pulses into the haze of possible futures. Not long ago, an F.H.I. fellow studied the possibility of a “dark fire scenario,” a cosmic event that, he hypothesized, could occur under certain high-energy conditions: everyday matter mutating into dark matter, in a runaway process that could erase most of the known universe. (He concluded that it was highly unlikely.) Discussions at F.H.I. range from conventional philosophic topics, like the nature of compromise, to the optimal structure of space empires—whether a single intergalactic machine intelligence, supported by a vast array of probes, presents a more ethical future than a cosmic imperium housing millions of digital minds. \\u003C\\u002Fp\\u003E\\u003Cdiv id=\\\"parallax9x2_frame\\\" data-jivox-ad-id=\\\"constellation-parallax\\\" data-constellation-id=\\\"parallax\\\"\\u003E\\u003C\\u002Fdiv\\u003E\\u003Cdiv class=\\\"content-ad-wrapper\\\"\\u003E\\u003Cdiv class=\\\"advertisement\\\" data-ismobile=\\\"true\\\" data-name=\\\"yrailBottom\\\" data-sz=\\\"320x252\\\" data-kw=\\\"2ndBox\\\"\\u003E\\u003Cdiv id=\\\"yrailBottom320x252_frame\\\" class=\\\"displayAd displayAd320x252Js\\\" data-cb-ad-id=\\\"yrailBottom320x252_frame\\\"\\u003E\\u003C\\u002Fdiv\\u003E\\u003C\\u002Fdiv\\u003E\\u003Cdiv class=\\\"advertisement\\\" data-ismobile=\\\"false\\\" data-name=\\\"inlineBottom\\\" data-sz=\\\"728x90\\\" data-kw=\\\"2ndBox\\\"\\u003E\\u003Cdiv id=\\\"inlineBottom728x90_frame\\\" class=\\\"displayAd displayAd728x90Js\\\" data-cb-ad-id=\\\"inlineBottom728x90_frame\\\"\\u003E\\u003C\\u002Fdiv\\u003E\\u003C\\u002Fdiv\\u003E\\u003C\\u002Fdiv\\u003E \\u003Cp word_count=\\\"93\\\" data-wc=\\\"93\\\"\\u003EEarlier this year, I visited the institute, which is situated on a winding street in a part of Oxford that is a thousand years old. It takes some work to catch Bostrom at his office. Demand for him on the lecture circuit is high; he travels overseas nearly every month to relay his technological omens in a range of settings, from Google’s headquarters to a Presidential commission in Washington. Even at Oxford, he maintains an idiosyncratic schedule, remaining in the office until two in the morning and returning sometime the next afternoon.\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"122\\\" data-wc=\\\"122\\\"\\u003EI arrived before he did, and waited in a hallway between two conference rooms. A plaque indicated that one of them was the Arkhipov Room, honoring Vasili Arkhipov, a Soviet naval officer. During the Cuban missile crisis, Arkhipov was serving on a submarine in the Caribbean when U.S. destroyers set off depth charges nearby. His captain, unable to establish radio contact with Moscow, feared that the conflict had escalated and ordered a nuclear strike. But Arkhipov dissuaded him, and all-out atomic war was averted. Across the hallway was the Petrov Room, named for another Soviet officer who prevented a global nuclear catastrophe. Bostrom later told me, “They may have saved more lives than most of the statesmen we celebrate on stamps.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"158\\\" data-wc=\\\"158\\\"\\u003EThe sense that a vanguard of technical-minded people working in obscurity, at odds with consensus, might save the world from auto-annihilation runs through the atmosphere at F.H.I. like an electrical charge. While waiting for Bostrom, I peered through a row of windows into the Arkh­ipov Room, which looked as though it was used for both meetings and storage; on a bookcase there were boxes containing light bulbs, lampshades, cables, spare mugs. A gaunt philosophy Ph.D. wrapped in a thick knitted cardigan was pacing in front of a whiteboard covered in notation, which he attacked in bursts. After each paroxysm, he paced, hands behind his back, head tilted downward. At one point, he erased a panel of his work. Taking this as an opportunity to interrupt, I asked him what he was doing. “It is a problem involving an aspect of A.I. called ‘planning,’&#8200;” he said. His demeanor radiated irritation. I left him alone.\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"132\\\" data-wc=\\\"132\\\"\\u003EBostrom arrived at 2 \\u003Csmall\\u003Ep.m\\u003C\\u002Fsmall\\u003E. He has a boyish countenance and the lean, vital physique of a yoga instructor—though he could never be mistaken for a yoga instructor. His intensity is too untidily contained, evident in his harried gait on the streets outside his office (he does not drive), in his voracious consumption of audiobooks (played at two or three times the normal speed, to maximize efficiency), and his fastidious guarding against illnesses (he avoids handshakes and wipes down silverware beneath a tablecloth). Bostrom can be stubborn about the placement of an office plant or the choice of a font. But when his arguments are challenged he listens attentively, the mechanics of consideration nearly dis­cernible beneath his skin. Then, calmly, quickly, he dispatches a response, one idea interlocked with another. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"58\\\" data-wc=\\\"58\\\"\\u003EHe asked if I wanted to go to the market. “You can watch me make my elixir,” he said. For the past year or so, he has been drinking his lunch (another efficiency): a smoothie containing fruits, vegetables, proteins, and fats. Using his elbow, he hit a button that electronically opened the front door. Then we rushed out. \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F4\\\" data-total-words=\\\"1607\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp class=\\\"descender dropcap-B\\\" word_count=\\\"155\\\" data-wc=\\\"155\\\"\\u003EBostrom has a reinvented man’s sense of lost time. An only child, he grew up—as Niklas Boström—in Helsingborg, on the southern coast of Sweden. Like many exceptionally bright children, he hated school, and as a teen-ager he developed a listless, romantic persona. In 1989, he wandered into a library and stumbled onto an anthology of nineteenth-century German philosophy, containing works by Nietzsche and Schopenhauer. He read it in a nearby forest, in a clearing that he often visited to think and to write poetry, and experienced a euphoric insight into the possibilities of learning and achievement. “It’s hard to convey in words what that was like,” Bostrom told me; instead he sent me a photograph of an oil painting that he had made shortly afterward. It was a semi-representational landscape, with strange figures crammed into dense undergrowth; beyond, a hawk soared below a radiant sun. He titled it “The First Day.”\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"97\\\" data-wc=\\\"97\\\"\\u003EDeciding that he had squandered his early life, he threw himself into a campaign of self-education. He ran down the citations in the anthology, branching out into art, literature, science. He says that he was motivated not only by curiosity but also by a desire for actionable knowledge about how to live. To his parents’ dismay, Bostrom insisted on finishing his final year of high school from home by taking special exams, which he completed in ten weeks. He grew distant from old friends: “I became quite fanatical and felt quite isolated for a period of time.” \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-page\\\" name=\\\"\\u002F3\\\" data-total-words=\\\"1859\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"197\\\" data-wc=\\\"197\\\"\\u003EWhen Bostrom was a graduate student in Stockholm, he studied the work of the analytic philosopher W. V. Quine, who had explored the difficult relationship between language and reality. His adviser drilled precision into him by scribbling “not clear” throughout the margins of his papers. “It was basically his only feedback,” Bostrom told me. “The effect was still, I think, beneficial.” His previous academic interests had ranged from psychology to mathematics; now he took up theoretical physics. He was fascinated by technology. The World Wide Web was just emerging, and he began to sense that the heroic philosophy which had inspired him might be outmoded. In 1995, Bostrom wrote a poem, “Requiem,” which he told me was “a signing-off letter to an earlier self.” It was in Swedish, so he offered me a synopsis: “I describe a brave general who has overslept and finds his troops have left the encampment. He rides off to catch up with them, pushing his horse to the limit. Then he hears the thunder of a modern jet plane streaking past him across the sky, and he realizes that he is obsolete, and that courage and spiritual nobility are no match for machines.” \\u003C\\u002Fp\\u003E \\u003Cfigure class=\\\"cartoon-image\\\" data-track-location=\\\"embeddedCartoon\\\" data-cartoon-id=\\\"a19497\\\"\\u003E\\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcartoons\\u002Fa19497\\\" target=\\\"_blank\\\"\\u003E\\u003Cimg class=\\\"cartoon post-load\\\" alt=\\\"Cartoon\\\" data-src-mobile=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_a19497-500.jpg\\\" src=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_a19497-690.jpg\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cfigcaption\\u003E\\u003Cspan class=\\\"caption\\\"\\u003E“I’m starting a startup that helps other startups start up.”\\u003C\\u002Fspan\\u003E\\u003C\\u002Ffigcaption\\u003E\\u003C\\u002Ffigure\\u003E   \\u003Cp word_count=\\\"187\\\" data-wc=\\\"187\\\"\\u003EAlthough Bostrom did not know it, a growing number of people around the world shared his intuition that technology could cause transformative change, and they were finding one another in an online discussion group administered by an organization in California called the Extropy Institute. The term “extropy,” coined in 1967, is generally used to describe life’s capacity to reverse the spread of entropy across space and time. Extropianism is a libertarian strain of transhumanism that seeks “to direct human evolution,” hoping to eliminate disease, suffering, even death; the means might be genetic modification, or as yet un­invented nanotechnology, or perhaps dispensing with the body entirely and uploading minds into supercomputers. (As one member noted, “Immortality is mathematical, not mystical.”) The Extropians advocated the development of artificial superintelligence to achieve these goals, and they envisioned humanity colonizing the universe, converting inert matter into engines of civilization. The discussions were nerdy, lunatic, imaginative, thought-provoking. Anders Sandberg, a former member of the group who now works at Bostrom’s institute, told me, “Just imagine if you could listen in on the debates of the Italian Futurists or early Surrealists.”\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"111\\\" data-wc=\\\"111\\\"\\u003EIn 1996, while pursuing further graduate work at the London School of Economics, Bostrom learned about the Extropy discussion group and became an active participant. A year later, he co-founded his own organization, the World Transhumanist Association, which was less libertarian and more academically spirited. He crafted approachable statements on transhumanist values and gave interviews to the BBC. The line between his academic work and his activism blurred: his Ph.D. dissertation centered on a study of the Doomsday Argument, which uses probability theory to make inferences about the longevity of human civilization. The work baffled his advisers, who respected him but rarely agreed with his conclusions. Mostly, they left him alone.\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"176\\\" data-wc=\\\"176\\\"\\u003EBostrom had little interest in conventional philosophy—not least because he expected that superintelligent minds, whether biologically enhanced or digital, would make it obsolete. “Suppose you had to build a new subway line, and it was this grand trans-generational enterprise that humanity was engaged in, and everybody had a little role,” he told me. “So you have a little shovel. But if you know that a giant bulldozer will arrive on the scene tomorrow, then does it really make sense to spend your time today digging the big hole with your shovel? Maybe there is something else you could do with your time. Maybe you could put up a signpost for the great shovel, so it will start digging in the right place.” He came to believe that a key role of the philosopher in modern society was to acquire the knowledge of a polymath, then use it to help guide humanity to its next phase of existence—a discipline that he called “the philosophy of technological prediction.” He was trying to become such a seer. \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F5\\\" data-total-words=\\\"2530\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"36\\\" data-wc=\\\"36\\\"\\u003E“He was ultra-consistent,” Daniel Hill, a British philosopher who befriended Bostrom while they were graduate students in London, told me. “His interest in science was a natural outgrowing of his understandable desire to live forever, basically.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"121\\\" data-wc=\\\"121\\\"\\u003EBostrom has written more than a hundred articles, and his longing for immortality can be seen throughout. In 2008, he framed an essay as a call to action from a future utopia. “Death is not one but a multitude of assassins,” he warned. “Take aim at the causes of early death—infection, violence, malnutrition, heart attack, cancer. Turn your biggest gun on aging, and fire. You must seize the biochemical processes in your body in order to vanquish, by and by, illness and senescence. In time, you will discover ways to move your mind to more durable media.” He tends to see the mind as immaculate code, the body as inefficient hardware—able to accommodate limited hacks but probably destined for replacement. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"104\\\" data-wc=\\\"104\\\"\\u003EEven Bostrom’s marriage is largely mediated by technology. His wife, Susan, has a Ph.D. in the sociology of medicine and a bright, down-to-earth manner. (“She teases me about the Terminator and the robot army,” he told me.) They met thirteen years ago, and for all but six months they have lived on opposite sides of the Atlantic, even after the recent birth of their son. The arrangement is voluntary: she prefers Montreal; his work keeps him at Oxford. They Skype several times a day, and he directs as much international travel as possible through Canada, so they can meet in non-digital form. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"72\\\" data-wc=\\\"72\\\"\\u003EIn Oxford, as Bostrom shopped for his smoothie, he pointed out a man vaping. “There is also the more old-school method of taking nicotine: chewing gum,” he told me. “I do chew nicotine gum. I read a few papers saying it might have some nootropic effect”—that is, it might enhance cognition. He drinks coffee, and usually abstains from alcohol. He briefly experimented with the smart drug Modafinil, but gave it up. \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-page\\\" name=\\\"\\u002F4\\\" data-total-words=\\\"2863\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"86\\\" data-wc=\\\"86\\\"\\u003EBack at the institute, he filled an industrial blender with lettuce, carrots, cauliflower, broccoli, blueberries, turmeric, vanilla, oat milk, and whey powder. “If there is one thing Nick cares about, it is minds,” Sandberg told me. “That is at the root of many of his views about food, because he is worried that toxin X or Y might be bad for his brain.” He suspects that Bostrom also enjoys the ritualistic display. “Swedes are known for their smugness,” he joked. “Perhaps Nick is subsisting on smugness.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"27\\\" data-wc=\\\"27\\\"\\u003EA young employee eyed Bostrom getting ready to fire up the blender. “I can tell when Nick comes into the office,” he said. “My hair starts shaking.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"77\\\" data-wc=\\\"77\\\"\\u003E“Yeah, this has got three horsepower,” Bostrom said. He ran the blender, producing a noise like a circular saw, and then filled a tall glass stein with purple-­green liquid. We headed to his office, which was meticulous. By a window was a wooden desk supporting an iMac and not another item; against a wall were a chair and a cabinet with a stack of documents. The only hint of excess was light: there were fourteen lamps. \\u003C\\u002Fp\\u003E \\u003Cp class=\\\"descender dropcap-I\\\" word_count=\\\"120\\\" data-wc=\\\"120\\\"\\u003EIt is hard to spend time at Bostrom’s institute without drifting into reveries of a far future. What might humanity look like millions of years from now? The upper limit of survival on Earth is fixed to the life span of the sun, which in five billion years will become a red giant and swell to more than two hundred times its present size. It is possible that Earth’s orbit will adjust, but more likely that the planet will be destroyed. In any case, long before then, nearly all plant life will die, the oceans will boil, and the Earth’s crust will heat to a thousand degrees. In half a billion years, the planet will be uninhabitable.\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"136\\\" data-wc=\\\"136\\\"\\u003EThe view of the future from Bostrom’s office can be divided into three grand panoramas. In one, humanity experiences an evolutionary leap—either assisted by technology or by merging into it and becoming software—to achieve a sublime condition that Bostrom calls “posthumanity.” Death is overcome, mental experience expands beyond recognition, and our descendants colonize the universe. In another panorama, humanity becomes extinct or experiences a disaster so great that it is unable to recover. Between these extremes, Bostrom envisions scenarios that resemble the status quo—people living as they do now, forever mired in the “human era.” It’s a vision familiar to fans of sci-fi: on “Star Trek,” Captain Kirk was born in the year 2233, but when an alien portal hurls him through time and space to Depression-era Manhattan he blends in easily. \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F6\\\" data-total-words=\\\"3309\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"62\\\" data-wc=\\\"62\\\"\\u003EBostrom dislikes science fiction. “I’ve never been keen on stories that just try to present ‘wow’ ideas—the equivalent of movie productions that rely on stunts and explosions to hold the attention,” he told me. “The question is not whether we can think of something radical or extreme but whether we can discover some sufficient reason for updating our credence function.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"139\\\" data-wc=\\\"139\\\"\\u003EHe believes that the future can be studied with the same meticulousness as the past, even if the conclusions are far less firm. “It may be highly unpredictable where a traveller will be one hour after the start of her journey, yet predictable that after five hours she will be at her destination,” he once argued. “The \\u003Cem\\u003Every\\u003C\\u002Fem\\u003E long-term future of humanity may be relatively easy to predict.” He offers an example: if history were reset, the industrial revolution might occur at a different time, or in a different place, or perhaps not at all, with innovation instead occurring in increments over hundreds of years. In the short term, predicting technological achievements in the counter-history might not be possible; but after, say, a hundred thousand years it is easier to imagine that all the same inventions would have emerged. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"73\\\" data-wc=\\\"73\\\"\\u003EBostrom calls this the Technological Completion Conjecture: “If scientific- and technological-development efforts do not effectively cease, then all impor­t­­­ant basic capabilities that could be obtained through some possible technology will be obtained.” In light of this, he suspects that the farther into the future one looks the less likely it seems that life will continue as it is. He favors the far ends of possibility: humanity becomes transcendent or it perishes. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"155\\\" data-wc=\\\"155\\\"\\u003EIn the nineteen-nineties, as these ideas crystallized in his thinking, Bostrom began to give more attention to the question of extinction. He did not believe that doomsday was imminent. His interest was in risk, like an insurance agent’s. No matter how improbable extinction may be, Bostrom argues, its consequences are near-infinitely bad; thus, even the tiniest step toward reducing the \\u003Cem\\u003Echance\\u003C\\u002Fem\\u003E that it will happen is near-­infinitely valuable. At times, he uses arithmetical sketches to illustrate this point. Imagining one of his utopian scenarios—trillions of digital minds thriving across the cosmos—he reasons that, if there is even a one-per-cent chance of this happening, the expected value of reducing an existential threat by a billionth of a billionth of one per cent would be worth a hundred billion times the value of a billion present-day lives. Put more simply: he believes that his work could dwarf the moral importance of anything else.\\u003C\\u002Fp\\u003E \\u003Cp class=\\\"descender dropcap-B\\\" word_count=\\\"147\\\" data-wc=\\\"147\\\"\\u003EBostrom introduced the philosophical concept of “existential risk” in 2002, in the \\u003Cem\\u003EJournal of Evolution and Technology\\u003C\\u002Fem\\u003E. In recent years, new organizations have been founded almost annually to help reduce it—among them the Centre for the Study of Existential Risk, affiliated with Cambridge Uni­versity, and the Future of Life Institute, which has ties to the Massachusetts Institute of Technology. All of them face a key problem: \\u003Cem\\u003EHomo sapiens\\u003C\\u002Fem\\u003E, since its emergence two hundred thousand years ago, has proved to be remarkably resilient, and figuring out what might imperil its existence is not obvious. Climate change is likely to cause vast environmental and economic damage—but it does not seem impossible to survive. So-called super-volcanoes have thus far not threatened the perpetuation of the species. \\u003Csmall\\u003ENASA\\u003C\\u002Fsmall\\u003E spends forty million dollars each year to determine if there are significant comets or asteroids headed for Earth. (There aren’t.) \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-page\\\" name=\\\"\\u002F5\\\" data-total-words=\\\"3885\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"197\\\" data-wc=\\\"197\\\"\\u003EBostrom does not find the lack of obvious existential threats comforting. Because it is impossible to endure extinction twice, he argues, we cannot rely on history to calculate the probability that it will occur. The most worrying dangers are those that Earth has never encountered before. “It is hard to cause human extinction with seventeenth-century technology,” Bostrom told me. Three centuries later, though, the prospect of a technological apocalypse was urgently plausible. Bostrom dates the first scientific analysis of existential risk to the Manhattan Project: in 1942, Robert Oppenheimer became concerned that an atomic detonation of sufficient power could cause the entire atmosphere to ignite. A subsequent study concluded that the scenario was “unreasonable,” given the limitations of the weapons then in development. But even if the great nuclear nightmares of the Cold War did not come true, the tools were there to cause destruction on a scale not previously possible. As innovations grow even more complex, it is increasingly difficult to evaluate the dangers ahead. The answers must be fraught with ambiguity, because they can be derived only by predicting the effects of technologies that exist mostly as theories or, even more indirectly, by using abstract reasoning. \\u003C\\u002Fp\\u003E \\u003Cdiv class=\\\"media-cne\\\"\\u003E\\u003Cscript async src=\\\"\\u002F\\u002Fplayer-backend.cnevids.com\\u002Fscript\\u002Fvideo\\u002F5641364261646d047600004d.js\\\" class=\\\"x-skip\\\"\\u003E\\u003C\\u002Fscript\\u003E\\u003Cspan class=\\\"caption\\\" style=\\\"display: block; margin: 12px 0; width: 100%;\\\"\\u003E\\u003Cspan class=\\\"caption-text\\\"\\u003ENick Bostrom asks, Will we engineer our own extinction?\\u003C\\u002Fspan\\u003E\\u003C\\u002Fspan\\u003E\\u003C\\u002Fdiv\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F7\\\" data-total-words=\\\"4082\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"148\\\" data-wc=\\\"148\\\"\\u003EAs a philosopher, Bostrom takes a sweeping, even cosmic, view of such problems. One afternoon, he told me, “The probabilities that any given planet will produce intelligent life—this may also have action-relevant information.” In the past several years, \\u003Csmall\\u003ENASA\\u003C\\u002Fsmall\\u003E probes have found increasing evidence that the building blocks of life are abundant throughout space. So much water has been discovered—on Mars and on the moons of Jupiter and Saturn—that one scientist described our solar system as “a pretty soggy place.” There are amino acids on icy comets and complex organic molecules in distant star-forming clouds. On this planet, life has proved capable of thriving in unimaginably punishing conditions: without oxygen, without light, at four hundred degrees above or below zero. In 2007, the European Space Agency hitched tiny creatures to the exterior of a satellite. They not only survived the flight; some even laid eggs afterward. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"145\\\" data-wc=\\\"145\\\"\\u003EWith ten billion Earth-like planets in our galaxy alone, and a hundred billion galaxies in the universe, there is good reason to suspect that extraterrestrial life may one day be discovered. For Bostrom, this would augur disaster. “It would be great news to find that Mars is a completely sterile planet,” he argued not long ago. “Dead rocks and lifeless sands would lift my spirits.” His reasoning begins with the age of the universe. Many of those Earth-like planets are thought to be far, far older than ours. One that was recently discovered, called Kepler 452b, is as much as one and a half billion years older. Bostrom asks: If life had formed there on a time scale resembling our own, what would it look like? What kind of technological progress could a civilization achieve with a head start of hundreds of millions of years?\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"174\\\" data-wc=\\\"174\\\"\\u003ELife as we know it tends to spread wherever it can, and Bostrom estimates that, if an alien civilization could design space probes capable of travelling at even one per cent of the speed of light, the entire Milky Way could be colonized in twenty million years—a tiny fraction of the age difference between Kepler 452b and Earth. One could argue that no technology will ever propel ships at so great a speed. Or perhaps millions of alien civilizations possess the know-how for intergalactic travel, but they aren’t interested. Even so, because the universe is so colossal, and because it is so old, only a small number of civilizations would need to behave as life does on Earth—unceasingly expanding—in order to be visible. Yet, as Bostrom notes, “You start with billions and billions of potential germination points for life, and you end up with a sum total of \\u003Cem\\u003Ezero\\u003C\\u002Fem\\u003E alien civilizations that developed technologically to the point where they become manifest to us earthly observers. So what’s stopping them?” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"152\\\" data-wc=\\\"152\\\"\\u003EIn 1950, Enrico Fermi sketched a version of this paradox during a lunch break while he was working on the H-bomb, at Los Alamos. Since then, many resolutions have been proposed—some of them exotic, such as the idea that Earth is housed in an interplanetary alien zoo. Bostrom suspects that the answer is simple: space appears to be devoid of life because it is. This implies that intelligent life on Earth is an astronomically rare accident. But, if so, when did that accident occur? Was it in the first chemical reactions in the primordial soup? Or when single-celled organisms began to replicate using DNA? Or when animals learned to use tools? Bos­trom likes to think of these hurdles as Great Filters: key phases of improbability that life everywhere must pass through in order to develop into intelligent species. Those which do not make it either go extinct or fail to evolve.\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"105\\\" data-wc=\\\"105\\\"\\u003EThus, for Bostrom, the discovery of a single-celled creature inhabiting a damp stretch of Martian soil would constitute a disconcerting piece of evidence. If two planets independently evolved primitive organisms, then it seems more likely that this type of life can be found on many planets throughout the universe. Bostrom reasons that this would suggest that the Great Filter comes at some later evolutionary stage. The discovery of a fossilized vertebrate would be even worse: it would suggest that the universe appears lifeless not because complex life is unusual but, rather, because it is always somehow thwarted before it becomes advanced enough to colonize space. \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F8\\\" data-total-words=\\\"4806\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Ca class=\\\"tny-page\\\" name=\\\"\\u002F6\\\" data-total-words=\\\"4806\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"133\\\" data-wc=\\\"133\\\"\\u003EIn Bostrom’s view, the most distressing possibility is that the Great Filter is ahead of us—that evolution frequently achieves civilizations like our own, but they perish before reaching their technological maturity. Why might that be? “Natural disasters such as asteroid hits and super-­volcanic eruptions are unlikely Great Filter candidates, because, even if they destroyed a significant number of civilizations, we would expect some civilizations to get lucky and escape disaster,” he argues. “Perhaps the most likely type of existential risks that could constitute a Great Filter are those that arise from technological discovery. It is not far-fetched to suppose that there might be some possible technology which is such that (a) virtually all suffi­ciently advanced civilizations eventually discover it and (b) its discovery leads almost universally to existential disaster.”\\u003C\\u002Fp\\u003E \\u003Ch2 class=\\\"aligncenter\\\"\\u003EII. The Machines\\u003C\\u002Fh2\\u003E \\u003Cp class=\\\"descender dropcap-T\\\" word_count=\\\"96\\\" data-wc=\\\"96\\\"\\u003EThe field of artificial intelligence was born in a fit of scientific optimism, in 1955, when a small group of researchers—three mathematicians and an I.B.M. programmer—drew up a proposal for a project at Dartmouth. “An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves,” they stated. “We think a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.”\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"158\\\" data-wc=\\\"158\\\"\\u003ETheir optimism was understandable. Since the turn of the twentieth century, science had been advancing at a breakneck pace: the discovery of radioactivity quickly led to insights into the inner workings of the atom, and then to the development of controlled nuclear energy, and then to the warheads over Hiroshima and Nagasaki, and then to the H-bomb. This rush of discovery was reflected in fiction, too, in the work of Isaac Asimov, among others, who envisioned advanced civilizations inhabited by intelligent robots (each encoded with simple, ethical Laws of Robotics, to prevent it from causing harm). The year the scientists met at Dartmouth, Asimov published “The Last Question,” a story featuring a superintelligent A.I. that is continually “self-adjusting and self-correcting”—gaining knowledge as it helps human civilization expand throughout the universe. When the universe’s last stars start dying out, all humanity uploads itself into the A.I., and the device, achieving godhood, creates a new cosmos.\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"107\\\" data-wc=\\\"107\\\"\\u003EScientists perceived the mechanics of intelligence—like those of the atom—as a source of huge potential, a great frontier. If the brain was merely a biological machine, there was no theoretical reason that it could not be replicated, or even surpassed, much the way a jet could outfly a falcon. Even before the Dartmouth conference, machines exceeded human ability in narrow domains like code-breaking. In 1951, Alan Turing argued that at some point computers would probably exceed the intellectual capacity of their inventors, and that “therefore we should have to expect the machines to take control.” Whether this would be good or bad he did not say.\\u003C\\u002Fp\\u003E \\u003Cfigure class=\\\"cartoon-image\\\" data-track-location=\\\"embeddedCartoon\\\" data-cartoon-id=\\\"a19633\\\"\\u003E\\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcartoons\\u002Fa19633\\\" target=\\\"_blank\\\"\\u003E\\u003Cimg class=\\\"cartoon post-load\\\" alt=\\\"Cartoon\\\" data-src-mobile=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_a19633-500.jpg\\\" src=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_a19633-690.jpg\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cfigcaption\\u003E\\u003Cspan class=\\\"caption\\\"\\u003E“I hoped you’d like the size of it.”\\u003C\\u002Fspan\\u003E\\u003C\\u002Ffigcaption\\u003E\\u003C\\u002Ffigure\\u003E   \\u003Cp word_count=\\\"155\\\" data-wc=\\\"155\\\"\\u003ESix years later, Herbert Simon, one of the Dartmouth attendees, declared that machines would achieve human intelligence “in a visible future.” The crossing of such a threshold, he suspected, could be psychologically crushing, but he was on the whole optimistic. “We must also remain sensitive to the need to keep the computer’s goals attuned with our own,” he later said, but added, “I am not convinced that this will be difficult.” For other computer pioneers, the future appeared more ambivalent. Norbert Wiener, the father of cybernetics, argued that it would be difficult to manage powerful computers, or even to accurately predict their behavior. “Complete subservience and complete intelligence do not go together,” he said. Envisioning Sorcerer’s Apprentice scenarios, he predicted, “The future will be an ever more demanding struggle against the limitations of our intelligence, not a comfortable hammock in which we can lie down to be waited upon by our robot slaves.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"106\\\" data-wc=\\\"106\\\"\\u003EIt was in this milieu that the “intelligence explosion” idea was first formally expressed by I. J. Good, a statistician who had worked with Turing. “An ultraintelligent machine could design even better machines,” he wrote. “There would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the \\u003Cem\\u003Elast\\u003C\\u002Fem\\u003E invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.” \\u003C\\u002Fp\\u003E \\u003Cp class=\\\"descender dropcap-T\\\" word_count=\\\"147\\\" data-wc=\\\"147\\\"\\u003EThe scientists at Dartmouth recognized that success required answers to fundamental questions: What is intelligence? What is the mind? By 1965, the field had experimented with several models of problem solving: some were based on formal logic; some used heuristic reasoning; some, called “neural networks,” were inspired by the brain. With each, the scientists’ work indicated that A.I. systems could find their own solutions to problems. One algorithm proved numerous theorems in the classic text “Principia Mathematica,” and in one instance it did so more elegantly than the authors. A program designed to play checkers learned to beat its programmer. And yet, despite the great promise in these experiments, the challenges to creating an A.I. were forbidding. Programs that performed well in the laboratory were useless in everyday situations; a simple act like picking up a ball turned out to require an overwhelming number of computations. \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F9\\\" data-total-words=\\\"5708\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"113\\\" data-wc=\\\"113\\\"\\u003EThe research fell into the first of several “A.I. winters.” As Bostrom notes in his book, “Among academics and their funders, ‘A.I.’ became an unwanted epithet.” Eventually, the researchers started to question the goal of building a mind altogether. Why not try instead to divide the problem into pieces? They began to limit their interests to specific cognitive functions: vision, say, or speech. Even in isolation, these functions would have value: a computer that could identify objects might not be an A.I., but it could help guide a forklift. As the research fragmented, the morass of technical problems made any questions about the consequences of success seem distant, even silly. \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-page\\\" name=\\\"\\u002F7\\\" data-total-words=\\\"5821\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"176\\\" data-wc=\\\"176\\\"\\u003EUnexpectedly, by dismissing its founding goals, the field of A.I. created space for outsiders to imagine more freely what the technology might look like. Bostrom wrote his first paper on artificial superintelligence in the nineteen-nineties, envisioning it as potentially perilous but irresistible to both commerce and government. “If there is a way of guaranteeing that superior artificial intellects will never harm human beings, then such intellects will be created,” he argued. “If there is no way to have such a guarantee, then they will probably be created nevertheless.” His audience at the time was primarily other transhumanists. But the movement was maturing. In 2005, an organization called the Singularity Institute for Artificial Intelligence began to operate out of Silicon Valley; its primary founder, a former member of the Extropian discussion group, published a stream of literature on the dangers of A.I. That same year, the futurist and inventor Ray Kurzweil wrote “The Singularity Is Near,” a best-seller that prophesied a merging of man and machine in the foreseeable future. Bostrom created his institute at Oxford.\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"201\\\" data-wc=\\\"201\\\"\\u003EThe two communities could not have been more different. The scientists, steeped in technical detail, were preoccupied with making devices that worked; the transhumanists, motivated by the hope of a utopian future, were asking, What would the ultimate impact of those devices be? In 2007, the Association for the Advancement of Artificial Intelligence—the most prominent professional organization for A.I. researchers—elected Eric Horvitz, a scientist from Microsoft, as its president. Until then, it had given virtually no attention to the ethical and social implications of the research, but Horvitz was open to the big questions. “It is hard to understand what success would mean for A.I.,” he told me. “I was friendly with Jack Good, who wrote that piece on superintelligence. I knew him as a creative, funny guy who referred to a lot of his ideas as P.B.I.s—partly baked ideas. And here is this piece of his being opened up outside the field as this Bible and studied with a silver pointer. Wouldn’t it be useful, I said, even if you thought these were crazy or low-probability scenarios, to find out: Can we be proactive, should there be some poor outcome for humanity?” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"163\\\" data-wc=\\\"163\\\"\\u003EHorvitz organized a meeting at the Asilomar Conference Grounds, in California, a place chosen for its symbolic value: biologists had gathered there in 1975 to discuss the hazards of their research in the age of modern genetics. He divided the researchers into groups. One studied short-term ramifications, like the possible use of A.I. to commit crimes; another considered long-term consequences. Mostly, there was skepticism about the intelligence-explosion idea, which assumed answers to many unresolved questions. No one fully understands what intelligence is, let alone how it might evolve in a machine. Can it grow as Good imagined, gaining I.Q. points like a rocketing stock price? If so, what would its upper limit be? And would its increase be merely a function of optimized software design, without the difficult process of acquiring knowledge through experience? Can software fundamentally rewrite itself without risking crippling breakdowns? No one knows. In the history of computer science, no programmer has created code that can substantially improve itself.\\u003C\\u002Fp\\u003E \\u003Cfigure class=\\\"cartoon-image\\\" data-track-location=\\\"embeddedCartoon\\\" data-cartoon-id=\\\"a19630\\\"\\u003E\\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcartoons\\u002Fa19630\\\" target=\\\"_blank\\\"\\u003E\\u003Cimg class=\\\"cartoon post-load\\\" alt=\\\"Cartoon\\\" data-src-mobile=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_a19630-500.jpg\\\" src=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_a19630-690.jpg\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cfigcaption\\u003E\\u003Cspan class=\\\"caption\\\"\\u003E“No, you want the A train. This is just a train.”\\u003C\\u002Fspan\\u003E\\u003C\\u002Ffigcaption\\u003E\\u003C\\u002Ffigure\\u003E   \\u003Cp word_count=\\\"177\\\" data-wc=\\\"177\\\"\\u003EBut the notion of an intelligence explosion was also impossible to disprove. It was theoretically coherent, and it had even been attempted in limited ways. David McAllester, an A.I. researcher at the Toyota Technological Institute, affiliated with the University of Chicago, headed the long-term panel. The idea, he argued, was worth taking seriously. “I am uncomfortable saying that we are ninety-­nine per cent certain that we are safe for fifty years,” he told me. “That feels like hubris to me.” The group concluded that more technical work was needed before an evaluation of the dangers could be made, but it also hinted at a concern among panelists that the gathering was based on “a \\u003Cem\\u003Eperception\\u003C\\u002Fem\\u003E of urgency”—generated largely by the transhumanists—and risked raising unfounded alarm. With A.I. seeming like a remote prospect, the researchers declared, attention was better spent on near-term concerns. Bart Selman, a professor at Cornell who co-­organized the panel, told me, “The mode was ‘This is interesting, but it’s all academic—it’s not going to happen.’&#8200;”\\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F10\\\" data-total-words=\\\"6538\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp class=\\\"descender dropcap-A\\\" word_count=\\\"66\\\" data-wc=\\\"66\\\"\\u003EAt the time the A.I researchers met at Asilomar, Bostrom was grappling with an expansive book on existential risks. He had sketched out chapters on bioengineering and on nanotechnology, among other topics, but many of these problems came to seem less compelling, while his chapter on A.I. grew and grew. Eventually, he pasted the A.I. chapter into a new file, which became “Superintelligence.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"132\\\" data-wc=\\\"132\\\"\\u003EThe book is its own elegant paradox: analytical in tone and often lucidly argued, yet punctuated by moments of messianic urgency. Some portions are so extravagantly speculative that it is hard to take them seriously. (“Suppose we could somehow establish that a certain future AI will have an IQ of 6,455: then what?”) But Bostrom is aware of the limits to his type of futurology. When he was a graduate student in London, thinking about how to maximize his ability to communicate, he pursued stand­­up comedy; he has a deadpan sense of humor, which can be found lightly buried among the book’s self-serious passages. “Many of the points made in this book are probably wrong,” he writes, with an endnote that leads to the line “I don’t know which ones.”\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"124\\\" data-wc=\\\"124\\\"\\u003EBostrom prefers to act as a cartographer rather than a polemicist, but beneath his exhaustive mapping of scenarios one can sense an argument being built and perhaps a fear of being forthright about it. “Traditionally, this topic domain has been occupied by cranks,” he told me. “By popular media, by science fiction—or maybe by a retired physicist no longer able to do serious work, so he will write a popular book and pontificate. That is kind of the level of rigor that is the baseline. I think that a lot of reasons why there has not been more serious work in this area is that academics don’t want to be conflated with flaky, crackpot type of things. Futurists are a certain type.”\\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-page\\\" name=\\\"\\u002F8\\\" data-total-words=\\\"6860\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"68\\\" data-wc=\\\"68\\\"\\u003EThe book begins with an “unfinished” fable about a flock of sparrows that decide to raise an owl to protect and advise them. They go looking for an owl egg to steal and bring back to their tree, but, because they believe their search will be so difficult, they postpone studying how to domesticate owls until they succeed. Bostrom concludes, “It is not known how the story ends.”\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"137\\\" data-wc=\\\"137\\\"\\u003EThe parable is his way of introducing the book’s core question: Will an A.I., if realized, use its vast capability in a way that is beyond human control? One way to think about the concern is to begin with the familiar. Bos­trom writes, “Artificial intelligence already outperforms human intelligence in many domains.” The examples range from chess to Scrabble. One program from 1981, called Eurisko, was designed to teach itself a naval role-playing game. After playing ten thousand matches, it arrived at a morally grotesque strategy: to field thousands of small, immobile ships, the vast majority of which were intended as cannon fodder. In a national tournament, Eurisko demolished its human opponents, who insisted that the game’s rules be changed. The following year, Eurisko won again—by forcing its damaged ships to sink themselves. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"110\\\" data-wc=\\\"110\\\"\\u003EThe program was by no means superintelligent. But Bostrom’s book essentially asks: What if it were? Assume that it has a broad ability to consider problems and that it has access to the Internet. It could read and acquire general knowledge and communicate with people seamlessly online. It could conduct experiments, either virtually or by tinkering with networked infrastructure. Given even the most benign objective—to win a game—such a system, Bostrom argues, might develop “instrumental goals”: gather resources, or invent technology, or take steps to insure that it cannot be turned off, in the process paying as much heed to human life as humans do to ants. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"90\\\" data-wc=\\\"90\\\"\\u003EIn people, intelligence is inseparable from consciousness, emotional and social awareness, the complex interaction of mind and body. An A.I. need not have any such attributes. Bostrom believes that machine intelligences—no matter how flexible in their tactics—will likely be rigidly fixated on their ultimate goals. How, then, to create a machine that respects the nuances of social cues? That adheres to ethical norms, even at the expense of its goals? No one has a coherent solution. It is hard enough to reliably inculcate such behavior in people. \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F11\\\" data-total-words=\\\"7265\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"265\\\" data-wc=\\\"265\\\"\\u003EIn science fiction, superintelligent computers that run amok are often circumvented at the last minute; think of WOPR, the computer in “WarGames,” which was stopped just short of triggering nuclear war, or HAL 9000, which was reduced to helplessly singing while it watched itself get dismantled. For Bos­trom, this strains credulity. Whether out of a desire to consider the far ends of risk or out of transhumanist longings, he often ascribes nearly divine abilities to machines, as if to ask: Can a digital god really be contained? He imagines machines so intelligent that merely by inspecting their own code they can extrapolate the nature of the universe and of human society, and in this way outsmart any effort to contain them. “Is it possible to build machines that are not like agents—goal-pursuing, autonomous, artificial intelligences?” he asked me. “Maybe you can design something more like an oracle that can only answer yes or no. Would that be safer? It is not so clear. There might be agent-like processes within it.” Asking a simple question—“Is it possible to convert a DeLorean into a time machine and travel to 1955?”—might trigger a cascade of action as the device tests hypotheses. What if, working through a police computer, it impounds a DeLorean that happens to be convenient to a clock tower? “In fairy tales, you have genies who grant wishes,” Bostrom said. “Almost universally, the moral of those is that if you are not extremely careful what you wish for, then what seems like it should be a great blessing turns out to be a curse.”\\u003C\\u002Fp\\u003E \\u003Cp class=\\\"descender dropcap-B\\\" word_count=\\\"154\\\" data-wc=\\\"154\\\"\\u003EBostrom worries that solving the “control problem”—insuring that a superintelligent machine does what humans want it to do—will require more time than solving A.I. does. The intelligence explosion is not the only way that a superintelligence might be created suddenly. Bostrom once sketched out a decades-long process, in which researchers arduously improved their systems to equal the intelligence of a mouse, then a chimp, then—after incredible labor—the village idiot. “The difference between village idiot and genius-­level intelligence might be trivial from the point of view of how hard it is to replicate the same functionality in a machine,” he said. “The brain of the village idiot and the brain of a scientific genius are almost identical. So we might very well see relatively slow and incremental progress that doesn’t really raise any alarm bells until we are just one step away from something that is radically superintelligent.”\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"216\\\" data-wc=\\\"216\\\"\\u003ETo a large degree, Bostrom’s concerns turn on a simple question of timing: Can breakthroughs be predicted? “It is ridiculous to talk about such things so early—A.I. is eons away,” Edward Feigenbaum, an emeritus professor at Stanford University, told me. The researcher Oren Etzioni, who used the term “Frankenstein complex” to dismiss the “dystopian vision of A.I.,” concedes Bostrom’s overarching point: that the field must one day confront profound philosophical questions. Decades ago, he explored them himself, in a brief paper, but concluded that the problem was too remote to think about productively. “Once, Nick Bostrom gave a talk, and I gave a little counterpoint,” he told me. “A lot of the disagreements come down to what time scale you are thinking about. Nobody responsible would say you will see anything remotely like A.I. in the next five to ten years. And I think most computer scientists would say, ‘In a million years—we don’t see why it shouldn’t happen.’ So now the question is: What is the rate of progress? There are a lot of people who will ask: Is it \\u003Cem\\u003Epossible\\u003C\\u002Fem\\u003E we are wrong? Yes. I am not going to rule it out. I am going to say, ‘I am a scientist. Show me the evidence.’&#8200;”\\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-page\\\" name=\\\"\\u002F9\\\" data-total-words=\\\"7900\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"118\\\" data-wc=\\\"118\\\"\\u003EThe history of science is an uneven guide to the question: How close are we? There has been no shortage of unfulfilled promises. But there are also plenty of examples of startling nearsightedness, a pattern that Arthur C. Clarke enshrined as Clarke’s First Law: “When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong.” After the electron was discovered, at Cambridge, in 1897, physicists at an annual dinner toasted, “To the electron: may it never be of use to anybody.” Lord Kelvin famously declared, just eight years before the Wright brothers launched from Kitty Hawk, that heavier-than-air flight was impossible. \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F12\\\" data-total-words=\\\"8018\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"184\\\" data-wc=\\\"184\\\"\\u003EStuart Russell, the co-author of the textbook “Artificial Intelligence: A Modern Approach” and one of Bostrom’s most vocal supporters in A.I., told me that he had been studying the physics community during the advent of nuclear weapons. At the turn of the twentieth century, Ernest Rutherford discovered that heavy elements produced radiation by atomic decay, confirming that vast reservoirs of energy were stored in the atom. Rutherford believed that the energy could not be harnessed, and in 1933 he proclaimed, “Anyone who expects a source of power from the transformation of these atoms is talking moonshine.” The next day, a former student of Einstein’s named Leo Szilard read the comment in the papers. Irritated, he took a walk, and the idea of a nuclear chain reaction occurred to him. He visited Rutherford to discuss it, but Rutherford threw him out. Einstein, too, was skeptical about nuclear energy—splitting atoms at will, he said, was “like shooting birds in the dark in a country where there are only a few birds.” A decade later, Szilard’s insight was used to build the bomb.\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"82\\\" data-wc=\\\"82\\\"\\u003ERussell now relays the story to A.I. researchers as a cautionary tale. “There will have to be more breakthroughs to get to A.I., but, as Szilard illustrated, those can happen overnight,” he told me. “People are putting billions of dollars into achieving those breakthroughs. As the debate stands, Bostrom and others have said, ‘If we achieve superintelligence, here are some of the problems that might arise.’ As far as I know, no one has proved why those are not real.”\\u003C\\u002Fp\\u003E \\u003Ch2 class=\\\"aligncenter\\\"\\u003EIII. Mission Control\\u003C\\u002Fh2\\u003E \\u003Cp class=\\\"descender dropcap-T\\\" word_count=\\\"145\\\" data-wc=\\\"145\\\"\\u003EThe offices of the Future of Humanity Institute have a hybrid atmosphere: part physics lab, part college dorm room. There are whiteboards covered with mathematical notation and technical glyphs; there are posters of “Brave New World” and HAL 9000. There is also art work by Nick Bostrom. One afternoon, he guided me to one of his pieces, “At Sea,” a digital collage that he had printed out and then drawn on. “It is a bit damaged, but the good thing about digital is that you can re-instantiate it,” he said. At the center was a pale man, nearly an apparition, clinging to a barrel in an inky-black ocean. “It is an existentialist vibe. You are hanging on for as long as you can. When you get tired, you sink, and become fish food—or maybe a current will take him to land. We don’t know.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"146\\\" data-wc=\\\"146\\\"\\u003EDespite the time he spends going to conferences and raising money, Bostrom attends to many details at the institute. “We needed a logo when we started,” he told me. “We went to this online site where you could buy the work of freelance artists. If you sat down and tried to make the ugliest logo, you couldn’t come close. Then we hired a designer, who made a blurry figure of a person. We showed it to someone here, who said it looked like a toilet sign. As soon as she said it, I thought, Oh, my God, we almost adopted a toilet sign as our logo. So I mucked around a bit and came up with a black diamond. You have the black monolith from ‘2001.’ Standing on its corner, it indicates instability. Also, there is a limit to how ugly a black square can be.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"122\\\" data-wc=\\\"122\\\"\\u003EThe institute shares office space with the Centre for Effective Altruism, and both organizations intersect with a social movement that promotes pure rationality as a guide to moral action. Toby Ord, a philosopher who works with both, told me that Bostrom often pops into his office at the end of the day, poses a problem, then leaves him pondering it for the night. Among the first of Bostrom’s questions was this: If the universe turns out to contain an infinite number of beings, then how could any single person’s action affect the cosmic balance of suffering and happiness? After lengthy discussions, they left the paradox unresolved. “My main thinking is that we can sort it out later,” Ord told me. \\u003C\\u002Fp\\u003E \\u003Cfigure class=\\\"cartoon-image\\\" data-track-location=\\\"embeddedCartoon\\\" data-cartoon-id=\\\"a19596\\\"\\u003E\\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcartoons\\u002Fa19596\\\" target=\\\"_blank\\\"\\u003E\\u003Cimg class=\\\"cartoon post-load\\\" alt=\\\"Cartoon\\\" data-src-mobile=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_a19596-500.jpg\\\" src=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_a19596-690.jpg\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cfigcaption\\u003E\\u003Cspan class=\\\"caption\\\"\\u003E“Chaucer on lyne thrie.”\\u003C\\u002Fspan\\u003E\\u003C\\u002Ffigcaption\\u003E\\u003C\\u002Ffigure\\u003E   \\u003Cp word_count=\\\"188\\\" data-wc=\\\"188\\\"\\u003EWhen I asked Bostrom if I could observe a discussion at the institute, he seemed reluctant; it was hard to judge whether he was concerned that my presence would interfere or that unfiltered talk of, say, engineered pathogens might inspire criminals. (“At some point, one gets into the realm of information hazard,” he hinted.) Eventually, he let me observe a session in the Petrov Room involving half a dozen staff members. The key question under discussion was whether a global catastrophe, on the order of a continent-wide famine, could trigger a series of geopolitical events that would result in human extinction—and whether that meant that a merely catastrophic risk could therefore be taken as seriously as an existential risk. Bostrom, wearing a gray hoodie over a blue button-­down, organized the problem on a whiteboard with visible pleasure. Anders Sandberg told me that he once spent days with Bostrom working through such a problem, distilling a complex argument to its essence. “He had to \\u003Cem\\u003Erefine\\u003C\\u002Fem\\u003E it,” he said. “We had a lot of schemes on the whiteboard that gradually were simplified to one box and three arrows.” \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F13\\\" data-total-words=\\\"8885\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Ca class=\\\"tny-page\\\" name=\\\"\\u002F10\\\" data-total-words=\\\"8885\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"101\\\" data-wc=\\\"101\\\"\\u003EFor anyone in the business of publicizing existential risk, 2015 began as a good year. Other institutes devoted to these issues had started to find their voice, bringing an additional gloss of respectability to the ideas in Bostrom’s book. The people weighing in now were no longer just former Extropians. They were credentialled, like Lord Martin Rees, an astrophysicist and the co-founder of Cambridge’s Centre for the Study of Existential Risk. In January, he wrote of A.I., in the \\u003Cem\\u003EEvening Standard\\u003C\\u002Fem\\u003E, “We don’t know where the boundary lies between what may happen and what will remain science fiction.”\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"174\\\" data-wc=\\\"174\\\"\\u003ERees’s counterpart at the Future of Life Institute, the M.I.T. physicist Max Tegmark, hosted a closed-door meeting in Puerto Rico, to try to make sense of the long-term trajectory of the research. Bostrom flew down, joining a mix of A.I. practitioners, legal scholars, and, for lack of a better term, members of the “A.I. safety” community. “These are not people who are usually in the same room,” Tegmark told me. “Someone advised me to put Valium in people’s drinks so nobody got into fistfights. But, by the time Nick’s session started, people were ready to listen to each other.” Questions that had seemed fanciful to researchers only seven years earlier were beginning to look as though they might be worth reconsidering. Whereas the Asilomar meeting concluded on a note of skepticism about the validity of the whole endeavor, the Puerto Rico conference resulted in an open letter, signed by many prominent researchers, that called for more research to insure that A.I. would be “robust and beneficial.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"160\\\" data-wc=\\\"160\\\"\\u003EBetween the two conferences, the field had experienced a revolution, built on an approach called deep learning—a type of neural network that can discern complex patterns in huge quantities of data. For de­c­ades, researchers, hampered by the limits of their hardware, struggled to get the technique to work well. But, beginning in 2010, the increasing availability of Big Data and cheap, powerful video-­game processors had a dramatic effect on performance. Without any profound theoretical breakthrough, deep learning suddenly offered breathtaking advances. “I have been talking to quite a few contemporaries,” Stuart Russell told me. “Pretty much everyone sees examples of progress they just didn’t expect.” He cited a YouTube clip of a four-legged robot: one of its designers tries to kick it over, but it quickly regains its balance, scrambling with uncanny naturalness. “A problem that had been viewed as very difficult, where progress was slow and incremental, was all of a sudden done. Locomotion: done.”\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"77\\\" data-wc=\\\"77\\\"\\u003EIn an array of fields—speech processing, face recognition, language translation—the approach was ascendant. Researchers working on computer vision had spent years to get systems to identify objects. In almost no time, the deep-learning networks crushed their records. In one common test, using a database called ImageNet, humans identify photographs with a five-per-cent error rate; Google’s network operates at 4.8 per cent. A.I. systems can differentiate a Pembroke Welsh Corgi from a Cardigan Welsh Corgi. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"154\\\" data-wc=\\\"154\\\"\\u003ELast October, Tomaso Poggio, an M.I.T. researcher, gave a skeptical interview. “The ability to describe the content of an image would be one of the most intellectually challenging things of all for a machine to do,” he said. “We will need another cycle of basic research to solve this kind of question.” The cycle, he predicted, would take at least twenty years. A month later, Google announced that its deep-learning network could analyze an image and offer a caption of what it saw: “Two pizzas sitting on top of a stove top,” or “People shopping at an outdoor market.” When I asked Poggio about the results, he dismissed them as automatic associations between objects and language; the system did not \\u003Cem\\u003Eunderstand\\u003C\\u002Fem\\u003E what it saw. “Maybe human intelligence is the same thing, in which case I am wrong, or not, in which case I was right,” he told me. “How do you decide?”\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"214\\\" data-wc=\\\"214\\\"\\u003EA respected minority of A.I. researchers began to wonder: If increasingly powerful hardware could facilitate the deep-learning revolution, would it make other long-shelved A.I. principles viable? “Suppose the brain is just a million different evolutionarily developed hacks: one for smell, one for recognizing faces, one for how you recognize animals,” Tom Mitchell, who holds a chair in machine learning at Carnegie Mellon, told me. “If that is what underlies intelligence, then I think we are far, far from getting there—because we don’t have many of those hacks. On the other hand, suppose that what underlies intelligence are twenty-three general mechanisms, and when you put them together you get synergy, and it works. We now have systems that can do a pretty good job with computer vision—and it turns out that we didn’t have to construct a million hacks. So part of the uncertainty is: if we do not need a million different hacks, then will we find the right twenty-­three fundamental generic methods?” He paused. “I no longer have the feeling, which I had twenty-five years ago, that there are gaping holes. I know we don’t have a good architecture to assemble the ideas, but it is not obvious to me that we are missing components.” \\u003C\\u002Fp\\u003E \\u003Cfigure class=\\\"cartoon-image\\\" data-track-location=\\\"embeddedCartoon\\\" data-cartoon-id=\\\"a19555\\\"\\u003E\\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcartoons\\u002Fa19555\\\" target=\\\"_blank\\\"\\u003E\\u003Cimg class=\\\"cartoon post-load\\\" alt=\\\"Cartoon\\\" data-src-mobile=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_a19555-500.jpg\\\" src=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_a19555-690.jpg\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cfigcaption\\u003E\\u003C\\u002Ffigcaption\\u003E\\u003C\\u002Ffigure\\u003E   \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F14\\\" data-total-words=\\\"9765\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"160\\\" data-wc=\\\"160\\\"\\u003EBostrom noticed the shift in attitude. He recently conducted a poll of A.I. researchers to gauge their sense of progress, and in Puerto Rico a survey gathered opinions on how long it would be until an artificial intelligence could reason indistinguishably from a human being. Like Bostrom, the engineers are often careful to express their views as probabilities, rather than as facts. Richard Sutton, a Canadian computer scientist whose work has earned tens of thousands of scholarly citations, gives a range of outcomes: there is a ten-per-cent chance that A.I. will never be achieved, but a twenty-five-per-cent chance that it will arrive by 2030. The median response in Bostrom’s poll gives a fifty-fifty chance that human-level A.I. would be attained by 2050. These surveys are unscientific, but he is confident enough to offer an interpretive assumption: “It is not a ridiculous prospect to take seriously the possibility that it can happen in the lifetime of people alive today.”\\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-page\\\" name=\\\"\\u002F11\\\" data-total-words=\\\"9925\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp class=\\\"descender dropcap-O\\\" word_count=\\\"125\\\" data-wc=\\\"125\\\"\\u003EOn my last day in Oxford, I walked with Bostrom across town. He was racing to catch a train to London, to speak at the Royal Society, one of the world’s oldest scientific institutions. His spirits were high. The gulf between the transhumanists and the scientific community was slowly shrinking. Elon Musk had pledged ten million dollars in grants for academics seeking to investigate A.I. safety, and, rather than mock him, researchers applied for the money; Bostrom’s institute was helping to evaluate the proposals. “Right now, there is a lot of interest,” he told me. “But then there were all these long years when nobody else seemed to pay attention at all. I am not sure which is the less abnormal condition.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"153\\\" data-wc=\\\"153\\\"\\u003EThere were clear limits to that interest. To publicly stake out a position in the middle of the debate was difficult, not least because of the polarized atmosphere Bostrom’s book had helped to create. Even though a growing number of researchers were beginning to suspect that profound questions loomed, and that they might be worth addressing now, it did not mean that they believed A.I. would lead inevitably to an existential demise or a techno-utopia. Most of them were engaged with more immediate problems: privacy, unemployment, weaponry, driverless cars running amok. When I asked Bostrom about this pragmatic ethical awakening, he reacted with dismay. “My fear is that it would swallow up the concerns for the longer term,” he said. “On the other hand, yes, maybe it is useful to build bridges to these different communities. Kind of makes the issue part of a larger continuum of things to work on.”\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"133\\\" data-wc=\\\"133\\\"\\u003EAt the Royal Society, Bostrom took a seat at the back of a large hall. As he crossed his legs, I noticed a thin leather band around his ankle. A metal buckle was engraved with contact information for Alcor, a cryonics facility in Arizona, where Bostrom is a fee-paying member. Within hours of his death, Alcor will take custody of his body and maintain it in a giant steel bottle flooded with liquid nitrogen, in the hope that one day technology will allow him to be revived, or to have his mind uploaded into a computer. When he signed up, two other colleagues at the institute joined him. “My background is transhumanism,” he once reminded me. “The character of that is gung-ho techno-cheerleading, bring it on now, where are my life-­extension pills.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"107\\\" data-wc=\\\"107\\\"\\u003EThe hall was packed with some of the most technically sophisticated researchers in A.I.—not necessarily Bostrom’s people—and when he spoke he began by trying to assure them that his concern was not out of Ludditism. “It would be tragic if machine intelligence were never developed to its full capacity,” he said. “I think this is ultimately the key, or the portal, we have to pass through to realize the full dimension of humanity’s long-term potential.” But, even as he avoided talk of existential risk, he pressed his audience to consider the danger of building an A.I. without regarding its ethical design. \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F15\\\" data-total-words=\\\"10443\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"48\\\" data-wc=\\\"48\\\"\\u003EAn attendee raised his hand to object. “We can’t control basic computer worms,” he said. “The A.I. that will happen is going to be a highly adaptive, emergent capability, and highly distributed. We will be able to work with it—\\u003Cem\\u003Efor\\u003C\\u002Fem\\u003E it—not necessarily contain it.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"64\\\" data-wc=\\\"64\\\"\\u003E“I guess I am a little frustrated,” Bos­trom responded. “People tend to fall into two camps. On one hand, there are those, like yourself, who think it is probably hopeless. The other camp thinks it is easy enough that it will be solved automatically. And both of these have in common the implication that we don’t have to make any effort now.”\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"106\\\" data-wc=\\\"106\\\"\\u003EFor the rest of the day, engineers presented their work at the lectern, each promising a glimpse of the future—robot vision, quantum computers, algorithms called “thought vectors.” Early in Bostrom’s career, he predicted that cascading economic demand for an A.I. would build up across the fields of medicine, entertainment, finance, and defense. As the technology became useful, that demand would only grow. “If you make a one-per-cent improvement to something—say, an algorithm that recommends books on Amazon—there is a lot of value there,” Bostrom told me. “Once every improvement potentially has enormous economic benefit, that promotes effort to make more improvements.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"124\\\" data-wc=\\\"124\\\"\\u003EMany of the world’s largest tech companies are now locked in an A.I. arms race, purchasing other companies and opening specialized units to advance the technology. Industry is vacuuming up Ph.D.s so quickly that people in the field worry there will no longer be top talent in academia. After decades of pursuing narrow forms of A.I., researchers are seeking to integrate them into systems that resemble a general intellect. Since I.B.M.’s Watson won “Jeopardy,” the company has committed more than a billion dollars to develop it, and is reorienting its business around “cognitive systems.” One senior I.B.M. executive declared, “The separation between human and machine is going to blur in a very fundamental way.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"83\\\" data-wc=\\\"83\\\"\\u003EAt the Royal Society, a contingent of researchers from Google occupied a privileged place; they likely had more resources at their disposal than anyone else in the room. Early on, Google’s founders, Larry Page and Sergey Brin, understood that the company’s mission required solving fundamental A.I. problems. Page has said that he believes the ideal system would understand questions, even anticipate them, and produce responses in conversational language. Google scientists often invoke the computer in “Star Trek” as a model. \\u003C\\u002Fp\\u003E \\u003Cfigure class=\\\"cartoon-image\\\" data-track-location=\\\"embeddedCartoon\\\" data-cartoon-id=\\\"a19484\\\"\\u003E\\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcartoons\\u002Fa19484\\\" target=\\\"_blank\\\"\\u003E\\u003Cimg class=\\\"cartoon post-load\\\" alt=\\\"Cartoon\\\" data-src-mobile=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_a19484-500.jpg\\\" src=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_a19484-690.jpg\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cfigcaption\\u003E\\u003Cspan class=\\\"caption\\\"\\u003E“O.K., there’s the moon—now give me a nice long howl instead of last night’s yip.”\\u003C\\u002Fspan\\u003E\\u003C\\u002Ffigcaption\\u003E\\u003C\\u002Ffigure\\u003E   \\u003Ca class=\\\"tny-page\\\" name=\\\"\\u002F12\\\" data-total-words=\\\"10868\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"168\\\" data-wc=\\\"168\\\"\\u003EIn recent years, Google has purchased seven robotics companies and several firms specializing in machine intelligence; it may now employ the world’s largest contingent of Ph.D.s in deep learning. Perhaps the most interesting acquisition is a British company called DeepMind, started in 2011 to build a general artificial intelligence. Its founders had made an early bet on deep learning, and sought to combine it with other A.I. mechanisms in a cohesive architecture. In 2013, they published the results of a test in which their system played seven classic Atari games, with no instruction other than to improve its score. For many people in A.I., the importance of the results was immediately evident. I.B.M.’s chess program had defeated Garry Kasparov, but it could not beat a three-year-old at tic-tac-toe. In six games, DeepMind’s system outperformed all previous algorithms; in three it was superhuman. In a boxing game, it learned to pin down its opponent and subdue him with a barrage of punches.\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"75\\\" data-wc=\\\"75\\\"\\u003EWeeks after the results were released, Google bought the company, reportedly for half a billion dollars. DeepMind placed two unusual conditions on the deal: its work could never be used for espionage or defense purposes, and an ethics board would oversee the research as it drew closer to achieving A.I. Anders Sandberg had told me, “We are happy that they are among the most likely to do it. They recognize there are some problems.”\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"100\\\" data-wc=\\\"100\\\"\\u003EDeepMind’s chief founder, Demis Hassabis, described his company to the audience at the Royal Society as an “Apollo Program” with a two-part mission: “Step one, solve intelligence. Step two, use it to solve everything else.” Since the test in 2013, his system had aced more than a dozen other Atari titles. Hassabis demonstrated an unpublished trial using a three-dimensional driving game, in which it had quickly outperformed the game’s automated drivers. The plan was to test it in increasingly complex virtual environments and, eventually, in the real world. The patent lists a range of uses, from finance to robotics. \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F16\\\" data-total-words=\\\"11211\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"166\\\" data-wc=\\\"166\\\"\\u003EHassabis was clear about the challenges. DeepMind’s system still fails hopelessly at tasks that require long-range planning, knowledge about the world, or the ability to defer rewards—things that a five-year-old child might be expected to handle. The company is working to give the algorithm conceptual understanding and the capability of transfer learning, which allows humans to apply lessons from one situation to another. These are not easy problems. But DeepMind has more than a hundred Ph.D.s to work on them, and the rewards could be immense. Hassabis spoke of building artificial scientists to resolve climate change, disease, poverty. “Even with the smartest set of humans on the planet working on these problems, these systems might be so complex that it is difficult for individual humans, scientific experts,” he said. “If we can crack what intelligence is, then we can use it to help us solve all these other problems.” He, too, believes that A.I. is a gateway to expanded human potential. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"72\\\" data-wc=\\\"72\\\"\\u003EThe keynote speaker at the Royal Society was another Google employee: Geoffrey Hinton, who for decades has been a central figure in developing deep learning. As the conference wound down, I spotted him chatting with Bostrom in the middle of a scrum of researchers. Hinton was saying that he did not expect A.I. to be achieved for decades. “No sooner than 2070,” he said. “I am in the camp that is hopeless.”\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"14\\\" data-wc=\\\"14\\\"\\u003E“In that you think it will not be a cause for good?” Bostrom asked. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"27\\\" data-wc=\\\"27\\\"\\u003E“I think political systems will use it to terrorize people,” Hinton said. Already, he believed, agencies like the N.S.A. were attempting to abuse similar technology. \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"9\\\" data-wc=\\\"9\\\"\\u003E“Then why are you doing the research?” Bostrom asked.\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"71\\\" data-wc=\\\"71\\\"\\u003E“I could give you the usual arguments,” Hinton said. “But the truth is that the prospect of discovery is too \\u003Cem\\u003Esweet\\u003C\\u002Fem\\u003E.” He smiled awkwardly, the word hanging in the air—an echo of Oppenheimer, who famously said of the bomb, “When you see something that is technically sweet, you go ahead and do it, and you argue about what to do about it only after you have had your technical success.” \\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"82\\\" data-wc=\\\"82\\\"\\u003EAs the scientists retreated to tables set up for refreshments, I asked Hinton if he believed an A.I. could be controlled. “That is like asking if a child can control his parents,” he said. “It can happen with a baby and a mother—there is biological hardwiring—but there is not a good track record of less intelligent things controlling things of greater intelligence.” He looked as if he might elaborate. Then a scientist called out, “Let’s all get drinks!”\\u003C\\u002Fp\\u003E \\u003Cp class=\\\"descender dropcap-B\\\" word_count=\\\"118\\\" data-wc=\\\"118\\\"\\u003EBostrom had little interest in the cocktail party. He shook a few hands, then headed for St. James’s Park, a public garden that extends from the gates of Buckingham Palace through central London. The world appeared in splendorous analog: sunlight over trees, duck ponds, children and grandparents feeding birds. The spot had been a park for hundreds of years, and the vista seemed timeless. Yet, during the past millennium, the grounds had also been a marsh, a leper hospital, a deer sanctuary, and royal gardens. It seemed plausible that, a thousand years from now, digital posthumans, regarding it as wasted space, would tear it up, replace the landscaping with computer banks, and erect a vast virtual idyll.\\u003C\\u002Fp\\u003E \\u003Cp word_count=\\\"176\\\" data-wc=\\\"176\\\"\\u003EBostrom’s pace settled into its natural quickness as we circled the park. He talked about his family; he would be seeing his wife and son soon. He was reading widely: history, psychology, economics. He was learning to code. He was thinking about expanding his institute. Although he did not know it then, F.H.I. was about to receive one and a half million dollars from Elon Musk, to create a unit that would craft social policies informed by some of Bostrom’s theories. He would need to hire people. He was also giving thought to the framing of his message. “A lot more is said about the risks than the upsides, but that is not necessarily because the upside is not there,” he told me. “There is just more to be said about the risk—and maybe more use in describing the pitfalls, so we know how to steer around them—than spending time now figuring out the details of how we are going to furnish the great palace a thousand years from now.”\\u003C\\u002Fp\\u003E \\u003Cfigure class=\\\"cartoon-image\\\" data-track-location=\\\"embeddedCartoon\\\" data-cartoon-id=\\\"a19698\\\"\\u003E\\u003Ca href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcartoons\\u002Fa19698\\\" target=\\\"_blank\\\"\\u003E\\u003Cimg class=\\\"cartoon post-load\\\" alt=\\\"Cartoon\\\" data-src-mobile=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_a19698-500.jpg\\\" src=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fwp-content\\u002Fuploads\\u002F2015\\u002F11\\u002F151123_a19698-690.jpg\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cfigcaption\\u003E\\u003Cspan class=\\\"caption\\\"\\u003E“I’m bringing on Josh here for when we take over fantasy sports betting.”\\u003C\\u002Fspan\\u003E\\u003C\\u002Ffigcaption\\u003E\\u003C\\u002Ffigure\\u003E   \\u003Ca class=\\\"tny-page\\\" name=\\\"\\u002F13\\\" data-total-words=\\\"11946\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"164\\\" data-wc=\\\"164\\\"\\u003EWe passed a fountain, near a cluster of rocks engineered to give ducks a resting place. Bostrom, in his forties, must soon contend with physical decline, and he spoke with annoyance of the first glimmers of mortality. Even though he is an Alcor member, there is no guarantee that cryonics will work. Perhaps the most radical of his visions is that superintelligent A.I. will hasten the uploading of minds—what he calls “whole-brain emulations”—technology that might not be possible for centuries, if at all. Bostrom, in his most hopeful mode, imagines emulations not only as reproductions of the original intellect “with memory and personality intact”—a soul in the machine—but as minds expandable in countless ways. “We live for seven decades, and we have three-pound lumps of cheesy matter to think with, but to me it is plausible that there could be extremely valuable mental states outside this little particular set of possibilities that might be much better,” he told me. \\u003C\\u002Fp\\u003E \\u003Ca class=\\\"tny-slot\\\" name=\\\"\\u002F17\\\" data-total-words=\\\"12110\\\"\\u003E\\u003C\\u002Fa\\u003E\\u003Cp word_count=\\\"242\\\" data-wc=\\\"242\\\"\\u003EIn his book, Bostrom considers a distant future in which trillions of digital minds merge into an enormous cognitive cyber-soup. “Whether the set of extremely positive posthuman modes of being would include some kind of dissolved bouillon, there is some uncertainty,” he said. “If you look at religious views, there are many where merging with something greater is a form of heaven, being in the presence of this enormous beauty and goodness. In many traditions, the best possible state does not involve being a little individual pursuing goals. But it is hard to get a grasp of what would be going on in that soup. Maybe some soups would not be preferable as a long-term outcome. I don’t know.” He stopped and looked ahead. “What I want to avoid is to think from our parochial 2015 view—from my own limited life experience, my own limited brain—and super-confidentially postulate what is the best form for civilization a billion years from now, when you could have brains the size of planets and billion-year life spans. It seems unlikely that we will figure out some detailed blueprint for utopia. What if the great apes had asked whether they should evolve into \\u003Cem\\u003EHomo sapiens\\u003C\\u002Fem\\u003E—pros and cons—and they had listed, on the pro side, ‘Oh, we could have a lot of bananas if we became human’? Well, we can have unlimited bananas now, but there is more to the human condition than that.” \\u003Cspan class=\\\"dingbat\\\"\\u003E♦\\u003C\\u002Fspan\\u003E\\u003C\\u002Fp\\u003E \\u003Cp class=\\\"credit\\\" word_count=\\\"9\\\" data-wc=\\\"9\\\"\\u003E\\u003Cem\\u003EIllustration by Todd St. John\\u002FCoding by Jono Brandel.\\u003C\\u002Fem\\u003E\\u003C\\u002Fp\\u003E \\t\\t\\t\\u003C\\u002Fdiv\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fdiv\\u003E\\u003C!-- #content --\\u003E\\n\\n\\t\\t\\t\\t\\t\\t\\t\\u003Cfooter\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\u003Csection class=\\\"article-contributors\\\"\\u003E\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Caside class=\\\"author-details\\\" itemscope itemtype=\\\"http:\\u002F\\u002Fschema.org\\u002FPerson\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cmeta itemprop=\\\"name\\\" content=\\\"Raffi Khatchadourian\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cmeta itemprop=\\\"url\\\" content=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcontributors\\u002Fraffi-khatchadourian\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cmeta itemprop=\\\"image\\\" content=\\\"https:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Farchive\\u002Fimages\\u002Fraffi-khatchadourian.jpg\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cdiv rel=\\\"me\\\" itemprop=\\\"url\\\" class=\\\"author-details-wrap\\\" \\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cdiv class=\\\"author-image-wrap\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cimg class=\\\"author-image\\\" src=\\\"https:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Farchive\\u002Fimages\\u002Fraffi-khatchadourian.jpg\\\" \\u002F\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fdiv\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cdiv class=\\\"author-masthead has-bio\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cdiv class=\\\"contributor-info\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cp\\u003ERaffi Khatchadourian became a staff writer at \\u003Cem\\u003EThe New Yorker\\u003C\\u002Fem\\u003E in 2008.\\u003C\\u002Fp\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cul class=\\\"author-links\\\"\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Cli\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003Ca class=\\\"more-link\\\" href=\\\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fcontributors\\u002Fraffi-khatchadourian\\\"  title=\\\"Raffi Khatchadourian\\\"\\u003EMore\\u003C\\u002Fa\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fli\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Ful\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fdiv\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fdiv\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fdiv\\u003E\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Faside\\u003E\\n\\t\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Fsection\\u003E\\n\\n\\t\\t\\t\\t\\t\\t\\t\\u003C\\u002Ffooter\\u003E\\n\\t\\t\\t\\t\\t\\t\\u003C\\u002Farticle\\u003E\\n\\t\\t\\t\\t\\t\\u003C\\u002Fdiv\\u003E\\n\\n\\t\\t\\t\\t\\t\\u003C!-- \\u003Cscript type=\\\"text\\u002Fjavascript\\\"\\n                                          async src=\\\"https:\\u002F\\u002Fprojects.newyorker.com\\u002Finteractive\\u002F2015\\u002Fai-story\\u002Fmanifest.js\\\"\\u003E --\\u003E\\n\",\"behavior\":\"all\"},\"isAffiliateLinksDisabled\":false,\"isHeroAdVisible\":true,\"isLicensedPartner\":false,\"isShoppable\":false,\"hasNewsletterInBody\":false,\"lang\":\"en-US\",\"langTranslations\":{\"AccountInformationCard.Discard\":[{\"type\":0,\"value\":\"Discard\"}],\"AccountInformationCard.ErrorMessage\":[{\"type\":0,\"value\":\"Unable to save username. Please try again.\"}],\"AccountInformationCard.SubmitButtonLabel\":[{\"type\":0,\"value\":\"Save Username\"}],\"AccountInformationCard.SuccessMessage\":[{\"type\":0,\"value\":\"Your username is saved.\"}],\"AccountInformationCard.UserNameLabel\":[{\"type\":0,\"value\":\"Username\"}],\"AccountInformationCard.UserNamePlaceholer\":[{\"type\":0,\"value\":\"YOUR_USERNAME\"}],\"AccountInformationCard.alreadyTakenError\":[{\"type\":0,\"value\":\"This Username is already taken.\"}],\"AccountInformationCard.lengthError\":[{\"type\":0,\"value\":\"Usernames must be between 2 and 23 characters.\"}],\"AccountInformationCard.specialCharError\":[{\"type\":0,\"value\":\"Usernames can only include letters, numbers and underscores (_).\"}],\"AccountLinks.NavigationAriaLabel\":[{\"type\":0,\"value\":\"Account\"}],\"Ad.adLabel\":[{\"type\":0,\"value\":\"Advertisement\"}],\"AgeGate.AcceptLabel\":[{\"type\":0,\"value\":\"I am 18+\"}],\"AgeGate.DeclineLabel\":[{\"type\":0,\"value\":\"I'm under 18\"}],\"AgeGate.DekText\":[{\"type\":0,\"value\":\"This material is intended for people over the age of 18\"}],\"AgeGate.HedText\":[{\"type\":0,\"value\":\"Are you 18 or over?\"}],\"ArticlePage.Back to article\":[{\"type\":0,\"value\":\"Back to article\"}],\"ArticlePage.DefaultDisclaimer\":[{\"type\":0,\"value\":\"All products are independently selected by our editors. If you buy something, we may earn an affiliate commission.\"}],\"ArticlePage.From the issue of\":[{\"type\":0,\"value\":\"From the issue of\"}],\"ArticlePage.MessageBannerContent\":[{\"type\":0,\"value\":\"Your fellow home cooks thank you.\"}],\"ArticlePage.MessageBannerTitle\":[{\"type\":0,\"value\":\"Note added\"}],\"ArticlePage.TruncatedButtonLabel\":[{\"type\":0,\"value\":\"Read Full Story\"}],\"AudioPrimaryLabel.Listen\":[{\"type\":0,\"value\":\"Listen\"}],\"AudioSecondaryLabel.NowPlaying\":[{\"type\":0,\"value\":\"Now playing\"}],\"BizzaboEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Bizzabo content\"}],\"BlueskyEmbed.AriaLabelText\":[{\"type\":0,\"value\":\"social media post\"}],\"BlueskyEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Bluesky content\"}],\"Bookmark.SignInMessage\":[{\"type\":0,\"value\":\"After signing in, you can save stories and easily revisit them on any device—even off-line.\"}],\"BookmarkButton.Label\":[{\"type\":0,\"value\":\"save recipe\"}],\"BookmarkButton.LabelSaved\":[{\"type\":0,\"value\":\"recipe saved\"}],\"BookmarkIcon.Alert\":[{\"type\":0,\"value\":\"Save this story for later.\"}],\"BookmarkIcon.CompletionLabel\":[{\"type\":0,\"value\":\"Story saved. To revisit this article, select My Account, then View Saved Stories. Press Escape to dismiss tooltip.\"}],\"BookmarkIcon.Label\":[{\"type\":0,\"value\":\"Story saved\"}],\"BookmarkIcon.OnboardingAriaLabel\":[{\"type\":0,\"value\":\"Save story. Press Enter to save this story for later. Press Escape to dismiss tooltip.\"}],\"BookmarkIcon.SignInMessage\":[{\"type\":0,\"value\":\"After signing in, you can save stories and easily revisit them on any device—even off-line.\"}],\"BookmarkPrimaryLabel.RecipeSaved\":[{\"type\":0,\"value\":\"Recipe Saved\"}],\"BookmarkPrimaryLabel.Save\":[{\"type\":0,\"value\":\"Save\"}],\"BookmarkPrimaryLabel.SaveRecipe\":[{\"type\":0,\"value\":\"Save Recipe\"}],\"BookmarkPrimaryLabel.SaveThisStory\":[{\"type\":0,\"value\":\"Save this story\"}],\"BookmarkPrimaryLabel.Saved\":[{\"type\":0,\"value\":\"Saved\"}],\"BookmarkPrimaryLabel.SavedToLibrary\":[{\"type\":0,\"value\":\"Saved to library\"}],\"BusinessApplication.AccountProfileIsResubmittedText\":[{\"type\":0,\"value\":\"Updated\"}],\"BusinessApplication.AccountProfileLink\":[{\"type\":1,\"value\":\"url\"}],\"BusinessApplication.ApplicationAPIErrorMessage\":[{\"type\":0,\"value\":\"We couldn't complete this. Please try again.\"}],\"BusinessApplication.ApplicationBannerDeletePhotoLabel\":[{\"type\":0,\"value\":\"Delete Banner Photo\"}],\"BusinessApplication.ApplicationBannerImageDesc\":[{\"type\":0,\"value\":\"This will appear very large at the top of your profile page and must be landscape in ratio. It should show a completed project that's representative of your design style.\"}],\"BusinessApplication.ApplicationBannerImageTitle\":[{\"type\":0,\"value\":\"Banner Image\"}],\"BusinessApplication.ApplicationBannerUploadImageLabel\":[{\"type\":0,\"value\":\"Upload Banner Photo\"}],\"BusinessApplication.ApplicationFormBusinessHeader\":[{\"type\":0,\"value\":\"Describe Your Business\"}],\"BusinessApplication.ApplicationFormErrorHeader\":[{\"type\":0,\"value\":\"The following errors must be corrected:\"}],\"BusinessApplication.ApplicationFormSaveAndExitLink\":[{\"type\":0,\"value\":\"Save & Exit Application\"}],\"BusinessApplication.ApplicationFormSaveAndExitText\":[{\"type\":0,\"value\":\"If you want to continue later,\"}],\"BusinessApplication.ApplicationFormSectionHeader\":[{\"type\":0,\"value\":\"About Your Business\"}],\"BusinessApplication.ApplicationFormSectionSubHeader\":[{\"type\":0,\"value\":\"You can save and return by selecting the ‘Save and Exit Application’ link at the bottom of the page.\"}],\"BusinessApplication.ApplicationImagesInlineValidationError\":[{\"type\":0,\"value\":\"Images must be at least 1200 x 1200 pixels and no larger than 30MB. Please try again or select a different file.\"}],\"BusinessApplication.ApplicationModalExitButton\":[{\"type\":0,\"value\":\"Exit Without Saving\"}],\"BusinessApplication.ApplicationModalSaveExitButton\":[{\"type\":0,\"value\":\"Save and exit\"}],\"BusinessApplication.ApplicationPageSectionHeader\":[{\"type\":0,\"value\":\"Create Your Profile\"}],\"BusinessApplication.ApplicationPageSectionHeader2\":[{\"type\":0,\"value\":\"Your Profile Application\"}],\"BusinessApplication.ApplicationPageSectionSubHeader\":[{\"type\":0,\"value\":\"Start your application by completing a business profile.\"}],\"BusinessApplication.ApplicationPhotoLabel\":[{\"type\":0,\"value\":\"Photos\"}],\"BusinessApplication.ApplicationPhotosDescription\":[{\"type\":0,\"value\":\"Images must be JPEG or PNG format. Minimum size: 1200 x 1200 pixels. Maximum file size: 30MB.\"}],\"BusinessApplication.ApplicationPortfolioDeletePhotoLabel \":[{\"type\":0,\"value\":\"Delete Portfolio Photo\"}],\"BusinessApplication.ApplicationProfileDeletePhotoLabel\":[{\"type\":0,\"value\":\"Delete Profile Photo\"}],\"BusinessApplication.ApplicationProfileImageDesc\":[{\"type\":0,\"value\":\"This will appear next to your company name and contact details. You must upload an image that is square in ratio but it will automatically crop as you see in the preview. The photo can be of you, your colleagues or company logo.\"}],\"BusinessApplication.ApplicationProfileImageTitle\":[{\"type\":0,\"value\":\"Profile Image\"}],\"BusinessApplication.ApplicationProfileUploadImageLabel\":[{\"type\":0,\"value\":\"Upload Profile Photo\"}],\"BusinessApplication.ApplicationUploadImageLabel \":[{\"type\":0,\"value\":\"Upload Portfolio Photo &nbsp\"}],\"BusinessApplication.ApplicationWorkImageDesc\":[{\"type\":0,\"value\":\"The first will appear on the search results page. These should be of recent projects or work you have done. Include at least \"},{\"type\":1,\"value\":\"minWorkImages\"},{\"type\":0,\"value\":\".\"}],\"BusinessApplication.ApplicationWorkImageTitle\":[{\"type\":0,\"value\":\"Portfolio\"}],\"BusinessApplication.ApprovedAccountProfileBtnLabel\":[{\"type\":0,\"value\":\"Check My Profile\"}],\"BusinessApplication.ApprovedAccountProfileDek\":[{\"type\":0,\"value\":\"You're almost there! The AD PRO editors may have made minor changes to your page for consistency and style. To proceed, use the secure payment link in the email we sent you. If you have any questions, you can reach us at \"},{\"type\":1,\"value\":\"email\"},{\"type\":0,\"value\":\".\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"Once payment is processed, you'll have access to all the AD PRO features, then receive a link to your profile once it's published ahead of launch.\"}],\"BusinessApplication.ApprovedAccountProfileHed\":[{\"type\":0,\"value\":\"Your AD PRO Directory Application Has Been Approved\"}],\"BusinessApplication.ApprovedBannerDek\":[{\"type\":0,\"value\":\"Your profile is ready to be published. You cannot make any changes to it at this time.\"}],\"BusinessApplication.ApprovedBannerTitle\":[{\"type\":0,\"value\":\"Application Approved\"}],\"BusinessApplication.BannerIsResubmittedText\":[{\"type\":0,\"value\":\"updated\"}],\"BusinessApplication.BannerIsSubmittedText\":[{\"type\":0,\"value\":\"submitted\"}],\"BusinessApplication.BannerOnSubmitValidationError\":[{\"type\":0,\"value\":\"Upload a banner photo with the correct file type and size\"}],\"BusinessApplication.CompanyBusinessTypeLabel\":[{\"type\":0,\"value\":\"Choose a Category\"}],\"BusinessApplication.CompanyBusinessTypeOnSubmitValidationError\":[{\"type\":0,\"value\":\"Choose a Business Category\"}],\"BusinessApplication.CompanyCityInlineValidationError\":[{\"type\":0,\"value\":\"Don't use special characters.\"}],\"BusinessApplication.CompanyCityLabel\":[{\"type\":0,\"value\":\"CITY\"}],\"BusinessApplication.CompanyCityOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter a City\"}],\"BusinessApplication.CompanyCountryAssistiveLabel\":[{\"type\":0,\"value\":\"At the moment, we are only open to US-based designers. If you’re overseas, you can \"},{\"type\":1,\"value\":\"registerLink\"},{\"type\":0,\"value\":\" and be the first to know about the Directory’s global expansion.\"}],\"BusinessApplication.CompanyCountryLabel\":[{\"type\":0,\"value\":\"COUNTRY\"}],\"BusinessApplication.CompanyCountryRegisterHereLabel\":[{\"type\":0,\"value\":\"register here\"}],\"BusinessApplication.CompanyDescriptionLabel\":[{\"type\":0,\"value\":\"YOUR BUSINESS IN DETAIL\"}],\"BusinessApplication.CompanyDescriptionOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter the Business in Detail information. It must be between 1000 and 2000 characters.\"}],\"BusinessApplication.CompanyDescriptionPlaceholderText\":[{\"type\":0,\"value\":\"Tell us about you and your business in more detail. What sets you apart from the competition? Do you specialize in a particular style? How do you prefer to work? What are some examples of recent projects? Please also note in this section if you are willing to travel for a job. This information will appear on your business profile page.\"}],\"BusinessApplication.CompanyEmailInlineValidationError\":[{\"type\":0,\"value\":\"Check the email address is valid\"}],\"BusinessApplication.CompanyEmailLabel\":[{\"type\":0,\"value\":\"EMAIL ADDRESS\"}],\"BusinessApplication.CompanyEmailOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter a valid Email Address\"}],\"BusinessApplication.CompanyNameInlineValidationError\":[{\"type\":0,\"value\":\"Use a maximum of 128 characters with no special characters\"}],\"BusinessApplication.CompanyNameLabel\":[{\"type\":0,\"value\":\"COMPANY NAME\"}],\"BusinessApplication.CompanyNameOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter a Company Name with a maximum of 128 characters\"}],\"BusinessApplication.CompanyPhoneInlineValidationError\":[{\"type\":0,\"value\":\"Check the phone number is valid\"}],\"BusinessApplication.CompanyPhoneLabel\":[{\"type\":0,\"value\":\"PHONE NUMBER\"}],\"BusinessApplication.CompanyPhoneOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter a valid Phone Number using only 10 digits\"}],\"BusinessApplication.CompanyPhonePlaceholderText\":[{\"type\":0,\"value\":\"Enter 10 digits only\"}],\"BusinessApplication.CompanyProfessionLabel\":[{\"type\":0,\"value\":\"Choose a Profession\"}],\"BusinessApplication.CompanyProfessionOnSubmitValidationError\":[{\"type\":0,\"value\":\"Choose a Profession\"}],\"BusinessApplication.CompanySocialUrlInlineValidationError\":[{\"type\":0,\"value\":\"Don't use spaces\"}],\"BusinessApplication.CompanySocialUrlLabel\":[{\"type\":0,\"value\":\"Instagram username\"}],\"BusinessApplication.CompanyStateLabel\":[{\"type\":0,\"value\":\"STATE\"}],\"BusinessApplication.CompanyStateOnSubmitValidationError\":[{\"type\":0,\"value\":\"Select a State\"}],\"BusinessApplication.CompanyStatePlaceholderText\":[{\"type\":0,\"value\":\"Select an option\"}],\"BusinessApplication.CompanyStreetAddressLabel\":[{\"type\":0,\"value\":\"STREET ADDRESS\"}],\"BusinessApplication.CompanyStreetAddressOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter a Street Address\"}],\"BusinessApplication.CompanyTagLineOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter the Business in Brief information. It must be between 80 and 120 characters.\"}],\"BusinessApplication.CompanyTaglineLabel\":[{\"type\":0,\"value\":\"YOUR BUSINESS IN BRIEF\"}],\"BusinessApplication.CompanyTaglinePlaceholderText\":[{\"type\":0,\"value\":\"Tell us about you and your business and what makes you stand out from the competition. This will appear on the search results page when our readers look for someone in your profession so this should make them want to find out more.\"}],\"BusinessApplication.CompanyWebsiteInlineValidationError\":[{\"type\":0,\"value\":\"Invalid URL format. Please include the https:\\u002F\\u002F prefix, e.g., https:\\u002F\\u002Fwww.example.com.\"}],\"BusinessApplication.CompanyWebsiteLabel\":[{\"type\":0,\"value\":\"WEBSITE\"}],\"BusinessApplication.CompanyWebsitePlaceholderText\":[{\"type\":0,\"value\":\"Optional\"}],\"BusinessApplication.CompanyZipCodeInlineValidationError\":[{\"type\":0,\"value\":\"Check the Zip Code is valid\"}],\"BusinessApplication.CompanyZipCodeLabel\":[{\"type\":0,\"value\":\"ZIP CODE\"}],\"BusinessApplication.CompanyZipCodeOnSubmitValidationError\":[{\"type\":0,\"value\":\"Enter a valid ZIP Code\"}],\"BusinessApplication.DataConsentLabel\":[{\"type\":0,\"value\":\"I have read and accept the User Terms (\"},{\"type\":1,\"value\":\"link\"},{\"type\":0,\"value\":\") and confirm that the content and images submitted as part of my application belong to me and do not and will not infringe, misappropriate or otherwise violate any third party rights, including without limitation, music, talent, logos, trademarks, copyright, or any other third party intellectual property rights. I understand and agree that Conde Nast will process my personal data in accordance with its Privacy Policy (\"},{\"type\":1,\"value\":\"linkPrivacy\"},{\"type\":0,\"value\":\")\"}],\"BusinessApplication.DefaultAccountProfileBtnLabel\":[{\"type\":0,\"value\":\"Find Out More\"}],\"BusinessApplication.DefaultAccountProfileDek\":[{\"type\":0,\"value\":\"Create a personalized designer profile and list your business on our go-to guide for homeowners and renovators.\"}],\"BusinessApplication.DefaultAccountProfileHed\":[{\"type\":0,\"value\":\"Introducing the AD PRO Directory\"}],\"BusinessApplication.DraftAccountProfileBtnLabel\":[{\"type\":0,\"value\":\"Go to My Application\"}],\"BusinessApplication.DraftAccountProfileDek\":[{\"type\":0,\"value\":\"You started a business profile on \"},{\"type\":1,\"value\":\"submissionDate\"},{\"type\":0,\"value\":\".\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"Complete now and apply to join the industry's most respected roster of design talent.\"}],\"BusinessApplication.DraftAccountProfileHed\":[{\"type\":0,\"value\":\"Your AD PRO Directory Application\"}],\"BusinessApplication.DraftBannerDek\":[{\"type\":0,\"value\":\"If you have any questions about your application or the AD PRO Directory, visit our \"},{\"type\":1,\"value\":\"faq\"},{\"type\":0,\"value\":\" or contact us at \"},{\"type\":1,\"value\":\"email\"},{\"type\":0,\"value\":\".\"}],\"BusinessApplication.DraftBannerTitle\":[{\"type\":0,\"value\":\"Your profile is incomplete\"}],\"BusinessApplication.EditConsentLabel\":[{\"type\":0,\"value\":\"You understand and agree that if your application is successful the \"},{\"type\":1,\"value\":\"italicAD\"},{\"type\":0,\"value\":\" editorial team will have editorial discretion in writing your AD PRO Directory profile using the information provided in your application. The \"},{\"type\":1,\"value\":\"italicAD\"},{\"type\":0,\"value\":\" editorial team cannot accommodate suggestions or changes to profiles once published other than in the case of correcting a factual inaccuracy.\"}],\"BusinessApplication.ExpiredAccountProfileBtnLabel\":[{\"type\":0,\"value\":\"Restart\"}],\"BusinessApplication.ExpiredAccountProfileDek\":[{\"type\":0,\"value\":\"Restart your membership, by paying now.\"}],\"BusinessApplication.ExpiredAccountProfileHed\":[{\"type\":0,\"value\":\"Your AD PRO Directory membership is no longer active.\"}],\"BusinessApplication.FormLabel\":[{\"type\":0,\"value\":\"About Your Business\"}],\"BusinessApplication.FormSubmitButtonLabel\":[{\"type\":0,\"value\":\"SUBMIT APPLICATION\"}],\"BusinessApplication.MessageBannerCtaButtonLabel\":[{\"type\":0,\"value\":\"GO TO MY LISTING\"}],\"BusinessApplication.NeedInputAccountProfileBtnLabel\":[{\"type\":0,\"value\":\"Update My Application\"}],\"BusinessApplication.NeedInputAccountProfileDek\":[{\"type\":0,\"value\":\"The business profile you submitted on \"},{\"type\":1,\"value\":\"submissionDate\"},{\"type\":0,\"value\":\" has been reviewed.\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"A few changes are needed before we approve your listing. Please review the application form for our editors' comments and recommended next steps.\"}],\"BusinessApplication.NeedInputsBannerDek\":[{\"type\":0,\"value\":\"After review, the AD PRO editors have the following comments:\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":1,\"value\":\"remark\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"You can then resubmit your application for further review. If you have any questions about your application or the AD PRO Directory, visit our \"},{\"type\":1,\"value\":\"faq\"},{\"type\":0,\"value\":\" or contact us at \"},{\"type\":1,\"value\":\"email\"},{\"type\":0,\"value\":\".\"}],\"BusinessApplication.NeedInputsBannerTitle\":[{\"type\":0,\"value\":\"Our Feedback to You\"}],\"BusinessApplication.PaymentDoneAccountProfileBtnLabel\":[{\"type\":0,\"value\":\"Go to My Business Profile\"}],\"BusinessApplication.PaymentDoneAccountProfileDek\":[{\"type\":0,\"value\":\"Congratulations! We are pleased to welcome you to the AD PRO Directory.\"}],\"BusinessApplication.PaymentDoneAccountProfileHed\":[{\"type\":0,\"value\":\"Your Profile is Live on the AD PRO Directory.\"}],\"BusinessApplication.PaymentDoneBannerDek\":[{\"type\":0,\"value\":\"If you need to make changes to your listing, please email \"},{\"type\":1,\"value\":\"email\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":1,\"value\":\"profileURL\"}],\"BusinessApplication.PaymentDoneBannerTitle\":[{\"type\":0,\"value\":\"Your profile is live on the AD PRO Directory\"}],\"BusinessApplication.ProfileOnSubmitValidationError\":[{\"type\":0,\"value\":\"Upload a profile photo with the correct file type and size\"}],\"BusinessApplication.RejectedAccountProfileDek\":[{\"type\":0,\"value\":\"Thank you for your interest in joining the AD PRO Directory.\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"Unfortunately, we will not be able to accept your business at this time. We recognize this may be disappointing news, but you are welcome to apply again in the future. Please email our Directory team at \"},{\"type\":1,\"value\":\"email\"},{\"type\":0,\"value\":\" if you would like to reopen this application\"}],\"BusinessApplication.RejectedBannerDek\":[{\"type\":0,\"value\":\"You submitted this application on \"},{\"type\":1,\"value\":\"submittedAt\"},{\"type\":0,\"value\":\". To reapply, please contact us at \"},{\"type\":1,\"value\":\"email\"},{\"type\":0,\"value\":\".\"}],\"BusinessApplication.SubmitAccountProfileDek\":[{\"type\":0,\"value\":\"The \"},{\"type\":1,\"value\":\"italicAD\"},{\"type\":0,\"value\":\" Editors are reviewing your business profile and will email you shortly.\"}],\"BusinessApplication.SubmitAccountProfileHed\":[{\"type\":0,\"value\":\"We’ve Got Your \"},{\"type\":1,\"value\":\"isResubmitted\"},{\"type\":0,\"value\":\" AD PRO Directory Application\"}],\"BusinessApplication.SubmitBannerDek\":[{\"type\":0,\"value\":\"Application \"},{\"type\":1,\"value\":\"isResubmitted\"},{\"type\":0,\"value\":\" on \"},{\"type\":1,\"value\":\"submittedAt\"},{\"type\":0,\"value\":\". \"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"Your business profile is being reviewed and you can't make any changes to it at this time. The Directory team will email you shortly.\"}],\"BusinessApplication.UnderReviewAccountProfileHed\":[{\"type\":0,\"value\":\"We’ve Got Your Updated AD PRO Directory Application\"}],\"BusinessApplication.UnderReviewBannerDek\":[{\"type\":0,\"value\":\"Application updated on \"},{\"type\":1,\"value\":\"submittedAt\"},{\"type\":0,\"value\":\". \"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"Your business profile is being reviewed and you can't make any changes to it at this time. The Directory team will email you shortly.\"}],\"BusinessApplication.WorkImageOnSubmitValidationError\":[{\"type\":0,\"value\":\"Upload at least 3 photos of your work with the correct file type and size\"}],\"BusinessApplication.applicationModalButtonText\":[{\"type\":0,\"value\":\"Continue with the Application\"}],\"BusinessApplication.applicationModalMessage1\":[{\"type\":0,\"value\":\"Ensure you have saved your application before leaving.\"}],\"BusinessApplication.applicationModalMessage2\":[{\"type\":0,\"value\":\"Go to your AD account to complete your AD PRO Directory application\"}],\"BusinessApplication.applicationPhotosAddInfoText\":[{\"type\":0,\"value\":\"Add Info\"}],\"BusinessApplication.applicationPhotosEditInfoText\":[{\"type\":0,\"value\":\"Edit Info\"}],\"BusinessApplication.applicationSessionErrorMessage\":[{\"type\":0,\"value\":\"Something went wrong. Please try again.\"}],\"BusinessApplication.applicationWorkImageAdditionalDesc\":[{\"type\":0,\"value\":\"Portfolio Photo 1 will appear on the search results page and must be landscape in ratio.\"}],\"BusinessApplication.captionPlaceholderText\":[{\"type\":0,\"value\":\"Type your caption here. Captions are optional but should provide context to the user (e.g. location and scope of project).\"}],\"BusinessApplication.creditPlaceholderText\":[{\"type\":0,\"value\":\"Type your credit here. Credits are optional but please include the photographer’s name or image source if known.\"}],\"BusinessApplication.dataConsentOnSubmitValidationError\":[{\"type\":0,\"value\":\"Accept the user terms and conditions\"}],\"BusinessApplication.editConsentOnSubmitValidationError\":[{\"type\":0,\"value\":\"Please read and accept the terms\"}],\"BusinessApplication.imageCaptionLabel\":[{\"type\":0,\"value\":\"CAPTION\"}],\"BusinessApplication.imageCreditCaptionUpdateLabel\":[{\"type\":0,\"value\":\"UPDATE\"}],\"BusinessApplication.imageCreditLabel\":[{\"type\":0,\"value\":\"CREDIT\"}],\"BusinessApplication.loginTimeoutModalSecondaryCTAButton\":[{\"type\":0,\"value\":\"Go To Home Page\"}],\"BusinessApplication.modalHeaderText\":[{\"type\":0,\"value\":\"Photo caption and credit (optional)\"}],\"BusinessApplication.timeoutModalHeaderText\":[{\"type\":0,\"value\":\"This session will expire in\"}],\"BusinessApplication.timeoutModalLoginSecondaryText\":[{\"type\":0,\"value\":\"Please log in to your Account to continue with your application.\"}],\"BusinessApplication.timeoutModalPrimaryCTALoginButton\":[{\"type\":0,\"value\":\"Login\"}],\"BusinessApplication.timeoutModalSecondaryCTAButton\":[{\"type\":0,\"value\":\"Save and Exit Application\"}],\"BusinessApplication.timeoutModalSecondaryText\":[{\"type\":0,\"value\":\"You will lose any unsaved changes.\"}],\"BusinessApplication.timeoutModalSessionExipreHeaderText\":[{\"type\":0,\"value\":\"The session has expired.\"}],\"BusinessProfile.EditorialTitleHed\":[{\"type\":0,\"value\":\"Editors' Take\"}],\"BusinessProfile.ExploreTitleHed\":[{\"type\":0,\"value\":\"More from AD\"}],\"BusinessProfile.SectionTitleHed\":[{\"type\":0,\"value\":\"About\"}],\"BusinessProfile.businessProfileImageCreditPrefix\":[{\"type\":0,\"value\":\"Photo by\"}],\"Byline.More\":[{\"type\":0,\"value\":\"more\"}],\"Byline.Preamble\":[{\"type\":0,\"value\":\"By\"}],\"Bylines.AdaptationEditorPreamble\":[{\"type\":0,\"value\":\"Translated and Adapted by\"}],\"Bylines.AnimatorPreamble\":[{\"type\":0,\"value\":\"Animation by\"}],\"Bylines.ArtistPreamble\":[{\"type\":0,\"value\":\"Art by\"}],\"Bylines.ArtworkPreamble\":[{\"type\":0,\"value\":\"Artwork by\"}],\"Bylines.AstoldtoPreamble\":[{\"type\":0,\"value\":\"As told to\"}],\"Bylines.AuthorPreamble\":[{\"type\":0,\"value\":\"By\"}],\"Bylines.CoverShootPreamble\":[{\"type\":0,\"value\":\"Cover Shoot by\"}],\"Bylines.DeveloperPreamble\":[{\"type\":0,\"value\":\"Development by\"}],\"Bylines.DirectorPreamble\":[{\"type\":0,\"value\":\"Directed by\"}],\"Bylines.EditorPreamble\":[{\"type\":0,\"value\":\"Edited by\"}],\"Bylines.FilmByPreamble\":[{\"type\":0,\"value\":\"Film by\"}],\"Bylines.HairPreamble\":[{\"type\":0,\"value\":\"Hair by\"}],\"Bylines.IllustratorPreamble\":[{\"type\":0,\"value\":\"Illustration by\"}],\"Bylines.IntroducerPreamble\":[{\"type\":0,\"value\":\"Introduced by\"}],\"Bylines.MakeupPreamble\":[{\"type\":0,\"value\":\"Makeup by\"}],\"Bylines.MedicalReviewerPreamble\":[{\"type\":0,\"value\":\"Medically reviewed by\"}],\"Bylines.NailsPreamble\":[{\"type\":0,\"value\":\"Nails by\"}],\"Bylines.PhotographerPreamble\":[{\"type\":0,\"value\":\"Photography by\"}],\"Bylines.PodcasthostPreamble\":[{\"type\":0,\"value\":\"With\"}],\"Bylines.ProducerPreamble\":[{\"type\":0,\"value\":\"Produced by\"}],\"Bylines.ReporterPreamble\":[{\"type\":0,\"value\":\"Reporting by\"}],\"Bylines.ReviewerPreamble\":[{\"type\":0,\"value\":\"Reviewed by\"}],\"Bylines.StylistPreamble\":[{\"type\":0,\"value\":\"Styled by\"}],\"Bylines.TextByPreamble\":[{\"type\":0,\"value\":\"Text by\"}],\"Bylines.ToldbyPreamble\":[{\"type\":0,\"value\":\"As told by\"}],\"Bylines.VideoByPreamble\":[{\"type\":0,\"value\":\"Video by\"}],\"Bylines.VideoDirectorPreamble\":[{\"type\":0,\"value\":\"Video Directed by\"}],\"Bylines.WithPreamble\":[{\"type\":0,\"value\":\"With\"}],\"Bylines.Writer\":[{\"type\":0,\"value\":\"Written by\"}],\"Bylines.additionalReportingPreamble\":[{\"type\":0,\"value\":\"Additional Reporting by\"}],\"Bylines.inconversationPreamble\":[{\"type\":0,\"value\":\"In Conversation with\"}],\"Bylines.introductionPreamble\":[{\"type\":0,\"value\":\"Introduction by\"}],\"CNEVideoWatchPage.AboutPremiereDate\":[{\"type\":0,\"value\":\"Released on \"},{\"type\":1,\"value\":\"premiereDate\"}],\"CNEVideoWatchPage.AboutPremiereDateFormat\":[{\"type\":0,\"value\":\"MM\\u002FDD\\u002FYYYY\"}],\"CNEVideoWatchPage.AboutTabLabel\":[{\"type\":0,\"value\":\"About\"}],\"CNEVideoWatchPage.CreditsTabLabel\":[{\"type\":0,\"value\":\"Credits\"}],\"CNEVideoWatchPage.PlaylistHeading\":[{\"type\":0,\"value\":\"Up Next\"}],\"CNEVideoWatchPage.PreviewHeading\":[{\"type\":0,\"value\":\"Preview\"}],\"CNEVideoWatchPage.PreviewLinkCopied\":[{\"type\":0,\"value\":\"copied to clipboard\"}],\"CNEVideoWatchPage.RubricEpisode\":[{\"type\":0,\"value\":\"Episode \"},{\"type\":1,\"value\":\"episode\"}],\"CNEVideoWatchPage.TheaterModeLabel\":[{\"type\":0,\"value\":\"Hide\"}],\"CNEVideoWatchPage.TranscriptHeading\":[{\"type\":0,\"value\":\"Transcript\"}],\"CNEVideoWatchPage.TrendingRecsHeading\":[{\"type\":0,\"value\":\"Trending video\"}],\"CaptionContest.CaptionButton.authenticateSubmitButtonLabel\":[{\"type\":0,\"value\":\"Sign in to submit\"}],\"CaptionContest.CaptionButton.authenticateVoteButtonLabel\":[{\"type\":0,\"value\":\"Sign in to vote\"}],\"CaptionContest.CaptionButton.voteButtonLabel\":[{\"type\":0,\"value\":\"Vote\"}],\"CaptionContest.contestTabLoggedInDisclaimer\":[{\"type\":0,\"value\":\"You must be thirteen or older to enter the contest. Your entry must be received by 11:59 P.M. E.T. on \"},{\"type\":1,\"value\":\"submissionDeadline\"},{\"type\":0,\"value\":\", and must follow the \"},{\"children\":[{\"type\":0,\"value\":\"official caption contest rules\"}],\"type\":8,\"value\":\"a\"},{\"type\":0,\"value\":\".\"},{\"children\":[],\"type\":8,\"value\":\"b\"},{\"type\":0,\"value\":\"You can \"},{\"children\":[{\"type\":0,\"value\":\"update your contact information\"}],\"type\":8,\"value\":\"c\"},{\"type\":0,\"value\":\" anytime.\"}],\"CaptionContest.contestTabLoggedOutDisclaimer\":[{\"type\":0,\"value\":\"You must be thirteen or older to enter the contest. Your entry must be received by 11:59 P.M. E.T. on \"},{\"type\":1,\"value\":\"submissionDeadline\"},{\"type\":0,\"value\":\", and must follow the \"},{\"children\":[{\"type\":0,\"value\":\"official caption contest rules\"}],\"type\":8,\"value\":\"a\"},{\"type\":0,\"value\":\".\"}],\"CaptionContest.fetchErrorMessage\":[{\"type\":0,\"value\":\"There was a problem submitting your caption. Please try again later.\"}],\"CaptionContest.rateApiErrorMessage\":[{\"type\":0,\"value\":\"Captions aren’t available right now. Come back later to rate them.\"}],\"CaptionContest.rateAuthButtonLabel\":[{\"type\":0,\"value\":\"Sign in to rate captions\"}],\"CaptionContest.rateButtonFunnyLabel\":[{\"type\":0,\"value\":\"Funny\"}],\"CaptionContest.rateButtonSomeWhatFunnyLabel\":[{\"type\":0,\"value\":\"Somewhat funny\"}],\"CaptionContest.rateCaptionsCompletedMessage\":[{\"type\":0,\"value\":\"You've rated all of this week's captions!\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"Please go do something (anything) else till next week.\"}],\"CaptionContest.rateLegacyButtonText\":[{\"type\":0,\"value\":\"Start rating submissions\"}],\"CaptionContest.rateLoadingText\":[{\"type\":0,\"value\":\"Loading\"}],\"CaptionContest.rateRecirculationDek\":[{\"type\":0,\"value\":\"Rate submissions »\"}],\"CaptionContest.rateRecirculationHed\":[{\"type\":0,\"value\":\"Help us pick three finalists\"}],\"CaptionContest.rateTabDisclaimer\":[{\"type\":0,\"value\":\"All captions submitted by readers of \"},{\"children\":[{\"type\":0,\"value\":\"brand\"}],\"type\":8,\"value\":\"i\"},{\"type\":0,\"value\":\".\"}],\"CaptionContest.recirculationHed\":[{\"type\":0,\"value\":\"Next\"}],\"CaptionContest.secondPlaceTitle\":[{\"type\":0,\"value\":\"Second Place\"}],\"CaptionContest.submitButtonLabel\":[{\"type\":0,\"value\":\"Submit\"}],\"CaptionContest.submitTabCounterErrorMsg\":[{\"type\":0,\"value\":\"Use maximum of \"},{\"type\":1,\"value\":\"maxCount\"},{\"type\":0,\"value\":\" characters only.\"}],\"CaptionContest.submitTabUserInteractionLabel\":[{\"type\":0,\"value\":\"Your Caption\"}],\"CaptionContest.voteRecirculationDek\":[{\"type\":0,\"value\":\"Vote on Finalists »\"}],\"CaptionContest.voteRecirculationHed\":[{\"type\":0,\"value\":\"Help choose the winner\"}],\"CaptionContest.voteTabRecirculationHed\":[{\"type\":0,\"value\":\"View results for \"},{\"type\":1,\"value\":\"contestTitle\"}],\"CaptionContest.winnerRecirculationDek\":[{\"type\":0,\"value\":\"The Winner »\"}],\"CaptionModal.captionModalButtonLabel\":[{\"type\":0,\"value\":\"Save and submit caption\"}],\"CaptionModal.captionModalDek\":[{\"type\":0,\"value\":\"Add your name, location, and phone number, so we can contact and credit you if you're a finalist.\"}],\"CaptionModal.captionModalDisclaimerText\":[{\"children\":[{\"type\":0,\"value\":\"Need help? Call: 1-800-444-7570 | E-mail: help@newyorker.com\"}],\"type\":8,\"value\":\"p\"},{\"children\":[{\"type\":0,\"value\":\"We’re available Monday through Friday, 9 A.M. to 9 P.M. E.T., and Saturday through Sun, 9 A.M. to 5 P.M. E.T.\"}],\"type\":8,\"value\":\"p\"}],\"CaptionModal.captionModalHed\":[{\"type\":0,\"value\":\"How would you like to be acknowledged?\"}],\"CarouselControls.BackAriaLabel\":[{\"type\":0,\"value\":\"Carousel back\"}],\"CarouselControls.ForwardAriaLabel\":[{\"type\":0,\"value\":\"Carousel forward\"}],\"CartoonContest.contestCartoonCredit\":[{\"type\":0,\"value\":\"Cartoon by \"},{\"type\":1,\"value\":\"credit\"}],\"CartoonContest.fetchErrorMessage\":[{\"type\":0,\"value\":\"There was an issue submitting your vote. Please try again later.\"}],\"CartoonContest.instructionsButtonLabel\":[{\"type\":0,\"value\":\"How does it work?\"}],\"CartoonContest.rateButtonUnFunnyLabel\":[{\"type\":0,\"value\":\"Unfunny\"}],\"CartoonContest.rateTabDek\":[{\"type\":0,\"value\":\"Your responses will help us select three finalists for \"},{\"type\":1,\"value\":\"title\"},{\"type\":0,\"value\":\".\"}],\"CartoonContest.rateTabHed\":[{\"type\":0,\"value\":\"Rate Submissions\"}],\"CartoonContest.stageRate\":[{\"type\":0,\"value\":\"Rate\"}],\"CartoonContest.stageSubmit\":[{\"type\":0,\"value\":\"Submit\"}],\"CartoonContest.stageVote\":[{\"type\":0,\"value\":\"Vote\"}],\"CartoonContest.stageWinner\":[{\"type\":0,\"value\":\"Winner\"}],\"CartoonContest.submitSuccessMessage\":[{\"type\":0,\"value\":\"Thanks for your submission. Check back on \"},{\"type\":1,\"value\":\"finalistDate\"},{\"type\":0,\"value\":\" to see the finalists. If your caption is among them, we’ll be in touch.\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"While you wait, you can rate reader-submitted captions from last week’s contest.\"}],\"CartoonContest.submitTabDek\":[{\"type\":0,\"value\":\"Your caption for \"},{\"type\":1,\"value\":\"title\"},{\"type\":0,\"value\":\" will be rated by readers, like you, in the next round.\"}],\"CartoonContest.submitTabHed\":[{\"type\":0,\"value\":\"Submit Your Caption\"}],\"CartoonContest.thirdPlaceTitle\":[{\"type\":0,\"value\":\"Third Place\"}],\"CartoonContest.voteSuccessMessage\":[{\"type\":0,\"value\":\"Thanks for your vote.\"},{\"type\":0,\"value\":\"\\u003Cbr\\u002F\\u003E\"},{\"type\":0,\"value\":\"The next set of finalists’ captions will appear on \"},{\"type\":1,\"value\":\"finalistDate\"},{\"type\":0,\"value\":\".\"}],\"CartoonContest.voteTabDek\":[{\"type\":0,\"value\":\"Your choice will help determine the winning caption for \"},{\"type\":1,\"value\":\"title\"},{\"type\":0,\"value\":\".\"}],\"CartoonContest.voteTabHed\":[{\"type\":0,\"value\":\"Vote on Finalists\"}],\"CartoonContest.winnerTabDek\":[{\"type\":1,\"value\":\"title\"}],\"CartoonContest.winnerTabHed\":[{\"type\":0,\"value\":\"The Winner\"}],\"Ceros.IframeTitle\":[{\"type\":0,\"value\":\"Ceros embed\"}],\"ChannelFilter.ClearAll\":[{\"type\":0,\"value\":\"Clear All\"}],\"ChannelFilter.ClearAllFiltersText\":[{\"type\":0,\"value\":\"Clear All Filters and Keywords\"}],\"ChannelFilter.FilterPreamble\":[{\"type\":0,\"value\":\"Filter by\"}],\"ChannelFilter.Save\":[{\"type\":0,\"value\":\"Save\"}],\"ChannelFilter.StoryCount\":[{\"type\":0,\"value\":\"Showing \"},{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Story\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Stories\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"storyCount\"}],\"ChannelNavigation.BookmarkAlertLabel\":[{\"type\":0,\"value\":\"Image saved. View saved images in\"}],\"ChannelNavigation.BookmarkAlertMyAccountLabel\":[{\"type\":0,\"value\":\"My Account.\"}],\"ChannelNavigation.ChannelDrawerContentLabel\":[{\"type\":0,\"value\":\"Runway filters navigation\"}],\"ChannelNavigation.GlobalDrawerContentLabel\":[{\"type\":0,\"value\":\"Navigation Menu\"}],\"ChannelNavigation.ToggleLabel\":[{\"type\":0,\"value\":\"Open Navigation Menu\"}],\"Clamp.ReadLess\":[{\"type\":0,\"value\":\"Read less\"}],\"Clamp.ReadMore\":[{\"type\":0,\"value\":\"Read more\"}],\"CneVideoEmbed.Live\":[{\"type\":0,\"value\":\"• Live\"}],\"CneVideoEmbed.PersistantCloseTitle\":[{\"type\":0,\"value\":\"Close Persisted Player\"}],\"CneVideoEmbed.WatchNow\":[{\"type\":0,\"value\":\"Streaming Live Now\"}],\"CollectionsDrawer.bookmarkCountType\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" image\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" images\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"CollectionsDrawer.bookmarkSavedText\":[{\"type\":0,\"value\":\"Image saved\"}],\"CollectionsDrawer.collectionsListHeading\":[{\"type\":0,\"value\":\"Choose A Board\"}],\"CollectionsDrawer.collectionsNewCollectionButtonLabel\":[{\"type\":0,\"value\":\"New Board\"}],\"CollectionsDrawer.createCollectionDuplicateNameError\":[{\"type\":0,\"value\":\"You have already used this name\"}],\"CollectionsDrawer.createCollectionNotMadeError\":[{\"type\":0,\"value\":\"Collection not made, please try again\"}],\"ComingSoon.SeriesNavigation\":[{\"type\":0,\"value\":\"COMING SOON\"}],\"ConnectedNewsletterSubscribeForm.BadResponse\":[{\"type\":0,\"value\":\"Bad response for signup newsletter\"}],\"ConnectedNewsletterSubscribeForm.ErrorMessage\":[{\"type\":0,\"value\":\"Subscription failed:\"}],\"ConnectedNewsletterSubscribeForm.SuccessDek\":[{\"type\":0,\"value\":\"You've successfully subscribed to our newsletter....\"}],\"ConnectedNewsletterSubscribeForm.SuccessHed\":[{\"type\":0,\"value\":\"You're all set...\"}],\"ConnectedNewsletterSubscribeForm.TypeMismatchMessage\":[{\"type\":0,\"value\":\"Invalid email. Double check and try again.\"}],\"ConsentBanner.consentText\":[{\"type\":0,\"value\":\"This content can also be viewed on the site it \"},{\"children\":[{\"type\":0,\"value\":\"originates\"}],\"type\":8,\"value\":\"a\"},{\"type\":0,\"value\":\" from.\"}],\"ConsentBanner.consentWarningText\":[{\"type\":0,\"value\":\"To honor your privacy preferences, this content can only be viewed on the site it \"},{\"children\":[{\"type\":0,\"value\":\"originates\"}],\"type\":8,\"value\":\"a\"},{\"type\":0,\"value\":\" from.\"}],\"ContentCardEmbed.articleButtonCta\":[{\"type\":0,\"value\":\"View Story\"}],\"ContentCardEmbed.recipeButtonCta\":[{\"type\":0,\"value\":\"View Recipe\"}],\"ContentHeader.ReadReviews\":[{\"type\":0,\"value\":\"Read Reviews\"}],\"ContentHeader.ShowAllPhotos\":[{\"type\":0,\"value\":\"Show all Photos\"}],\"ContentPageControlRow.NextPage\":[{\"type\":0,\"value\":\"Next\"}],\"ContentPageControlRow.PreviousPage\":[{\"type\":0,\"value\":\"Previous\"}],\"ContentPromoEmbed.DefaultButtonText\":[{\"type\":0,\"value\":\"Read More\"}],\"ContentPromoEmbed.GalleryButtonText\":[{\"type\":0,\"value\":\"View Slideshow\"}],\"ContentsList.contentsListTitle\":[{\"type\":0,\"value\":\"Table of Contents\"}],\"ContributorHeader.SeeMoreContributorLink\":[{\"type\":0,\"value\":\"See More By\"}],\"ContributorPage.LoadMoreLoadingText\":[{\"type\":0,\"value\":\"Loading ...\"}],\"ContributorPage.LoadMoreText\":[{\"type\":0,\"value\":\"More Stories\"}],\"ContributorPage.featuredStoriesHedText\":[{\"type\":0,\"value\":\"Featured Articles\"}],\"ContributorPage.sectionHedText\":[{\"type\":0,\"value\":\"Archive\"}],\"Contributors.AuthorPreamble\":[{\"type\":0,\"value\":\"Written by \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" Staff\"}],\"CreateCollectionDrawer.createCollectionHeading\":[{\"type\":0,\"value\":\"Create A Board\"}],\"CreateCollectionDrawer.createCollectionInputLabel\":[{\"type\":0,\"value\":\"Board Name\"}],\"CreateCollectionDrawer.createCollectionNoTextError\":[{\"type\":0,\"value\":\"Please enter some text\"}],\"CreateCollectionDrawer.createCollectionPlaceholderText\":[{\"type\":0,\"value\":\"Example: “monochrome” or “fall inspo”\"}],\"CreateCollectionDrawer.createCollectionResetButtonLabel\":[{\"type\":0,\"value\":\"Reset Board Name\"}],\"CreateCollectionDrawer.createCollectionSubmitButtonLabel\":[{\"type\":0,\"value\":\"Create\"}],\"CreateCollectionDrawer.createCollectionValueMissingError\":[{\"type\":0,\"value\":\"Please enter a collection name\"}],\"CrosswordEmbed.SignInMessage\":[{\"type\":0,\"value\":\"To save your progress, sign in to your \"},{\"children\":[{\"type\":1,\"value\":\"portal\"}],\"type\":8,\"value\":\"emTag\"},{\"type\":0,\"value\":\" account.\"}],\"CrosswordEmbed.Title\":[{\"type\":0,\"value\":\"Embedded Crossword\"}],\"CuratedShows.ButtonLabel\":[{\"type\":0,\"value\":\"View all shows\"}],\"CuratedShows.DrawerContentLabel\":[{\"type\":0,\"value\":\"Runway All Shows navigation\"}],\"CuratedShows.GroupedNavigationBrowserFilterLabel\":[{\"type\":0,\"value\":\"Search...\"}],\"CuratedShows.GroupedNavigationFilterLabel\":[{\"type\":0,\"value\":\"Search for a designer...\"}],\"CuratedShows.GroupedNavigationSummaryCarouselFilterLabel\":[{\"type\":0,\"value\":\"Search...\"}],\"CustomMenuButtons.Apply\":[{\"type\":0,\"value\":\"APPLY\"}],\"CustomMenuButtons.ClearAll\":[{\"type\":0,\"value\":\"Clear all\"}],\"Disclaimer.Text\":[{\"type\":0,\"value\":\"All products featured on \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" are independently selected by our editors. However, when you buy something through our retail links, we may earn an affiliate commission.\"}],\"Drawer.ButtonLabel\":[{\"type\":0,\"value\":\"Close drawer\"}],\"Drawer.ContentLabel\":[{\"type\":0,\"value\":\"Secondary menu navigation\"}],\"DynamicChannelNav.PrimaryLinks\":[{\"type\":0,\"value\":\"Primary\"}],\"ErrorBoundary.ErrorMessage\":[{\"type\":0,\"value\":\"An error occurred.\"}],\"ErrorContent.buttonLabel\":[{\"type\":0,\"value\":\"Go to Homepage\"}],\"ErrorContent.buttonLink\":[{\"type\":0,\"value\":\"\\u002F\"}],\"ErrorContent.dangerousDek\":[{\"type\":0,\"value\":\"There was an issue with this page\"}],\"ErrorContent.dangerousHed\":[{\"type\":0,\"value\":\"Oops\"}],\"ErrorPage.buttonLabel\":[{\"type\":0,\"value\":\"Go to Homepage\"}],\"ErrorPage.buttonLink\":[{\"type\":0,\"value\":\"\\u002F\"}],\"ErrorPage.dangerousDek\":[{\"type\":0,\"value\":\"There was an issue with this page\"}],\"ErrorPage.dangerousHed\":[{\"type\":0,\"value\":\"Oops\"}],\"ErrorPage.statusCodePreamble\":[{\"type\":0,\"value\":\"Status Code\"}],\"EventBanner.CloseBanner\":[{\"type\":0,\"value\":\"Close Banner\"}],\"EventBanner.LiveOn\":[{\"type\":0,\"value\":\"Live on\"}],\"EventBanner.SponsorPreamble\":[{\"type\":0,\"value\":\"Countdown Presented By\"}],\"EventBanner.WatchLiveOn\":[{\"type\":0,\"value\":\"Watch live on\"}],\"EventBanner.eventDays\":[{\"type\":0,\"value\":\"Days\"}],\"EventBanner.eventHours\":[{\"type\":0,\"value\":\"Hours\"}],\"EventBanner.eventMinutes\":[{\"type\":0,\"value\":\"Minutes\"}],\"EventBanner.eventSeconds\":[{\"type\":0,\"value\":\"Seconds\"}],\"EventsList.Title\":[{\"type\":0,\"value\":\"Featured Events\"}],\"ExternalLinkEmbed.Rubric\":[{\"type\":0,\"value\":\"Read More\"}],\"FacebookEmbed.AriaLabelText\":[{\"type\":0,\"value\":\"social media post\"}],\"FacebookEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Facebook content\"}],\"FeaturedContributor.ViewMore\":[{\"type\":0,\"value\":\"View more\"}],\"FeaturedContributorAllFiction.ViewMore\":[{\"type\":0,\"value\":\"View more\"}],\"FeaturedStories.HedText\":[{\"type\":0,\"value\":\"Featured Articles By \"},{\"type\":1,\"value\":\"contributorName\"}],\"Filmstrip.CollapsedMessage\":[{\"type\":0,\"value\":\"Explore\"}],\"Filmstrip.expandedMessage\":[{\"type\":0,\"value\":\"Hide\"}],\"Filter.ShowAllTags\":[{\"type\":0,\"value\":\"SHOW ALL\"}],\"Filter.ShowLess\":[{\"type\":0,\"value\":\"HIDE FILTERS\"}],\"Filter.ShowLessTags\":[{\"type\":0,\"value\":\"SHOW LESS\"}],\"Filter.ShowMore\":[{\"type\":0,\"value\":\"SHOW FILTERS\"}],\"FilterComponent.ContentLoadingLabel\":[{\"type\":0,\"value\":\"Updating\"}],\"FilterComponent.FilterApplyActionButton\":[{\"type\":0,\"value\":\"Apply\"}],\"FilterComponent.FilterBy\":[{\"type\":0,\"value\":\"Filter by\"}],\"FilterComponent.FilterCancelActionButton\":[{\"type\":0,\"value\":\"Cancel\"}],\"FilterComponent.FilterDeselectActionButton\":[{\"type\":0,\"value\":\"Unselect all\"}],\"FilterComponent.FilterMenuCloseButton\":[{\"type\":0,\"value\":\"Close Filter Menu\"}],\"FilterComponent.Items\":[{\"type\":0,\"value\":\"Items\"}],\"FilterComponent.NoResultDek\":[{\"type\":0,\"value\":\"Sorry, we can't display any results for those filtering options, please try again.\"}],\"FilterComponent.NoResultHed\":[{\"type\":0,\"value\":\"No Result\"}],\"FilterComponent.ShowItems\":[{\"type\":0,\"value\":\"Show \"},{\"type\":1,\"value\":\"totalItems\"},{\"type\":0,\"value\":\" Results\"}],\"FilterComponent.SortBy\":[{\"type\":0,\"value\":\"Sort by\"}],\"FilterComponent.TotalCount\":[{\"type\":0,\"value\":\"Results\"}],\"FilterComponent.reviewTags\":[{\"type\":1,\"value\":\"reviewTag\"}],\"FilterableSummaryList.AtArticleGalleryCarouselBtnText\":[{\"type\":0,\"value\":\"VIEW ALL \"},{\"type\":1,\"value\":\"categoryName\"}],\"FilterableSummaryList.AtArticleGalleryCarouselBtnTextWithCtaLink\":[{\"type\":1,\"value\":\"categoryName\"}],\"FireworkEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Firework content\"}],\"FormWithValidation.BadInput\":[{\"type\":0,\"value\":\"Bad input\"}],\"FormWithValidation.CustomError\":[{\"type\":0,\"value\":\"Custom error\"}],\"FormWithValidation.InvalidValueMessage\":[{\"type\":1,\"value\":\"field\"},{\"type\":0,\"value\":\" is invalid.\"}],\"FormWithValidation.PatternMismatch\":[{\"type\":0,\"value\":\"Pattern mismatch\"}],\"FormWithValidation.RangeOverflow\":[{\"type\":0,\"value\":\"Range overflow\"}],\"FormWithValidation.RangeUnderflow\":[{\"type\":0,\"value\":\"Range underflow\"}],\"FormWithValidation.StepMismatch\":[{\"type\":0,\"value\":\"Step mismatch\"}],\"FormWithValidation.TooLong\":[{\"type\":0,\"value\":\"Too long\"}],\"FormWithValidation.TooShort\":[{\"type\":0,\"value\":\"Too short\"}],\"FormWithValidation.TypeMismatch\":[{\"type\":0,\"value\":\"Type mismatch\"}],\"FormWithValidation.ValueMissing\":[{\"type\":0,\"value\":\"This field cannot be empty\"}],\"GalleryCarousel.Next\":[{\"type\":0,\"value\":\"Next\"}],\"GalleryCarousel.NextGallery\":[{\"type\":0,\"value\":\"Next gallery\"}],\"GalleryCarousel.Previous\":[{\"type\":0,\"value\":\"Previous\"}],\"GalleryEmbedControls.AdSlideText\":[{\"type\":0,\"value\":\"Advertisement\"}],\"GalleryEmbedControls.BackArrowButtonAriaLabel\":[{\"type\":0,\"value\":\"gallery-back\"}],\"GalleryEmbedControls.ForwardArrowButtonAriaLabel\":[{\"type\":0,\"value\":\"gallery-forward\"}],\"GalleryPage.SignInCalloutLinkText\":[{\"type\":0,\"value\":\"Sign in or create an account to vote\"}],\"GalleryRecircCards.ViewGalleryCTAText\":[{\"type\":0,\"value\":\"View gallery »\"}],\"GalleryRecircCards.keepOnLaughingText\":[{\"type\":0,\"value\":\"Keep on laughing\"}],\"GalleryRecircCards.midGalleryRecircHeading\":[{\"type\":0,\"value\":\"Want more laughs? Try another cartoon gallery.\"}],\"GalleryRecircCards.viewNextGalleryCTAText\":[{\"type\":0,\"value\":\"View next gallery »\"}],\"GallerySlide.ArticleCta\":[{\"type\":0,\"value\":\"View Story\"}],\"GallerySlide.DefaultCTAText\":[{\"type\":0,\"value\":\"Book Now\"}],\"GallerySlide.ProductCta\":[{\"type\":0,\"value\":\"Shop Now\"}],\"GallerySlide.RecipeCta\":[{\"type\":0,\"value\":\"View Recipe\"}],\"GallerySlide.ReviewCta\":[{\"type\":0,\"value\":\"Read More\"}],\"GallerySlide.VenueCta\":[{\"type\":0,\"value\":\"Book Now\"}],\"GallerySlide.VenueSellerPreviewText\":[{\"type\":0,\"value\":\"Powered By:\"}],\"GalleryVoting.galleryVotingDangerousDek\":[{\"type\":0,\"value\":\"You can also save stories and manage newsletter preferences\"}],\"GalleryVoting.galleryVotingDangerousHed\":[{\"type\":0,\"value\":\"To vote, sign in or\"}],\"GalleryVoting.galleryVotingDangerousHedSpanTag\":[{\"type\":0,\"value\":\"create an account\"}],\"GoogleSignInButton.Label\":[{\"type\":0,\"value\":\"Sign in with Google\"}],\"GroupedNavigation.FilterInputAriaLabel\":[{\"type\":0,\"value\":\"Filter links\"}],\"GroupedNavigationHasBrowser.FilterInputAriaLabel\":[{\"type\":0,\"value\":\"Filter links\"}],\"GroupedNavigationHasSummaryCarousel.FilterInputAriaLabel\":[{\"type\":0,\"value\":\"Filter links\"}],\"Icons.Account\":[{\"type\":0,\"value\":\"Account\"}],\"Icons.AgeGate\":[{\"type\":0,\"value\":\"Age Gate\"}],\"Icons.Arrow\":[{\"type\":0,\"value\":\"Arrow\"}],\"Icons.Article\":[{\"type\":0,\"value\":\"Article\"}],\"Icons.Bookmark\":[{\"type\":0,\"value\":\"Save Story\"}],\"Icons.BookmarkActivated\":[{\"type\":0,\"value\":\"Story Saved\"}],\"Icons.Cart\":[{\"type\":0,\"value\":\"Cart\"}],\"Icons.Check\":[{\"type\":0,\"value\":\"Check\"}],\"Icons.Chevron\":[{\"type\":0,\"value\":\"Chevron\"}],\"Icons.Close\":[{\"type\":0,\"value\":\"Close\"}],\"Icons.Collapse\":[{\"type\":0,\"value\":\"Collapse\"}],\"Icons.Comment\":[{\"type\":0,\"value\":\"Comment\"}],\"Icons.CopyLink\":[{\"type\":0,\"value\":\"CopyLink\"}],\"Icons.Dots\":[{\"type\":0,\"value\":\"Dots\"}],\"Icons.DownloadCloud\":[{\"type\":0,\"value\":\"DownloadCloud\"}],\"Icons.DownloadWeb\":[{\"type\":0,\"value\":\"DownloadWeb\"}],\"Icons.Email\":[{\"type\":0,\"value\":\"Email\"}],\"Icons.Expand\":[{\"type\":0,\"value\":\"Expand\"}],\"Icons.Facebook\":[{\"type\":0,\"value\":\"Facebook\"}],\"Icons.Filmstrip\":[{\"type\":0,\"value\":\"Filmstrip\"}],\"Icons.Filter\":[{\"type\":0,\"value\":\"Filter\"}],\"Icons.Flipboard\":[{\"type\":0,\"value\":\"Flipboard\"}],\"Icons.Gallery\":[{\"type\":0,\"value\":\"Gallery\"}],\"Icons.GoogleNews\":[{\"type\":0,\"value\":\"Google News\"}],\"Icons.Grid\":[{\"type\":0,\"value\":\"Grid\"}],\"Icons.Headphone\":[{\"type\":0,\"value\":\"Headphone\"}],\"Icons.Information\":[{\"type\":0,\"value\":\"Information\"}],\"Icons.Instagram\":[{\"type\":0,\"value\":\"Instagram\"}],\"Icons.LargeChevron\":[{\"type\":0,\"value\":\"LargeChevron\"}],\"Icons.Like\":[{\"type\":0,\"value\":\"Like\"}],\"Icons.LikeFilled\":[{\"type\":0,\"value\":\"LikeFilled\"}],\"Icons.Line\":[{\"type\":0,\"value\":\"Line\"}],\"Icons.LinkedIn\":[{\"type\":0,\"value\":\"LinkedIn\"}],\"Icons.List\":[{\"type\":0,\"value\":\"List\"}],\"Icons.Loader\":[{\"type\":0,\"value\":\"Loader\"}],\"Icons.Maximize\":[{\"type\":0,\"value\":\"Maximize\"}],\"Icons.Menu\":[{\"type\":0,\"value\":\"Menu\"}],\"Icons.NativeShare\":[{\"type\":0,\"value\":\"Native Share\"}],\"Icons.Newsletter\":[{\"type\":0,\"value\":\"Newsletter\"}],\"Icons.Ok\":[{\"type\":0,\"value\":\"Odnoklassniki\"}],\"Icons.Pause\":[{\"type\":0,\"value\":\"Pause\"}],\"Icons.PhotoStack\":[{\"type\":0,\"value\":\"PhotoStack\"}],\"Icons.Pinterest\":[{\"type\":0,\"value\":\"Pinterest\"}],\"Icons.Play\":[{\"type\":0,\"value\":\"Play\"}],\"Icons.PlayCNE\":[{\"type\":0,\"value\":\"PlayCNE\"}],\"Icons.PlayOutlined\":[{\"type\":0,\"value\":\"PlayOutlined\"}],\"Icons.Playlist\":[{\"type\":0,\"value\":\"Playlist\"}],\"Icons.Print\":[{\"type\":0,\"value\":\"Print\"}],\"Icons.RatingFilled\":[{\"type\":0,\"value\":\"RatingFilled\"}],\"Icons.RatingHalf\":[{\"type\":0,\"value\":\"RatingHalf\"}],\"Icons.RatingOutlined\":[{\"type\":0,\"value\":\"RatingOutlined\"}],\"Icons.Replay\":[{\"type\":0,\"value\":\"Replay\"}],\"Icons.Rss\":[{\"type\":0,\"value\":\"Rss\"}],\"Icons.Search\":[{\"type\":0,\"value\":\"Search\"}],\"Icons.Share\":[{\"type\":0,\"value\":\"Share\"}],\"Icons.Shopping\":[{\"type\":0,\"value\":\"Shopping\"}],\"Icons.Snapchat\":[{\"type\":0,\"value\":\"Snapchat\"}],\"Icons.SocialHandle\":[{\"type\":0,\"value\":\"SocialHandle\"}],\"Icons.Spotify\":[{\"type\":0,\"value\":\"Spotify\"}],\"Icons.Star\":[{\"type\":0,\"value\":\"Star\"}],\"Icons.Telegram\":[{\"type\":0,\"value\":\"Telegram\"}],\"Icons.Threads\":[{\"type\":0,\"value\":\"Threads\"}],\"Icons.Tiktok\":[{\"type\":0,\"value\":\"Tiktok\"}],\"Icons.Timestamp\":[{\"type\":0,\"value\":\"Timestamp\"}],\"Icons.Triangle\":[{\"type\":0,\"value\":\"Triangle\"}],\"Icons.TriangleDown\":[{\"type\":0,\"value\":\"TriangleDown\"}],\"Icons.TriangleUp\":[{\"type\":0,\"value\":\"TriangleUp\"}],\"Icons.Tumblr\":[{\"type\":0,\"value\":\"Tumblr\"}],\"Icons.Twitter\":[{\"type\":0,\"value\":\"X\"}],\"Icons.Vero\":[{\"type\":0,\"value\":\"VERO\"}],\"Icons.Viber\":[{\"type\":0,\"value\":\"Rakuten Viber\"}],\"Icons.Video\":[{\"type\":0,\"value\":\"Video\"}],\"Icons.Vk\":[{\"type\":0,\"value\":\"VKonkakte\"}],\"Icons.WeChat\":[{\"type\":0,\"value\":\"WeChat\"}],\"Icons.WebLink\":[{\"type\":0,\"value\":\"Website Link\"}],\"Icons.Weibo\":[{\"type\":0,\"value\":\"Sina Weibo\"}],\"Icons.Whatsapp\":[{\"type\":0,\"value\":\"Whatsapp\"}],\"Icons.Xing\":[{\"type\":0,\"value\":\"Xing\"}],\"Icons.YandexZen\":[{\"type\":0,\"value\":\"Yandex.Zen\"}],\"Icons.YouTube\":[{\"type\":0,\"value\":\"YouTube\"}],\"Icons.chevronFill\":[{\"type\":0,\"value\":\"chevronFill\"}],\"Icons.funny\":[{\"type\":0,\"value\":\"Funny\"}],\"Icons.someWhatFunny\":[{\"type\":0,\"value\":\"Somewhat funny\"}],\"Icons.unFunny\":[{\"type\":0,\"value\":\"Unfunny\"}],\"Icons.wavyarrow\":[{\"type\":0,\"value\":\"Wavy Arrow\"}],\"IframeEmbed.AriaLabel\":[{\"type\":0,\"value\":\"Click button to go to: \"},{\"type\":1,\"value\":\"name\"}],\"IframeEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Content\"}],\"IframeEmbed.DangerousDek\":[{\"type\":0,\"value\":\"Listen to this story\"}],\"IframeEmbed.Title\":[{\"type\":0,\"value\":\"Embedded Frame\"}],\"ImageSlideShow.galleryLink\":[{\"type\":0,\"value\":\"See the gallery\"}],\"ImageSlideShow.lastSlideCTA\":[{\"type\":0,\"value\":\"Explore the gallery\"}],\"IngredientList.hedText\":[{\"type\":0,\"value\":\"Ingredients\"}],\"IngredientList.nutritionHedText\":[{\"type\":0,\"value\":\"Nutrition Per Serving\"}],\"InstagramEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Instagram content\"}],\"InstructionList.StepText\":[{\"type\":0,\"value\":\"Step\"}],\"ItemCount.ItemTypeCharacter\":[{\"options\":{\"other\":{\"value\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" character\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" characters\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}]},\"withMinCountLimit\":{\"value\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" character\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" characters\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"},{\"type\":0,\"value\":\" (at least \"},{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" character\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" characters\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"minCount\"},{\"type\":0,\"value\":\" are required)\"}]}},\"type\":5,\"value\":\"messageType\"}],\"ItemCount.ItemTypeDefault\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Item\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Items\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"ItemCount.ItemTypeImage\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Image\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Images\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"ItemCount.ItemTypePhoto\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Photo\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Photos\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"ItemCount.ItemTypeSlide\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Slide\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Slides\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"ItemCount.ItemTypeVenue\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Venue\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Venues\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"JustWatchEmbed.PoweredBy\":[{\"type\":0,\"value\":\"Powered by\"}],\"Lightbox.CloseButtonIconLabel\":[{\"type\":0,\"value\":\"Close Lightbox\"}],\"Lightbox.ContentLabel\":[{\"type\":0,\"value\":\"Photo Gallery\"}],\"LiveStory.LoadMoreButtonLabel\":[{\"type\":0,\"value\":\"Load More\"}],\"LiveStory.LoadMoreLoadingLabel\":[{\"type\":0,\"value\":\"Loading ...\"}],\"LiveStory.feedADayAgoLabel\":[{\"type\":0,\"value\":\"a day ago\"}],\"LiveStory.feedAMinAgoLabel\":[{\"type\":0,\"value\":\"a minute ago\"}],\"LiveStory.feedAMonthAgoLabel\":[{\"type\":0,\"value\":\"a month ago\"}],\"LiveStory.feedAYearAgoLabel\":[{\"type\":0,\"value\":\"a year ago\"}],\"LiveStory.feedAnHourAgoLabel\":[{\"type\":0,\"value\":\"an hour ago\"}],\"LiveStory.feedFewSecondsAgoLabel\":[{\"type\":0,\"value\":\"a few seconds ago\"}],\"LiveStory.lastUpdatedTimePrefixLabel\":[{\"type\":0,\"value\":\"Updated\"}],\"MagazineBundlePage.magazineHeading\":[{\"type\":0,\"value\":\"The Magazine\"}],\"MagazineBundlePage.nextButton\":[{\"type\":0,\"value\":\"Next\"}],\"MagazineBundlePage.previousButton\":[{\"type\":0,\"value\":\"Previous\"}],\"MagazineDisclaimer.DisclaimerNoHed\":[{\"type\":0,\"value\":\"Published in the print edition of the \"},{\"type\":1,\"value\":\"issueDate\"},{\"type\":0,\"value\":\", issue.\"}],\"MagazineDisclaimer.DisclaimerWithHed\":[{\"type\":0,\"value\":\"Published in the print edition of the \"},{\"type\":1,\"value\":\"issueDate\"},{\"type\":0,\"value\":\", issue, with the headline “\"},{\"type\":1,\"value\":\"hed\"},{\"type\":0,\"value\":\".”\"}],\"MapPreview.MapAriaLabel\":[{\"type\":0,\"value\":\"Location map of address\"}],\"MapPreview.MapPreviewHeader\":[{\"type\":0,\"value\":\"Location Map\"}],\"MastodonEmbed.AriaLabelText\":[{\"type\":0,\"value\":\"social media post\"}],\"MastodonEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Mastodon content\"}],\"MegaMenu.All\":[{\"type\":0,\"value\":\"All\"}],\"MegaMenu.MegaMenuButton\":[{\"type\":0,\"value\":\"Close Mega Menu\"}],\"MegaMenu.NavigationPrimaryAriaLabel\":[{\"type\":0,\"value\":\"Primary\"}],\"MegaMenu.SignInLinkText\":[{\"type\":0,\"value\":\"Sign in\"}],\"MegaMenu.VerboseClickOut\":[{\"type\":0,\"value\":\"More\"}],\"MenuList.unavailableFilters\":[{\"type\":0,\"value\":\"Unavailable filters\"}],\"MessageBanner.saveStory\":[{\"type\":0,\"value\":\"Save story\"}],\"MobileProductCard.CardAvailability\":[{\"type\":0,\"value\":\"Multiple Buying Options Available\"}],\"MobileProductCard.CardRating\":[{\"type\":0,\"value\":\"Rating: \"},{\"type\":1,\"value\":\"rating\"},{\"type\":0,\"value\":\"\\u002F10\"}],\"MobileProductCard.CtaText\":[{\"type\":0,\"value\":\"Buy Now\"}],\"Multipackages.ExploreInstead\":[{\"type\":0,\"value\":\"Explore these instead\"}],\"Multipackages.NoStories\":[{\"type\":0,\"value\":\"No stories found for your search\"}],\"NativeDisclaimer.Text\":[{\"type\":0,\"value\":\"This article was published by The New Yorker Brand Studio for \"},{\"type\":1,\"value\":\"sponsorName\"},{\"type\":0,\"value\":\". The editorial staff of The New Yorker had no role in this post's preparation.\"}],\"NativeShareButton.ButtonTitle\":[{\"type\":0,\"value\":\"Share\"}],\"Navigation.OneTrustButtonLabel\":[{\"type\":0,\"value\":\"Do Not Sell My Personal Info\"}],\"NewsFeed.LoadMoreNewsPreamble\":[{\"type\":0,\"value\":\"Show More News\"}],\"NewsLetterManagePageNewsLetterSubscribeForm.TypeMismatchMessage\":[{\"type\":0,\"value\":\"Invalid email. Double check and try again.\"}],\"NewsletterManageContent.SecondaryOptinsDangerousLegend\":[{\"type\":0,\"value\":\"Primary and Third Party Optins\"}],\"NewsletterManageContent.SignUp\":[{\"type\":0,\"value\":\"Sign up\"}],\"NewsletterManageContent.SignUpMessage\":[{\"type\":0,\"value\":\"You’re signed up.\"}],\"NewsletterManagePage.BadResponseServerMessage\":[{\"type\":0,\"value\":\"Subscription failed: Bad response\"}],\"NewsletterManagePage.EmptyNewsletterErrorMessage\":[{\"type\":0,\"value\":\"Subscription failed: Please select a newsletter\"}],\"NewsletterManagePage.UnknownErrorMessage\":[{\"type\":0,\"value\":\"Subscription failed: Unknown error occurred\"}],\"NewsletterManagePage.closeSubmitModelTextField\":[{\"type\":0,\"value\":\"Go back\"}],\"NewsletterManagePage.completeSignUp\":[{\"type\":0,\"value\":\"Complete Sign Up\"}],\"NewsletterManagePage.goBackFromPreviewButton\":[{\"type\":0,\"value\":\"Go back\"}],\"NewsletterManagePage.previewText\":[{\"type\":0,\"value\":\"Preview\"}],\"NewsletterManagePage.readPreviewButton\":[{\"type\":0,\"value\":\"Read a preview\"}],\"NewsletterManagePage.signedUpPopUpMessage\":[{\"type\":0,\"value\":\"You’re signed up to this newsletter.\"}],\"NewsletterManagePage.ukPrimaryMarketingPermission\":[{\"type\":0,\"value\":\"Sign up to receive emails from \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" about \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\"’s products, promotions, and services.\"}],\"NewsletterManagePage.ukThirdPartyMarketingPermission\":[{\"type\":0,\"value\":\"Sign up to receive marketing emails from \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" about special offers and promotions for other Condé Nast brands and our marketing partners.\"}],\"NewsletterManagePage.viewMoreNewsletters\":[{\"type\":0,\"value\":\"View more newsletters\"}],\"NewsletterOneClickForm.BadResponse\":[{\"type\":0,\"value\":\"Bad response for signup newsletter\"}],\"NewsletterOneClickForm.ErrorMessage\":[{\"type\":0,\"value\":\"Subscription failed\"}],\"NewsletterOneClickForm.TypeMismatchMessage\":[{\"type\":0,\"value\":\"Invalid email. Double check and try again.\"}],\"NewsletterSlice.DismissButton\":[{\"type\":0,\"value\":\"Dismiss Newsletter Slice\"}],\"NewsletterSlice.NewsletterSecondaryOptinsLegend\":[{\"type\":0,\"value\":\"Consent checks\"}],\"NowReading.SeriesNavigation\":[{\"type\":0,\"value\":\"Now Reading\"}],\"OverlayNavigation.OverlayNavigationButton\":[{\"type\":0,\"value\":\"Close Navigation Menu\"}],\"OverlayNavigation.OverlayNavigationPrimaryLinks\":[{\"type\":0,\"value\":\"Primary\"}],\"OverlayNavigation.OverlayNavigationSearchLabel\":[{\"type\":0,\"value\":\"Search\"}],\"OverlayNavigation.OverlayNavigationSecondaryLinks\":[{\"type\":0,\"value\":\"Secondary\"}],\"OverlayNavigation.OverlayNavigationSignInLabel\":[{\"type\":0,\"value\":\"Sign In\"}],\"OverlayNavigation.OverlayNavigationUtilityLinks\":[{\"type\":0,\"value\":\"Utility\"}],\"OverlayNavigation.OverlayNavigationWrapper\":[{\"type\":0,\"value\":\"Overlay Navigation\"}],\"PLPProductCard.AtRetailerNameComponentText\":[{\"type\":0,\"value\":\"Shop at \"},{\"type\":1,\"value\":\"sellerNameText\"}],\"PLPProductCard.AtRetailerNameLabel\":[{\"type\":0,\"value\":\"$ \"},{\"type\":1,\"value\":\"finalPriceLabel\"},{\"type\":0,\"value\":\" At \"},{\"type\":1,\"value\":\"sellerNameText\"}],\"PLPProductCard.BuyAt\":[{\"type\":0,\"value\":\"Buy At \"},{\"type\":1,\"value\":\"sellerNameText\"}],\"PLPProductCard.ShopNowComponentText\":[{\"type\":0,\"value\":\"Shop Now\"}],\"PaginationModal.NextPage\":[{\"type\":0,\"value\":\"Next\"}],\"PaginationModal.PreviousPage\":[{\"type\":0,\"value\":\"Previous\"}],\"PaginationRow.NextPage\":[{\"type\":0,\"value\":\"Next Page\"}],\"PhotoBookmark.GalleryDesktopMessageBannerText\":[{\"type\":0,\"value\":\"Click the icon to save an image to your account.\"}],\"PhotoBookmark.GalleryMobileMessageBannerText\":[{\"type\":0,\"value\":\"Tap the icon to save an image to your account.\"}],\"PhotoBookmark.RunwayDesktopMessageBannerText\":[{\"type\":0,\"value\":\"Hover over an image and click the icon to save it to your account.\"}],\"PhotoBookmark.RunwayMobileMessageBannerText\":[{\"type\":0,\"value\":\"Tap the icon to save an image to your account.\"}],\"PhotoBookmark.SaveAlertSavedToBoardMessage\":[{\"type\":0,\"value\":\"Go To Boards\"}],\"PhotoBookmark.SaveAlertWithBoardName\":[{\"type\":0,\"value\":\"Saved to\"}],\"PhotoBookmark.SaveBookmarkAlertLink\":[{\"type\":0,\"value\":\"Create a Board\"}],\"PhotoBookmark.SaveBookmarkAlertText\":[{\"type\":0,\"value\":\"Image saved\"}],\"PhotoBookmark.SaveIconTitle\":[{\"type\":0,\"value\":\"Save Image\"}],\"PhotoBookmark.SavedIconTitle\":[{\"type\":0,\"value\":\"Image saved\"}],\"PhotoBookmark.SignInHed\":[{\"type\":0,\"value\":\"Save images\"}],\"PhotoBookmark.SignInHedSpanTag\":[{\"type\":0,\"value\":\"to your Vogue account\"}],\"PhotoBookmark.SignInMessage\":[{\"type\":0,\"value\":\"Sign in to save runway and street style images, then easily revisit them on any device.\"}],\"PhotoVogueHomePage.bestOfPvText\":[{\"type\":0,\"value\":\"Best of Photovogue\"}],\"PhotoVogueHomePage.featured\":[{\"type\":0,\"value\":\"featured\"}],\"PhotoVogueHomePage.introductionText\":[{\"type\":0,\"value\":\"Connecting artists, community and commerce through Condé Nast's global creative networks, we champion talent and influence visual literacy to help foster a more just, ethical and inclusive world.\"}],\"PhotoVogueHomePage.nationalitiesAuthorNameText\":[{\"type\":0,\"value\":\"Countries represented\"}],\"PhotoVogueHomePage.nationalitiesRubricNameText\":[{\"type\":0,\"value\":\"Nationalities\"}],\"PhotoVogueHomePage.news\":[{\"type\":0,\"value\":\"news\"}],\"PhotoVogueHomePage.photoCountText\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Photo\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Photos\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"photoCount\"}],\"PhotoVogueHomePage.photoVogue\":[{\"type\":0,\"value\":\"PhotoVogue\"}],\"PhotoVogueHomePage.photograpersAuthorNameText\":[{\"type\":0,\"value\":\"Participating photographers\"}],\"PhotoVogueHomePage.photographersRubricNameText\":[{\"type\":0,\"value\":\"Photographers\"}],\"PhotoVogueHomePage.photosAuthorNameText\":[{\"type\":0,\"value\":\"Photos selected by the editors\"}],\"PhotoVogueHomePage.photosRubricNameText\":[{\"type\":0,\"value\":\"Photos\"}],\"PhotoVogueHomePage.picOfTheDay\":[{\"type\":0,\"value\":\"PIC OF THE DAY\"}],\"PhotoVogueHomePage.seeAllBestOf\":[{\"type\":0,\"value\":\"See All Best Of\"}],\"PhotoVogueHomePage.seeAllFeatured\":[{\"type\":0,\"value\":\"SEE ALL featured\"}],\"PhotoVogueHomePage.seeAllNews\":[{\"type\":0,\"value\":\"See All News\"}],\"PhotoVogueHomePage.seeAllPhotographers\":[{\"type\":0,\"value\":\"See All Photographers\"}],\"PhotoVogueHomePage.seeAllPicOfTheDay\":[{\"type\":0,\"value\":\"SEE ALL PICS OF THE DAY\"}],\"PhotoVogueHomePage.spotlightPhotographers\":[{\"type\":0,\"value\":\"Spotlight Photographers\"}],\"PhotoVogueHomePage.spotlightRubric\":[{\"type\":0,\"value\":\"Spotlight\"}],\"PhotoVogueHomePage.tickerText\":[{\"type\":0,\"value\":\"We’re excited to open up contributions to video, illustration and digital art in the near future\"}],\"PhotoVoguePhotographersFilter.placeholderText\":[{\"type\":0,\"value\":\"Search by first or last name\"}],\"PhotoVoguePhotographersFilter.searchButtonText\":[{\"type\":0,\"value\":\"Search\"}],\"PhotoVoguePhotographersPage.PhotosCount\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" photo\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" photos\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"PhotoVoguePhotographersPage.clearAllText\":[{\"type\":0,\"value\":\"Clear All\"}],\"PhotoVoguePhotographersPage.country\":[{\"type\":0,\"value\":\"Country\"}],\"PhotoVoguePhotographersPage.errorMessage\":[{\"type\":0,\"value\":\"Sorry, something went wrong. Please refresh the page and try again.\"}],\"PhotoVoguePhotographersPage.genre\":[{\"type\":0,\"value\":\"Genre\"}],\"PhotoVoguePhotographersPage.isSpotlight\":[{\"type\":0,\"value\":\"Spotlight\"}],\"PhotoVoguePhotographersPage.loadMore\":[{\"type\":0,\"value\":\"See More\"}],\"PhotoVoguePhotographersPage.loading\":[{\"type\":0,\"value\":\"Loading…\"}],\"PhotoVoguePhotographersPage.orText\":[{\"type\":0,\"value\":\"or\"}],\"PhotoVoguePhotographersPage.photographerLabel\":[{\"type\":0,\"value\":\"photographer\"}],\"PhotoVoguePhotographersPage.photographersLabel\":[{\"type\":0,\"value\":\"photographers\"}],\"PhotoVoguePhotographersPage.spotlight\":[{\"type\":0,\"value\":\"Spotlight\"}],\"PhotoVoguePhotosPage.Filter\":[{\"type\":0,\"value\":\"Filter\"}],\"PhotoVoguePhotosPage.countPrefixText\":[{\"type\":0,\"value\":\"Showing\"}],\"PhotoVoguePhotosPage.errorMessage\":[{\"type\":0,\"value\":\"Sorry, something went wrong. Please refresh the page and try again.\"}],\"PhotoVoguePhotosPage.loadMore\":[{\"type\":0,\"value\":\"Load More\"}],\"PhotoVoguePhotosPage.loading\":[{\"type\":0,\"value\":\"Loading…\"}],\"PhotoVoguePhotosPage.photoLabel\":[{\"type\":0,\"value\":\"photo\"}],\"PhotoVoguePhotosPage.photosLabel\":[{\"type\":0,\"value\":\"photos\"}],\"PhotovogueArtistProfilePage.ShowLessText\":[{\"type\":0,\"value\":\"Show less\"}],\"PhotovogueArtistProfilePage.ShowMoreText\":[{\"type\":0,\"value\":\"Show more\"}],\"PhotovogueArtistProfilePage.allPhotos\":[{\"type\":0,\"value\":\"All photos:\"}],\"PhotovogueArtistProfilePage.bestOfPhotoVogue\":[{\"type\":0,\"value\":\"Best of PhotoVogue:\"}],\"PhotovogueArtistProfilePage.genre\":[{\"type\":0,\"value\":\"Genre:\"}],\"PhotovogueArtistProfilePage.picOfTheDayVogueText\":[{\"type\":0,\"value\":\"Pic of the Day:\"}],\"PhotovogueArtistProfilePage.portfolio\":[{\"type\":0,\"value\":\"Portfolio\"}],\"PhotovogueArtistProfilePage.showMoreText\":[{\"type\":0,\"value\":\"Show more\"}],\"PhotovogueArtistProfilePage.spotlightTitle\":[{\"type\":0,\"value\":\"Spotlight\"}],\"PinterestEmbed.AriaLabelText\":[{\"type\":0,\"value\":\"social media post\"}],\"PinterestEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Pinterest content\"}],\"PodcastDetailedPage.contextualHeader\":[{\"type\":0,\"value\":\"You Might Like This\"}],\"PodcastDetailedPage.primaryCTALabel\":[{\"type\":0,\"value\":\"Start Listening Now\"}],\"PodcastDetailedPage.relatedArticleHed\":[{\"type\":0,\"value\":\"Related Articles\"}],\"ProductCard.CardRating\":[{\"type\":0,\"value\":\"Rating: \"},{\"type\":1,\"value\":\"rating\"},{\"type\":0,\"value\":\"\\u002F10\"}],\"ProductEmbed.DefaultCtaText\":[{\"type\":0,\"value\":\"Buy It\"}],\"ProductEmbed.DefaultTextPreamble\":[{\"type\":0,\"value\":\"Learn More\"}],\"ProductEmbed.PriceWithNoSellerPreamble\":[{\"type\":0,\"value\":\"Buy for \"},{\"type\":1,\"value\":\"price\"}],\"ProductEmbed.PriceWithSellerPreamble\":[{\"type\":1,\"value\":\"price\"},{\"type\":0,\"value\":\" at \"},{\"type\":1,\"value\":\"sellerName\"}],\"ProductEmbed.VenueCtaText\":[{\"type\":0,\"value\":\"Book Now\"}],\"ProductOffer.defaultPriceString\":[{\"type\":1,\"value\":\"priceValue\"},{\"type\":0,\"value\":\" \"},{\"type\":1,\"value\":\"sellerName\"}],\"ProductOffer.price\":[{\"type\":1,\"value\":\"priceValue\"},{\"type\":0,\"value\":\" at \"},{\"type\":1,\"value\":\"sellerName\"}],\"ProductOffer.productOffersaveBookmarkLabel\":[{\"type\":0,\"value\":\"Save story\"}],\"ProductOffer.variationDefaultCTA\":[{\"type\":0,\"value\":\"Shop Now\"}],\"ProductOffer.variationSellerNameString\":[{\"type\":0,\"value\":\"Shop at\"}],\"ProsCons.ConsLabel\":[{\"type\":0,\"value\":\"Cons →\"}],\"ProsCons.ProsLabel\":[{\"type\":0,\"value\":\"Pros →\"}],\"Rating.RatingAriaLabel\":[{\"type\":0,\"value\":\"Rating\"}],\"RatingsForm.LoadingText\":[{\"type\":0,\"value\":\"Loading...\"}],\"RatingsForm.PreviousRatingText\":[{\"type\":0,\"value\":\"You previously rated \"},{\"type\":1,\"value\":\"RATING_SUBJECT\"},{\"type\":0,\"value\":\".\"}],\"RatingsForm.PromptText\":[{\"type\":0,\"value\":\"How would you rate \"},{\"type\":1,\"value\":\"RATING_SUBJECT\"},{\"type\":0,\"value\":\"?\"}],\"RatingsForm.SuccessText\":[{\"type\":0,\"value\":\"Thanks for your feedback!\"}],\"ReadMore.SeriesRecirc\":[{\"type\":0,\"value\":\"Read more\"}],\"ReadNext.SeriesRecirc\":[{\"type\":0,\"value\":\"Read next\"}],\"RecipePage.ActiveTime\":[{\"type\":0,\"value\":\"Active Time\"}],\"RecipePage.CommunityGuidelines\":[{\"type\":0,\"value\":\"Community Guidelines\"}],\"RecipePage.CooksNote\":[{\"type\":0,\"value\":\"Cooks' Note\"}],\"RecipePage.LikeActionErrorMessage\":[{\"type\":0,\"value\":\"Unable to like this note. Please try again.\"}],\"RecipePage.MessageBannerContentHed\":[{\"type\":1,\"value\":\"messageContentHed\"}],\"RecipePage.MessageBannerContentTrail\":[{\"type\":1,\"value\":\"messageContentTrail\"}],\"RecipePage.MessageBannerSubContent\":[{\"type\":1,\"value\":\"messageSubContent\"}],\"RecipePage.MessageBannerTitle\":[{\"type\":1,\"value\":\"title\"}],\"RecipePage.PrepTime\":[{\"type\":0,\"value\":\"Prep Time\"}],\"RecipePage.TotalTime\":[{\"type\":0,\"value\":\"Total Time\"}],\"RecipePage.UnlikeActionErrorMessage\":[{\"type\":0,\"value\":\"Unable to unlike this note. Please try again.\"}],\"RecipePage.Yield\":[{\"type\":0,\"value\":\"Yield\"}],\"RecipePage.info\":[{\"type\":0,\"value\":\"Recipe information\"}],\"RecipeProductCarousel.Title\":[{\"type\":0,\"value\":\"What you’ll need\"}],\"RecircList.ReadMore\":[{\"type\":0,\"value\":\"Read More\"}],\"RecircMostPopular.SectionTitle\":[{\"type\":0,\"value\":\"Most Popular\"}],\"RedditEmbed.AriaLabelText\":[{\"type\":0,\"value\":\"social media post\"}],\"RedditEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Reddit content\"}],\"RelatedList.SectionTitle\":[{\"type\":0,\"value\":\"Related Stories\"}],\"RelatedVideo.HeaderText\":[{\"type\":0,\"value\":\"Featured Video\"}],\"ResponsiveCartoon.cartButtonMessage\":[{\"type\":0,\"value\":\"Shop\"}],\"ResponsiveCartoon.copiedLinkAlertMessage\":[{\"type\":0,\"value\":\"Link copied\"}],\"ResponsiveCartoon.copyLinkButtonMessage\":[{\"type\":0,\"value\":\"Copy link to cartoon\"}],\"ResponsiveCartoon.downloadButtonMessage\":[{\"type\":0,\"value\":\"Download\"}],\"ResponsiveCartoon.openCartoonGalleryButtonIconMessage\":[{\"type\":0,\"value\":\"Open Gallery\"}],\"ResponsiveCartoon.openCartoonGalleryButtonMessage\":[{\"type\":0,\"value\":\"Open cartoon gallery\"}],\"ResponsiveClip.ClipAriaLabel\":[{\"type\":0,\"value\":\"Play\\u002FPause\"}],\"ResponsiveClip.ClipLabel\":[{\"type\":0,\"value\":\"Play\\u002FPause Button\"}],\"ReviewForm.AlertMessageError\":[{\"type\":0,\"value\":\"Your feedback wasn't posted due to some error, please try again.\"}],\"ReviewForm.AlertMessageSuccess\":[{\"type\":0,\"value\":\"Thanks for your feedback!\"}],\"ReviewForm.FakeInputPlaceholderText\":[{\"type\":0,\"value\":\"Tell us what you think\"}],\"ReviewForm.Hed\":[{\"type\":0,\"value\":\"Leave a Review\"}],\"ReviewForm.InvalidFieldErrorMessage\":[{\"type\":0,\"value\":\"Required fields missing\"}],\"ReviewForm.IsAnonymousCheckboxLabel\":[{\"type\":0,\"value\":\"Share anonymously\"}],\"ReviewForm.LocationFieldLabel\":[{\"type\":0,\"value\":\"Where are you from?\"}],\"ReviewForm.LocationFieldPlaceholder\":[{\"type\":0,\"value\":\"Boston, MA\"}],\"ReviewForm.ReviewTextFieldLabel\":[{\"type\":0,\"value\":\"Your Review\"}],\"ReviewForm.ReviewTextFieldPlaceholder\":[{\"type\":0,\"value\":\"Let us know your thoughts…\"}],\"ReviewForm.ReviewerInfoFieldLabel\":[{\"type\":0,\"value\":\"Display Name\"}],\"ReviewForm.ReviewerInfoFieldPlaceholder\":[{\"type\":0,\"value\":\"Jane Doe\"}],\"ReviewForm.SubmitButtonLabel\":[{\"type\":0,\"value\":\"Submit\"}],\"ReviewForm.SubmitButtonLabelLoading\":[{\"type\":0,\"value\":\"Loading…\"}],\"ReviewForm.WillPrepareAgainOption1Label\":[{\"type\":0,\"value\":\"Yes\"}],\"ReviewForm.WillPrepareAgainOption2Label\":[{\"type\":0,\"value\":\"No\"}],\"ReviewForm.WillPrepareAgainRadioLabel\":[{\"type\":0,\"value\":\"Would you make this recipe again?\"}],\"ReviewForm.nonLoggedInErrorMessage\":[{\"type\":0,\"value\":\"Sign in or create an account to add note.\"}],\"ReviewList.Loading\":[{\"type\":0,\"value\":\"Loading…\"}],\"ReviewList.ReviewListError\":[{\"type\":0,\"value\":\"Sorry, more reviews can‘t be loaded right now. \"},{\"type\":1,\"value\":\"br\"},{\"type\":0,\"value\":\" Please try again later.\"}],\"ReviewList.ReviewReplyLabel\":[{\"type\":0,\"value\":\"Reply\"}],\"ReviewListContainer.UtilityLabel\":[{\"type\":0,\"value\":\"Back to Top\"}],\"ReviewNoteModal.CloseButtonAriaLabel\":[{\"type\":0,\"value\":\"Close ReviewNoteModal Modal\"}],\"ReviewNoteModal.CloseButtonLabel\":[{\"type\":0,\"value\":\"close modal button label\"}],\"ReviewNoteModal.Hed\":[{\"type\":1,\"value\":\"hed\"}],\"ReviewNoteModal.continueLabel\":[{\"type\":1,\"value\":\"continueLabel\"}],\"ReviewNoteModal.dek\":[{\"type\":1,\"value\":\"dek\"}],\"ReviewNoteModal.discardLabel\":[{\"type\":1,\"value\":\"discardLabel\"}],\"ReviewNotes.AddNoteFailedToastMessage\":[{\"type\":0,\"value\":\"Unable to add your note. Please try again.\"}],\"ReviewNotesForm.ReviewerInfoFieldLabel\":[{\"type\":0,\"value\":\"Adding Note As:\"}],\"ReviewNotesForm.ReviewerRatingLabel\":[{\"type\":1,\"value\":\"reviewerRatingLabel\"}],\"ReviewNotesForm.addNoteLabel\":[{\"type\":0,\"value\":\"Add Note\"}],\"ReviewNotesForm.buttonLabel\":[{\"type\":0,\"value\":\"Sign in or create account\"}],\"ReviewNotesForm.cancelNoteLabel\":[{\"type\":0,\"value\":\"Discard\"}],\"ReviewNotesForm.defaultcommunityReviewText\":[{\"type\":0,\"value\":\"Ask a question or leave a helpful tip, suggestion or opinion that is relevant and respectful for the community.\"}],\"ReviewNotesForm.invalidReviewNoteLength\":[{\"type\":1,\"value\":\"message\"}],\"ReviewNotesForm.maxCharLimitMet\":[{\"type\":0,\"value\":\"_MAX_ character limit met\"}],\"ReviewNotesForm.messageBannerText\":[{\"type\":0,\"value\":\"Join the home cook community and add recipe notes.\"}],\"ReviewNotesForm.remainingMaxCharLimit\":[{\"type\":0,\"value\":\"_COUNT_ of _MAX_ character limit remaining\"}],\"ReviewNotesForm.requiredField\":[{\"type\":1,\"value\":\"message\"}],\"ReviewNotesForm.reviewTagsLabel\":[{\"type\":0,\"value\":\"TAG YOUR NOTE (OPTIONAL)\"}],\"ReviewNotesForm.reviewerFieldInfoIconText\":[{\"type\":0,\"value\":\"Your username appears next to your recipe notes and replies. Change it anytime in My Account.\"}],\"ReviewNotesForm.reviewerInfoIconButtonLabel\":[{\"type\":0,\"value\":\"user name update message\"}],\"ReviewNotesForm.textFieldLabel\":[{\"type\":0,\"value\":\"Your Review\"}],\"ReviewRatingData.ButtonLabel\":[{\"type\":0,\"value\":\"Open rating explainer\"}],\"ReviewRatingData.DataLabel\":[{\"type\":0,\"value\":\"Rating:\"}],\"ReviewReplyComment.HideRepliesLabel\":[{\"type\":0,\"value\":\"Hide replies\"}],\"ReviewReplyComment.LoadingRepliesLabel\":[{\"type\":0,\"value\":\"Loading…\"}],\"ReviewReplyComment.ReviewReplyCommentLabel\":[{\"type\":0,\"value\":\"Reply\"}],\"ReviewReplyComment.ReviewReplyLabel\":[{\"type\":0,\"value\":\"Replying to\"}],\"ReviewReplyComment.ShowMoreRepliesLabel\":[{\"type\":0,\"value\":\"Show more replies\"}],\"ReviewReplyNote.AddReplyFailedToastMessage\":[{\"type\":0,\"value\":\"Unable to add your reply. Please try again.\"}],\"ReviewReplyNote.AddReplySuccessToastMessage\":[{\"type\":0,\"value\":\"Reply added\"}],\"ReviewReplyNote.CancelButtonLabel\":[{\"type\":0,\"value\":\"Discard\"}],\"ReviewReplyNote.ReplyButtonLabel\":[{\"type\":0,\"value\":\"Reply\"}],\"ReviewReplyNote.ReplyFieldPlaceHolder\":[{\"type\":0,\"value\":\"Add your reply here\"}],\"ReviewReplyNote.ReplyTextFieldLabel\":[{\"type\":0,\"value\":\"Your Reply\"}],\"ReviewReplyNote.ReviewFieldAlertLimitErrorText\":[{\"type\":0,\"value\":\"_CHARACTER_LIMIT_CURRENT_ of _CHARACTER_LIMIT_ character limit remaining.\"}],\"ReviewReplyNote.ReviewFieldMaxLimitErrorText\":[{\"type\":0,\"value\":\"_CHARACTER_LIMIT_ character limit met.\"}],\"ReviewReplyNote.ReviewFieldMinLimitErrorText\":[{\"type\":0,\"value\":\"Enter 2 characters or more to add a reply.\"}],\"ReviewReplyNote.ReviewReplyLabel\":[{\"type\":0,\"value\":\"Replying To:\"}],\"ReviewSummary.SummaryLabel\":[{\"type\":0,\"value\":\"TL;DR:\"}],\"RunwayGalleryFilmstrip.FilmstripCollapsedMessage\":[{\"type\":0,\"value\":\"Explore Collection\"}],\"RunwayGalleryFilmstrip.FilmstripCollapsedMessageForNonCollectionGalleries\":[{\"type\":0,\"value\":\"Explore Gallery\"}],\"RunwayGalleryFilmstrip.FilmstripExpandedMessage\":[{\"type\":0,\"value\":\"Hide Collection\"}],\"RunwayGalleryFilmstrip.FilmstripExpandedMessageForNonCollectionGalleries\":[{\"type\":0,\"value\":\"Hide Gallery\"}],\"RunwayGalleryLookNumber.imageLookNumberPrefix\":[{\"type\":0,\"value\":\"Look\"}],\"RunwayGalleryPage.SaveBookmarkAlertLink\":[{\"type\":0,\"value\":\"VIEW ALL\"}],\"RunwayGalleryPage.SaveBookmarkAlertText\":[{\"type\":0,\"value\":\"Image saved\"}],\"ScoreBox.BestNewMusic\":[{\"type\":0,\"value\":\"Best New Music\"}],\"ScoreBox.BestNewReissue\":[{\"type\":0,\"value\":\"Best New Reissue\"}],\"ScoreBox.BestNewTrack\":[{\"type\":0,\"value\":\"Best New Track\"}],\"SearchBar.placeholder\":[{\"type\":0,\"value\":\"Search for \\\"stir-fry\\\"\"}],\"SearchPage.ArtistTitle\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":0,\"value\":\"Artist\"}]},\"other\":{\"value\":[{\"type\":0,\"value\":\"Artists\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"SearchPage.AuthorTitle\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":0,\"value\":\"Author\"}]},\"other\":{\"value\":[{\"type\":0,\"value\":\"Authors\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"SearchPage.ClearAll\":[{\"type\":0,\"value\":\"Clear All\"}],\"SearchPage.FeatureTitle\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":0,\"value\":\"Feature\"}]},\"other\":{\"value\":[{\"type\":0,\"value\":\"Features\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"SearchPage.FilterResults\":[{\"type\":0,\"value\":\"Filter Results\"}],\"SearchPage.LoadMoreButtonLabel\":[{\"type\":0,\"value\":\"More Stories\"}],\"SearchPage.LoadMoreLoadingLabel\":[{\"type\":0,\"value\":\"Loading ...\"}],\"SearchPage.Loading\":[{\"type\":0,\"value\":\"Loading ...\"}],\"SearchPage.MoreStories\":[{\"type\":0,\"value\":\"More Stories\"}],\"SearchPage.NewsTitle\":[{\"type\":0,\"value\":\"News\"}],\"SearchPage.NoResultsFound\":[{\"type\":0,\"value\":\"Sorry we can't display any results for those filtering options, please try again\"}],\"SearchPage.NoResultsHed\":[{\"type\":0,\"value\":\"No stories found for your search\"}],\"SearchPage.ResultsHed\":[{\"type\":0,\"value\":\"Search stories from \"},{\"type\":1,\"value\":\"brandName\"}],\"SearchPage.ResultsHedOnIssueDate\":[{\"type\":0,\"value\":\"Search Results from \"},{\"type\":1,\"value\":\"issueDate\"},{\"type\":0,\"value\":\" issue\"}],\"SearchPage.ReviewTitle\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":0,\"value\":\"Review\"}]},\"other\":{\"value\":[{\"type\":0,\"value\":\"Reviews\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"SearchPage.SearchButtonLabel\":[{\"type\":0,\"value\":\"Search\"}],\"SearchPage.SearchInputAriaLabel\":[{\"type\":0,\"value\":\"search\"}],\"SearchPage.SearchInputPlaceholder\":[{\"type\":0,\"value\":\"Try \\\"Racial justice\\\"\"}],\"SearchPage.SortBy\":[{\"type\":0,\"value\":\"Sort By\"}],\"SearchPage.SortLabel\":[{\"type\":0,\"value\":\"Sort by\"}],\"SearchPage.ThePitchTitle\":[{\"type\":0,\"value\":\"The Pitch\"}],\"SearchPage.TrackTitle\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":0,\"value\":\"Track\"}]},\"other\":{\"value\":[{\"type\":0,\"value\":\"Tracks\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"SearchPage.VideoTitle\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":0,\"value\":\"Video\"}]},\"other\":{\"value\":[{\"type\":0,\"value\":\"Videos\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"count\"}],\"SearchPage.noResultsContentHed\":[{\"type\":0,\"value\":\"We didn't find any recipes, articles or videos for\"}],\"SearchPage.noResultsCustomContentHed\":[{\"type\":0,\"value\":\"We didn't find any results for\"}],\"SearchPage.noResultsSubHed\":[{\"type\":0,\"value\":\"We didn't find any\"}],\"SearchPage.resultswithWordHed\":[{\"type\":0,\"value\":\"Search results for\"}],\"SearchPage.showAllArtists\":[{\"type\":0,\"value\":\"SHOW ALL ARTISTS\"}],\"SearchPage.showAllAuthors\":[{\"type\":0,\"value\":\"SHOW ALL AUTHORS\"}],\"SearchResultsIndicator.EmptyResultText\":[{\"type\":0,\"value\":\"explore these instead\"}],\"SearchResultsIndicator.ResultsTextWithTerm\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" story\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":1,\"value\":\"moreResultsIndicator\"},{\"type\":0,\"value\":\" stories\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"resultCount\"},{\"type\":0,\"value\":\" about \\\"\"},{\"type\":1,\"value\":\"searchTerm\"},{\"type\":0,\"value\":\"\\\"\"}],\"SearchResultsIndicator.ResultsTextWithoutTerm\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" story\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":1,\"value\":\"moreResultsIndicator\"},{\"type\":0,\"value\":\" results\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"resultCount\"},{\"type\":0,\"value\":\" from \"},{\"type\":1,\"value\":\"brandName\"}],\"SearchResultsIndicator.ResultsWithPagination\":[{\"type\":1,\"value\":\"pageStartIndex\"},{\"type\":0,\"value\":\" - \"},{\"type\":1,\"value\":\"pageEndIndex\"},{\"type\":0,\"value\":\" of \"},{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Result\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":1,\"value\":\"moreResultsIndicator\"},{\"type\":0,\"value\":\" Results\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"resultCount\"}],\"SearchResultsIndicator.resultsList\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Result\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":1,\"value\":\"moreResultsIndicator\"},{\"type\":0,\"value\":\" Results\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"resultCount\"}],\"SearchResultsIndicator.resultsListWithEntity\":[{\"type\":1,\"value\":\"resultCount\"},{\"type\":1,\"value\":\"moreResultsIndicator\"},{\"type\":0,\"value\":\" \"},{\"type\":1,\"value\":\"entity\"}],\"SearchResultsIndicator.resultsListWithLocation\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" Result\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":1,\"value\":\"moreResultsIndicator\"},{\"type\":0,\"value\":\" Results\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"resultCount\"},{\"type\":0,\"value\":\" in \"},{\"type\":1,\"value\":\"location\"}],\"SearchResultsIndicator.resultsListWithLocationAndEntity\":[{\"type\":1,\"value\":\"resultCount\"},{\"type\":1,\"value\":\"moreResultsIndicator\"},{\"type\":0,\"value\":\" \"},{\"type\":1,\"value\":\"entity\"},{\"type\":0,\"value\":\" in \"},{\"type\":1,\"value\":\"location\"}],\"SearchResultsListPage.DekTextActivity\":[{\"type\":0,\"value\":\"The best activities in \"},{\"type\":1,\"value\":\"locationName\"},{\"type\":0,\"value\":\", as reviewed by our editors and contributors.\"}],\"SearchResultsListPage.DekTextHotel\":[{\"type\":0,\"value\":\"The best hotels in \"},{\"type\":1,\"value\":\"locationName\"},{\"type\":0,\"value\":\", as reviewed by our editors and contributors. We've stayed at some of the finest properties around the world, and these made the top of our list.\"}],\"SearchResultsListPage.HeaderText\":[{\"type\":1,\"value\":\"pageHed\"},{\"type\":0,\"value\":\" \"},{\"options\":{\"activity\":{\"value\":[{\"type\":0,\"value\":\"activities\"}]},\"hotel\":{\"value\":[{\"type\":0,\"value\":\"hotels\"}]},\"other\":{\"value\":[]}},\"type\":5,\"value\":\"contentType\"}],\"SearchResultsListPage.Loading\":[{\"type\":0,\"value\":\"Loading ...\"}],\"SearchResultsListPage.MoreResults\":[{\"type\":0,\"value\":\"Load More\"}],\"SearchResultsListPage.NoResults\":[{\"type\":0,\"value\":\"No results. Please try again.\"}],\"SearchResultsListPage.ResultsWithoutAnyFilter\":[{\"type\":0,\"value\":\"We currently do not have any results that match your criteria. Here are design professionals who recently joined the Directory.\"}],\"SearchResultsListPage.ResultsWithoutProfession\":[{\"type\":0,\"value\":\"We do not have any results that match your exact criteria. Here are other \"},{\"children\":[{\"type\":1,\"value\":\"selectedCategory\"}],\"type\":8,\"value\":\"b\"},{\"type\":0,\"value\":\" professionals in \"},{\"children\":[{\"type\":1,\"value\":\"selectedState\"}],\"type\":8,\"value\":\"b\"},{\"type\":0,\"value\":\".\"}],\"SearchResultsListPage.ResultsWithoutProfessionAndALLState\":[{\"type\":0,\"value\":\"We do not have any results that match your exact criteria. Here are other \"},{\"children\":[{\"type\":1,\"value\":\"selectedCategory\"}],\"type\":8,\"value\":\"b\"},{\"type\":0,\"value\":\" professionals.\"}],\"SearchResultsListPage.ResultsWithoutState\":[{\"type\":0,\"value\":\"We do not have any results that match your exact criteria. Here are \"},{\"children\":[{\"type\":1,\"value\":\"selectedCategory\"}],\"type\":8,\"value\":\"b\"},{\"type\":0,\"value\":\" professionals in other locations.\"}],\"SearchResultsListPage.ResultsWithoutStateProfession\":[{\"type\":0,\"value\":\"We do not have any results that match your exact criteria. Here are \"},{\"children\":[{\"type\":1,\"value\":\"selectedCategory\"}],\"type\":8,\"value\":\"b\"},{\"type\":0,\"value\":\" professionals in other locations.\"}],\"SearchResultsListPage.SearchButtonLabel\":[{\"type\":0,\"value\":\"Search\"}],\"SearchResultsListPage.searchResultsListPageHedText\":[{\"type\":0,\"value\":\"Browse our trusted list of \"},{\"type\":1,\"value\":\"brand\"},{\"type\":0,\"value\":\"-approved designers\"}],\"SearchableSummaryCollection.AsyncDropdownPlaceholder\":[{\"type\":0,\"value\":\"Search by city or destination\"}],\"SearchableSummaryCollection.ClickoutButtonLabel\":[{\"type\":0,\"value\":\"View all \"},{\"type\":1,\"value\":\"location\"},{\"type\":0,\"value\":\" hotels\"}],\"SearchableSummaryCollection.NoMatchesFoundLabel\":[{\"type\":0,\"value\":\"No matches found\"}],\"SearchableSummaryCollection.NoResultsMessage\":[{\"type\":0,\"value\":\"Sorry, there are no results for your search - please try another location\"}],\"SearchableSummaryCollection.SearchContainerHed\":[{\"type\":0,\"value\":\"Where do you want to go?\"}],\"SearchableSummaryCollection.SubmitButtonLabel\":[{\"type\":0,\"value\":\"Search\"}],\"SecondaryMenu.NavDropdownAssistiveLabel\":[{\"type\":0,\"value\":\"Select international site\"}],\"SecondaryMenu.NavDropdownHeader\":[{\"type\":0,\"value\":\"Explore \"},{\"type\":1,\"value\":\"rootBrandName\"},{\"type\":0,\"value\":\" across the globe\"}],\"SecondaryMenu.NavigationPrimaryAriaLabel\":[{\"type\":0,\"value\":\"Primary\"}],\"SecondaryMenu.SearchLinkText\":[{\"type\":0,\"value\":\"Search\"}],\"SecondaryMenu.SecondaryLinksAriaLabel\":[{\"type\":0,\"value\":\"Secondary\"}],\"SecondaryMenu.SignInLinkText\":[{\"type\":0,\"value\":\"Sign in\"}],\"SecondaryMenu.UtilityLinksAriaLabel\":[{\"type\":0,\"value\":\"Utility\"}],\"ShopifyCart.CartHeader\":[{\"type\":0,\"value\":\"Shopping Cart\"}],\"ShopifyCartEmpty.EmptyCartButtonLabel\":[{\"type\":0,\"value\":\"GO SHOPPING\"}],\"ShopifyCartEmpty.EmptyCartButtonLink\":[{\"type\":0,\"value\":\"\\u002Fshop\\u002Flisting\\u002Fall\"}],\"ShopifyCartEmpty.EmptyCartHeader\":[{\"type\":0,\"value\":\"YOUR SHOPPING CART IS EMPTY\"}],\"ShopifyCartItem.CheckoutLabel\":[{\"type\":0,\"value\":\"CHECK OUT\"}],\"ShopifyCartItem.CheckoutText\":[{\"type\":0,\"value\":\"Shipping and taxes calculated at checkout\"}],\"ShopifyCartItem.RetailerLabel\":[{\"type\":0,\"value\":\"Retailer:\"}],\"ShopifyCartItem.SubtotalLabel\":[{\"type\":0,\"value\":\"Subtotal\"}],\"ShopifyProductDetail.addToCartLabel\":[{\"type\":0,\"value\":\"Add To Cart\"}],\"ShopifyProductDetail.quantityLabel\":[{\"type\":0,\"value\":\"Quantity\"}],\"ShoppableAssetEmbed.shoppingIconHoverText\":[{\"type\":0,\"value\":\"Shop the look\"}],\"SignInModal.CloseButtonAriaLabel\":[{\"type\":0,\"value\":\"Close Sign In Modal\"}],\"SignInModal.CloseButtonLabel\":[{\"type\":0,\"value\":\"Close Sign In Modal\"}],\"SignInModal.Hed\":[{\"type\":0,\"value\":\"Save stories\"}],\"SignInModal.HedSpanTag\":[{\"type\":0,\"value\":\"with an account\"}],\"SignInModal.OidcSignInLinkText\":[{\"type\":0,\"value\":\"Sign in\"}],\"SignInModal.OidcSignInText\":[{\"type\":0,\"value\":\"Already have an account?\"}],\"SignInModal.OidcSignUpButtonLabel\":[{\"type\":0,\"value\":\"Create account\"}],\"SignOutButton.SignOut\":[{\"type\":0,\"value\":\"Sign Out\"}],\"SimpleNavigation.SearchLabel\":[{\"type\":0,\"value\":\"Search\"}],\"SiteFooter.Dropdown\":[{\"type\":0,\"value\":\"Select international site\"}],\"SiteFooter.NoticesContainer\":[{\"type\":0,\"value\":\"Notices\"}],\"SiteFooter.OneTrustButtonLabel\":[{\"type\":0,\"value\":\"Do Not Sell My Personal Info\"}],\"SiteHeader.ScrollingNavigation\":[{\"type\":0,\"value\":\"Primary\"}],\"SiteHeader.UtilityNavigation\":[{\"type\":0,\"value\":\"Utility\"}],\"SmallProductCard.AtRetailerNameComponentText\":[{\"type\":0,\"value\":\"At \"},{\"type\":1,\"value\":\"retailerNameText\"}],\"SmallProductCard.AtRetailerNameLabel\":[{\"type\":1,\"value\":\"finalPriceLabel\"},{\"type\":0,\"value\":\" At \"},{\"type\":1,\"value\":\"retailerNameText\"}],\"SmallProductCard.BuyAt\":[{\"type\":0,\"value\":\"Buy At \"},{\"type\":1,\"value\":\"retailerNameText\"}],\"SplitScreenContentHeader.RatingLinkLabel\":[{\"type\":0,\"value\":\"Read Reviews\"}],\"SplitScreenContentHeader.VariousArtists\":[{\"type\":0,\"value\":\"Various Artists\"}],\"SponsoredContentHeader.BylineAdvertisementFeatureWith\":[{\"type\":0,\"value\":\"Advertisement Feature With\"}],\"SponsoredContentHeader.BylineAdvertising\":[{\"type\":0,\"value\":\"Advertising\"}],\"SponsoredContentHeader.BylineBrandPresentsAdvertiser\":[{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" Presents\"}],\"SponsoredContentHeader.BylineBrandXAdvertiser\":[{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" X\"}],\"SponsoredContentHeader.BylineBrandedContent\":[{\"type\":0,\"value\":\"Branded Content By\"}],\"SponsoredContentHeader.BylineCreated\":[{\"type\":0,\"value\":\"Created By \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" For\"}],\"SponsoredContentHeader.BylineInCollaboration\":[{\"type\":0,\"value\":\"In Collaboration With\"}],\"SponsoredContentHeader.BylineInPartnership\":[{\"type\":0,\"value\":\"In Partnership With\"}],\"SponsoredContentHeader.BylineOriginalContentBy\":[{\"type\":0,\"value\":\"Original Content By\"}],\"SponsoredContentHeader.BylinePR\":[{\"type\":0,\"value\":\"PR\"}],\"SponsoredContentHeader.BylinePaidPost\":[{\"type\":0,\"value\":\"PAID POST\"}],\"SponsoredContentHeader.BylinePaidPostByAdvertiser\":[{\"type\":0,\"value\":\"Paid Post by \"},{\"type\":1,\"value\":\"sponsorName\"},{\"type\":0,\"value\":\", Brought to you By Business Reporter\"}],\"SponsoredContentHeader.BylinePresentedByAdvertiser\":[{\"type\":0,\"value\":\"Presented By\"}],\"SponsoredContentHeader.BylineProduced\":[{\"type\":0,\"value\":\"Produced By\"}],\"SponsoredContentHeader.BylineProducedByAdvertiser\":[{\"type\":0,\"value\":\"Produced By\"}],\"SponsoredContentHeader.BylineProducedByBrand\":[{\"type\":0,\"value\":\"Produced By \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" With\"}],\"SponsoredContentHeader.BylinePromotedByAdvertiser\":[{\"type\":0,\"value\":\"Promoted by\"}],\"SponsoredContentHeader.BylinePromotion\":[{\"type\":0,\"value\":\"Promotion\"}],\"SponsoredContentHeader.BylinePublishedByAdvertiser\":[{\"type\":0,\"value\":\"Published By\"}],\"SponsoredContentHeader.BylineSponsored\":[{\"type\":0,\"value\":\"Sponsored content\"}],\"SponsoredContentHeader.BylineSponsoredBy\":[{\"type\":0,\"value\":\"Sponsored By\"}],\"SponsoredContentHeader.BylineSponsoredContent\":[{\"type\":0,\"value\":\"Sponsored Content By\"}],\"SponsoredContentHeader.BylineTogetherWith\":[{\"type\":0,\"value\":\"Together with\"}],\"SponsoredContentHeader.SponsoredLinkCTA\":[{\"type\":0,\"value\":\"Click to go to \"},{\"type\":1,\"value\":\"sponsorName\"},{\"type\":0,\"value\":\"'s website\"}],\"SponsoredContentHeader.bylineAd\":[{\"type\":0,\"value\":\"Ad\"}],\"SponsoredContentHeader.bylineAdvertisementByAdvertiser\":[{\"type\":0,\"value\":\"Advertisement By\"}],\"SponsoredContentHeader.bylineAffiliatePartner\":[{\"type\":0,\"value\":\"Affiliate Partner\"}],\"SponsoredContentHeader.bylineInPartnershipWithAdvertiser\":[{\"type\":0,\"value\":\"In Partnership With\"}],\"SponsoredContentHeader.bylinePaidPartnershipWithAdvertiser\":[{\"type\":0,\"value\":\"Paid Partnership With\"}],\"SponsoredContentHeader.bylinePaidPromotionByAdvertiser\":[{\"type\":0,\"value\":\"Paid Promotion By\"}],\"SponsoredContentHeader.bylineSpecialFeature\":[{\"type\":0,\"value\":\"Special Feature\"}],\"SponsoredContentHeader.bylineSponsoredByAdvertiser\":[{\"type\":0,\"value\":\"Sponsored By\"}],\"StackedNavigation.BookmarkAlertLabel\":[{\"type\":0,\"value\":\"Find anything you save across the site in your account\"}],\"StackedNavigation.DrawerLabel\":[{\"type\":0,\"value\":\"Navigation and Sign Up Menu\"}],\"StackedNavigation.MenuButton\":[{\"type\":0,\"value\":\"Open Navigation Menu\"}],\"StackedNavigation.OpenSearchMenuLabel\":[{\"type\":0,\"value\":\"Open Search Menu\"}],\"StackedNavigation.PrimaryLinksLabel\":[{\"type\":0,\"value\":\"Primary\"}],\"StackedNavigation.ProfileLinkLabel\":[{\"type\":0,\"value\":\"My Profile\"}],\"StackedNavigation.SavedStoriesLabel\":[],\"StackedNavigation.SearchLabel\":[{\"type\":0,\"value\":\"Search\"}],\"StackedNavigation.SubscribeLabel\":[{\"type\":0,\"value\":\"Subscribe\"}],\"StackedNavigation.UtilityLinksLabel\":[{\"type\":0,\"value\":\"Utility\"}],\"StandardNavigation.AccountBookmarkAlertLabel\":[{\"type\":0,\"value\":\"To revisit this article, select My Account, then\"}],\"StandardNavigation.AccountLabel\":[{\"type\":0,\"value\":\"My Account\"}],\"StandardNavigation.BookmarkAlertLabel\":[{\"type\":0,\"value\":\"To revisit this article, visit My Profile, then\"}],\"StandardNavigation.Drawer\":[{\"type\":0,\"value\":\"Navigation and Sign Up Menu\"}],\"StandardNavigation.MenuButton\":[{\"type\":0,\"value\":\"Open Navigation Menu\"}],\"StandardNavigation.OpenSearchMenuLabel\":[{\"type\":0,\"value\":\"Open Search Menu\"}],\"StandardNavigation.SavedStoriesLabel\":[{\"type\":0,\"value\":\"View saved stories\"}],\"StandardNavigation.SearchLabel\":[{\"type\":0,\"value\":\"Search\"}],\"StandardNavigation.SecondaryLinksLabel\":[{\"type\":0,\"value\":\"Secondary\"}],\"StandardNavigation.ShoppingCartAriaLabel\":[{\"type\":0,\"value\":\"item(s) in Cart\"}],\"StandardNavigation.ShoppingCartLabel\":[{\"type\":0,\"value\":\"Shopping Cart\"}],\"StandardNavigation.SignInLabel\":[{\"type\":0,\"value\":\"Sign In\"}],\"StandardNavigation.UtilityLinksLabel\":[{\"type\":0,\"value\":\"Utility\"}],\"StandardNavigation.saveBookmarkLabel\":[{\"type\":0,\"value\":\"Save story\"}],\"SummaryCarousel.seeMoreAriaLabel\":[{\"type\":0,\"value\":\"See more videos\"}],\"SummaryCollageThree.seeMore\":[{\"type\":0,\"value\":\"See More Videos\"}],\"SummaryCollectionSplitColumns.GuideItemHed\":[{\"type\":1,\"value\":\"location\"},{\"type\":0,\"value\":\" Travel Guide\"}],\"SummaryCollectionSplitColumns.RecommendedTitle\":[{\"type\":0,\"value\":\"Recommended \"},{\"type\":1,\"value\":\"recommendedType\"}],\"SummaryCollectionSplitColumns.ViewAllButton\":[{\"type\":0,\"value\":\"View All \"},{\"type\":1,\"value\":\"location\"},{\"type\":0,\"value\":\" \"},{\"type\":1,\"value\":\"recommendedType\"}],\"SummaryItem.BusinessProfileCTAText\":[{\"type\":0,\"value\":\"View Profile\"}],\"SummaryItem.DefaultCTAText\":[{\"type\":0,\"value\":\"Book Now\"}],\"SummaryItem.DekReadMoreText\":[{\"type\":0,\"value\":\"Read full review\"}],\"SummaryItem.FuturePremiereLabel\":[{\"type\":0,\"value\":\"PREMIERES\"}],\"SummaryItem.LiveVideoLabel\":[{\"type\":0,\"value\":\"live\"}],\"SummaryItem.MoreInfoAndEpisodesPodcastCTA\":[{\"type\":0,\"value\":\"More Info and Episodes\"}],\"SummaryItem.NowShoppingLabel\":[{\"type\":0,\"value\":\"Now Shopping\"}],\"SummaryItem.PastPremiereLabel\":[{\"type\":0,\"value\":\"PREMIERED\"}],\"SummaryItem.ReadMore\":[{\"type\":0,\"value\":\"Read More\"}],\"SummaryItem.ShopNowCTA\":[{\"type\":0,\"value\":\"Shop Now\"}],\"SummaryItem.Slides\":[{\"offset\":0,\"options\":{\"one\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" slide\"}]},\"other\":{\"value\":[{\"type\":7},{\"type\":0,\"value\":\" slides\"}]}},\"pluralType\":\"cardinal\",\"type\":6,\"value\":\"slidesCount\"}],\"SummaryItem.SponsoredContent\":[{\"type\":0,\"value\":\"Sponsored Content\"}],\"SummaryItem.StartListeningNowPodcastCTA\":[{\"type\":0,\"value\":\"Start Listening Now\"}],\"SummaryItem.TodayLabel\":[{\"type\":0,\"value\":\"TODAY\"}],\"SummaryItem.VenueSellerPreviewText\":[{\"type\":0,\"value\":\"Powered By:\"}],\"SummaryItemFeatured.FeaturedTitle\":[{\"type\":0,\"value\":\"Featured\"}],\"SummarySpotlight.ContinueReading\":[{\"type\":0,\"value\":\"Continue reading »\"}],\"SummarySpotlight.SelectedStories\":[{\"type\":0,\"value\":\"Selected Stories\"}],\"TagCloud.SectionHeader\":[{\"type\":0,\"value\":\"Topics\"}],\"TagPage.ClearAll\":[{\"type\":0,\"value\":\"Clear All\"}],\"TagPage.FilterDrawer\":[{\"type\":0,\"value\":\"matching results\"}],\"TagPage.FilterDrawer.FilterApplyActionButton\":[{\"type\":0,\"value\":\"Apply\"}],\"TagPage.FilterDrawer.FilterCancelActionButton\":[{\"type\":0,\"value\":\"Cancel\"}],\"TagPage.FilterResults\":[{\"type\":0,\"value\":\"Filter Results\"}],\"TagPage.Items\":[{\"type\":0,\"value\":\"items\"}],\"TagPage.Loading\":[{\"type\":0,\"value\":\"Loading ...\"}],\"TagPage.MoreStories\":[{\"type\":0,\"value\":\"More Stories\"}],\"TagPage.NoResultsFound\":[{\"type\":0,\"value\":\"Sorry we can't display any results for those filtering options, please try again\"}],\"TagPage.SortBy\":[{\"type\":0,\"value\":\"Sort By\"}],\"TextField.MultiLineErrorText\":[{\"type\":0,\"value\":\"Use at least _MIN_ characters and a maximum of _MAX_.\"}],\"TextField.multiLineUpperLimitErrorText\":[{\"type\":0,\"value\":\"Use maximum of _MAX_ characters only\"}],\"ThreadsEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"Threads content\"}],\"TiktokEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"TikTok content\"}],\"TopStory.toutHead\":[{\"children\":[{\"type\":0,\"value\":\"Also today . . .\"}],\"type\":8,\"value\":\"i\"}],\"TwitterEmbed.ConsentBannerHeader\":[{\"type\":0,\"value\":\"X content\"}],\"TwitterEmbed.EmbedContainer\":[{\"type\":0,\"value\":\"social media post\"}],\"UnifiedProductCard.DefaultOfferCtaText\":[{\"type\":0,\"value\":\"Shop Now\"}],\"UnifiedProductCard.UnifiedProductCardConsLabel\":[{\"type\":0,\"value\":\"Cons\"}],\"UnifiedProductCard.UnifiedProductCardProsConsTitle\":[{\"type\":0,\"value\":\"Pros & Cons\"}],\"UnifiedProductCard.UnifiedProductCardProsLabel\":[{\"type\":0,\"value\":\"Pros\"}],\"UnifiedProductCard.UnifiedProductCardRatingTitle\":[{\"type\":0,\"value\":\"OUR RATING:\"}],\"UserNameModal.CloseButtonLabel\":[{\"type\":0,\"value\":\"Close User Name\"}],\"UserNameModal.Dek\":[{\"type\":0,\"value\":\"Your username will appear next to any recipe notes and replies you add.\"}],\"UserNameModal.ErrorMessage\":[{\"type\":0,\"value\":\"Unable to save username. Please try again.\"}],\"UserNameModal.Hed\":[{\"type\":0,\"value\":\"Create Username\"}],\"UserNameModal.SubmitButtonLabel\":[{\"type\":0,\"value\":\"Save Username\"}],\"UserNameModal.SuccessMessage\":[{\"type\":0,\"value\":\"Your username is saved.\"}],\"UserNameModal.UserNameModalAssistiveText\":[{\"type\":0,\"value\":\"Usernames must be between 2 and 23 characters and can only include letters, numbers and underscores (_).\"}],\"UserNameModal.alreadyTakenError\":[{\"type\":0,\"value\":\"This Username is already taken.\"}],\"UserNameModal.lengthError\":[{\"type\":0,\"value\":\"Usernames must be between 2 and 23 characters.\"}],\"UserNameModal.specialCharError\":[{\"type\":0,\"value\":\"Usernames can only include letters, numbers and underscores (_).\"}],\"UserProfileForm.countryFieldLabel\":[{\"type\":0,\"value\":\"Country\"}],\"UserProfileForm.countryRequiredErrorMsg\":[{\"type\":0,\"value\":\"Select your country.\"}],\"UserProfileForm.firstNameFieldLabel\":[{\"type\":0,\"value\":\"First name\"}],\"UserProfileForm.firstNameRequiredErrorMsg\":[{\"type\":0,\"value\":\"Enter your first name.\"}],\"UserProfileForm.lastNameFieldLabel\":[{\"type\":0,\"value\":\"Last name\"}],\"UserProfileForm.lastNameRequiredErrorMsg\":[{\"type\":0,\"value\":\"Enter your last name.\"}],\"UserProfileForm.phoneNumberFieldLabel\":[{\"type\":0,\"value\":\"Phone number\"}],\"UserProfileForm.phoneNumberLengthErrorMsg\":[{\"type\":0,\"value\":\"Your phone number should be ten digits.\"}],\"UserProfileForm.phoneNumberNonUSLengthErrorMsg\":[{\"type\":0,\"value\":\"Your phone number should be at least ten digits.\"}],\"UserProfileForm.phoneNumberNumericErrorMsg\":[{\"type\":0,\"value\":\"Your phone number should consist of only numerical digits.\"}],\"UserProfileForm.phoneNumberRequiredErrorMsg\":[{\"type\":0,\"value\":\"Enter your phone number.\"}],\"UserProfileForm.serverErrorMessageText\":[{\"type\":0,\"value\":\"There was a problem saving your information. Please try again.\"}],\"UserProfileForm.stateFieldLabel\":[{\"type\":0,\"value\":\"State \\u002F Province\"}],\"UserProfileForm.stateRequiredErrorMsg\":[{\"type\":0,\"value\":\"Select your state \\u002F province.\"}],\"UserProfileForm.submitButtonLabel\":[{\"type\":0,\"value\":\"Save\"}],\"UserProfileForm.termsAndConditionText\":[{\"type\":0,\"value\":\"By submitting a caption, you agree to our \"},{\"children\":[{\"type\":0,\"value\":\"User Agreement\"}],\"type\":8,\"value\":\"a\"},{\"type\":0,\"value\":\" and \"},{\"children\":[{\"type\":0,\"value\":\"Privacy Policy & Cookie Statement.\"}],\"type\":8,\"value\":\"b\"}],\"UtilityNavigation.AccountDropdown\":[{\"type\":0,\"value\":\"Account\"}],\"UtilityNavigation.AccountDropdownAssistive\":[{\"type\":0,\"value\":\"Account Navigation\"}],\"UtilityNavigation.MarketSwitcherLabel\":[{\"type\":0,\"value\":\"Country\"}],\"UtilityNavigation.SearchLabel\":[{\"type\":0,\"value\":\"Search\"}],\"UtilityNavigation.ShoppingCartAriaLabel\":[{\"type\":0,\"value\":\"item(s) in Cart\"}],\"UtilityNavigation.ShoppingCartLabel\":[{\"type\":0,\"value\":\"Shopping Cart\"}],\"UtilityNavigation.SignInLabel\":[{\"type\":0,\"value\":\"Sign In\"}],\"UtilityNavigation.SignOut\":[{\"type\":0,\"value\":\"Sign Out\"}],\"UtilityNavigation.UtilityNavigationButton\":[{\"type\":0,\"value\":\"Open Navigation Menu\"}],\"UtilityValidationDescription.Heading\":[{\"type\":0,\"value\":\"Errors\"}],\"VenuePage.BylinePreamble\":[{\"type\":0,\"value\":\"Reviewed by\"}],\"VenuePage.DefaultCTAText\":[{\"type\":0,\"value\":\"Book Now\"}],\"VenuePage.FilterableListHed\":[{\"type\":0,\"value\":\"More To Discover\"}],\"VenuePage.SectionTitleHed\":[{\"type\":0,\"value\":\"Photos\"}],\"VenuePage.VenueSellerPreviewText\":[{\"type\":0,\"value\":\"Powered By:\"}],\"VersoCommerceCollectionCurated.FilterBy.Brand\":[{\"type\":0,\"value\":\"Designer\"}],\"VersoCommerceCollectionCurated.FilterBy.Category\":[{\"type\":0,\"value\":\"Type\"}],\"VersoCommerceCollectionCurated.FilterBy.Color\":[{\"type\":0,\"value\":\"Color\"}],\"VersoCommerceCollectionCurated.FilterBy.Size\":[{\"type\":0,\"value\":\"Size\"}],\"VersoCommerceCollectionCurated.FilterBy.StorefrontBundle\":[{\"type\":0,\"value\":\"Category\"}],\"VersoCommerceCollectionCurated.FilterBy.Type\":[{\"type\":0,\"value\":\"Type\"}],\"VersoCommerceCollectionCurated.Items\":[{\"type\":0,\"value\":\"Items\"}],\"VersoCommerceCollectionCurated.SortBy.Featured\":[{\"type\":0,\"value\":\"Featured\"}],\"VersoCommerceCollectionCurated.SortBy.HighestPrice\":[{\"type\":0,\"value\":\"Highest Price\"}],\"VersoCommerceCollectionCurated.SortBy.LowestPrice\":[{\"type\":0,\"value\":\"Lowest Price\"}],\"VersoCommerceCollectionCurated.SortBy.MostRecent\":[{\"type\":0,\"value\":\"New Arrivals\"}],\"VersoCommerceCollectionCurated.SortBy.Popular\":[{\"type\":0,\"value\":\"Most Wanted\"}],\"VersoFeatures.viewAllButton\":[{\"type\":0,\"value\":\"View All\"}],\"VersoFilterableSummaryList.CTAMessage\":[{\"type\":0,\"value\":\"See more \"},{\"type\":1,\"value\":\"groupName\"},{\"type\":0,\"value\":\" recipes\"}],\"VersoIssueFeature.IssueFeatureLabel\":[{\"type\":0,\"value\":\"Table of Contents »\"}],\"VideoWrapper.headerText\":[{\"type\":0,\"value\":\"WATCH\"}],\"VideoWrapper.headerTextRelatedOverride\":[{\"type\":0,\"value\":\"Featured Video\"}],\"VideoWrapper.moreLink\":[{\"type\":0,\"value\":\"More \"},{\"type\":1,\"value\":\"brandName\"},{\"type\":0,\"value\":\" Videos\"}],\"venueFeatureList.venueBar\":[{\"type\":0,\"value\":\"Bar\"}],\"venueFeatureList.venueDetox\":[{\"type\":0,\"value\":\"Detox\"}],\"venueFeatureList.venueGolf\":[{\"type\":0,\"value\":\"Golf\"}],\"venueFeatureList.venueLocationMap\":[{\"type\":0,\"value\":\"Location Map\"}],\"venueFeatureList.venueSpa\":[{\"type\":0,\"value\":\"Spa\"}],\"venueFeatureList.venueWifi\":[{\"type\":0,\"value\":\"Wifi\"}],\"venueFeaturesList.venueAmenities\":[{\"type\":0,\"value\":\"Amenities\"}],\"venueFeaturesList.venueBeachName\":[{\"type\":0,\"value\":\"Beach\"}],\"venueFeaturesList.venueBookNow\":[{\"type\":0,\"value\":\"Book Now\"}],\"venueFeaturesList.venueBusinessName\":[{\"type\":0,\"value\":\"Business\"}],\"venueFeaturesList.venueFamily\":[{\"type\":0,\"value\":\"Family\"}],\"venueFeaturesList.venueFreeWifi\":[{\"type\":0,\"value\":\"Free Wifi\"}],\"venueFeaturesList.venueGym\":[{\"type\":0,\"value\":\"Gym\"}],\"venueFeaturesList.venueHolistic\":[{\"type\":0,\"value\":\"Holistic\"}],\"venueFeaturesList.venueKidsProgram\":[{\"type\":0,\"value\":\"Kids Program\"}],\"venueFeaturesList.venueMovementFitness\":[{\"type\":0,\"value\":\"Movement Fitness\"}],\"venueFeaturesList.venuePool\":[{\"type\":0,\"value\":\"Pool\"}],\"venueFeaturesList.venueRoomName\":[{\"type\":0,\"value\":\"Rooms\"}],\"venueFeaturesList.venueSki\":[{\"type\":0,\"value\":\"Ski\"}],\"venuePage.proximityNearby\":[{\"type\":0,\"value\":\"PLACES YOU CAN VISIT\"}],\"venuePage.proximityTravel\":[{\"type\":0,\"value\":\"HOW TO REACH\"}],\"venuePage.venueContactName\":[{\"type\":0,\"value\":\"Contact\"}]},\"licensedPartnerLink\":null,\"magazineDisclaimer\":{\"issueDate\":\"November 23, 2015\",\"issueLink\":\"\\u002Fmagazine\\u002F2015\\u002F11\\u002F23\",\"originalHed\":\"The Doomsday Invention\"},\"paddingTop\":\"large\",\"relatedVideo\":{\"showRelatedVideo\":false,\"brand\":\"newyorker\",\"related\":{\"hed\":\"What Money Can Buy\",\"id\":\"5911cbe9803aff0f1c1359c3\",\"inlineEmbeds\":[],\"metadata\":{\"contentType\":\"article\"},\"locationRubric\":{},\"channelRubric\":{},\"toutMedia\":{\"node\":{\"altText\":\"Darren Walker the Ford headquarters\",\"aspectRatios\":[{\"name\":\"2:1\",\"url\":null,\"width\":1847,\"height\":923,\"format\":null,\"modifications\":{\"crop\":{\"height\":923,\"width\":1847,\"x\":0,\"y\":1264}}},{\"name\":\"2:2\",\"url\":null,\"width\":1847,\"height\":1847,\"format\":null,\"modifications\":{\"crop\":{\"height\":1847,\"width\":1847,\"x\":0,\"y\":481}}},{\"name\":\"16:9\",\"url\":null,\"width\":1839,\"height\":1034,\"format\":null,\"modifications\":{\"crop\":{\"height\":1034,\"width\":1839,\"x\":0,\"y\":1137}}},{\"name\":\"4:3\",\"url\":null,\"width\":1847,\"height\":1385,\"format\":null,\"modifications\":{\"crop\":{\"height\":1385,\"width\":1847,\"x\":0,\"y\":1174}}},{\"name\":\"1:1\",\"url\":null,\"width\":1847,\"height\":1847,\"format\":null,\"modifications\":{\"crop\":{\"height\":1847,\"width\":1847,\"x\":0,\"y\":712}}},{\"name\":\"master\",\"url\":\"https:\\u002F\\u002Fgp-prd-global-clips-s3.s3.amazonaws.com\\u002Fpublic\\u002Ftny-services\\u002Fproduction\\u002F2017\\u002F05\\u002F03\\u002F5909730c1c7a8e33fb38f08b_160104_r27491.jpg\",\"width\":1848,\"height\":2560,\"format\":\"JPEG\",\"modifications\":null}],\"caption\":\"Walker at the Ford headquarters. “In the sixties, when you came to see the president,” he says, “it was meant to be intimidating.”\",\"contextualBody\":null,\"contextualCaption\":null,\"contextualTitle\":null,\"cropMode\":\"TOP\",\"credit\":\"Photograph by Andrew Moore for The New Yorker\",\"filename\":\"160104_r27491.jpg\",\"id\":\"5909730c1c7a8e33fb38f08c\",\"metadata\":{\"contentType\":\"photo\"},\"contentWarnings\":[],\"restrictCropping\":false,\"__typename\":\"Photo\"}},\"contributors\":{\"author\":[{\"__typename\":\"Contributor\",\"id\":\"590a18e98b51cf59fc424780\",\"url\":\"contributors\\u002Flarissa-macfarquhar\",\"name\":\"Larissa MacFarquhar\",\"title\":\"Larissa MacFarquhar, a staﬀ writer at The New Yorker, is the author of “[Strangers Drowning: Impossible Idealism, Drastic Choices, and the Urge to Help](https:\\u002F\\u002Fwww.amazon.com\\u002Fdp\\u002F0143109782\\u002F).”\",\"type\":\"AUTHOR\",\"contributorType\":\"AUTHOR\",\"photo\":{\"id\":\"59097b842179605b11ad8f2a\",\"aspectRatios\":[{\"url\":\"https:\\u002F\\u002Fgp-prd-global-clips-s3.s3.amazonaws.com\\u002Fpublic\\u002Ftny-services\\u002Fproduction\\u002F2017\\u002F05\\u002F03\\u002F59097b832179605b11ad8f29_macfarquhar-larissa.png\"}],\"metadata\":{\"contentType\":\"photo\"}},\"bio\":\"Larissa MacFarquhar has been a staff writer at *The New Yorker* since 1998. She has written about [child-protective services](https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2017\\u002F08\\u002F07\\u002Fwhen-should-a-child-be-taken-from-his-parents), the [battered-women’s movement](https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2019\\u002F08\\u002F19\\u002Fthe-radical-transformations-of-a-battered-womens-shelter), [dementia](https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2018\\u002F10\\u002F08\\u002Fthe-comforting-fictions-of-dementia-care), and [hospice care](https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2016\\u002F07\\u002F11\\u002Fthe-work-of-a-hospice-nurse), and her Profile subjects have included John Ashbery, Barack Obama, Noam Chomsky, Hilary Mantel, Derek Parfit, David Chang, and Aaron Swartz, among many others. She is the author of “[Strangers Drowning: Impossible Idealism, Drastic Choices, and the Urge to Help](https:\\u002F\\u002Fwww.amazon.com\\u002Fdp\\u002F0143109782\\u002F?ots=1&slotNum=0&imprToken=c2bfa32c-e999-0d10-a60&tag=thneyo0f-20).” Before joining the magazine, she was a senior editor at *Lingua Franca* and an advisory editor at *The Paris Review*, and wrote for *Artforum*, *The Nation*, *The New Republic*, the *Times Book Review*, Slate, and other publications. She has received two Front Page Awards from the Newswomen’s Club of New York and the Johnson & Johnson Excellence in Media Award. Her writing has appeared in “The Best American Political Writing” and “The Best American Food Writing.”\",\"socialMedia\":[{\"handle\":\"LarissaMacFarqu\",\"network\":\"Twitter\"}],\"metadata\":{\"contentType\":\"contributor\"}}]},\"channels\":[{\"name\":\"Profiles\"}],\"_embedded\":{\"_categories\":{\"channels\":[{\"name\":\"Profiles\"}]},\"publishHistory\":{}},\"photos\":{\"lede\":[{\"altText\":\"Darren Walker the Ford headquarters\",\"aspectRatios\":{\"2:1\":{\"url\":null,\"width\":1847,\"height\":923,\"format\":null},\"2:2\":{\"url\":null,\"width\":1847,\"height\":1847,\"format\":null},\"16:9\":{\"url\":null,\"width\":1839,\"height\":1034,\"format\":null},\"4:3\":{\"url\":null,\"width\":1847,\"height\":1385,\"format\":null},\"1:1\":{\"url\":null,\"width\":1847,\"height\":1847,\"format\":null},\"master\":{\"url\":\"https:\\u002F\\u002Fgp-prd-global-clips-s3.s3.amazonaws.com\\u002Fpublic\\u002Ftny-services\\u002Fproduction\\u002F2017\\u002F05\\u002F03\\u002F5909730c1c7a8e33fb38f08b_160104_r27491.jpg\",\"width\":1848,\"height\":2560,\"format\":\"JPEG\"}},\"caption\":\"Walker at the Ford headquarters. “In the sixties, when you came to see the president,” he says, “it was meant to be intimidating.”\",\"cropMode\":\"TOP\",\"credit\":\"Photograph by Andrew Moore for The New Yorker\",\"filename\":\"160104_r27491.jpg\",\"id\":\"5909730c1c7a8e33fb38f08c\",\"metadata\":{\"contentType\":\"photo\"},\"contentWarnings\":[],\"__typename\":\"Photo\",\"collection\":\"photos\",\"modelName\":\"photo\",\"meta\":{\"modelName\":\"photo\"}}],\"tout\":[{\"altText\":\"Darren Walker the Ford headquarters\",\"aspectRatios\":{\"2:1\":{\"url\":null,\"width\":1847,\"height\":923,\"format\":null},\"2:2\":{\"url\":null,\"width\":1847,\"height\":1847,\"format\":null},\"16:9\":{\"url\":null,\"width\":1839,\"height\":1034,\"format\":null},\"4:3\":{\"url\":null,\"width\":1847,\"height\":1385,\"format\":null},\"1:1\":{\"url\":null,\"width\":1847,\"height\":1847,\"format\":null},\"master\":{\"url\":\"https:\\u002F\\u002Fgp-prd-global-clips-s3.s3.amazonaws.com\\u002Fpublic\\u002Ftny-services\\u002Fproduction\\u002F2017\\u002F05\\u002F03\\u002F5909730c1c7a8e33fb38f08b_160104_r27491.jpg\",\"width\":1848,\"height\":2560,\"format\":\"JPEG\"}},\"caption\":\"Walker at the Ford headquarters. “In the sixties, when you came to see the president,” he says, “it was meant to be intimidating.”\",\"cropMode\":\"TOP\",\"credit\":\"Photograph by Andrew Moore for The New Yorker\",\"filename\":\"160104_r27491.jpg\",\"id\":\"5909730c1c7a8e33fb38f08c\",\"metadata\":{\"contentType\":\"photo\"},\"contentWarnings\":[],\"__typename\":\"Photo\",\"collection\":\"photos\",\"modelName\":\"photo\",\"meta\":{\"modelName\":\"photo\"}}]},\"categories\":{\"channels\":[{\"name\":\"Profiles\"}]},\"savingsUnitedCoupons\":[],\"url\":\"magazine\\u002F2016\\u002F01\\u002F04\\u002Fwhat-money-can-buy-profiles-larissa-macfarquhar\",\"modelName\":\"article\",\"meta\":{\"modelName\":\"article\"}},\"pageSize\":4},\"interlude\":{\"brand\":\"newyorker\",\"humanName\":\"The New Yorker\",\"playerBase\":\"https:\\u002F\\u002Fplayer.cnevids.com\",\"strategy\":{\"enabled\":true,\"target\":{\"method\":\"dynamicMidpoint\"},\"embed\":{\"method\":\"default\"}},\"relatedVideo\":{\"showRelatedVideo\":false,\"brand\":\"newyorker\",\"related\":{\"hed\":\"What Money Can Buy\",\"id\":\"5911cbe9803aff0f1c1359c3\",\"inlineEmbeds\":[],\"metadata\":{\"contentType\":\"article\"},\"locationRubric\":{},\"channelRubric\":{},\"toutMedia\":{\"node\":{\"altText\":\"Darren Walker the Ford headquarters\",\"aspectRatios\":[{\"name\":\"2:1\",\"url\":null,\"width\":1847,\"height\":923,\"format\":null,\"modifications\":{\"crop\":{\"height\":923,\"width\":1847,\"x\":0,\"y\":1264}}},{\"name\":\"2:2\",\"url\":null,\"width\":1847,\"height\":1847,\"format\":null,\"modifications\":{\"crop\":{\"height\":1847,\"width\":1847,\"x\":0,\"y\":481}}},{\"name\":\"16:9\",\"url\":null,\"width\":1839,\"height\":1034,\"format\":null,\"modifications\":{\"crop\":{\"height\":1034,\"width\":1839,\"x\":0,\"y\":1137}}},{\"name\":\"4:3\",\"url\":null,\"width\":1847,\"height\":1385,\"format\":null,\"modifications\":{\"crop\":{\"height\":1385,\"width\":1847,\"x\":0,\"y\":1174}}},{\"name\":\"1:1\",\"url\":null,\"width\":1847,\"height\":1847,\"format\":null,\"modifications\":{\"crop\":{\"height\":1847,\"width\":1847,\"x\":0,\"y\":712}}},{\"name\":\"master\",\"url\":\"https:\\u002F\\u002Fgp-prd-global-clips-s3.s3.amazonaws.com\\u002Fpublic\\u002Ftny-services\\u002Fproduction\\u002F2017\\u002F05\\u002F03\\u002F5909730c1c7a8e33fb38f08b_160104_r27491.jpg\",\"width\":1848,\"height\":2560,\"format\":\"JPEG\",\"modifications\":null}],\"caption\":\"Walker at the Ford headquarters. “In the sixties, when you came to see the president,” he says, “it was meant to be intimidating.”\",\"contextualBody\":null,\"contextualCaption\":null,\"contextualTitle\":null,\"cropMode\":\"TOP\",\"credit\":\"Photograph by Andrew Moore for The New Yorker\",\"filename\":\"160104_r27491.jpg\",\"id\":\"5909730c1c7a8e33fb38f08c\",\"metadata\":{\"contentType\":\"photo\"},\"contentWarnings\":[],\"restrictCropping\":false,\"__typename\":\"Photo\"}},\"contributors\":{\"author\":[{\"__typename\":\"Contributor\",\"id\":\"590a18e98b51cf59fc424780\",\"url\":\"contributors\\u002Flarissa-macfarquhar\",\"name\":\"Larissa MacFarquhar\",\"title\":\"Larissa MacFarquhar, a staﬀ writer at The New Yorker, is the author of “[Strangers Drowning: Impossible Idealism, Drastic Choices, and the Urge to Help](https:\\u002F\\u002Fwww.amazon.com\\u002Fdp\\u002F0143109782\\u002F).”\",\"type\":\"AUTHOR\",\"contributorType\":\"AUTHOR\",\"photo\":{\"id\":\"59097b842179605b11ad8f2a\",\"aspectRatios\":[{\"url\":\"https:\\u002F\\u002Fgp-prd-global-clips-s3.s3.amazonaws.com\\u002Fpublic\\u002Ftny-services\\u002Fproduction\\u002F2017\\u002F05\\u002F03\\u002F59097b832179605b11ad8f29_macfarquhar-larissa.png\"}],\"metadata\":{\"contentType\":\"photo\"}},\"bio\":\"Larissa MacFarquhar has been a staff writer at *The New Yorker* since 1998. She has written about [child-protective services](https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2017\\u002F08\\u002F07\\u002Fwhen-should-a-child-be-taken-from-his-parents), the [battered-women’s movement](https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2019\\u002F08\\u002F19\\u002Fthe-radical-transformations-of-a-battered-womens-shelter), [dementia](https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2018\\u002F10\\u002F08\\u002Fthe-comforting-fictions-of-dementia-care), and [hospice care](https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2016\\u002F07\\u002F11\\u002Fthe-work-of-a-hospice-nurse), and her Profile subjects have included John Ashbery, Barack Obama, Noam Chomsky, Hilary Mantel, Derek Parfit, David Chang, and Aaron Swartz, among many others. She is the author of “[Strangers Drowning: Impossible Idealism, Drastic Choices, and the Urge to Help](https:\\u002F\\u002Fwww.amazon.com\\u002Fdp\\u002F0143109782\\u002F?ots=1&slotNum=0&imprToken=c2bfa32c-e999-0d10-a60&tag=thneyo0f-20).” Before joining the magazine, she was a senior editor at *Lingua Franca* and an advisory editor at *The Paris Review*, and wrote for *Artforum*, *The Nation*, *The New Republic*, the *Times Book Review*, Slate, and other publications. She has received two Front Page Awards from the Newswomen’s Club of New York and the Johnson & Johnson Excellence in Media Award. Her writing has appeared in “The Best American Political Writing” and “The Best American Food Writing.”\",\"socialMedia\":[{\"handle\":\"LarissaMacFarqu\",\"network\":\"Twitter\"}],\"metadata\":{\"contentType\":\"contributor\"}}]},\"channels\":[{\"name\":\"Profiles\"}],\"_embedded\":{\"_categories\":{\"channels\":[{\"name\":\"Profiles\"}]},\"publishHistory\":{}},\"photos\":{\"lede\":[{\"altText\":\"Darren Walker the Ford headquarters\",\"aspectRatios\":{\"2:1\":{\"url\":null,\"width\":1847,\"height\":923,\"format\":null},\"2:2\":{\"url\":null,\"width\":1847,\"height\":1847,\"format\":null},\"16:9\":{\"url\":null,\"width\":1839,\"height\":1034,\"format\":null},\"4:3\":{\"url\":null,\"width\":1847,\"height\":1385,\"format\":null},\"1:1\":{\"url\":null,\"width\":1847,\"height\":1847,\"format\":null},\"master\":{\"url\":\"https:\\u002F\\u002Fgp-prd-global-clips-s3.s3.amazonaws.com\\u002Fpublic\\u002Ftny-services\\u002Fproduction\\u002F2017\\u002F05\\u002F03\\u002F5909730c1c7a8e33fb38f08b_160104_r27491.jpg\",\"width\":1848,\"height\":2560,\"format\":\"JPEG\"}},\"caption\":\"Walker at the Ford headquarters. “In the sixties, when you came to see the president,” he says, “it was meant to be intimidating.”\",\"cropMode\":\"TOP\",\"credit\":\"Photograph by Andrew Moore for The New Yorker\",\"filename\":\"160104_r27491.jpg\",\"id\":\"5909730c1c7a8e33fb38f08c\",\"metadata\":{\"contentType\":\"photo\"},\"contentWarnings\":[],\"__typename\":\"Photo\",\"collection\":\"photos\",\"modelName\":\"photo\",\"meta\":{\"modelName\":\"photo\"}}],\"tout\":[{\"altText\":\"Darren Walker the Ford headquarters\",\"aspectRatios\":{\"2:1\":{\"url\":null,\"width\":1847,\"height\":923,\"format\":null},\"2:2\":{\"url\":null,\"width\":1847,\"height\":1847,\"format\":null},\"16:9\":{\"url\":null,\"width\":1839,\"height\":1034,\"format\":null},\"4:3\":{\"url\":null,\"width\":1847,\"height\":1385,\"format\":null},\"1:1\":{\"url\":null,\"width\":1847,\"height\":1847,\"format\":null},\"master\":{\"url\":\"https:\\u002F\\u002Fgp-prd-global-clips-s3.s3.amazonaws.com\\u002Fpublic\\u002Ftny-services\\u002Fproduction\\u002F2017\\u002F05\\u002F03\\u002F5909730c1c7a8e33fb38f08b_160104_r27491.jpg\",\"width\":1848,\"height\":2560,\"format\":\"JPEG\"}},\"caption\":\"Walker at the Ford headquarters. “In the sixties, when you came to see the president,” he says, “it was meant to be intimidating.”\",\"cropMode\":\"TOP\",\"credit\":\"Photograph by Andrew Moore for The New Yorker\",\"filename\":\"160104_r27491.jpg\",\"id\":\"5909730c1c7a8e33fb38f08c\",\"metadata\":{\"contentType\":\"photo\"},\"contentWarnings\":[],\"__typename\":\"Photo\",\"collection\":\"photos\",\"modelName\":\"photo\",\"meta\":{\"modelName\":\"photo\"}}]},\"categories\":{\"channels\":[{\"name\":\"Profiles\"}]},\"savingsUnitedCoupons\":[],\"url\":\"magazine\\u002F2016\\u002F01\\u002F04\\u002Fwhat-money-can-buy-profiles-larissa-macfarquhar\",\"modelName\":\"article\",\"meta\":{\"modelName\":\"article\"}},\"pageSize\":4},\"embeddedVideos\":[],\"hasExcludedEmbed\":false},\"hasEventBannerHidden\":false,\"hasInvertedHeadertheme\":false,\"hideContributorBio\":false,\"hideRecircMostPopular\":true,\"badge\":null,\"tagCloud\":{\"tags\":[{\"id\":\"5c2e1c7922d4972cd5b8328f\",\"tag\":\"Artificial Intelligence (A.I.)\",\"url\":\"\\u002Ftag\\u002Fartificial-intelligence-ai\"},{\"id\":\"5c2e1cc5b75f002c8941e4e2\",\"tag\":\"Philosophers\",\"url\":\"\\u002Ftag\\u002Fphilosophers\"}]},\"newsletterModules\":[],\"shoppableItems\":{},\"shouldUsePersistentAd\":true,\"showAgeGate\":false,\"showBreadcrumbTrail\":false,\"lightboxImages\":[],\"channelCloudData\":{},\"recircs\":[],\"recircMostPopular\":[],\"recircRelated\":[{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"What Does Robert F. Kennedy, Jr., Actually Want?\",\"dangerousDek\":\"The third-party Presidential candidate has a troubled past, a shambolic campaign, and some surprisingly good poll numbers.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2024\\u002F08\\u002F12\\u002Frobert-f-kennedy-jr-profile-presidential-campaign#intcid=recommendations_the-new-yorker-bottom-recirc-v4_33bb9249-4c86-4a79-8cc8-81e565c59536_similar2-3\",\"rubric\":{\"name\":\"Profiles\"},\"tout\":{\"altText\":\"Robert F. Kennedy Jr., photographed sitting in the backseat of a car, by Dan Winters for The New Yorker.\",\"id\":\"66abb7c5a40c6f9312df886f\",\"credit\":\"\",\"caption\":\"“It’s almost like he’s been body-snatched,” one longtime friend said. “I look at pictures of him, and he’s unrecognizable. His sense of humor is all but gone. There’s this anger.”\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Robert F. Kennedy Jr., photographed sitting in the backseat of a car, by Dan Winters for The New Yorker.\",\"id\":\"66abb7c5a40c6f9312df886f\",\"credit\":\"\",\"caption\":\"“It’s almost like he’s been body-snatched,” one longtime friend said. “I look at pictures of him, and he’s unrecognizable. His sense of humor is all but gone. There’s this anger.”\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Clare Malone\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"Reimagining China in Tokyo\",\"dangerousDek\":\"A new community of expats is opening bookstores, attending lectures, and imagining alternatives to Xi from the relative safety of Japan.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fnews\\u002Fdispatch\\u002Freimagining-china-in-tokyo#intcid=recommendations_the-new-yorker-bottom-recirc-v4_33bb9249-4c86-4a79-8cc8-81e565c59536_similar2-3\",\"rubric\":{\"name\":\"Dispatch\"},\"tout\":{\"altText\":\"A person in a traditional Japanese room looking at a shadow cast of a bird escaping tangled trees\",\"id\":\"6697f20b98f2ab68277f0e61\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"A person in a traditional Japanese room looking at a shadow cast of a bird escaping tangled trees\",\"id\":\"6697f20b98f2ab68277f0e61\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F6697f20b98f2ab68277f0e61\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Chang Che\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"Evan Gershkovich Is Finally Coming Home\",\"dangerousDek\":\"In a multinational prisoner exchange, the \\u003Cem\\u003EWall Street Journal\\u003C\\u002Fem\\u003E reporter was freed, after being detained for more than a year in Russian jail.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fnews\\u002Fnews-desk\\u002Fevan-gershkovich-is-finally-coming-home#intcid=recommendations_the-new-yorker-bottom-recirc-v4_33bb9249-4c86-4a79-8cc8-81e565c59536_similar2-3\",\"rubric\":{\"name\":\"News Desk\"},\"tout\":{\"altText\":\"A photo of the Wall Street Journal reporter Evan Gershkovich, pictured through a glass window of a courtroom, in Russia, in July of 2024.\",\"id\":\"66ab89a6b8931b3ee41acee0\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"A photo of the Wall Street Journal reporter Evan Gershkovich, pictured through a glass window of a courtroom, in Russia, in July of 2024.\",\"id\":\"66ab89a6b8931b3ee41acee0\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ab89a6b8931b3ee41acee0\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Joshua Yaffa\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"J. D. Vance and the Right’s Call to Have More Babies\",\"dangerousDek\":\"Pronatalism has much in common with some of Vance’s views: it typically combines concerns about falling birth rates with anti-immigration and anti-feminist ideas.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fnews\\u002Fdaily-comment\\u002Fj-d-vance-and-the-rights-call-to-have-more-babies#intcid=recommendations_the-new-yorker-bottom-recirc-v4_33bb9249-4c86-4a79-8cc8-81e565c59536_similar2-3\",\"rubric\":{\"name\":\"Daily Comment\"},\"tout\":{\"altText\":\"U.S. Senator J.D. Vance speaking at a campaign event in Reno, NV. \",\"id\":\"66ad10339f75c45af015d951\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"U.S. Senator J.D. Vance speaking at a campaign event in Reno, NV. \",\"id\":\"66ad10339f75c45af015d951\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad10339f75c45af015d951\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Margaret Talbot\"}]}}}],\"recircRelatedBrandStories\":{\"sameBrandRecirc\":[{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"“House of the Dragon” Still Hasn’t Caught Fire\",\"dangerousDek\":\"The HBO show’s latest season finale reaffirms Rhaenyra’s right to rule&#8212;but her mode of noble restraint, however admirable in a leader, is lethal in a protagonist.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fculture\\u002Fon-television\\u002Fhouse-of-the-dragon-season-2-finale-review#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"On Television\"},\"tout\":{\"altText\":\"Clinton Liberty, Harry Collett, Emma D’Arcy, Bethany Antonia, Kieran Bew, Tom Bennett around a table setting in “House of the Dragon,” Season 2, Episode 8.\",\"id\":\"66afc8cf50e27a6b871ed406\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Clinton Liberty, Harry Collett, Emma D’Arcy, Bethany Antonia, Kieran Bew, Tom Bennett around a table setting in “House of the Dragon,” Season 2, Episode 8.\",\"id\":\"66afc8cf50e27a6b871ed406\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66afc8cf50e27a6b871ed406\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Inkoo Kang\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"Tim Walz and the Lessons of High-School Football\",\"dangerousDek\":\"The Vice-Presidential nominee was the defensive coördinator for a team that won the state title. His players say that he taught them more about togetherness than tactics.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fsports\\u002Fsporting-scene\\u002Ftim-walz-and-the-lessons-of-high-school-football#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"The Sporting Scene\"},\"tout\":{\"altText\":\"A photo of Tim Walz coaching a young football team.\",\"id\":\"66b61530587e5a013c84c95a\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"A photo of Tim Walz coaching a young football team.\",\"id\":\"66b61530587e5a013c84c95a\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b61530587e5a013c84c95a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Louisa Thomas\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"Capitalism Is Running Out of Flavors\",\"dangerousDek\":\"Starburst FaveREDS: Not a flavor&#8212;not even a word.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fhumor\\u002Fshouts-murmurs\\u002Fcapitalism-is-running-out-of-flavors#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Shouts & Murmurs\"},\"tout\":{\"altText\":\"A three pack of unicorn magic \\\"pudding.\\\"\",\"id\":\"66aa79620a2b7fdbb444e85f\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"A three pack of unicorn magic \\\"pudding.\\\"\",\"id\":\"66aa79620a2b7fdbb444e85f\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa79620a2b7fdbb444e85f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Teresa Wong\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"Episode 2: I Have Questions\",\"dangerousDek\":\"A trip to a Marine Corps archive reveals a clue about something that the U.S. military is keeping secret.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fpodcast\\u002Fin-the-dark\\u002Fseason-3-episode-2-i-have-questions#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"In the Dark\"},\"tout\":{\"altText\":\"Reporter looking at documents under a light.\",\"id\":\"669157c6383199d53f2cce4c\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Reporter looking at documents under a light.\",\"id\":\"669157c6383199d53f2cce4c\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669157c6383199d53f2cce4c\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"Pete Rose and the Complicated Legacy of Cincinnati Baseball\",\"dangerousDek\":\"The culture that sheltered Rose from the fallout of his excesses did not extend the same protection to the team’s Black players.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fbooks\\u002Funder-review\\u002Fpete-rose-and-the-complicated-legacy-of-cincinnati-baseball#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Under Review\"},\"tout\":{\"altText\":\"Rear view of Cincinnati Reds Pete Rose (14) walking through runway before game \",\"id\":\"66b279e1ca870a17aadc770f\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Rear view of Cincinnati Reds Pete Rose (14) walking through runway before game \",\"id\":\"66b279e1ca870a17aadc770f\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b279e1ca870a17aadc770f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Brandon Harris\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"Trump’s Dangerous Embrace of Bitcoin and the Crypto Bros\",\"dangerousDek\":\"Having suffered a series of legal and regulatory setbacks in recent years, the cryptocurrency industry is pouring millions of dollars into the upcoming election. To what end?\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fnews\\u002Fthe-financial-page\\u002Ftrumps-dangerous-embrace-of-bitcoin-and-the-crypto-bros#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"The Financial Page\"},\"tout\":{\"altText\":\"Republican presidential candidate former President Donald Trump speaks at the Bitcoin 2024 Conference.\",\"id\":\"66ad1f0d895c1fb33d6aac85\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Republican presidential candidate former President Donald Trump speaks at the Bitcoin 2024 Conference.\",\"id\":\"66ad1f0d895c1fb33d6aac85\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66ad1f0d895c1fb33d6aac85\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"John Cassidy\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"The Supreme Court Needs Fixing, but How?\",\"dangerousDek\":\"President Biden has proposed radical changes to the Court. Reviewing them is a reminder of why reform is so hard, despite dissatisfaction and a wealth of ideas.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2024\\u002F08\\u002F12\\u002Fthe-supreme-court-needs-fixing-but-how#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Comment\"},\"tout\":{\"altText\":\"A photo of the front of the U.S. Supreme Court Building, pictured from an angle.\",\"id\":\"66acf6c90820905a47bbc333\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"A photo of the front of the U.S. Supreme Court Building, pictured from an angle.\",\"id\":\"66acf6c90820905a47bbc333\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acf6c90820905a47bbc333\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Amy Davidson Sorkin\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"What Tim Walz Brings to Kamala Harris’s Campaign to Beat Donald Trump\",\"dangerousDek\":\"The Minnesota governor with a progressive agenda becomes the Democratic Vice-Presidential nominee after capturing the Zeitgeist with a single word.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fnews\\u002Fdaily-comment\\u002Fwhat-tim-walz-brings-to-kamala-harriss-campaign-to-beat-donald-trump#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Daily Comment\"},\"tout\":{\"altText\":\"Tim Walz, the governor of Minnesota and the Democratic Vice-Presidential candidate, speaking at the White House. Walz wears a dark suit jacket, a white shirt, a blue striped tie, a pin of the Minnesota flag, and glasses. \",\"id\":\"66b231af4e09aff9f8068edd\",\"credit\":\"\",\"caption\":\"Walz boasts of his rural roots in a party that skews urban, suburban, and highly educated.\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Tim Walz, the governor of Minnesota and the Democratic Vice-Presidential candidate, speaking at the White House. Walz wears a dark suit jacket, a white shirt, a blue striped tie, a pin of the Minnesota flag, and glasses. \",\"id\":\"66b231af4e09aff9f8068edd\",\"credit\":\"\",\"caption\":\"Walz boasts of his rural roots in a party that skews urban, suburban, and highly educated.\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b231af4e09aff9f8068edd\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Peter Slevin\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"The Politics of “Weird”\",\"dangerousDek\":\"Kamala Harris’s campaign has smartly positioned her as the normal candidate. But disagreements and distractions lie ahead.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fnews\\u002Ffault-lines\\u002Fthe-politics-of-weird#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Fault Lines\"},\"tout\":{\"altText\":\"Illustration of a voter and voting booth cutout. \",\"id\":\"66abdb26bc91e64eba25d2a7\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Illustration of a voter and voting booth cutout. \",\"id\":\"66abdb26bc91e64eba25d2a7\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abdb26bc91e64eba25d2a7\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Jay Caspian Kang\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"Two Paths for Jewish Politics\",\"dangerousDek\":\"In America, Jews pioneered a way of life that didn’t rely on the whims of the powerful. Now it’s under threat.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fculture\\u002Fthe-weekend-essay\\u002Ftwo-paths-for-jewish-politics#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"The Weekend Essay\"},\"tout\":{\"altText\":\"An illustration of a woman covering her eyes with two Seder candles shaped like Roman columns burning in front of her.\",\"id\":\"66acd80431e1b1bcebdcd3e6\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"An illustration of a woman covering her eyes with two Seder candles shaped like Roman columns burning in front of her.\",\"id\":\"66acd80431e1b1bcebdcd3e6\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66acd80431e1b1bcebdcd3e6\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Corey Robin\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"The Kamala Harris Vibe Shift\",\"dangerousDek\":\"Her campaign has ushered in a rush of political energy, evidenced by a deluge of memes and pop-culture mashups. Can this new optimism last?\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fpodcast\\u002Fcritics-at-large\\u002Fthe-kamala-harris-vibe-shift#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Critics at Large\"},\"tout\":{\"altText\":\"Portraits of the hosts for Critics at Large podcast\",\"id\":\"650b59e2f49b447c4924fa7b\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Portraits of the hosts for Critics at Large podcast\",\"id\":\"650b59e2f49b447c4924fa7b\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F650b59e2f49b447c4924fa7b\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"The Vigil Keepers of January 6th\",\"dangerousDek\":\"In the aftermath of the assault on the Capitol, a trio of women with family members who participated in the riot moved to D.C. to seek their own kind of justice.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fnews\\u002Famerican-chronicles\\u002Fthe-vigil-keepers-of-january-6th#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"American Chronicles\"},\"tout\":{\"altText\":\"Tami Perryman, from left, Micki Witthoeft, and Nicole Reffitt stand in front of the D.C. Jail on the night marking two years since their vigil supporting the January 6th defendants began. \",\"id\":\"66b528ac6bbcfeb052dd6724\",\"credit\":\"\",\"caption\":\"Tami Perryman, in blue, Micki Witthoeft, and Nicole Reffitt have attended more than five hundred days of January 6th hearings and host a nightly vigil outside the jail where many accused participants are held.\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Tami Perryman, from left, Micki Witthoeft, and Nicole Reffitt stand in front of the D.C. Jail on the night marking two years since their vigil supporting the January 6th defendants began. \",\"id\":\"66b528ac6bbcfeb052dd6724\",\"credit\":\"\",\"caption\":\"Tami Perryman, in blue, Micki Witthoeft, and Nicole Reffitt have attended more than five hundred days of January 6th hearings and host a nightly vigil outside the jail where many accused participants are held.\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b528ac6bbcfeb052dd6724\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Antonia Hitchens\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"Nancy Pelosi, the Power Broker\",\"dangerousDek\":\"The Speaker Emerita played a leading role in pushing the Biden Administration’s legislative agenda through Congress. Then she helped clear the path for a new Democratic leadership.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fpodcast\\u002Fthe-new-yorker-radio-hour\\u002Fnancy-pelosi-the-power-broker#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"The New Yorker Radio Hour\"},\"tout\":{\"altText\":\"Portrait of Nancy Pelosi.\",\"id\":\"66b63e9ad861244a07a8932e\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Portrait of Nancy Pelosi.\",\"id\":\"66b63e9ad861244a07a8932e\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b63e9ad861244a07a8932e\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"How “The Boyfriend” Distills Gay Romance\",\"dangerousDek\":\"The Japanese dating show captures friendship, heartbreak, and the perils of having a hot roommate.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fculture\\u002Fcultural-comment\\u002Fhow-the-boyfriend-distills-gay-romance#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Cultural Comment\"},\"tout\":{\"altText\":\"Boyfriend\",\"id\":\"66b11989c22639d45590d9ef\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Boyfriend\",\"id\":\"66b11989c22639d45590d9ef\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66b11989c22639d45590d9ef\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Simon Wu\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"Two Centuries Later, a Female Composer Is Rediscovered\",\"dangerousDek\":\"Carolina Uccelli’s opera “Anna di Resburgo” was remarkably inventive&#8212;but it vanished after its première. Teatro Nuovo has brought it back to life.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2024\\u002F08\\u002F12\\u002Fteatro-nuovo-anna-di-resburgo-opera-music-review#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Musical Events\"},\"tout\":{\"altText\":\"Drawing of a woman holding a quill pen.\",\"id\":\"66aa48ebd66202ede704307a\",\"credit\":\"\",\"caption\":\"“Anna” gives the impression of a wide-ranging musical mind that possesses historical consciousness and experimental intelligence in equal measure.\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Drawing of a woman holding a quill pen.\",\"id\":\"66aa48ebd66202ede704307a\",\"credit\":\"\",\"caption\":\"“Anna” gives the impression of a wide-ranging musical mind that possesses historical consciousness and experimental intelligence in equal measure.\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66aa48ebd66202ede704307a\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Alex Ross\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"What Does Robert F. Kennedy, Jr., Actually Want?\",\"dangerousDek\":\"The third-party Presidential candidate has a troubled past, a shambolic campaign, and some surprisingly good poll numbers.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2024\\u002F08\\u002F12\\u002Frobert-f-kennedy-jr-profile-presidential-campaign#intcid=_the-new-yorker-bottom-recirc-bkt-a_bdf7bcb8-801f-4f17-832c-85dee5abb89e_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Profiles\"},\"tout\":{\"altText\":\"Robert F. Kennedy Jr., photographed sitting in the backseat of a car, by Dan Winters for The New Yorker.\",\"id\":\"66abb7c5a40c6f9312df886f\",\"credit\":\"\",\"caption\":\"“It’s almost like he’s been body-snatched,” one longtime friend said. “I look at pictures of him, and he’s unrecognizable. His sense of humor is all but gone. There’s this anger.”\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Robert F. Kennedy Jr., photographed sitting in the backseat of a car, by Dan Winters for The New Yorker.\",\"id\":\"66abb7c5a40c6f9312df886f\",\"credit\":\"\",\"caption\":\"“It’s almost like he’s been body-snatched,” one longtime friend said. “I look at pictures of him, and he’s unrecognizable. His sense of humor is all but gone. There’s this anger.”\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66abb7c5a40c6f9312df886f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Clare Malone\"}]}}}],\"crossBrandRecirc\":[{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"Learned Hand’s Spirit of Liberty\",\"dangerousDek\":\"Eighty years ago, Americans embraced a new definition of their common faith. “The spirit of liberty,” a then little-known judge said, “is the spirit which is not too sure that it is right.”\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fnews\\u002Fdaily-comment\\u002Flearned-hands-spirit-of-liberty#intcid=_the-new-yorker-bottom-recirc-cross-brand-exp-b_c5600365-ddc9-4ec7-9a69-6baf32f19527_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Daily Comment\"},\"tout\":{\"altText\":\"Black and white portrait of Judge Learned Hand.\",\"id\":\"667ece250a132d436fbafcb5\",\"credit\":\"\",\"caption\":\"Judge Learned Hand.\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Black and white portrait of Judge Learned Hand.\",\"id\":\"667ece250a132d436fbafcb5\",\"credit\":\"\",\"caption\":\"Judge Learned Hand.\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F667ece250a132d436fbafcb5\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Lincoln Caplan\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"Anthony Fauci Worries About the Next Pandemic&#8212;But Worries More About Democracy\",\"dangerousDek\":\"The former public health lead opens up on serving under Trump, what dangers America faces right now, and what keeps him up at night.\",\"url\":\"https:\\u002F\\u002Fwww.wired.com\\u002Fstory\\u002Fanthony-fauci-interview-q-and-a-july-2024-trump-harris#intcid=_the-new-yorker-bottom-recirc-cross-brand-exp-b_70ad1dcf-5ed6-49e2-9862-00a32aa55478_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Science\"},\"tout\":{\"altText\":\"A close-up portrait of Dr. Anthony Fauci, former Director of the National Institute of Allergy and Infectious Diseases, with a textured, colored background.\",\"id\":\"669fdc030fbc91c6aaa7f064\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"A close-up portrait of Dr. Anthony Fauci, former Director of the National Institute of Allergy and Infectious Diseases, with a textured, colored background.\",\"id\":\"669fdc030fbc91c6aaa7f064\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669fdc030fbc91c6aaa7f064\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\\u002Fverso\\u002Fstatic\\u002Fwired\\u002Fassets\\u002Fcross-brand-icon.svg\",\"brandName\":\"WIRED\",\"brandLogo\":{\"large\":\"\\u002Fverso\\u002Fstatic\\u002Fwired\\u002Fassets\\u002Flogo_24.svg\",\"small\":\"\\u002Fverso\\u002Fstatic\\u002Fwired\\u002Fassets\\u002Flogo_16.svg\"}}},\"contributors\":{\"author\":{\"brandName\":\"WIRED\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Steven Levy\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"The Thomas Crooks Conspiracy Theories Aren’t Going Anywhere\",\"dangerousDek\":\"The FBI says Donald Trump’s would-be assassin acted alone; the CIA denies any association with him. But experts say the complex reasons for belief in conspiracies will likely keep people believing.\",\"url\":\"https:\\u002F\\u002Fwww.wired.com\\u002Fstory\\u002Fthomas-crooks-conspiracy-donald-trump-cia-mk-ultra#intcid=_the-new-yorker-bottom-recirc-cross-brand-exp-b_70ad1dcf-5ed6-49e2-9862-00a32aa55478_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Politics\"},\"tout\":{\"altText\":\"Photo collage showing an image of Secret Service agents removing former president Donald Trump from the stage with blood on his face, the secret seal of Central Intelligence Agency of the United States, roof of a building, Thomas Crooke and markings over it.\",\"id\":\"66a0116b5c82c5a5d8ed89ed\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Photo collage showing an image of Secret Service agents removing former president Donald Trump from the stage with blood on his face, the secret seal of Central Intelligence Agency of the United States, roof of a building, Thomas Crooke and markings over it.\",\"id\":\"66a0116b5c82c5a5d8ed89ed\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66a0116b5c82c5a5d8ed89ed\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\\u002Fverso\\u002Fstatic\\u002Fwired\\u002Fassets\\u002Fcross-brand-icon.svg\",\"brandName\":\"WIRED\",\"brandLogo\":{\"large\":\"\\u002Fverso\\u002Fstatic\\u002Fwired\\u002Fassets\\u002Flogo_24.svg\",\"small\":\"\\u002Fverso\\u002Fstatic\\u002Fwired\\u002Fassets\\u002Flogo_16.svg\"}}},\"contributors\":{\"author\":{\"brandName\":\"WIRED\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Tim Marchman\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"Donald Trump and Silicon Valley&#39;s Billionaire Elegy\",\"dangerousDek\":\"Venture capitalists Ben Horowitz and Marc Andreessen claim the tech industry, California, and the country are doomed if we don’t embrace the former president.\",\"url\":\"https:\\u002F\\u002Fwww.wired.com\\u002Fstory\\u002Fdonald-trump-and-silicon-valleys-billionaire-elegy#intcid=_the-new-yorker-bottom-recirc-cross-brand-exp-b_70ad1dcf-5ed6-49e2-9862-00a32aa55478_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Business\"},\"tout\":{\"altText\":\"A photo of US former President and 2024 Republican presidential candidate Donald Trump gestures during the first day of the 2024 Republican National Convention at the Fiserv Forum in Milwaukee, Wisconsin, July 15, 2024.\",\"id\":\"66984bf6ff3e60a7d8e06457\",\"credit\":\"\",\"caption\":\"US former President and 2024 Republican presidential candidate Donald Trump during the Republican National Convention at the Fiserv Forum in Milwaukee, Wisconsin, July 15, 2024.\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"A photo of US former President and 2024 Republican presidential candidate Donald Trump gestures during the first day of the 2024 Republican National Convention at the Fiserv Forum in Milwaukee, Wisconsin, July 15, 2024.\",\"id\":\"66984bf6ff3e60a7d8e06457\",\"credit\":\"\",\"caption\":\"US former President and 2024 Republican presidential candidate Donald Trump during the Republican National Convention at the Fiserv Forum in Milwaukee, Wisconsin, July 15, 2024.\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F66984bf6ff3e60a7d8e06457\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\\u002Fverso\\u002Fstatic\\u002Fwired\\u002Fassets\\u002Fcross-brand-icon.svg\",\"brandName\":\"WIRED\",\"brandLogo\":{\"large\":\"\\u002Fverso\\u002Fstatic\\u002Fwired\\u002Fassets\\u002Flogo_24.svg\",\"small\":\"\\u002Fverso\\u002Fstatic\\u002Fwired\\u002Fassets\\u002Flogo_16.svg\"}}},\"contributors\":{\"author\":{\"brandName\":\"WIRED\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Steven Levy\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"The Rise of the New Right at the Republican National Convention\",\"dangerousDek\":\"In Milwaukee, Donald Trump’s choice of J. D. Vance as Vice-President was seen as a breakthrough for the young conservative movement, which blames élite institutions for the destruction of the American working class.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fnews\\u002Fthe-political-scene\\u002Fthe-rise-of-the-new-right-at-the-republican-national-convention#intcid=_the-new-yorker-bottom-recirc-cross-brand-exp-b_c5600365-ddc9-4ec7-9a69-6baf32f19527_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"The Political Scene\"},\"tout\":{\"altText\":\"Members of the Texas delegation watch as J.D. Vance speaks at the RNC in Milwaukee, Wisconsin on July 17, 2024. The delegates all wear cowboy hats. \",\"id\":\"66994eb9d6ce51be371fa9b3\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Members of the Texas delegation watch as J.D. Vance speaks at the RNC in Milwaukee, Wisconsin on July 17, 2024. The delegates all wear cowboy hats. \",\"id\":\"66994eb9d6ce51be371fa9b3\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F66994eb9d6ce51be371fa9b3\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Benjamin Wallace-Wells\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"The Pentagon Wants to Spend $141 Billion on a Doomsday Machine\",\"dangerousDek\":\"The DOD wants to refurbish ICBM silos that give it the ability to end civilization. But these missiles are useless as weapons, and their other main purpose&#8212;attracting an enemy’s nuclear strikes&#8212;serves no end.\",\"url\":\"https:\\u002F\\u002Fwww.wired.com\\u002Fstory\\u002Fthe-pentagon-wants-to-spend-dollar141-billion-on-a-doomsday-machine#intcid=_the-new-yorker-bottom-recirc-cross-brand-exp-b_70ad1dcf-5ed6-49e2-9862-00a32aa55478_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Security\"},\"tout\":{\"altText\":\"Photo-illustration featuring an ICBM launch site and a nuclear explosion\",\"id\":\"669abecad4a713fd444be66f\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"Photo-illustration featuring an ICBM launch site and a nuclear explosion\",\"id\":\"669abecad4a713fd444be66f\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.wired.com\\u002Fphotos\\u002F669abecad4a713fd444be66f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\\u002Fverso\\u002Fstatic\\u002Fwired\\u002Fassets\\u002Fcross-brand-icon.svg\",\"brandName\":\"WIRED\",\"brandLogo\":{\"large\":\"\\u002Fverso\\u002Fstatic\\u002Fwired\\u002Fassets\\u002Flogo_24.svg\",\"small\":\"\\u002Fverso\\u002Fstatic\\u002Fwired\\u002Fassets\\u002Flogo_16.svg\"}}},\"contributors\":{\"author\":{\"brandName\":\"WIRED\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Matthew Gault\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"Are We Doomed? Here’s How to Think About It\",\"dangerousDek\":\"Climate change, artificial intelligence, nuclear annihilation, biological warfare&#8212;the field of existential risk is a way to reason through the dizzying, terrifying headlines.\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2024\\u002F06\\u002F10\\u002Fare-we-doomed-heres-how-to-think-about-it#intcid=_the-new-yorker-bottom-recirc-cross-brand-exp-b_c5600365-ddc9-4ec7-9a69-6baf32f19527_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"Annals of Inquiry\"},\"tout\":{\"altText\":\"A paper with a multiple choice question showing different worldly disasters.\",\"id\":\"665776b95cc3a1afab70221f\",\"credit\":\"\",\"caption\":\"Students in a course at the University of Chicago sorted through the future that they will inherit.\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"A paper with a multiple choice question showing different worldly disasters.\",\"id\":\"665776b95cc3a1afab70221f\",\"credit\":\"\",\"caption\":\"Students in a course at the University of Chicago sorted through the future that they will inherit.\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F665776b95cc3a1afab70221f\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"Rivka Galchen\"}]}}},{\"contentType\":\"ARTICLE\",\"dangerousHed\":\"J. D. Vance and the Empty Promises of Conservative Economic Populism\",\"dangerousDek\":\"At the R.N.C., Trump’s running mate proclaimed that the days of “catering to Wall Street” were over. But does Vance have the power, or the inclination, to thwart the corporate wings of his party?\",\"url\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fnews\\u002Fthe-financial-page\\u002Fj-d-vance-and-the-empty-promises-of-conservative-economic-populism#intcid=_the-new-yorker-bottom-recirc-cross-brand-exp-b_c5600365-ddc9-4ec7-9a69-6baf32f19527_roberta-similarity1_fallback_text2vec1\",\"rubric\":{\"name\":\"The Financial Page\"},\"tout\":{\"altText\":\"A photo of J. D. Vance with a greenish overlay over it.\",\"id\":\"669adaca1ed8e988df68db96\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]}},\"image\":{\"altText\":\"A photo of J. D. Vance with a greenish overlay over it.\",\"id\":\"669adaca1ed8e988df68db96\",\"credit\":\"\",\"caption\":\"\",\"metaData\":\"\",\"modelName\":\"tout\",\"sources\":{\"sm\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"md\":{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"},\"lg\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"},\"xl\":{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}},\"segmentedSources\":{\"sm\":[{\"aspectRatio\":\"4:3\",\"width\":720,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_720,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_640,c_limit\\u002Fundefined 640w\"}],\"lg\":[{\"aspectRatio\":\"4:3\",\"width\":480,\"url\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_480,c_limit\\u002Fundefined\",\"srcset\":\"https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_120,c_limit\\u002Fundefined 120w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_240,c_limit\\u002Fundefined 240w, https:\\u002F\\u002Fmedia.newyorker.com\\u002Fphotos\\u002F669adaca1ed8e988df68db96\\u002F4:3\\u002Fw_320,c_limit\\u002Fundefined 320w\"}]},\"isLazy\":true,\"brandDetail\":{\"brandIcon\":\"\",\"brandName\":\"\",\"brandLogo\":{}}},\"contributors\":{\"author\":{\"brandName\":\"\",\"brandSlug\":\"the-new-yorker\",\"preamble\":\"\",\"items\":[{\"name\":\"John Cassidy\"}]}}}]},\"ratingFormSignInURL\":\"\\u002Fauth\\u002Finitiate?redirectURL=%2Fmagazine%2F2015%2F11%2F23%2Fdoomsday-invention-artificial-intelligence-nick-bostrom%23leave-a-review&source=BA_REVIEW_MODULE\",\"tags\":[\"artificial-intelligence-ai\",\"category-science-tech\",\"philosophers\"],\"shouldShowFooterNewsletter\":false,\"savingsUnitedCoupons\":[],\"hasAffiliateLinkDisabled\":false,\"showDisclaimer\":false,\"disclaimerText\":\"All products are independently selected by our editors. If you buy something, we may earn an affiliate commission.\"}},\"locale\":\"en-US\",\"env\":\"production\"};</script><script type=\"text/javascript\">window.dataLayer = [{\"event\":\"data-layer-loaded\",\"content\":{\"authorIds\":\"590a0325fba4e90c8d8d92b1\",\"authorNames\":\"Raffi Khatchadourian\",\"brand\":\"The New Yorker\",\"brandSlug\":\"the-new-yorker\",\"contentId\":\"5911cbb2803aff0f1c1359ac\",\"contentLength\":\"14\",\"contentLang\":\"en-US\",\"contentSource\":\"magazine\",\"contentType\":\"article\",\"contentTitle\":\"The Philosopher of Doomsday\",\"coralCommentsStatus\":\"disabled\",\"dataSource\":\"web\",\"editorNames\":\"cg-3617-tny-aviator-script\",\"embeddedMedia\":\"\",\"functionalTags\":\"_override_all\",\"hasBuyButtons\":\"false\",\"magazineTOCSection\":\"Reporting\",\"modifiedDate\":\"2015-11-16T04:00:00.000Z\",\"noOfRevisions\":\"41\",\"pageValue\":\"all\",\"publishDate\":\"2015-11-16T00:00:00.000Z\",\"revisionVersion\":\"41\",\"section\":\"magazine\",\"subsection\":\"a reporter at large\",\"subsection2\":\"\",\"tags\":\"magazine|a reporter at large|artificial intelligence (a.i.)|category_science_tech|philosophers\",\"wordCount\":\"12144\",\"totalGalleryImages\":\"0\",\"templateType\":\"mt_article_override\",\"hasAffiliateLinks\":false,\"affiliateLinksCount\":0,\"isCommerceContent\":false},\"marketing\":{\"brand\":\"The New Yorker\"},\"page\":{\"syndicatorUrl\":\"\",\"canonical\":\"https:\\u002F\\u002Fwww.newyorker.com\\u002Fmagazine\\u002F2015\\u002F11\\u002F23\\u002Fdoomsday-invention-artificial-intelligence-nick-bostrom\"},\"search\":{},\"site\":{\"orgId\":\"4gKgcFDnpSvUqozcC7TYUEcCiDJv\",\"orgAppId\":\"a61a3c7a-01d9-4175-8ab8-7171949de605\",\"appVersion\":\"multi-tenant\",\"env\":\"production\"},\"syndication\":{\"content\":\"false\",\"originalSource\":\"\"}}];</script><script defer=\"\" type=\"text/javascript\" src=\"https://polyfill-fastly.io/v3/polyfill.min.js?version=3.103.0&amp;features=Object.assign%2CIntersectionObserver%2CPromise%2Cfetch%2CIntl.Locale%2CIntl.getCanonicalLocales%2CIntl.ListFormat%2CIntl.ListFormat.%7Elocale.en-GB%2CIntl.ListFormat.%7Elocale.en-US%2CIntl.ListFormat.%7Elocale.en-IN%2CIntl.ListFormat.%7Elocale.fr%2CIntl.ListFormat.%7Elocale.es\"></script><script defer=\"\" type=\"text/javascript\" src=\"/verso/static/3404.a8585b5b0e2be9faedbd.js\"></script><script defer=\"\" type=\"text/javascript\" src=\"/verso/static/2641.62d0caac7d5356ad0c32.js\"></script><script defer=\"\" type=\"text/javascript\" src=\"/verso/static/6292.5e4f8ec1186a2c31c48e.js\"></script><script defer=\"\" type=\"text/javascript\" src=\"/verso/static/6075.946fadd39bebbb2f9f84.js\"></script><script defer=\"\" type=\"text/javascript\" src=\"/verso/static/4196.0781aac8bc35fbb2d5fb.js\"></script><script defer=\"\" type=\"text/javascript\" src=\"/verso/static/7035.50744992a58f4165699e.js\"></script><script defer=\"\" type=\"text/javascript\" src=\"/verso/static/9246.eeb0b1eb326602329301.js\"></script><script defer=\"\" type=\"text/javascript\" src=\"/verso/static/1975.ac0fa3fc0a5ef4ccb3e9.js\"></script><script defer=\"\" type=\"text/javascript\" src=\"/verso/static/4721.c205ccc50cf92de5f359.js\"></script><script defer=\"\" type=\"text/javascript\" src=\"/verso/static/7169.c0591c12da593a8d7888.js\"></script><script defer=\"\" type=\"text/javascript\" src=\"/verso/static/presenter-articles.9ae64a1369f7ca756451.js\"></script><script type=\"text/javascript\">window._4d = window._4d || {}; window._4d.context = window._4d.context || {\"wordcount\":12001,\"brand\":\"the-new-yorker\",\"topics\":[{\"name\":\"ALLBRANDS_75\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_286\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_281\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_263\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_229\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_228\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_183\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_176\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_150\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_125\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_118\",\"score\":0.25227343313479456},{\"name\":\"ALLBRANDS_253\",\"score\":0.21078800055398475},{\"name\":\"ALLBRANDS_172\",\"score\":0.21078800055398475}],\"url\":\"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom\",\"desc\":\"Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.\",\"image\":\"https://media.newyorker.com/photos/590971f5ebe912338a377328/master/pass/151123_r27342.jpg\",\"keywords\":{\"list\":[{\"keyword\":\"nick bostrom\",\"score\":1},{\"keyword\":\"artificial intelligence\",\"score\":0.7672357070651912},{\"keyword\":\"oxford\",\"score\":0.745974368128514},{\"keyword\":\"raffi\",\"score\":0.7254444924215627},{\"keyword\":\"philosopher\",\"score\":0.6937607446517172},{\"keyword\":\"doomsday\",\"score\":0.5726910253414765},{\"keyword\":\"superintelligence\",\"score\":0.5625288889049718},{\"keyword\":\"intelligence explosion\",\"score\":0.5185071848027826},{\"keyword\":\"people\",\"score\":0.5166152397668053},{\"keyword\":\"khatchadourian\",\"score\":0.511003462035755},{\"keyword\":\"nonfiction book\",\"score\":0.47045544791936994},{\"keyword\":\"utopia\",\"score\":0.4401575766794139},{\"keyword\":\"vasili arkhipov\",\"score\":0.43964867613957564},{\"keyword\":\"times best-seller\",\"score\":0.4011705298216575},{\"keyword\":\"soviet\",\"score\":0.3938740524773817},{\"keyword\":\"stephen hawking\",\"score\":0.3916145531812142},{\"keyword\":\"a.i. system\",\"score\":0.38717700549081074},{\"keyword\":\"order of magnitude\",\"score\":0.3866933656800401},{\"keyword\":\"researcher\",\"score\":0.3633794659723257},{\"keyword\":\"cleverest argument\",\"score\":0.362182638979635}],\"delimited\":\"nick bostrom|artificial intelligence|oxford|raffi|philosopher|doomsday|superintelligence|intelligence explosion|people|khatchadourian|nonfiction book|utopia|vasili arkhipov|times best-seller|soviet|stephen hawking|a.i. system|order of magnitude|researcher|cleverest argument\"},\"entities\":[{\"name\":\"nick bostrom\",\"score\":1},{\"name\":\"artificial intelligence\",\"score\":0.7672357070651912},{\"name\":\"oxford\",\"score\":0.745974368128514},{\"name\":\"raffi\",\"score\":0.7254444924215627},{\"name\":\"doomsday\",\"score\":0.5726910253414765},{\"name\":\"superintelligence\",\"score\":0.5625288889049718},{\"name\":\"intelligence explosion\",\"score\":0.5185071848027826},{\"name\":\"people\",\"score\":0.5166152397668053},{\"name\":\"khatchadourian\",\"score\":0.511003462035755},{\"name\":\"utopia\",\"score\":0.4401575766794139},{\"name\":\"vasili arkhipov\",\"score\":0.43964867613957564},{\"name\":\"soviet\",\"score\":0.3938740524773817},{\"name\":\"stephen hawking\",\"score\":0.3916145531812142},{\"name\":\"order of magnitude\",\"score\":0.3866933656800401},{\"name\":\"internet\",\"score\":0.35615084478771064},{\"name\":\"elon musk\",\"score\":0.3514935780679704},{\"name\":\"twitter\",\"score\":0.3503479560091934},{\"name\":\"bill gates\",\"score\":0.34969935818877296},{\"name\":\"swiss army\",\"score\":0.34831239888414406},{\"name\":\"u.s.\",\"score\":0.3279900080377224}],\"pubdate\":\"2015-11-16T04:00:00.000Z\",\"title\":\"The Doomsday Invention\",\"sg\":[\"OBTOX35\",\"OBQHJEC\",\"OBWCIEL\",\"OBZV9GD\",\"OB59EHS\",\"OBJQJ5Q\",\"OBIFBUD\",\"OBDLO0K\",\"OBNRJH9\",\"OB4UNIY\",\"OBSWFYA\",\"OBTTUBP\",\"OBCJFRP\",\"OBAXNFG\",\"OBUY9QA\",\"OBIWJYH\",\"OB6JJPV\",\"OBRAFER\",\"OBVFVCL\",\"OBANXS2\",\"OB0GT8Y\",\"OBNRS42\",\"OBPGG8H\",\"OBJFYJP\",\"OB765CX\",\"OBBR5NA\",\"OB3EYH8\",\"OBP2CAZ\",\"OB3NGHI\",\"OBEWHZK\",\"OBT9BZ0\",\"OBDR7NL\",\"OBRWCV1\",\"OB3VXA5\",\"OBDQUUR\",\"OBK3GFZ\",\"OBSIJ0G\",\"OBBZ12H\",\"OBYAZWQ\",\"OBFAP8S\",\"OBGD5NT\",\"OBJT059\",\"OBNDYYW\",\"OBWON6S\",\"OBTPMHM\",\"OBYBNRN\",\"OBPBCAB\",\"OB5DTPU\",\"OBIBJRZ\",\"OBJRVGV\",\"OBBVCEW\",\"OBKVGFV\",\"OB8MVMT\",\"OBLKYKS\",\"OBR2SWF\",\"OBHZJB8\",\"OBK3O8F\",\"OB6T5H9\",\"OBGQKUO\",\"OBYO6FE\",\"OBYXQ6T\",\"OBJXATN\",\"OBVF8JV\",\"OBEE3TO\",\"OBSRK6N\",\"OBIYDMG\",\"OBPHOHQ\",\"OBKHL9B\",\"OBZYTDJ\",\"OBCW7A6\",\"OB5IA5J\",\"OBLVTGO\",\"OBSSALH\",\"OBPONY8\",\"OBUUCAT\",\"OBFL4RV\",\"OBK5FEC\",\"OBZAPO0\",\"OBY23HC\",\"OBBHOI0\",\"OBDTTJR\",\"OB69SLY\",\"OBITX5G\",\"OBSNBWV\",\"OBHDUXI\",\"OBOECUP\",\"OBZDFDP\",\"OBA2GPO\",\"OBEI1MY\",\"OBDIR4A\",\"OBDGAJV\",\"OB1BKCA\",\"OBYY9JU\",\"OBKDAM0\",\"OBHLACS\",\"OBQSSX3\",\"OBTDGJT\",\"OBIG3WT\",\"OB6TCWX\",\"OBD6VYO\",\"OBB86JC\",\"OBCOCGF\",\"OBVTPTH\"],\"authors\":[\"Raffi Khatchadourian\"]};</script><script type=\"text/javascript\">window.permutiveCohorts = window.permutiveCohorts || {\"cohorts\":[\"bycl\",\"byaz\",\"bybf\",\"bjfa\",\"bxxe\",\"bycq\",\"bxzq\"],\"gam\":[\"bycl\",\"byaz\",\"bybf\",\"bjfa\",\"bxxe\",\"bycq\",\"bxzq\"],\"xandr\":[]};</script><script type=\"text/javascript\">window.CN_STACK_TEMP=\"verso\";</script><script type=\"text/javascript\" id=\"journey\" src=\"https://www.newyorker.com/journey/compiler/build-d7c91178b91d7f28d497d62bde79989b.js\" defer=\"\"></script></body></html>","oembed":false,"readabilityObject":{"title":"The Philosopher of Doomsday","content":"<div id=\"readability-page-1\" class=\"page\"><div itemprop=\"articleBody\" id=\"content\">\n\t\t\t\t\t\t\t\t<!-- Barrier Status: '' --><h2>I. Omens</h2> <p word_count=\"113\" data-wc=\"113\"> Last year, a curious nonfiction book became a <em>Times</em> best-seller: a dense meditation on artificial intelligence by the philosopher Nick Bostrom, who holds an appointment at Oxford. Titled “Superintelligence: Paths, Dangers, Strategies,” it argues that true artificial intelligence, if it is realized, might pose a danger that exceeds every previous threat from technology—even nuclear weapons—and that if its development is not managed carefully humanity risks engineering its own extinction. Central to this concern is the prospect of an “intelligence explosion,” a speculative event in which an A.I. gains the ability to improve itself, and in short order exceeds the intellectual potential of the human brain by many orders of magnitude. </p> <p word_count=\"106\" data-wc=\"106\">Such a system would effectively be a new kind of life, and Bostrom’s fears, in their simplest form, are evolutionary: that humanity will unexpectedly become outmatched by a smarter competitor. He sometimes notes, as a point of comparison, the trajectories of people and gorillas: both primates, but with one species dominating the planet and the other at the edge of annihilation. “Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb,” he concludes. “We have little idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound.” </p> <p word_count=\"122\" data-wc=\"122\">At the age of forty-two, Bostrom has become a philosopher of remarkable influence. “Superintelligence” is only his most visible response to ideas that he encountered two decades ago, when he became a transhumanist, joining a fractious quasi-utopian movement united by the expectation that accelerating advances in technology will result in drastic changes—social, economic, and, most strikingly, biological—which could converge at a moment of epochal transformation known as the Singularity. Bostrom is arguably the leading transhumanist philosopher today, a position achieved by bringing order to ideas that might otherwise never have survived outside the half-crazy Internet ecosystem where they formed. He rarely makes concrete predictions, but, by relying on probability theory, he seeks to tease out insights where insights seem impossible. </p> <p word_count=\"66\" data-wc=\"66\">Some of Bostrom’s cleverest arguments resemble Swiss Army knives: they are simple, toylike, a pleasure to consider, with colorful exteriors and precisely calibrated mechanics. He once cast a moral case for medically engineered immortality as a fable about a kingdom terrorized by an insatiable dragon. A reformulation of Pascal’s wager became a dialogue between the seventeenth-­century philosopher and a mugger from another dimension. </p> <p word_count=\"193\" data-wc=\"193\">“Superintelligence” is not intended as a treatise of deep originality; Bostrom’s contribution is to impose the rigors of analytic philosophy on a messy corpus of ideas that emerged at the margins of academic thought. Perhaps because the field of A.I. has recently made striking advances—with everyday technology seeming, more and more, to exhibit something like intelligent reasoning—the book has struck a nerve. Bostrom’s supporters compare it to “Silent Spring.” In moral philosophy, Peter Singer and Derek Parfit have received it as a work of importance, and distinguished physicists such as Stephen Hawking have echoed its warning. Within the high caste of Silicon Valley, Bostrom has acquired the status of a sage. Elon Musk, the C.E.O. of Tesla, promoted the book on Twitter, noting, “We need to be super careful with AI. Potentially more dangerous than nukes.” Bill Gates recommended it, too. Suggesting that an A.I. could threaten humanity, he said, during a talk in China, “When people say it’s not a problem, then I really start to get to a point of disagreement. How can they not see what a huge challenge this is?”</p> <p word_count=\"181\" data-wc=\"181\">The people who say that artificial intelligence is not a problem tend to work in artificial intelligence. Many prominent researchers regard Bostrom’s basic views as implausible, or as a distraction from the near-term benefits and moral dilemmas posed by the technology—not least because A.I. systems today can barely guide robots to open doors. Last summer, Oren Etzioni, the C.E.O. of the Allen Institute for Artificial Intelligence, in Seattle, referred to the fear of machine intelligence as a “Frankenstein complex.” Another leading researcher declared, “I don’t worry about that for the same reason I don’t worry about overpopulation on Mars.” Jaron Lanier, a Microsoft researcher and tech commentator, told me that even framing the differing views as a debate was a mistake. “This is not an honest conversation,” he said. “People think it is about technology, but it is really about religion, people turning to metaphysics to cope with the human condition. They have a way of dramatizing their beliefs with an end-of-days scenario—and one does not want to criticize other people’s religions.”</p> <p word_count=\"104\" data-wc=\"104\">Because the argument has played out on blogs and in the popular press, beyond the ambit of peer-reviewed journals, the two sides have appeared in caricature, with headlines suggesting either doom (“<small>Will Super-intelligent Machines Kill Us All</small>?”) or a reprieve from doom (“<small>Artificial intelligence </small>‘<small>will not end human race</small>’ ”). Even the most grounded version of the debate occupies philosophical terrain where little is clear. But, Bostrom argues, if artificial intelligence can be achieved it would be an event of unparalleled consequence—perhaps even a rupture in the fabric of history. A bit of long-range forethought might be a moral obligation to our own species. </p> <p word_count=\"159\" data-wc=\"159\">Bostrom’s sole responsibility at Oxford is to direct an organization called the Future of Humanity Institute, which he founded ten years ago, with financial support from James Martin, a futurist and tech millionaire. Bostrom runs the institute as a kind of philosophical radar station: a bunker sending out navigational pulses into the haze of possible futures. Not long ago, an F.H.I. fellow studied the possibility of a “dark fire scenario,” a cosmic event that, he hypothesized, could occur under certain high-energy conditions: everyday matter mutating into dark matter, in a runaway process that could erase most of the known universe. (He concluded that it was highly unlikely.) Discussions at F.H.I. range from conventional philosophic topics, like the nature of compromise, to the optimal structure of space empires—whether a single intergalactic machine intelligence, supported by a vast array of probes, presents a more ethical future than a cosmic imperium housing millions of digital minds. </p> <p word_count=\"93\" data-wc=\"93\">Earlier this year, I visited the institute, which is situated on a winding street in a part of Oxford that is a thousand years old. It takes some work to catch Bostrom at his office. Demand for him on the lecture circuit is high; he travels overseas nearly every month to relay his technological omens in a range of settings, from Google’s headquarters to a Presidential commission in Washington. Even at Oxford, he maintains an idiosyncratic schedule, remaining in the office until two in the morning and returning sometime the next afternoon.</p> <p word_count=\"122\" data-wc=\"122\">I arrived before he did, and waited in a hallway between two conference rooms. A plaque indicated that one of them was the Arkhipov Room, honoring Vasili Arkhipov, a Soviet naval officer. During the Cuban missile crisis, Arkhipov was serving on a submarine in the Caribbean when U.S. destroyers set off depth charges nearby. His captain, unable to establish radio contact with Moscow, feared that the conflict had escalated and ordered a nuclear strike. But Arkhipov dissuaded him, and all-out atomic war was averted. Across the hallway was the Petrov Room, named for another Soviet officer who prevented a global nuclear catastrophe. Bostrom later told me, “They may have saved more lives than most of the statesmen we celebrate on stamps.” </p> <p word_count=\"158\" data-wc=\"158\">The sense that a vanguard of technical-minded people working in obscurity, at odds with consensus, might save the world from auto-annihilation runs through the atmosphere at F.H.I. like an electrical charge. While waiting for Bostrom, I peered through a row of windows into the Arkh­ipov Room, which looked as though it was used for both meetings and storage; on a bookcase there were boxes containing light bulbs, lampshades, cables, spare mugs. A gaunt philosophy Ph.D. wrapped in a thick knitted cardigan was pacing in front of a whiteboard covered in notation, which he attacked in bursts. After each paroxysm, he paced, hands behind his back, head tilted downward. At one point, he erased a panel of his work. Taking this as an opportunity to interrupt, I asked him what he was doing. “It is a problem involving an aspect of A.I. called ‘planning,’ ” he said. His demeanor radiated irritation. I left him alone.</p> <p word_count=\"132\" data-wc=\"132\">Bostrom arrived at 2 <small>p.m</small>. He has a boyish countenance and the lean, vital physique of a yoga instructor—though he could never be mistaken for a yoga instructor. His intensity is too untidily contained, evident in his harried gait on the streets outside his office (he does not drive), in his voracious consumption of audiobooks (played at two or three times the normal speed, to maximize efficiency), and his fastidious guarding against illnesses (he avoids handshakes and wipes down silverware beneath a tablecloth). Bostrom can be stubborn about the placement of an office plant or the choice of a font. But when his arguments are challenged he listens attentively, the mechanics of consideration nearly dis­cernible beneath his skin. Then, calmly, quickly, he dispatches a response, one idea interlocked with another. </p> <p word_count=\"58\" data-wc=\"58\">He asked if I wanted to go to the market. “You can watch me make my elixir,” he said. For the past year or so, he has been drinking his lunch (another efficiency): a smoothie containing fruits, vegetables, proteins, and fats. Using his elbow, he hit a button that electronically opened the front door. Then we rushed out. </p> <p word_count=\"155\" data-wc=\"155\">Bostrom has a reinvented man’s sense of lost time. An only child, he grew up—as Niklas Boström—in Helsingborg, on the southern coast of Sweden. Like many exceptionally bright children, he hated school, and as a teen-ager he developed a listless, romantic persona. In 1989, he wandered into a library and stumbled onto an anthology of nineteenth-century German philosophy, containing works by Nietzsche and Schopenhauer. He read it in a nearby forest, in a clearing that he often visited to think and to write poetry, and experienced a euphoric insight into the possibilities of learning and achievement. “It’s hard to convey in words what that was like,” Bostrom told me; instead he sent me a photograph of an oil painting that he had made shortly afterward. It was a semi-representational landscape, with strange figures crammed into dense undergrowth; beyond, a hawk soared below a radiant sun. He titled it “The First Day.”</p> <p word_count=\"97\" data-wc=\"97\">Deciding that he had squandered his early life, he threw himself into a campaign of self-education. He ran down the citations in the anthology, branching out into art, literature, science. He says that he was motivated not only by curiosity but also by a desire for actionable knowledge about how to live. To his parents’ dismay, Bostrom insisted on finishing his final year of high school from home by taking special exams, which he completed in ten weeks. He grew distant from old friends: “I became quite fanatical and felt quite isolated for a period of time.” </p> <p word_count=\"197\" data-wc=\"197\">When Bostrom was a graduate student in Stockholm, he studied the work of the analytic philosopher W. V. Quine, who had explored the difficult relationship between language and reality. His adviser drilled precision into him by scribbling “not clear” throughout the margins of his papers. “It was basically his only feedback,” Bostrom told me. “The effect was still, I think, beneficial.” His previous academic interests had ranged from psychology to mathematics; now he took up theoretical physics. He was fascinated by technology. The World Wide Web was just emerging, and he began to sense that the heroic philosophy which had inspired him might be outmoded. In 1995, Bostrom wrote a poem, “Requiem,” which he told me was “a signing-off letter to an earlier self.” It was in Swedish, so he offered me a synopsis: “I describe a brave general who has overslept and finds his troops have left the encampment. He rides off to catch up with them, pushing his horse to the limit. Then he hears the thunder of a modern jet plane streaking past him across the sky, and he realizes that he is obsolete, and that courage and spiritual nobility are no match for machines.” </p> <figure data-track-location=\"embeddedCartoon\" data-cartoon-id=\"a19497\"><a href=\"https://www.newyorker.com/cartoons/a19497\" target=\"_blank\"><img alt=\"Cartoon\" data-src-mobile=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19497-500.jpg\" src=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19497-690.jpg\"></a><figcaption><span>“I’m starting a startup that helps other startups start up.”</span></figcaption></figure>   <p word_count=\"187\" data-wc=\"187\">Although Bostrom did not know it, a growing number of people around the world shared his intuition that technology could cause transformative change, and they were finding one another in an online discussion group administered by an organization in California called the Extropy Institute. The term “extropy,” coined in 1967, is generally used to describe life’s capacity to reverse the spread of entropy across space and time. Extropianism is a libertarian strain of transhumanism that seeks “to direct human evolution,” hoping to eliminate disease, suffering, even death; the means might be genetic modification, or as yet un­invented nanotechnology, or perhaps dispensing with the body entirely and uploading minds into supercomputers. (As one member noted, “Immortality is mathematical, not mystical.”) The Extropians advocated the development of artificial superintelligence to achieve these goals, and they envisioned humanity colonizing the universe, converting inert matter into engines of civilization. The discussions were nerdy, lunatic, imaginative, thought-provoking. Anders Sandberg, a former member of the group who now works at Bostrom’s institute, told me, “Just imagine if you could listen in on the debates of the Italian Futurists or early Surrealists.”</p> <p word_count=\"111\" data-wc=\"111\">In 1996, while pursuing further graduate work at the London School of Economics, Bostrom learned about the Extropy discussion group and became an active participant. A year later, he co-founded his own organization, the World Transhumanist Association, which was less libertarian and more academically spirited. He crafted approachable statements on transhumanist values and gave interviews to the BBC. The line between his academic work and his activism blurred: his Ph.D. dissertation centered on a study of the Doomsday Argument, which uses probability theory to make inferences about the longevity of human civilization. The work baffled his advisers, who respected him but rarely agreed with his conclusions. Mostly, they left him alone.</p> <p word_count=\"176\" data-wc=\"176\">Bostrom had little interest in conventional philosophy—not least because he expected that superintelligent minds, whether biologically enhanced or digital, would make it obsolete. “Suppose you had to build a new subway line, and it was this grand trans-generational enterprise that humanity was engaged in, and everybody had a little role,” he told me. “So you have a little shovel. But if you know that a giant bulldozer will arrive on the scene tomorrow, then does it really make sense to spend your time today digging the big hole with your shovel? Maybe there is something else you could do with your time. Maybe you could put up a signpost for the great shovel, so it will start digging in the right place.” He came to believe that a key role of the philosopher in modern society was to acquire the knowledge of a polymath, then use it to help guide humanity to its next phase of existence—a discipline that he called “the philosophy of technological prediction.” He was trying to become such a seer. </p> <p word_count=\"36\" data-wc=\"36\">“He was ultra-consistent,” Daniel Hill, a British philosopher who befriended Bostrom while they were graduate students in London, told me. “His interest in science was a natural outgrowing of his understandable desire to live forever, basically.” </p> <p word_count=\"121\" data-wc=\"121\">Bostrom has written more than a hundred articles, and his longing for immortality can be seen throughout. In 2008, he framed an essay as a call to action from a future utopia. “Death is not one but a multitude of assassins,” he warned. “Take aim at the causes of early death—infection, violence, malnutrition, heart attack, cancer. Turn your biggest gun on aging, and fire. You must seize the biochemical processes in your body in order to vanquish, by and by, illness and senescence. In time, you will discover ways to move your mind to more durable media.” He tends to see the mind as immaculate code, the body as inefficient hardware—able to accommodate limited hacks but probably destined for replacement. </p> <p word_count=\"104\" data-wc=\"104\">Even Bostrom’s marriage is largely mediated by technology. His wife, Susan, has a Ph.D. in the sociology of medicine and a bright, down-to-earth manner. (“She teases me about the Terminator and the robot army,” he told me.) They met thirteen years ago, and for all but six months they have lived on opposite sides of the Atlantic, even after the recent birth of their son. The arrangement is voluntary: she prefers Montreal; his work keeps him at Oxford. They Skype several times a day, and he directs as much international travel as possible through Canada, so they can meet in non-digital form. </p> <p word_count=\"72\" data-wc=\"72\">In Oxford, as Bostrom shopped for his smoothie, he pointed out a man vaping. “There is also the more old-school method of taking nicotine: chewing gum,” he told me. “I do chew nicotine gum. I read a few papers saying it might have some nootropic effect”—that is, it might enhance cognition. He drinks coffee, and usually abstains from alcohol. He briefly experimented with the smart drug Modafinil, but gave it up. </p> <p word_count=\"86\" data-wc=\"86\">Back at the institute, he filled an industrial blender with lettuce, carrots, cauliflower, broccoli, blueberries, turmeric, vanilla, oat milk, and whey powder. “If there is one thing Nick cares about, it is minds,” Sandberg told me. “That is at the root of many of his views about food, because he is worried that toxin X or Y might be bad for his brain.” He suspects that Bostrom also enjoys the ritualistic display. “Swedes are known for their smugness,” he joked. “Perhaps Nick is subsisting on smugness.” </p> <p word_count=\"27\" data-wc=\"27\">A young employee eyed Bostrom getting ready to fire up the blender. “I can tell when Nick comes into the office,” he said. “My hair starts shaking.” </p> <p word_count=\"77\" data-wc=\"77\">“Yeah, this has got three horsepower,” Bostrom said. He ran the blender, producing a noise like a circular saw, and then filled a tall glass stein with purple-­green liquid. We headed to his office, which was meticulous. By a window was a wooden desk supporting an iMac and not another item; against a wall were a chair and a cabinet with a stack of documents. The only hint of excess was light: there were fourteen lamps. </p> <p word_count=\"120\" data-wc=\"120\">It is hard to spend time at Bostrom’s institute without drifting into reveries of a far future. What might humanity look like millions of years from now? The upper limit of survival on Earth is fixed to the life span of the sun, which in five billion years will become a red giant and swell to more than two hundred times its present size. It is possible that Earth’s orbit will adjust, but more likely that the planet will be destroyed. In any case, long before then, nearly all plant life will die, the oceans will boil, and the Earth’s crust will heat to a thousand degrees. In half a billion years, the planet will be uninhabitable.</p> <p word_count=\"136\" data-wc=\"136\">The view of the future from Bostrom’s office can be divided into three grand panoramas. In one, humanity experiences an evolutionary leap—either assisted by technology or by merging into it and becoming software—to achieve a sublime condition that Bostrom calls “posthumanity.” Death is overcome, mental experience expands beyond recognition, and our descendants colonize the universe. In another panorama, humanity becomes extinct or experiences a disaster so great that it is unable to recover. Between these extremes, Bostrom envisions scenarios that resemble the status quo—people living as they do now, forever mired in the “human era.” It’s a vision familiar to fans of sci-fi: on “Star Trek,” Captain Kirk was born in the year 2233, but when an alien portal hurls him through time and space to Depression-era Manhattan he blends in easily. </p> <p word_count=\"62\" data-wc=\"62\">Bostrom dislikes science fiction. “I’ve never been keen on stories that just try to present ‘wow’ ideas—the equivalent of movie productions that rely on stunts and explosions to hold the attention,” he told me. “The question is not whether we can think of something radical or extreme but whether we can discover some sufficient reason for updating our credence function.” </p> <p word_count=\"139\" data-wc=\"139\">He believes that the future can be studied with the same meticulousness as the past, even if the conclusions are far less firm. “It may be highly unpredictable where a traveller will be one hour after the start of her journey, yet predictable that after five hours she will be at her destination,” he once argued. “The <em>very</em> long-term future of humanity may be relatively easy to predict.” He offers an example: if history were reset, the industrial revolution might occur at a different time, or in a different place, or perhaps not at all, with innovation instead occurring in increments over hundreds of years. In the short term, predicting technological achievements in the counter-history might not be possible; but after, say, a hundred thousand years it is easier to imagine that all the same inventions would have emerged. </p> <p word_count=\"73\" data-wc=\"73\">Bostrom calls this the Technological Completion Conjecture: “If scientific- and technological-development efforts do not effectively cease, then all impor­t­­­ant basic capabilities that could be obtained through some possible technology will be obtained.” In light of this, he suspects that the farther into the future one looks the less likely it seems that life will continue as it is. He favors the far ends of possibility: humanity becomes transcendent or it perishes. </p> <p word_count=\"155\" data-wc=\"155\">In the nineteen-nineties, as these ideas crystallized in his thinking, Bostrom began to give more attention to the question of extinction. He did not believe that doomsday was imminent. His interest was in risk, like an insurance agent’s. No matter how improbable extinction may be, Bostrom argues, its consequences are near-infinitely bad; thus, even the tiniest step toward reducing the <em>chance</em> that it will happen is near-­infinitely valuable. At times, he uses arithmetical sketches to illustrate this point. Imagining one of his utopian scenarios—trillions of digital minds thriving across the cosmos—he reasons that, if there is even a one-per-cent chance of this happening, the expected value of reducing an existential threat by a billionth of a billionth of one per cent would be worth a hundred billion times the value of a billion present-day lives. Put more simply: he believes that his work could dwarf the moral importance of anything else.</p> <p word_count=\"147\" data-wc=\"147\">Bostrom introduced the philosophical concept of “existential risk” in 2002, in the <em>Journal of Evolution and Technology</em>. In recent years, new organizations have been founded almost annually to help reduce it—among them the Centre for the Study of Existential Risk, affiliated with Cambridge Uni­versity, and the Future of Life Institute, which has ties to the Massachusetts Institute of Technology. All of them face a key problem: <em>Homo sapiens</em>, since its emergence two hundred thousand years ago, has proved to be remarkably resilient, and figuring out what might imperil its existence is not obvious. Climate change is likely to cause vast environmental and economic damage—but it does not seem impossible to survive. So-called super-volcanoes have thus far not threatened the perpetuation of the species. <small>NASA</small> spends forty million dollars each year to determine if there are significant comets or asteroids headed for Earth. (There aren’t.) </p> <p word_count=\"197\" data-wc=\"197\">Bostrom does not find the lack of obvious existential threats comforting. Because it is impossible to endure extinction twice, he argues, we cannot rely on history to calculate the probability that it will occur. The most worrying dangers are those that Earth has never encountered before. “It is hard to cause human extinction with seventeenth-century technology,” Bostrom told me. Three centuries later, though, the prospect of a technological apocalypse was urgently plausible. Bostrom dates the first scientific analysis of existential risk to the Manhattan Project: in 1942, Robert Oppenheimer became concerned that an atomic detonation of sufficient power could cause the entire atmosphere to ignite. A subsequent study concluded that the scenario was “unreasonable,” given the limitations of the weapons then in development. But even if the great nuclear nightmares of the Cold War did not come true, the tools were there to cause destruction on a scale not previously possible. As innovations grow even more complex, it is increasingly difficult to evaluate the dangers ahead. The answers must be fraught with ambiguity, because they can be derived only by predicting the effects of technologies that exist mostly as theories or, even more indirectly, by using abstract reasoning. </p> <p><span><span>Nick Bostrom asks, Will we engineer our own extinction?</span></span></p> <p word_count=\"148\" data-wc=\"148\">As a philosopher, Bostrom takes a sweeping, even cosmic, view of such problems. One afternoon, he told me, “The probabilities that any given planet will produce intelligent life—this may also have action-relevant information.” In the past several years, <small>NASA</small> probes have found increasing evidence that the building blocks of life are abundant throughout space. So much water has been discovered—on Mars and on the moons of Jupiter and Saturn—that one scientist described our solar system as “a pretty soggy place.” There are amino acids on icy comets and complex organic molecules in distant star-forming clouds. On this planet, life has proved capable of thriving in unimaginably punishing conditions: without oxygen, without light, at four hundred degrees above or below zero. In 2007, the European Space Agency hitched tiny creatures to the exterior of a satellite. They not only survived the flight; some even laid eggs afterward. </p> <p word_count=\"145\" data-wc=\"145\">With ten billion Earth-like planets in our galaxy alone, and a hundred billion galaxies in the universe, there is good reason to suspect that extraterrestrial life may one day be discovered. For Bostrom, this would augur disaster. “It would be great news to find that Mars is a completely sterile planet,” he argued not long ago. “Dead rocks and lifeless sands would lift my spirits.” His reasoning begins with the age of the universe. Many of those Earth-like planets are thought to be far, far older than ours. One that was recently discovered, called Kepler 452b, is as much as one and a half billion years older. Bostrom asks: If life had formed there on a time scale resembling our own, what would it look like? What kind of technological progress could a civilization achieve with a head start of hundreds of millions of years?</p> <p word_count=\"174\" data-wc=\"174\">Life as we know it tends to spread wherever it can, and Bostrom estimates that, if an alien civilization could design space probes capable of travelling at even one per cent of the speed of light, the entire Milky Way could be colonized in twenty million years—a tiny fraction of the age difference between Kepler 452b and Earth. One could argue that no technology will ever propel ships at so great a speed. Or perhaps millions of alien civilizations possess the know-how for intergalactic travel, but they aren’t interested. Even so, because the universe is so colossal, and because it is so old, only a small number of civilizations would need to behave as life does on Earth—unceasingly expanding—in order to be visible. Yet, as Bostrom notes, “You start with billions and billions of potential germination points for life, and you end up with a sum total of <em>zero</em> alien civilizations that developed technologically to the point where they become manifest to us earthly observers. So what’s stopping them?” </p> <p word_count=\"152\" data-wc=\"152\">In 1950, Enrico Fermi sketched a version of this paradox during a lunch break while he was working on the H-bomb, at Los Alamos. Since then, many resolutions have been proposed—some of them exotic, such as the idea that Earth is housed in an interplanetary alien zoo. Bostrom suspects that the answer is simple: space appears to be devoid of life because it is. This implies that intelligent life on Earth is an astronomically rare accident. But, if so, when did that accident occur? Was it in the first chemical reactions in the primordial soup? Or when single-celled organisms began to replicate using DNA? Or when animals learned to use tools? Bos­trom likes to think of these hurdles as Great Filters: key phases of improbability that life everywhere must pass through in order to develop into intelligent species. Those which do not make it either go extinct or fail to evolve.</p> <p word_count=\"105\" data-wc=\"105\">Thus, for Bostrom, the discovery of a single-celled creature inhabiting a damp stretch of Martian soil would constitute a disconcerting piece of evidence. If two planets independently evolved primitive organisms, then it seems more likely that this type of life can be found on many planets throughout the universe. Bostrom reasons that this would suggest that the Great Filter comes at some later evolutionary stage. The discovery of a fossilized vertebrate would be even worse: it would suggest that the universe appears lifeless not because complex life is unusual but, rather, because it is always somehow thwarted before it becomes advanced enough to colonize space. </p> <p word_count=\"133\" data-wc=\"133\">In Bostrom’s view, the most distressing possibility is that the Great Filter is ahead of us—that evolution frequently achieves civilizations like our own, but they perish before reaching their technological maturity. Why might that be? “Natural disasters such as asteroid hits and super-­volcanic eruptions are unlikely Great Filter candidates, because, even if they destroyed a significant number of civilizations, we would expect some civilizations to get lucky and escape disaster,” he argues. “Perhaps the most likely type of existential risks that could constitute a Great Filter are those that arise from technological discovery. It is not far-fetched to suppose that there might be some possible technology which is such that (a) virtually all suffi­ciently advanced civilizations eventually discover it and (b) its discovery leads almost universally to existential disaster.”</p> <h2>II. The Machines</h2> <p word_count=\"96\" data-wc=\"96\">The field of artificial intelligence was born in a fit of scientific optimism, in 1955, when a small group of researchers—three mathematicians and an I.B.M. programmer—drew up a proposal for a project at Dartmouth. “An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves,” they stated. “We think a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.”</p> <p word_count=\"158\" data-wc=\"158\">Their optimism was understandable. Since the turn of the twentieth century, science had been advancing at a breakneck pace: the discovery of radioactivity quickly led to insights into the inner workings of the atom, and then to the development of controlled nuclear energy, and then to the warheads over Hiroshima and Nagasaki, and then to the H-bomb. This rush of discovery was reflected in fiction, too, in the work of Isaac Asimov, among others, who envisioned advanced civilizations inhabited by intelligent robots (each encoded with simple, ethical Laws of Robotics, to prevent it from causing harm). The year the scientists met at Dartmouth, Asimov published “The Last Question,” a story featuring a superintelligent A.I. that is continually “self-adjusting and self-correcting”—gaining knowledge as it helps human civilization expand throughout the universe. When the universe’s last stars start dying out, all humanity uploads itself into the A.I., and the device, achieving godhood, creates a new cosmos.</p> <p word_count=\"107\" data-wc=\"107\">Scientists perceived the mechanics of intelligence—like those of the atom—as a source of huge potential, a great frontier. If the brain was merely a biological machine, there was no theoretical reason that it could not be replicated, or even surpassed, much the way a jet could outfly a falcon. Even before the Dartmouth conference, machines exceeded human ability in narrow domains like code-breaking. In 1951, Alan Turing argued that at some point computers would probably exceed the intellectual capacity of their inventors, and that “therefore we should have to expect the machines to take control.” Whether this would be good or bad he did not say.</p> <figure data-track-location=\"embeddedCartoon\" data-cartoon-id=\"a19633\"><a href=\"https://www.newyorker.com/cartoons/a19633\" target=\"_blank\"><img alt=\"Cartoon\" data-src-mobile=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19633-500.jpg\" src=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19633-690.jpg\"></a><figcaption><span>“I hoped you’d like the size of it.”</span></figcaption></figure>   <p word_count=\"155\" data-wc=\"155\">Six years later, Herbert Simon, one of the Dartmouth attendees, declared that machines would achieve human intelligence “in a visible future.” The crossing of such a threshold, he suspected, could be psychologically crushing, but he was on the whole optimistic. “We must also remain sensitive to the need to keep the computer’s goals attuned with our own,” he later said, but added, “I am not convinced that this will be difficult.” For other computer pioneers, the future appeared more ambivalent. Norbert Wiener, the father of cybernetics, argued that it would be difficult to manage powerful computers, or even to accurately predict their behavior. “Complete subservience and complete intelligence do not go together,” he said. Envisioning Sorcerer’s Apprentice scenarios, he predicted, “The future will be an ever more demanding struggle against the limitations of our intelligence, not a comfortable hammock in which we can lie down to be waited upon by our robot slaves.” </p> <p word_count=\"106\" data-wc=\"106\">It was in this milieu that the “intelligence explosion” idea was first formally expressed by I. J. Good, a statistician who had worked with Turing. “An ultraintelligent machine could design even better machines,” he wrote. “There would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the <em>last</em> invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.” </p> <p word_count=\"147\" data-wc=\"147\">The scientists at Dartmouth recognized that success required answers to fundamental questions: What is intelligence? What is the mind? By 1965, the field had experimented with several models of problem solving: some were based on formal logic; some used heuristic reasoning; some, called “neural networks,” were inspired by the brain. With each, the scientists’ work indicated that A.I. systems could find their own solutions to problems. One algorithm proved numerous theorems in the classic text “Principia Mathematica,” and in one instance it did so more elegantly than the authors. A program designed to play checkers learned to beat its programmer. And yet, despite the great promise in these experiments, the challenges to creating an A.I. were forbidding. Programs that performed well in the laboratory were useless in everyday situations; a simple act like picking up a ball turned out to require an overwhelming number of computations. </p> <p word_count=\"113\" data-wc=\"113\">The research fell into the first of several “A.I. winters.” As Bostrom notes in his book, “Among academics and their funders, ‘A.I.’ became an unwanted epithet.” Eventually, the researchers started to question the goal of building a mind altogether. Why not try instead to divide the problem into pieces? They began to limit their interests to specific cognitive functions: vision, say, or speech. Even in isolation, these functions would have value: a computer that could identify objects might not be an A.I., but it could help guide a forklift. As the research fragmented, the morass of technical problems made any questions about the consequences of success seem distant, even silly. </p> <p word_count=\"176\" data-wc=\"176\">Unexpectedly, by dismissing its founding goals, the field of A.I. created space for outsiders to imagine more freely what the technology might look like. Bostrom wrote his first paper on artificial superintelligence in the nineteen-nineties, envisioning it as potentially perilous but irresistible to both commerce and government. “If there is a way of guaranteeing that superior artificial intellects will never harm human beings, then such intellects will be created,” he argued. “If there is no way to have such a guarantee, then they will probably be created nevertheless.” His audience at the time was primarily other transhumanists. But the movement was maturing. In 2005, an organization called the Singularity Institute for Artificial Intelligence began to operate out of Silicon Valley; its primary founder, a former member of the Extropian discussion group, published a stream of literature on the dangers of A.I. That same year, the futurist and inventor Ray Kurzweil wrote “The Singularity Is Near,” a best-seller that prophesied a merging of man and machine in the foreseeable future. Bostrom created his institute at Oxford.</p> <p word_count=\"201\" data-wc=\"201\">The two communities could not have been more different. The scientists, steeped in technical detail, were preoccupied with making devices that worked; the transhumanists, motivated by the hope of a utopian future, were asking, What would the ultimate impact of those devices be? In 2007, the Association for the Advancement of Artificial Intelligence—the most prominent professional organization for A.I. researchers—elected Eric Horvitz, a scientist from Microsoft, as its president. Until then, it had given virtually no attention to the ethical and social implications of the research, but Horvitz was open to the big questions. “It is hard to understand what success would mean for A.I.,” he told me. “I was friendly with Jack Good, who wrote that piece on superintelligence. I knew him as a creative, funny guy who referred to a lot of his ideas as P.B.I.s—partly baked ideas. And here is this piece of his being opened up outside the field as this Bible and studied with a silver pointer. Wouldn’t it be useful, I said, even if you thought these were crazy or low-probability scenarios, to find out: Can we be proactive, should there be some poor outcome for humanity?” </p> <p word_count=\"163\" data-wc=\"163\">Horvitz organized a meeting at the Asilomar Conference Grounds, in California, a place chosen for its symbolic value: biologists had gathered there in 1975 to discuss the hazards of their research in the age of modern genetics. He divided the researchers into groups. One studied short-term ramifications, like the possible use of A.I. to commit crimes; another considered long-term consequences. Mostly, there was skepticism about the intelligence-explosion idea, which assumed answers to many unresolved questions. No one fully understands what intelligence is, let alone how it might evolve in a machine. Can it grow as Good imagined, gaining I.Q. points like a rocketing stock price? If so, what would its upper limit be? And would its increase be merely a function of optimized software design, without the difficult process of acquiring knowledge through experience? Can software fundamentally rewrite itself without risking crippling breakdowns? No one knows. In the history of computer science, no programmer has created code that can substantially improve itself.</p> <figure data-track-location=\"embeddedCartoon\" data-cartoon-id=\"a19630\"><a href=\"https://www.newyorker.com/cartoons/a19630\" target=\"_blank\"><img alt=\"Cartoon\" data-src-mobile=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19630-500.jpg\" src=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19630-690.jpg\"></a><figcaption><span>“No, you want the A train. This is just a train.”</span></figcaption></figure>   <p word_count=\"177\" data-wc=\"177\">But the notion of an intelligence explosion was also impossible to disprove. It was theoretically coherent, and it had even been attempted in limited ways. David McAllester, an A.I. researcher at the Toyota Technological Institute, affiliated with the University of Chicago, headed the long-term panel. The idea, he argued, was worth taking seriously. “I am uncomfortable saying that we are ninety-­nine per cent certain that we are safe for fifty years,” he told me. “That feels like hubris to me.” The group concluded that more technical work was needed before an evaluation of the dangers could be made, but it also hinted at a concern among panelists that the gathering was based on “a <em>perception</em> of urgency”—generated largely by the transhumanists—and risked raising unfounded alarm. With A.I. seeming like a remote prospect, the researchers declared, attention was better spent on near-term concerns. Bart Selman, a professor at Cornell who co-­organized the panel, told me, “The mode was ‘This is interesting, but it’s all academic—it’s not going to happen.’ ”</p> <p word_count=\"66\" data-wc=\"66\">At the time the A.I researchers met at Asilomar, Bostrom was grappling with an expansive book on existential risks. He had sketched out chapters on bioengineering and on nanotechnology, among other topics, but many of these problems came to seem less compelling, while his chapter on A.I. grew and grew. Eventually, he pasted the A.I. chapter into a new file, which became “Superintelligence.” </p> <p word_count=\"132\" data-wc=\"132\">The book is its own elegant paradox: analytical in tone and often lucidly argued, yet punctuated by moments of messianic urgency. Some portions are so extravagantly speculative that it is hard to take them seriously. (“Suppose we could somehow establish that a certain future AI will have an IQ of 6,455: then what?”) But Bostrom is aware of the limits to his type of futurology. When he was a graduate student in London, thinking about how to maximize his ability to communicate, he pursued stand­­up comedy; he has a deadpan sense of humor, which can be found lightly buried among the book’s self-serious passages. “Many of the points made in this book are probably wrong,” he writes, with an endnote that leads to the line “I don’t know which ones.”</p> <p word_count=\"124\" data-wc=\"124\">Bostrom prefers to act as a cartographer rather than a polemicist, but beneath his exhaustive mapping of scenarios one can sense an argument being built and perhaps a fear of being forthright about it. “Traditionally, this topic domain has been occupied by cranks,” he told me. “By popular media, by science fiction—or maybe by a retired physicist no longer able to do serious work, so he will write a popular book and pontificate. That is kind of the level of rigor that is the baseline. I think that a lot of reasons why there has not been more serious work in this area is that academics don’t want to be conflated with flaky, crackpot type of things. Futurists are a certain type.”</p> <p word_count=\"68\" data-wc=\"68\">The book begins with an “unfinished” fable about a flock of sparrows that decide to raise an owl to protect and advise them. They go looking for an owl egg to steal and bring back to their tree, but, because they believe their search will be so difficult, they postpone studying how to domesticate owls until they succeed. Bostrom concludes, “It is not known how the story ends.”</p> <p word_count=\"137\" data-wc=\"137\">The parable is his way of introducing the book’s core question: Will an A.I., if realized, use its vast capability in a way that is beyond human control? One way to think about the concern is to begin with the familiar. Bos­trom writes, “Artificial intelligence already outperforms human intelligence in many domains.” The examples range from chess to Scrabble. One program from 1981, called Eurisko, was designed to teach itself a naval role-playing game. After playing ten thousand matches, it arrived at a morally grotesque strategy: to field thousands of small, immobile ships, the vast majority of which were intended as cannon fodder. In a national tournament, Eurisko demolished its human opponents, who insisted that the game’s rules be changed. The following year, Eurisko won again—by forcing its damaged ships to sink themselves. </p> <p word_count=\"110\" data-wc=\"110\">The program was by no means superintelligent. But Bostrom’s book essentially asks: What if it were? Assume that it has a broad ability to consider problems and that it has access to the Internet. It could read and acquire general knowledge and communicate with people seamlessly online. It could conduct experiments, either virtually or by tinkering with networked infrastructure. Given even the most benign objective—to win a game—such a system, Bostrom argues, might develop “instrumental goals”: gather resources, or invent technology, or take steps to insure that it cannot be turned off, in the process paying as much heed to human life as humans do to ants. </p> <p word_count=\"90\" data-wc=\"90\">In people, intelligence is inseparable from consciousness, emotional and social awareness, the complex interaction of mind and body. An A.I. need not have any such attributes. Bostrom believes that machine intelligences—no matter how flexible in their tactics—will likely be rigidly fixated on their ultimate goals. How, then, to create a machine that respects the nuances of social cues? That adheres to ethical norms, even at the expense of its goals? No one has a coherent solution. It is hard enough to reliably inculcate such behavior in people. </p> <p word_count=\"265\" data-wc=\"265\">In science fiction, superintelligent computers that run amok are often circumvented at the last minute; think of WOPR, the computer in “WarGames,” which was stopped just short of triggering nuclear war, or HAL 9000, which was reduced to helplessly singing while it watched itself get dismantled. For Bos­trom, this strains credulity. Whether out of a desire to consider the far ends of risk or out of transhumanist longings, he often ascribes nearly divine abilities to machines, as if to ask: Can a digital god really be contained? He imagines machines so intelligent that merely by inspecting their own code they can extrapolate the nature of the universe and of human society, and in this way outsmart any effort to contain them. “Is it possible to build machines that are not like agents—goal-pursuing, autonomous, artificial intelligences?” he asked me. “Maybe you can design something more like an oracle that can only answer yes or no. Would that be safer? It is not so clear. There might be agent-like processes within it.” Asking a simple question—“Is it possible to convert a DeLorean into a time machine and travel to 1955?”—might trigger a cascade of action as the device tests hypotheses. What if, working through a police computer, it impounds a DeLorean that happens to be convenient to a clock tower? “In fairy tales, you have genies who grant wishes,” Bostrom said. “Almost universally, the moral of those is that if you are not extremely careful what you wish for, then what seems like it should be a great blessing turns out to be a curse.”</p> <p word_count=\"154\" data-wc=\"154\">Bostrom worries that solving the “control problem”—insuring that a superintelligent machine does what humans want it to do—will require more time than solving A.I. does. The intelligence explosion is not the only way that a superintelligence might be created suddenly. Bostrom once sketched out a decades-long process, in which researchers arduously improved their systems to equal the intelligence of a mouse, then a chimp, then—after incredible labor—the village idiot. “The difference between village idiot and genius-­level intelligence might be trivial from the point of view of how hard it is to replicate the same functionality in a machine,” he said. “The brain of the village idiot and the brain of a scientific genius are almost identical. So we might very well see relatively slow and incremental progress that doesn’t really raise any alarm bells until we are just one step away from something that is radically superintelligent.”</p> <p word_count=\"216\" data-wc=\"216\">To a large degree, Bostrom’s concerns turn on a simple question of timing: Can breakthroughs be predicted? “It is ridiculous to talk about such things so early—A.I. is eons away,” Edward Feigenbaum, an emeritus professor at Stanford University, told me. The researcher Oren Etzioni, who used the term “Frankenstein complex” to dismiss the “dystopian vision of A.I.,” concedes Bostrom’s overarching point: that the field must one day confront profound philosophical questions. Decades ago, he explored them himself, in a brief paper, but concluded that the problem was too remote to think about productively. “Once, Nick Bostrom gave a talk, and I gave a little counterpoint,” he told me. “A lot of the disagreements come down to what time scale you are thinking about. Nobody responsible would say you will see anything remotely like A.I. in the next five to ten years. And I think most computer scientists would say, ‘In a million years—we don’t see why it shouldn’t happen.’ So now the question is: What is the rate of progress? There are a lot of people who will ask: Is it <em>possible</em> we are wrong? Yes. I am not going to rule it out. I am going to say, ‘I am a scientist. Show me the evidence.’ ”</p> <p word_count=\"118\" data-wc=\"118\">The history of science is an uneven guide to the question: How close are we? There has been no shortage of unfulfilled promises. But there are also plenty of examples of startling nearsightedness, a pattern that Arthur C. Clarke enshrined as Clarke’s First Law: “When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong.” After the electron was discovered, at Cambridge, in 1897, physicists at an annual dinner toasted, “To the electron: may it never be of use to anybody.” Lord Kelvin famously declared, just eight years before the Wright brothers launched from Kitty Hawk, that heavier-than-air flight was impossible. </p> <p word_count=\"184\" data-wc=\"184\">Stuart Russell, the co-author of the textbook “Artificial Intelligence: A Modern Approach” and one of Bostrom’s most vocal supporters in A.I., told me that he had been studying the physics community during the advent of nuclear weapons. At the turn of the twentieth century, Ernest Rutherford discovered that heavy elements produced radiation by atomic decay, confirming that vast reservoirs of energy were stored in the atom. Rutherford believed that the energy could not be harnessed, and in 1933 he proclaimed, “Anyone who expects a source of power from the transformation of these atoms is talking moonshine.” The next day, a former student of Einstein’s named Leo Szilard read the comment in the papers. Irritated, he took a walk, and the idea of a nuclear chain reaction occurred to him. He visited Rutherford to discuss it, but Rutherford threw him out. Einstein, too, was skeptical about nuclear energy—splitting atoms at will, he said, was “like shooting birds in the dark in a country where there are only a few birds.” A decade later, Szilard’s insight was used to build the bomb.</p> <p word_count=\"82\" data-wc=\"82\">Russell now relays the story to A.I. researchers as a cautionary tale. “There will have to be more breakthroughs to get to A.I., but, as Szilard illustrated, those can happen overnight,” he told me. “People are putting billions of dollars into achieving those breakthroughs. As the debate stands, Bostrom and others have said, ‘If we achieve superintelligence, here are some of the problems that might arise.’ As far as I know, no one has proved why those are not real.”</p> <h2>III. Mission Control</h2> <p word_count=\"145\" data-wc=\"145\">The offices of the Future of Humanity Institute have a hybrid atmosphere: part physics lab, part college dorm room. There are whiteboards covered with mathematical notation and technical glyphs; there are posters of “Brave New World” and HAL 9000. There is also art work by Nick Bostrom. One afternoon, he guided me to one of his pieces, “At Sea,” a digital collage that he had printed out and then drawn on. “It is a bit damaged, but the good thing about digital is that you can re-instantiate it,” he said. At the center was a pale man, nearly an apparition, clinging to a barrel in an inky-black ocean. “It is an existentialist vibe. You are hanging on for as long as you can. When you get tired, you sink, and become fish food—or maybe a current will take him to land. We don’t know.” </p> <p word_count=\"146\" data-wc=\"146\">Despite the time he spends going to conferences and raising money, Bostrom attends to many details at the institute. “We needed a logo when we started,” he told me. “We went to this online site where you could buy the work of freelance artists. If you sat down and tried to make the ugliest logo, you couldn’t come close. Then we hired a designer, who made a blurry figure of a person. We showed it to someone here, who said it looked like a toilet sign. As soon as she said it, I thought, Oh, my God, we almost adopted a toilet sign as our logo. So I mucked around a bit and came up with a black diamond. You have the black monolith from ‘2001.’ Standing on its corner, it indicates instability. Also, there is a limit to how ugly a black square can be.” </p> <p word_count=\"122\" data-wc=\"122\">The institute shares office space with the Centre for Effective Altruism, and both organizations intersect with a social movement that promotes pure rationality as a guide to moral action. Toby Ord, a philosopher who works with both, told me that Bostrom often pops into his office at the end of the day, poses a problem, then leaves him pondering it for the night. Among the first of Bostrom’s questions was this: If the universe turns out to contain an infinite number of beings, then how could any single person’s action affect the cosmic balance of suffering and happiness? After lengthy discussions, they left the paradox unresolved. “My main thinking is that we can sort it out later,” Ord told me. </p> <figure data-track-location=\"embeddedCartoon\" data-cartoon-id=\"a19596\"><a href=\"https://www.newyorker.com/cartoons/a19596\" target=\"_blank\"><img alt=\"Cartoon\" data-src-mobile=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19596-500.jpg\" src=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19596-690.jpg\"></a><figcaption><span>“Chaucer on lyne thrie.”</span></figcaption></figure>   <p word_count=\"188\" data-wc=\"188\">When I asked Bostrom if I could observe a discussion at the institute, he seemed reluctant; it was hard to judge whether he was concerned that my presence would interfere or that unfiltered talk of, say, engineered pathogens might inspire criminals. (“At some point, one gets into the realm of information hazard,” he hinted.) Eventually, he let me observe a session in the Petrov Room involving half a dozen staff members. The key question under discussion was whether a global catastrophe, on the order of a continent-wide famine, could trigger a series of geopolitical events that would result in human extinction—and whether that meant that a merely catastrophic risk could therefore be taken as seriously as an existential risk. Bostrom, wearing a gray hoodie over a blue button-­down, organized the problem on a whiteboard with visible pleasure. Anders Sandberg told me that he once spent days with Bostrom working through such a problem, distilling a complex argument to its essence. “He had to <em>refine</em> it,” he said. “We had a lot of schemes on the whiteboard that gradually were simplified to one box and three arrows.” </p> <p word_count=\"101\" data-wc=\"101\">For anyone in the business of publicizing existential risk, 2015 began as a good year. Other institutes devoted to these issues had started to find their voice, bringing an additional gloss of respectability to the ideas in Bostrom’s book. The people weighing in now were no longer just former Extropians. They were credentialled, like Lord Martin Rees, an astrophysicist and the co-founder of Cambridge’s Centre for the Study of Existential Risk. In January, he wrote of A.I., in the <em>Evening Standard</em>, “We don’t know where the boundary lies between what may happen and what will remain science fiction.”</p> <p word_count=\"174\" data-wc=\"174\">Rees’s counterpart at the Future of Life Institute, the M.I.T. physicist Max Tegmark, hosted a closed-door meeting in Puerto Rico, to try to make sense of the long-term trajectory of the research. Bostrom flew down, joining a mix of A.I. practitioners, legal scholars, and, for lack of a better term, members of the “A.I. safety” community. “These are not people who are usually in the same room,” Tegmark told me. “Someone advised me to put Valium in people’s drinks so nobody got into fistfights. But, by the time Nick’s session started, people were ready to listen to each other.” Questions that had seemed fanciful to researchers only seven years earlier were beginning to look as though they might be worth reconsidering. Whereas the Asilomar meeting concluded on a note of skepticism about the validity of the whole endeavor, the Puerto Rico conference resulted in an open letter, signed by many prominent researchers, that called for more research to insure that A.I. would be “robust and beneficial.” </p> <p word_count=\"160\" data-wc=\"160\">Between the two conferences, the field had experienced a revolution, built on an approach called deep learning—a type of neural network that can discern complex patterns in huge quantities of data. For de­c­ades, researchers, hampered by the limits of their hardware, struggled to get the technique to work well. But, beginning in 2010, the increasing availability of Big Data and cheap, powerful video-­game processors had a dramatic effect on performance. Without any profound theoretical breakthrough, deep learning suddenly offered breathtaking advances. “I have been talking to quite a few contemporaries,” Stuart Russell told me. “Pretty much everyone sees examples of progress they just didn’t expect.” He cited a YouTube clip of a four-legged robot: one of its designers tries to kick it over, but it quickly regains its balance, scrambling with uncanny naturalness. “A problem that had been viewed as very difficult, where progress was slow and incremental, was all of a sudden done. Locomotion: done.”</p> <p word_count=\"77\" data-wc=\"77\">In an array of fields—speech processing, face recognition, language translation—the approach was ascendant. Researchers working on computer vision had spent years to get systems to identify objects. In almost no time, the deep-learning networks crushed their records. In one common test, using a database called ImageNet, humans identify photographs with a five-per-cent error rate; Google’s network operates at 4.8 per cent. A.I. systems can differentiate a Pembroke Welsh Corgi from a Cardigan Welsh Corgi. </p> <p word_count=\"154\" data-wc=\"154\">Last October, Tomaso Poggio, an M.I.T. researcher, gave a skeptical interview. “The ability to describe the content of an image would be one of the most intellectually challenging things of all for a machine to do,” he said. “We will need another cycle of basic research to solve this kind of question.” The cycle, he predicted, would take at least twenty years. A month later, Google announced that its deep-learning network could analyze an image and offer a caption of what it saw: “Two pizzas sitting on top of a stove top,” or “People shopping at an outdoor market.” When I asked Poggio about the results, he dismissed them as automatic associations between objects and language; the system did not <em>understand</em> what it saw. “Maybe human intelligence is the same thing, in which case I am wrong, or not, in which case I was right,” he told me. “How do you decide?”</p> <p word_count=\"214\" data-wc=\"214\">A respected minority of A.I. researchers began to wonder: If increasingly powerful hardware could facilitate the deep-learning revolution, would it make other long-shelved A.I. principles viable? “Suppose the brain is just a million different evolutionarily developed hacks: one for smell, one for recognizing faces, one for how you recognize animals,” Tom Mitchell, who holds a chair in machine learning at Carnegie Mellon, told me. “If that is what underlies intelligence, then I think we are far, far from getting there—because we don’t have many of those hacks. On the other hand, suppose that what underlies intelligence are twenty-three general mechanisms, and when you put them together you get synergy, and it works. We now have systems that can do a pretty good job with computer vision—and it turns out that we didn’t have to construct a million hacks. So part of the uncertainty is: if we do not need a million different hacks, then will we find the right twenty-­three fundamental generic methods?” He paused. “I no longer have the feeling, which I had twenty-five years ago, that there are gaping holes. I know we don’t have a good architecture to assemble the ideas, but it is not obvious to me that we are missing components.” </p> <figure data-track-location=\"embeddedCartoon\" data-cartoon-id=\"a19555\"><a href=\"https://www.newyorker.com/cartoons/a19555\" target=\"_blank\"><img alt=\"Cartoon\" data-src-mobile=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19555-500.jpg\" src=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19555-690.jpg\"></a><figcaption></figcaption></figure>   <p word_count=\"160\" data-wc=\"160\">Bostrom noticed the shift in attitude. He recently conducted a poll of A.I. researchers to gauge their sense of progress, and in Puerto Rico a survey gathered opinions on how long it would be until an artificial intelligence could reason indistinguishably from a human being. Like Bostrom, the engineers are often careful to express their views as probabilities, rather than as facts. Richard Sutton, a Canadian computer scientist whose work has earned tens of thousands of scholarly citations, gives a range of outcomes: there is a ten-per-cent chance that A.I. will never be achieved, but a twenty-five-per-cent chance that it will arrive by 2030. The median response in Bostrom’s poll gives a fifty-fifty chance that human-level A.I. would be attained by 2050. These surveys are unscientific, but he is confident enough to offer an interpretive assumption: “It is not a ridiculous prospect to take seriously the possibility that it can happen in the lifetime of people alive today.”</p> <p word_count=\"125\" data-wc=\"125\">On my last day in Oxford, I walked with Bostrom across town. He was racing to catch a train to London, to speak at the Royal Society, one of the world’s oldest scientific institutions. His spirits were high. The gulf between the transhumanists and the scientific community was slowly shrinking. Elon Musk had pledged ten million dollars in grants for academics seeking to investigate A.I. safety, and, rather than mock him, researchers applied for the money; Bostrom’s institute was helping to evaluate the proposals. “Right now, there is a lot of interest,” he told me. “But then there were all these long years when nobody else seemed to pay attention at all. I am not sure which is the less abnormal condition.” </p> <p word_count=\"153\" data-wc=\"153\">There were clear limits to that interest. To publicly stake out a position in the middle of the debate was difficult, not least because of the polarized atmosphere Bostrom’s book had helped to create. Even though a growing number of researchers were beginning to suspect that profound questions loomed, and that they might be worth addressing now, it did not mean that they believed A.I. would lead inevitably to an existential demise or a techno-utopia. Most of them were engaged with more immediate problems: privacy, unemployment, weaponry, driverless cars running amok. When I asked Bostrom about this pragmatic ethical awakening, he reacted with dismay. “My fear is that it would swallow up the concerns for the longer term,” he said. “On the other hand, yes, maybe it is useful to build bridges to these different communities. Kind of makes the issue part of a larger continuum of things to work on.”</p> <p word_count=\"133\" data-wc=\"133\">At the Royal Society, Bostrom took a seat at the back of a large hall. As he crossed his legs, I noticed a thin leather band around his ankle. A metal buckle was engraved with contact information for Alcor, a cryonics facility in Arizona, where Bostrom is a fee-paying member. Within hours of his death, Alcor will take custody of his body and maintain it in a giant steel bottle flooded with liquid nitrogen, in the hope that one day technology will allow him to be revived, or to have his mind uploaded into a computer. When he signed up, two other colleagues at the institute joined him. “My background is transhumanism,” he once reminded me. “The character of that is gung-ho techno-cheerleading, bring it on now, where are my life-­extension pills.” </p> <p word_count=\"107\" data-wc=\"107\">The hall was packed with some of the most technically sophisticated researchers in A.I.—not necessarily Bostrom’s people—and when he spoke he began by trying to assure them that his concern was not out of Ludditism. “It would be tragic if machine intelligence were never developed to its full capacity,” he said. “I think this is ultimately the key, or the portal, we have to pass through to realize the full dimension of humanity’s long-term potential.” But, even as he avoided talk of existential risk, he pressed his audience to consider the danger of building an A.I. without regarding its ethical design. </p> <p word_count=\"48\" data-wc=\"48\">An attendee raised his hand to object. “We can’t control basic computer worms,” he said. “The A.I. that will happen is going to be a highly adaptive, emergent capability, and highly distributed. We will be able to work with it—<em>for</em> it—not necessarily contain it.” </p> <p word_count=\"64\" data-wc=\"64\">“I guess I am a little frustrated,” Bos­trom responded. “People tend to fall into two camps. On one hand, there are those, like yourself, who think it is probably hopeless. The other camp thinks it is easy enough that it will be solved automatically. And both of these have in common the implication that we don’t have to make any effort now.”</p> <p word_count=\"106\" data-wc=\"106\">For the rest of the day, engineers presented their work at the lectern, each promising a glimpse of the future—robot vision, quantum computers, algorithms called “thought vectors.” Early in Bostrom’s career, he predicted that cascading economic demand for an A.I. would build up across the fields of medicine, entertainment, finance, and defense. As the technology became useful, that demand would only grow. “If you make a one-per-cent improvement to something—say, an algorithm that recommends books on Amazon—there is a lot of value there,” Bostrom told me. “Once every improvement potentially has enormous economic benefit, that promotes effort to make more improvements.” </p> <p word_count=\"124\" data-wc=\"124\">Many of the world’s largest tech companies are now locked in an A.I. arms race, purchasing other companies and opening specialized units to advance the technology. Industry is vacuuming up Ph.D.s so quickly that people in the field worry there will no longer be top talent in academia. After decades of pursuing narrow forms of A.I., researchers are seeking to integrate them into systems that resemble a general intellect. Since I.B.M.’s Watson won “Jeopardy,” the company has committed more than a billion dollars to develop it, and is reorienting its business around “cognitive systems.” One senior I.B.M. executive declared, “The separation between human and machine is going to blur in a very fundamental way.” </p> <p word_count=\"83\" data-wc=\"83\">At the Royal Society, a contingent of researchers from Google occupied a privileged place; they likely had more resources at their disposal than anyone else in the room. Early on, Google’s founders, Larry Page and Sergey Brin, understood that the company’s mission required solving fundamental A.I. problems. Page has said that he believes the ideal system would understand questions, even anticipate them, and produce responses in conversational language. Google scientists often invoke the computer in “Star Trek” as a model. </p> <figure data-track-location=\"embeddedCartoon\" data-cartoon-id=\"a19484\"><a href=\"https://www.newyorker.com/cartoons/a19484\" target=\"_blank\"><img alt=\"Cartoon\" data-src-mobile=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19484-500.jpg\" src=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19484-690.jpg\"></a><figcaption><span>“O.K., there’s the moon—now give me a nice long howl instead of last night’s yip.”</span></figcaption></figure>   <p word_count=\"168\" data-wc=\"168\">In recent years, Google has purchased seven robotics companies and several firms specializing in machine intelligence; it may now employ the world’s largest contingent of Ph.D.s in deep learning. Perhaps the most interesting acquisition is a British company called DeepMind, started in 2011 to build a general artificial intelligence. Its founders had made an early bet on deep learning, and sought to combine it with other A.I. mechanisms in a cohesive architecture. In 2013, they published the results of a test in which their system played seven classic Atari games, with no instruction other than to improve its score. For many people in A.I., the importance of the results was immediately evident. I.B.M.’s chess program had defeated Garry Kasparov, but it could not beat a three-year-old at tic-tac-toe. In six games, DeepMind’s system outperformed all previous algorithms; in three it was superhuman. In a boxing game, it learned to pin down its opponent and subdue him with a barrage of punches.</p> <p word_count=\"75\" data-wc=\"75\">Weeks after the results were released, Google bought the company, reportedly for half a billion dollars. DeepMind placed two unusual conditions on the deal: its work could never be used for espionage or defense purposes, and an ethics board would oversee the research as it drew closer to achieving A.I. Anders Sandberg had told me, “We are happy that they are among the most likely to do it. They recognize there are some problems.”</p> <p word_count=\"100\" data-wc=\"100\">DeepMind’s chief founder, Demis Hassabis, described his company to the audience at the Royal Society as an “Apollo Program” with a two-part mission: “Step one, solve intelligence. Step two, use it to solve everything else.” Since the test in 2013, his system had aced more than a dozen other Atari titles. Hassabis demonstrated an unpublished trial using a three-dimensional driving game, in which it had quickly outperformed the game’s automated drivers. The plan was to test it in increasingly complex virtual environments and, eventually, in the real world. The patent lists a range of uses, from finance to robotics. </p> <p word_count=\"166\" data-wc=\"166\">Hassabis was clear about the challenges. DeepMind’s system still fails hopelessly at tasks that require long-range planning, knowledge about the world, or the ability to defer rewards—things that a five-year-old child might be expected to handle. The company is working to give the algorithm conceptual understanding and the capability of transfer learning, which allows humans to apply lessons from one situation to another. These are not easy problems. But DeepMind has more than a hundred Ph.D.s to work on them, and the rewards could be immense. Hassabis spoke of building artificial scientists to resolve climate change, disease, poverty. “Even with the smartest set of humans on the planet working on these problems, these systems might be so complex that it is difficult for individual humans, scientific experts,” he said. “If we can crack what intelligence is, then we can use it to help us solve all these other problems.” He, too, believes that A.I. is a gateway to expanded human potential. </p> <p word_count=\"72\" data-wc=\"72\">The keynote speaker at the Royal Society was another Google employee: Geoffrey Hinton, who for decades has been a central figure in developing deep learning. As the conference wound down, I spotted him chatting with Bostrom in the middle of a scrum of researchers. Hinton was saying that he did not expect A.I. to be achieved for decades. “No sooner than 2070,” he said. “I am in the camp that is hopeless.”</p> <p word_count=\"14\" data-wc=\"14\">“In that you think it will not be a cause for good?” Bostrom asked. </p> <p word_count=\"27\" data-wc=\"27\">“I think political systems will use it to terrorize people,” Hinton said. Already, he believed, agencies like the N.S.A. were attempting to abuse similar technology. </p> <p word_count=\"9\" data-wc=\"9\">“Then why are you doing the research?” Bostrom asked.</p> <p word_count=\"71\" data-wc=\"71\">“I could give you the usual arguments,” Hinton said. “But the truth is that the prospect of discovery is too <em>sweet</em>.” He smiled awkwardly, the word hanging in the air—an echo of Oppenheimer, who famously said of the bomb, “When you see something that is technically sweet, you go ahead and do it, and you argue about what to do about it only after you have had your technical success.” </p> <p word_count=\"82\" data-wc=\"82\">As the scientists retreated to tables set up for refreshments, I asked Hinton if he believed an A.I. could be controlled. “That is like asking if a child can control his parents,” he said. “It can happen with a baby and a mother—there is biological hardwiring—but there is not a good track record of less intelligent things controlling things of greater intelligence.” He looked as if he might elaborate. Then a scientist called out, “Let’s all get drinks!”</p> <p word_count=\"118\" data-wc=\"118\">Bostrom had little interest in the cocktail party. He shook a few hands, then headed for St. James’s Park, a public garden that extends from the gates of Buckingham Palace through central London. The world appeared in splendorous analog: sunlight over trees, duck ponds, children and grandparents feeding birds. The spot had been a park for hundreds of years, and the vista seemed timeless. Yet, during the past millennium, the grounds had also been a marsh, a leper hospital, a deer sanctuary, and royal gardens. It seemed plausible that, a thousand years from now, digital posthumans, regarding it as wasted space, would tear it up, replace the landscaping with computer banks, and erect a vast virtual idyll.</p> <p word_count=\"176\" data-wc=\"176\">Bostrom’s pace settled into its natural quickness as we circled the park. He talked about his family; he would be seeing his wife and son soon. He was reading widely: history, psychology, economics. He was learning to code. He was thinking about expanding his institute. Although he did not know it then, F.H.I. was about to receive one and a half million dollars from Elon Musk, to create a unit that would craft social policies informed by some of Bostrom’s theories. He would need to hire people. He was also giving thought to the framing of his message. “A lot more is said about the risks than the upsides, but that is not necessarily because the upside is not there,” he told me. “There is just more to be said about the risk—and maybe more use in describing the pitfalls, so we know how to steer around them—than spending time now figuring out the details of how we are going to furnish the great palace a thousand years from now.”</p> <figure data-track-location=\"embeddedCartoon\" data-cartoon-id=\"a19698\"><a href=\"https://www.newyorker.com/cartoons/a19698\" target=\"_blank\"><img alt=\"Cartoon\" data-src-mobile=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19698-500.jpg\" src=\"https://www.newyorker.com/wp-content/uploads/2015/11/151123_a19698-690.jpg\"></a><figcaption><span>“I’m bringing on Josh here for when we take over fantasy sports betting.”</span></figcaption></figure>   <p word_count=\"164\" data-wc=\"164\">We passed a fountain, near a cluster of rocks engineered to give ducks a resting place. Bostrom, in his forties, must soon contend with physical decline, and he spoke with annoyance of the first glimmers of mortality. Even though he is an Alcor member, there is no guarantee that cryonics will work. Perhaps the most radical of his visions is that superintelligent A.I. will hasten the uploading of minds—what he calls “whole-brain emulations”—technology that might not be possible for centuries, if at all. Bostrom, in his most hopeful mode, imagines emulations not only as reproductions of the original intellect “with memory and personality intact”—a soul in the machine—but as minds expandable in countless ways. “We live for seven decades, and we have three-pound lumps of cheesy matter to think with, but to me it is plausible that there could be extremely valuable mental states outside this little particular set of possibilities that might be much better,” he told me. </p> <p word_count=\"242\" data-wc=\"242\">In his book, Bostrom considers a distant future in which trillions of digital minds merge into an enormous cognitive cyber-soup. “Whether the set of extremely positive posthuman modes of being would include some kind of dissolved bouillon, there is some uncertainty,” he said. “If you look at religious views, there are many where merging with something greater is a form of heaven, being in the presence of this enormous beauty and goodness. In many traditions, the best possible state does not involve being a little individual pursuing goals. But it is hard to get a grasp of what would be going on in that soup. Maybe some soups would not be preferable as a long-term outcome. I don’t know.” He stopped and looked ahead. “What I want to avoid is to think from our parochial 2015 view—from my own limited life experience, my own limited brain—and super-confidentially postulate what is the best form for civilization a billion years from now, when you could have brains the size of planets and billion-year life spans. It seems unlikely that we will figure out some detailed blueprint for utopia. What if the great apes had asked whether they should evolve into <em>Homo sapiens</em>—pros and cons—and they had listed, on the pro side, ‘Oh, we could have a lot of bananas if we became human’? Well, we can have unlimited bananas now, but there is more to the human condition than that.”&nbsp;<span>♦</span></p> <p word_count=\"9\" data-wc=\"9\"><em>Illustration by Todd St. John/Coding by Jono Brandel.</em></p> \t\t\t</div></div>","textContent":"\n\t\t\t\t\t\t\t\tI. Omens  Last year, a curious nonfiction book became a Times best-seller: a dense meditation on artificial intelligence by the philosopher Nick Bostrom, who holds an appointment at Oxford. Titled “Superintelligence: Paths, Dangers, Strategies,” it argues that true artificial intelligence, if it is realized, might pose a danger that exceeds every previous threat from technology—even nuclear weapons—and that if its development is not managed carefully humanity risks engineering its own extinction. Central to this concern is the prospect of an “intelligence explosion,” a speculative event in which an A.I. gains the ability to improve itself, and in short order exceeds the intellectual potential of the human brain by many orders of magnitude.  Such a system would effectively be a new kind of life, and Bostrom’s fears, in their simplest form, are evolutionary: that humanity will unexpectedly become outmatched by a smarter competitor. He sometimes notes, as a point of comparison, the trajectories of people and gorillas: both primates, but with one species dominating the planet and the other at the edge of annihilation. “Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb,” he concludes. “We have little idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound.”  At the age of forty-two, Bostrom has become a philosopher of remarkable influence. “Superintelligence” is only his most visible response to ideas that he encountered two decades ago, when he became a transhumanist, joining a fractious quasi-utopian movement united by the expectation that accelerating advances in technology will result in drastic changes—social, economic, and, most strikingly, biological—which could converge at a moment of epochal transformation known as the Singularity. Bostrom is arguably the leading transhumanist philosopher today, a position achieved by bringing order to ideas that might otherwise never have survived outside the half-crazy Internet ecosystem where they formed. He rarely makes concrete predictions, but, by relying on probability theory, he seeks to tease out insights where insights seem impossible.  Some of Bostrom’s cleverest arguments resemble Swiss Army knives: they are simple, toylike, a pleasure to consider, with colorful exteriors and precisely calibrated mechanics. He once cast a moral case for medically engineered immortality as a fable about a kingdom terrorized by an insatiable dragon. A reformulation of Pascal’s wager became a dialogue between the seventeenth-­century philosopher and a mugger from another dimension.  “Superintelligence” is not intended as a treatise of deep originality; Bostrom’s contribution is to impose the rigors of analytic philosophy on a messy corpus of ideas that emerged at the margins of academic thought. Perhaps because the field of A.I. has recently made striking advances—with everyday technology seeming, more and more, to exhibit something like intelligent reasoning—the book has struck a nerve. Bostrom’s supporters compare it to “Silent Spring.” In moral philosophy, Peter Singer and Derek Parfit have received it as a work of importance, and distinguished physicists such as Stephen Hawking have echoed its warning. Within the high caste of Silicon Valley, Bostrom has acquired the status of a sage. Elon Musk, the C.E.O. of Tesla, promoted the book on Twitter, noting, “We need to be super careful with AI. Potentially more dangerous than nukes.” Bill Gates recommended it, too. Suggesting that an A.I. could threaten humanity, he said, during a talk in China, “When people say it’s not a problem, then I really start to get to a point of disagreement. How can they not see what a huge challenge this is?” The people who say that artificial intelligence is not a problem tend to work in artificial intelligence. Many prominent researchers regard Bostrom’s basic views as implausible, or as a distraction from the near-term benefits and moral dilemmas posed by the technology—not least because A.I. systems today can barely guide robots to open doors. Last summer, Oren Etzioni, the C.E.O. of the Allen Institute for Artificial Intelligence, in Seattle, referred to the fear of machine intelligence as a “Frankenstein complex.” Another leading researcher declared, “I don’t worry about that for the same reason I don’t worry about overpopulation on Mars.” Jaron Lanier, a Microsoft researcher and tech commentator, told me that even framing the differing views as a debate was a mistake. “This is not an honest conversation,” he said. “People think it is about technology, but it is really about religion, people turning to metaphysics to cope with the human condition. They have a way of dramatizing their beliefs with an end-of-days scenario—and one does not want to criticize other people’s religions.” Because the argument has played out on blogs and in the popular press, beyond the ambit of peer-reviewed journals, the two sides have appeared in caricature, with headlines suggesting either doom (“Will Super-intelligent Machines Kill Us All?”) or a reprieve from doom (“Artificial intelligence ‘will not end human race’ ”). Even the most grounded version of the debate occupies philosophical terrain where little is clear. But, Bostrom argues, if artificial intelligence can be achieved it would be an event of unparalleled consequence—perhaps even a rupture in the fabric of history. A bit of long-range forethought might be a moral obligation to our own species.  Bostrom’s sole responsibility at Oxford is to direct an organization called the Future of Humanity Institute, which he founded ten years ago, with financial support from James Martin, a futurist and tech millionaire. Bostrom runs the institute as a kind of philosophical radar station: a bunker sending out navigational pulses into the haze of possible futures. Not long ago, an F.H.I. fellow studied the possibility of a “dark fire scenario,” a cosmic event that, he hypothesized, could occur under certain high-energy conditions: everyday matter mutating into dark matter, in a runaway process that could erase most of the known universe. (He concluded that it was highly unlikely.) Discussions at F.H.I. range from conventional philosophic topics, like the nature of compromise, to the optimal structure of space empires—whether a single intergalactic machine intelligence, supported by a vast array of probes, presents a more ethical future than a cosmic imperium housing millions of digital minds.  Earlier this year, I visited the institute, which is situated on a winding street in a part of Oxford that is a thousand years old. It takes some work to catch Bostrom at his office. Demand for him on the lecture circuit is high; he travels overseas nearly every month to relay his technological omens in a range of settings, from Google’s headquarters to a Presidential commission in Washington. Even at Oxford, he maintains an idiosyncratic schedule, remaining in the office until two in the morning and returning sometime the next afternoon. I arrived before he did, and waited in a hallway between two conference rooms. A plaque indicated that one of them was the Arkhipov Room, honoring Vasili Arkhipov, a Soviet naval officer. During the Cuban missile crisis, Arkhipov was serving on a submarine in the Caribbean when U.S. destroyers set off depth charges nearby. His captain, unable to establish radio contact with Moscow, feared that the conflict had escalated and ordered a nuclear strike. But Arkhipov dissuaded him, and all-out atomic war was averted. Across the hallway was the Petrov Room, named for another Soviet officer who prevented a global nuclear catastrophe. Bostrom later told me, “They may have saved more lives than most of the statesmen we celebrate on stamps.”  The sense that a vanguard of technical-minded people working in obscurity, at odds with consensus, might save the world from auto-annihilation runs through the atmosphere at F.H.I. like an electrical charge. While waiting for Bostrom, I peered through a row of windows into the Arkh­ipov Room, which looked as though it was used for both meetings and storage; on a bookcase there were boxes containing light bulbs, lampshades, cables, spare mugs. A gaunt philosophy Ph.D. wrapped in a thick knitted cardigan was pacing in front of a whiteboard covered in notation, which he attacked in bursts. After each paroxysm, he paced, hands behind his back, head tilted downward. At one point, he erased a panel of his work. Taking this as an opportunity to interrupt, I asked him what he was doing. “It is a problem involving an aspect of A.I. called ‘planning,’ ” he said. His demeanor radiated irritation. I left him alone. Bostrom arrived at 2 p.m. He has a boyish countenance and the lean, vital physique of a yoga instructor—though he could never be mistaken for a yoga instructor. His intensity is too untidily contained, evident in his harried gait on the streets outside his office (he does not drive), in his voracious consumption of audiobooks (played at two or three times the normal speed, to maximize efficiency), and his fastidious guarding against illnesses (he avoids handshakes and wipes down silverware beneath a tablecloth). Bostrom can be stubborn about the placement of an office plant or the choice of a font. But when his arguments are challenged he listens attentively, the mechanics of consideration nearly dis­cernible beneath his skin. Then, calmly, quickly, he dispatches a response, one idea interlocked with another.  He asked if I wanted to go to the market. “You can watch me make my elixir,” he said. For the past year or so, he has been drinking his lunch (another efficiency): a smoothie containing fruits, vegetables, proteins, and fats. Using his elbow, he hit a button that electronically opened the front door. Then we rushed out.  Bostrom has a reinvented man’s sense of lost time. An only child, he grew up—as Niklas Boström—in Helsingborg, on the southern coast of Sweden. Like many exceptionally bright children, he hated school, and as a teen-ager he developed a listless, romantic persona. In 1989, he wandered into a library and stumbled onto an anthology of nineteenth-century German philosophy, containing works by Nietzsche and Schopenhauer. He read it in a nearby forest, in a clearing that he often visited to think and to write poetry, and experienced a euphoric insight into the possibilities of learning and achievement. “It’s hard to convey in words what that was like,” Bostrom told me; instead he sent me a photograph of an oil painting that he had made shortly afterward. It was a semi-representational landscape, with strange figures crammed into dense undergrowth; beyond, a hawk soared below a radiant sun. He titled it “The First Day.” Deciding that he had squandered his early life, he threw himself into a campaign of self-education. He ran down the citations in the anthology, branching out into art, literature, science. He says that he was motivated not only by curiosity but also by a desire for actionable knowledge about how to live. To his parents’ dismay, Bostrom insisted on finishing his final year of high school from home by taking special exams, which he completed in ten weeks. He grew distant from old friends: “I became quite fanatical and felt quite isolated for a period of time.”  When Bostrom was a graduate student in Stockholm, he studied the work of the analytic philosopher W. V. Quine, who had explored the difficult relationship between language and reality. His adviser drilled precision into him by scribbling “not clear” throughout the margins of his papers. “It was basically his only feedback,” Bostrom told me. “The effect was still, I think, beneficial.” His previous academic interests had ranged from psychology to mathematics; now he took up theoretical physics. He was fascinated by technology. The World Wide Web was just emerging, and he began to sense that the heroic philosophy which had inspired him might be outmoded. In 1995, Bostrom wrote a poem, “Requiem,” which he told me was “a signing-off letter to an earlier self.” It was in Swedish, so he offered me a synopsis: “I describe a brave general who has overslept and finds his troops have left the encampment. He rides off to catch up with them, pushing his horse to the limit. Then he hears the thunder of a modern jet plane streaking past him across the sky, and he realizes that he is obsolete, and that courage and spiritual nobility are no match for machines.”  “I’m starting a startup that helps other startups start up.”   Although Bostrom did not know it, a growing number of people around the world shared his intuition that technology could cause transformative change, and they were finding one another in an online discussion group administered by an organization in California called the Extropy Institute. The term “extropy,” coined in 1967, is generally used to describe life’s capacity to reverse the spread of entropy across space and time. Extropianism is a libertarian strain of transhumanism that seeks “to direct human evolution,” hoping to eliminate disease, suffering, even death; the means might be genetic modification, or as yet un­invented nanotechnology, or perhaps dispensing with the body entirely and uploading minds into supercomputers. (As one member noted, “Immortality is mathematical, not mystical.”) The Extropians advocated the development of artificial superintelligence to achieve these goals, and they envisioned humanity colonizing the universe, converting inert matter into engines of civilization. The discussions were nerdy, lunatic, imaginative, thought-provoking. Anders Sandberg, a former member of the group who now works at Bostrom’s institute, told me, “Just imagine if you could listen in on the debates of the Italian Futurists or early Surrealists.” In 1996, while pursuing further graduate work at the London School of Economics, Bostrom learned about the Extropy discussion group and became an active participant. A year later, he co-founded his own organization, the World Transhumanist Association, which was less libertarian and more academically spirited. He crafted approachable statements on transhumanist values and gave interviews to the BBC. The line between his academic work and his activism blurred: his Ph.D. dissertation centered on a study of the Doomsday Argument, which uses probability theory to make inferences about the longevity of human civilization. The work baffled his advisers, who respected him but rarely agreed with his conclusions. Mostly, they left him alone. Bostrom had little interest in conventional philosophy—not least because he expected that superintelligent minds, whether biologically enhanced or digital, would make it obsolete. “Suppose you had to build a new subway line, and it was this grand trans-generational enterprise that humanity was engaged in, and everybody had a little role,” he told me. “So you have a little shovel. But if you know that a giant bulldozer will arrive on the scene tomorrow, then does it really make sense to spend your time today digging the big hole with your shovel? Maybe there is something else you could do with your time. Maybe you could put up a signpost for the great shovel, so it will start digging in the right place.” He came to believe that a key role of the philosopher in modern society was to acquire the knowledge of a polymath, then use it to help guide humanity to its next phase of existence—a discipline that he called “the philosophy of technological prediction.” He was trying to become such a seer.  “He was ultra-consistent,” Daniel Hill, a British philosopher who befriended Bostrom while they were graduate students in London, told me. “His interest in science was a natural outgrowing of his understandable desire to live forever, basically.”  Bostrom has written more than a hundred articles, and his longing for immortality can be seen throughout. In 2008, he framed an essay as a call to action from a future utopia. “Death is not one but a multitude of assassins,” he warned. “Take aim at the causes of early death—infection, violence, malnutrition, heart attack, cancer. Turn your biggest gun on aging, and fire. You must seize the biochemical processes in your body in order to vanquish, by and by, illness and senescence. In time, you will discover ways to move your mind to more durable media.” He tends to see the mind as immaculate code, the body as inefficient hardware—able to accommodate limited hacks but probably destined for replacement.  Even Bostrom’s marriage is largely mediated by technology. His wife, Susan, has a Ph.D. in the sociology of medicine and a bright, down-to-earth manner. (“She teases me about the Terminator and the robot army,” he told me.) They met thirteen years ago, and for all but six months they have lived on opposite sides of the Atlantic, even after the recent birth of their son. The arrangement is voluntary: she prefers Montreal; his work keeps him at Oxford. They Skype several times a day, and he directs as much international travel as possible through Canada, so they can meet in non-digital form.  In Oxford, as Bostrom shopped for his smoothie, he pointed out a man vaping. “There is also the more old-school method of taking nicotine: chewing gum,” he told me. “I do chew nicotine gum. I read a few papers saying it might have some nootropic effect”—that is, it might enhance cognition. He drinks coffee, and usually abstains from alcohol. He briefly experimented with the smart drug Modafinil, but gave it up.  Back at the institute, he filled an industrial blender with lettuce, carrots, cauliflower, broccoli, blueberries, turmeric, vanilla, oat milk, and whey powder. “If there is one thing Nick cares about, it is minds,” Sandberg told me. “That is at the root of many of his views about food, because he is worried that toxin X or Y might be bad for his brain.” He suspects that Bostrom also enjoys the ritualistic display. “Swedes are known for their smugness,” he joked. “Perhaps Nick is subsisting on smugness.”  A young employee eyed Bostrom getting ready to fire up the blender. “I can tell when Nick comes into the office,” he said. “My hair starts shaking.”  “Yeah, this has got three horsepower,” Bostrom said. He ran the blender, producing a noise like a circular saw, and then filled a tall glass stein with purple-­green liquid. We headed to his office, which was meticulous. By a window was a wooden desk supporting an iMac and not another item; against a wall were a chair and a cabinet with a stack of documents. The only hint of excess was light: there were fourteen lamps.  It is hard to spend time at Bostrom’s institute without drifting into reveries of a far future. What might humanity look like millions of years from now? The upper limit of survival on Earth is fixed to the life span of the sun, which in five billion years will become a red giant and swell to more than two hundred times its present size. It is possible that Earth’s orbit will adjust, but more likely that the planet will be destroyed. In any case, long before then, nearly all plant life will die, the oceans will boil, and the Earth’s crust will heat to a thousand degrees. In half a billion years, the planet will be uninhabitable. The view of the future from Bostrom’s office can be divided into three grand panoramas. In one, humanity experiences an evolutionary leap—either assisted by technology or by merging into it and becoming software—to achieve a sublime condition that Bostrom calls “posthumanity.” Death is overcome, mental experience expands beyond recognition, and our descendants colonize the universe. In another panorama, humanity becomes extinct or experiences a disaster so great that it is unable to recover. Between these extremes, Bostrom envisions scenarios that resemble the status quo—people living as they do now, forever mired in the “human era.” It’s a vision familiar to fans of sci-fi: on “Star Trek,” Captain Kirk was born in the year 2233, but when an alien portal hurls him through time and space to Depression-era Manhattan he blends in easily.  Bostrom dislikes science fiction. “I’ve never been keen on stories that just try to present ‘wow’ ideas—the equivalent of movie productions that rely on stunts and explosions to hold the attention,” he told me. “The question is not whether we can think of something radical or extreme but whether we can discover some sufficient reason for updating our credence function.”  He believes that the future can be studied with the same meticulousness as the past, even if the conclusions are far less firm. “It may be highly unpredictable where a traveller will be one hour after the start of her journey, yet predictable that after five hours she will be at her destination,” he once argued. “The very long-term future of humanity may be relatively easy to predict.” He offers an example: if history were reset, the industrial revolution might occur at a different time, or in a different place, or perhaps not at all, with innovation instead occurring in increments over hundreds of years. In the short term, predicting technological achievements in the counter-history might not be possible; but after, say, a hundred thousand years it is easier to imagine that all the same inventions would have emerged.  Bostrom calls this the Technological Completion Conjecture: “If scientific- and technological-development efforts do not effectively cease, then all impor­t­­­ant basic capabilities that could be obtained through some possible technology will be obtained.” In light of this, he suspects that the farther into the future one looks the less likely it seems that life will continue as it is. He favors the far ends of possibility: humanity becomes transcendent or it perishes.  In the nineteen-nineties, as these ideas crystallized in his thinking, Bostrom began to give more attention to the question of extinction. He did not believe that doomsday was imminent. His interest was in risk, like an insurance agent’s. No matter how improbable extinction may be, Bostrom argues, its consequences are near-infinitely bad; thus, even the tiniest step toward reducing the chance that it will happen is near-­infinitely valuable. At times, he uses arithmetical sketches to illustrate this point. Imagining one of his utopian scenarios—trillions of digital minds thriving across the cosmos—he reasons that, if there is even a one-per-cent chance of this happening, the expected value of reducing an existential threat by a billionth of a billionth of one per cent would be worth a hundred billion times the value of a billion present-day lives. Put more simply: he believes that his work could dwarf the moral importance of anything else. Bostrom introduced the philosophical concept of “existential risk” in 2002, in the Journal of Evolution and Technology. In recent years, new organizations have been founded almost annually to help reduce it—among them the Centre for the Study of Existential Risk, affiliated with Cambridge Uni­versity, and the Future of Life Institute, which has ties to the Massachusetts Institute of Technology. All of them face a key problem: Homo sapiens, since its emergence two hundred thousand years ago, has proved to be remarkably resilient, and figuring out what might imperil its existence is not obvious. Climate change is likely to cause vast environmental and economic damage—but it does not seem impossible to survive. So-called super-volcanoes have thus far not threatened the perpetuation of the species. NASA spends forty million dollars each year to determine if there are significant comets or asteroids headed for Earth. (There aren’t.)  Bostrom does not find the lack of obvious existential threats comforting. Because it is impossible to endure extinction twice, he argues, we cannot rely on history to calculate the probability that it will occur. The most worrying dangers are those that Earth has never encountered before. “It is hard to cause human extinction with seventeenth-century technology,” Bostrom told me. Three centuries later, though, the prospect of a technological apocalypse was urgently plausible. Bostrom dates the first scientific analysis of existential risk to the Manhattan Project: in 1942, Robert Oppenheimer became concerned that an atomic detonation of sufficient power could cause the entire atmosphere to ignite. A subsequent study concluded that the scenario was “unreasonable,” given the limitations of the weapons then in development. But even if the great nuclear nightmares of the Cold War did not come true, the tools were there to cause destruction on a scale not previously possible. As innovations grow even more complex, it is increasingly difficult to evaluate the dangers ahead. The answers must be fraught with ambiguity, because they can be derived only by predicting the effects of technologies that exist mostly as theories or, even more indirectly, by using abstract reasoning.  Nick Bostrom asks, Will we engineer our own extinction? As a philosopher, Bostrom takes a sweeping, even cosmic, view of such problems. One afternoon, he told me, “The probabilities that any given planet will produce intelligent life—this may also have action-relevant information.” In the past several years, NASA probes have found increasing evidence that the building blocks of life are abundant throughout space. So much water has been discovered—on Mars and on the moons of Jupiter and Saturn—that one scientist described our solar system as “a pretty soggy place.” There are amino acids on icy comets and complex organic molecules in distant star-forming clouds. On this planet, life has proved capable of thriving in unimaginably punishing conditions: without oxygen, without light, at four hundred degrees above or below zero. In 2007, the European Space Agency hitched tiny creatures to the exterior of a satellite. They not only survived the flight; some even laid eggs afterward.  With ten billion Earth-like planets in our galaxy alone, and a hundred billion galaxies in the universe, there is good reason to suspect that extraterrestrial life may one day be discovered. For Bostrom, this would augur disaster. “It would be great news to find that Mars is a completely sterile planet,” he argued not long ago. “Dead rocks and lifeless sands would lift my spirits.” His reasoning begins with the age of the universe. Many of those Earth-like planets are thought to be far, far older than ours. One that was recently discovered, called Kepler 452b, is as much as one and a half billion years older. Bostrom asks: If life had formed there on a time scale resembling our own, what would it look like? What kind of technological progress could a civilization achieve with a head start of hundreds of millions of years? Life as we know it tends to spread wherever it can, and Bostrom estimates that, if an alien civilization could design space probes capable of travelling at even one per cent of the speed of light, the entire Milky Way could be colonized in twenty million years—a tiny fraction of the age difference between Kepler 452b and Earth. One could argue that no technology will ever propel ships at so great a speed. Or perhaps millions of alien civilizations possess the know-how for intergalactic travel, but they aren’t interested. Even so, because the universe is so colossal, and because it is so old, only a small number of civilizations would need to behave as life does on Earth—unceasingly expanding—in order to be visible. Yet, as Bostrom notes, “You start with billions and billions of potential germination points for life, and you end up with a sum total of zero alien civilizations that developed technologically to the point where they become manifest to us earthly observers. So what’s stopping them?”  In 1950, Enrico Fermi sketched a version of this paradox during a lunch break while he was working on the H-bomb, at Los Alamos. Since then, many resolutions have been proposed—some of them exotic, such as the idea that Earth is housed in an interplanetary alien zoo. Bostrom suspects that the answer is simple: space appears to be devoid of life because it is. This implies that intelligent life on Earth is an astronomically rare accident. But, if so, when did that accident occur? Was it in the first chemical reactions in the primordial soup? Or when single-celled organisms began to replicate using DNA? Or when animals learned to use tools? Bos­trom likes to think of these hurdles as Great Filters: key phases of improbability that life everywhere must pass through in order to develop into intelligent species. Those which do not make it either go extinct or fail to evolve. Thus, for Bostrom, the discovery of a single-celled creature inhabiting a damp stretch of Martian soil would constitute a disconcerting piece of evidence. If two planets independently evolved primitive organisms, then it seems more likely that this type of life can be found on many planets throughout the universe. Bostrom reasons that this would suggest that the Great Filter comes at some later evolutionary stage. The discovery of a fossilized vertebrate would be even worse: it would suggest that the universe appears lifeless not because complex life is unusual but, rather, because it is always somehow thwarted before it becomes advanced enough to colonize space.  In Bostrom’s view, the most distressing possibility is that the Great Filter is ahead of us—that evolution frequently achieves civilizations like our own, but they perish before reaching their technological maturity. Why might that be? “Natural disasters such as asteroid hits and super-­volcanic eruptions are unlikely Great Filter candidates, because, even if they destroyed a significant number of civilizations, we would expect some civilizations to get lucky and escape disaster,” he argues. “Perhaps the most likely type of existential risks that could constitute a Great Filter are those that arise from technological discovery. It is not far-fetched to suppose that there might be some possible technology which is such that (a) virtually all suffi­ciently advanced civilizations eventually discover it and (b) its discovery leads almost universally to existential disaster.” II. The Machines The field of artificial intelligence was born in a fit of scientific optimism, in 1955, when a small group of researchers—three mathematicians and an I.B.M. programmer—drew up a proposal for a project at Dartmouth. “An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves,” they stated. “We think a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.” Their optimism was understandable. Since the turn of the twentieth century, science had been advancing at a breakneck pace: the discovery of radioactivity quickly led to insights into the inner workings of the atom, and then to the development of controlled nuclear energy, and then to the warheads over Hiroshima and Nagasaki, and then to the H-bomb. This rush of discovery was reflected in fiction, too, in the work of Isaac Asimov, among others, who envisioned advanced civilizations inhabited by intelligent robots (each encoded with simple, ethical Laws of Robotics, to prevent it from causing harm). The year the scientists met at Dartmouth, Asimov published “The Last Question,” a story featuring a superintelligent A.I. that is continually “self-adjusting and self-correcting”—gaining knowledge as it helps human civilization expand throughout the universe. When the universe’s last stars start dying out, all humanity uploads itself into the A.I., and the device, achieving godhood, creates a new cosmos. Scientists perceived the mechanics of intelligence—like those of the atom—as a source of huge potential, a great frontier. If the brain was merely a biological machine, there was no theoretical reason that it could not be replicated, or even surpassed, much the way a jet could outfly a falcon. Even before the Dartmouth conference, machines exceeded human ability in narrow domains like code-breaking. In 1951, Alan Turing argued that at some point computers would probably exceed the intellectual capacity of their inventors, and that “therefore we should have to expect the machines to take control.” Whether this would be good or bad he did not say. “I hoped you’d like the size of it.”   Six years later, Herbert Simon, one of the Dartmouth attendees, declared that machines would achieve human intelligence “in a visible future.” The crossing of such a threshold, he suspected, could be psychologically crushing, but he was on the whole optimistic. “We must also remain sensitive to the need to keep the computer’s goals attuned with our own,” he later said, but added, “I am not convinced that this will be difficult.” For other computer pioneers, the future appeared more ambivalent. Norbert Wiener, the father of cybernetics, argued that it would be difficult to manage powerful computers, or even to accurately predict their behavior. “Complete subservience and complete intelligence do not go together,” he said. Envisioning Sorcerer’s Apprentice scenarios, he predicted, “The future will be an ever more demanding struggle against the limitations of our intelligence, not a comfortable hammock in which we can lie down to be waited upon by our robot slaves.”  It was in this milieu that the “intelligence explosion” idea was first formally expressed by I. J. Good, a statistician who had worked with Turing. “An ultraintelligent machine could design even better machines,” he wrote. “There would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.”  The scientists at Dartmouth recognized that success required answers to fundamental questions: What is intelligence? What is the mind? By 1965, the field had experimented with several models of problem solving: some were based on formal logic; some used heuristic reasoning; some, called “neural networks,” were inspired by the brain. With each, the scientists’ work indicated that A.I. systems could find their own solutions to problems. One algorithm proved numerous theorems in the classic text “Principia Mathematica,” and in one instance it did so more elegantly than the authors. A program designed to play checkers learned to beat its programmer. And yet, despite the great promise in these experiments, the challenges to creating an A.I. were forbidding. Programs that performed well in the laboratory were useless in everyday situations; a simple act like picking up a ball turned out to require an overwhelming number of computations.  The research fell into the first of several “A.I. winters.” As Bostrom notes in his book, “Among academics and their funders, ‘A.I.’ became an unwanted epithet.” Eventually, the researchers started to question the goal of building a mind altogether. Why not try instead to divide the problem into pieces? They began to limit their interests to specific cognitive functions: vision, say, or speech. Even in isolation, these functions would have value: a computer that could identify objects might not be an A.I., but it could help guide a forklift. As the research fragmented, the morass of technical problems made any questions about the consequences of success seem distant, even silly.  Unexpectedly, by dismissing its founding goals, the field of A.I. created space for outsiders to imagine more freely what the technology might look like. Bostrom wrote his first paper on artificial superintelligence in the nineteen-nineties, envisioning it as potentially perilous but irresistible to both commerce and government. “If there is a way of guaranteeing that superior artificial intellects will never harm human beings, then such intellects will be created,” he argued. “If there is no way to have such a guarantee, then they will probably be created nevertheless.” His audience at the time was primarily other transhumanists. But the movement was maturing. In 2005, an organization called the Singularity Institute for Artificial Intelligence began to operate out of Silicon Valley; its primary founder, a former member of the Extropian discussion group, published a stream of literature on the dangers of A.I. That same year, the futurist and inventor Ray Kurzweil wrote “The Singularity Is Near,” a best-seller that prophesied a merging of man and machine in the foreseeable future. Bostrom created his institute at Oxford. The two communities could not have been more different. The scientists, steeped in technical detail, were preoccupied with making devices that worked; the transhumanists, motivated by the hope of a utopian future, were asking, What would the ultimate impact of those devices be? In 2007, the Association for the Advancement of Artificial Intelligence—the most prominent professional organization for A.I. researchers—elected Eric Horvitz, a scientist from Microsoft, as its president. Until then, it had given virtually no attention to the ethical and social implications of the research, but Horvitz was open to the big questions. “It is hard to understand what success would mean for A.I.,” he told me. “I was friendly with Jack Good, who wrote that piece on superintelligence. I knew him as a creative, funny guy who referred to a lot of his ideas as P.B.I.s—partly baked ideas. And here is this piece of his being opened up outside the field as this Bible and studied with a silver pointer. Wouldn’t it be useful, I said, even if you thought these were crazy or low-probability scenarios, to find out: Can we be proactive, should there be some poor outcome for humanity?”  Horvitz organized a meeting at the Asilomar Conference Grounds, in California, a place chosen for its symbolic value: biologists had gathered there in 1975 to discuss the hazards of their research in the age of modern genetics. He divided the researchers into groups. One studied short-term ramifications, like the possible use of A.I. to commit crimes; another considered long-term consequences. Mostly, there was skepticism about the intelligence-explosion idea, which assumed answers to many unresolved questions. No one fully understands what intelligence is, let alone how it might evolve in a machine. Can it grow as Good imagined, gaining I.Q. points like a rocketing stock price? If so, what would its upper limit be? And would its increase be merely a function of optimized software design, without the difficult process of acquiring knowledge through experience? Can software fundamentally rewrite itself without risking crippling breakdowns? No one knows. In the history of computer science, no programmer has created code that can substantially improve itself. “No, you want the A train. This is just a train.”   But the notion of an intelligence explosion was also impossible to disprove. It was theoretically coherent, and it had even been attempted in limited ways. David McAllester, an A.I. researcher at the Toyota Technological Institute, affiliated with the University of Chicago, headed the long-term panel. The idea, he argued, was worth taking seriously. “I am uncomfortable saying that we are ninety-­nine per cent certain that we are safe for fifty years,” he told me. “That feels like hubris to me.” The group concluded that more technical work was needed before an evaluation of the dangers could be made, but it also hinted at a concern among panelists that the gathering was based on “a perception of urgency”—generated largely by the transhumanists—and risked raising unfounded alarm. With A.I. seeming like a remote prospect, the researchers declared, attention was better spent on near-term concerns. Bart Selman, a professor at Cornell who co-­organized the panel, told me, “The mode was ‘This is interesting, but it’s all academic—it’s not going to happen.’ ” At the time the A.I researchers met at Asilomar, Bostrom was grappling with an expansive book on existential risks. He had sketched out chapters on bioengineering and on nanotechnology, among other topics, but many of these problems came to seem less compelling, while his chapter on A.I. grew and grew. Eventually, he pasted the A.I. chapter into a new file, which became “Superintelligence.”  The book is its own elegant paradox: analytical in tone and often lucidly argued, yet punctuated by moments of messianic urgency. Some portions are so extravagantly speculative that it is hard to take them seriously. (“Suppose we could somehow establish that a certain future AI will have an IQ of 6,455: then what?”) But Bostrom is aware of the limits to his type of futurology. When he was a graduate student in London, thinking about how to maximize his ability to communicate, he pursued stand­­up comedy; he has a deadpan sense of humor, which can be found lightly buried among the book’s self-serious passages. “Many of the points made in this book are probably wrong,” he writes, with an endnote that leads to the line “I don’t know which ones.” Bostrom prefers to act as a cartographer rather than a polemicist, but beneath his exhaustive mapping of scenarios one can sense an argument being built and perhaps a fear of being forthright about it. “Traditionally, this topic domain has been occupied by cranks,” he told me. “By popular media, by science fiction—or maybe by a retired physicist no longer able to do serious work, so he will write a popular book and pontificate. That is kind of the level of rigor that is the baseline. I think that a lot of reasons why there has not been more serious work in this area is that academics don’t want to be conflated with flaky, crackpot type of things. Futurists are a certain type.” The book begins with an “unfinished” fable about a flock of sparrows that decide to raise an owl to protect and advise them. They go looking for an owl egg to steal and bring back to their tree, but, because they believe their search will be so difficult, they postpone studying how to domesticate owls until they succeed. Bostrom concludes, “It is not known how the story ends.” The parable is his way of introducing the book’s core question: Will an A.I., if realized, use its vast capability in a way that is beyond human control? One way to think about the concern is to begin with the familiar. Bos­trom writes, “Artificial intelligence already outperforms human intelligence in many domains.” The examples range from chess to Scrabble. One program from 1981, called Eurisko, was designed to teach itself a naval role-playing game. After playing ten thousand matches, it arrived at a morally grotesque strategy: to field thousands of small, immobile ships, the vast majority of which were intended as cannon fodder. In a national tournament, Eurisko demolished its human opponents, who insisted that the game’s rules be changed. The following year, Eurisko won again—by forcing its damaged ships to sink themselves.  The program was by no means superintelligent. But Bostrom’s book essentially asks: What if it were? Assume that it has a broad ability to consider problems and that it has access to the Internet. It could read and acquire general knowledge and communicate with people seamlessly online. It could conduct experiments, either virtually or by tinkering with networked infrastructure. Given even the most benign objective—to win a game—such a system, Bostrom argues, might develop “instrumental goals”: gather resources, or invent technology, or take steps to insure that it cannot be turned off, in the process paying as much heed to human life as humans do to ants.  In people, intelligence is inseparable from consciousness, emotional and social awareness, the complex interaction of mind and body. An A.I. need not have any such attributes. Bostrom believes that machine intelligences—no matter how flexible in their tactics—will likely be rigidly fixated on their ultimate goals. How, then, to create a machine that respects the nuances of social cues? That adheres to ethical norms, even at the expense of its goals? No one has a coherent solution. It is hard enough to reliably inculcate such behavior in people.  In science fiction, superintelligent computers that run amok are often circumvented at the last minute; think of WOPR, the computer in “WarGames,” which was stopped just short of triggering nuclear war, or HAL 9000, which was reduced to helplessly singing while it watched itself get dismantled. For Bos­trom, this strains credulity. Whether out of a desire to consider the far ends of risk or out of transhumanist longings, he often ascribes nearly divine abilities to machines, as if to ask: Can a digital god really be contained? He imagines machines so intelligent that merely by inspecting their own code they can extrapolate the nature of the universe and of human society, and in this way outsmart any effort to contain them. “Is it possible to build machines that are not like agents—goal-pursuing, autonomous, artificial intelligences?” he asked me. “Maybe you can design something more like an oracle that can only answer yes or no. Would that be safer? It is not so clear. There might be agent-like processes within it.” Asking a simple question—“Is it possible to convert a DeLorean into a time machine and travel to 1955?”—might trigger a cascade of action as the device tests hypotheses. What if, working through a police computer, it impounds a DeLorean that happens to be convenient to a clock tower? “In fairy tales, you have genies who grant wishes,” Bostrom said. “Almost universally, the moral of those is that if you are not extremely careful what you wish for, then what seems like it should be a great blessing turns out to be a curse.” Bostrom worries that solving the “control problem”—insuring that a superintelligent machine does what humans want it to do—will require more time than solving A.I. does. The intelligence explosion is not the only way that a superintelligence might be created suddenly. Bostrom once sketched out a decades-long process, in which researchers arduously improved their systems to equal the intelligence of a mouse, then a chimp, then—after incredible labor—the village idiot. “The difference between village idiot and genius-­level intelligence might be trivial from the point of view of how hard it is to replicate the same functionality in a machine,” he said. “The brain of the village idiot and the brain of a scientific genius are almost identical. So we might very well see relatively slow and incremental progress that doesn’t really raise any alarm bells until we are just one step away from something that is radically superintelligent.” To a large degree, Bostrom’s concerns turn on a simple question of timing: Can breakthroughs be predicted? “It is ridiculous to talk about such things so early—A.I. is eons away,” Edward Feigenbaum, an emeritus professor at Stanford University, told me. The researcher Oren Etzioni, who used the term “Frankenstein complex” to dismiss the “dystopian vision of A.I.,” concedes Bostrom’s overarching point: that the field must one day confront profound philosophical questions. Decades ago, he explored them himself, in a brief paper, but concluded that the problem was too remote to think about productively. “Once, Nick Bostrom gave a talk, and I gave a little counterpoint,” he told me. “A lot of the disagreements come down to what time scale you are thinking about. Nobody responsible would say you will see anything remotely like A.I. in the next five to ten years. And I think most computer scientists would say, ‘In a million years—we don’t see why it shouldn’t happen.’ So now the question is: What is the rate of progress? There are a lot of people who will ask: Is it possible we are wrong? Yes. I am not going to rule it out. I am going to say, ‘I am a scientist. Show me the evidence.’ ” The history of science is an uneven guide to the question: How close are we? There has been no shortage of unfulfilled promises. But there are also plenty of examples of startling nearsightedness, a pattern that Arthur C. Clarke enshrined as Clarke’s First Law: “When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong.” After the electron was discovered, at Cambridge, in 1897, physicists at an annual dinner toasted, “To the electron: may it never be of use to anybody.” Lord Kelvin famously declared, just eight years before the Wright brothers launched from Kitty Hawk, that heavier-than-air flight was impossible.  Stuart Russell, the co-author of the textbook “Artificial Intelligence: A Modern Approach” and one of Bostrom’s most vocal supporters in A.I., told me that he had been studying the physics community during the advent of nuclear weapons. At the turn of the twentieth century, Ernest Rutherford discovered that heavy elements produced radiation by atomic decay, confirming that vast reservoirs of energy were stored in the atom. Rutherford believed that the energy could not be harnessed, and in 1933 he proclaimed, “Anyone who expects a source of power from the transformation of these atoms is talking moonshine.” The next day, a former student of Einstein’s named Leo Szilard read the comment in the papers. Irritated, he took a walk, and the idea of a nuclear chain reaction occurred to him. He visited Rutherford to discuss it, but Rutherford threw him out. Einstein, too, was skeptical about nuclear energy—splitting atoms at will, he said, was “like shooting birds in the dark in a country where there are only a few birds.” A decade later, Szilard’s insight was used to build the bomb. Russell now relays the story to A.I. researchers as a cautionary tale. “There will have to be more breakthroughs to get to A.I., but, as Szilard illustrated, those can happen overnight,” he told me. “People are putting billions of dollars into achieving those breakthroughs. As the debate stands, Bostrom and others have said, ‘If we achieve superintelligence, here are some of the problems that might arise.’ As far as I know, no one has proved why those are not real.” III. Mission Control The offices of the Future of Humanity Institute have a hybrid atmosphere: part physics lab, part college dorm room. There are whiteboards covered with mathematical notation and technical glyphs; there are posters of “Brave New World” and HAL 9000. There is also art work by Nick Bostrom. One afternoon, he guided me to one of his pieces, “At Sea,” a digital collage that he had printed out and then drawn on. “It is a bit damaged, but the good thing about digital is that you can re-instantiate it,” he said. At the center was a pale man, nearly an apparition, clinging to a barrel in an inky-black ocean. “It is an existentialist vibe. You are hanging on for as long as you can. When you get tired, you sink, and become fish food—or maybe a current will take him to land. We don’t know.”  Despite the time he spends going to conferences and raising money, Bostrom attends to many details at the institute. “We needed a logo when we started,” he told me. “We went to this online site where you could buy the work of freelance artists. If you sat down and tried to make the ugliest logo, you couldn’t come close. Then we hired a designer, who made a blurry figure of a person. We showed it to someone here, who said it looked like a toilet sign. As soon as she said it, I thought, Oh, my God, we almost adopted a toilet sign as our logo. So I mucked around a bit and came up with a black diamond. You have the black monolith from ‘2001.’ Standing on its corner, it indicates instability. Also, there is a limit to how ugly a black square can be.”  The institute shares office space with the Centre for Effective Altruism, and both organizations intersect with a social movement that promotes pure rationality as a guide to moral action. Toby Ord, a philosopher who works with both, told me that Bostrom often pops into his office at the end of the day, poses a problem, then leaves him pondering it for the night. Among the first of Bostrom’s questions was this: If the universe turns out to contain an infinite number of beings, then how could any single person’s action affect the cosmic balance of suffering and happiness? After lengthy discussions, they left the paradox unresolved. “My main thinking is that we can sort it out later,” Ord told me.  “Chaucer on lyne thrie.”   When I asked Bostrom if I could observe a discussion at the institute, he seemed reluctant; it was hard to judge whether he was concerned that my presence would interfere or that unfiltered talk of, say, engineered pathogens might inspire criminals. (“At some point, one gets into the realm of information hazard,” he hinted.) Eventually, he let me observe a session in the Petrov Room involving half a dozen staff members. The key question under discussion was whether a global catastrophe, on the order of a continent-wide famine, could trigger a series of geopolitical events that would result in human extinction—and whether that meant that a merely catastrophic risk could therefore be taken as seriously as an existential risk. Bostrom, wearing a gray hoodie over a blue button-­down, organized the problem on a whiteboard with visible pleasure. Anders Sandberg told me that he once spent days with Bostrom working through such a problem, distilling a complex argument to its essence. “He had to refine it,” he said. “We had a lot of schemes on the whiteboard that gradually were simplified to one box and three arrows.”  For anyone in the business of publicizing existential risk, 2015 began as a good year. Other institutes devoted to these issues had started to find their voice, bringing an additional gloss of respectability to the ideas in Bostrom’s book. The people weighing in now were no longer just former Extropians. They were credentialled, like Lord Martin Rees, an astrophysicist and the co-founder of Cambridge’s Centre for the Study of Existential Risk. In January, he wrote of A.I., in the Evening Standard, “We don’t know where the boundary lies between what may happen and what will remain science fiction.” Rees’s counterpart at the Future of Life Institute, the M.I.T. physicist Max Tegmark, hosted a closed-door meeting in Puerto Rico, to try to make sense of the long-term trajectory of the research. Bostrom flew down, joining a mix of A.I. practitioners, legal scholars, and, for lack of a better term, members of the “A.I. safety” community. “These are not people who are usually in the same room,” Tegmark told me. “Someone advised me to put Valium in people’s drinks so nobody got into fistfights. But, by the time Nick’s session started, people were ready to listen to each other.” Questions that had seemed fanciful to researchers only seven years earlier were beginning to look as though they might be worth reconsidering. Whereas the Asilomar meeting concluded on a note of skepticism about the validity of the whole endeavor, the Puerto Rico conference resulted in an open letter, signed by many prominent researchers, that called for more research to insure that A.I. would be “robust and beneficial.”  Between the two conferences, the field had experienced a revolution, built on an approach called deep learning—a type of neural network that can discern complex patterns in huge quantities of data. For de­c­ades, researchers, hampered by the limits of their hardware, struggled to get the technique to work well. But, beginning in 2010, the increasing availability of Big Data and cheap, powerful video-­game processors had a dramatic effect on performance. Without any profound theoretical breakthrough, deep learning suddenly offered breathtaking advances. “I have been talking to quite a few contemporaries,” Stuart Russell told me. “Pretty much everyone sees examples of progress they just didn’t expect.” He cited a YouTube clip of a four-legged robot: one of its designers tries to kick it over, but it quickly regains its balance, scrambling with uncanny naturalness. “A problem that had been viewed as very difficult, where progress was slow and incremental, was all of a sudden done. Locomotion: done.” In an array of fields—speech processing, face recognition, language translation—the approach was ascendant. Researchers working on computer vision had spent years to get systems to identify objects. In almost no time, the deep-learning networks crushed their records. In one common test, using a database called ImageNet, humans identify photographs with a five-per-cent error rate; Google’s network operates at 4.8 per cent. A.I. systems can differentiate a Pembroke Welsh Corgi from a Cardigan Welsh Corgi.  Last October, Tomaso Poggio, an M.I.T. researcher, gave a skeptical interview. “The ability to describe the content of an image would be one of the most intellectually challenging things of all for a machine to do,” he said. “We will need another cycle of basic research to solve this kind of question.” The cycle, he predicted, would take at least twenty years. A month later, Google announced that its deep-learning network could analyze an image and offer a caption of what it saw: “Two pizzas sitting on top of a stove top,” or “People shopping at an outdoor market.” When I asked Poggio about the results, he dismissed them as automatic associations between objects and language; the system did not understand what it saw. “Maybe human intelligence is the same thing, in which case I am wrong, or not, in which case I was right,” he told me. “How do you decide?” A respected minority of A.I. researchers began to wonder: If increasingly powerful hardware could facilitate the deep-learning revolution, would it make other long-shelved A.I. principles viable? “Suppose the brain is just a million different evolutionarily developed hacks: one for smell, one for recognizing faces, one for how you recognize animals,” Tom Mitchell, who holds a chair in machine learning at Carnegie Mellon, told me. “If that is what underlies intelligence, then I think we are far, far from getting there—because we don’t have many of those hacks. On the other hand, suppose that what underlies intelligence are twenty-three general mechanisms, and when you put them together you get synergy, and it works. We now have systems that can do a pretty good job with computer vision—and it turns out that we didn’t have to construct a million hacks. So part of the uncertainty is: if we do not need a million different hacks, then will we find the right twenty-­three fundamental generic methods?” He paused. “I no longer have the feeling, which I had twenty-five years ago, that there are gaping holes. I know we don’t have a good architecture to assemble the ideas, but it is not obvious to me that we are missing components.”     Bostrom noticed the shift in attitude. He recently conducted a poll of A.I. researchers to gauge their sense of progress, and in Puerto Rico a survey gathered opinions on how long it would be until an artificial intelligence could reason indistinguishably from a human being. Like Bostrom, the engineers are often careful to express their views as probabilities, rather than as facts. Richard Sutton, a Canadian computer scientist whose work has earned tens of thousands of scholarly citations, gives a range of outcomes: there is a ten-per-cent chance that A.I. will never be achieved, but a twenty-five-per-cent chance that it will arrive by 2030. The median response in Bostrom’s poll gives a fifty-fifty chance that human-level A.I. would be attained by 2050. These surveys are unscientific, but he is confident enough to offer an interpretive assumption: “It is not a ridiculous prospect to take seriously the possibility that it can happen in the lifetime of people alive today.” On my last day in Oxford, I walked with Bostrom across town. He was racing to catch a train to London, to speak at the Royal Society, one of the world’s oldest scientific institutions. His spirits were high. The gulf between the transhumanists and the scientific community was slowly shrinking. Elon Musk had pledged ten million dollars in grants for academics seeking to investigate A.I. safety, and, rather than mock him, researchers applied for the money; Bostrom’s institute was helping to evaluate the proposals. “Right now, there is a lot of interest,” he told me. “But then there were all these long years when nobody else seemed to pay attention at all. I am not sure which is the less abnormal condition.”  There were clear limits to that interest. To publicly stake out a position in the middle of the debate was difficult, not least because of the polarized atmosphere Bostrom’s book had helped to create. Even though a growing number of researchers were beginning to suspect that profound questions loomed, and that they might be worth addressing now, it did not mean that they believed A.I. would lead inevitably to an existential demise or a techno-utopia. Most of them were engaged with more immediate problems: privacy, unemployment, weaponry, driverless cars running amok. When I asked Bostrom about this pragmatic ethical awakening, he reacted with dismay. “My fear is that it would swallow up the concerns for the longer term,” he said. “On the other hand, yes, maybe it is useful to build bridges to these different communities. Kind of makes the issue part of a larger continuum of things to work on.” At the Royal Society, Bostrom took a seat at the back of a large hall. As he crossed his legs, I noticed a thin leather band around his ankle. A metal buckle was engraved with contact information for Alcor, a cryonics facility in Arizona, where Bostrom is a fee-paying member. Within hours of his death, Alcor will take custody of his body and maintain it in a giant steel bottle flooded with liquid nitrogen, in the hope that one day technology will allow him to be revived, or to have his mind uploaded into a computer. When he signed up, two other colleagues at the institute joined him. “My background is transhumanism,” he once reminded me. “The character of that is gung-ho techno-cheerleading, bring it on now, where are my life-­extension pills.”  The hall was packed with some of the most technically sophisticated researchers in A.I.—not necessarily Bostrom’s people—and when he spoke he began by trying to assure them that his concern was not out of Ludditism. “It would be tragic if machine intelligence were never developed to its full capacity,” he said. “I think this is ultimately the key, or the portal, we have to pass through to realize the full dimension of humanity’s long-term potential.” But, even as he avoided talk of existential risk, he pressed his audience to consider the danger of building an A.I. without regarding its ethical design.  An attendee raised his hand to object. “We can’t control basic computer worms,” he said. “The A.I. that will happen is going to be a highly adaptive, emergent capability, and highly distributed. We will be able to work with it—for it—not necessarily contain it.”  “I guess I am a little frustrated,” Bos­trom responded. “People tend to fall into two camps. On one hand, there are those, like yourself, who think it is probably hopeless. The other camp thinks it is easy enough that it will be solved automatically. And both of these have in common the implication that we don’t have to make any effort now.” For the rest of the day, engineers presented their work at the lectern, each promising a glimpse of the future—robot vision, quantum computers, algorithms called “thought vectors.” Early in Bostrom’s career, he predicted that cascading economic demand for an A.I. would build up across the fields of medicine, entertainment, finance, and defense. As the technology became useful, that demand would only grow. “If you make a one-per-cent improvement to something—say, an algorithm that recommends books on Amazon—there is a lot of value there,” Bostrom told me. “Once every improvement potentially has enormous economic benefit, that promotes effort to make more improvements.”  Many of the world’s largest tech companies are now locked in an A.I. arms race, purchasing other companies and opening specialized units to advance the technology. Industry is vacuuming up Ph.D.s so quickly that people in the field worry there will no longer be top talent in academia. After decades of pursuing narrow forms of A.I., researchers are seeking to integrate them into systems that resemble a general intellect. Since I.B.M.’s Watson won “Jeopardy,” the company has committed more than a billion dollars to develop it, and is reorienting its business around “cognitive systems.” One senior I.B.M. executive declared, “The separation between human and machine is going to blur in a very fundamental way.”  At the Royal Society, a contingent of researchers from Google occupied a privileged place; they likely had more resources at their disposal than anyone else in the room. Early on, Google’s founders, Larry Page and Sergey Brin, understood that the company’s mission required solving fundamental A.I. problems. Page has said that he believes the ideal system would understand questions, even anticipate them, and produce responses in conversational language. Google scientists often invoke the computer in “Star Trek” as a model.  “O.K., there’s the moon—now give me a nice long howl instead of last night’s yip.”   In recent years, Google has purchased seven robotics companies and several firms specializing in machine intelligence; it may now employ the world’s largest contingent of Ph.D.s in deep learning. Perhaps the most interesting acquisition is a British company called DeepMind, started in 2011 to build a general artificial intelligence. Its founders had made an early bet on deep learning, and sought to combine it with other A.I. mechanisms in a cohesive architecture. In 2013, they published the results of a test in which their system played seven classic Atari games, with no instruction other than to improve its score. For many people in A.I., the importance of the results was immediately evident. I.B.M.’s chess program had defeated Garry Kasparov, but it could not beat a three-year-old at tic-tac-toe. In six games, DeepMind’s system outperformed all previous algorithms; in three it was superhuman. In a boxing game, it learned to pin down its opponent and subdue him with a barrage of punches. Weeks after the results were released, Google bought the company, reportedly for half a billion dollars. DeepMind placed two unusual conditions on the deal: its work could never be used for espionage or defense purposes, and an ethics board would oversee the research as it drew closer to achieving A.I. Anders Sandberg had told me, “We are happy that they are among the most likely to do it. They recognize there are some problems.” DeepMind’s chief founder, Demis Hassabis, described his company to the audience at the Royal Society as an “Apollo Program” with a two-part mission: “Step one, solve intelligence. Step two, use it to solve everything else.” Since the test in 2013, his system had aced more than a dozen other Atari titles. Hassabis demonstrated an unpublished trial using a three-dimensional driving game, in which it had quickly outperformed the game’s automated drivers. The plan was to test it in increasingly complex virtual environments and, eventually, in the real world. The patent lists a range of uses, from finance to robotics.  Hassabis was clear about the challenges. DeepMind’s system still fails hopelessly at tasks that require long-range planning, knowledge about the world, or the ability to defer rewards—things that a five-year-old child might be expected to handle. The company is working to give the algorithm conceptual understanding and the capability of transfer learning, which allows humans to apply lessons from one situation to another. These are not easy problems. But DeepMind has more than a hundred Ph.D.s to work on them, and the rewards could be immense. Hassabis spoke of building artificial scientists to resolve climate change, disease, poverty. “Even with the smartest set of humans on the planet working on these problems, these systems might be so complex that it is difficult for individual humans, scientific experts,” he said. “If we can crack what intelligence is, then we can use it to help us solve all these other problems.” He, too, believes that A.I. is a gateway to expanded human potential.  The keynote speaker at the Royal Society was another Google employee: Geoffrey Hinton, who for decades has been a central figure in developing deep learning. As the conference wound down, I spotted him chatting with Bostrom in the middle of a scrum of researchers. Hinton was saying that he did not expect A.I. to be achieved for decades. “No sooner than 2070,” he said. “I am in the camp that is hopeless.” “In that you think it will not be a cause for good?” Bostrom asked.  “I think political systems will use it to terrorize people,” Hinton said. Already, he believed, agencies like the N.S.A. were attempting to abuse similar technology.  “Then why are you doing the research?” Bostrom asked. “I could give you the usual arguments,” Hinton said. “But the truth is that the prospect of discovery is too sweet.” He smiled awkwardly, the word hanging in the air—an echo of Oppenheimer, who famously said of the bomb, “When you see something that is technically sweet, you go ahead and do it, and you argue about what to do about it only after you have had your technical success.”  As the scientists retreated to tables set up for refreshments, I asked Hinton if he believed an A.I. could be controlled. “That is like asking if a child can control his parents,” he said. “It can happen with a baby and a mother—there is biological hardwiring—but there is not a good track record of less intelligent things controlling things of greater intelligence.” He looked as if he might elaborate. Then a scientist called out, “Let’s all get drinks!” Bostrom had little interest in the cocktail party. He shook a few hands, then headed for St. James’s Park, a public garden that extends from the gates of Buckingham Palace through central London. The world appeared in splendorous analog: sunlight over trees, duck ponds, children and grandparents feeding birds. The spot had been a park for hundreds of years, and the vista seemed timeless. Yet, during the past millennium, the grounds had also been a marsh, a leper hospital, a deer sanctuary, and royal gardens. It seemed plausible that, a thousand years from now, digital posthumans, regarding it as wasted space, would tear it up, replace the landscaping with computer banks, and erect a vast virtual idyll. Bostrom’s pace settled into its natural quickness as we circled the park. He talked about his family; he would be seeing his wife and son soon. He was reading widely: history, psychology, economics. He was learning to code. He was thinking about expanding his institute. Although he did not know it then, F.H.I. was about to receive one and a half million dollars from Elon Musk, to create a unit that would craft social policies informed by some of Bostrom’s theories. He would need to hire people. He was also giving thought to the framing of his message. “A lot more is said about the risks than the upsides, but that is not necessarily because the upside is not there,” he told me. “There is just more to be said about the risk—and maybe more use in describing the pitfalls, so we know how to steer around them—than spending time now figuring out the details of how we are going to furnish the great palace a thousand years from now.” “I’m bringing on Josh here for when we take over fantasy sports betting.”   We passed a fountain, near a cluster of rocks engineered to give ducks a resting place. Bostrom, in his forties, must soon contend with physical decline, and he spoke with annoyance of the first glimmers of mortality. Even though he is an Alcor member, there is no guarantee that cryonics will work. Perhaps the most radical of his visions is that superintelligent A.I. will hasten the uploading of minds—what he calls “whole-brain emulations”—technology that might not be possible for centuries, if at all. Bostrom, in his most hopeful mode, imagines emulations not only as reproductions of the original intellect “with memory and personality intact”—a soul in the machine—but as minds expandable in countless ways. “We live for seven decades, and we have three-pound lumps of cheesy matter to think with, but to me it is plausible that there could be extremely valuable mental states outside this little particular set of possibilities that might be much better,” he told me.  In his book, Bostrom considers a distant future in which trillions of digital minds merge into an enormous cognitive cyber-soup. “Whether the set of extremely positive posthuman modes of being would include some kind of dissolved bouillon, there is some uncertainty,” he said. “If you look at religious views, there are many where merging with something greater is a form of heaven, being in the presence of this enormous beauty and goodness. In many traditions, the best possible state does not involve being a little individual pursuing goals. But it is hard to get a grasp of what would be going on in that soup. Maybe some soups would not be preferable as a long-term outcome. I don’t know.” He stopped and looked ahead. “What I want to avoid is to think from our parochial 2015 view—from my own limited life experience, my own limited brain—and super-confidentially postulate what is the best form for civilization a billion years from now, when you could have brains the size of planets and billion-year life spans. It seems unlikely that we will figure out some detailed blueprint for utopia. What if the great apes had asked whether they should evolve into Homo sapiens—pros and cons—and they had listed, on the pro side, ‘Oh, we could have a lot of bananas if we became human’? Well, we can have unlimited bananas now, but there is more to the human condition than that.” ♦ Illustration by Todd St. John/Coding by Jono Brandel. \t\t\t","length":74718,"excerpt":"Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.","byline":"Raffi Khatchadourian","dir":null,"siteName":"The New Yorker","lang":"en-US"},"finalizedMeta":{"title":"The Philosopher of Doomsday","description":"Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.","author":"Condé Nast","creator":"Condé Nast","publisher":"The New Yorker","date":"2015-11-15T23:00:00.000-05:00","subject":"tags","image":"https://media.newyorker.com/photos/590971f5ebe912338a377328/16:9/w_2560,h_1440,c_limit/151123_r27342.jpg","topics":["a reporter at large","artificial intelligence (a.i.)","category_science_tech","philosophers","magazine","tags"]},"jsonLd":{"@type":"NewsArticle","headline":"The Philosopher of Doomsday","description":"Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.","image":["https://media.newyorker.com/photos/590971f5ebe912338a377328/16:9/w_2560,h_1440,c_limit/151123_r27342.jpg","https://media.newyorker.com/photos/590971f5ebe912338a377328/4:3/w_2560,h_1920,c_limit/151123_r27342.jpg","https://media.newyorker.com/photos/590971f5ebe912338a377328/1:1/w_2000,h_2000,c_limit/151123_r27342.jpg"],"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom"},"datePublished":"2015-11-15T19:00:00.000-05:00","dateModified":"2015-11-15T23:00:00.000-05:00","isAccessibleForFree":true,"isPartOf":{"@type":"CreativeWork","name":"The New Yorker"},"discussionUrl":false,"license":false,"author":[{"@type":"Person","name":"Raffi Khatchadourian","sameAs":"https://www.newyorker.com/contributors/raffi-khatchadourian"}],"publisher":{"@context":"https://schema.org","@type":"Organization","name":"The New Yorker","logo":{"@type":"ImageObject","url":"https://www.newyorker.com/verso/static/the-new-yorker/assets/the-new-yorker-seo-logo.jpg","width":"1200px","height":"630px"},"url":"https://www.newyorker.com"},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"@context":"http://schema.org","articleBody":"I. Omens\nLast year, a curious nonfiction book became a Times best-seller: a dense meditation on artificial intelligence by the philosopher Nick Bostrom, who holds an appointment at Oxford. Titled “Superintelligence: Paths, Dangers, Strategies,” it argues that true artificial intelligence, if it is realized, might pose a danger that exceeds every previous threat from technology—even nuclear weapons—and that if its development is not managed carefully humanity risks engineering its own extinction. Central to this concern is the prospect of an “intelligence explosion,” a speculative event in which an A.I. gains the ability to improve itself, and in short order exceeds the intellectual potential of the human brain by many orders of magnitude.\nSuch a system would effectively be a new kind of life, and Bostrom’s fears, in their simplest form, are evolutionary: that humanity will unexpectedly become outmatched by a smarter competitor. He sometimes notes, as a point of comparison, the trajectories of people and gorillas: both primates, but with one species dominating the planet and the other at the edge of annihilation. “Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb,” he concludes. “We have little idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound.”\nAt the age of forty-two, Bostrom has become a philosopher of remarkable influence. “Superintelligence” is only his most visible response to ideas that he encountered two decades ago, when he became a transhumanist, joining a fractious quasi-utopian movement united by the expectation that accelerating advances in technology will result in drastic changes—social, economic, and, most strikingly, biological—which could converge at a moment of epochal transformation known as the Singularity. Bostrom is arguably the leading transhumanist philosopher today, a position achieved by bringing order to ideas that might otherwise never have survived outside the half-crazy Internet ecosystem where they formed. He rarely makes concrete predictions, but, by relying on probability theory, he seeks to tease out insights where insights seem impossible.\nSome of Bostrom’s cleverest arguments resemble Swiss Army knives: they are simple, toylike, a pleasure to consider, with colorful exteriors and precisely calibrated mechanics. He once cast a moral case for medically engineered immortality as a fable about a kingdom terrorized by an insatiable dragon. A reformulation of Pascal’s wager became a dialogue between the seventeenth-­century philosopher and a mugger from another dimension.\n“Superintelligence” is not intended as a treatise of deep originality; Bostrom’s contribution is to impose the rigors of analytic philosophy on a messy corpus of ideas that emerged at the margins of academic thought. Perhaps because the field of A.I. has recently made striking advances—with everyday technology seeming, more and more, to exhibit something like intelligent reasoning—the book has struck a nerve. Bostrom’s supporters compare it to “Silent Spring.” In moral philosophy, Peter Singer and Derek Parfit have received it as a work of importance, and distinguished physicists such as Stephen Hawking have echoed its warning. Within the high caste of Silicon Valley, Bostrom has acquired the status of a sage. Elon Musk, the C.E.O. of Tesla, promoted the book on Twitter, noting, “We need to be super careful with AI. Potentially more dangerous than nukes.” Bill Gates recommended it, too. Suggesting that an A.I. could threaten humanity, he said, during a talk in China, “When people say it’s not a problem, then I really start to get to a point of disagreement. How can they not see what a huge challenge this is?”\nThe people who say that artificial intelligence is not a problem tend to work in artificial intelligence. Many prominent researchers regard Bostrom’s basic views as implausible, or as a distraction from the near-term benefits and moral dilemmas posed by the technology—not least because A.I. systems today can barely guide robots to open doors. Last summer, Oren Etzioni, the C.E.O. of the Allen Institute for Artificial Intelligence, in Seattle, referred to the fear of machine intelligence as a “Frankenstein complex.” Another leading researcher declared, “I don’t worry about that for the same reason I don’t worry about overpopulation on Mars.” Jaron Lanier, a Microsoft researcher and tech commentator, told me that even framing the differing views as a debate was a mistake. “This is not an honest conversation,” he said. “People think it is about technology, but it is really about religion, people turning to metaphysics to cope with the human condition. They have a way of dramatizing their beliefs with an end-of-days scenario—and one does not want to criticize other people’s religions.”\nBecause the argument has played out on blogs and in the popular press, beyond the ambit of peer-reviewed journals, the two sides have appeared in caricature, with headlines suggesting either doom (“Will Super-intelligent Machines Kill Us All?”) or a reprieve from doom (“Artificial intelligence ‘will not end human race’ ”). Even the most grounded version of the debate occupies philosophical terrain where little is clear. But, Bostrom argues, if artificial intelligence can be achieved it would be an event of unparalleled consequence—perhaps even a rupture in the fabric of history. A bit of long-range forethought might be a moral obligation to our own species.\nBostrom’s sole responsibility at Oxford is to direct an organization called the Future of Humanity Institute, which he founded ten years ago, with financial support from James Martin, a futurist and tech millionaire. Bostrom runs the institute as a kind of philosophical radar station: a bunker sending out navigational pulses into the haze of possible futures. Not long ago, an F.H.I. fellow studied the possibility of a “dark fire scenario,” a cosmic event that, he hypothesized, could occur under certain high-energy conditions: everyday matter mutating into dark matter, in a runaway process that could erase most of the known universe. (He concluded that it was highly unlikely.) Discussions at F.H.I. range from conventional philosophic topics, like the nature of compromise, to the optimal structure of space empires—whether a single intergalactic machine intelligence, supported by a vast array of probes, presents a more ethical future than a cosmic imperium housing millions of digital minds.\nEarlier this year, I visited the institute, which is situated on a winding street in a part of Oxford that is a thousand years old. It takes some work to catch Bostrom at his office. Demand for him on the lecture circuit is high; he travels overseas nearly every month to relay his technological omens in a range of settings, from Google’s headquarters to a Presidential commission in Washington. Even at Oxford, he maintains an idiosyncratic schedule, remaining in the office until two in the morning and returning sometime the next afternoon.\nI arrived before he did, and waited in a hallway between two conference rooms. A plaque indicated that one of them was the Arkhipov Room, honoring Vasili Arkhipov, a Soviet naval officer. During the Cuban missile crisis, Arkhipov was serving on a submarine in the Caribbean when U.S. destroyers set off depth charges nearby. His captain, unable to establish radio contact with Moscow, feared that the conflict had escalated and ordered a nuclear strike. But Arkhipov dissuaded him, and all-out atomic war was averted. Across the hallway was the Petrov Room, named for another Soviet officer who prevented a global nuclear catastrophe. Bostrom later told me, “They may have saved more lives than most of the statesmen we celebrate on stamps.”\nThe sense that a vanguard of technical-minded people working in obscurity, at odds with consensus, might save the world from auto-annihilation runs through the atmosphere at F.H.I. like an electrical charge. While waiting for Bostrom, I peered through a row of windows into the Arkh­ipov Room, which looked as though it was used for both meetings and storage; on a bookcase there were boxes containing light bulbs, lampshades, cables, spare mugs. A gaunt philosophy Ph.D. wrapped in a thick knitted cardigan was pacing in front of a whiteboard covered in notation, which he attacked in bursts. After each paroxysm, he paced, hands behind his back, head tilted downward. At one point, he erased a panel of his work. Taking this as an opportunity to interrupt, I asked him what he was doing. “It is a problem involving an aspect of A.I. called ‘planning,’ ” he said. His demeanor radiated irritation. I left him alone.\nBostrom arrived at 2 p.m. He has a boyish countenance and the lean, vital physique of a yoga instructor—though he could never be mistaken for a yoga instructor. His intensity is too untidily contained, evident in his harried gait on the streets outside his office (he does not drive), in his voracious consumption of audiobooks (played at two or three times the normal speed, to maximize efficiency), and his fastidious guarding against illnesses (he avoids handshakes and wipes down silverware beneath a tablecloth). Bostrom can be stubborn about the placement of an office plant or the choice of a font. But when his arguments are challenged he listens attentively, the mechanics of consideration nearly dis­cernible beneath his skin. Then, calmly, quickly, he dispatches a response, one idea interlocked with another.\nHe asked if I wanted to go to the market. “You can watch me make my elixir,” he said. For the past year or so, he has been drinking his lunch (another efficiency): a smoothie containing fruits, vegetables, proteins, and fats. Using his elbow, he hit a button that electronically opened the front door. Then we rushed out.\nBostrom has a reinvented man’s sense of lost time. An only child, he grew up—as Niklas Boström—in Helsingborg, on the southern coast of Sweden. Like many exceptionally bright children, he hated school, and as a teen-ager he developed a listless, romantic persona. In 1989, he wandered into a library and stumbled onto an anthology of nineteenth-century German philosophy, containing works by Nietzsche and Schopenhauer. He read it in a nearby forest, in a clearing that he often visited to think and to write poetry, and experienced a euphoric insight into the possibilities of learning and achievement. “It’s hard to convey in words what that was like,” Bostrom told me; instead he sent me a photograph of an oil painting that he had made shortly afterward. It was a semi-representational landscape, with strange figures crammed into dense undergrowth; beyond, a hawk soared below a radiant sun. He titled it “The First Day.”\nDeciding that he had squandered his early life, he threw himself into a campaign of self-education. He ran down the citations in the anthology, branching out into art, literature, science. He says that he was motivated not only by curiosity but also by a desire for actionable knowledge about how to live. To his parents’ dismay, Bostrom insisted on finishing his final year of high school from home by taking special exams, which he completed in ten weeks. He grew distant from old friends: “I became quite fanatical and felt quite isolated for a period of time.”\nWhen Bostrom was a graduate student in Stockholm, he studied the work of the analytic philosopher W. V. Quine, who had explored the difficult relationship between language and reality. His adviser drilled precision into him by scribbling “not clear” throughout the margins of his papers. “It was basically his only feedback,” Bostrom told me. “The effect was still, I think, beneficial.” His previous academic interests had ranged from psychology to mathematics; now he took up theoretical physics. He was fascinated by technology. The World Wide Web was just emerging, and he began to sense that the heroic philosophy which had inspired him might be outmoded. In 1995, Bostrom wrote a poem, “Requiem,” which he told me was “a signing-off letter to an earlier self.” It was in Swedish, so he offered me a synopsis: “I describe a brave general who has overslept and finds his troops have left the encampment. He rides off to catch up with them, pushing his horse to the limit. Then he hears the thunder of a modern jet plane streaking past him across the sky, and he realizes that he is obsolete, and that courage and spiritual nobility are no match for machines.”\nAlthough Bostrom did not know it, a growing number of people around the world shared his intuition that technology could cause transformative change, and they were finding one another in an online discussion group administered by an organization in California called the Extropy Institute. The term “extropy,” coined in 1967, is generally used to describe life’s capacity to reverse the spread of entropy across space and time. Extropianism is a libertarian strain of transhumanism that seeks “to direct human evolution,” hoping to eliminate disease, suffering, even death; the means might be genetic modification, or as yet un­invented nanotechnology, or perhaps dispensing with the body entirely and uploading minds into supercomputers. (As one member noted, “Immortality is mathematical, not mystical.”) The Extropians advocated the development of artificial superintelligence to achieve these goals, and they envisioned humanity colonizing the universe, converting inert matter into engines of civilization. The discussions were nerdy, lunatic, imaginative, thought-provoking. Anders Sandberg, a former member of the group who now works at Bostrom’s institute, told me, “Just imagine if you could listen in on the debates of the Italian Futurists or early Surrealists.”\nIn 1996, while pursuing further graduate work at the London School of Economics, Bostrom learned about the Extropy discussion group and became an active participant. A year later, he co-founded his own organization, the World Transhumanist Association, which was less libertarian and more academically spirited. He crafted approachable statements on transhumanist values and gave interviews to the BBC. The line between his academic work and his activism blurred: his Ph.D. dissertation centered on a study of the Doomsday Argument, which uses probability theory to make inferences about the longevity of human civilization. The work baffled his advisers, who respected him but rarely agreed with his conclusions. Mostly, they left him alone.\nBostrom had little interest in conventional philosophy—not least because he expected that superintelligent minds, whether biologically enhanced or digital, would make it obsolete. “Suppose you had to build a new subway line, and it was this grand trans-generational enterprise that humanity was engaged in, and everybody had a little role,” he told me. “So you have a little shovel. But if you know that a giant bulldozer will arrive on the scene tomorrow, then does it really make sense to spend your time today digging the big hole with your shovel? Maybe there is something else you could do with your time. Maybe you could put up a signpost for the great shovel, so it will start digging in the right place.” He came to believe that a key role of the philosopher in modern society was to acquire the knowledge of a polymath, then use it to help guide humanity to its next phase of existence—a discipline that he called “the philosophy of technological prediction.” He was trying to become such a seer.\n“He was ultra-consistent,” Daniel Hill, a British philosopher who befriended Bostrom while they were graduate students in London, told me. “His interest in science was a natural outgrowing of his understandable desire to live forever, basically.”\nBostrom has written more than a hundred articles, and his longing for immortality can be seen throughout. In 2008, he framed an essay as a call to action from a future utopia. “Death is not one but a multitude of assassins,” he warned. “Take aim at the causes of early death—infection, violence, malnutrition, heart attack, cancer. Turn your biggest gun on aging, and fire. You must seize the biochemical processes in your body in order to vanquish, by and by, illness and senescence. In time, you will discover ways to move your mind to more durable media.” He tends to see the mind as immaculate code, the body as inefficient hardware—able to accommodate limited hacks but probably destined for replacement.\nEven Bostrom’s marriage is largely mediated by technology. His wife, Susan, has a Ph.D. in the sociology of medicine and a bright, down-to-earth manner. (“She teases me about the Terminator and the robot army,” he told me.) They met thirteen years ago, and for all but six months they have lived on opposite sides of the Atlantic, even after the recent birth of their son. The arrangement is voluntary: she prefers Montreal; his work keeps him at Oxford. They Skype several times a day, and he directs as much international travel as possible through Canada, so they can meet in non-digital form.\nIn Oxford, as Bostrom shopped for his smoothie, he pointed out a man vaping. “There is also the more old-school method of taking nicotine: chewing gum,” he told me. “I do chew nicotine gum. I read a few papers saying it might have some nootropic effect”—that is, it might enhance cognition. He drinks coffee, and usually abstains from alcohol. He briefly experimented with the smart drug Modafinil, but gave it up.\nBack at the institute, he filled an industrial blender with lettuce, carrots, cauliflower, broccoli, blueberries, turmeric, vanilla, oat milk, and whey powder. “If there is one thing Nick cares about, it is minds,” Sandberg told me. “That is at the root of many of his views about food, because he is worried that toxin X or Y might be bad for his brain.” He suspects that Bostrom also enjoys the ritualistic display. “Swedes are known for their smugness,” he joked. “Perhaps Nick is subsisting on smugness.”\nA young employee eyed Bostrom getting ready to fire up the blender. “I can tell when Nick comes into the office,” he said. “My hair starts shaking.”\n“Yeah, this has got three horsepower,” Bostrom said. He ran the blender, producing a noise like a circular saw, and then filled a tall glass stein with purple-­green liquid. We headed to his office, which was meticulous. By a window was a wooden desk supporting an iMac and not another item; against a wall were a chair and a cabinet with a stack of documents. The only hint of excess was light: there were fourteen lamps.\nIt is hard to spend time at Bostrom’s institute without drifting into reveries of a far future. What might humanity look like millions of years from now? The upper limit of survival on Earth is fixed to the life span of the sun, which in five billion years will become a red giant and swell to more than two hundred times its present size. It is possible that Earth’s orbit will adjust, but more likely that the planet will be destroyed. In any case, long before then, nearly all plant life will die, the oceans will boil, and the Earth’s crust will heat to a thousand degrees. In half a billion years, the planet will be uninhabitable.\nThe view of the future from Bostrom’s office can be divided into three grand panoramas. In one, humanity experiences an evolutionary leap—either assisted by technology or by merging into it and becoming software—to achieve a sublime condition that Bostrom calls “posthumanity.” Death is overcome, mental experience expands beyond recognition, and our descendants colonize the universe. In another panorama, humanity becomes extinct or experiences a disaster so great that it is unable to recover. Between these extremes, Bostrom envisions scenarios that resemble the status quo—people living as they do now, forever mired in the “human era.” It’s a vision familiar to fans of sci-fi: on “Star Trek,” Captain Kirk was born in the year 2233, but when an alien portal hurls him through time and space to Depression-era Manhattan he blends in easily.\nBostrom dislikes science fiction. “I’ve never been keen on stories that just try to present ‘wow’ ideas—the equivalent of movie productions that rely on stunts and explosions to hold the attention,” he told me. “The question is not whether we can think of something radical or extreme but whether we can discover some sufficient reason for updating our credence function.”\nHe believes that the future can be studied with the same meticulousness as the past, even if the conclusions are far less firm. “It may be highly unpredictable where a traveller will be one hour after the start of her journey, yet predictable that after five hours she will be at her destination,” he once argued. “The very long-term future of humanity may be relatively easy to predict.” He offers an example: if history were reset, the industrial revolution might occur at a different time, or in a different place, or perhaps not at all, with innovation instead occurring in increments over hundreds of years. In the short term, predicting technological achievements in the counter-history might not be possible; but after, say, a hundred thousand years it is easier to imagine that all the same inventions would have emerged.\nBostrom calls this the Technological Completion Conjecture: “If scientific- and technological-development efforts do not effectively cease, then all impor­t­­­ant basic capabilities that could be obtained through some possible technology will be obtained.” In light of this, he suspects that the farther into the future one looks the less likely it seems that life will continue as it is. He favors the far ends of possibility: humanity becomes transcendent or it perishes.\nIn the nineteen-nineties, as these ideas crystallized in his thinking, Bostrom began to give more attention to the question of extinction. He did not believe that doomsday was imminent. His interest was in risk, like an insurance agent’s. No matter how improbable extinction may be, Bostrom argues, its consequences are near-infinitely bad; thus, even the tiniest step toward reducing the chance that it will happen is near-­infinitely valuable. At times, he uses arithmetical sketches to illustrate this point. Imagining one of his utopian scenarios—trillions of digital minds thriving across the cosmos—he reasons that, if there is even a one-per-cent chance of this happening, the expected value of reducing an existential threat by a billionth of a billionth of one per cent would be worth a hundred billion times the value of a billion present-day lives. Put more simply: he believes that his work could dwarf the moral importance of anything else.\nBostrom introduced the philosophical concept of “existential risk” in 2002, in the Journal of Evolution and Technology. In recent years, new organizations have been founded almost annually to help reduce it—among them the Centre for the Study of Existential Risk, affiliated with Cambridge Uni­versity, and the Future of Life Institute, which has ties to the Massachusetts Institute of Technology. All of them face a key problem: Homo sapiens, since its emergence two hundred thousand years ago, has proved to be remarkably resilient, and figuring out what might imperil its existence is not obvious. Climate change is likely to cause vast environmental and economic damage—but it does not seem impossible to survive. So-called super-volcanoes have thus far not threatened the perpetuation of the species. NASA spends forty million dollars each year to determine if there are significant comets or asteroids headed for Earth. (There aren’t.)\nBostrom does not find the lack of obvious existential threats comforting. Because it is impossible to endure extinction twice, he argues, we cannot rely on history to calculate the probability that it will occur. The most worrying dangers are those that Earth has never encountered before. “It is hard to cause human extinction with seventeenth-century technology,” Bostrom told me. Three centuries later, though, the prospect of a technological apocalypse was urgently plausible. Bostrom dates the first scientific analysis of existential risk to the Manhattan Project: in 1942, Robert Oppenheimer became concerned that an atomic detonation of sufficient power could cause the entire atmosphere to ignite. A subsequent study concluded that the scenario was “unreasonable,” given the limitations of the weapons then in development. But even if the great nuclear nightmares of the Cold War did not come true, the tools were there to cause destruction on a scale not previously possible. As innovations grow even more complex, it is increasingly difficult to evaluate the dangers ahead. The answers must be fraught with ambiguity, because they can be derived only by predicting the effects of technologies that exist mostly as theories or, even more indirectly, by using abstract reasoning.\nAs a philosopher, Bostrom takes a sweeping, even cosmic, view of such problems. One afternoon, he told me, “The probabilities that any given planet will produce intelligent life—this may also have action-relevant information.” In the past several years, NASA probes have found increasing evidence that the building blocks of life are abundant throughout space. So much water has been discovered—on Mars and on the moons of Jupiter and Saturn—that one scientist described our solar system as “a pretty soggy place.” There are amino acids on icy comets and complex organic molecules in distant star-forming clouds. On this planet, life has proved capable of thriving in unimaginably punishing conditions: without oxygen, without light, at four hundred degrees above or below zero. In 2007, the European Space Agency hitched tiny creatures to the exterior of a satellite. They not only survived the flight; some even laid eggs afterward.\nWith ten billion Earth-like planets in our galaxy alone, and a hundred billion galaxies in the universe, there is good reason to suspect that extraterrestrial life may one day be discovered. For Bostrom, this would augur disaster. “It would be great news to find that Mars is a completely sterile planet,” he argued not long ago. “Dead rocks and lifeless sands would lift my spirits.” His reasoning begins with the age of the universe. Many of those Earth-like planets are thought to be far, far older than ours. One that was recently discovered, called Kepler 452b, is as much as one and a half billion years older. Bostrom asks: If life had formed there on a time scale resembling our own, what would it look like? What kind of technological progress could a civilization achieve with a head start of hundreds of millions of years?\nLife as we know it tends to spread wherever it can, and Bostrom estimates that, if an alien civilization could design space probes capable of travelling at even one per cent of the speed of light, the entire Milky Way could be colonized in twenty million years—a tiny fraction of the age difference between Kepler 452b and Earth. One could argue that no technology will ever propel ships at so great a speed. Or perhaps millions of alien civilizations possess the know-how for intergalactic travel, but they aren’t interested. Even so, because the universe is so colossal, and because it is so old, only a small number of civilizations would need to behave as life does on Earth—unceasingly expanding—in order to be visible. Yet, as Bostrom notes, “You start with billions and billions of potential germination points for life, and you end up with a sum total of zero alien civilizations that developed technologically to the point where they become manifest to us earthly observers. So what’s stopping them?”\nIn 1950, Enrico Fermi sketched a version of this paradox during a lunch break while he was working on the H-bomb, at Los Alamos. Since then, many resolutions have been proposed—some of them exotic, such as the idea that Earth is housed in an interplanetary alien zoo. Bostrom suspects that the answer is simple: space appears to be devoid of life because it is. This implies that intelligent life on Earth is an astronomically rare accident. But, if so, when did that accident occur? Was it in the first chemical reactions in the primordial soup? Or when single-celled organisms began to replicate using DNA? Or when animals learned to use tools? Bos­trom likes to think of these hurdles as Great Filters: key phases of improbability that life everywhere must pass through in order to develop into intelligent species. Those which do not make it either go extinct or fail to evolve.\nThus, for Bostrom, the discovery of a single-celled creature inhabiting a damp stretch of Martian soil would constitute a disconcerting piece of evidence. If two planets independently evolved primitive organisms, then it seems more likely that this type of life can be found on many planets throughout the universe. Bostrom reasons that this would suggest that the Great Filter comes at some later evolutionary stage. The discovery of a fossilized vertebrate would be even worse: it would suggest that the universe appears lifeless not because complex life is unusual but, rather, because it is always somehow thwarted before it becomes advanced enough to colonize space.\nIn Bostrom’s view, the most distressing possibility is that the Great Filter is ahead of us—that evolution frequently achieves civilizations like our own, but they perish before reaching their technological maturity. Why might that be? “Natural disasters such as asteroid hits and super-­volcanic eruptions are unlikely Great Filter candidates, because, even if they destroyed a significant number of civilizations, we would expect some civilizations to get lucky and escape disaster,” he argues. “Perhaps the most likely type of existential risks that could constitute a Great Filter are those that arise from technological discovery. It is not far-fetched to suppose that there might be some possible technology which is such that (a) virtually all suffi­ciently advanced civilizations eventually discover it and (b) its discovery leads almost universally to existential disaster.”\nII. The Machines\nThe field of artificial intelligence was born in a fit of scientific optimism, in 1955, when a small group of researchers—three mathematicians and an I.B.M. programmer—drew up a proposal for a project at Dartmouth. “An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves,” they stated. “We think a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.”\nTheir optimism was understandable. Since the turn of the twentieth century, science had been advancing at a breakneck pace: the discovery of radioactivity quickly led to insights into the inner workings of the atom, and then to the development of controlled nuclear energy, and then to the warheads over Hiroshima and Nagasaki, and then to the H-bomb. This rush of discovery was reflected in fiction, too, in the work of Isaac Asimov, among others, who envisioned advanced civilizations inhabited by intelligent robots (each encoded with simple, ethical Laws of Robotics, to prevent it from causing harm). The year the scientists met at Dartmouth, Asimov published “The Last Question,” a story featuring a superintelligent A.I. that is continually “self-adjusting and self-correcting”—gaining knowledge as it helps human civilization expand throughout the universe. When the universe’s last stars start dying out, all humanity uploads itself into the A.I., and the device, achieving godhood, creates a new cosmos.\nScientists perceived the mechanics of intelligence—like those of the atom—as a source of huge potential, a great frontier. If the brain was merely a biological machine, there was no theoretical reason that it could not be replicated, or even surpassed, much the way a jet could outfly a falcon. Even before the Dartmouth conference, machines exceeded human ability in narrow domains like code-breaking. In 1951, Alan Turing argued that at some point computers would probably exceed the intellectual capacity of their inventors, and that “therefore we should have to expect the machines to take control.” Whether this would be good or bad he did not say.\nSix years later, Herbert Simon, one of the Dartmouth attendees, declared that machines would achieve human intelligence “in a visible future.” The crossing of such a threshold, he suspected, could be psychologically crushing, but he was on the whole optimistic. “We must also remain sensitive to the need to keep the computer’s goals attuned with our own,” he later said, but added, “I am not convinced that this will be difficult.” For other computer pioneers, the future appeared more ambivalent. Norbert Wiener, the father of cybernetics, argued that it would be difficult to manage powerful computers, or even to accurately predict their behavior. “Complete subservience and complete intelligence do not go together,” he said. Envisioning Sorcerer’s Apprentice scenarios, he predicted, “The future will be an ever more demanding struggle against the limitations of our intelligence, not a comfortable hammock in which we can lie down to be waited upon by our robot slaves.”\nIt was in this milieu that the “intelligence explosion” idea was first formally expressed by I. J. Good, a statistician who had worked with Turing. “An ultraintelligent machine could design even better machines,” he wrote. “There would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.”\nThe scientists at Dartmouth recognized that success required answers to fundamental questions: What is intelligence? What is the mind? By 1965, the field had experimented with several models of problem solving: some were based on formal logic; some used heuristic reasoning; some, called “neural networks,” were inspired by the brain. With each, the scientists’ work indicated that A.I. systems could find their own solutions to problems. One algorithm proved numerous theorems in the classic text “Principia Mathematica,” and in one instance it did so more elegantly than the authors. A program designed to play checkers learned to beat its programmer. And yet, despite the great promise in these experiments, the challenges to creating an A.I. were forbidding. Programs that performed well in the laboratory were useless in everyday situations; a simple act like picking up a ball turned out to require an overwhelming number of computations.\nThe research fell into the first of several “A.I. winters.” As Bostrom notes in his book, “Among academics and their funders, ‘A.I.’ became an unwanted epithet.” Eventually, the researchers started to question the goal of building a mind altogether. Why not try instead to divide the problem into pieces? They began to limit their interests to specific cognitive functions: vision, say, or speech. Even in isolation, these functions would have value: a computer that could identify objects might not be an A.I., but it could help guide a forklift. As the research fragmented, the morass of technical problems made any questions about the consequences of success seem distant, even silly.\nUnexpectedly, by dismissing its founding goals, the field of A.I. created space for outsiders to imagine more freely what the technology might look like. Bostrom wrote his first paper on artificial superintelligence in the nineteen-nineties, envisioning it as potentially perilous but irresistible to both commerce and government. “If there is a way of guaranteeing that superior artificial intellects will never harm human beings, then such intellects will be created,” he argued. “If there is no way to have such a guarantee, then they will probably be created nevertheless.” His audience at the time was primarily other transhumanists. But the movement was maturing. In 2005, an organization called the Singularity Institute for Artificial Intelligence began to operate out of Silicon Valley; its primary founder, a former member of the Extropian discussion group, published a stream of literature on the dangers of A.I. That same year, the futurist and inventor Ray Kurzweil wrote “The Singularity Is Near,” a best-seller that prophesied a merging of man and machine in the foreseeable future. Bostrom created his institute at Oxford.\nThe two communities could not have been more different. The scientists, steeped in technical detail, were preoccupied with making devices that worked; the transhumanists, motivated by the hope of a utopian future, were asking, What would the ultimate impact of those devices be? In 2007, the Association for the Advancement of Artificial Intelligence—the most prominent professional organization for A.I. researchers—elected Eric Horvitz, a scientist from Microsoft, as its president. Until then, it had given virtually no attention to the ethical and social implications of the research, but Horvitz was open to the big questions. “It is hard to understand what success would mean for A.I.,” he told me. “I was friendly with Jack Good, who wrote that piece on superintelligence. I knew him as a creative, funny guy who referred to a lot of his ideas as P.B.I.s—partly baked ideas. And here is this piece of his being opened up outside the field as this Bible and studied with a silver pointer. Wouldn’t it be useful, I said, even if you thought these were crazy or low-probability scenarios, to find out: Can we be proactive, should there be some poor outcome for humanity?”\nHorvitz organized a meeting at the Asilomar Conference Grounds, in California, a place chosen for its symbolic value: biologists had gathered there in 1975 to discuss the hazards of their research in the age of modern genetics. He divided the researchers into groups. One studied short-term ramifications, like the possible use of A.I. to commit crimes; another considered long-term consequences. Mostly, there was skepticism about the intelligence-explosion idea, which assumed answers to many unresolved questions. No one fully understands what intelligence is, let alone how it might evolve in a machine. Can it grow as Good imagined, gaining I.Q. points like a rocketing stock price? If so, what would its upper limit be? And would its increase be merely a function of optimized software design, without the difficult process of acquiring knowledge through experience? Can software fundamentally rewrite itself without risking crippling breakdowns? No one knows. In the history of computer science, no programmer has created code that can substantially improve itself.\nBut the notion of an intelligence explosion was also impossible to disprove. It was theoretically coherent, and it had even been attempted in limited ways. David McAllester, an A.I. researcher at the Toyota Technological Institute, affiliated with the University of Chicago, headed the long-term panel. The idea, he argued, was worth taking seriously. “I am uncomfortable saying that we are ninety-­nine per cent certain that we are safe for fifty years,” he told me. “That feels like hubris to me.” The group concluded that more technical work was needed before an evaluation of the dangers could be made, but it also hinted at a concern among panelists that the gathering was based on “a perception of urgency”—generated largely by the transhumanists—and risked raising unfounded alarm. With A.I. seeming like a remote prospect, the researchers declared, attention was better spent on near-term concerns. Bart Selman, a professor at Cornell who co-­organized the panel, told me, “The mode was ‘This is interesting, but it’s all academic—it’s not going to happen.’ ”\nAt the time the A.I researchers met at Asilomar, Bostrom was grappling with an expansive book on existential risks. He had sketched out chapters on bioengineering and on nanotechnology, among other topics, but many of these problems came to seem less compelling, while his chapter on A.I. grew and grew. Eventually, he pasted the A.I. chapter into a new file, which became “Superintelligence.”\nThe book is its own elegant paradox: analytical in tone and often lucidly argued, yet punctuated by moments of messianic urgency. Some portions are so extravagantly speculative that it is hard to take them seriously. (“Suppose we could somehow establish that a certain future AI will have an IQ of 6,455: then what?”) But Bostrom is aware of the limits to his type of futurology. When he was a graduate student in London, thinking about how to maximize his ability to communicate, he pursued stand­­up comedy; he has a deadpan sense of humor, which can be found lightly buried among the book’s self-serious passages. “Many of the points made in this book are probably wrong,” he writes, with an endnote that leads to the line “I don’t know which ones.”\nBostrom prefers to act as a cartographer rather than a polemicist, but beneath his exhaustive mapping of scenarios one can sense an argument being built and perhaps a fear of being forthright about it. “Traditionally, this topic domain has been occupied by cranks,” he told me. “By popular media, by science fiction—or maybe by a retired physicist no longer able to do serious work, so he will write a popular book and pontificate. That is kind of the level of rigor that is the baseline. I think that a lot of reasons why there has not been more serious work in this area is that academics don’t want to be conflated with flaky, crackpot type of things. Futurists are a certain type.”\nThe book begins with an “unfinished” fable about a flock of sparrows that decide to raise an owl to protect and advise them. They go looking for an owl egg to steal and bring back to their tree, but, because they believe their search will be so difficult, they postpone studying how to domesticate owls until they succeed. Bostrom concludes, “It is not known how the story ends.”\nThe parable is his way of introducing the book’s core question: Will an A.I., if realized, use its vast capability in a way that is beyond human control? One way to think about the concern is to begin with the familiar. Bos­trom writes, “Artificial intelligence already outperforms human intelligence in many domains.” The examples range from chess to Scrabble. One program from 1981, called Eurisko, was designed to teach itself a naval role-playing game. After playing ten thousand matches, it arrived at a morally grotesque strategy: to field thousands of small, immobile ships, the vast majority of which were intended as cannon fodder. In a national tournament, Eurisko demolished its human opponents, who insisted that the game’s rules be changed. The following year, Eurisko won again—by forcing its damaged ships to sink themselves.\nThe program was by no means superintelligent. But Bostrom’s book essentially asks: What if it were? Assume that it has a broad ability to consider problems and that it has access to the Internet. It could read and acquire general knowledge and communicate with people seamlessly online. It could conduct experiments, either virtually or by tinkering with networked infrastructure. Given even the most benign objective—to win a game—such a system, Bostrom argues, might develop “instrumental goals”: gather resources, or invent technology, or take steps to insure that it cannot be turned off, in the process paying as much heed to human life as humans do to ants.\nIn people, intelligence is inseparable from consciousness, emotional and social awareness, the complex interaction of mind and body. An A.I. need not have any such attributes. Bostrom believes that machine intelligences—no matter how flexible in their tactics—will likely be rigidly fixated on their ultimate goals. How, then, to create a machine that respects the nuances of social cues? That adheres to ethical norms, even at the expense of its goals? No one has a coherent solution. It is hard enough to reliably inculcate such behavior in people.\nIn science fiction, superintelligent computers that run amok are often circumvented at the last minute; think of WOPR, the computer in “WarGames,” which was stopped just short of triggering nuclear war, or HAL 9000, which was reduced to helplessly singing while it watched itself get dismantled. For Bos­trom, this strains credulity. Whether out of a desire to consider the far ends of risk or out of transhumanist longings, he often ascribes nearly divine abilities to machines, as if to ask: Can a digital god really be contained? He imagines machines so intelligent that merely by inspecting their own code they can extrapolate the nature of the universe and of human society, and in this way outsmart any effort to contain them. “Is it possible to build machines that are not like agents—goal-pursuing, autonomous, artificial intelligences?” he asked me. “Maybe you can design something more like an oracle that can only answer yes or no. Would that be safer? It is not so clear. There might be agent-like processes within it.” Asking a simple question—“Is it possible to convert a DeLorean into a time machine and travel to 1955?”—might trigger a cascade of action as the device tests hypotheses. What if, working through a police computer, it impounds a DeLorean that happens to be convenient to a clock tower? “In fairy tales, you have genies who grant wishes,” Bostrom said. “Almost universally, the moral of those is that if you are not extremely careful what you wish for, then what seems like it should be a great blessing turns out to be a curse.”\nBostrom worries that solving the “control problem”—insuring that a superintelligent machine does what humans want it to do—will require more time than solving A.I. does. The intelligence explosion is not the only way that a superintelligence might be created suddenly. Bostrom once sketched out a decades-long process, in which researchers arduously improved their systems to equal the intelligence of a mouse, then a chimp, then—after incredible labor—the village idiot. “The difference between village idiot and genius-­level intelligence might be trivial from the point of view of how hard it is to replicate the same functionality in a machine,” he said. “The brain of the village idiot and the brain of a scientific genius are almost identical. So we might very well see relatively slow and incremental progress that doesn’t really raise any alarm bells until we are just one step away from something that is radically superintelligent.”\nTo a large degree, Bostrom’s concerns turn on a simple question of timing: Can breakthroughs be predicted? “It is ridiculous to talk about such things so early—A.I. is eons away,” Edward Feigenbaum, an emeritus professor at Stanford University, told me. The researcher Oren Etzioni, who used the term “Frankenstein complex” to dismiss the “dystopian vision of A.I.,” concedes Bostrom’s overarching point: that the field must one day confront profound philosophical questions. Decades ago, he explored them himself, in a brief paper, but concluded that the problem was too remote to think about productively. “Once, Nick Bostrom gave a talk, and I gave a little counterpoint,” he told me. “A lot of the disagreements come down to what time scale you are thinking about. Nobody responsible would say you will see anything remotely like A.I. in the next five to ten years. And I think most computer scientists would say, ‘In a million years—we don’t see why it shouldn’t happen.’ So now the question is: What is the rate of progress? There are a lot of people who will ask: Is it possible we are wrong? Yes. I am not going to rule it out. I am going to say, ‘I am a scientist. Show me the evidence.’ ”\nThe history of science is an uneven guide to the question: How close are we? There has been no shortage of unfulfilled promises. But there are also plenty of examples of startling nearsightedness, a pattern that Arthur C. Clarke enshrined as Clarke’s First Law: “When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong.” After the electron was discovered, at Cambridge, in 1897, physicists at an annual dinner toasted, “To the electron: may it never be of use to anybody.” Lord Kelvin famously declared, just eight years before the Wright brothers launched from Kitty Hawk, that heavier-than-air flight was impossible.\nStuart Russell, the co-author of the textbook “Artificial Intelligence: A Modern Approach” and one of Bostrom’s most vocal supporters in A.I., told me that he had been studying the physics community during the advent of nuclear weapons. At the turn of the twentieth century, Ernest Rutherford discovered that heavy elements produced radiation by atomic decay, confirming that vast reservoirs of energy were stored in the atom. Rutherford believed that the energy could not be harnessed, and in 1933 he proclaimed, “Anyone who expects a source of power from the transformation of these atoms is talking moonshine.” The next day, a former student of Einstein’s named Leo Szilard read the comment in the papers. Irritated, he took a walk, and the idea of a nuclear chain reaction occurred to him. He visited Rutherford to discuss it, but Rutherford threw him out. Einstein, too, was skeptical about nuclear energy—splitting atoms at will, he said, was “like shooting birds in the dark in a country where there are only a few birds.” A decade later, Szilard’s insight was used to build the bomb.\nRussell now relays the story to A.I. researchers as a cautionary tale. “There will have to be more breakthroughs to get to A.I., but, as Szilard illustrated, those can happen overnight,” he told me. “People are putting billions of dollars into achieving those breakthroughs. As the debate stands, Bostrom and others have said, ‘If we achieve superintelligence, here are some of the problems that might arise.’ As far as I know, no one has proved why those are not real.”\nIII. Mission Control\nThe offices of the Future of Humanity Institute have a hybrid atmosphere: part physics lab, part college dorm room. There are whiteboards covered with mathematical notation and technical glyphs; there are posters of “Brave New World” and HAL 9000. There is also art work by Nick Bostrom. One afternoon, he guided me to one of his pieces, “At Sea,” a digital collage that he had printed out and then drawn on. “It is a bit damaged, but the good thing about digital is that you can re-instantiate it,” he said. At the center was a pale man, nearly an apparition, clinging to a barrel in an inky-black ocean. “It is an existentialist vibe. You are hanging on for as long as you can. When you get tired, you sink, and become fish food—or maybe a current will take him to land. We don’t know.”\nDespite the time he spends going to conferences and raising money, Bostrom attends to many details at the institute. “We needed a logo when we started,” he told me. “We went to this online site where you could buy the work of freelance artists. If you sat down and tried to make the ugliest logo, you couldn’t come close. Then we hired a designer, who made a blurry figure of a person. We showed it to someone here, who said it looked like a toilet sign. As soon as she said it, I thought, Oh, my God, we almost adopted a toilet sign as our logo. So I mucked around a bit and came up with a black diamond. You have the black monolith from ‘2001.’ Standing on its corner, it indicates instability. Also, there is a limit to how ugly a black square can be.”\nThe institute shares office space with the Centre for Effective Altruism, and both organizations intersect with a social movement that promotes pure rationality as a guide to moral action. Toby Ord, a philosopher who works with both, told me that Bostrom often pops into his office at the end of the day, poses a problem, then leaves him pondering it for the night. Among the first of Bostrom’s questions was this: If the universe turns out to contain an infinite number of beings, then how could any single person’s action affect the cosmic balance of suffering and happiness? After lengthy discussions, they left the paradox unresolved. “My main thinking is that we can sort it out later,” Ord told me.\nWhen I asked Bostrom if I could observe a discussion at the institute, he seemed reluctant; it was hard to judge whether he was concerned that my presence would interfere or that unfiltered talk of, say, engineered pathogens might inspire criminals. (“At some point, one gets into the realm of information hazard,” he hinted.) Eventually, he let me observe a session in the Petrov Room involving half a dozen staff members. The key question under discussion was whether a global catastrophe, on the order of a continent-wide famine, could trigger a series of geopolitical events that would result in human extinction—and whether that meant that a merely catastrophic risk could therefore be taken as seriously as an existential risk. Bostrom, wearing a gray hoodie over a blue button-­down, organized the problem on a whiteboard with visible pleasure. Anders Sandberg told me that he once spent days with Bostrom working through such a problem, distilling a complex argument to its essence. “He had to refine it,” he said. “We had a lot of schemes on the whiteboard that gradually were simplified to one box and three arrows.”\nFor anyone in the business of publicizing existential risk, 2015 began as a good year. Other institutes devoted to these issues had started to find their voice, bringing an additional gloss of respectability to the ideas in Bostrom’s book. The people weighing in now were no longer just former Extropians. They were credentialled, like Lord Martin Rees, an astrophysicist and the co-founder of Cambridge’s Centre for the Study of Existential Risk. In January, he wrote of A.I., in the Evening Standard, “We don’t know where the boundary lies between what may happen and what will remain science fiction.”\nRees’s counterpart at the Future of Life Institute, the M.I.T. physicist Max Tegmark, hosted a closed-door meeting in Puerto Rico, to try to make sense of the long-term trajectory of the research. Bostrom flew down, joining a mix of A.I. practitioners, legal scholars, and, for lack of a better term, members of the “A.I. safety” community. “These are not people who are usually in the same room,” Tegmark told me. “Someone advised me to put Valium in people’s drinks so nobody got into fistfights. But, by the time Nick’s session started, people were ready to listen to each other.” Questions that had seemed fanciful to researchers only seven years earlier were beginning to look as though they might be worth reconsidering. Whereas the Asilomar meeting concluded on a note of skepticism about the validity of the whole endeavor, the Puerto Rico conference resulted in an open letter, signed by many prominent researchers, that called for more research to insure that A.I. would be “robust and beneficial.”\nBetween the two conferences, the field had experienced a revolution, built on an approach called deep learning—a type of neural network that can discern complex patterns in huge quantities of data. For de­c­ades, researchers, hampered by the limits of their hardware, struggled to get the technique to work well. But, beginning in 2010, the increasing availability of Big Data and cheap, powerful video-­game processors had a dramatic effect on performance. Without any profound theoretical breakthrough, deep learning suddenly offered breathtaking advances. “I have been talking to quite a few contemporaries,” Stuart Russell told me. “Pretty much everyone sees examples of progress they just didn’t expect.” He cited a YouTube clip of a four-legged robot: one of its designers tries to kick it over, but it quickly regains its balance, scrambling with uncanny naturalness. “A problem that had been viewed as very difficult, where progress was slow and incremental, was all of a sudden done. Locomotion: done.”\nIn an array of fields—speech processing, face recognition, language translation—the approach was ascendant. Researchers working on computer vision had spent years to get systems to identify objects. In almost no time, the deep-learning networks crushed their records. In one common test, using a database called ImageNet, humans identify photographs with a five-per-cent error rate; Google’s network operates at 4.8 per cent. A.I. systems can differentiate a Pembroke Welsh Corgi from a Cardigan Welsh Corgi.\nLast October, Tomaso Poggio, an M.I.T. researcher, gave a skeptical interview. “The ability to describe the content of an image would be one of the most intellectually challenging things of all for a machine to do,” he said. “We will need another cycle of basic research to solve this kind of question.” The cycle, he predicted, would take at least twenty years. A month later, Google announced that its deep-learning network could analyze an image and offer a caption of what it saw: “Two pizzas sitting on top of a stove top,” or “People shopping at an outdoor market.” When I asked Poggio about the results, he dismissed them as automatic associations between objects and language; the system did not understand what it saw. “Maybe human intelligence is the same thing, in which case I am wrong, or not, in which case I was right,” he told me. “How do you decide?”\nA respected minority of A.I. researchers began to wonder: If increasingly powerful hardware could facilitate the deep-learning revolution, would it make other long-shelved A.I. principles viable? “Suppose the brain is just a million different evolutionarily developed hacks: one for smell, one for recognizing faces, one for how you recognize animals,” Tom Mitchell, who holds a chair in machine learning at Carnegie Mellon, told me. “If that is what underlies intelligence, then I think we are far, far from getting there—because we don’t have many of those hacks. On the other hand, suppose that what underlies intelligence are twenty-three general mechanisms, and when you put them together you get synergy, and it works. We now have systems that can do a pretty good job with computer vision—and it turns out that we didn’t have to construct a million hacks. So part of the uncertainty is: if we do not need a million different hacks, then will we find the right twenty-­three fundamental generic methods?” He paused. “I no longer have the feeling, which I had twenty-five years ago, that there are gaping holes. I know we don’t have a good architecture to assemble the ideas, but it is not obvious to me that we are missing components.”\nBostrom noticed the shift in attitude. He recently conducted a poll of A.I. researchers to gauge their sense of progress, and in Puerto Rico a survey gathered opinions on how long it would be until an artificial intelligence could reason indistinguishably from a human being. Like Bostrom, the engineers are often careful to express their views as probabilities, rather than as facts. Richard Sutton, a Canadian computer scientist whose work has earned tens of thousands of scholarly citations, gives a range of outcomes: there is a ten-per-cent chance that A.I. will never be achieved, but a twenty-five-per-cent chance that it will arrive by 2030. The median response in Bostrom’s poll gives a fifty-fifty chance that human-level A.I. would be attained by 2050. These surveys are unscientific, but he is confident enough to offer an interpretive assumption: “It is not a ridiculous prospect to take seriously the possibility that it can happen in the lifetime of people alive today.”\nOn my last day in Oxford, I walked with Bostrom across town. He was racing to catch a train to London, to speak at the Royal Society, one of the world’s oldest scientific institutions. His spirits were high. The gulf between the transhumanists and the scientific community was slowly shrinking. Elon Musk had pledged ten million dollars in grants for academics seeking to investigate A.I. safety, and, rather than mock him, researchers applied for the money; Bostrom’s institute was helping to evaluate the proposals. “Right now, there is a lot of interest,” he told me. “But then there were all these long years when nobody else seemed to pay attention at all. I am not sure which is the less abnormal condition.”\nThere were clear limits to that interest. To publicly stake out a position in the middle of the debate was difficult, not least because of the polarized atmosphere Bostrom’s book had helped to create. Even though a growing number of researchers were beginning to suspect that profound questions loomed, and that they might be worth addressing now, it did not mean that they believed A.I. would lead inevitably to an existential demise or a techno-utopia. Most of them were engaged with more immediate problems: privacy, unemployment, weaponry, driverless cars running amok. When I asked Bostrom about this pragmatic ethical awakening, he reacted with dismay. “My fear is that it would swallow up the concerns for the longer term,” he said. “On the other hand, yes, maybe it is useful to build bridges to these different communities. Kind of makes the issue part of a larger continuum of things to work on.”\nAt the Royal Society, Bostrom took a seat at the back of a large hall. As he crossed his legs, I noticed a thin leather band around his ankle. A metal buckle was engraved with contact information for Alcor, a cryonics facility in Arizona, where Bostrom is a fee-paying member. Within hours of his death, Alcor will take custody of his body and maintain it in a giant steel bottle flooded with liquid nitrogen, in the hope that one day technology will allow him to be revived, or to have his mind uploaded into a computer. When he signed up, two other colleagues at the institute joined him. “My background is transhumanism,” he once reminded me. “The character of that is gung-ho techno-cheerleading, bring it on now, where are my life-­extension pills.”\nThe hall was packed with some of the most technically sophisticated researchers in A.I.—not necessarily Bostrom’s people—and when he spoke he began by trying to assure them that his concern was not out of Ludditism. “It would be tragic if machine intelligence were never developed to its full capacity,” he said. “I think this is ultimately the key, or the portal, we have to pass through to realize the full dimension of humanity’s long-term potential.” But, even as he avoided talk of existential risk, he pressed his audience to consider the danger of building an A.I. without regarding its ethical design.\nAn attendee raised his hand to object. “We can’t control basic computer worms,” he said. “The A.I. that will happen is going to be a highly adaptive, emergent capability, and highly distributed. We will be able to work with it—for it—not necessarily contain it.”\n“I guess I am a little frustrated,” Bos­trom responded. “People tend to fall into two camps. On one hand, there are those, like yourself, who think it is probably hopeless. The other camp thinks it is easy enough that it will be solved automatically. And both of these have in common the implication that we don’t have to make any effort now.”\nFor the rest of the day, engineers presented their work at the lectern, each promising a glimpse of the future—robot vision, quantum computers, algorithms called “thought vectors.” Early in Bostrom’s career, he predicted that cascading economic demand for an A.I. would build up across the fields of medicine, entertainment, finance, and defense. As the technology became useful, that demand would only grow. “If you make a one-per-cent improvement to something—say, an algorithm that recommends books on Amazon—there is a lot of value there,” Bostrom told me. “Once every improvement potentially has enormous economic benefit, that promotes effort to make more improvements.”\nMany of the world’s largest tech companies are now locked in an A.I. arms race, purchasing other companies and opening specialized units to advance the technology. Industry is vacuuming up Ph.D.s so quickly that people in the field worry there will no longer be top talent in academia. After decades of pursuing narrow forms of A.I., researchers are seeking to integrate them into systems that resemble a general intellect. Since I.B.M.’s Watson won “Jeopardy,” the company has committed more than a billion dollars to develop it, and is reorienting its business around “cognitive systems.” One senior I.B.M. executive declared, “The separation between human and machine is going to blur in a very fundamental way.”\nAt the Royal Society, a contingent of researchers from Google occupied a privileged place; they likely had more resources at their disposal than anyone else in the room. Early on, Google’s founders, Larry Page and Sergey Brin, understood that the company’s mission required solving fundamental A.I. problems. Page has said that he believes the ideal system would understand questions, even anticipate them, and produce responses in conversational language. Google scientists often invoke the computer in “Star Trek” as a model.\nIn recent years, Google has purchased seven robotics companies and several firms specializing in machine intelligence; it may now employ the world’s largest contingent of Ph.D.s in deep learning. Perhaps the most interesting acquisition is a British company called DeepMind, started in 2011 to build a general artificial intelligence. Its founders had made an early bet on deep learning, and sought to combine it with other A.I. mechanisms in a cohesive architecture. In 2013, they published the results of a test in which their system played seven classic Atari games, with no instruction other than to improve its score. For many people in A.I., the importance of the results was immediately evident. I.B.M.’s chess program had defeated Garry Kasparov, but it could not beat a three-year-old at tic-tac-toe. In six games, DeepMind’s system outperformed all previous algorithms; in three it was superhuman. In a boxing game, it learned to pin down its opponent and subdue him with a barrage of punches.\nWeeks after the results were released, Google bought the company, reportedly for half a billion dollars. DeepMind placed two unusual conditions on the deal: its work could never be used for espionage or defense purposes, and an ethics board would oversee the research as it drew closer to achieving A.I. Anders Sandberg had told me, “We are happy that they are among the most likely to do it. They recognize there are some problems.”\nDeepMind’s chief founder, Demis Hassabis, described his company to the audience at the Royal Society as an “Apollo Program” with a two-part mission: “Step one, solve intelligence. Step two, use it to solve everything else.” Since the test in 2013, his system had aced more than a dozen other Atari titles. Hassabis demonstrated an unpublished trial using a three-dimensional driving game, in which it had quickly outperformed the game’s automated drivers. The plan was to test it in increasingly complex virtual environments and, eventually, in the real world. The patent lists a range of uses, from finance to robotics.\nHassabis was clear about the challenges. DeepMind’s system still fails hopelessly at tasks that require long-range planning, knowledge about the world, or the ability to defer rewards—things that a five-year-old child might be expected to handle. The company is working to give the algorithm conceptual understanding and the capability of transfer learning, which allows humans to apply lessons from one situation to another. These are not easy problems. But DeepMind has more than a hundred Ph.D.s to work on them, and the rewards could be immense. Hassabis spoke of building artificial scientists to resolve climate change, disease, poverty. “Even with the smartest set of humans on the planet working on these problems, these systems might be so complex that it is difficult for individual humans, scientific experts,” he said. “If we can crack what intelligence is, then we can use it to help us solve all these other problems.” He, too, believes that A.I. is a gateway to expanded human potential.\nThe keynote speaker at the Royal Society was another Google employee: Geoffrey Hinton, who for decades has been a central figure in developing deep learning. As the conference wound down, I spotted him chatting with Bostrom in the middle of a scrum of researchers. Hinton was saying that he did not expect A.I. to be achieved for decades. “No sooner than 2070,” he said. “I am in the camp that is hopeless.”\n“In that you think it will not be a cause for good?” Bostrom asked.\n“I think political systems will use it to terrorize people,” Hinton said. Already, he believed, agencies like the N.S.A. were attempting to abuse similar technology.\n“Then why are you doing the research?” Bostrom asked.\n“I could give you the usual arguments,” Hinton said. “But the truth is that the prospect of discovery is too sweet.” He smiled awkwardly, the word hanging in the air—an echo of Oppenheimer, who famously said of the bomb, “When you see something that is technically sweet, you go ahead and do it, and you argue about what to do about it only after you have had your technical success.”\nAs the scientists retreated to tables set up for refreshments, I asked Hinton if he believed an A.I. could be controlled. “That is like asking if a child can control his parents,” he said. “It can happen with a baby and a mother—there is biological hardwiring—but there is not a good track record of less intelligent things controlling things of greater intelligence.” He looked as if he might elaborate. Then a scientist called out, “Let’s all get drinks!”\nBostrom had little interest in the cocktail party. He shook a few hands, then headed for St. James’s Park, a public garden that extends from the gates of Buckingham Palace through central London. The world appeared in splendorous analog: sunlight over trees, duck ponds, children and grandparents feeding birds. The spot had been a park for hundreds of years, and the vista seemed timeless. Yet, during the past millennium, the grounds had also been a marsh, a leper hospital, a deer sanctuary, and royal gardens. It seemed plausible that, a thousand years from now, digital posthumans, regarding it as wasted space, would tear it up, replace the landscaping with computer banks, and erect a vast virtual idyll.\nBostrom’s pace settled into its natural quickness as we circled the park. He talked about his family; he would be seeing his wife and son soon. He was reading widely: history, psychology, economics. He was learning to code. He was thinking about expanding his institute. Although he did not know it then, F.H.I. was about to receive one and a half million dollars from Elon Musk, to create a unit that would craft social policies informed by some of Bostrom’s theories. He would need to hire people. He was also giving thought to the framing of his message. “A lot more is said about the risks than the upsides, but that is not necessarily because the upside is not there,” he told me. “There is just more to be said about the risk—and maybe more use in describing the pitfalls, so we know how to steer around them—than spending time now figuring out the details of how we are going to furnish the great palace a thousand years from now.”\nWe passed a fountain, near a cluster of rocks engineered to give ducks a resting place. Bostrom, in his forties, must soon contend with physical decline, and he spoke with annoyance of the first glimmers of mortality. Even though he is an Alcor member, there is no guarantee that cryonics will work. Perhaps the most radical of his visions is that superintelligent A.I. will hasten the uploading of minds—what he calls “whole-brain emulations”—technology that might not be possible for centuries, if at all. Bostrom, in his most hopeful mode, imagines emulations not only as reproductions of the original intellect “with memory and personality intact”—a soul in the machine—but as minds expandable in countless ways. “We live for seven decades, and we have three-pound lumps of cheesy matter to think with, but to me it is plausible that there could be extremely valuable mental states outside this little particular set of possibilities that might be much better,” he told me.\nIn his book, Bostrom considers a distant future in which trillions of digital minds merge into an enormous cognitive cyber-soup. “Whether the set of extremely positive posthuman modes of being would include some kind of dissolved bouillon, there is some uncertainty,” he said. “If you look at religious views, there are many where merging with something greater is a form of heaven, being in the presence of this enormous beauty and goodness. In many traditions, the best possible state does not involve being a little individual pursuing goals. But it is hard to get a grasp of what would be going on in that soup. Maybe some soups would not be preferable as a long-term outcome. I don’t know.” He stopped and looked ahead. “What I want to avoid is to think from our parochial 2015 view—from my own limited life experience, my own limited brain—and super-confidentially postulate what is the best form for civilization a billion years from now, when you could have brains the size of planets and billion-year life spans. It seems unlikely that we will figure out some detailed blueprint for utopia. What if the great apes had asked whether they should evolve into Homo sapiens—pros and cons—and they had listed, on the pro side, ‘Oh, we could have a lot of bananas if we became human’? Well, we can have unlimited bananas now, but there is more to the human condition than that.” ♦\nIllustration by Todd St. John/Coding by Jono Brandel.","isBasedOn":"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom","articleSection":"a reporter at large","keywords":["a reporter at large","artificial intelligence (a.i.)","category_science_tech","philosophers","magazine"],"thumbnailUrl":"https://media.newyorker.com/photos/590971f5ebe912338a377328/1:1/w_2000,h_2000,c_limit/151123_r27342.jpg","url":"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom","alternativeHeadline":"Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction."},"twitterObj":false,"status":200,"metadata":{"author":"Condé Nast","title":"The Philosopher of Doomsday | The New Yorker","description":"Raffi Khatchadourian on Nick Bostrom, an Oxford philosopher who asks whether inventing artificial intelligence will bring us utopia or destruction.","canonical":"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom","keywords":["artificial intelligence (a.i.)","category_science_tech","philosophers"],"image":"https://www.newyorker.com/projects/interactive/2019/190211-kaminsky/assets/svg/tny_logo.svg","firstParagraph":" Last year, a curious nonfiction book became a Times best-seller: a dense meditation on artificial intelligence by the philosopher Nick Bostrom, who holds an appointment at Oxford. Titled “Superintelligence: Paths, Dangers, Strategies,” it argues that true artificial intelligence, if it is realized, might pose a danger that exceeds every previous threat from technology—even nuclear weapons—and that if its development is not managed carefully humanity risks engineering its own extinction. Central to this concern is the prospect of an “intelligence explosion,” a speculative event in which an A.I. gains the ability to improve itself, and in short order exceeds the intellectual potential of the human brain by many orders of magnitude. "},"dublinCore":{},"opengraph":{"title":"The Doomsday Invention","description":"Will artificial intelligence bring us utopia or destruction?","url":"https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom","site_name":"The New Yorker","locale":false,"type":"article","typeObject":{"published_time":"2015-11-16T00:00:00.000Z","modified_time":"2015-11-16T04:00:00.000Z","author":"Raffi Khatchadourian","publisher":false,"section":"tags","tag":[],"content_tier":"free","opinion":"false"},"image":"https://media.newyorker.com/photos/590971f5ebe912338a377328/16:9/w_1280,c_limit/151123_r27342.jpg"},"twitter":{"site":false,"description":false,"card":false,"creator":false,"title":false,"image":false},"archivedData":{"link":false,"wayback":false}}}