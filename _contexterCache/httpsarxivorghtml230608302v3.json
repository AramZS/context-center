{"initialLink":"https://arxiv.org/html/2306.08302v3","sanitizedLink":"https://arxiv.org/html/2306.08302v3","finalLink":"https://arxiv.org/html/2306.08302v3","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://arxiv.org/html/2306.08302v3\" itemprop=\"url\">Unifying Large Language Models and Knowledge Graphs: A Roadmap</a></contexter-box-head></contexter-box-head><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2024-11-04T17:06:16.283Z\">11/4/2024</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>Shirui Pan, Senior Member, IEEE, Linhao Luo,Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu, Fellow, IEEEShirui Pan is with the School of Information and Communication Technology and Institute for Integrated and Intelligent Systems (IIIS), Griffith University, Queensland, Australia.Email: s.pan@griffith.edu.au;Linhao Luo and Yufei Wang are with the Department of Data Science and AI, Monash University, Melbourne, Australia. E-mail: linhao.luo@monash.edu, garyyufei@gmail.com.Chen Chen is with the Nanyang Technological University, Singapore. E-mail: s190009@ntu.edu.sg.Jiapu Wang is with the Faculty of Information Technology, Beijing University of Technology, Beijing, China. E-mail: jpwang@emails.bjut.edu.cn.Xindong Wu is with the Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Technology, Hefei, China, and also with the Research Center for Knowledge Engineering, Zhejiang Lab, Hangzhou, China.Email: xwu@hfut.edu.cn.Shirui Pan and Linhao Luo contributed equally to this work. Corresponding Author: Xindong Wu.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"><span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\">Natural Language Processing</span>, <span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\">Large Language Models</span>, <span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\">Generative Pre-Training</span>, <span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\">Knowledge Graphs</span>, <span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\">Roadmap</span>, <span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\">Bidirectional Reasoning.</span></contexter-keywordset><a is=\"contexter-link\" href=\"https://arxiv.org/html/2306.08302v3\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"21cb2ba85e01d5760e0cd521e21b881298296ea0","data":{"originalLink":"https://arxiv.org/html/2306.08302v3","sanitizedLink":"https://arxiv.org/html/2306.08302v3","canonical":"https://arxiv.org/html/2306.08302v3","htmlText":"<!DOCTYPE html>\n\n<html lang=\"en\">\n<head>\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"content-type\"/>\n<title>Unifying Large Language Models and Knowledge Graphs: A Roadmap</title>\n<!--Generated on Thu Jan 25 00:42:33 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->\n<meta content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" name=\"viewport\"/>\n<link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/ar5iv_0.7.4.min.css\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/static/browse/0.3.4/css/latexml_styles.css\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js\"></script>\n<script src=\"/static/browse/0.3.4/js/addons.js\"></script>\n<script src=\"/static/browse/0.3.4/js/feedbackOverlay.js\"></script>\n<meta content=\" Natural Language Processing,  Large Language Models,  Generative Pre-Training,  Knowledge Graphs,  Roadmap,  Bidirectional Reasoning.\n\" lang=\"en\" name=\"keywords\"/>\n<base href=\"/html/2306.08302v3/\"/></head>\n<body>\n<nav class=\"ltx_page_navbar\">\n<nav class=\"ltx_TOC\">\n<ol class=\"ltx_toclist\">\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S1\" title=\"1 Introduction ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">1 </span><span class=\"ltx_text ltx_font_smallcaps\">Introduction</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2\" title=\"2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2 </span><span class=\"ltx_text ltx_font_smallcaps\">Background</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.SS1\" title=\"2.1 Large Language models (LLMs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.1 </span><span class=\"ltx_text ltx_font_italic\">Large Language models (LLMs)</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.SS1.SSS1\" title=\"2.1.1 Encoder-only LLMs. ‣ 2.1 Large Language models (LLMs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.1.1 </span>Encoder-only LLMs.</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.SS1.SSS2\" title=\"2.1.2 Encoder-decoder LLMs. ‣ 2.1 Large Language models (LLMs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.1.2 </span>Encoder-decoder LLMs.</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.SS1.SSS3\" title=\"2.1.3 Decoder-only LLMs. ‣ 2.1 Large Language models (LLMs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.1.3 </span>Decoder-only LLMs.</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.SS1.SSS4\" title=\"2.1.4 Prompt Engineering ‣ 2.1 Large Language models (LLMs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.1.4 </span>Prompt Engineering</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.SS2\" title=\"2.2 Knowledge Graphs (KGs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.2 </span><span class=\"ltx_text ltx_font_italic\">Knowledge Graphs (KGs)</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.SS2.SSS1\" title=\"2.2.1 Encyclopedic Knowledge Graphs. ‣ 2.2 Knowledge Graphs (KGs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.2.1 </span>Encyclopedic Knowledge Graphs.</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.SS2.SSS2\" title=\"2.2.2 Commonsense Knowledge Graphs. ‣ 2.2 Knowledge Graphs (KGs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.2.2 </span>Commonsense Knowledge Graphs.</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.SS2.SSS3\" title=\"2.2.3 Domain-specific Knowledge Graphs ‣ 2.2 Knowledge Graphs (KGs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.2.3 </span>Domain-specific Knowledge Graphs</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.SS2.SSS4\" title=\"2.2.4 Multi-modal Knowledge Graphs. ‣ 2.2 Knowledge Graphs (KGs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.2.4 </span>Multi-modal Knowledge Graphs.</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.SS3\" title=\"2.3 Applications ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.3 </span><span class=\"ltx_text ltx_font_italic\">Applications</span></span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S3\" title=\"3 Roadmap &amp; Categorization ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3 </span><span class=\"ltx_text ltx_font_smallcaps\">Roadmap &amp; Categorization</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S3.SS1\" title=\"3.1 Roadmap ‣ 3 Roadmap &amp; Categorization ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.1 </span><span class=\"ltx_text ltx_font_italic\">Roadmap</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S3.SS1.SSS1\" title=\"3.1.1 KG-enhanced LLMs ‣ 3.1 Roadmap ‣ 3 Roadmap &amp; Categorization ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.1.1 </span>KG-enhanced LLMs</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S3.SS1.SSS2\" title=\"3.1.2 LLM-augmented KGs ‣ 3.1 Roadmap ‣ 3 Roadmap &amp; Categorization ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.1.2 </span>LLM-augmented KGs</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S3.SS1.SSS3\" title=\"3.1.3 Synergized LLMs + KGs ‣ 3.1 Roadmap ‣ 3 Roadmap &amp; Categorization ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.1.3 </span>Synergized LLMs + KGs</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S3.SS2\" title=\"3.2 Categorization ‣ 3 Roadmap &amp; Categorization ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.2 </span><span class=\"ltx_text ltx_font_italic\">Categorization</span></span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4\" title=\"4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4 </span><span class=\"ltx_text ltx_font_smallcaps\">KG-enhanced LLMs</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.SS1\" title=\"4.1 KG-enhanced LLM Pre-training ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.1 </span><span class=\"ltx_text ltx_font_italic\">KG-enhanced LLM Pre-training</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.SS1.SSS1\" title=\"4.1.1 Integrating KGs into Training Objective ‣ 4.1 KG-enhanced LLM Pre-training ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.1.1 </span>Integrating KGs into Training Objective</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.SS1.SSS2\" title=\"4.1.2 Integrating KGs into LLM Inputs ‣ 4.1 KG-enhanced LLM Pre-training ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.1.2 </span>Integrating KGs into LLM Inputs</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.SS1.SSS3\" title=\"4.1.3 KGs Instruction-tuning ‣ 4.1 KG-enhanced LLM Pre-training ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.1.3 </span>KGs Instruction-tuning</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.SS2\" title=\"4.2 KG-enhanced LLM Inference ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.2 </span><span class=\"ltx_text ltx_font_italic\">KG-enhanced LLM Inference</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.SS2.SSS1\" title=\"4.2.1 Retrieval-Augmented Knowledge Fusion ‣ 4.2 KG-enhanced LLM Inference ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.2.1 </span>Retrieval-Augmented Knowledge Fusion</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.SS2.SSS2\" title=\"4.2.2 KGs Prompting ‣ 4.2 KG-enhanced LLM Inference ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.2.2 </span>KGs Prompting</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.SS3\" title=\"4.3 Comparison between KG-enhanced LLM Pre-training and Inference ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.3 </span><span class=\"ltx_text ltx_font_italic\">Comparison between KG-enhanced LLM Pre-training and Inference</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.SS4\" title=\"4.4 KG-enhanced LLM Interpretability ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.4 </span><span class=\"ltx_text ltx_font_italic\">KG-enhanced LLM Interpretability</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.SS4.SSS1\" title=\"4.4.1 KGs for LLM Probing ‣ 4.4 KG-enhanced LLM Interpretability ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.4.1 </span>KGs for LLM Probing</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.SS4.SSS2\" title=\"4.4.2 KGs for LLM Analysis ‣ 4.4 KG-enhanced LLM Interpretability ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.4.2 </span>KGs for LLM Analysis</span></a></li>\n</ol>\n</li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5\" title=\"5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5 </span><span class=\"ltx_text ltx_font_smallcaps\">LLM-augmented KGs</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS1\" title=\"5.1 LLM-augmented KG Embedding ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.1 </span><span class=\"ltx_text ltx_font_italic\">LLM-augmented KG Embedding</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS1.SSS1\" title=\"5.1.1 LLMs as Text Encoders ‣ 5.1 LLM-augmented KG Embedding ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.1.1 </span>LLMs as Text Encoders</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS1.SSS2\" title=\"5.1.2 LLMs for Joint Text and KG Embedding ‣ 5.1 LLM-augmented KG Embedding ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.1.2 </span>LLMs for Joint Text and KG Embedding</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS2\" title=\"5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.2 </span><span class=\"ltx_text ltx_font_italic\">LLM-augmented KG Completion</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS2.SSS1\" title=\"5.2.1 LLM as Encoders (PaE). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.2.1 </span>LLM as Encoders (PaE).</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS2.SSS2\" title=\"5.2.2 LLM as Generators (PaG). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.2.2 </span>LLM as Generators (PaG).</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS2.SSS3\" title=\"5.2.3 Model Analysis ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.2.3 </span>Model Analysis</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS3\" title=\"5.3 LLM-augmented KG Construction ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.3 </span><span class=\"ltx_text ltx_font_italic\">LLM-augmented KG Construction</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS3.SSS1\" title=\"5.3.1 Entity Discovery ‣ 5.3 LLM-augmented KG Construction ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.3.1 </span>Entity Discovery</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS3.SSS2\" title=\"5.3.2 Coreference Resolution (CR) ‣ 5.3 LLM-augmented KG Construction ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.3.2 </span>Coreference Resolution (CR)</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS3.SSS3\" title=\"5.3.3 Relation Extraction (RE) ‣ 5.3 LLM-augmented KG Construction ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.3.3 </span>Relation Extraction (RE)</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS3.SSS4\" title=\"5.3.4 Distilling Knowledge Graphs from LLMs ‣ 5.3 LLM-augmented KG Construction ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.3.4 </span>Distilling Knowledge Graphs from LLMs</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS4\" title=\"5.4 LLM-augmented KG-to-text Generation ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.4 </span><span class=\"ltx_text ltx_font_italic\">LLM-augmented KG-to-text Generation</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS4.SSS1\" title=\"5.4.1 Leveraging Knowledge from LLMs ‣ 5.4 LLM-augmented KG-to-text Generation ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.4.1 </span>Leveraging Knowledge from LLMs</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS4.SSS2\" title=\"5.4.2 Constructing large weakly KG-text aligned Corpus ‣ 5.4 LLM-augmented KG-to-text Generation ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.4.2 </span>Constructing large weakly KG-text aligned Corpus</span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS5\" title=\"5.5 LLM-augmented KG Question Answering ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.5 </span><span class=\"ltx_text ltx_font_italic\">LLM-augmented KG Question Answering</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS5.SSS1\" title=\"5.5.1 LLMs as Entity/relation Extractors ‣ 5.5 LLM-augmented KG Question Answering ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.5.1 </span>LLMs as Entity/relation Extractors</span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsubsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.SS5.SSS2\" title=\"5.5.2 LLMs as Answer Reasoners ‣ 5.5 LLM-augmented KG Question Answering ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5.5.2 </span>LLMs as Answer Reasoners</span></a></li>\n</ol>\n</li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S6\" title=\"6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">6 </span><span class=\"ltx_text ltx_font_smallcaps\">Synergized LLMs + KGs</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S6.SS1\" title=\"6.1 Synergized Knowledge Representation ‣ 6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">6.1 </span><span class=\"ltx_text ltx_font_italic\">Synergized Knowledge Representation</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S6.SS2\" title=\"6.2 Synergized Reasoning ‣ 6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">6.2 </span><span class=\"ltx_text ltx_font_italic\">Synergized Reasoning</span></span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\">\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S7\" title=\"7 Future Directions and Milestones ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">7 </span><span class=\"ltx_text ltx_font_smallcaps\">Future Directions and Milestones</span></span></a>\n<ol class=\"ltx_toclist ltx_toclist_section\">\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S7.SS1\" title=\"7.1 KGs for Hallucination Detection in LLMs ‣ 7 Future Directions and Milestones ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">7.1 </span><span class=\"ltx_text ltx_font_italic\">KGs for Hallucination Detection in LLMs</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S7.SS2\" title=\"7.2 KGs for Editing Knowledge in LLMs ‣ 7 Future Directions and Milestones ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">7.2 </span><span class=\"ltx_text ltx_font_italic\">KGs for Editing Knowledge in LLMs</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S7.SS3\" title=\"7.3 KGs for Black-box LLMs Knowledge Injection ‣ 7 Future Directions and Milestones ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">7.3 </span><span class=\"ltx_text ltx_font_italic\">KGs for Black-box LLMs Knowledge Injection</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S7.SS4\" title=\"7.4 Multi-Modal LLMs for KGs ‣ 7 Future Directions and Milestones ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">7.4 </span><span class=\"ltx_text ltx_font_italic\">Multi-Modal LLMs for KGs</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S7.SS5\" title=\"7.5 LLMs for Understanding KG Structure ‣ 7 Future Directions and Milestones ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">7.5 </span><span class=\"ltx_text ltx_font_italic\">LLMs for Understanding KG Structure</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S7.SS6\" title=\"7.6 Synergized LLMs and KGs for Birectional Reasoning ‣ 7 Future Directions and Milestones ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">7.6 </span><span class=\"ltx_text ltx_font_italic\">Synergized LLMs and KGs for Birectional Reasoning</span></span></a></li>\n</ol>\n</li>\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S8\" title=\"8 Conclusion ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">8 </span><span class=\"ltx_text ltx_font_smallcaps\">Conclusion</span></span></a></li>\n<li class=\"ltx_tocentry ltx_tocentry_appendix\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#A1\" title=\"Appendix A Pros and Cons for LLMs and KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">A </span>Pros and Cons for LLMs and KGs</span></a></li>\n</ol></nav>\n</nav>\n<div class=\"ltx_page_main\">\n<div class=\"ltx_page_content\"><div class=\"section\" id=\"target-section\"><div id=\"license-tr\">License: arXiv.org perpetual non-exclusive license</div><div id=\"watermark-tr\">arXiv:2306.08302v3 [cs.CL] 25 Jan 2024</div></div>\n<article class=\"ltx_document ltx_authors_1line\">\n<h1 class=\"ltx_title ltx_title_document\">Unifying Large Language Models and Knowledge Graphs: A Roadmap</h1>\n<div class=\"ltx_authors\">\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\">Shirui Pan, <em class=\"ltx_emph ltx_font_italic\" id=\"id1.1.id1\">Senior Member, IEEE</em>, Linhao Luo,\n<br class=\"ltx_break\"/> Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu, <span class=\"ltx_text ltx_font_italic\" id=\"id2.2.id2\">Fellow, IEEE</span>\n</span><span class=\"ltx_author_notes\">\nShirui Pan is with the School of Information and Communication Technology and Institute for Integrated and Intelligent Systems (IIIS), Griffith University, Queensland, Australia.\nEmail: s.pan@griffith.edu.au;\nLinhao Luo and Yufei Wang are with the Department of Data Science and AI, Monash University, Melbourne, Australia. E-mail: linhao.luo@monash.edu, garyyufei@gmail.com.\nChen Chen is with the Nanyang Technological University, Singapore. E-mail: s190009@ntu.edu.sg.\nJiapu Wang is with the Faculty of Information Technology, Beijing University of Technology, Beijing, China. E-mail: jpwang@emails.bjut.edu.cn.\nXindong Wu is with the Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Technology, Hefei, China, and also with the Research Center for Knowledge Engineering, Zhejiang Lab, Hangzhou, China.\nEmail: xwu@hfut.edu.cn.\nShirui Pan and Linhao Luo contributed equally to this work. Corresponding Author: Xindong Wu.\n</span></span>\n</div>\n<div class=\"ltx_abstract\">\n<h6 class=\"ltx_title ltx_title_abstract\">Abstract</h6>\n<p class=\"ltx_p\" id=\"id3.id1\">Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages.\nIn this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, <span class=\"ltx_text ltx_font_italic\" id=\"id3.id1.1\">1) KG-enhanced LLMs,</span> which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; <span class=\"ltx_text ltx_font_italic\" id=\"id3.id1.2\">2) LLM-augmented KGs,</span> that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and <span class=\"ltx_text ltx_font_italic\" id=\"id3.id1.3\">3) Synergized LLMs + KGs</span>, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.\n</p>\n</div>\n<div class=\"ltx_keywords\">\n<h6 class=\"ltx_title ltx_title_keywords\">Index Terms: </h6> Natural Language Processing, Large Language Models, Generative Pre-Training, Knowledge Graphs, Roadmap, Bidirectional Reasoning.\n\n</div>\n<span class=\"ltx_note ltx_note_frontmatter ltx_role_publicationid\" id=\"id1\"><sup class=\"ltx_note_mark\">†</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">†</sup><span class=\"ltx_note_type\">publicationid: </span>pubid: 0000–0000/00$00.00 © 2023 IEEE</span></span></span>\n<section class=\"ltx_section\" id=\"S1\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S1.1.1\">Introduction</span>\n</h2>\n<div class=\"ltx_para\" id=\"S1.p1\">\n<p class=\"ltx_p\" id=\"S1.p1.1\">Large language models (LLMs)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>LLMs are also known as pre-trained language models (PLMs).</span></span></span> (e.g., BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib1\" title=\"\">1</a>]</cite>, RoBERTA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib2\" title=\"\">2</a>]</cite>, and T5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib3\" title=\"\">3</a>]</cite>), pre-trained on the large-scale corpus, have shown great performance in various natural language processing (NLP) tasks, such as question answering <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib4\" title=\"\">4</a>]</cite>, machine translation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib5\" title=\"\">5</a>]</cite>, and text generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib6\" title=\"\">6</a>]</cite>. Recently, the dramatically increasing model size further enables the LLMs with the emergent ability <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib7\" title=\"\">7</a>]</cite>, paving the road for applying LLMs as Artificial General Intelligence (AGI). Advanced LLMs like ChatGPT<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/blog/chatgpt\" title=\"\">https://openai.com/blog/chatgpt</a></span></span></span> and PaLM2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://ai.google/discover/palm2\" title=\"\">https://ai.google/discover/palm2</a></span></span></span>, with billions of parameters, exhibit great potential in many complex practical tasks, such as education <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib8\" title=\"\">8</a>]</cite>, code generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib9\" title=\"\">9</a>]</cite> and recommendation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib10\" title=\"\">10</a>]</cite>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S1.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"387\" id=\"S1.F1.g1\" src=\"extracted/5367551/figs/LLM_vs_KG.png\" width=\"598\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>Summarization of the pros and cons for LLMs and KGs. LLM pros: <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.19.1\">General Knowledge</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib11\" title=\"\">11</a>]</cite>, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.20.2\">Language Processing</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib12\" title=\"\">12</a>]</cite>, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.21.3\">Generalizability</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib13\" title=\"\">13</a>]</cite>; LLM cons: <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.22.4\">Implicit Knowledge</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a>]</cite>, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.23.5\">Hallucination</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib15\" title=\"\">15</a>]</cite>, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.24.6\">Indecisiveness</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib16\" title=\"\">16</a>]</cite>, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.25.7\">Black-box</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib17\" title=\"\">17</a>]</cite>, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.26.8\">Lacking Domain-specific/New Knowledge</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib18\" title=\"\">18</a>]</cite>. KG pros: <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.27.9\">Structural Knowledge</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib19\" title=\"\">19</a>]</cite>, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.28.10\">Accuracy</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib20\" title=\"\">20</a>]</cite>, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.29.11\">Decisiveness</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib21\" title=\"\">21</a>]</cite>, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.30.12\">Interpretability</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib22\" title=\"\">22</a>]</cite>, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.31.13\">Domain-specific Knowledge</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib23\" title=\"\">23</a>]</cite>, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.32.14\">Evolving Knowledge</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib24\" title=\"\">24</a>]</cite>; KG cons: <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.33.15\">Incompleteness</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib25\" title=\"\">25</a>]</cite>, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.34.16\">Lacking Language Understanding</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib26\" title=\"\">26</a>]</cite>, <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.35.17\">Unseen Facts</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib27\" title=\"\">27</a>]</cite>. Pros. and Cons. are selected based on their representativeness. Detailed discussion can be found in <em class=\"ltx_emph ltx_font_italic\" id=\"S1.F1.36.18\">Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#A1\" title=\"Appendix A Pros and Cons for LLMs and KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</em></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S1.p2\">\n<p class=\"ltx_p\" id=\"S1.p2.1\">Despite their success in many applications, LLMs have been criticized for their lack of factual knowledge. Specifically, LLMs memorize facts and knowledge contained in the training corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a>]</cite>. However, further studies reveal that LLMs are not able to recall facts and often experience hallucinations by generating statements that are factually incorrect <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib15\" title=\"\">15</a>]</cite>. For example, LLMs might say “Einstein discovered gravity in 1687” when asked, “When did Einstein discover gravity?”, which contradicts the fact that Isaac Newton formulated the gravitational theory. This issue severely impairs the trustworthiness of LLMs.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p3\">\n<p class=\"ltx_p\" id=\"S1.p3.1\">As black-box models, LLMs are also criticized for their lack of interpretability. LLMs represent knowledge implicitly in their parameters. It is difficult to interpret or validate the knowledge obtained by LLMs. Moreover, LLMs perform reasoning by a probability model, which is an indecisive process <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib16\" title=\"\">16</a>]</cite>. The specific patterns and functions LLMs used to arrive at predictions or decisions are not directly accessible or explainable to humans <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib17\" title=\"\">17</a>]</cite>. Even though some LLMs are equipped to explain their predictions by applying chain-of-thought <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib29\" title=\"\">29</a>]</cite>, their reasoning explanations also suffer from the hallucination issue <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib30\" title=\"\">30</a>]</cite>. This severely impairs the application of LLMs in high-stakes scenarios, such as medical diagnosis and legal judgment. For instance, in a medical diagnosis scenario, LLMs may incorrectly diagnose a disease and provide explanations that contradict medical commonsense. This raises another issue that LLMs trained on general corpus might not be able to generalize well to specific domains or new knowledge due to the lack of domain-specific knowledge or new training data <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib18\" title=\"\">18</a>]</cite>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p4\">\n<p class=\"ltx_p\" id=\"S1.p4.1\">To address the above issues, a potential solution is to incorporate knowledge graphs (KGs) into LLMs. Knowledge graphs (KGs), storing enormous facts in the way of triples, i.e., <math alttext=\"(head~{}entity,relation,tail~{}entity)\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.1.m1.3\"><semantics id=\"S1.p4.1.m1.3a\"><mrow id=\"S1.p4.1.m1.3.3.3\" xref=\"S1.p4.1.m1.3.3.4.cmml\"><mo id=\"S1.p4.1.m1.3.3.3.4\" stretchy=\"false\" xref=\"S1.p4.1.m1.3.3.4.cmml\">(</mo><mrow id=\"S1.p4.1.m1.1.1.1.1\" xref=\"S1.p4.1.m1.1.1.1.1.cmml\"><mi id=\"S1.p4.1.m1.1.1.1.1.2\" xref=\"S1.p4.1.m1.1.1.1.1.2.cmml\">h</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.3\" xref=\"S1.p4.1.m1.1.1.1.1.3.cmml\">e</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1a\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.4\" xref=\"S1.p4.1.m1.1.1.1.1.4.cmml\">a</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1b\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.5\" xref=\"S1.p4.1.m1.1.1.1.1.5.cmml\">d</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1c\" lspace=\"0.330em\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.6\" xref=\"S1.p4.1.m1.1.1.1.1.6.cmml\">e</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1d\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.7\" xref=\"S1.p4.1.m1.1.1.1.1.7.cmml\">n</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1e\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.8\" xref=\"S1.p4.1.m1.1.1.1.1.8.cmml\">t</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1f\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.9\" xref=\"S1.p4.1.m1.1.1.1.1.9.cmml\">i</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1g\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.10\" xref=\"S1.p4.1.m1.1.1.1.1.10.cmml\">t</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1h\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.11\" xref=\"S1.p4.1.m1.1.1.1.1.11.cmml\">y</mi></mrow><mo id=\"S1.p4.1.m1.3.3.3.5\" xref=\"S1.p4.1.m1.3.3.4.cmml\">,</mo><mrow id=\"S1.p4.1.m1.2.2.2.2\" xref=\"S1.p4.1.m1.2.2.2.2.cmml\"><mi id=\"S1.p4.1.m1.2.2.2.2.2\" xref=\"S1.p4.1.m1.2.2.2.2.2.cmml\">r</mi><mo id=\"S1.p4.1.m1.2.2.2.2.1\" xref=\"S1.p4.1.m1.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.2.2.2.2.3\" xref=\"S1.p4.1.m1.2.2.2.2.3.cmml\">e</mi><mo id=\"S1.p4.1.m1.2.2.2.2.1a\" xref=\"S1.p4.1.m1.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.2.2.2.2.4\" xref=\"S1.p4.1.m1.2.2.2.2.4.cmml\">l</mi><mo id=\"S1.p4.1.m1.2.2.2.2.1b\" xref=\"S1.p4.1.m1.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.2.2.2.2.5\" xref=\"S1.p4.1.m1.2.2.2.2.5.cmml\">a</mi><mo id=\"S1.p4.1.m1.2.2.2.2.1c\" xref=\"S1.p4.1.m1.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.2.2.2.2.6\" xref=\"S1.p4.1.m1.2.2.2.2.6.cmml\">t</mi><mo id=\"S1.p4.1.m1.2.2.2.2.1d\" xref=\"S1.p4.1.m1.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.2.2.2.2.7\" xref=\"S1.p4.1.m1.2.2.2.2.7.cmml\">i</mi><mo id=\"S1.p4.1.m1.2.2.2.2.1e\" xref=\"S1.p4.1.m1.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.2.2.2.2.8\" xref=\"S1.p4.1.m1.2.2.2.2.8.cmml\">o</mi><mo id=\"S1.p4.1.m1.2.2.2.2.1f\" xref=\"S1.p4.1.m1.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.2.2.2.2.9\" xref=\"S1.p4.1.m1.2.2.2.2.9.cmml\">n</mi></mrow><mo id=\"S1.p4.1.m1.3.3.3.6\" xref=\"S1.p4.1.m1.3.3.4.cmml\">,</mo><mrow id=\"S1.p4.1.m1.3.3.3.3\" xref=\"S1.p4.1.m1.3.3.3.3.cmml\"><mi id=\"S1.p4.1.m1.3.3.3.3.2\" xref=\"S1.p4.1.m1.3.3.3.3.2.cmml\">t</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.3\" xref=\"S1.p4.1.m1.3.3.3.3.3.cmml\">a</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1a\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.4\" xref=\"S1.p4.1.m1.3.3.3.3.4.cmml\">i</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1b\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.5\" xref=\"S1.p4.1.m1.3.3.3.3.5.cmml\">l</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1c\" lspace=\"0.330em\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.6\" xref=\"S1.p4.1.m1.3.3.3.3.6.cmml\">e</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1d\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.7\" xref=\"S1.p4.1.m1.3.3.3.3.7.cmml\">n</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1e\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.8\" xref=\"S1.p4.1.m1.3.3.3.3.8.cmml\">t</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1f\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.9\" xref=\"S1.p4.1.m1.3.3.3.3.9.cmml\">i</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1g\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.10\" xref=\"S1.p4.1.m1.3.3.3.3.10.cmml\">t</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1h\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.11\" xref=\"S1.p4.1.m1.3.3.3.3.11.cmml\">y</mi></mrow><mo id=\"S1.p4.1.m1.3.3.3.7\" stretchy=\"false\" xref=\"S1.p4.1.m1.3.3.4.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S1.p4.1.m1.3b\"><vector id=\"S1.p4.1.m1.3.3.4.cmml\" xref=\"S1.p4.1.m1.3.3.3\"><apply id=\"S1.p4.1.m1.1.1.1.1.cmml\" xref=\"S1.p4.1.m1.1.1.1.1\"><times id=\"S1.p4.1.m1.1.1.1.1.1.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.1\"></times><ci id=\"S1.p4.1.m1.1.1.1.1.2.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.2\">ℎ</ci><ci id=\"S1.p4.1.m1.1.1.1.1.3.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.3\">𝑒</ci><ci id=\"S1.p4.1.m1.1.1.1.1.4.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.4\">𝑎</ci><ci id=\"S1.p4.1.m1.1.1.1.1.5.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.5\">𝑑</ci><ci id=\"S1.p4.1.m1.1.1.1.1.6.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.6\">𝑒</ci><ci id=\"S1.p4.1.m1.1.1.1.1.7.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.7\">𝑛</ci><ci id=\"S1.p4.1.m1.1.1.1.1.8.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.8\">𝑡</ci><ci id=\"S1.p4.1.m1.1.1.1.1.9.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.9\">𝑖</ci><ci id=\"S1.p4.1.m1.1.1.1.1.10.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.10\">𝑡</ci><ci id=\"S1.p4.1.m1.1.1.1.1.11.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.11\">𝑦</ci></apply><apply id=\"S1.p4.1.m1.2.2.2.2.cmml\" xref=\"S1.p4.1.m1.2.2.2.2\"><times id=\"S1.p4.1.m1.2.2.2.2.1.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.1\"></times><ci id=\"S1.p4.1.m1.2.2.2.2.2.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.2\">𝑟</ci><ci id=\"S1.p4.1.m1.2.2.2.2.3.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.3\">𝑒</ci><ci id=\"S1.p4.1.m1.2.2.2.2.4.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.4\">𝑙</ci><ci id=\"S1.p4.1.m1.2.2.2.2.5.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.5\">𝑎</ci><ci id=\"S1.p4.1.m1.2.2.2.2.6.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.6\">𝑡</ci><ci id=\"S1.p4.1.m1.2.2.2.2.7.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.7\">𝑖</ci><ci id=\"S1.p4.1.m1.2.2.2.2.8.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.8\">𝑜</ci><ci id=\"S1.p4.1.m1.2.2.2.2.9.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.9\">𝑛</ci></apply><apply id=\"S1.p4.1.m1.3.3.3.3.cmml\" xref=\"S1.p4.1.m1.3.3.3.3\"><times id=\"S1.p4.1.m1.3.3.3.3.1.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.1\"></times><ci id=\"S1.p4.1.m1.3.3.3.3.2.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.2\">𝑡</ci><ci id=\"S1.p4.1.m1.3.3.3.3.3.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.3\">𝑎</ci><ci id=\"S1.p4.1.m1.3.3.3.3.4.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.4\">𝑖</ci><ci id=\"S1.p4.1.m1.3.3.3.3.5.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.5\">𝑙</ci><ci id=\"S1.p4.1.m1.3.3.3.3.6.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.6\">𝑒</ci><ci id=\"S1.p4.1.m1.3.3.3.3.7.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.7\">𝑛</ci><ci id=\"S1.p4.1.m1.3.3.3.3.8.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.8\">𝑡</ci><ci id=\"S1.p4.1.m1.3.3.3.3.9.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.9\">𝑖</ci><ci id=\"S1.p4.1.m1.3.3.3.3.10.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.10\">𝑡</ci><ci id=\"S1.p4.1.m1.3.3.3.3.11.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.11\">𝑦</ci></apply></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.p4.1.m1.3c\">(head~{}entity,relation,tail~{}entity)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.p4.1.m1.3d\">( italic_h italic_e italic_a italic_d italic_e italic_n italic_t italic_i italic_t italic_y , italic_r italic_e italic_l italic_a italic_t italic_i italic_o italic_n , italic_t italic_a italic_i italic_l italic_e italic_n italic_t italic_i italic_t italic_y )</annotation></semantics></math>, are a structured and decisive manner of knowledge representation (e.g., Wikidata <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib20\" title=\"\">20</a>]</cite>, YAGO <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib31\" title=\"\">31</a>]</cite>, and NELL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib32\" title=\"\">32</a>]</cite>). KGs are crucial for various applications as they offer accurate explicit knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib19\" title=\"\">19</a>]</cite>.\nBesides, they are renowned for their symbolic reasoning ability <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib22\" title=\"\">22</a>]</cite>, which generates interpretable results.\nKGs can also actively evolve with new knowledge continuously added in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib24\" title=\"\">24</a>]</cite>. Additionally, experts can construct domain-specific KGs to provide precise and dependable domain-specific knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib23\" title=\"\">23</a>]</cite>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p5\">\n<p class=\"ltx_p\" id=\"S1.p5.1\">Nevertheless, KGs are difficult to construct <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib25\" title=\"\">25</a>]</cite>, and current approaches in KGs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib27\" title=\"\">27</a>]</cite> are inadequate in handling the incomplete and dynamically changing nature of real-world KGs. These approaches fail to effectively model unseen entities and represent new facts. In addition, they often ignore the abundant textual information in KGs. Moreover, existing methods in KGs are often customized for specific KGs or tasks, which are not generalizable enough. Therefore, it is also necessary to utilize LLMs to address the challenges faced in KGs. We summarize the pros and cons of LLMs and KGs in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, respectively.</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p6\">\n<p class=\"ltx_p\" id=\"S1.p6.1\">Recently, the possibility of unifying LLMs with KGs has attracted increasing attention from researchers and practitioners. LLMs and KGs are inherently interconnected and can mutually enhance each other. In <em class=\"ltx_emph ltx_font_italic\" id=\"S1.p6.1.1\">KG-enhanced LLMs</em>, KGs can not only be incorporated into the pre-training and inference stages of LLMs to provide external knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib35\" title=\"\">35</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib36\" title=\"\">36</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib37\" title=\"\">37</a>]</cite>, but also used for analyzing LLMs and providing interpretability <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib39\" title=\"\">39</a>]</cite>. In <em class=\"ltx_emph ltx_font_italic\" id=\"S1.p6.1.2\">LLM-augmented KGs</em>, LLMs have been used in various KG-related tasks, e.g., KG embedding <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite>, KG completion <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib26\" title=\"\">26</a>]</cite>, KG construction <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib41\" title=\"\">41</a>]</cite>, KG-to-text generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib42\" title=\"\">42</a>]</cite>, and KGQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib43\" title=\"\">43</a>]</cite>, to improve the performance and facilitate the application of KGs. In <em class=\"ltx_emph ltx_font_italic\" id=\"S1.p6.1.3\">Synergized LLM + KG</em>, researchers marries the merits of LLMs and KGs to mutually enhance performance in knowledge representation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib44\" title=\"\">44</a>]</cite> and reasoning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib45\" title=\"\">45</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib46\" title=\"\">46</a>]</cite>.\nAlthough there are some surveys on knowledge-enhanced LLMs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib47\" title=\"\">47</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib48\" title=\"\">48</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib49\" title=\"\">49</a>]</cite>, which mainly focus on using KGs as an external knowledge to enhance LLMs, they ignore other possibilities of integrating KGs for LLMs and the potential role of LLMs in KG applications.\n</p>\n</div>\n<div class=\"ltx_para\" id=\"S1.p7\">\n<p class=\"ltx_p\" id=\"S1.p7.1\">In this article, we present a forward-looking roadmap for unifying both LLMs and KGs, to leverage their respective strengths and overcome the limitations of each approach, for various downstream tasks. We propose detailed categorization, conduct comprehensive reviews, and pinpoint emerging directions in these fast-growing fields.\nOur main contributions are summarized as follows:</p>\n<ol class=\"ltx_enumerate\" id=\"S1.I1\">\n<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span>\n<div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.I1.i1.p1.1.1\">Roadmap.</span> We present a forward-looking roadmap for integrating LLMs and KGs. Our roadmap, consisting of three general frameworks to unify LLMs and KGs, namely, <span class=\"ltx_text ltx_font_italic\" id=\"S1.I1.i1.p1.1.2\">KG-enhanced LLMs</span>, <span class=\"ltx_text ltx_font_italic\" id=\"S1.I1.i1.p1.1.3\">LLM-augmented KGs</span>, and <span class=\"ltx_text ltx_font_italic\" id=\"S1.I1.i1.p1.1.4\">Synergized LLMs + KGs</span>, provides guidelines for the unification of these two distinct but complementary technologies.\n</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span>\n<div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i2.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.I1.i2.p1.1.1\">Categorization and review.</span> For each integration framework of our roadmap, we present a detailed categorization and novel taxonomies of research on unifying LLMs and KGs. In each category, we review the research from the perspectives of different integration strategies and tasks, which provides more insights into each framework.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.</span>\n<div class=\"ltx_para\" id=\"S1.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.I1.i3.p1.1.1\">Coverage of emerging advances.</span> We cover the advanced techniques in both LLMs and KGs. We include the discussion of state-of-the-art LLMs like ChatGPT and GPT-4 as well as the novel KGs e.g., multi-modal knowledge graphs.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">4.</span>\n<div class=\"ltx_para\" id=\"S1.I1.i4.p1\">\n<p class=\"ltx_p\" id=\"S1.I1.i4.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.I1.i4.p1.1.1\">Summary of challenges and future directions.</span> We highlight the challenges in existing research and present several promising future research directions.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ltx_para\" id=\"S1.p8\">\n<p class=\"ltx_p\" id=\"S1.p8.1\">The rest of this article is organized as follows. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2\" title=\"2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> first explains the background of LLMs and KGs. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S3\" title=\"3 Roadmap &amp; Categorization ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> introduces the roadmap and the overall categorization of this article. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4\" title=\"4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the different KGs-enhanced LLM approaches. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5\" title=\"5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> describes the possible LLM-augmented KG methods. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S6\" title=\"6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the approaches of synergizing LLMs and KGs. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S7\" title=\"7 Future Directions and Milestones ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> discusses the challenges and future research directions. Finally, Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S8\" title=\"8 Conclusion ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> concludes this paper.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S2\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S2.1.1\">Background</span>\n</h2>\n<div class=\"ltx_para\" id=\"S2.p1\">\n<p class=\"ltx_p\" id=\"S2.p1.1\">In this section, we will first briefly introduce a few representative large language models (LLMs) and discuss the prompt engineering that efficiently uses LLMs for varieties of applications. Then, we illustrate the concept of knowledge graphs (KGs) and present different categories of KGs.\n</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S2.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"376\" id=\"S2.F2.g1\" src=\"x1.png\" width=\"664\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>Representative large language models (LLMs) in recent years. Open-source models are represented by solid squares, while closed source models are represented by hollow squares.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"S2.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"308\" id=\"S2.F3.g1\" src=\"x2.png\" width=\"664\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span>An illustration of the Transformer-based LLMs with self-attention mechanism.</figcaption>\n</figure>\n<section class=\"ltx_subsection\" id=\"S2.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">2.1 </span><span class=\"ltx_text ltx_font_italic\" id=\"S2.SS1.1.1\">Large Language models (LLMs)</span>\n</h3>\n<div class=\"ltx_para\" id=\"S2.SS1.p1\">\n<p class=\"ltx_p\" id=\"S2.SS1.p1.1\">Large language models (LLMs) pre-trained on large-scale corpus have shown great potential in various NLP tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib13\" title=\"\">13</a>]</cite>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.F3\" title=\"Figure 3 ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, most LLMs derive from the Transformer design <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib50\" title=\"\">50</a>]</cite>, which contains the encoder and decoder modules empowered by a self-attention mechanism. Based on the architecture structure, LLMs can be categorized into three groups: <em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.p1.1.1\">1) encoder-only LLMs</em>, <em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.p1.1.2\">2) encoder-decoder LLMs</em>, and <em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.p1.1.3\">3) decoder-only LLMs</em>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.F2\" title=\"Figure 2 ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we summarize several representative LLMs with different model architectures, model sizes, and open-source availabilities.</p>\n</div>\n<section class=\"ltx_subsubsection\" id=\"S2.SS1.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">2.1.1 </span>Encoder-only LLMs.</h4>\n<div class=\"ltx_para\" id=\"S2.SS1.SSS1.p1\">\n<p class=\"ltx_p\" id=\"S2.SS1.SSS1.p1.1\">Encoder-only large language models only use the encoder to encode the sentence and understand the relationships between words. The common training paradigm for these model is to predict the mask words in an input sentence. This method is unsupervised and can be trained on the large-scale corpus. Encoder-only LLMs like BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib1\" title=\"\">1</a>]</cite>, ALBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib51\" title=\"\">51</a>]</cite>, RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib2\" title=\"\">2</a>]</cite>, and ELECTRA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib52\" title=\"\">52</a>]</cite> require adding an extra prediction head to resolve downstream tasks. These models are most effective for tasks that require understanding the entire sentence, such as text classification <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib26\" title=\"\">26</a>]</cite> and named entity recognition <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib53\" title=\"\">53</a>]</cite>.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S2.SS1.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">2.1.2 </span>Encoder-decoder LLMs.</h4>\n<div class=\"ltx_para\" id=\"S2.SS1.SSS2.p1\">\n<p class=\"ltx_p\" id=\"S2.SS1.SSS2.p1.1\">Encoder-decoder large language models adopt both the encoder and decoder module. The encoder module is responsible for encoding the input sentence into a hidden-space, and the decoder is used to generate the target output text. The training strategies in encoder-decoder LLMs can be more flexible. For example, T5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib3\" title=\"\">3</a>]</cite> is pre-trained by masking and predicting spans of masking words. UL2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib54\" title=\"\">54</a>]</cite> unifies several training targets such as different masking spans and masking frequencies. Encoder-decoder LLMs (e.g., T0 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib55\" title=\"\">55</a>]</cite>, ST-MoE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib56\" title=\"\">56</a>]</cite>, and GLM-130B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib57\" title=\"\">57</a>]</cite>) are able to directly resolve tasks that generate sentences based on some context, such as summariaztion, translation, and question answering <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib58\" title=\"\">58</a>]</cite>.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S2.SS1.SSS3\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">2.1.3 </span>Decoder-only LLMs.</h4>\n<div class=\"ltx_para\" id=\"S2.SS1.SSS3.p1\">\n<p class=\"ltx_p\" id=\"S2.SS1.SSS3.p1.1\">Decoder-only large language models only adopt the decoder module to generate target output text. The training paradigm for these models is to predict the next word in the sentence. Large-scale decoder-only LLMs can generally perform downstream tasks from a few examples or simple instructions, without adding prediction heads or finetuning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib59\" title=\"\">59</a>]</cite>. Many state-of-the-art LLMs (e.g., Chat-GPT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib60\" title=\"\">60</a>]</cite> and GPT-4<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/product/gpt-4\" title=\"\">https://openai.com/product/gpt-4</a></span></span></span>) follow the decoder-only architecture. However, since these models are closed-source, it is challenging for academic researchers to conduct further research. Recently, Alpaca<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/tatsu-lab/stanford_alpaca\" title=\"\">https://github.com/tatsu-lab/stanford_alpaca</a></span></span></span> and Vicuna<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://lmsys.org/blog/2023-03-30-vicuna/\" title=\"\">https://lmsys.org/blog/2023-03-30-vicuna/</a></span></span></span> are released as open-source decoder-only LLMs. These models are finetuned based on LLaMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib61\" title=\"\">61</a>]</cite> and achieve comparable performance with ChatGPT and GPT-4.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S2.SS1.SSS4\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">2.1.4 </span>Prompt Engineering</h4>\n<div class=\"ltx_para\" id=\"S2.SS1.SSS4.p1\">\n<p class=\"ltx_p\" id=\"S2.SS1.SSS4.p1.1\">Prompt engineering is a novel field that focuses on creating and refining prompts to maximize the effectiveness of large language models (LLMs) across various applications and research areas <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib62\" title=\"\">62</a>]</cite>.\nAs shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.F4\" title=\"Figure 4 ‣ 2.1.4 Prompt Engineering ‣ 2.1 Large Language models (LLMs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, a prompt is a sequence of natural language inputs for LLMs that are specified for the task, such as sentiment classification. A prompt could contain several elements, i.e., <em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.SSS4.p1.1.1\">1) Instruction</em>, <em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.SSS4.p1.1.2\">2) Context</em>, and <em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.SSS4.p1.1.3\">3) Input Text</em>. <em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.SSS4.p1.1.4\">Instruction</em> is a short sentence that instructs the model to perform a specific task. <em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.SSS4.p1.1.5\">Context</em> provides the context for the input text or few-shot examples. <em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS1.SSS4.p1.1.6\">Input Text</em> is the text that needs to be processed by the model.</p>\n</div>\n<div class=\"ltx_para\" id=\"S2.SS1.SSS4.p2\">\n<p class=\"ltx_p\" id=\"S2.SS1.SSS4.p2.1\">Prompt engineering seeks to improve the capacity of large large language models (e.g., ChatGPT) in diverse complex tasks such as question answering, sentiment classification, and common sense reasoning. Chain-of-thought (CoT) prompt <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib63\" title=\"\">63</a>]</cite> enables complex reasoning capabilities through intermediate reasoning steps.\nPrompt engineering also enables the integration of structural data like knowledge graphs (KGs) into LLMs. Li et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib64\" title=\"\">64</a>]</cite> simply linearizes the KGs and uses templates to convert the KGs into passages.\nMindmap <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib65\" title=\"\">65</a>]</cite> designs a KG prompt to convert graph structure into a mind map that enables LLMs to perform reasoning on it.\nPrompt offers a simple way to utilize the potential of LLMs without finetuning. Proficiency in prompt engineering leads to a better understanding of the strengths and weaknesses of LLMs.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S2.F4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"565\" id=\"S2.F4.g1\" src=\"x3.png\" width=\"706\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span>An example of sentiment classification prompt.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"S2.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"1020\" id=\"S2.F5.g1\" src=\"x4.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span>Examples of different categories’ knowledge graphs, i.e., <em class=\"ltx_emph ltx_font_italic\" id=\"S2.F5.5.1\">encyclopedic KGs</em>, <em class=\"ltx_emph ltx_font_italic\" id=\"S2.F5.6.2\">commonsense KGs</em>, <em class=\"ltx_emph ltx_font_italic\" id=\"S2.F5.7.3\">domain-specific KGs</em>, and <em class=\"ltx_emph ltx_font_italic\" id=\"S2.F5.8.4\">multi-modal KGs</em>.</figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S2.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">2.2 </span><span class=\"ltx_text ltx_font_italic\" id=\"S2.SS2.1.1\">Knowledge Graphs (KGs)</span>\n</h3>\n<div class=\"ltx_para\" id=\"S2.SS2.p1\">\n<p class=\"ltx_p\" id=\"S2.SS2.p1.3\">Knowledge graphs (KGs) store structured knowledge as a collection of triples <math alttext=\"\\mathcal{KG}=\\{(h,r,t)\\subseteq\\mathcal{E}\\times\\mathcal{R}\\times\\mathcal{E}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.1.m1.4\"><semantics id=\"S2.SS2.p1.1.m1.4a\"><mrow id=\"S2.SS2.p1.1.m1.4.4\" xref=\"S2.SS2.p1.1.m1.4.4.cmml\"><mrow id=\"S2.SS2.p1.1.m1.4.4.3\" xref=\"S2.SS2.p1.1.m1.4.4.3.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.SS2.p1.1.m1.4.4.3.2\" xref=\"S2.SS2.p1.1.m1.4.4.3.2.cmml\">𝒦</mi><mo id=\"S2.SS2.p1.1.m1.4.4.3.1\" xref=\"S2.SS2.p1.1.m1.4.4.3.1.cmml\">⁢</mo><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.SS2.p1.1.m1.4.4.3.3\" xref=\"S2.SS2.p1.1.m1.4.4.3.3.cmml\">𝒢</mi></mrow><mo id=\"S2.SS2.p1.1.m1.4.4.2\" xref=\"S2.SS2.p1.1.m1.4.4.2.cmml\">=</mo><mrow id=\"S2.SS2.p1.1.m1.4.4.1.1\" xref=\"S2.SS2.p1.1.m1.4.4.1.2.cmml\"><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.2\" stretchy=\"false\" xref=\"S2.SS2.p1.1.m1.4.4.1.2.cmml\">{</mo><mrow id=\"S2.SS2.p1.1.m1.4.4.1.1.1\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.cmml\"><mrow id=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.2\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.1.cmml\"><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.2.1\" stretchy=\"false\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.1.cmml\">(</mo><mi id=\"S2.SS2.p1.1.m1.1.1\" xref=\"S2.SS2.p1.1.m1.1.1.cmml\">h</mi><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.2.2\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.1.cmml\">,</mo><mi id=\"S2.SS2.p1.1.m1.2.2\" xref=\"S2.SS2.p1.1.m1.2.2.cmml\">r</mi><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.2.3\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.1.cmml\">,</mo><mi id=\"S2.SS2.p1.1.m1.3.3\" xref=\"S2.SS2.p1.1.m1.3.3.cmml\">t</mi><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.2.4\" stretchy=\"false\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.1.cmml\">)</mo></mrow><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.1.1\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.1.cmml\">⊆</mo><mrow id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.2\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.2.cmml\">ℰ</mi><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.1\" lspace=\"0.222em\" rspace=\"0.222em\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.1.cmml\">×</mo><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.3\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.3.cmml\">ℛ</mi><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.1a\" lspace=\"0.222em\" rspace=\"0.222em\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.1.cmml\">×</mo><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.4\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.4.cmml\">ℰ</mi></mrow></mrow><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.3\" stretchy=\"false\" xref=\"S2.SS2.p1.1.m1.4.4.1.2.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p1.1.m1.4b\"><apply id=\"S2.SS2.p1.1.m1.4.4.cmml\" xref=\"S2.SS2.p1.1.m1.4.4\"><eq id=\"S2.SS2.p1.1.m1.4.4.2.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.2\"></eq><apply id=\"S2.SS2.p1.1.m1.4.4.3.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.3\"><times id=\"S2.SS2.p1.1.m1.4.4.3.1.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.3.1\"></times><ci id=\"S2.SS2.p1.1.m1.4.4.3.2.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.3.2\">𝒦</ci><ci id=\"S2.SS2.p1.1.m1.4.4.3.3.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.3.3\">𝒢</ci></apply><set id=\"S2.SS2.p1.1.m1.4.4.1.2.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1\"><apply id=\"S2.SS2.p1.1.m1.4.4.1.1.1.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1\"><subset id=\"S2.SS2.p1.1.m1.4.4.1.1.1.1.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.1\"></subset><vector id=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.1.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.2\"><ci id=\"S2.SS2.p1.1.m1.1.1.cmml\" xref=\"S2.SS2.p1.1.m1.1.1\">ℎ</ci><ci id=\"S2.SS2.p1.1.m1.2.2.cmml\" xref=\"S2.SS2.p1.1.m1.2.2\">𝑟</ci><ci id=\"S2.SS2.p1.1.m1.3.3.cmml\" xref=\"S2.SS2.p1.1.m1.3.3\">𝑡</ci></vector><apply id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3\"><times id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.1.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.1\"></times><ci id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.2.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.2\">ℰ</ci><ci id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.3.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.3\">ℛ</ci><ci id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.4.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.4\">ℰ</ci></apply></apply></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p1.1.m1.4c\">\\mathcal{KG}=\\{(h,r,t)\\subseteq\\mathcal{E}\\times\\mathcal{R}\\times\\mathcal{E}\\}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p1.1.m1.4d\">caligraphic_K caligraphic_G = { ( italic_h , italic_r , italic_t ) ⊆ caligraphic_E × caligraphic_R × caligraphic_E }</annotation></semantics></math>, where <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.2.m2.1\"><semantics id=\"S2.SS2.p1.2.m2.1a\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.SS2.p1.2.m2.1.1\" xref=\"S2.SS2.p1.2.m2.1.1.cmml\">ℰ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p1.2.m2.1b\"><ci id=\"S2.SS2.p1.2.m2.1.1.cmml\" xref=\"S2.SS2.p1.2.m2.1.1\">ℰ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p1.2.m2.1c\">\\mathcal{E}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p1.2.m2.1d\">caligraphic_E</annotation></semantics></math> and <math alttext=\"\\mathcal{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.3.m3.1\"><semantics id=\"S2.SS2.p1.3.m3.1a\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.SS2.p1.3.m3.1.1\" xref=\"S2.SS2.p1.3.m3.1.1.cmml\">ℛ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p1.3.m3.1b\"><ci id=\"S2.SS2.p1.3.m3.1.1.cmml\" xref=\"S2.SS2.p1.3.m3.1.1\">ℛ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p1.3.m3.1c\">\\mathcal{R}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p1.3.m3.1d\">caligraphic_R</annotation></semantics></math> respectively denote the set of entities and relations. Existing knowledge graphs (KGs) can be classified into four groups based on the stored information: <em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS2.p1.3.1\">1) encyclopedic KGs</em>, <em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS2.p1.3.2\">2) commonsense KGs</em>, <em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS2.p1.3.3\">3) domain-specific KGs</em>, and <em class=\"ltx_emph ltx_font_italic\" id=\"S2.SS2.p1.3.4\">4) multi-modal KGs</em>. We illustrate the examples of KGs of different categories in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.F5\" title=\"Figure 5 ‣ 2.1.4 Prompt Engineering ‣ 2.1 Large Language models (LLMs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n</div>\n<section class=\"ltx_subsubsection\" id=\"S2.SS2.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">2.2.1 </span>Encyclopedic Knowledge Graphs.</h4>\n<div class=\"ltx_para\" id=\"S2.SS2.SSS1.p1\">\n<p class=\"ltx_p\" id=\"S2.SS2.SSS1.p1.1\">Encyclopedic knowledge graphs are the most ubiquitous KGs, which represent the general knowledge in real-world. Encyclopedic knowledge graphs are often constructed by integrating information from diverse and extensive sources, including human experts, encyclopedias, and databases. Wikidata <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib20\" title=\"\">20</a>]</cite> is one of the most widely used encyclopedic knowledge graphs, which incorporates varieties of knowledge extracted from articles on Wikipedia. Other typical encyclopedic knowledge graphs, like Freebase <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib66\" title=\"\">66</a>]</cite>, Dbpedia <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib67\" title=\"\">67</a>]</cite>, and YAGO <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib31\" title=\"\">31</a>]</cite> are also derived from Wikipedia. In addition, NELL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib32\" title=\"\">32</a>]</cite> is a continuously improving encyclopedic knowledge graph, which automatically extracts knowledge from the web, and uses that knowledge to improve its performance over time. There are several encyclopedic knowledge graphs available in languages other than English such as CN-DBpedia <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib68\" title=\"\">68</a>]</cite> and Vikidia <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib69\" title=\"\">69</a>]</cite>. The largest knowledge graph, named Knowledge Occean (KO)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://ko.zhonghuapu.com/\" title=\"\">https://ko.zhonghuapu.com/</a></span></span></span>, currently contains 4,8784,3636 entities and\n17,3115,8349 relations in both English and Chinese.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S2.SS2.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">2.2.2 </span>Commonsense Knowledge Graphs.</h4>\n<div class=\"ltx_para\" id=\"S2.SS2.SSS2.p1\">\n<p class=\"ltx_p\" id=\"S2.SS2.SSS2.p1.1\">Commonsense knowledge graphs formulate the knowledge about daily concepts, e.g., objects, and events, as well as their relationships <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib70\" title=\"\">70</a>]</cite>. Compared with encyclopedic knowledge graphs, commonsense knowledge graphs often model the tacit knowledge extracted from text such as <span class=\"ltx_text ltx_font_italic\" id=\"S2.SS2.SSS2.p1.1.1\">(Car, UsedFor, Drive)</span>. ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib71\" title=\"\">71</a>]</cite> contains a wide range of commonsense concepts and relations, which can help computers understand the meanings of words people use. ATOMIC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib72\" title=\"\">72</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib73\" title=\"\">73</a>]</cite> and ASER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib74\" title=\"\">74</a>]</cite> focus on the causal effects between events, which can be used for commonsense reasoning. Some other commonsense knowledge graphs, such as TransOMCS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib75\" title=\"\">75</a>]</cite> and CausalBanK <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib76\" title=\"\">76</a>]</cite> are automatically constructed to provide commonsense knowledge.\n</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S2.SS2.SSS3\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">2.2.3 </span>Domain-specific Knowledge Graphs</h4>\n<div class=\"ltx_para\" id=\"S2.SS2.SSS3.p1\">\n<p class=\"ltx_p\" id=\"S2.SS2.SSS3.p1.1\">Domain-specific knowledge graphs are often constructed to represent knowledge in a specific domain, e.g., medical, biology, and finance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib23\" title=\"\">23</a>]</cite>. Compared with encyclopedic knowledge graphs, domain-specific knowledge graphs are often smaller in size, but more accurate and reliable. For example, UMLS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib77\" title=\"\">77</a>]</cite> is a domain-specific knowledge graph in the medical domain, which contains biomedical concepts and their relationships. In addition, there are some domain-specific knowledge graphs in other domains, such as finance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib78\" title=\"\">78</a>]</cite>, geology <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib79\" title=\"\">79</a>]</cite>, biology <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib80\" title=\"\">80</a>]</cite>, chemistry <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib81\" title=\"\">81</a>]</cite> and genealogy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib82\" title=\"\">82</a>]</cite>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S2.F6\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"201\" id=\"S2.F6.g1\" src=\"x5.png\" width=\"747\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span>The general roadmap of unifying KGs and LLMs. (a.) KG-enhanced LLMs. (b.) LLM-augmented KGs. (c.) Synergized LLMs + KGs.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S2.SS2.SSS4\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">2.2.4 </span>Multi-modal Knowledge Graphs.</h4>\n<div class=\"ltx_para\" id=\"S2.SS2.SSS4.p1\">\n<p class=\"ltx_p\" id=\"S2.SS2.SSS4.p1.1\">Unlike conventional knowledge graphs that only contain textual information, multi-modal knowledge graphs represent facts in multiple modalities such as images, sounds, and videos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib83\" title=\"\">83</a>]</cite>. For example, IMGpedia <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib84\" title=\"\">84</a>]</cite>, MMKG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib85\" title=\"\">85</a>]</cite>, and Richpedia <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib86\" title=\"\">86</a>]</cite> incorporate both the text and image information into the knowledge graphs. These knowledge graphs can be used for various multi-modal tasks such as image-text matching <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib87\" title=\"\">87</a>]</cite>, visual question answering <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib88\" title=\"\">88</a>]</cite>, and recommendation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib89\" title=\"\">89</a>]</cite>.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S2.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE I: </span>Representative applications of using LLMs and KGs.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S2.T1.1\" style=\"width:433.6pt;height:297pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(46.0pt,-31.5pt) scale(1.26918087363362,1.26918087363362) ;\">\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S2.T1.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S2.T1.1.1.1.1.1\">Name</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S2.T1.1.1.1.1.2\">Category</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S2.T1.1.1.1.1.3\">LLMs</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S2.T1.1.1.1.1.4\">KGs</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.1.5\">URL</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.1.2.1.1\">ChatGPT/GPT-4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.1.2.1.2\">Chat Bot</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.1.2.1.3\">✓</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S2.T1.1.1.2.1.4\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2.1.5\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://shorturl.at/cmsE0\" title=\"\">https://shorturl.at/cmsE0</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.3.2.1\">ERNIE 3.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.3.2.2\">Chat Bot</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.3.2.3\">✓</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.3.2.4\">✓</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.3.2.5\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://shorturl.at/sCLV9\" title=\"\">https://shorturl.at/sCLV9</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.4.3.1\">Bard</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.4.3.2\">Chat Bot</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.4.3.3\">✓</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.4.3.4\">✓</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.4.3.5\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://shorturl.at/pDLY6\" title=\"\">https://shorturl.at/pDLY6</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.5.4.1\">Firefly</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.5.4.2\">Photo Editing</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.5.4.3\">✓</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.1.1.5.4.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.5.4.5\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://shorturl.at/fkzJV\" title=\"\">https://shorturl.at/fkzJV</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.6.5.1\">AutoGPT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.6.5.2\">AI Assistant</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.6.5.3\">✓</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.1.1.6.5.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.6.5.5\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://shorturl.at/bkoSY\" title=\"\">https://shorturl.at/bkoSY</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.7.6.1\">Copilot</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.7.6.2\">Coding Assistant</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.7.6.3\">✓</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.1.1.7.6.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.7.6.5\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://shorturl.at/lKLUV\" title=\"\">https://shorturl.at/lKLUV</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.8.7.1\">New Bing</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.8.7.2\">Web Search</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.8.7.3\">✓</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.1.1.8.7.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.8.7.5\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://shorturl.at/bimps\" title=\"\">https://shorturl.at/bimps</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.9.8.1\">Shop.ai</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.9.8.2\">Recommendation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.9.8.3\">✓</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.1.1.9.8.4\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.9.8.5\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://shorturl.at/alCY7\" title=\"\">https://shorturl.at/alCY7</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.10.9.1\">Wikidata</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.10.9.2\">Knowledge Base</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.1.1.10.9.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.10.9.4\">✓</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.10.9.5\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://shorturl.at/lyMY5\" title=\"\">https://shorturl.at/lyMY5</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.11.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.11.10.1\">KO</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.11.10.2\">Knowledge Base</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.1.1.11.10.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.11.10.4\">✓</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.11.10.5\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://shorturl.at/sx238\" title=\"\">https://shorturl.at/sx238</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.12.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.12.11.1\">OpenBG</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.12.11.2\">Recommendation</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S2.T1.1.1.12.11.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.1.12.11.4\">✓</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.12.11.5\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://shorturl.at/pDMV9\" title=\"\">https://shorturl.at/pDMV9</a></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.13.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S2.T1.1.1.13.12.1\">Doctor.ai</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S2.T1.1.1.13.12.2\">Health Care Assistant</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S2.T1.1.1.13.12.3\">✓</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S2.T1.1.1.13.12.4\">✓</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S2.T1.1.1.13.12.5\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://shorturl.at/dhlK0\" title=\"\">https://shorturl.at/dhlK0</a></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S2.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">2.3 </span><span class=\"ltx_text ltx_font_italic\" id=\"S2.SS3.1.1\">Applications</span>\n</h3>\n<div class=\"ltx_para\" id=\"S2.SS3.p1\">\n<p class=\"ltx_p\" id=\"S2.SS3.p1.1\">LLMs as KGs have been widely applied in various real-world applications. We summarize some representative applications of using LLMs and KGs in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.T1\" title=\"TABLE I ‣ 2.2.4 Multi-modal Knowledge Graphs. ‣ 2.2 Knowledge Graphs (KGs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>. ChatGPT/GPT-4 are LLM-based chatbots that can communicate with humans in a natural dialogue format. To improve knowledge awareness of LLMs, ERNIE 3.0 and Bard incorporate KGs into their chatbot applications. Instead of Chatbot. Firefly develops a photo editing application that allows users to edit photos by using natural language descriptions. Copilot, New Bing, and Shop.ai adopt LLMs to empower their applications in the areas of coding assistant, web search, and recommendation, respectively. Wikidata and KO are two representative knowledge graph applications that are used to provide external knowledge.\nOpenBG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib90\" title=\"\">90</a>]</cite> is a knowledge graph designed for recommendation. Doctor.ai develops a health care assistant that incorporates LLMs and KGs to provide medical advice.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S3\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S3.1.1\">Roadmap &amp; Categorization</span>\n</h2>\n<div class=\"ltx_para\" id=\"S3.p1\">\n<p class=\"ltx_p\" id=\"S3.p1.1\">In this section, we first present a road map of explicit frameworks that unify LLMs and KGs. Then, we present the categorization of research on unifying LLMs and KGs.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S3.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.1 </span><span class=\"ltx_text ltx_font_italic\" id=\"S3.SS1.1.1\">Roadmap</span>\n</h3>\n<div class=\"ltx_para\" id=\"S3.SS1.p1\">\n<p class=\"ltx_p\" id=\"S3.SS1.p1.1\">The roadmap of unifying KGs and LLMs is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S2.F6\" title=\"Figure 6 ‣ 2.2.3 Domain-specific Knowledge Graphs ‣ 2.2 Knowledge Graphs (KGs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. In the roadmap, we identify three frameworks for the unification of LLMs and KGs, including KG-enhanced LLMs, LLM-augmented KGs, and Synergized LLMs + KGs. The KG-enhanced LLMs and LLM-augmented KGs are two parallel frameworks that aim to enhance the capabilities of LLMs and KGs, respectively. Building upon these frameworks, Synergized LLMs + KGs is a unified framework that aims to synergize LLMs and KGs to mutually enhance each other.</p>\n</div>\n<section class=\"ltx_subsubsection\" id=\"S3.SS1.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">3.1.1 </span>KG-enhanced LLMs</h4>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS1.p1\">\n<p class=\"ltx_p\" id=\"S3.SS1.SSS1.p1.1\">LLMs are renowned for their ability to learn knowledge from large-scale corpus and achieve state-of-the-art performance in various NLP tasks. However, LLMs are often criticized for their hallucination issues <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib15\" title=\"\">15</a>]</cite>, and lacking of interpretability. To address these issues, researchers have proposed to enhance LLMs with knowledge graphs (KGs).</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS1.p2\">\n<p class=\"ltx_p\" id=\"S3.SS1.SSS1.p2.1\">KGs store enormous knowledge in an explicit and structured way, which can be used to enhance the knowledge awareness of LLMs. Some researchers have proposed to incorporate KGs into LLMs during the pre-training stage, which can help LLMs learn knowledge from KGs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib35\" title=\"\">35</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib91\" title=\"\">91</a>]</cite>. Other researchers have proposed to incorporate KGs into LLMs during the inference stage. By retrieving knowledge from KGs, it can significantly improve the performance of LLMs in accessing domain-specific knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib92\" title=\"\">92</a>]</cite>. To improve the interpretability of LLMs, researchers also utilize KGs to interpret the facts <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a>]</cite> and the reasoning process of LLMs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib38\" title=\"\">38</a>]</cite>.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S3.SS1.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">3.1.2 </span>LLM-augmented KGs</h4>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS2.p1\">\n<p class=\"ltx_p\" id=\"S3.SS1.SSS2.p1.1\">KGs store structure knowledge playing an essential role in many real-word applications <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib19\" title=\"\">19</a>]</cite>. Existing methods in KGs fall short of handling incomplete KGs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib33\" title=\"\">33</a>]</cite> and processing text corpus to construct KGs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib93\" title=\"\">93</a>]</cite>. With the generalizability of LLMs, many researchers are trying to harness the power of LLMs for addressing KG-related tasks.\n</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS2.p2\">\n<p class=\"ltx_p\" id=\"S3.SS1.SSS2.p2.1\">The most straightforward way to apply LLMs as text encoders for KG-related tasks. Researchers take advantage of LLMs to process the textual corpus in the KGs and then use the representations of the text to enrich KGs representation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib94\" title=\"\">94</a>]</cite>. Some studies also use LLMs to process the original corpus and extract relations and entities for KG construction <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib95\" title=\"\">95</a>]</cite>. Recent studies try to design a KG prompt that\ncan effectively convert structural KGs into a format that can be comprehended by LLMs. In this way, LLMs can be directly applied to KG-related tasks, e.g., KG completion <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib96\" title=\"\">96</a>]</cite> and KG reasoning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib97\" title=\"\">97</a>]</cite>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S3.F7\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"315\" id=\"S3.F7.g1\" src=\"x6.png\" width=\"358\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 7: </span>The general framework of the <span class=\"ltx_text ltx_font_italic\" id=\"S3.F7.6.1\">Synergized LLMs + KGs</span>, which contains four layers: <em class=\"ltx_emph ltx_font_italic\" id=\"S3.F7.7.2\">1) Data</em>, <em class=\"ltx_emph ltx_font_italic\" id=\"S3.F7.8.3\">2) Synergized Model</em>, <em class=\"ltx_emph ltx_font_italic\" id=\"S3.F7.9.4\">3) Technique</em>, and <em class=\"ltx_emph ltx_font_italic\" id=\"S3.F7.10.5\">4) Application</em>.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S3.SS1.SSS3\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">3.1.3 </span>Synergized LLMs + KGs</h4>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS3.p1\">\n<p class=\"ltx_p\" id=\"S3.SS1.SSS3.p1.1\">The synergy of LLMs and KGs has attracted increasing attention from researchers these years <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib42\" title=\"\">42</a>]</cite>. LLMs and KGs are two inherently complementary techniques, which should be unified into a general framework to mutually enhance each other.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS1.SSS3.p2\">\n<p class=\"ltx_p\" id=\"S3.SS1.SSS3.p2.1\">To further explore the unification, we propose a unified framework of the synergized LLMs + KGs in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S3.F7\" title=\"Figure 7 ‣ 3.1.2 LLM-augmented KGs ‣ 3.1 Roadmap ‣ 3 Roadmap &amp; Categorization ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. The unified framework contains four layers: <em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.SSS3.p2.1.1\">1) Data</em>, <em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.SSS3.p2.1.2\">2) Synergized Model</em>, <em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.SSS3.p2.1.3\">3) Technique</em>, and <em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.SSS3.p2.1.4\">4) Application</em>. In the <em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.SSS3.p2.1.5\">Data</em> layer, LLMs and KGs are used to process the textual and structural data, respectively. With the development of multi-modal LLMs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib98\" title=\"\">98</a>]</cite> and KGs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib99\" title=\"\">99</a>]</cite>, this framework can be extended to process multi-modal data, such as video, audio, and images. In the <em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.SSS3.p2.1.6\">Synergized Model</em> layer, LLMs and KGs could synergize with each other to improve their capabilities. In <em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.SSS3.p2.1.7\">Technique</em> layer, related techniques that have been used in LLMs and KGs can be incorporated into this framework to further enhance the performance. In the <em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS1.SSS3.p2.1.8\">Application</em> layer, LLMs and KGs can be integrated to address various real-world applications, such as search engines <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib100\" title=\"\">100</a>]</cite>, recommender systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib10\" title=\"\">10</a>]</cite>, and AI assistants <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib101\" title=\"\">101</a>]</cite>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S3.F8\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"437\" id=\"S3.F8.g1\" src=\"x7.png\" width=\"623\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 8: </span>Fine-grained categorization of research on unifying large language models (LLMs) with knowledge graphs (KGs).</figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.2 </span><span class=\"ltx_text ltx_font_italic\" id=\"S3.SS2.1.1\">Categorization</span>\n</h3>\n<div class=\"ltx_para\" id=\"S3.SS2.p1\">\n<p class=\"ltx_p\" id=\"S3.SS2.p1.1\">To better understand the research on unifying LLMs and KGs, we further provide a fine-grained categorization for each framework in the roadmap. Specifically, we focus on different ways of integrating KGs and LLMs, i.e., KG-enhanced LLMs, KG-augmented LLMs, and Synergized LLMs + KGs. The fine-grained categorization of the research is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S3.F8\" title=\"Figure 8 ‣ 3.1.3 Synergized LLMs + KGs ‣ 3.1 Roadmap ‣ 3 Roadmap &amp; Categorization ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS2.p2\">\n<p class=\"ltx_p\" id=\"S3.SS2.p2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.SS2.p2.1.1\">KG-enhanced LLMs.</span> Integrating KGs can enhance the performance and interpretability of LLMs in various downstream tasks. We categorize the research on KG-enhanced LLMs into three groups:</p>\n<ol class=\"ltx_enumerate\" id=\"S3.I1\">\n<li class=\"ltx_item\" id=\"S3.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span>\n<div class=\"ltx_para\" id=\"S3.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S3.I1.i1.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.I1.i1.p1.1.1\">KG-enhanced LLM pre-training</em> includes works that apply KGs during the pre-training stage and improve the knowledge expression of LLMs.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span>\n<div class=\"ltx_para\" id=\"S3.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"S3.I1.i2.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.I1.i2.p1.1.1\">KG-enhanced LLM inference</em> includes research that utilizes KGs during the inference stage of LLMs, which enables LLMs to access the latest knowledge without retraining.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.</span>\n<div class=\"ltx_para\" id=\"S3.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"S3.I1.i3.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.I1.i3.p1.1.1\">KG-enhanced LLM interpretability</em> includes works that use KGs to understand the knowledge learned by LLMs and interpret the reasoning process of LLMs.</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS2.p3\">\n<p class=\"ltx_p\" id=\"S3.SS2.p3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.SS2.p3.1.1\">LLM-augmented KGs.</span> LLMs can be applied to augment various KG-related tasks. We categorize the research on LLM-augmented KGs into five groups based on the task types:</p>\n<ol class=\"ltx_enumerate\" id=\"S3.I2\">\n<li class=\"ltx_item\" id=\"S3.I2.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span>\n<div class=\"ltx_para\" id=\"S3.I2.i1.p1\">\n<p class=\"ltx_p\" id=\"S3.I2.i1.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.I2.i1.p1.1.1\">LLM-augmented KG embedding</em> includes studies that apply LLMs to enrich representations of KGs by encoding the textual descriptions\nof entities and relations.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I2.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span>\n<div class=\"ltx_para\" id=\"S3.I2.i2.p1\">\n<p class=\"ltx_p\" id=\"S3.I2.i2.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.I2.i2.p1.1.1\">LLM-augmented KG completion</em> includes papers that utilize LLMs to encode text or generate facts for better KGC performance.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I2.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.</span>\n<div class=\"ltx_para\" id=\"S3.I2.i3.p1\">\n<p class=\"ltx_p\" id=\"S3.I2.i3.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.I2.i3.p1.1.1\">LLM-augmented KG construction</em> includes works that apply LLMs to address the entity discovery, coreference resolution, and relation extraction tasks for KG construction.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I2.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">4.</span>\n<div class=\"ltx_para\" id=\"S3.I2.i4.p1\">\n<p class=\"ltx_p\" id=\"S3.I2.i4.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.I2.i4.p1.1.1\">LLM-augmented KG-to-text Generation</em> includes research that utilizes LLMs to generate natural language that describes the facts from KGs.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I2.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">5.</span>\n<div class=\"ltx_para\" id=\"S3.I2.i5.p1\">\n<p class=\"ltx_p\" id=\"S3.I2.i5.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"S3.I2.i5.p1.1.1\">LLM-augmented KG question answering</em> includes studies that apply LLMs to bridge the gap between natural language questions and retrieve answers from KGs.\n</p>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS2.p4\">\n<p class=\"ltx_p\" id=\"S3.SS2.p4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.SS2.p4.1.1\">Synergized LLMs + KGs.</span> The synergy of LLMs and KGs aims to integrate LLMs and KGs into a unified framework to mutually enhance each other. In this categorization, we review the recent attempts of Synergized LLMs + KGs from the perspectives of <em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS2.p4.1.2\">knowledge representation</em> and <em class=\"ltx_emph ltx_font_italic\" id=\"S3.SS2.p4.1.3\">reasoning</em>.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS2.p5\">\n<p class=\"ltx_p\" id=\"S3.SS2.p5.1\">In the following sections (Sec <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4\" title=\"4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5\" title=\"5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S6\" title=\"6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>), we will provide details on these categorizations.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S4\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.1.1\">KG-enhanced LLMs</span>\n</h2>\n<div class=\"ltx_para\" id=\"S4.p1\">\n<p class=\"ltx_p\" id=\"S4.p1.1\">Large language models (LLMs) achieve promising results in many natural language processing tasks. However, LLMs have been criticized for their lack of practical knowledge and tendency to generate factual errors during inference. To address this issue, researchers have proposed integrating knowledge graphs (KGs) to enhance LLMs. In this section, we first introduce the KG-enhanced LLM pre-training, which aims to inject knowledge into LLMs during the pre-training stage. Then, we introduce the KG-enhanced LLM inference, which enables LLMs to consider the latest knowledge while generating sentences. Finally, we introduce the KG-enhanced LLM interpretability, which aims to improve the interpretability of LLMs by using KGs. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.T2\" title=\"TABLE II ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> summarizes the typical methods that integrate KGs for LLMs.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE II: </span>Summary of KG-enhanced LLM methods.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T2.1\" style=\"width:433.6pt;height:414.1pt;vertical-align:-17.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-108.0pt,98.7pt) scale(0.667591346250751,0.667591346250751) ;\">\n<p class=\"ltx_p\" id=\"S4.T2.1.1\">[b]\n\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1.1\">\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T2.1.1.1.1.1.1\">Task</span>\n<span class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T2.1.1.1.1.1.2\">Method</span>\n<span class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T2.1.1.1.1.1.3\">Year</span>\n<span class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T2.1.1.1.1.1.4\">KG</span>\n<span class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T2.1.1.1.1.1.5\">Technique</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.2.2\">\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_15\" id=\"S4.T2.1.1.1.2.2.1\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.2.2.1.1\">KG-enhanced LLM pre-training</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.2.2.2\">ERNIE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib35\" title=\"\">35</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.2.2.3\">2019</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.2.2.4.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.2.2.5\">Integrating KGs into Training Objective</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.3.3\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.3.3.1\">GLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib102\" title=\"\">102</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.3.3.2\">2020</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.3.3.3.1\">C</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.3.3.4\">Integrating KGs into Training Objective</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.4.4\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.4.4.1\">Ebert <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib103\" title=\"\">103</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.4.4.2\">2020</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.4.4.3.1\">D</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.4.4.4\">Integrating KGs into Training Objective</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.5.5\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.5.5.1\">KEPLER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.5.5.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.5.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.5.5.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.5.5.4\">Integrating KGs into Training Objective</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.6.6\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.6.6.1\">Deterministic LLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib104\" title=\"\">104</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.6.6.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.6.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.6.6.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.6.6.4\">Integrating KGs into Training Objective</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.7.7\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.7.7.1\">KALA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib105\" title=\"\">105</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.7.7.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.7.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.7.7.3.1\">D</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.7.7.4\">Integrating KGs into Training Objective</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.8.8\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.8.8.1\">WKLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib106\" title=\"\">106</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.8.8.2\">2020</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.8.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.8.8.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.8.8.4\">Integrating KGs into Training Objective</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.9.9\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.9.9.1\">K-BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib36\" title=\"\">36</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.9.9.2\">2020</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.9.9.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.9.9.3.1\">E</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.9.9.3.2\">D</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.9.9.4\">Integrating KGs into Language Model Inputs</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.10.10\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.10.10.1\">CoLAKE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib107\" title=\"\">107</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.10.10.2\">2020</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.10.10.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.10.10.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.10.10.4\">Integrating KGs into Language Model Inputs</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.11.11\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.11.11.1\">ERNIE3.0 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib101\" title=\"\">101</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.11.11.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.11.11.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.11.11.3.1\">E</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.11.11.3.2\">D</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.11.11.4\">Integrating KGs into Language Model Inputs</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.12.12\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.12.12.1\">DkLLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib108\" title=\"\">108</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.12.12.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.12.12.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.12.12.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.12.12.4\">Integrating KGs into Language Model Inputs</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.13.13\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.13.13.1\">KP-PLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib109\" title=\"\">109</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.13.13.2\">2022</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.13.13.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.13.13.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.13.13.4\">KGs Instruction-tuning</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.14.14\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.14.14.1\">OntoPrompt <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib110\" title=\"\">110</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.14.14.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.14.14.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.14.14.3.1\">E</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.14.14.3.2\">D</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.14.14.4\">KGs Instruction-tuning</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.15.15\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.15.15.1\">ChatKBQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib111\" title=\"\">111</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.15.15.2\">2023</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.15.15.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.15.15.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.15.15.4\">KGs Instruction-tuning</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.16.16\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.16.16.1\">RoG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib112\" title=\"\">112</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.16.16.2\">2023</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.16.16.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.16.16.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.16.16.4\">KGs Instruction-tuning</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.17.17\">\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_8\" id=\"S4.T2.1.1.1.17.17.1\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.17.17.1.1\">KG-enhanced LLM inference</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.17.17.2\">KGLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib113\" title=\"\">113</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.17.17.3\">2019</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.17.17.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.17.17.4.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.17.17.5\">Retrival-augmented knowledge fusion</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.18.18\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.18.18.1\">REALM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib114\" title=\"\">114</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.18.18.2\">2020</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.18.18.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.18.18.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.18.18.4\">Retrival-augmented knowledge fusion</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.19.19\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.19.19.1\">RAG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib92\" title=\"\">92</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.19.19.2\">2020</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.19.19.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.19.19.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.19.19.4\">Retrival-augmented knowledge fusion</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.20.20\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.20.20.1\">EMAT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib115\" title=\"\">115</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.20.20.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.20.20.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.20.20.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.20.20.4\">Retrival-augmented knowledge fusion</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.21.21\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.21.21.1\">Li et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib64\" title=\"\">64</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.21.21.2\">2023</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.21.21.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.21.21.3.1\">C</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.21.21.4\">KGs Prompting</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.22.22\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.22.22.1\">Mindmap <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib65\" title=\"\">65</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.22.22.2\">2023</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.22.22.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.22.22.3.1\">E</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.22.22.3.2\">D</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.22.22.4\">KGs Prompting</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.23.23\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.23.23.1\">ChatRule <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib116\" title=\"\">116</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.23.23.2\">2023</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.23.23.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.23.23.3.1\">E</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.23.23.3.2\">D</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.23.23.4\">KGs Prompting</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.24.24\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.24.24.1\">CoK <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib117\" title=\"\">117</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.24.24.2\">2023</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.24.24.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.24.24.3.1\">E</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.24.24.3.2\">C</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.24.24.3.3\">D</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.24.24.4\">KGs Prompting</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.25.25\">\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_10\" id=\"S4.T2.1.1.1.25.25.1\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.25.25.1.1\">KG-enhanced LLM interpretability</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.25.25.2\">LAMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.25.25.3\">2019</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.25.25.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.25.25.4.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.25.25.5\">KGs for LLM probing</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.26.26\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.26.26.1\">LPAQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib118\" title=\"\">118</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.26.26.2\">2020</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.26.26.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.26.26.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.26.26.4\">KGs for LLM probing</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.27.27\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.27.27.1\">Autoprompt <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib119\" title=\"\">119</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.27.27.2\">2020</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.27.27.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.27.27.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.27.27.4\">KGs for LLM probing</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.28.28\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.28.28.1\">MedLAMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib120\" title=\"\">120</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.28.28.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.28.28.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.28.28.3.1\">D</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.28.28.4\">KGs for LLM probing</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.29.29\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.29.29.1\">LLM-facteval <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib121\" title=\"\">121</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.29.29.2\">2023</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.29.29.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.29.29.3.1\">E</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.29.29.3.2\">D</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.29.29.4\">KGs for LLM probing</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.30.30\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.30.30.1\">KagNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib38\" title=\"\">38</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.30.30.2\">2019</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.30.30.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.30.30.3.1\">C</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.1.1.30.30.4\">KGs for LLM analysis</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.31.31\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.31.31.1\">Interpret-lm <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib122\" title=\"\">122</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.31.31.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.31.31.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.31.31.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.31.31.4\">KGs for LLM analysis</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.32.32\">\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.32.32.1\">knowledge-neurons <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib39\" title=\"\">39</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.32.32.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.32.32.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.32.32.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.1.1.32.32.4\">KGs for LLM analysis</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.1.1.1.33.33\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T2.1.1.1.33.33.1\">Shaobo et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib123\" title=\"\">123</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T2.1.1.1.33.33.2\">2022</span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T2.1.1.1.33.33.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.33.33.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T2.1.1.1.33.33.4\">KGs for LLM analysis</span></span>\n</span>\n</span></p>\n<ul class=\"ltx_itemize\" id=\"S4.I1\">\n<li class=\"ltx_item\" id=\"S4.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S4.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S4.I1.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.I1.i1.p1.1.1\">E</span>: Encyclopedic Knowledge Graphs, <span class=\"ltx_text ltx_font_bold\" id=\"S4.I1.i1.p1.1.2\">C</span>: Commonsense Knowledge Graphs, <span class=\"ltx_text ltx_font_bold\" id=\"S4.I1.i1.p1.1.3\">D</span>: Domain-Specific Knowledge Graphs.</p>\n</div>\n</li>\n</ul>\n</span></div>\n</figure>\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span><span class=\"ltx_text ltx_font_italic\" id=\"S4.SS1.1.1\">KG-enhanced LLM Pre-training</span>\n</h3>\n<div class=\"ltx_para\" id=\"S4.SS1.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.p1.1\">Existing large language models mostly rely on unsupervised training on the large-scale corpus. While these models may exhibit impressive performance on downstream tasks, they often lack practical knowledge relevant to the real world. Previous works that integrate KGs into large language models can be categorized into three parts: <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS1.p1.1.1\">1) Integrating KGs into training objective</em>, <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS1.p1.1.2\">2) Integrating KGs into LLM inputs</em>, and <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS1.p1.1.3\">3) KGs Instruction-tuning</em>.</p>\n</div>\n<section class=\"ltx_subsubsection\" id=\"S4.SS1.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">4.1.1 </span>Integrating KGs into Training Objective</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS1.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS1.p1.1\">The research efforts in this category focus on designing novel knowledge-aware training objectives. An intuitive idea is to expose more knowledge entities in the pre-training objective. GLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib102\" title=\"\">102</a>]</cite> leverages the knowledge graph structure to assign a masking probability. Specifically, entities that can be reached within a certain number of hops are considered to be the most important entities for learning, and they are given a higher masking probability during pre-training. Furthermore, E-BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib103\" title=\"\">103</a>]</cite> further controls the balance between the token-level and entity-level training losses.\nThe training loss values are used as indications of the learning process for token and entity, which dynamically determines their ratio for the next training epochs. SKEP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib124\" title=\"\">124</a>]</cite> also follows a similar fusion to inject sentiment knowledge during LLMs pre-training. SKEP first determines words with positive and negative sentiment by utilizing PMI along with a predefined set of seed sentiment words. Then, it assigns a higher masking probability to those identified sentiment words in the word masking objective.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS1.p2\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS1.p2.1\">The other line of work explicitly leverages the connections with knowledge and input text. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.F9\" title=\"Figure 9 ‣ 4.1.1 Integrating KGs into Training Objective ‣ 4.1 KG-enhanced LLM Pre-training ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, ERNIE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib35\" title=\"\">35</a>]</cite> proposes a novel word-entity alignment training objective as a pre-training objective. Specifically, ERNIE feeds both sentences and corresponding entities mentioned in the text into LLMs, and then trains the LLMs to predict alignment links between textual tokens and entities in knowledge graphs. Similarly, KALM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib91\" title=\"\">91</a>]</cite> enhances the input tokens by incorporating entity embeddings and includes an entity prediction pre-training task in addition to the token-only pre-training objective. This approach aims to improve the ability of LLMs to capture knowledge related to entities. Finally, KEPLER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite> directly employs both knowledge graph embedding training objective and Masked token pre-training objective into a shared transformer-based encoder. Deterministic LLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib104\" title=\"\">104</a>]</cite> focuses on pre-training language models to capture <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS1.SSS1.p2.1.1\">deterministic</em> factual knowledge. It only masks the span that has a deterministic entity as the question and introduces additional clue contrast learning and clue classification objective. WKLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib106\" title=\"\">106</a>]</cite> first replaces entities in the text with other same-type entities and then feeds them into LLMs. The model is further pre-trained to distinguish whether the entities have been replaced or not.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F9\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"377\" id=\"S4.F9.g1\" src=\"x8.png\" width=\"664\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 9: </span>Injecting KG information into LLMs training objective via text-knowledge alignment loss, where <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S4.F9.2.m1.1\"><semantics id=\"S4.F9.2.m1.1b\"><mi id=\"S4.F9.2.m1.1.1\" xref=\"S4.F9.2.m1.1.1.cmml\">h</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.F9.2.m1.1c\"><ci id=\"S4.F9.2.m1.1.1.cmml\" xref=\"S4.F9.2.m1.1.1\">ℎ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F9.2.m1.1d\">h</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.F9.2.m1.1e\">italic_h</annotation></semantics></math> denotes the hidden representation generated by LLMs.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S4.SS1.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">4.1.2 </span>Integrating KGs into LLM Inputs</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS2.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS2.p1.1\">As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.F10\" title=\"Figure 10 ‣ 4.1.2 Integrating KGs into LLM Inputs ‣ 4.1 KG-enhanced LLM Pre-training ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, this kind of research focus on introducing relevant knowledge sub-graph into the inputs of LLMs. Given a knowledge graph triple and the corresponding sentences, ERNIE 3.0 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib101\" title=\"\">101</a>]</cite> represents the triple as a sequence of tokens and directly concatenates them with the sentences. It further randomly masks either the relation token in the triple or tokens in the sentences\nto better combine knowledge with textual representations. However, such\ndirect knowledge triple concatenation method allows the tokens in the sentence to intensively interact with the tokens in the knowledge sub-graph, which could result in <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS1.SSS2.p1.1.1\">Knowledge Noise</em> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib36\" title=\"\">36</a>]</cite>. To solve this issue, K-BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib36\" title=\"\">36</a>]</cite> takes the first step to inject the knowledge triple into the sentence via a <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS1.SSS2.p1.1.2\">visible matrix</em> where only the knowledge entities have access to the knowledge triple information, while the tokens in the sentences can only see each other in the self-attention module. To further reduce <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS1.SSS2.p1.1.3\">Knowledge Noise</em>, Colake <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib107\" title=\"\">107</a>]</cite> proposes a unified word-knowledge graph (shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.F10\" title=\"Figure 10 ‣ 4.1.2 Integrating KGs into LLM Inputs ‣ 4.1 KG-enhanced LLM Pre-training ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>) where the tokens in the input sentences form a fully connected word graph where tokens aligned with knowledge entities are connected with their neighboring entities.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS2.p2\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS2.p2.1\">The above methods can indeed inject a large amount of knowledge into LLMs. However, they mostly focus on popular entities and overlook the low-frequent and long-tail ones. DkLLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib108\" title=\"\">108</a>]</cite> aims to improve the LLMs representations towards those entities. DkLLM first proposes a novel measurement to determine long-tail entities and then replaces these selected entities in the text with pseudo token embedding as new input to the large language models. Furthermore, Dict-BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib125\" title=\"\">125</a>]</cite> proposes to leverage external dictionaries to solve this issue. Specifically, Dict-BERT improves the representation quality of rare words by appending their definitions from the dictionary at the end of input text and trains the language model to locally align rare word representations in input sentences and dictionary definitions as well as to discriminate whether the input text and definition are correctly mapped.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F10\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"574\" id=\"S4.F10.g1\" src=\"x9.png\" width=\"581\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 10: </span>Injecting KG information into LLMs inputs using graph structure.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S4.SS1.SSS3\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">4.1.3 </span>KGs Instruction-tuning</h4>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS3.p1\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS3.p1.1\">Instead of injecting factual knowledge into LLMs, the KGs Instruction-tuning aims to fine-tune LLMs to better comprehend the structure of KGs and effectively follow user instructions to conduct complex tasks. KGs Instruction-tuning utilizes both facts and the structure of KGs to create instruction-tuning datasets. LLMs finetuned on these datasets can extract both factual and structural knowledge from KGs, enhancing the reasoning ability of LLMs.\nKP-PLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib109\" title=\"\">109</a>]</cite> first designs several prompt templates to transfer structural graphs into natural language text. Then, two self-supervised tasks are proposed to finetune LLMs to further leverage the knowledge from these prompts. OntoPrompt <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib110\" title=\"\">110</a>]</cite> proposes an ontology-enhanced prompt-tuning that can place knowledge of entities into the context of LLMs, which are further finetuned on several downstream tasks. ChatKBQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib111\" title=\"\">111</a>]</cite> finetunes LLMs on KG structure to generate logical queries, which can be executed on KGs to obtain answers. To better reason on graphs, RoG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib112\" title=\"\">112</a>]</cite> presents a planning-retrieval-reasoning framework. RoG is finetuned on KG structure to generate relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning and generate interpretable results.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.SSS3.p2\">\n<p class=\"ltx_p\" id=\"S4.SS1.SSS3.p2.1\">KGs Instruction-tuning can better leverage the knowledge from KGs for downstream tasks. However, it requires retraining the models, which is time-consuming and requires lots of resources.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span><span class=\"ltx_text ltx_font_italic\" id=\"S4.SS2.1.1\">KG-enhanced LLM Inference</span>\n</h3>\n<div class=\"ltx_para\" id=\"S4.SS2.p1\">\n<p class=\"ltx_p\" id=\"S4.SS2.p1.1\">The above methods could effectively fuse knowledge into LLMs. However, real-world knowledge is subject to change and the limitation of these approaches is that they do not permit updates to the incorporated knowledge without retraining the model. As a result, they may not generalize well to the unseen knowledge during inference <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib126\" title=\"\">126</a>]</cite>. Therefore, considerable research has been devoted to keeping the knowledge space and text space separate and injecting the knowledge while inference. These methods mostly focus on the Question Answering (QA) tasks, because QA requires the model to capture both textual semantic meanings and up-to-date real-world knowledge.</p>\n</div>\n<section class=\"ltx_subsubsection\" id=\"S4.SS2.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">4.2.1 </span>Retrieval-Augmented Knowledge Fusion</h4>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS1.p1\">\n<p class=\"ltx_p\" id=\"S4.SS2.SSS1.p1.1\">Retrieval-Augmented Knowledge Fusion is a popular method to inject knowledge into LLMs during inference. The key idea is to retrieve relevant knowledge from a large corpus and then fuse the retrieved knowledge into LLMs. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.F11\" title=\"Figure 11 ‣ 4.2.1 Retrieval-Augmented Knowledge Fusion ‣ 4.2 KG-enhanced LLM Inference ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, RAG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib92\" title=\"\">92</a>]</cite> proposes to combine non-parametric and parametric modules to handle the external knowledge. Given the input text, RAG first searches for relevant KG in the non-parametric module via MIPS to obtain several documents. RAG then treats these documents as hidden variables <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS1.p1.1.m1.1\"><semantics id=\"S4.SS2.SSS1.p1.1.m1.1a\"><mi id=\"S4.SS2.SSS1.p1.1.m1.1.1\" xref=\"S4.SS2.SSS1.p1.1.m1.1.1.cmml\">z</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.SSS1.p1.1.m1.1b\"><ci id=\"S4.SS2.SSS1.p1.1.m1.1.1.cmml\" xref=\"S4.SS2.SSS1.p1.1.m1.1.1\">𝑧</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.SSS1.p1.1.m1.1c\">z</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS2.SSS1.p1.1.m1.1d\">italic_z</annotation></semantics></math> and feeds them into the output generator, empowered by Seq2Seq LLMs, as additional context information. The research indicates that using different retrieved documents as conditions at different generation steps performs better than only using a single document to guide the whole generation process.\nThe experimental results show that RAG outperforms other parametric-only and non-parametric-only baseline models in open-domain QA. RAG can also generate more specific, diverse, and factual text than other parameter-only baselines. Story-fragments <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib127\" title=\"\">127</a>]</cite> further improves architecture by adding an additional module to determine salient knowledge entities and fuse them into the generator to improve the quality of generated long stories. EMAT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib115\" title=\"\">115</a>]</cite> further improves the efficiency of such a system by encoding external knowledge into a key-value memory and exploiting the\nfast maximum inner product search for memory querying. REALM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib114\" title=\"\">114</a>]</cite> proposes a novel knowledge retriever to help the model to retrieve and attend over documents from a large corpus during the pre-training stage and successfully improves the performance of open-domain question answering. KGLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib113\" title=\"\">113</a>]</cite> selects the facts from a knowledge graph using the current context to generate factual sentences. With the help of an external knowledge graph, KGLM could describe facts using out-of-domain words or phrases.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F11\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"339\" id=\"S4.F11.g1\" src=\"x10.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 11: </span>Retrieving external knowledge to enhance the LLM generation.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S4.SS2.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">4.2.2 </span>KGs Prompting</h4>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS2.p1\">\n<p class=\"ltx_p\" id=\"S4.SS2.SSS2.p1.1\">To better feed the KG structure into the LLM during inference, KGs prompting aims to design a crafted prompt that converts structured KGs into text sequences, which can be fed as context into LLMs. In this way, LLMs can better take advantage of the structure of KGs to perform reasoning. Li et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib64\" title=\"\">64</a>]</cite> adopt the pre-defined template to convert each triple into a short sentence, which can be understood by LLMs for reasoning. Mindmap <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib65\" title=\"\">65</a>]</cite> designs a KG prompt to convert graph structure into a mind map that enables LLMs to perform reasoning by consolidating the facts in KGs and the implicit knowledge from LLMs.\nChatRule <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib116\" title=\"\">116</a>]</cite> samples several relation paths from KGs, which are verbalized and fed into LLMs. Then, LLMs are prompted to generate meaningful logical rules that can be used for reasoning. CoK <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib117\" title=\"\">117</a>]</cite> proposes a chain-of-knowledge prompting that uses a sequence of triples to elicit the reasoning ability of LLMs to reach the final answer.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS2.SSS2.p2\">\n<p class=\"ltx_p\" id=\"S4.SS2.SSS2.p2.1\">KGs prompting presents a simple way to synergize LLMs and KGs. By using the prompt, we can easily harness the power of LLMs to perform reasoning based on KGs without retraining the models. However, the prompt is usually designed manually, which requires lots of human effort.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span><span class=\"ltx_text ltx_font_italic\" id=\"S4.SS3.1.1\">Comparison between KG-enhanced LLM Pre-training and Inference</span>\n</h3>\n<div class=\"ltx_para\" id=\"S4.SS3.p1\">\n<p class=\"ltx_p\" id=\"S4.SS3.p1.1\">KG-enhanced LLM Pre-training methods commonly enrich large-amount of unlabeled corpus with semantically relevant real-world knowledge. These methods allow the knowledge representations to be aligned with appropriate linguistic context and explicitly train LLMs to leverage those knowledge from scratch. When applying the resulting LLMs to downstream knowledge-intensive tasks, they should achieve optimal performance. In contrast, KG-enhanced LLM inference methods only present the knowledge to LLMs in the inference stage and the underlying LLMs may not be trained to fully leverage these knowledge when conducting downstream tasks, potentially resulting in sub-optimal model performance.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS3.p2\">\n<p class=\"ltx_p\" id=\"S4.SS3.p2.1\">However, real-world knowledge is dynamic and requires frequent updates. Despite being effective, the KG-enhanced LLM Pre-training methods never permit knowledge updates or editing without model re-training. As a result, the KG-enhanced LLM Pre-training methods could generalize poorly to recent or unseen knowledge. KG-enhanced LLM inference methods can easily maintain knowledge updates by changing the inference inputs. These methods help improve LLMs performance on new knowledge and domains.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS3.p3\">\n<p class=\"ltx_p\" id=\"S4.SS3.p3.1\">In summary, when to use these methods depends on the application scenarios. If one wishes to apply LLMs to handle time-insensitive knowledge in particular domains (e.g., commonsense and reasoning knowledge), KG-enhanced LLM Pre-training methods should be considered. Otherwise, KG-enhanced LLM inference methods can be used to handle open-domain knowledge with frequent updates.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F12\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"241\" id=\"S4.F12.g1\" src=\"extracted/5367551/figs/LLM_probing.png\" width=\"419\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 12: </span>The general framework of using knowledge graph for language model probing.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.4 </span><span class=\"ltx_text ltx_font_italic\" id=\"S4.SS4.1.1\">KG-enhanced LLM Interpretability</span>\n</h3>\n<div class=\"ltx_para\" id=\"S4.SS4.p1\">\n<p class=\"ltx_p\" id=\"S4.SS4.p1.1\">Although LLMs have achieved remarkable success in many NLP tasks, they are still criticized for their lack of interpretability. The large language model (LLM) interpretability refers to the understanding and explanation of the inner workings and decision-making processes of a large language model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib17\" title=\"\">17</a>]</cite>. This can improve the trustworthiness of LLMs and facilitate their applications in high-stakes scenarios such as medical diagnosis and legal judgment.\nKnowledge graphs (KGs) represent the knowledge structurally and can provide good interpretability for the reasoning results. Therefore, researchers try to utilize KGs to improve the interpretability of LLMs, which can be roughly grouped into two categories: <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS4.p1.1.1\">1) KGs for language model probing</em>, and <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS4.p1.1.2\">2) KGs for language model analysis</em>.</p>\n</div>\n<section class=\"ltx_subsubsection\" id=\"S4.SS4.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">4.4.1 </span>KGs for LLM Probing</h4>\n<div class=\"ltx_para\" id=\"S4.SS4.SSS1.p1\">\n<p class=\"ltx_p\" id=\"S4.SS4.SSS1.p1.1\">The large language model (LLM) probing aims to understand the knowledge stored in LLMs. LLMs, trained on large-scale corpus, are often known as containing enormous knowledge. However, LLMs store the knowledge in a hidden way, making it hard to figure out the stored knowledge. Moreover, LLMs suffer from the hallucination problem <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib15\" title=\"\">15</a>]</cite>, which results in generating statements that contradict facts. This issue significantly affects the reliability of LLMs. Therefore, it is necessary to probe and verify the knowledge stored in LLMs.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS4.SSS1.p2\">\n<p class=\"ltx_p\" id=\"S4.SS4.SSS1.p2.1\">LAMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a>]</cite> is the first work to probe the knowledge in LLMs by using KGs. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.F12\" title=\"Figure 12 ‣ 4.3 Comparison between KG-enhanced LLM Pre-training and Inference ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, LAMA first converts the facts in KGs into cloze statements by a pre-defined prompt template and then uses LLMs to predict the missing entity. The prediction results are used to evaluate the knowledge stored in LLMs. For example, we try to probe whether LLMs know the fact <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS4.SSS1.p2.1.1\">(Obama, profession, president)</span>. We first convert the fact triple into a cloze question “Obama’s profession is <math alttext=\"\\_\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS1.p2.1.m1.1\"><semantics id=\"S4.SS4.SSS1.p2.1.m1.1a\"><mi id=\"S4.SS4.SSS1.p2.1.m1.1.1\" mathvariant=\"normal\" xref=\"S4.SS4.SSS1.p2.1.m1.1.1.cmml\">_</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.SSS1.p2.1.m1.1b\"><ci id=\"S4.SS4.SSS1.p2.1.m1.1.1.cmml\" xref=\"S4.SS4.SSS1.p2.1.m1.1.1\">_</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.SSS1.p2.1.m1.1c\">\\_</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.SSS1.p2.1.m1.1d\">_</annotation></semantics></math>.” with the object masked. Then, we test if the LLMs can predict the object “president” correctly.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS4.SSS1.p3\">\n<p class=\"ltx_p\" id=\"S4.SS4.SSS1.p3.1\">However, LAMA ignores the fact that the prompts are inappropriate. For example, the prompt <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS4.SSS1.p3.1.1\">“Obama worked as a _”</span> may be more favorable to the prediction of the blank by the language models than <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS4.SSS1.p3.1.2\">“Obama is a _ by profession”</span>. Thus, LPAQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib118\" title=\"\">118</a>]</cite> proposes a mining and paraphrasing-based method to automatically generate high-quality and diverse prompts for a more accurate assessment of the knowledge contained in the language model. Moreover, Adolphs et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib128\" title=\"\">128</a>]</cite> attempt to use examples to make the language model understand the query, and experiments obtain substantial improvements for BERT-large on the T-REx data. Unlike using manually defined prompt templates, Autoprompt <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib119\" title=\"\">119</a>]</cite> proposes an automated method, which is based on the gradient-guided search to create prompts. LLM-facteval <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib121\" title=\"\">121</a>]</cite> designs a systematic framework that automatically generates probing questions from KGs. The generated questions are then used to evaluate the factual knowledge stored in LLMs.</p>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS4.SSS1.p4\">\n<p class=\"ltx_p\" id=\"S4.SS4.SSS1.p4.1\">Instead of probing the general knowledge by using the encyclopedic and commonsense knowledge graphs, BioLAMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib129\" title=\"\">129</a>]</cite> and MedLAMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib120\" title=\"\">120</a>]</cite> probe the medical knowledge in LLMs by using medical knowledge graphs.\nAlex et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib130\" title=\"\">130</a>]</cite> investigate the capacity of LLMs to retain less popular factual knowledge. They select unpopular facts from Wikidata knowledge graphs which have low-frequency clicked entities. These facts are then used for the evaluation, where the results indicate that LLMs encounter difficulties with such knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the tail.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F13\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"280\" id=\"S4.F13.g1\" src=\"extracted/5367551/figs/LLM_analysis.png\" width=\"419\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 13: </span>The general framework of using knowledge graph for language model analysis.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S4.SS4.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">4.4.2 </span>KGs for LLM Analysis</h4>\n<div class=\"ltx_para\" id=\"S4.SS4.SSS2.p1\">\n<p class=\"ltx_p\" id=\"S4.SS4.SSS2.p1.1\">Knowledge graphs (KGs) for pre-train language models (LLMs) analysis aims to answer the following questions such as “how do LLMs generate the results?”, and “how do the function and structure work in LLMs?”.\nTo analyze the inference process of LLMs, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4.F13\" title=\"Figure 13 ‣ 4.4.1 KGs for LLM Probing ‣ 4.4 KG-enhanced LLM Interpretability ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, KagNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib38\" title=\"\">38</a>]</cite> and QA-GNN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib131\" title=\"\">131</a>]</cite> make the results generated by LLMs at each reasoning step grounded by knowledge graphs. In this way, the reasoning process of LLMs can be explained by extracting the graph structure from KGs. Shaobo et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib123\" title=\"\">123</a>]</cite> investigate how LLMs generate the results correctly. They adopt the causal-inspired analysis from facts extracted from KGs. This analysis quantitatively measures the word patterns that LLMs depend on to generate the results. The results show that LLMs generate the missing factual more by the positionally closed words rather than the knowledge-dependent words. Thus, they claim that LLMs are inadequate to memorize factual knowledge because of the inaccurate dependence.\nTo interpret the training of LLMs, Swamy et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib122\" title=\"\">122</a>]</cite> adopt the language model during pre-training to generate knowledge graphs. The knowledge acquired by LLMs during training can be unveiled by the facts in KGs explicitly. To explore how implicit knowledge is stored in parameters of LLMs, Dai et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib39\" title=\"\">39</a>]</cite> propose the concept of <em class=\"ltx_emph ltx_font_italic\" id=\"S4.SS4.SSS2.p1.1.1\">knowledge neurons</em>. Specifically, activation of the identified knowledge neurons is highly correlated with knowledge expression. Thus, they explore the knowledge and facts represented by each neuron by suppressing and amplifying knowledge neurons.</p>\n</div>\n</section>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.1.1\">LLM-augmented KGs</span>\n</h2>\n<div class=\"ltx_para\" id=\"S5.p1\">\n<p class=\"ltx_p\" id=\"S5.p1.1\">Knowledge graphs are famous for representing knowledge in a structural manner. They have been applied in many downstream tasks such as question answering, recommendation, and web search. However, the conventional KGs are often incomplete and existing methods often lack considering textual information. To address these issues, recent research has explored integrating LLMs to augment KGs to consider the textual information and improve the performance in downstream tasks. In this section, we will introduce the recent research on LLM-augmented KGs. We will introduce the methods that integrate LLMs for KG embedding, KG completion, KG construction, KG-to-text generation, and KG question answering, respectively. Representative works are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.T3\" title=\"TABLE III ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S5.T3\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE III: </span>Summary of representative LLM-augmented KG methods.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S5.T3.1\" style=\"width:433.6pt;height:705.7pt;vertical-align:-8.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-113.6pt,182.6pt) scale(0.656183983074175,0.656183983074175) ;\">\n<p class=\"ltx_p\" id=\"S5.T3.1.1\">[b]\n\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T3.1.1.1\">\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.1.1\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S5.T3.1.1.1.1.1.1\">Task</span>\n<span class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T3.1.1.1.1.1.2\">Method</span>\n<span class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T3.1.1.1.1.1.3\">Year</span>\n<span class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T3.1.1.1.1.1.4\">LLM</span>\n<span class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T3.1.1.1.1.1.5\">Technique</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.2.2\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_8\" id=\"S5.T3.1.1.1.2.2.1\"><span class=\"ltx_text\" id=\"S5.T3.1.1.1.2.2.1.1\">LLM-augmented KG embedding</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.2.2.2\">Pretrain-KGE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib94\" title=\"\">94</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.2.2.3\">2020</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.2.2.4.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.2.2.5\">LLMs as Text Encoders</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.3.3\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.3.3.1\">KEPLER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.3.3.2\">2020</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.3.3.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.3.3.4\">LLMs as Text Encoders</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.4.4\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.4.4.1\">Nayyeri et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib132\" title=\"\">132</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.4.4.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.4.4.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.4.4.4\">LLMs as Text Encoders</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.5.5\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.5.5.1\">Huang et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib133\" title=\"\">133</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.5.5.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.5.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.5.5.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.5.5.4\">LLMs as Text Encoders</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.6.6\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.6.6.1\">CoDEx <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib134\" title=\"\">134</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.6.6.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.6.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.6.6.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.6.6.4\">LLMs as Text Encoders</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.7.7\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.7.7.1\">LMKE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib135\" title=\"\">135</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.7.7.2\">2022</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.7.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.7.7.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.7.7.4\">LLMs for Joint Text and KG Embedding</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.8.8\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.8.8.1\">kNN-KGE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib136\" title=\"\">136</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.8.8.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.8.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.8.8.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.8.8.4\">LLMs for Joint Text and KG Embedding</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.9.9\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.9.9.1\">LambdaKG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib137\" title=\"\">137</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.9.9.2\">2023</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.9.9.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.9.9.3.1\">E</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.9.9.3.2\">D</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.9.9.3.3\">ED</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.9.9.4\">LLMs for Joint Text and KG Embedding</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.10.10\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_12\" id=\"S5.T3.1.1.1.10.10.1\"><span class=\"ltx_text\" id=\"S5.T3.1.1.1.10.10.1.1\">LLM-augmented KG completion</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.10.10.2\">KG-BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib26\" title=\"\">26</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.10.10.3\">2019</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.10.10.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.10.10.4.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.10.10.5\">Joint Encoding</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.11.11\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.11.11.1\">MTL-KGC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib138\" title=\"\">138</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.11.11.2\">2020</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.11.11.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.11.11.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.11.11.4\">Joint Encoding</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.12.12\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.12.12.1\">PKGC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib139\" title=\"\">139</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.12.12.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.12.12.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.12.12.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.12.12.4\">Joint Encoding</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.13.13\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.13.13.1\">LASS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib140\" title=\"\">140</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.13.13.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.13.13.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.13.13.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.13.13.4\">Joint Encoding</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.14.14\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.14.14.1\">MEM-KGC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib141\" title=\"\">141</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.14.14.2\">2021</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.14.14.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.14.14.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.14.14.4\">MLM Encoding</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.15.15\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.15.15.1\">OpenWorld KGC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib142\" title=\"\">142</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.15.15.2\">2023</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.15.15.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.15.15.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.15.15.4\">MLM Encoding</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.16.16\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.16.16.1\">StAR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib143\" title=\"\">143</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.16.16.2\">2021</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.16.16.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.16.16.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.16.16.4\">Separated Encoding</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.17.17\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.17.17.1\">SimKGC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib144\" title=\"\">144</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.17.17.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.17.17.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.17.17.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.17.17.4\">Separated Encoding</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.18.18\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.18.18.1\">LP-BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib145\" title=\"\">145</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.18.18.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.18.18.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.18.18.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.18.18.4\">Separated Encoding</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.19.19\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.19.19.1\">GenKGC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib96\" title=\"\">96</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.19.19.2\">2022</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.19.19.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.19.19.3.1\">ED</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.19.19.4\">LLM as decoders</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.20.20\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.20.20.1\">KGT5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib146\" title=\"\">146</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.20.20.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.20.20.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.20.20.3.1\">ED</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.20.20.4\">LLM as decoders</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.21.21\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.21.21.1\">KG-S2S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib147\" title=\"\">147</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.21.21.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.21.21.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.21.21.3.1\">ED</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.21.21.4\">LLM as decoders</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.22.22\">\n<span class=\"ltx_td ltx_border_r\" id=\"S5.T3.1.1.1.22.22.1\"></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.22.22.2\">AutoKG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib93\" title=\"\">93</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.22.22.3\">2023</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.22.22.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.22.22.4.1\">D</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.22.22.5\">LLM as decoders</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.23.23\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_18\" id=\"S5.T3.1.1.1.23.23.1\"><span class=\"ltx_text\" id=\"S5.T3.1.1.1.23.23.1.1\">LLM-augmented KG construction</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.23.23.2\">ELMO <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib148\" title=\"\">148</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.23.23.3\">2018</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.23.23.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.23.23.4.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.23.23.5\">Named Entity Recognition</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.24.24\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.24.24.1\">GenerativeNER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib149\" title=\"\">149</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.24.24.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.24.24.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.24.24.3.1\">ED</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.24.24.4\">Named Entity Recognition</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.25.25\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.25.25.1\">LDET <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib150\" title=\"\">150</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.25.25.2\">2019</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.25.25.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.25.25.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.25.25.4\">Entity Typing</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.26.26\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.26.26.1\">BOX4Types <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib151\" title=\"\">151</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.26.26.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.26.26.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.26.26.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.26.26.4\">Entity Typing</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.27.27\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.27.27.1\">ELQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib152\" title=\"\">152</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.27.27.2\">2020</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.27.27.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.27.27.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.27.27.4\">Entity Linking</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.28.28\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.28.28.1\">ReFinED <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib153\" title=\"\">153</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.28.28.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.28.28.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.28.28.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.28.28.4\">Entity Linking</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.29.29\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.29.29.1\">BertCR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib154\" title=\"\">154</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.29.29.2\">2019</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.29.29.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.29.29.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.29.29.4\">CR (Within-document)</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.30.30\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.30.30.1\">Spanbert <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib155\" title=\"\">155</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.30.30.2\">2020</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.30.30.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.30.30.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.30.30.4\">CR (Within-document)</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.31.31\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.31.31.1\">CDLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib156\" title=\"\">156</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.31.31.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.31.31.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.31.31.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.31.31.4\">CR (Cross-document)</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.32.32\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.32.32.1\">CrossCR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib157\" title=\"\">157</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.32.32.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.32.32.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.32.32.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.32.32.4\">CR (Cross-document)</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.33.33\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.33.33.1\">CR-RL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib158\" title=\"\">158</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.33.33.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.33.33.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.33.33.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.33.33.4\">CR (Cross-document)</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.34.34\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.34.34.1\">SentRE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib159\" title=\"\">159</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.34.34.2\">2019</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.34.34.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.34.34.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.34.34.4\">RE (Sentence-level)</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.35.35\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.35.35.1\">Curriculum-RE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib160\" title=\"\">160</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.35.35.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.35.35.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.35.35.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.35.35.4\">RE (Sentence-level)</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.36.36\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.36.36.1\">DREEAM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib161\" title=\"\">161</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.36.36.2\">2023</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.36.36.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.36.36.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.36.36.4\">RE (Document-level)</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.37.37\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.37.37.1\">Kumar et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib95\" title=\"\">95</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.37.37.2\">2020</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.37.37.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.37.37.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.37.37.4\">End-to-End Construction</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.38.38\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.38.38.1\">Guo et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib162\" title=\"\">162</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.38.38.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.38.38.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.38.38.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.38.38.4\">End-to-End Construction</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.39.39\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.39.39.1\">Grapher <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib41\" title=\"\">41</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.39.39.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.39.39.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.39.39.3.1\">ED</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.39.39.4\">End-to-End Construction</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.40.40\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.40.40.1\">PiVE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib163\" title=\"\">163</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.40.40.2\">2023</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.40.40.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.40.40.3.1\">D</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.40.40.3.2\">ED</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.40.40.4\">End-to-End Construction</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.41.41\">\n<span class=\"ltx_td ltx_border_r\" id=\"S5.T3.1.1.1.41.41.1\"></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.41.41.2\">COMET <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib164\" title=\"\">164</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.41.41.3\">2019</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.41.41.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.41.41.4.1\">D</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.41.41.5\">Distilling KGs from LLMs</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.42.42\">\n<span class=\"ltx_td ltx_border_r\" id=\"S5.T3.1.1.1.42.42.1\"></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.42.42.2\">BertNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib165\" title=\"\">165</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.42.42.3\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.42.42.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.42.42.4.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.42.42.5\">Distilling KGs from LLMs</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.43.43\">\n<span class=\"ltx_td ltx_border_r\" id=\"S5.T3.1.1.1.43.43.1\"></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.43.43.2\">West et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib166\" title=\"\">166</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.43.43.3\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.43.43.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.43.43.4.1\">D</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.43.43.5\">Distilling KGs from LLMs</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.44.44\">\n<span class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_6\" id=\"S5.T3.1.1.1.44.44.1\"><span class=\"ltx_text\" id=\"S5.T3.1.1.1.44.44.1.1\">LLM-augmented KG-to-text Generation</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.44.44.2\">Ribeiro et al <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib167\" title=\"\">167</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.44.44.3\">2021</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.44.44.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.44.44.4.1\">ED</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.44.44.5\">Leveraging Knowledge from LLMs</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.45.45\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.45.45.1\">JointGT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib42\" title=\"\">42</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.45.45.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.45.45.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.45.45.3.1\">ED</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.45.45.4\">Leveraging Knowledge from LLMs</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.46.46\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.46.46.1\">FSKG2Text <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib168\" title=\"\">168</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.46.46.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.46.46.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.46.46.3.1\">D</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.46.46.3.2\">ED</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.46.46.4\">Leveraging Knowledge from LLMs</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.47.47\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.47.47.1\">GAP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib169\" title=\"\">169</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.47.47.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.47.47.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.47.47.3.1\">ED</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.47.47.4\">Leveraging Knowledge from LLMs</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.48.48\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.48.48.1\">GenWiki <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib170\" title=\"\">170</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.48.48.2\">2020</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.48.48.3\">-</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.48.48.4\">Constructing KG-text aligned Corpus</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.49.49\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.49.49.1\">KGPT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib171\" title=\"\">171</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.49.49.2\">2020</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.49.49.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.49.49.3.1\">ED</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.49.49.4\">Constructing KG-text aligned Corpus</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.50.50\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_10\" id=\"S5.T3.1.1.1.50.50.1\"><span class=\"ltx_text\" id=\"S5.T3.1.1.1.50.50.1.1\">LLM-augmented KGQA</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.50.50.2\">Lukovnikov et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib172\" title=\"\">172</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.50.50.3\">2019</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.50.50.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.50.50.4.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.50.50.5\">Entity/Relation Extractor</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.51.51\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.51.51.1\">Luo et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib173\" title=\"\">173</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.51.51.2\">2020</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.51.51.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.51.51.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.51.51.4\">Entity/Relation Extractor</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.52.52\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.52.52.1\">QA-GNN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib131\" title=\"\">131</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.52.52.2\">2021</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.52.52.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.52.52.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.52.52.4\">Entity/Relation Extractor</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.53.53\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.53.53.1\">Nan et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib174\" title=\"\">174</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.53.53.2\">2023</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.53.53.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.53.53.3.1\">E</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.53.53.3.2\">D</span> + <span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.53.53.3.3\">ED</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.53.53.4\">Entity/Relation Extractor</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.54.54\">\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.54.54.1\">DEKCOR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib175\" title=\"\">175</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.54.54.2\">2021</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.54.54.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.54.54.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.1.1.54.54.4\">Answer Reasoner</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.55.55\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.55.55.1\">DRLK <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib176\" title=\"\">176</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.55.55.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.55.55.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.55.55.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.55.55.4\">Answer Reasoner</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.56.56\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.56.56.1\">OreoLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib177\" title=\"\">177</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.56.56.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.56.56.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.56.56.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.56.56.4\">Answer Reasoner</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.57.57\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.57.57.1\">GreaseLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib178\" title=\"\">178</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.57.57.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.57.57.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.57.57.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.57.57.4\">Answer Reasoner</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.58.58\">\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.58.58.1\">ReLMKG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib179\" title=\"\">179</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.58.58.2\">2022</span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.58.58.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.58.58.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.58.58.4\">Answer Reasoner</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.1.1.59.59\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T3.1.1.1.59.59.1\">UniKGQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib43\" title=\"\">43</a>]</cite></span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T3.1.1.1.59.59.2\">2023</span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T3.1.1.1.59.59.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.1.1.59.59.3.1\">E</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T3.1.1.1.59.59.4\">Answer Reasoner</span></span>\n</span>\n</span></p>\n<ul class=\"ltx_itemize\" id=\"S5.I1\">\n<li class=\"ltx_item\" id=\"S5.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"S5.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"S5.I1.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.I1.i1.p1.1.1\">E</span>: Encoder-only LLMs, <span class=\"ltx_text ltx_font_bold\" id=\"S5.I1.i1.p1.1.2\">D</span>: Decoder-only LLMs, <span class=\"ltx_text ltx_font_bold\" id=\"S5.I1.i1.p1.1.3\">ED</span>: Encoder-decoder LLMs.</p>\n</div>\n</li>\n</ul>\n</span></div>\n</figure>\n<section class=\"ltx_subsection\" id=\"S5.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.1 </span><span class=\"ltx_text ltx_font_italic\" id=\"S5.SS1.1.1\">LLM-augmented KG Embedding</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS1.p1\">\n<p class=\"ltx_p\" id=\"S5.SS1.p1.1\">Knowledge graph embedding (KGE) aims to map each entity and relation into a low-dimensional vector (embedding) space. These embeddings contain both semantic and structural information of KGs, which can be utilized for various tasks such as question answering <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib180\" title=\"\">180</a>]</cite>, reasoning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib38\" title=\"\">38</a>]</cite>, and recommendation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib181\" title=\"\">181</a>]</cite>. Conventional knowledge graph embedding methods mainly rely on the structural information of KGs to optimize a scoring function defined on embeddings (e.g., TransE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib33\" title=\"\">33</a>]</cite>, and DisMult <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib182\" title=\"\">182</a>]</cite>). However, these approaches often fall short in representing unseen entities and long-tailed relations due to their limited structural connectivity <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib183\" title=\"\">183</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib184\" title=\"\">184</a>]</cite>. To address this issue, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.F14\" title=\"Figure 14 ‣ 5.1 LLM-augmented KG Embedding ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, recent research adopts LLMs to enrich representations of KGs by encoding the textual descriptions of entities and relations <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib94\" title=\"\">94</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F14\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"563\" id=\"S5.F14.g1\" src=\"x11.png\" width=\"581\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 14: </span>LLMs as text encoder for knowledge graph embedding (KGE).</figcaption>\n</figure>\n<section class=\"ltx_subsubsection\" id=\"S5.SS1.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.1.1 </span>LLMs as Text Encoders</h4>\n<div class=\"ltx_para\" id=\"S5.SS1.SSS1.p1\">\n<p class=\"ltx_p\" id=\"S5.SS1.SSS1.p1.4\">Pretrain-KGE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib94\" title=\"\">94</a>]</cite> is a representative method that follows the framework shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.F14\" title=\"Figure 14 ‣ 5.1 LLM-augmented KG Embedding ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>. Given a triple <math alttext=\"(h,r,t)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.1.m1.3\"><semantics id=\"S5.SS1.SSS1.p1.1.m1.3a\"><mrow id=\"S5.SS1.SSS1.p1.1.m1.3.4.2\" xref=\"S5.SS1.SSS1.p1.1.m1.3.4.1.cmml\"><mo id=\"S5.SS1.SSS1.p1.1.m1.3.4.2.1\" stretchy=\"false\" xref=\"S5.SS1.SSS1.p1.1.m1.3.4.1.cmml\">(</mo><mi id=\"S5.SS1.SSS1.p1.1.m1.1.1\" xref=\"S5.SS1.SSS1.p1.1.m1.1.1.cmml\">h</mi><mo id=\"S5.SS1.SSS1.p1.1.m1.3.4.2.2\" xref=\"S5.SS1.SSS1.p1.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS1.SSS1.p1.1.m1.2.2\" xref=\"S5.SS1.SSS1.p1.1.m1.2.2.cmml\">r</mi><mo id=\"S5.SS1.SSS1.p1.1.m1.3.4.2.3\" xref=\"S5.SS1.SSS1.p1.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS1.SSS1.p1.1.m1.3.3\" xref=\"S5.SS1.SSS1.p1.1.m1.3.3.cmml\">t</mi><mo id=\"S5.SS1.SSS1.p1.1.m1.3.4.2.4\" stretchy=\"false\" xref=\"S5.SS1.SSS1.p1.1.m1.3.4.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.1.m1.3b\"><vector id=\"S5.SS1.SSS1.p1.1.m1.3.4.1.cmml\" xref=\"S5.SS1.SSS1.p1.1.m1.3.4.2\"><ci id=\"S5.SS1.SSS1.p1.1.m1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.1.m1.1.1\">ℎ</ci><ci id=\"S5.SS1.SSS1.p1.1.m1.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.1.m1.2.2\">𝑟</ci><ci id=\"S5.SS1.SSS1.p1.1.m1.3.3.cmml\" xref=\"S5.SS1.SSS1.p1.1.m1.3.3\">𝑡</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.1.m1.3c\">(h,r,t)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.1.m1.3d\">( italic_h , italic_r , italic_t )</annotation></semantics></math> from KGs, it firsts uses a LLM encoder to encode the textual descriptions of entities <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.2.m2.1\"><semantics id=\"S5.SS1.SSS1.p1.2.m2.1a\"><mi id=\"S5.SS1.SSS1.p1.2.m2.1.1\" xref=\"S5.SS1.SSS1.p1.2.m2.1.1.cmml\">h</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.2.m2.1b\"><ci id=\"S5.SS1.SSS1.p1.2.m2.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.2.m2.1.1\">ℎ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.2.m2.1c\">h</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.2.m2.1d\">italic_h</annotation></semantics></math>, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.3.m3.1\"><semantics id=\"S5.SS1.SSS1.p1.3.m3.1a\"><mi id=\"S5.SS1.SSS1.p1.3.m3.1.1\" xref=\"S5.SS1.SSS1.p1.3.m3.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.3.m3.1b\"><ci id=\"S5.SS1.SSS1.p1.3.m3.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.3.m3.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.3.m3.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.3.m3.1d\">italic_t</annotation></semantics></math>, and relations <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.4.m4.1\"><semantics id=\"S5.SS1.SSS1.p1.4.m4.1a\"><mi id=\"S5.SS1.SSS1.p1.4.m4.1.1\" xref=\"S5.SS1.SSS1.p1.4.m4.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.4.m4.1b\"><ci id=\"S5.SS1.SSS1.p1.4.m4.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.4.m4.1.1\">𝑟</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.4.m4.1c\">r</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.4.m4.1d\">italic_r</annotation></semantics></math> into representations as</p>\n<table class=\"ltx_equationgroup ltx_eqn_gather ltx_eqn_table\" id=\"A1.EGx1\">\n<tbody id=\"S5.E1\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\displaystyle e_{h}=\\text{LLM}(\\text{Text}_{h}),e_{t}=\\text{LLM}(\\text{Text}_{%\nt}),e_{r}=\\text{LLM}(\\text{Text}_{r}),\" class=\"ltx_Math\" display=\"block\" id=\"S5.E1.m1.1\"><semantics id=\"S5.E1.m1.1a\"><mrow id=\"S5.E1.m1.1.1.1\"><mrow id=\"S5.E1.m1.1.1.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.3.cmml\"><mrow id=\"S5.E1.m1.1.1.1.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.1.1.cmml\"><msub id=\"S5.E1.m1.1.1.1.1.1.1.3\" xref=\"S5.E1.m1.1.1.1.1.1.1.3.cmml\"><mi id=\"S5.E1.m1.1.1.1.1.1.1.3.2\" xref=\"S5.E1.m1.1.1.1.1.1.1.3.2.cmml\">e</mi><mi id=\"S5.E1.m1.1.1.1.1.1.1.3.3\" xref=\"S5.E1.m1.1.1.1.1.1.1.3.3.cmml\">h</mi></msub><mo id=\"S5.E1.m1.1.1.1.1.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.1.1.2.cmml\">=</mo><mrow id=\"S5.E1.m1.1.1.1.1.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.cmml\"><mtext id=\"S5.E1.m1.1.1.1.1.1.1.1.3\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.3a.cmml\">LLM</mtext><mo id=\"S5.E1.m1.1.1.1.1.1.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml\">(</mo><msub id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml\"><mtext id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.2a.cmml\">Text</mtext><mi id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.3\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml\">h</mi></msub><mo id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E1.m1.1.1.1.1.2.3\" xref=\"S5.E1.m1.1.1.1.1.3a.cmml\">,</mo><mrow id=\"S5.E1.m1.1.1.1.1.2.2.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.3.cmml\"><mrow id=\"S5.E1.m1.1.1.1.1.2.2.1.1\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.cmml\"><msub id=\"S5.E1.m1.1.1.1.1.2.2.1.1.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.cmml\"><mi id=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.2.cmml\">e</mi><mi id=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.3.cmml\">t</mi></msub><mo id=\"S5.E1.m1.1.1.1.1.2.2.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.2.cmml\">=</mo><mrow id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.cmml\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.3a.cmml\">LLM</mtext><mo id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml\"><mo id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml\">(</mo><msub id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.2a.cmml\">Text</mtext><mi id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.3.cmml\">t</mi></msub><mo id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E1.m1.1.1.1.1.2.2.2.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.3a.cmml\">,</mo><mrow id=\"S5.E1.m1.1.1.1.1.2.2.2.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.cmml\"><msub id=\"S5.E1.m1.1.1.1.1.2.2.2.2.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.cmml\"><mi id=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.2.cmml\">e</mi><mi id=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.3.cmml\">r</mi></msub><mo id=\"S5.E1.m1.1.1.1.1.2.2.2.2.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.2.cmml\">=</mo><mrow id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.cmml\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.3a.cmml\">LLM</mtext><mo id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.2.cmml\">⁢</mo><mrow id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.cmml\"><mo id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.2\" stretchy=\"false\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.cmml\">(</mo><msub id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.cmml\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.2a.cmml\">Text</mtext><mi id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.3.cmml\">r</mi></msub><mo id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.3\" stretchy=\"false\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow></mrow></mrow><mo id=\"S5.E1.m1.1.1.1.2\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E1.m1.1b\"><apply id=\"S5.E1.m1.1.1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.3a.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.3\">formulae-sequence</csymbol><apply id=\"S5.E1.m1.1.1.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1\"><eq id=\"S5.E1.m1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.2\"></eq><apply id=\"S5.E1.m1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.1.1.3.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S5.E1.m1.1.1.1.1.1.1.3.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.3.2\">𝑒</ci><ci id=\"S5.E1.m1.1.1.1.1.1.1.3.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.3.3\">ℎ</ci></apply><apply id=\"S5.E1.m1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1\"><times id=\"S5.E1.m1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.2\"></times><ci id=\"S5.E1.m1.1.1.1.1.1.1.1.3a.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.3\"><mtext id=\"S5.E1.m1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.3\">LLM</mtext></ci><apply id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.2a.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.2\"><mtext id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.2\">Text</mtext></ci><ci id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.3\">ℎ</ci></apply></apply></apply><apply id=\"S5.E1.m1.1.1.1.1.2.2.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.2.2.3a.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.3\">formulae-sequence</csymbol><apply id=\"S5.E1.m1.1.1.1.1.2.2.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1\"><eq id=\"S5.E1.m1.1.1.1.1.2.2.1.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.2\"></eq><apply id=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.3\">subscript</csymbol><ci id=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.2\">𝑒</ci><ci id=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.3\">𝑡</ci></apply><apply id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1\"><times id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.2\"></times><ci id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.3a.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.3\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.3\">LLM</mtext></ci><apply id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1\">subscript</csymbol><ci id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.2a.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.2\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.2\">Text</mtext></ci><ci id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.3\">𝑡</ci></apply></apply></apply><apply id=\"S5.E1.m1.1.1.1.1.2.2.2.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2\"><eq id=\"S5.E1.m1.1.1.1.1.2.2.2.2.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.2\"></eq><apply id=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.3\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.3\">subscript</csymbol><ci id=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.2\">𝑒</ci><ci id=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.3\">𝑟</ci></apply><apply id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1\"><times id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.2\"></times><ci id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.3a.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.3\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.3\">LLM</mtext></ci><apply id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1\">subscript</csymbol><ci id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.2a.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.2\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.2\">Text</mtext></ci><ci id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.3\">𝑟</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E1.m1.1c\">\\displaystyle e_{h}=\\text{LLM}(\\text{Text}_{h}),e_{t}=\\text{LLM}(\\text{Text}_{%\nt}),e_{r}=\\text{LLM}(\\text{Text}_{r}),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E1.m1.1d\">italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = LLM ( Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = LLM ( Text start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , italic_e start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = LLM ( Text start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS1.SSS1.p1.11\">where <math alttext=\"e_{h},e_{r},\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.5.m1.1\"><semantics id=\"S5.SS1.SSS1.p1.5.m1.1a\"><mrow id=\"S5.SS1.SSS1.p1.5.m1.1.1.1\"><mrow id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.3.cmml\"><msub id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.cmml\"><mi id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.2\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.2.cmml\">e</mi><mi id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.3\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.3.cmml\">h</mi></msub><mo id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.3\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.3.cmml\">,</mo><msub id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.cmml\"><mi id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.2\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.2.cmml\">e</mi><mi id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.3\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.3.cmml\">r</mi></msub></mrow><mo id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.2\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.5.m1.1b\"><list id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.3.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2\"><apply id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.2.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.2\">𝑒</ci><ci id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.3.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.3\">ℎ</ci></apply><apply id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.1.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2\">subscript</csymbol><ci id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.2\">𝑒</ci><ci id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.3\">𝑟</ci></apply></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.5.m1.1c\">e_{h},e_{r},</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.5.m1.1d\">italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ,</annotation></semantics></math> and <math alttext=\"e_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.6.m2.1\"><semantics id=\"S5.SS1.SSS1.p1.6.m2.1a\"><msub id=\"S5.SS1.SSS1.p1.6.m2.1.1\" xref=\"S5.SS1.SSS1.p1.6.m2.1.1.cmml\"><mi id=\"S5.SS1.SSS1.p1.6.m2.1.1.2\" xref=\"S5.SS1.SSS1.p1.6.m2.1.1.2.cmml\">e</mi><mi id=\"S5.SS1.SSS1.p1.6.m2.1.1.3\" xref=\"S5.SS1.SSS1.p1.6.m2.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.6.m2.1b\"><apply id=\"S5.SS1.SSS1.p1.6.m2.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.6.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.6.m2.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.6.m2.1.1\">subscript</csymbol><ci id=\"S5.SS1.SSS1.p1.6.m2.1.1.2.cmml\" xref=\"S5.SS1.SSS1.p1.6.m2.1.1.2\">𝑒</ci><ci id=\"S5.SS1.SSS1.p1.6.m2.1.1.3.cmml\" xref=\"S5.SS1.SSS1.p1.6.m2.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.6.m2.1c\">e_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.6.m2.1d\">italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> denotes the initial embeddings of entities <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.7.m3.1\"><semantics id=\"S5.SS1.SSS1.p1.7.m3.1a\"><mi id=\"S5.SS1.SSS1.p1.7.m3.1.1\" xref=\"S5.SS1.SSS1.p1.7.m3.1.1.cmml\">h</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.7.m3.1b\"><ci id=\"S5.SS1.SSS1.p1.7.m3.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.7.m3.1.1\">ℎ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.7.m3.1c\">h</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.7.m3.1d\">italic_h</annotation></semantics></math>, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.8.m4.1\"><semantics id=\"S5.SS1.SSS1.p1.8.m4.1a\"><mi id=\"S5.SS1.SSS1.p1.8.m4.1.1\" xref=\"S5.SS1.SSS1.p1.8.m4.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.8.m4.1b\"><ci id=\"S5.SS1.SSS1.p1.8.m4.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.8.m4.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.8.m4.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.8.m4.1d\">italic_t</annotation></semantics></math>, and relations <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.9.m5.1\"><semantics id=\"S5.SS1.SSS1.p1.9.m5.1a\"><mi id=\"S5.SS1.SSS1.p1.9.m5.1.1\" xref=\"S5.SS1.SSS1.p1.9.m5.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.9.m5.1b\"><ci id=\"S5.SS1.SSS1.p1.9.m5.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.9.m5.1.1\">𝑟</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.9.m5.1c\">r</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.9.m5.1d\">italic_r</annotation></semantics></math>, respectively. Pretrain-KGE uses the BERT as the LLM encoder in experiments. Then, the initial embeddings are fed into a KGE model to generate the final embeddings <math alttext=\"v_{h},v_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.10.m6.2\"><semantics id=\"S5.SS1.SSS1.p1.10.m6.2a\"><mrow id=\"S5.SS1.SSS1.p1.10.m6.2.2.2\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.3.cmml\"><msub id=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1\" xref=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.cmml\"><mi id=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.2\" xref=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.2.cmml\">v</mi><mi id=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.3\" xref=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.3.cmml\">h</mi></msub><mo id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.3\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.3.cmml\">,</mo><msub id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.cmml\"><mi id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.2\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.2.cmml\">v</mi><mi id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.3\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.3.cmml\">r</mi></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.10.m6.2b\"><list id=\"S5.SS1.SSS1.p1.10.m6.2.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2\"><apply id=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1\">subscript</csymbol><ci id=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.2.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.2\">𝑣</ci><ci id=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.3.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.3\">ℎ</ci></apply><apply id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.1.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2\">subscript</csymbol><ci id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.2\">𝑣</ci><ci id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.3\">𝑟</ci></apply></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.10.m6.2c\">v_{h},v_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.10.m6.2d\">italic_v start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext=\"v_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.11.m7.1\"><semantics id=\"S5.SS1.SSS1.p1.11.m7.1a\"><msub id=\"S5.SS1.SSS1.p1.11.m7.1.1\" xref=\"S5.SS1.SSS1.p1.11.m7.1.1.cmml\"><mi id=\"S5.SS1.SSS1.p1.11.m7.1.1.2\" xref=\"S5.SS1.SSS1.p1.11.m7.1.1.2.cmml\">v</mi><mi id=\"S5.SS1.SSS1.p1.11.m7.1.1.3\" xref=\"S5.SS1.SSS1.p1.11.m7.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.11.m7.1b\"><apply id=\"S5.SS1.SSS1.p1.11.m7.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.11.m7.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.11.m7.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.11.m7.1.1\">subscript</csymbol><ci id=\"S5.SS1.SSS1.p1.11.m7.1.1.2.cmml\" xref=\"S5.SS1.SSS1.p1.11.m7.1.1.2\">𝑣</ci><ci id=\"S5.SS1.SSS1.p1.11.m7.1.1.3.cmml\" xref=\"S5.SS1.SSS1.p1.11.m7.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.11.m7.1c\">v_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.11.m7.1d\">italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. During the KGE training phase, they optimize the KGE model by following the standard KGE loss function as\n</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S5.E2\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\mathcal{L}=[\\gamma+f(v_{h},v_{r},v_{t})-f(v^{\\prime}_{h},v^{\\prime}_{r},v^{%\n\\prime}_{t})],\" class=\"ltx_Math\" display=\"block\" id=\"S5.E2.m1.1\"><semantics id=\"S5.E2.m1.1a\"><mrow id=\"S5.E2.m1.1.1.1\" xref=\"S5.E2.m1.1.1.1.1.cmml\"><mrow id=\"S5.E2.m1.1.1.1.1\" xref=\"S5.E2.m1.1.1.1.1.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S5.E2.m1.1.1.1.1.3\" xref=\"S5.E2.m1.1.1.1.1.3.cmml\">ℒ</mi><mo id=\"S5.E2.m1.1.1.1.1.2\" xref=\"S5.E2.m1.1.1.1.1.2.cmml\">=</mo><mrow id=\"S5.E2.m1.1.1.1.1.1.1\" xref=\"S5.E2.m1.1.1.1.1.1.2.cmml\"><mo id=\"S5.E2.m1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E2.m1.1.1.1.1.1.2.1.cmml\">[</mo><mrow id=\"S5.E2.m1.1.1.1.1.1.1.1\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.cmml\"><mrow id=\"S5.E2.m1.1.1.1.1.1.1.1.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.3.5\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.5.cmml\">γ</mi><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.3.4\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.4.cmml\">+</mo><mrow id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.5\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.5.cmml\">f</mi><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.4\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.4.cmml\">⁢</mo><mrow id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.4.cmml\"><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.4\" stretchy=\"false\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.4.cmml\">(</mo><msub id=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml\">v</mi><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml\">h</mi></msub><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.5\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.4.cmml\">,</mo><msub id=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml\">v</mi><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml\">r</mi></msub><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.6\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.4.cmml\">,</mo><msub id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.2.cmml\">v</mi><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.3.cmml\">t</mi></msub><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.7\" stretchy=\"false\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.4.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.7\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.7.cmml\">−</mo><mrow id=\"S5.E2.m1.1.1.1.1.1.1.1.6\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.6.5\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.5.cmml\">f</mi><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.6.4\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.4.cmml\">⁢</mo><mrow id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.4.cmml\"><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.4\" stretchy=\"false\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.4.cmml\">(</mo><msubsup id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.2.cmml\">v</mi><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.3.cmml\">h</mi><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.3.cmml\">′</mo></msubsup><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.5\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.4.cmml\">,</mo><msubsup id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.2.cmml\">v</mi><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.3.cmml\">r</mi><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.3.cmml\">′</mo></msubsup><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.6\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.4.cmml\">,</mo><msubsup id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.2.cmml\">v</mi><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.3.cmml\">t</mi><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.3.cmml\">′</mo></msubsup><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.7\" stretchy=\"false\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.4.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E2.m1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E2.m1.1.1.1.1.1.2.1.cmml\">]</mo></mrow></mrow><mo id=\"S5.E2.m1.1.1.1.2\" xref=\"S5.E2.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E2.m1.1b\"><apply id=\"S5.E2.m1.1.1.1.1.cmml\" xref=\"S5.E2.m1.1.1.1\"><eq id=\"S5.E2.m1.1.1.1.1.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.2\"></eq><ci id=\"S5.E2.m1.1.1.1.1.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.3\">ℒ</ci><apply id=\"S5.E2.m1.1.1.1.1.1.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1\"><csymbol cd=\"latexml\" id=\"S5.E2.m1.1.1.1.1.1.2.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.2\">delimited-[]</csymbol><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1\"><minus id=\"S5.E2.m1.1.1.1.1.1.1.1.7.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.7\"></minus><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3\"><plus id=\"S5.E2.m1.1.1.1.1.1.1.1.3.4.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.4\"></plus><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.3.5.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.5\">𝛾</ci><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3\"><times id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.4.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.4\"></times><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.5.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.5\">𝑓</ci><vector id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.4.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3\"><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2\">𝑣</ci><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3\">ℎ</ci></apply><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2\">subscript</csymbol><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.2\">𝑣</ci><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.3\">𝑟</ci></apply><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3\">subscript</csymbol><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.2\">𝑣</ci><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.3\">𝑡</ci></apply></vector></apply></apply><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.6.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6\"><times id=\"S5.E2.m1.1.1.1.1.1.1.1.6.4.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.4\"></times><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.6.5.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.5\">𝑓</ci><vector id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.4.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3\"><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1\">subscript</csymbol><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1\">superscript</csymbol><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.2\">𝑣</ci><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.3\">′</ci></apply><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.3\">ℎ</ci></apply><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2\">subscript</csymbol><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2\">superscript</csymbol><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.2\">𝑣</ci><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.3\">′</ci></apply><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.3\">𝑟</ci></apply><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3\">subscript</csymbol><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3\">superscript</csymbol><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.2\">𝑣</ci><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.3\">′</ci></apply><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.3\">𝑡</ci></apply></vector></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E2.m1.1c\">\\mathcal{L}=[\\gamma+f(v_{h},v_{r},v_{t})-f(v^{\\prime}_{h},v^{\\prime}_{r},v^{%\n\\prime}_{t})],</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E2.m1.1d\">caligraphic_L = [ italic_γ + italic_f ( italic_v start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - italic_f ( italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ] ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS1.SSS1.p1.15\">where <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.12.m1.1\"><semantics id=\"S5.SS1.SSS1.p1.12.m1.1a\"><mi id=\"S5.SS1.SSS1.p1.12.m1.1.1\" xref=\"S5.SS1.SSS1.p1.12.m1.1.1.cmml\">f</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.12.m1.1b\"><ci id=\"S5.SS1.SSS1.p1.12.m1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.12.m1.1.1\">𝑓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.12.m1.1c\">f</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.12.m1.1d\">italic_f</annotation></semantics></math> is the KGE scoring function, <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.13.m2.1\"><semantics id=\"S5.SS1.SSS1.p1.13.m2.1a\"><mi id=\"S5.SS1.SSS1.p1.13.m2.1.1\" xref=\"S5.SS1.SSS1.p1.13.m2.1.1.cmml\">γ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.13.m2.1b\"><ci id=\"S5.SS1.SSS1.p1.13.m2.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.13.m2.1.1\">𝛾</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.13.m2.1c\">\\gamma</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.13.m2.1d\">italic_γ</annotation></semantics></math> is a margin hyperparameter, and <math alttext=\"v^{\\prime}_{h},v^{\\prime}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.14.m3.2\"><semantics id=\"S5.SS1.SSS1.p1.14.m3.2a\"><mrow id=\"S5.SS1.SSS1.p1.14.m3.2.2.2\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.3.cmml\"><msubsup id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.cmml\"><mi id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.2\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.2.cmml\">v</mi><mi id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.3\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.3.cmml\">h</mi><mo id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.3\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.3.cmml\">′</mo></msubsup><mo id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.3\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.3.cmml\">,</mo><msubsup id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.cmml\"><mi id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.2\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.2.cmml\">v</mi><mi id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.3\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.3.cmml\">r</mi><mo id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.3\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.3.cmml\">′</mo></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.14.m3.2b\"><list id=\"S5.SS1.SSS1.p1.14.m3.2.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2\"><apply id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1\">subscript</csymbol><apply id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.1.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1\">superscript</csymbol><ci id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.2\">𝑣</ci><ci id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.3\">′</ci></apply><ci id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.3.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.3\">ℎ</ci></apply><apply id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.1.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2\">subscript</csymbol><apply id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.1.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2\">superscript</csymbol><ci id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.2\">𝑣</ci><ci id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.3\">′</ci></apply><ci id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.3\">𝑟</ci></apply></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.14.m3.2c\">v^{\\prime}_{h},v^{\\prime}_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.14.m3.2d\">italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext=\"v^{\\prime}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS1.p1.15.m4.1\"><semantics id=\"S5.SS1.SSS1.p1.15.m4.1a\"><msubsup id=\"S5.SS1.SSS1.p1.15.m4.1.1\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1.cmml\"><mi id=\"S5.SS1.SSS1.p1.15.m4.1.1.2.2\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1.2.2.cmml\">v</mi><mi id=\"S5.SS1.SSS1.p1.15.m4.1.1.3\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1.3.cmml\">t</mi><mo id=\"S5.SS1.SSS1.p1.15.m4.1.1.2.3\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1.2.3.cmml\">′</mo></msubsup><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.15.m4.1b\"><apply id=\"S5.SS1.SSS1.p1.15.m4.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.15.m4.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1\">subscript</csymbol><apply id=\"S5.SS1.SSS1.p1.15.m4.1.1.2.cmml\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.15.m4.1.1.2.1.cmml\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1\">superscript</csymbol><ci id=\"S5.SS1.SSS1.p1.15.m4.1.1.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1.2.2\">𝑣</ci><ci id=\"S5.SS1.SSS1.p1.15.m4.1.1.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1.2.3\">′</ci></apply><ci id=\"S5.SS1.SSS1.p1.15.m4.1.1.3.cmml\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.15.m4.1c\">v^{\\prime}_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.15.m4.1d\">italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> are the negative samples. In this way, the KGE model could learn adequate structure information, while reserving partial knowledge from LLM enabling better knowledge graph embedding. KEPLER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite> offers a unified model for knowledge embedding and pre-trained language representation. This model not only generates effective text-enhanced knowledge embedding using powerful LLMs but also seamlessly integrates factual knowledge into LLMs. Nayyeri et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib132\" title=\"\">132</a>]</cite> use LLMs to generate the world-level, sentence-level, and document-level representations. They are integrated with graph structure embeddings into a unified vector by Dihedron and Quaternion representations of 4D hypercomplex numbers. Huang et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib133\" title=\"\">133</a>]</cite> combine LLMs with other vision and graph encoders to learn multi-modal knowledge graph embedding that enhances the performance of downstream tasks. CoDEx <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib134\" title=\"\">134</a>]</cite> presents a novel loss function empowered by LLMs that guides the KGE models in measuring the likelihood of triples by considering the textual information. The proposed loss function is agnostic to model structure that can be incorporated with any KGE model.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S5.SS1.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.1.2 </span>LLMs for Joint Text and KG Embedding</h4>\n<div class=\"ltx_para\" id=\"S5.SS1.SSS2.p1\">\n<p class=\"ltx_p\" id=\"S5.SS1.SSS2.p1.3\">Instead of using KGE model to consider graph structure, another line of methods directly employs LLMs to incorporate both the graph structure and textual information into the embedding space simultaneously. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.F15\" title=\"Figure 15 ‣ 5.1.2 LLMs for Joint Text and KG Embedding ‣ 5.1 LLM-augmented KG Embedding ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>, <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p1.1.m1.1\"><semantics id=\"S5.SS1.SSS2.p1.1.m1.1a\"><mi id=\"S5.SS1.SSS2.p1.1.m1.1.1\" xref=\"S5.SS1.SSS2.p1.1.m1.1.1.cmml\">k</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS2.p1.1.m1.1b\"><ci id=\"S5.SS1.SSS2.p1.1.m1.1.1.cmml\" xref=\"S5.SS1.SSS2.p1.1.m1.1.1\">𝑘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS2.p1.1.m1.1c\">k</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS2.p1.1.m1.1d\">italic_k</annotation></semantics></math>NN-KGE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib136\" title=\"\">136</a>]</cite> treats the entities and relations as special tokens in the LLM. During training, it transfers each triple <math alttext=\"(h,r,t)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p1.2.m2.3\"><semantics id=\"S5.SS1.SSS2.p1.2.m2.3a\"><mrow id=\"S5.SS1.SSS2.p1.2.m2.3.4.2\" xref=\"S5.SS1.SSS2.p1.2.m2.3.4.1.cmml\"><mo id=\"S5.SS1.SSS2.p1.2.m2.3.4.2.1\" stretchy=\"false\" xref=\"S5.SS1.SSS2.p1.2.m2.3.4.1.cmml\">(</mo><mi id=\"S5.SS1.SSS2.p1.2.m2.1.1\" xref=\"S5.SS1.SSS2.p1.2.m2.1.1.cmml\">h</mi><mo id=\"S5.SS1.SSS2.p1.2.m2.3.4.2.2\" xref=\"S5.SS1.SSS2.p1.2.m2.3.4.1.cmml\">,</mo><mi id=\"S5.SS1.SSS2.p1.2.m2.2.2\" xref=\"S5.SS1.SSS2.p1.2.m2.2.2.cmml\">r</mi><mo id=\"S5.SS1.SSS2.p1.2.m2.3.4.2.3\" xref=\"S5.SS1.SSS2.p1.2.m2.3.4.1.cmml\">,</mo><mi id=\"S5.SS1.SSS2.p1.2.m2.3.3\" xref=\"S5.SS1.SSS2.p1.2.m2.3.3.cmml\">t</mi><mo id=\"S5.SS1.SSS2.p1.2.m2.3.4.2.4\" stretchy=\"false\" xref=\"S5.SS1.SSS2.p1.2.m2.3.4.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS2.p1.2.m2.3b\"><vector id=\"S5.SS1.SSS2.p1.2.m2.3.4.1.cmml\" xref=\"S5.SS1.SSS2.p1.2.m2.3.4.2\"><ci id=\"S5.SS1.SSS2.p1.2.m2.1.1.cmml\" xref=\"S5.SS1.SSS2.p1.2.m2.1.1\">ℎ</ci><ci id=\"S5.SS1.SSS2.p1.2.m2.2.2.cmml\" xref=\"S5.SS1.SSS2.p1.2.m2.2.2\">𝑟</ci><ci id=\"S5.SS1.SSS2.p1.2.m2.3.3.cmml\" xref=\"S5.SS1.SSS2.p1.2.m2.3.3\">𝑡</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS2.p1.2.m2.3c\">(h,r,t)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS2.p1.2.m2.3d\">( italic_h , italic_r , italic_t )</annotation></semantics></math> and corresponding text descriptions into a sentence <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p1.3.m3.1\"><semantics id=\"S5.SS1.SSS2.p1.3.m3.1a\"><mi id=\"S5.SS1.SSS2.p1.3.m3.1.1\" xref=\"S5.SS1.SSS2.p1.3.m3.1.1.cmml\">x</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS2.p1.3.m3.1b\"><ci id=\"S5.SS1.SSS2.p1.3.m3.1.1.cmml\" xref=\"S5.SS1.SSS2.p1.3.m3.1.1\">𝑥</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS2.p1.3.m3.1c\">x</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS2.p1.3.m3.1d\">italic_x</annotation></semantics></math> as</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S5.E3\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"x=\\texttt{[CLS]}\\ h\\ \\ \\text{Text}_{h}\\texttt{[SEP]}\\ r\\ \\texttt{[SEP]}\\ %\n\\texttt{[MASK]}\\ \\ \\text{Text}_{t}\\texttt{[SEP]},\" class=\"ltx_Math\" display=\"block\" id=\"S5.E3.m1.1\"><semantics id=\"S5.E3.m1.1a\"><mrow id=\"S5.E3.m1.1.1.1\" xref=\"S5.E3.m1.1.1.1.1.cmml\"><mrow id=\"S5.E3.m1.1.1.1.1\" xref=\"S5.E3.m1.1.1.1.1.cmml\"><mi id=\"S5.E3.m1.1.1.1.1.5\" xref=\"S5.E3.m1.1.1.1.1.5.cmml\">x</mi><mo id=\"S5.E3.m1.1.1.1.1.4\" xref=\"S5.E3.m1.1.1.1.1.4.cmml\">=</mo><mrow id=\"S5.E3.m1.1.1.1.1.3.3\" xref=\"S5.E3.m1.1.1.1.1.3.4.cmml\"><mrow id=\"S5.E3.m1.1.1.1.1.1.1.1\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.cmml\"><mtext id=\"S5.E3.m1.1.1.1.1.1.1.1.2\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.2a.cmml\">[CLS]</mtext><mo id=\"S5.E3.m1.1.1.1.1.1.1.1.1\" lspace=\"0.500em\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S5.E3.m1.1.1.1.1.1.1.1.3\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.3.cmml\">h</mi></mrow><mspace id=\"S5.E3.m1.1.1.1.1.3.3.4\" width=\"1em\" xref=\"S5.E3.m1.1.1.1.1.3.4.cmml\"></mspace><mrow id=\"S5.E3.m1.1.1.1.1.2.2.2\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.cmml\"><msub id=\"S5.E3.m1.1.1.1.1.2.2.2.2\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2.cmml\"><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.2.2\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2.2a.cmml\">Text</mtext><mi id=\"S5.E3.m1.1.1.1.1.2.2.2.2.3\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2.3.cmml\">h</mi></msub><mo id=\"S5.E3.m1.1.1.1.1.2.2.2.1\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.1.cmml\">⁢</mo><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.3\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.3a.cmml\">[SEP]</mtext><mo id=\"S5.E3.m1.1.1.1.1.2.2.2.1a\" lspace=\"0.500em\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.1.cmml\">⁢</mo><mi id=\"S5.E3.m1.1.1.1.1.2.2.2.4\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.4.cmml\">r</mi><mo id=\"S5.E3.m1.1.1.1.1.2.2.2.1b\" lspace=\"0.500em\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.1.cmml\">⁢</mo><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.5\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.5a.cmml\">[SEP]</mtext><mo id=\"S5.E3.m1.1.1.1.1.2.2.2.1c\" lspace=\"0.500em\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.1.cmml\">⁢</mo><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.6\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.6a.cmml\">[MASK]</mtext></mrow><mspace id=\"S5.E3.m1.1.1.1.1.3.3.5\" width=\"1em\" xref=\"S5.E3.m1.1.1.1.1.3.4.cmml\"></mspace><mrow id=\"S5.E3.m1.1.1.1.1.3.3.3\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.cmml\"><msub id=\"S5.E3.m1.1.1.1.1.3.3.3.2\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2.cmml\"><mtext id=\"S5.E3.m1.1.1.1.1.3.3.3.2.2\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2.2a.cmml\">Text</mtext><mi id=\"S5.E3.m1.1.1.1.1.3.3.3.2.3\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2.3.cmml\">t</mi></msub><mo id=\"S5.E3.m1.1.1.1.1.3.3.3.1\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.1.cmml\">⁢</mo><mtext id=\"S5.E3.m1.1.1.1.1.3.3.3.3\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.3a.cmml\">[SEP]</mtext></mrow></mrow></mrow><mo id=\"S5.E3.m1.1.1.1.2\" xref=\"S5.E3.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E3.m1.1b\"><apply id=\"S5.E3.m1.1.1.1.1.cmml\" xref=\"S5.E3.m1.1.1.1\"><eq id=\"S5.E3.m1.1.1.1.1.4.cmml\" xref=\"S5.E3.m1.1.1.1.1.4\"></eq><ci id=\"S5.E3.m1.1.1.1.1.5.cmml\" xref=\"S5.E3.m1.1.1.1.1.5\">𝑥</ci><list id=\"S5.E3.m1.1.1.1.1.3.4.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3\"><apply id=\"S5.E3.m1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E3.m1.1.1.1.1.1.1.1\"><times id=\"S5.E3.m1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.1\"></times><ci id=\"S5.E3.m1.1.1.1.1.1.1.1.2a.cmml\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.2\"><mtext id=\"S5.E3.m1.1.1.1.1.1.1.1.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.2\">[CLS]</mtext></ci><ci id=\"S5.E3.m1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.3\">ℎ</ci></apply><apply id=\"S5.E3.m1.1.1.1.1.2.2.2.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2\"><times id=\"S5.E3.m1.1.1.1.1.2.2.2.1.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.1\"></times><apply id=\"S5.E3.m1.1.1.1.1.2.2.2.2.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E3.m1.1.1.1.1.2.2.2.2.1.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2\">subscript</csymbol><ci id=\"S5.E3.m1.1.1.1.1.2.2.2.2.2a.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2.2\"><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.2.2.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2.2\">Text</mtext></ci><ci id=\"S5.E3.m1.1.1.1.1.2.2.2.2.3.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2.3\">ℎ</ci></apply><ci id=\"S5.E3.m1.1.1.1.1.2.2.2.3a.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.3\"><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.3.cmml\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.3\">[SEP]</mtext></ci><ci id=\"S5.E3.m1.1.1.1.1.2.2.2.4.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.4\">𝑟</ci><ci id=\"S5.E3.m1.1.1.1.1.2.2.2.5a.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.5\"><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.5.cmml\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.5\">[SEP]</mtext></ci><ci id=\"S5.E3.m1.1.1.1.1.2.2.2.6a.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.6\"><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.6.cmml\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.6\">[MASK]</mtext></ci></apply><apply id=\"S5.E3.m1.1.1.1.1.3.3.3.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3\"><times id=\"S5.E3.m1.1.1.1.1.3.3.3.1.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.1\"></times><apply id=\"S5.E3.m1.1.1.1.1.3.3.3.2.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2\"><csymbol cd=\"ambiguous\" id=\"S5.E3.m1.1.1.1.1.3.3.3.2.1.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2\">subscript</csymbol><ci id=\"S5.E3.m1.1.1.1.1.3.3.3.2.2a.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2.2\"><mtext id=\"S5.E3.m1.1.1.1.1.3.3.3.2.2.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2.2\">Text</mtext></ci><ci id=\"S5.E3.m1.1.1.1.1.3.3.3.2.3.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2.3\">𝑡</ci></apply><ci id=\"S5.E3.m1.1.1.1.1.3.3.3.3a.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.3\"><mtext id=\"S5.E3.m1.1.1.1.1.3.3.3.3.cmml\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.3\">[SEP]</mtext></ci></apply></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E3.m1.1c\">x=\\texttt{[CLS]}\\ h\\ \\ \\text{Text}_{h}\\texttt{[SEP]}\\ r\\ \\texttt{[SEP]}\\ %\n\\texttt{[MASK]}\\ \\ \\text{Text}_{t}\\texttt{[SEP]},</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E3.m1.1d\">italic_x = [CLS] italic_h Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] italic_r [SEP] [MASK] Text start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [SEP] ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS1.SSS2.p1.6\">where the tailed entities are replaced by <span class=\"ltx_text ltx_font_typewriter\" id=\"S5.SS1.SSS2.p1.6.1\">[MASK]</span>. The sentence is fed into a LLM, which then finetunes the model to predict the masked entity, formulated as</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S5.E4\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"P_{LLM}(t|h,r)=P(\\texttt{[MASK]=t}|x,\\Theta),\" class=\"ltx_Math\" display=\"block\" id=\"S5.E4.m1.5\"><semantics id=\"S5.E4.m1.5a\"><mrow id=\"S5.E4.m1.5.5.1\" xref=\"S5.E4.m1.5.5.1.1.cmml\"><mrow id=\"S5.E4.m1.5.5.1.1\" xref=\"S5.E4.m1.5.5.1.1.cmml\"><mrow id=\"S5.E4.m1.5.5.1.1.1\" xref=\"S5.E4.m1.5.5.1.1.1.cmml\"><msub id=\"S5.E4.m1.5.5.1.1.1.3\" xref=\"S5.E4.m1.5.5.1.1.1.3.cmml\"><mi id=\"S5.E4.m1.5.5.1.1.1.3.2\" xref=\"S5.E4.m1.5.5.1.1.1.3.2.cmml\">P</mi><mrow id=\"S5.E4.m1.5.5.1.1.1.3.3\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.cmml\"><mi id=\"S5.E4.m1.5.5.1.1.1.3.3.2\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.2.cmml\">L</mi><mo id=\"S5.E4.m1.5.5.1.1.1.3.3.1\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.1.cmml\">⁢</mo><mi id=\"S5.E4.m1.5.5.1.1.1.3.3.3\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.3.cmml\">L</mi><mo id=\"S5.E4.m1.5.5.1.1.1.3.3.1a\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.1.cmml\">⁢</mo><mi id=\"S5.E4.m1.5.5.1.1.1.3.3.4\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.4.cmml\">M</mi></mrow></msub><mo id=\"S5.E4.m1.5.5.1.1.1.2\" xref=\"S5.E4.m1.5.5.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E4.m1.5.5.1.1.1.1.1\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.cmml\"><mo id=\"S5.E4.m1.5.5.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S5.E4.m1.5.5.1.1.1.1.1.1\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.cmml\"><mi id=\"S5.E4.m1.5.5.1.1.1.1.1.1.2\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.2.cmml\">t</mi><mo fence=\"false\" id=\"S5.E4.m1.5.5.1.1.1.1.1.1.1\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.1.cmml\">|</mo><mrow id=\"S5.E4.m1.5.5.1.1.1.1.1.1.3.2\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.3.1.cmml\"><mi id=\"S5.E4.m1.1.1\" xref=\"S5.E4.m1.1.1.cmml\">h</mi><mo id=\"S5.E4.m1.5.5.1.1.1.1.1.1.3.2.1\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.3.1.cmml\">,</mo><mi id=\"S5.E4.m1.2.2\" xref=\"S5.E4.m1.2.2.cmml\">r</mi></mrow></mrow><mo id=\"S5.E4.m1.5.5.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo id=\"S5.E4.m1.5.5.1.1.3\" xref=\"S5.E4.m1.5.5.1.1.3.cmml\">=</mo><mrow id=\"S5.E4.m1.5.5.1.1.2\" xref=\"S5.E4.m1.5.5.1.1.2.cmml\"><mi id=\"S5.E4.m1.5.5.1.1.2.3\" xref=\"S5.E4.m1.5.5.1.1.2.3.cmml\">P</mi><mo id=\"S5.E4.m1.5.5.1.1.2.2\" xref=\"S5.E4.m1.5.5.1.1.2.2.cmml\">⁢</mo><mrow id=\"S5.E4.m1.5.5.1.1.2.1.1\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.cmml\"><mo id=\"S5.E4.m1.5.5.1.1.2.1.1.2\" stretchy=\"false\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.cmml\">(</mo><mrow id=\"S5.E4.m1.5.5.1.1.2.1.1.1\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.cmml\"><mtext id=\"S5.E4.m1.5.5.1.1.2.1.1.1.2\" mathvariant=\"monospace\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.2a.cmml\">[MASK]=t</mtext><mo fence=\"false\" id=\"S5.E4.m1.5.5.1.1.2.1.1.1.1\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.1.cmml\">|</mo><mrow id=\"S5.E4.m1.5.5.1.1.2.1.1.1.3.2\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.3.1.cmml\"><mi id=\"S5.E4.m1.3.3\" xref=\"S5.E4.m1.3.3.cmml\">x</mi><mo id=\"S5.E4.m1.5.5.1.1.2.1.1.1.3.2.1\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.3.1.cmml\">,</mo><mi id=\"S5.E4.m1.4.4\" mathvariant=\"normal\" xref=\"S5.E4.m1.4.4.cmml\">Θ</mi></mrow></mrow><mo id=\"S5.E4.m1.5.5.1.1.2.1.1.3\" stretchy=\"false\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E4.m1.5.5.1.2\" xref=\"S5.E4.m1.5.5.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E4.m1.5b\"><apply id=\"S5.E4.m1.5.5.1.1.cmml\" xref=\"S5.E4.m1.5.5.1\"><eq id=\"S5.E4.m1.5.5.1.1.3.cmml\" xref=\"S5.E4.m1.5.5.1.1.3\"></eq><apply id=\"S5.E4.m1.5.5.1.1.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.1\"><times id=\"S5.E4.m1.5.5.1.1.1.2.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.2\"></times><apply id=\"S5.E4.m1.5.5.1.1.1.3.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.E4.m1.5.5.1.1.1.3.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3\">subscript</csymbol><ci id=\"S5.E4.m1.5.5.1.1.1.3.2.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3.2\">𝑃</ci><apply id=\"S5.E4.m1.5.5.1.1.1.3.3.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3.3\"><times id=\"S5.E4.m1.5.5.1.1.1.3.3.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.1\"></times><ci id=\"S5.E4.m1.5.5.1.1.1.3.3.2.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.2\">𝐿</ci><ci id=\"S5.E4.m1.5.5.1.1.1.3.3.3.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.3\">𝐿</ci><ci id=\"S5.E4.m1.5.5.1.1.1.3.3.4.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.4\">𝑀</ci></apply></apply><apply id=\"S5.E4.m1.5.5.1.1.1.1.1.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.1.1\"><csymbol cd=\"latexml\" id=\"S5.E4.m1.5.5.1.1.1.1.1.1.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.1\">conditional</csymbol><ci id=\"S5.E4.m1.5.5.1.1.1.1.1.1.2.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.2\">𝑡</ci><list id=\"S5.E4.m1.5.5.1.1.1.1.1.1.3.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.3.2\"><ci id=\"S5.E4.m1.1.1.cmml\" xref=\"S5.E4.m1.1.1\">ℎ</ci><ci id=\"S5.E4.m1.2.2.cmml\" xref=\"S5.E4.m1.2.2\">𝑟</ci></list></apply></apply><apply id=\"S5.E4.m1.5.5.1.1.2.cmml\" xref=\"S5.E4.m1.5.5.1.1.2\"><times id=\"S5.E4.m1.5.5.1.1.2.2.cmml\" xref=\"S5.E4.m1.5.5.1.1.2.2\"></times><ci id=\"S5.E4.m1.5.5.1.1.2.3.cmml\" xref=\"S5.E4.m1.5.5.1.1.2.3\">𝑃</ci><apply id=\"S5.E4.m1.5.5.1.1.2.1.1.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.2.1.1\"><csymbol cd=\"latexml\" id=\"S5.E4.m1.5.5.1.1.2.1.1.1.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.1\">conditional</csymbol><ci id=\"S5.E4.m1.5.5.1.1.2.1.1.1.2a.cmml\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.2\"><mtext id=\"S5.E4.m1.5.5.1.1.2.1.1.1.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.2\">[MASK]=t</mtext></ci><list id=\"S5.E4.m1.5.5.1.1.2.1.1.1.3.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.3.2\"><ci id=\"S5.E4.m1.3.3.cmml\" xref=\"S5.E4.m1.3.3\">𝑥</ci><ci id=\"S5.E4.m1.4.4.cmml\" xref=\"S5.E4.m1.4.4\">Θ</ci></list></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E4.m1.5c\">P_{LLM}(t|h,r)=P(\\texttt{[MASK]=t}|x,\\Theta),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E4.m1.5d\">italic_P start_POSTSUBSCRIPT italic_L italic_L italic_M end_POSTSUBSCRIPT ( italic_t | italic_h , italic_r ) = italic_P ( [MASK]=t | italic_x , roman_Θ ) ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(4)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS1.SSS2.p1.5\">where <math alttext=\"\\Theta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p1.4.m1.1\"><semantics id=\"S5.SS1.SSS2.p1.4.m1.1a\"><mi id=\"S5.SS1.SSS2.p1.4.m1.1.1\" mathvariant=\"normal\" xref=\"S5.SS1.SSS2.p1.4.m1.1.1.cmml\">Θ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS2.p1.4.m1.1b\"><ci id=\"S5.SS1.SSS2.p1.4.m1.1.1.cmml\" xref=\"S5.SS1.SSS2.p1.4.m1.1.1\">Θ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS2.p1.4.m1.1c\">\\Theta</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS2.p1.4.m1.1d\">roman_Θ</annotation></semantics></math> denotes the parameters of the LLM. The LLM is optimized to maximize the probability of the correct entity <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS2.p1.5.m2.1\"><semantics id=\"S5.SS1.SSS2.p1.5.m2.1a\"><mi id=\"S5.SS1.SSS2.p1.5.m2.1.1\" xref=\"S5.SS1.SSS2.p1.5.m2.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS2.p1.5.m2.1b\"><ci id=\"S5.SS1.SSS2.p1.5.m2.1.1.cmml\" xref=\"S5.SS1.SSS2.p1.5.m2.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS2.p1.5.m2.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS2.p1.5.m2.1d\">italic_t</annotation></semantics></math>. After training, the corresponding token representations in LLMs are used as embeddings for entities and relations. Similarly, LMKE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib135\" title=\"\">135</a>]</cite> proposes a contrastive learning method to improve the learning of embeddings generated by LLMs for KGE. Meanwhile, to better capture graph structure, LambdaKG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib137\" title=\"\">137</a>]</cite> samples 1-hop neighbor entities and concatenates their tokens with the triple as a sentence feeding into LLMs.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F15\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"378\" id=\"S5.F15.g1\" src=\"x12.png\" width=\"664\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 15: </span>LLMs for joint text and knowledge graph embedding.</figcaption>\n</figure>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.2 </span><span class=\"ltx_text ltx_font_italic\" id=\"S5.SS2.1.1\">LLM-augmented KG Completion</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS2.p1\">\n<p class=\"ltx_p\" id=\"S5.SS2.p1.1\">Knowledge Graph Completion (KGC) refers to the task of inferring missing facts in a given knowledge graph. Similar to KGE, conventional KGC methods mainly focused on the structure of the KG, without considering the extensive textual information. However, the recent integration of LLMs enables KGC methods to encode text or generate facts for better KGC performance. These methods fall into two distinct categories based on their utilization styles: <em class=\"ltx_emph ltx_font_italic\" id=\"S5.SS2.p1.1.1\">1) LLM as Encoders (PaE)</em>, and <em class=\"ltx_emph ltx_font_italic\" id=\"S5.SS2.p1.1.2\">2) LLM as Generators (PaG)</em>.</p>\n</div>\n<section class=\"ltx_subsubsection\" id=\"S5.SS2.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.2.1 </span>LLM as Encoders (PaE).</h4>\n<div class=\"ltx_para\" id=\"S5.SS2.SSS1.p1\">\n<p class=\"ltx_p\" id=\"S5.SS2.SSS1.p1.1\">As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.F16\" title=\"Figure 16 ‣ 5.2.1 LLM as Encoders (PaE). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> (a), (b), and (c), this line of work first uses encoder-only LLMs to encode textual information as well as KG facts. Then, they predict the plausibility of the triples or masked entities by feeding the encoded representation into a prediction head, which could be a simple MLP or conventional KG score function (e.g., TransE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib33\" title=\"\">33</a>]</cite> and TransR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib185\" title=\"\">185</a>]</cite>).</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS2.SSS1.p2\">\n<p class=\"ltx_p\" id=\"S5.SS2.SSS1.p2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.SS2.SSS1.p2.1.1\">Joint Encoding.</span>\nSince the encoder-only LLMs (e.g., Bert <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib1\" title=\"\">1</a>]</cite>) are well at encoding text sequences, KG-BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib26\" title=\"\">26</a>]</cite> represents a triple <math alttext=\"(h,r,t)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p2.1.m1.3\"><semantics id=\"S5.SS2.SSS1.p2.1.m1.3a\"><mrow id=\"S5.SS2.SSS1.p2.1.m1.3.4.2\" xref=\"S5.SS2.SSS1.p2.1.m1.3.4.1.cmml\"><mo id=\"S5.SS2.SSS1.p2.1.m1.3.4.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p2.1.m1.3.4.1.cmml\">(</mo><mi id=\"S5.SS2.SSS1.p2.1.m1.1.1\" xref=\"S5.SS2.SSS1.p2.1.m1.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS1.p2.1.m1.3.4.2.2\" xref=\"S5.SS2.SSS1.p2.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p2.1.m1.2.2\" xref=\"S5.SS2.SSS1.p2.1.m1.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS1.p2.1.m1.3.4.2.3\" xref=\"S5.SS2.SSS1.p2.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p2.1.m1.3.3\" xref=\"S5.SS2.SSS1.p2.1.m1.3.3.cmml\">t</mi><mo id=\"S5.SS2.SSS1.p2.1.m1.3.4.2.4\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p2.1.m1.3.4.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.1.m1.3b\"><vector id=\"S5.SS2.SSS1.p2.1.m1.3.4.1.cmml\" xref=\"S5.SS2.SSS1.p2.1.m1.3.4.2\"><ci id=\"S5.SS2.SSS1.p2.1.m1.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.1.m1.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS1.p2.1.m1.2.2.cmml\" xref=\"S5.SS2.SSS1.p2.1.m1.2.2\">𝑟</ci><ci id=\"S5.SS2.SSS1.p2.1.m1.3.3.cmml\" xref=\"S5.SS2.SSS1.p2.1.m1.3.3\">𝑡</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.1.m1.3c\">(h,r,t)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.1.m1.3d\">( italic_h , italic_r , italic_t )</annotation></semantics></math> as a text sequence and encodes it with LLM Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.F16\" title=\"Figure 16 ‣ 5.2.1 LLM as Encoders (PaE). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>(a).</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S5.E5\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"x=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}\\ \\texttt{[%\nSEP]}\\ \\text{Text}_{t}\\ \\texttt{[SEP]},\" class=\"ltx_Math\" display=\"block\" id=\"S5.E5.m1.1\"><semantics id=\"S5.E5.m1.1a\"><mrow id=\"S5.E5.m1.1.1.1\" xref=\"S5.E5.m1.1.1.1.1.cmml\"><mrow id=\"S5.E5.m1.1.1.1.1\" xref=\"S5.E5.m1.1.1.1.1.cmml\"><mi id=\"S5.E5.m1.1.1.1.1.2\" xref=\"S5.E5.m1.1.1.1.1.2.cmml\">x</mi><mo id=\"S5.E5.m1.1.1.1.1.1\" xref=\"S5.E5.m1.1.1.1.1.1.cmml\">=</mo><mrow id=\"S5.E5.m1.1.1.1.1.3\" xref=\"S5.E5.m1.1.1.1.1.3.cmml\"><mtext id=\"S5.E5.m1.1.1.1.1.3.2\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.2a.cmml\">[CLS]</mtext><mo id=\"S5.E5.m1.1.1.1.1.3.1\" lspace=\"0.500em\" xref=\"S5.E5.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E5.m1.1.1.1.1.3.3\" xref=\"S5.E5.m1.1.1.1.1.3.3.cmml\"><mtext id=\"S5.E5.m1.1.1.1.1.3.3.2\" xref=\"S5.E5.m1.1.1.1.1.3.3.2a.cmml\">Text</mtext><mi id=\"S5.E5.m1.1.1.1.1.3.3.3\" xref=\"S5.E5.m1.1.1.1.1.3.3.3.cmml\">h</mi></msub><mo id=\"S5.E5.m1.1.1.1.1.3.1a\" xref=\"S5.E5.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E5.m1.1.1.1.1.3.4\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.4a.cmml\">[SEP]</mtext><mo id=\"S5.E5.m1.1.1.1.1.3.1b\" lspace=\"0.500em\" xref=\"S5.E5.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E5.m1.1.1.1.1.3.5\" xref=\"S5.E5.m1.1.1.1.1.3.5.cmml\"><mtext id=\"S5.E5.m1.1.1.1.1.3.5.2\" xref=\"S5.E5.m1.1.1.1.1.3.5.2a.cmml\">Text</mtext><mi id=\"S5.E5.m1.1.1.1.1.3.5.3\" xref=\"S5.E5.m1.1.1.1.1.3.5.3.cmml\">r</mi></msub><mo id=\"S5.E5.m1.1.1.1.1.3.1c\" xref=\"S5.E5.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E5.m1.1.1.1.1.3.6\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.6a.cmml\">[SEP]</mtext><mo id=\"S5.E5.m1.1.1.1.1.3.1d\" lspace=\"0.500em\" xref=\"S5.E5.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E5.m1.1.1.1.1.3.7\" xref=\"S5.E5.m1.1.1.1.1.3.7.cmml\"><mtext id=\"S5.E5.m1.1.1.1.1.3.7.2\" xref=\"S5.E5.m1.1.1.1.1.3.7.2a.cmml\">Text</mtext><mi id=\"S5.E5.m1.1.1.1.1.3.7.3\" xref=\"S5.E5.m1.1.1.1.1.3.7.3.cmml\">t</mi></msub><mo id=\"S5.E5.m1.1.1.1.1.3.1e\" xref=\"S5.E5.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E5.m1.1.1.1.1.3.8\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.8a.cmml\">[SEP]</mtext></mrow></mrow><mo id=\"S5.E5.m1.1.1.1.2\" xref=\"S5.E5.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E5.m1.1b\"><apply id=\"S5.E5.m1.1.1.1.1.cmml\" xref=\"S5.E5.m1.1.1.1\"><eq id=\"S5.E5.m1.1.1.1.1.1.cmml\" xref=\"S5.E5.m1.1.1.1.1.1\"></eq><ci id=\"S5.E5.m1.1.1.1.1.2.cmml\" xref=\"S5.E5.m1.1.1.1.1.2\">𝑥</ci><apply id=\"S5.E5.m1.1.1.1.1.3.cmml\" xref=\"S5.E5.m1.1.1.1.1.3\"><times id=\"S5.E5.m1.1.1.1.1.3.1.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.1\"></times><ci id=\"S5.E5.m1.1.1.1.1.3.2a.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.2\"><mtext id=\"S5.E5.m1.1.1.1.1.3.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.2\">[CLS]</mtext></ci><apply id=\"S5.E5.m1.1.1.1.1.3.3.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E5.m1.1.1.1.1.3.3.1.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.3\">subscript</csymbol><ci id=\"S5.E5.m1.1.1.1.1.3.3.2a.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.3.2\"><mtext id=\"S5.E5.m1.1.1.1.1.3.3.2.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.3.2\">Text</mtext></ci><ci id=\"S5.E5.m1.1.1.1.1.3.3.3.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.3.3\">ℎ</ci></apply><ci id=\"S5.E5.m1.1.1.1.1.3.4a.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.4\"><mtext id=\"S5.E5.m1.1.1.1.1.3.4.cmml\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.4\">[SEP]</mtext></ci><apply id=\"S5.E5.m1.1.1.1.1.3.5.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.5\"><csymbol cd=\"ambiguous\" id=\"S5.E5.m1.1.1.1.1.3.5.1.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.5\">subscript</csymbol><ci id=\"S5.E5.m1.1.1.1.1.3.5.2a.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.5.2\"><mtext id=\"S5.E5.m1.1.1.1.1.3.5.2.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.5.2\">Text</mtext></ci><ci id=\"S5.E5.m1.1.1.1.1.3.5.3.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.5.3\">𝑟</ci></apply><ci id=\"S5.E5.m1.1.1.1.1.3.6a.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.6\"><mtext id=\"S5.E5.m1.1.1.1.1.3.6.cmml\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.6\">[SEP]</mtext></ci><apply id=\"S5.E5.m1.1.1.1.1.3.7.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.7\"><csymbol cd=\"ambiguous\" id=\"S5.E5.m1.1.1.1.1.3.7.1.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.7\">subscript</csymbol><ci id=\"S5.E5.m1.1.1.1.1.3.7.2a.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.7.2\"><mtext id=\"S5.E5.m1.1.1.1.1.3.7.2.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.7.2\">Text</mtext></ci><ci id=\"S5.E5.m1.1.1.1.1.3.7.3.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.7.3\">𝑡</ci></apply><ci id=\"S5.E5.m1.1.1.1.1.3.8a.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.8\"><mtext id=\"S5.E5.m1.1.1.1.1.3.8.cmml\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.8\">[SEP]</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E5.m1.1c\">x=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}\\ \\texttt{[%\nSEP]}\\ \\text{Text}_{t}\\ \\texttt{[SEP]},</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E5.m1.1d\">italic_x = [CLS] Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] Text start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT [SEP] Text start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [SEP] ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(5)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS2.SSS1.p2.2\">The final hidden state of the <span class=\"ltx_text ltx_markedasmath ltx_font_typewriter\" id=\"S5.SS2.SSS1.p2.2.1\">[CLS]</span> token is fed into a classifier to predict the possibility of the triple, formulated as</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S5.E6\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"s=\\sigma(\\text{MLP}(e_{\\texttt{[CLS]}})),\" class=\"ltx_Math\" display=\"block\" id=\"S5.E6.m1.1\"><semantics id=\"S5.E6.m1.1a\"><mrow id=\"S5.E6.m1.1.1.1\" xref=\"S5.E6.m1.1.1.1.1.cmml\"><mrow id=\"S5.E6.m1.1.1.1.1\" xref=\"S5.E6.m1.1.1.1.1.cmml\"><mi id=\"S5.E6.m1.1.1.1.1.3\" xref=\"S5.E6.m1.1.1.1.1.3.cmml\">s</mi><mo id=\"S5.E6.m1.1.1.1.1.2\" xref=\"S5.E6.m1.1.1.1.1.2.cmml\">=</mo><mrow id=\"S5.E6.m1.1.1.1.1.1\" xref=\"S5.E6.m1.1.1.1.1.1.cmml\"><mi id=\"S5.E6.m1.1.1.1.1.1.3\" xref=\"S5.E6.m1.1.1.1.1.1.3.cmml\">σ</mi><mo id=\"S5.E6.m1.1.1.1.1.1.2\" xref=\"S5.E6.m1.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E6.m1.1.1.1.1.1.1.1\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S5.E6.m1.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S5.E6.m1.1.1.1.1.1.1.1.1\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.cmml\"><mtext id=\"S5.E6.m1.1.1.1.1.1.1.1.1.3\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.3a.cmml\">MLP</mtext><mo id=\"S5.E6.m1.1.1.1.1.1.1.1.1.2\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml\">(</mo><msub id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml\">e</mi><mtext id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3\" mathvariant=\"monospace\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml\">[CLS]</mtext></msub><mo id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo id=\"S5.E6.m1.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E6.m1.1.1.1.2\" xref=\"S5.E6.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E6.m1.1b\"><apply id=\"S5.E6.m1.1.1.1.1.cmml\" xref=\"S5.E6.m1.1.1.1\"><eq id=\"S5.E6.m1.1.1.1.1.2.cmml\" xref=\"S5.E6.m1.1.1.1.1.2\"></eq><ci id=\"S5.E6.m1.1.1.1.1.3.cmml\" xref=\"S5.E6.m1.1.1.1.1.3\">𝑠</ci><apply id=\"S5.E6.m1.1.1.1.1.1.cmml\" xref=\"S5.E6.m1.1.1.1.1.1\"><times id=\"S5.E6.m1.1.1.1.1.1.2.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.2\"></times><ci id=\"S5.E6.m1.1.1.1.1.1.3.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.3\">𝜎</ci><apply id=\"S5.E6.m1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1\"><times id=\"S5.E6.m1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.2\"></times><ci id=\"S5.E6.m1.1.1.1.1.1.1.1.1.3a.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.3\"><mtext id=\"S5.E6.m1.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.3\">MLP</mtext></ci><apply id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2\">𝑒</ci><ci id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3\"><mtext id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml\" mathsize=\"70%\" mathvariant=\"monospace\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3\">[CLS]</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E6.m1.1c\">s=\\sigma(\\text{MLP}(e_{\\texttt{[CLS]}})),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E6.m1.1d\">italic_s = italic_σ ( MLP ( italic_e start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(6)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS2.SSS1.p2.10\">where <math alttext=\"\\sigma(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p2.3.m1.1\"><semantics id=\"S5.SS2.SSS1.p2.3.m1.1a\"><mrow id=\"S5.SS2.SSS1.p2.3.m1.1.2\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.cmml\"><mi id=\"S5.SS2.SSS1.p2.3.m1.1.2.2\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.2.cmml\">σ</mi><mo id=\"S5.SS2.SSS1.p2.3.m1.1.2.1\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.1.cmml\">⁢</mo><mrow id=\"S5.SS2.SSS1.p2.3.m1.1.2.3.2\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.cmml\"><mo id=\"S5.SS2.SSS1.p2.3.m1.1.2.3.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.cmml\">(</mo><mo id=\"S5.SS2.SSS1.p2.3.m1.1.1\" lspace=\"0em\" rspace=\"0em\" xref=\"S5.SS2.SSS1.p2.3.m1.1.1.cmml\">⋅</mo><mo id=\"S5.SS2.SSS1.p2.3.m1.1.2.3.2.2\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.3.m1.1b\"><apply id=\"S5.SS2.SSS1.p2.3.m1.1.2.cmml\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2\"><times id=\"S5.SS2.SSS1.p2.3.m1.1.2.1.cmml\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.1\"></times><ci id=\"S5.SS2.SSS1.p2.3.m1.1.2.2.cmml\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.2\">𝜎</ci><ci id=\"S5.SS2.SSS1.p2.3.m1.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.3.m1.1.1\">⋅</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.3.m1.1c\">\\sigma(\\cdot)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.3.m1.1d\">italic_σ ( ⋅ )</annotation></semantics></math> denotes the sigmoid function and <math alttext=\"e_{\\texttt{[CLS]}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p2.4.m2.1\"><semantics id=\"S5.SS2.SSS1.p2.4.m2.1a\"><msub id=\"S5.SS2.SSS1.p2.4.m2.1.1\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1.cmml\"><mi id=\"S5.SS2.SSS1.p2.4.m2.1.1.2\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1.2.cmml\">e</mi><mtext id=\"S5.SS2.SSS1.p2.4.m2.1.1.3\" mathvariant=\"monospace\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1.3a.cmml\">[CLS]</mtext></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.4.m2.1b\"><apply id=\"S5.SS2.SSS1.p2.4.m2.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS2.SSS1.p2.4.m2.1.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1\">subscript</csymbol><ci id=\"S5.SS2.SSS1.p2.4.m2.1.1.2.cmml\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1.2\">𝑒</ci><ci id=\"S5.SS2.SSS1.p2.4.m2.1.1.3a.cmml\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1.3\"><mtext id=\"S5.SS2.SSS1.p2.4.m2.1.1.3.cmml\" mathsize=\"70%\" mathvariant=\"monospace\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1.3\">[CLS]</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.4.m2.1c\">e_{\\texttt{[CLS]}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.4.m2.1d\">italic_e start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT</annotation></semantics></math> denotes the representation encoded by LLMs.\nTo improve the efficacy of KG-BERT, MTL-KGC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib138\" title=\"\">138</a>]</cite> proposed a Multi-Task Learning for the KGC framework which incorporates additional auxiliary tasks into the model’s training, i.e. prediction (RP) and relevance ranking (RR). PKGC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib139\" title=\"\">139</a>]</cite> assesses the validity of a triplet <math alttext=\"(h,r,t)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p2.5.m3.3\"><semantics id=\"S5.SS2.SSS1.p2.5.m3.3a\"><mrow id=\"S5.SS2.SSS1.p2.5.m3.3.4.2\" xref=\"S5.SS2.SSS1.p2.5.m3.3.4.1.cmml\"><mo id=\"S5.SS2.SSS1.p2.5.m3.3.4.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p2.5.m3.3.4.1.cmml\">(</mo><mi id=\"S5.SS2.SSS1.p2.5.m3.1.1\" xref=\"S5.SS2.SSS1.p2.5.m3.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS1.p2.5.m3.3.4.2.2\" xref=\"S5.SS2.SSS1.p2.5.m3.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p2.5.m3.2.2\" xref=\"S5.SS2.SSS1.p2.5.m3.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS1.p2.5.m3.3.4.2.3\" xref=\"S5.SS2.SSS1.p2.5.m3.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p2.5.m3.3.3\" xref=\"S5.SS2.SSS1.p2.5.m3.3.3.cmml\">t</mi><mo id=\"S5.SS2.SSS1.p2.5.m3.3.4.2.4\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p2.5.m3.3.4.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.5.m3.3b\"><vector id=\"S5.SS2.SSS1.p2.5.m3.3.4.1.cmml\" xref=\"S5.SS2.SSS1.p2.5.m3.3.4.2\"><ci id=\"S5.SS2.SSS1.p2.5.m3.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.5.m3.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS1.p2.5.m3.2.2.cmml\" xref=\"S5.SS2.SSS1.p2.5.m3.2.2\">𝑟</ci><ci id=\"S5.SS2.SSS1.p2.5.m3.3.3.cmml\" xref=\"S5.SS2.SSS1.p2.5.m3.3.3\">𝑡</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.5.m3.3c\">(h,r,t)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.5.m3.3d\">( italic_h , italic_r , italic_t )</annotation></semantics></math> by transforming the triple and its supporting information into natural language sentences with pre-defined templates. These sentences are then processed by LLMs for binary classification. The supporting information of the triplet is derived from the attributes of <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p2.6.m4.1\"><semantics id=\"S5.SS2.SSS1.p2.6.m4.1a\"><mi id=\"S5.SS2.SSS1.p2.6.m4.1.1\" xref=\"S5.SS2.SSS1.p2.6.m4.1.1.cmml\">h</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.6.m4.1b\"><ci id=\"S5.SS2.SSS1.p2.6.m4.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.6.m4.1.1\">ℎ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.6.m4.1c\">h</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.6.m4.1d\">italic_h</annotation></semantics></math> and <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p2.7.m5.1\"><semantics id=\"S5.SS2.SSS1.p2.7.m5.1a\"><mi id=\"S5.SS2.SSS1.p2.7.m5.1.1\" xref=\"S5.SS2.SSS1.p2.7.m5.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.7.m5.1b\"><ci id=\"S5.SS2.SSS1.p2.7.m5.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.7.m5.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.7.m5.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.7.m5.1d\">italic_t</annotation></semantics></math> with a verbalizing function. For instance, if the triple is <span class=\"ltx_text ltx_font_italic\" id=\"S5.SS2.SSS1.p2.10.1\">(Lebron James, member of sports team, Lakers)</span>, the information regarding Lebron James is verbalized as ”Lebron James: American basketball player”. LASS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib140\" title=\"\">140</a>]</cite> observes that language semantics and graph structures are equally vital to KGC. As a result, LASS is proposed to jointly learn two types of embeddings: semantic embedding and structure embedding. In this method, the full text of a triple is forwarded to the LLM, and the mean pooling of the corresponding LLM outputs for <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p2.8.m6.1\"><semantics id=\"S5.SS2.SSS1.p2.8.m6.1a\"><mi id=\"S5.SS2.SSS1.p2.8.m6.1.1\" xref=\"S5.SS2.SSS1.p2.8.m6.1.1.cmml\">h</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.8.m6.1b\"><ci id=\"S5.SS2.SSS1.p2.8.m6.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.8.m6.1.1\">ℎ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.8.m6.1c\">h</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.8.m6.1d\">italic_h</annotation></semantics></math>, <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p2.9.m7.1\"><semantics id=\"S5.SS2.SSS1.p2.9.m7.1a\"><mi id=\"S5.SS2.SSS1.p2.9.m7.1.1\" xref=\"S5.SS2.SSS1.p2.9.m7.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.9.m7.1b\"><ci id=\"S5.SS2.SSS1.p2.9.m7.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.9.m7.1.1\">𝑟</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.9.m7.1c\">r</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.9.m7.1d\">italic_r</annotation></semantics></math>, and <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p2.10.m8.1\"><semantics id=\"S5.SS2.SSS1.p2.10.m8.1a\"><mi id=\"S5.SS2.SSS1.p2.10.m8.1.1\" xref=\"S5.SS2.SSS1.p2.10.m8.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.10.m8.1b\"><ci id=\"S5.SS2.SSS1.p2.10.m8.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.10.m8.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.10.m8.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.10.m8.1d\">italic_t</annotation></semantics></math> are separately calculated. These embeddings are then passed to a graph-based method, i.e. TransE, to reconstruct the KG structures.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F16\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"705\" id=\"S5.F16.g1\" src=\"x13.png\" width=\"581\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 16: </span>The general framework of adopting LLMs as encoders (PaE) for KG Completion.</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS2.SSS1.p3\">\n<p class=\"ltx_p\" id=\"S5.SS2.SSS1.p3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.SS2.SSS1.p3.3.1\">MLM Encoding.</span> Instead of encoding the full text of a triple, many works introduce the concept of Masked Language Model (MLM) to encode KG text (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.F16\" title=\"Figure 16 ‣ 5.2.1 LLM as Encoders (PaE). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>(b)). MEM-KGC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib141\" title=\"\">141</a>]</cite> uses Masked Entity Model (MEM) classification mechanism to predict the masked entities of the triple. The input text is in the form of</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S5.E7\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"x=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}\\ \\texttt{[%\nSEP]}\\ \\texttt{[MASK]}\\ \\texttt{[SEP]},\" class=\"ltx_Math\" display=\"block\" id=\"S5.E7.m1.1\"><semantics id=\"S5.E7.m1.1a\"><mrow id=\"S5.E7.m1.1.1.1\" xref=\"S5.E7.m1.1.1.1.1.cmml\"><mrow id=\"S5.E7.m1.1.1.1.1\" xref=\"S5.E7.m1.1.1.1.1.cmml\"><mi id=\"S5.E7.m1.1.1.1.1.2\" xref=\"S5.E7.m1.1.1.1.1.2.cmml\">x</mi><mo id=\"S5.E7.m1.1.1.1.1.1\" xref=\"S5.E7.m1.1.1.1.1.1.cmml\">=</mo><mrow id=\"S5.E7.m1.1.1.1.1.3\" xref=\"S5.E7.m1.1.1.1.1.3.cmml\"><mtext id=\"S5.E7.m1.1.1.1.1.3.2\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.2a.cmml\">[CLS]</mtext><mo id=\"S5.E7.m1.1.1.1.1.3.1\" lspace=\"0.500em\" xref=\"S5.E7.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E7.m1.1.1.1.1.3.3\" xref=\"S5.E7.m1.1.1.1.1.3.3.cmml\"><mtext id=\"S5.E7.m1.1.1.1.1.3.3.2\" xref=\"S5.E7.m1.1.1.1.1.3.3.2a.cmml\">Text</mtext><mi id=\"S5.E7.m1.1.1.1.1.3.3.3\" xref=\"S5.E7.m1.1.1.1.1.3.3.3.cmml\">h</mi></msub><mo id=\"S5.E7.m1.1.1.1.1.3.1a\" xref=\"S5.E7.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E7.m1.1.1.1.1.3.4\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.4a.cmml\">[SEP]</mtext><mo id=\"S5.E7.m1.1.1.1.1.3.1b\" lspace=\"0.500em\" xref=\"S5.E7.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E7.m1.1.1.1.1.3.5\" xref=\"S5.E7.m1.1.1.1.1.3.5.cmml\"><mtext id=\"S5.E7.m1.1.1.1.1.3.5.2\" xref=\"S5.E7.m1.1.1.1.1.3.5.2a.cmml\">Text</mtext><mi id=\"S5.E7.m1.1.1.1.1.3.5.3\" xref=\"S5.E7.m1.1.1.1.1.3.5.3.cmml\">r</mi></msub><mo id=\"S5.E7.m1.1.1.1.1.3.1c\" xref=\"S5.E7.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E7.m1.1.1.1.1.3.6\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.6a.cmml\">[SEP]</mtext><mo id=\"S5.E7.m1.1.1.1.1.3.1d\" lspace=\"0.500em\" xref=\"S5.E7.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E7.m1.1.1.1.1.3.7\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.7a.cmml\">[MASK]</mtext><mo id=\"S5.E7.m1.1.1.1.1.3.1e\" lspace=\"0.500em\" xref=\"S5.E7.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E7.m1.1.1.1.1.3.8\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.8a.cmml\">[SEP]</mtext></mrow></mrow><mo id=\"S5.E7.m1.1.1.1.2\" xref=\"S5.E7.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E7.m1.1b\"><apply id=\"S5.E7.m1.1.1.1.1.cmml\" xref=\"S5.E7.m1.1.1.1\"><eq id=\"S5.E7.m1.1.1.1.1.1.cmml\" xref=\"S5.E7.m1.1.1.1.1.1\"></eq><ci id=\"S5.E7.m1.1.1.1.1.2.cmml\" xref=\"S5.E7.m1.1.1.1.1.2\">𝑥</ci><apply id=\"S5.E7.m1.1.1.1.1.3.cmml\" xref=\"S5.E7.m1.1.1.1.1.3\"><times id=\"S5.E7.m1.1.1.1.1.3.1.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.1\"></times><ci id=\"S5.E7.m1.1.1.1.1.3.2a.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.2\"><mtext id=\"S5.E7.m1.1.1.1.1.3.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.2\">[CLS]</mtext></ci><apply id=\"S5.E7.m1.1.1.1.1.3.3.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E7.m1.1.1.1.1.3.3.1.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.3\">subscript</csymbol><ci id=\"S5.E7.m1.1.1.1.1.3.3.2a.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.3.2\"><mtext id=\"S5.E7.m1.1.1.1.1.3.3.2.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.3.2\">Text</mtext></ci><ci id=\"S5.E7.m1.1.1.1.1.3.3.3.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.3.3\">ℎ</ci></apply><ci id=\"S5.E7.m1.1.1.1.1.3.4a.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.4\"><mtext id=\"S5.E7.m1.1.1.1.1.3.4.cmml\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.4\">[SEP]</mtext></ci><apply id=\"S5.E7.m1.1.1.1.1.3.5.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.5\"><csymbol cd=\"ambiguous\" id=\"S5.E7.m1.1.1.1.1.3.5.1.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.5\">subscript</csymbol><ci id=\"S5.E7.m1.1.1.1.1.3.5.2a.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.5.2\"><mtext id=\"S5.E7.m1.1.1.1.1.3.5.2.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.5.2\">Text</mtext></ci><ci id=\"S5.E7.m1.1.1.1.1.3.5.3.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.5.3\">𝑟</ci></apply><ci id=\"S5.E7.m1.1.1.1.1.3.6a.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.6\"><mtext id=\"S5.E7.m1.1.1.1.1.3.6.cmml\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.6\">[SEP]</mtext></ci><ci id=\"S5.E7.m1.1.1.1.1.3.7a.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.7\"><mtext id=\"S5.E7.m1.1.1.1.1.3.7.cmml\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.7\">[MASK]</mtext></ci><ci id=\"S5.E7.m1.1.1.1.1.3.8a.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.8\"><mtext id=\"S5.E7.m1.1.1.1.1.3.8.cmml\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.8\">[SEP]</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E7.m1.1c\">x=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}\\ \\texttt{[%\nSEP]}\\ \\texttt{[MASK]}\\ \\texttt{[SEP]},</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E7.m1.1d\">italic_x = [CLS] Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] Text start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT [SEP] [MASK] [SEP] ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(7)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS2.SSS1.p3.1\">Similar to Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.E4\" title=\"4 ‣ 5.1.2 LLMs for Joint Text and KG Embedding ‣ 5.1 LLM-augmented KG Embedding ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, it tries to maximize the probability that the masked entity is the correct entity <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p3.1.m1.1\"><semantics id=\"S5.SS2.SSS1.p3.1.m1.1a\"><mi id=\"S5.SS2.SSS1.p3.1.m1.1.1\" xref=\"S5.SS2.SSS1.p3.1.m1.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p3.1.m1.1b\"><ci id=\"S5.SS2.SSS1.p3.1.m1.1.1.cmml\" xref=\"S5.SS2.SSS1.p3.1.m1.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p3.1.m1.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p3.1.m1.1d\">italic_t</annotation></semantics></math>.\nAdditionally, to enable the model to learn unseen entities, MEM-KGC integrates multitask learning for entities and super-class prediction based on the text description of entities:</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S5.E8\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"x=\\texttt{[CLS]}\\ \\texttt{[MASK]}\\ \\texttt{[SEP]}\\ \\text{Text}_{h}\\ \\texttt{[%\nSEP]}.\" class=\"ltx_Math\" display=\"block\" id=\"S5.E8.m1.1\"><semantics id=\"S5.E8.m1.1a\"><mrow id=\"S5.E8.m1.1.1.1\" xref=\"S5.E8.m1.1.1.1.1.cmml\"><mrow id=\"S5.E8.m1.1.1.1.1\" xref=\"S5.E8.m1.1.1.1.1.cmml\"><mi id=\"S5.E8.m1.1.1.1.1.2\" xref=\"S5.E8.m1.1.1.1.1.2.cmml\">x</mi><mo id=\"S5.E8.m1.1.1.1.1.1\" xref=\"S5.E8.m1.1.1.1.1.1.cmml\">=</mo><mrow id=\"S5.E8.m1.1.1.1.1.3\" xref=\"S5.E8.m1.1.1.1.1.3.cmml\"><mtext id=\"S5.E8.m1.1.1.1.1.3.2\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.2a.cmml\">[CLS]</mtext><mo id=\"S5.E8.m1.1.1.1.1.3.1\" lspace=\"0.500em\" xref=\"S5.E8.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E8.m1.1.1.1.1.3.3\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.3a.cmml\">[MASK]</mtext><mo id=\"S5.E8.m1.1.1.1.1.3.1a\" lspace=\"0.500em\" xref=\"S5.E8.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E8.m1.1.1.1.1.3.4\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.4a.cmml\">[SEP]</mtext><mo id=\"S5.E8.m1.1.1.1.1.3.1b\" lspace=\"0.500em\" xref=\"S5.E8.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E8.m1.1.1.1.1.3.5\" xref=\"S5.E8.m1.1.1.1.1.3.5.cmml\"><mtext id=\"S5.E8.m1.1.1.1.1.3.5.2\" xref=\"S5.E8.m1.1.1.1.1.3.5.2a.cmml\">Text</mtext><mi id=\"S5.E8.m1.1.1.1.1.3.5.3\" xref=\"S5.E8.m1.1.1.1.1.3.5.3.cmml\">h</mi></msub><mo id=\"S5.E8.m1.1.1.1.1.3.1c\" xref=\"S5.E8.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E8.m1.1.1.1.1.3.6\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.6a.cmml\">[SEP]</mtext></mrow></mrow><mo id=\"S5.E8.m1.1.1.1.2\" lspace=\"0em\" xref=\"S5.E8.m1.1.1.1.1.cmml\">.</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E8.m1.1b\"><apply id=\"S5.E8.m1.1.1.1.1.cmml\" xref=\"S5.E8.m1.1.1.1\"><eq id=\"S5.E8.m1.1.1.1.1.1.cmml\" xref=\"S5.E8.m1.1.1.1.1.1\"></eq><ci id=\"S5.E8.m1.1.1.1.1.2.cmml\" xref=\"S5.E8.m1.1.1.1.1.2\">𝑥</ci><apply id=\"S5.E8.m1.1.1.1.1.3.cmml\" xref=\"S5.E8.m1.1.1.1.1.3\"><times id=\"S5.E8.m1.1.1.1.1.3.1.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.1\"></times><ci id=\"S5.E8.m1.1.1.1.1.3.2a.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.2\"><mtext id=\"S5.E8.m1.1.1.1.1.3.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.2\">[CLS]</mtext></ci><ci id=\"S5.E8.m1.1.1.1.1.3.3a.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.3\"><mtext id=\"S5.E8.m1.1.1.1.1.3.3.cmml\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.3\">[MASK]</mtext></ci><ci id=\"S5.E8.m1.1.1.1.1.3.4a.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.4\"><mtext id=\"S5.E8.m1.1.1.1.1.3.4.cmml\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.4\">[SEP]</mtext></ci><apply id=\"S5.E8.m1.1.1.1.1.3.5.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.5\"><csymbol cd=\"ambiguous\" id=\"S5.E8.m1.1.1.1.1.3.5.1.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.5\">subscript</csymbol><ci id=\"S5.E8.m1.1.1.1.1.3.5.2a.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.5.2\"><mtext id=\"S5.E8.m1.1.1.1.1.3.5.2.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.5.2\">Text</mtext></ci><ci id=\"S5.E8.m1.1.1.1.1.3.5.3.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.5.3\">ℎ</ci></apply><ci id=\"S5.E8.m1.1.1.1.1.3.6a.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.6\"><mtext id=\"S5.E8.m1.1.1.1.1.3.6.cmml\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.6\">[SEP]</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E8.m1.1c\">x=\\texttt{[CLS]}\\ \\texttt{[MASK]}\\ \\texttt{[SEP]}\\ \\text{Text}_{h}\\ \\texttt{[%\nSEP]}.</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E8.m1.1d\">italic_x = [CLS] [MASK] [SEP] Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] .</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(8)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS2.SSS1.p3.2\">OpenWorld KGC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib142\" title=\"\">142</a>]</cite> expands the MEM-KGC model to address the challenges of open-world KGC with a pipeline framework, where two sequential MLM-based modules are defined: Entity Description Prediction (EDP), an auxiliary module that predicts a corresponding entity with a given textual description; Incomplete Triple Prediction (ITP), the target module that predicts a plausible entity for a given incomplete triple <math alttext=\"(h,r,?)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p3.2.m1.3\"><semantics id=\"S5.SS2.SSS1.p3.2.m1.3a\"><mrow id=\"S5.SS2.SSS1.p3.2.m1.3.4.2\" xref=\"S5.SS2.SSS1.p3.2.m1.3.4.1.cmml\"><mo id=\"S5.SS2.SSS1.p3.2.m1.3.4.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p3.2.m1.3.4.1.cmml\">(</mo><mi id=\"S5.SS2.SSS1.p3.2.m1.1.1\" xref=\"S5.SS2.SSS1.p3.2.m1.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS1.p3.2.m1.3.4.2.2\" xref=\"S5.SS2.SSS1.p3.2.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p3.2.m1.2.2\" xref=\"S5.SS2.SSS1.p3.2.m1.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS1.p3.2.m1.3.4.2.3\" xref=\"S5.SS2.SSS1.p3.2.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p3.2.m1.3.3\" mathvariant=\"normal\" xref=\"S5.SS2.SSS1.p3.2.m1.3.3.cmml\">?</mi><mo id=\"S5.SS2.SSS1.p3.2.m1.3.4.2.4\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p3.2.m1.3.4.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p3.2.m1.3b\"><vector id=\"S5.SS2.SSS1.p3.2.m1.3.4.1.cmml\" xref=\"S5.SS2.SSS1.p3.2.m1.3.4.2\"><ci id=\"S5.SS2.SSS1.p3.2.m1.1.1.cmml\" xref=\"S5.SS2.SSS1.p3.2.m1.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS1.p3.2.m1.2.2.cmml\" xref=\"S5.SS2.SSS1.p3.2.m1.2.2\">𝑟</ci><ci id=\"S5.SS2.SSS1.p3.2.m1.3.3.cmml\" xref=\"S5.SS2.SSS1.p3.2.m1.3.3\">?</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p3.2.m1.3c\">(h,r,?)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p3.2.m1.3d\">( italic_h , italic_r , ? )</annotation></semantics></math>. EDP first encodes the triple with Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.E8\" title=\"8 ‣ 5.2.1 LLM as Encoders (PaE). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> and generates the final hidden state, which is then forwarded into ITP as an embedding of the head entity in Eq. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.E7\" title=\"7 ‣ 5.2.1 LLM as Encoders (PaE). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> to predict target entities.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS2.SSS1.p4\">\n<p class=\"ltx_p\" id=\"S5.SS2.SSS1.p4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.SS2.SSS1.p4.3.1\">Separated Encoding.</span> As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.F16\" title=\"Figure 16 ‣ 5.2.1 LLM as Encoders (PaE). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">16</span></a>(c), these methods involve partitioning a triple <math alttext=\"(h,r,t)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p4.1.m1.3\"><semantics id=\"S5.SS2.SSS1.p4.1.m1.3a\"><mrow id=\"S5.SS2.SSS1.p4.1.m1.3.4.2\" xref=\"S5.SS2.SSS1.p4.1.m1.3.4.1.cmml\"><mo id=\"S5.SS2.SSS1.p4.1.m1.3.4.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p4.1.m1.3.4.1.cmml\">(</mo><mi id=\"S5.SS2.SSS1.p4.1.m1.1.1\" xref=\"S5.SS2.SSS1.p4.1.m1.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS1.p4.1.m1.3.4.2.2\" xref=\"S5.SS2.SSS1.p4.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p4.1.m1.2.2\" xref=\"S5.SS2.SSS1.p4.1.m1.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS1.p4.1.m1.3.4.2.3\" xref=\"S5.SS2.SSS1.p4.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p4.1.m1.3.3\" xref=\"S5.SS2.SSS1.p4.1.m1.3.3.cmml\">t</mi><mo id=\"S5.SS2.SSS1.p4.1.m1.3.4.2.4\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p4.1.m1.3.4.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p4.1.m1.3b\"><vector id=\"S5.SS2.SSS1.p4.1.m1.3.4.1.cmml\" xref=\"S5.SS2.SSS1.p4.1.m1.3.4.2\"><ci id=\"S5.SS2.SSS1.p4.1.m1.1.1.cmml\" xref=\"S5.SS2.SSS1.p4.1.m1.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS1.p4.1.m1.2.2.cmml\" xref=\"S5.SS2.SSS1.p4.1.m1.2.2\">𝑟</ci><ci id=\"S5.SS2.SSS1.p4.1.m1.3.3.cmml\" xref=\"S5.SS2.SSS1.p4.1.m1.3.3\">𝑡</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p4.1.m1.3c\">(h,r,t)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p4.1.m1.3d\">( italic_h , italic_r , italic_t )</annotation></semantics></math> into two distinct parts, i.e. <math alttext=\"(h,r)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p4.2.m2.2\"><semantics id=\"S5.SS2.SSS1.p4.2.m2.2a\"><mrow id=\"S5.SS2.SSS1.p4.2.m2.2.3.2\" xref=\"S5.SS2.SSS1.p4.2.m2.2.3.1.cmml\"><mo id=\"S5.SS2.SSS1.p4.2.m2.2.3.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p4.2.m2.2.3.1.cmml\">(</mo><mi id=\"S5.SS2.SSS1.p4.2.m2.1.1\" xref=\"S5.SS2.SSS1.p4.2.m2.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS1.p4.2.m2.2.3.2.2\" xref=\"S5.SS2.SSS1.p4.2.m2.2.3.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p4.2.m2.2.2\" xref=\"S5.SS2.SSS1.p4.2.m2.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS1.p4.2.m2.2.3.2.3\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p4.2.m2.2.3.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p4.2.m2.2b\"><interval closure=\"open\" id=\"S5.SS2.SSS1.p4.2.m2.2.3.1.cmml\" xref=\"S5.SS2.SSS1.p4.2.m2.2.3.2\"><ci id=\"S5.SS2.SSS1.p4.2.m2.1.1.cmml\" xref=\"S5.SS2.SSS1.p4.2.m2.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS1.p4.2.m2.2.2.cmml\" xref=\"S5.SS2.SSS1.p4.2.m2.2.2\">𝑟</ci></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p4.2.m2.2c\">(h,r)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p4.2.m2.2d\">( italic_h , italic_r )</annotation></semantics></math> and <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p4.3.m3.1\"><semantics id=\"S5.SS2.SSS1.p4.3.m3.1a\"><mi id=\"S5.SS2.SSS1.p4.3.m3.1.1\" xref=\"S5.SS2.SSS1.p4.3.m3.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p4.3.m3.1b\"><ci id=\"S5.SS2.SSS1.p4.3.m3.1.1.cmml\" xref=\"S5.SS2.SSS1.p4.3.m3.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p4.3.m3.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p4.3.m3.1d\">italic_t</annotation></semantics></math>, which can be expressed as</p>\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"A1.EGx2\">\n<tbody id=\"S5.E9\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle x_{(h,r)}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.E9.m1.2\"><semantics id=\"S5.E9.m1.2a\"><msub id=\"S5.E9.m1.2.3\" xref=\"S5.E9.m1.2.3.cmml\"><mi id=\"S5.E9.m1.2.3.2\" xref=\"S5.E9.m1.2.3.2.cmml\">x</mi><mrow id=\"S5.E9.m1.2.2.2.4\" xref=\"S5.E9.m1.2.2.2.3.cmml\"><mo id=\"S5.E9.m1.2.2.2.4.1\" stretchy=\"false\" xref=\"S5.E9.m1.2.2.2.3.cmml\">(</mo><mi id=\"S5.E9.m1.1.1.1.1\" xref=\"S5.E9.m1.1.1.1.1.cmml\">h</mi><mo id=\"S5.E9.m1.2.2.2.4.2\" xref=\"S5.E9.m1.2.2.2.3.cmml\">,</mo><mi id=\"S5.E9.m1.2.2.2.2\" xref=\"S5.E9.m1.2.2.2.2.cmml\">r</mi><mo id=\"S5.E9.m1.2.2.2.4.3\" stretchy=\"false\" xref=\"S5.E9.m1.2.2.2.3.cmml\">)</mo></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.E9.m1.2b\"><apply id=\"S5.E9.m1.2.3.cmml\" xref=\"S5.E9.m1.2.3\"><csymbol cd=\"ambiguous\" id=\"S5.E9.m1.2.3.1.cmml\" xref=\"S5.E9.m1.2.3\">subscript</csymbol><ci id=\"S5.E9.m1.2.3.2.cmml\" xref=\"S5.E9.m1.2.3.2\">𝑥</ci><interval closure=\"open\" id=\"S5.E9.m1.2.2.2.3.cmml\" xref=\"S5.E9.m1.2.2.2.4\"><ci id=\"S5.E9.m1.1.1.1.1.cmml\" xref=\"S5.E9.m1.1.1.1.1\">ℎ</ci><ci id=\"S5.E9.m1.2.2.2.2.cmml\" xref=\"S5.E9.m1.2.2.2.2\">𝑟</ci></interval></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E9.m1.2c\">\\displaystyle x_{(h,r)}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E9.m1.2d\">italic_x start_POSTSUBSCRIPT ( italic_h , italic_r ) end_POSTSUBSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}%\n\\ \\texttt{[SEP]},\" class=\"ltx_Math\" display=\"inline\" id=\"S5.E9.m2.1\"><semantics id=\"S5.E9.m2.1a\"><mrow id=\"S5.E9.m2.1.1.1\" xref=\"S5.E9.m2.1.1.1.1.cmml\"><mrow id=\"S5.E9.m2.1.1.1.1\" xref=\"S5.E9.m2.1.1.1.1.cmml\"><mi id=\"S5.E9.m2.1.1.1.1.2\" xref=\"S5.E9.m2.1.1.1.1.2.cmml\"></mi><mo id=\"S5.E9.m2.1.1.1.1.1\" xref=\"S5.E9.m2.1.1.1.1.1.cmml\">=</mo><mrow id=\"S5.E9.m2.1.1.1.1.3\" xref=\"S5.E9.m2.1.1.1.1.3.cmml\"><mtext id=\"S5.E9.m2.1.1.1.1.3.2\" mathvariant=\"monospace\" xref=\"S5.E9.m2.1.1.1.1.3.2a.cmml\">[CLS]</mtext><mo id=\"S5.E9.m2.1.1.1.1.3.1\" lspace=\"0.500em\" xref=\"S5.E9.m2.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E9.m2.1.1.1.1.3.3\" xref=\"S5.E9.m2.1.1.1.1.3.3.cmml\"><mtext id=\"S5.E9.m2.1.1.1.1.3.3.2\" xref=\"S5.E9.m2.1.1.1.1.3.3.2a.cmml\">Text</mtext><mi id=\"S5.E9.m2.1.1.1.1.3.3.3\" xref=\"S5.E9.m2.1.1.1.1.3.3.3.cmml\">h</mi></msub><mo id=\"S5.E9.m2.1.1.1.1.3.1a\" xref=\"S5.E9.m2.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E9.m2.1.1.1.1.3.4\" mathvariant=\"monospace\" xref=\"S5.E9.m2.1.1.1.1.3.4a.cmml\">[SEP]</mtext><mo id=\"S5.E9.m2.1.1.1.1.3.1b\" lspace=\"0.500em\" xref=\"S5.E9.m2.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E9.m2.1.1.1.1.3.5\" xref=\"S5.E9.m2.1.1.1.1.3.5.cmml\"><mtext id=\"S5.E9.m2.1.1.1.1.3.5.2\" xref=\"S5.E9.m2.1.1.1.1.3.5.2a.cmml\">Text</mtext><mi id=\"S5.E9.m2.1.1.1.1.3.5.3\" xref=\"S5.E9.m2.1.1.1.1.3.5.3.cmml\">r</mi></msub><mo id=\"S5.E9.m2.1.1.1.1.3.1c\" xref=\"S5.E9.m2.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E9.m2.1.1.1.1.3.6\" mathvariant=\"monospace\" xref=\"S5.E9.m2.1.1.1.1.3.6a.cmml\">[SEP]</mtext></mrow></mrow><mo id=\"S5.E9.m2.1.1.1.2\" xref=\"S5.E9.m2.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E9.m2.1b\"><apply id=\"S5.E9.m2.1.1.1.1.cmml\" xref=\"S5.E9.m2.1.1.1\"><eq id=\"S5.E9.m2.1.1.1.1.1.cmml\" xref=\"S5.E9.m2.1.1.1.1.1\"></eq><csymbol cd=\"latexml\" id=\"S5.E9.m2.1.1.1.1.2.cmml\" xref=\"S5.E9.m2.1.1.1.1.2\">absent</csymbol><apply id=\"S5.E9.m2.1.1.1.1.3.cmml\" xref=\"S5.E9.m2.1.1.1.1.3\"><times id=\"S5.E9.m2.1.1.1.1.3.1.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.1\"></times><ci id=\"S5.E9.m2.1.1.1.1.3.2a.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.2\"><mtext id=\"S5.E9.m2.1.1.1.1.3.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E9.m2.1.1.1.1.3.2\">[CLS]</mtext></ci><apply id=\"S5.E9.m2.1.1.1.1.3.3.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E9.m2.1.1.1.1.3.3.1.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.3\">subscript</csymbol><ci id=\"S5.E9.m2.1.1.1.1.3.3.2a.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.3.2\"><mtext id=\"S5.E9.m2.1.1.1.1.3.3.2.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.3.2\">Text</mtext></ci><ci id=\"S5.E9.m2.1.1.1.1.3.3.3.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.3.3\">ℎ</ci></apply><ci id=\"S5.E9.m2.1.1.1.1.3.4a.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.4\"><mtext id=\"S5.E9.m2.1.1.1.1.3.4.cmml\" mathvariant=\"monospace\" xref=\"S5.E9.m2.1.1.1.1.3.4\">[SEP]</mtext></ci><apply id=\"S5.E9.m2.1.1.1.1.3.5.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.5\"><csymbol cd=\"ambiguous\" id=\"S5.E9.m2.1.1.1.1.3.5.1.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.5\">subscript</csymbol><ci id=\"S5.E9.m2.1.1.1.1.3.5.2a.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.5.2\"><mtext id=\"S5.E9.m2.1.1.1.1.3.5.2.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.5.2\">Text</mtext></ci><ci id=\"S5.E9.m2.1.1.1.1.3.5.3.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.5.3\">𝑟</ci></apply><ci id=\"S5.E9.m2.1.1.1.1.3.6a.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.6\"><mtext id=\"S5.E9.m2.1.1.1.1.3.6.cmml\" mathvariant=\"monospace\" xref=\"S5.E9.m2.1.1.1.1.3.6\">[SEP]</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E9.m2.1c\">\\displaystyle=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}%\n\\ \\texttt{[SEP]},</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E9.m2.1d\">= [CLS] Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] Text start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT [SEP] ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(9)</span></td>\n</tr></tbody>\n<tbody id=\"S5.E10\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.E10.m1.1\"><semantics id=\"S5.E10.m1.1a\"><msub id=\"S5.E10.m1.1.1\" xref=\"S5.E10.m1.1.1.cmml\"><mi id=\"S5.E10.m1.1.1.2\" xref=\"S5.E10.m1.1.1.2.cmml\">x</mi><mi id=\"S5.E10.m1.1.1.3\" xref=\"S5.E10.m1.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.E10.m1.1b\"><apply id=\"S5.E10.m1.1.1.cmml\" xref=\"S5.E10.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E10.m1.1.1.1.cmml\" xref=\"S5.E10.m1.1.1\">subscript</csymbol><ci id=\"S5.E10.m1.1.1.2.cmml\" xref=\"S5.E10.m1.1.1.2\">𝑥</ci><ci id=\"S5.E10.m1.1.1.3.cmml\" xref=\"S5.E10.m1.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E10.m1.1c\">\\displaystyle x_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E10.m1.1d\">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\texttt{[CLS]}\\ \\text{Text}_{t}\\ \\texttt{[SEP]}.\" class=\"ltx_Math\" display=\"inline\" id=\"S5.E10.m2.1\"><semantics id=\"S5.E10.m2.1a\"><mrow id=\"S5.E10.m2.1.1.1\" xref=\"S5.E10.m2.1.1.1.1.cmml\"><mrow id=\"S5.E10.m2.1.1.1.1\" xref=\"S5.E10.m2.1.1.1.1.cmml\"><mi id=\"S5.E10.m2.1.1.1.1.2\" xref=\"S5.E10.m2.1.1.1.1.2.cmml\"></mi><mo id=\"S5.E10.m2.1.1.1.1.1\" xref=\"S5.E10.m2.1.1.1.1.1.cmml\">=</mo><mrow id=\"S5.E10.m2.1.1.1.1.3\" xref=\"S5.E10.m2.1.1.1.1.3.cmml\"><mtext id=\"S5.E10.m2.1.1.1.1.3.2\" mathvariant=\"monospace\" xref=\"S5.E10.m2.1.1.1.1.3.2a.cmml\">[CLS]</mtext><mo id=\"S5.E10.m2.1.1.1.1.3.1\" lspace=\"0.500em\" xref=\"S5.E10.m2.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E10.m2.1.1.1.1.3.3\" xref=\"S5.E10.m2.1.1.1.1.3.3.cmml\"><mtext id=\"S5.E10.m2.1.1.1.1.3.3.2\" xref=\"S5.E10.m2.1.1.1.1.3.3.2a.cmml\">Text</mtext><mi id=\"S5.E10.m2.1.1.1.1.3.3.3\" xref=\"S5.E10.m2.1.1.1.1.3.3.3.cmml\">t</mi></msub><mo id=\"S5.E10.m2.1.1.1.1.3.1a\" xref=\"S5.E10.m2.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E10.m2.1.1.1.1.3.4\" mathvariant=\"monospace\" xref=\"S5.E10.m2.1.1.1.1.3.4a.cmml\">[SEP]</mtext></mrow></mrow><mo id=\"S5.E10.m2.1.1.1.2\" lspace=\"0em\" xref=\"S5.E10.m2.1.1.1.1.cmml\">.</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E10.m2.1b\"><apply id=\"S5.E10.m2.1.1.1.1.cmml\" xref=\"S5.E10.m2.1.1.1\"><eq id=\"S5.E10.m2.1.1.1.1.1.cmml\" xref=\"S5.E10.m2.1.1.1.1.1\"></eq><csymbol cd=\"latexml\" id=\"S5.E10.m2.1.1.1.1.2.cmml\" xref=\"S5.E10.m2.1.1.1.1.2\">absent</csymbol><apply id=\"S5.E10.m2.1.1.1.1.3.cmml\" xref=\"S5.E10.m2.1.1.1.1.3\"><times id=\"S5.E10.m2.1.1.1.1.3.1.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.1\"></times><ci id=\"S5.E10.m2.1.1.1.1.3.2a.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.2\"><mtext id=\"S5.E10.m2.1.1.1.1.3.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E10.m2.1.1.1.1.3.2\">[CLS]</mtext></ci><apply id=\"S5.E10.m2.1.1.1.1.3.3.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E10.m2.1.1.1.1.3.3.1.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.3\">subscript</csymbol><ci id=\"S5.E10.m2.1.1.1.1.3.3.2a.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.3.2\"><mtext id=\"S5.E10.m2.1.1.1.1.3.3.2.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.3.2\">Text</mtext></ci><ci id=\"S5.E10.m2.1.1.1.1.3.3.3.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.3.3\">𝑡</ci></apply><ci id=\"S5.E10.m2.1.1.1.1.3.4a.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.4\"><mtext id=\"S5.E10.m2.1.1.1.1.3.4.cmml\" mathvariant=\"monospace\" xref=\"S5.E10.m2.1.1.1.1.3.4\">[SEP]</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E10.m2.1c\">\\displaystyle=\\texttt{[CLS]}\\ \\text{Text}_{t}\\ \\texttt{[SEP]}.</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E10.m2.1d\">= [CLS] Text start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [SEP] .</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(10)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS2.SSS1.p4.6\">Then the two parts are encoded separately by LLMs, and the final hidden states of the <span class=\"ltx_text ltx_markedasmath ltx_font_typewriter\" id=\"S5.SS2.SSS1.p4.6.1\">[CLS]</span> tokens are used as the representations of <math alttext=\"(h,r)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p4.5.m2.2\"><semantics id=\"S5.SS2.SSS1.p4.5.m2.2a\"><mrow id=\"S5.SS2.SSS1.p4.5.m2.2.3.2\" xref=\"S5.SS2.SSS1.p4.5.m2.2.3.1.cmml\"><mo id=\"S5.SS2.SSS1.p4.5.m2.2.3.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p4.5.m2.2.3.1.cmml\">(</mo><mi id=\"S5.SS2.SSS1.p4.5.m2.1.1\" xref=\"S5.SS2.SSS1.p4.5.m2.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS1.p4.5.m2.2.3.2.2\" xref=\"S5.SS2.SSS1.p4.5.m2.2.3.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p4.5.m2.2.2\" xref=\"S5.SS2.SSS1.p4.5.m2.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS1.p4.5.m2.2.3.2.3\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p4.5.m2.2.3.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p4.5.m2.2b\"><interval closure=\"open\" id=\"S5.SS2.SSS1.p4.5.m2.2.3.1.cmml\" xref=\"S5.SS2.SSS1.p4.5.m2.2.3.2\"><ci id=\"S5.SS2.SSS1.p4.5.m2.1.1.cmml\" xref=\"S5.SS2.SSS1.p4.5.m2.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS1.p4.5.m2.2.2.cmml\" xref=\"S5.SS2.SSS1.p4.5.m2.2.2\">𝑟</ci></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p4.5.m2.2c\">(h,r)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p4.5.m2.2d\">( italic_h , italic_r )</annotation></semantics></math> and <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p4.6.m3.1\"><semantics id=\"S5.SS2.SSS1.p4.6.m3.1a\"><mi id=\"S5.SS2.SSS1.p4.6.m3.1.1\" xref=\"S5.SS2.SSS1.p4.6.m3.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p4.6.m3.1b\"><ci id=\"S5.SS2.SSS1.p4.6.m3.1.1.cmml\" xref=\"S5.SS2.SSS1.p4.6.m3.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p4.6.m3.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p4.6.m3.1d\">italic_t</annotation></semantics></math>, respectively. The representations are then fed into a scoring function to predict the possibility of the triple, formulated as</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S5.E11\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"s=f_{score}(e_{(h,r)},e_{t}),\" class=\"ltx_Math\" display=\"block\" id=\"S5.E11.m1.3\"><semantics id=\"S5.E11.m1.3a\"><mrow id=\"S5.E11.m1.3.3.1\" xref=\"S5.E11.m1.3.3.1.1.cmml\"><mrow id=\"S5.E11.m1.3.3.1.1\" xref=\"S5.E11.m1.3.3.1.1.cmml\"><mi id=\"S5.E11.m1.3.3.1.1.4\" xref=\"S5.E11.m1.3.3.1.1.4.cmml\">s</mi><mo id=\"S5.E11.m1.3.3.1.1.3\" xref=\"S5.E11.m1.3.3.1.1.3.cmml\">=</mo><mrow id=\"S5.E11.m1.3.3.1.1.2\" xref=\"S5.E11.m1.3.3.1.1.2.cmml\"><msub id=\"S5.E11.m1.3.3.1.1.2.4\" xref=\"S5.E11.m1.3.3.1.1.2.4.cmml\"><mi id=\"S5.E11.m1.3.3.1.1.2.4.2\" xref=\"S5.E11.m1.3.3.1.1.2.4.2.cmml\">f</mi><mrow id=\"S5.E11.m1.3.3.1.1.2.4.3\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.cmml\"><mi id=\"S5.E11.m1.3.3.1.1.2.4.3.2\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.2.cmml\">s</mi><mo id=\"S5.E11.m1.3.3.1.1.2.4.3.1\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.1.cmml\">⁢</mo><mi id=\"S5.E11.m1.3.3.1.1.2.4.3.3\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.3.cmml\">c</mi><mo id=\"S5.E11.m1.3.3.1.1.2.4.3.1a\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.1.cmml\">⁢</mo><mi id=\"S5.E11.m1.3.3.1.1.2.4.3.4\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.4.cmml\">o</mi><mo id=\"S5.E11.m1.3.3.1.1.2.4.3.1b\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.1.cmml\">⁢</mo><mi id=\"S5.E11.m1.3.3.1.1.2.4.3.5\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.5.cmml\">r</mi><mo id=\"S5.E11.m1.3.3.1.1.2.4.3.1c\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.1.cmml\">⁢</mo><mi id=\"S5.E11.m1.3.3.1.1.2.4.3.6\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.6.cmml\">e</mi></mrow></msub><mo id=\"S5.E11.m1.3.3.1.1.2.3\" xref=\"S5.E11.m1.3.3.1.1.2.3.cmml\">⁢</mo><mrow id=\"S5.E11.m1.3.3.1.1.2.2.2\" xref=\"S5.E11.m1.3.3.1.1.2.2.3.cmml\"><mo id=\"S5.E11.m1.3.3.1.1.2.2.2.3\" stretchy=\"false\" xref=\"S5.E11.m1.3.3.1.1.2.2.3.cmml\">(</mo><msub id=\"S5.E11.m1.3.3.1.1.1.1.1.1\" xref=\"S5.E11.m1.3.3.1.1.1.1.1.1.cmml\"><mi id=\"S5.E11.m1.3.3.1.1.1.1.1.1.2\" xref=\"S5.E11.m1.3.3.1.1.1.1.1.1.2.cmml\">e</mi><mrow id=\"S5.E11.m1.2.2.2.4\" xref=\"S5.E11.m1.2.2.2.3.cmml\"><mo id=\"S5.E11.m1.2.2.2.4.1\" stretchy=\"false\" xref=\"S5.E11.m1.2.2.2.3.cmml\">(</mo><mi id=\"S5.E11.m1.1.1.1.1\" xref=\"S5.E11.m1.1.1.1.1.cmml\">h</mi><mo id=\"S5.E11.m1.2.2.2.4.2\" xref=\"S5.E11.m1.2.2.2.3.cmml\">,</mo><mi id=\"S5.E11.m1.2.2.2.2\" xref=\"S5.E11.m1.2.2.2.2.cmml\">r</mi><mo id=\"S5.E11.m1.2.2.2.4.3\" stretchy=\"false\" xref=\"S5.E11.m1.2.2.2.3.cmml\">)</mo></mrow></msub><mo id=\"S5.E11.m1.3.3.1.1.2.2.2.4\" xref=\"S5.E11.m1.3.3.1.1.2.2.3.cmml\">,</mo><msub id=\"S5.E11.m1.3.3.1.1.2.2.2.2\" xref=\"S5.E11.m1.3.3.1.1.2.2.2.2.cmml\"><mi id=\"S5.E11.m1.3.3.1.1.2.2.2.2.2\" xref=\"S5.E11.m1.3.3.1.1.2.2.2.2.2.cmml\">e</mi><mi id=\"S5.E11.m1.3.3.1.1.2.2.2.2.3\" xref=\"S5.E11.m1.3.3.1.1.2.2.2.2.3.cmml\">t</mi></msub><mo id=\"S5.E11.m1.3.3.1.1.2.2.2.5\" stretchy=\"false\" xref=\"S5.E11.m1.3.3.1.1.2.2.3.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E11.m1.3.3.1.2\" xref=\"S5.E11.m1.3.3.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E11.m1.3b\"><apply id=\"S5.E11.m1.3.3.1.1.cmml\" xref=\"S5.E11.m1.3.3.1\"><eq id=\"S5.E11.m1.3.3.1.1.3.cmml\" xref=\"S5.E11.m1.3.3.1.1.3\"></eq><ci id=\"S5.E11.m1.3.3.1.1.4.cmml\" xref=\"S5.E11.m1.3.3.1.1.4\">𝑠</ci><apply id=\"S5.E11.m1.3.3.1.1.2.cmml\" xref=\"S5.E11.m1.3.3.1.1.2\"><times id=\"S5.E11.m1.3.3.1.1.2.3.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.3\"></times><apply id=\"S5.E11.m1.3.3.1.1.2.4.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4\"><csymbol cd=\"ambiguous\" id=\"S5.E11.m1.3.3.1.1.2.4.1.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4\">subscript</csymbol><ci id=\"S5.E11.m1.3.3.1.1.2.4.2.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.2\">𝑓</ci><apply id=\"S5.E11.m1.3.3.1.1.2.4.3.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.3\"><times id=\"S5.E11.m1.3.3.1.1.2.4.3.1.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.1\"></times><ci id=\"S5.E11.m1.3.3.1.1.2.4.3.2.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.2\">𝑠</ci><ci id=\"S5.E11.m1.3.3.1.1.2.4.3.3.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.3\">𝑐</ci><ci id=\"S5.E11.m1.3.3.1.1.2.4.3.4.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.4\">𝑜</ci><ci id=\"S5.E11.m1.3.3.1.1.2.4.3.5.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.5\">𝑟</ci><ci id=\"S5.E11.m1.3.3.1.1.2.4.3.6.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.6\">𝑒</ci></apply></apply><interval closure=\"open\" id=\"S5.E11.m1.3.3.1.1.2.2.3.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.2.2\"><apply id=\"S5.E11.m1.3.3.1.1.1.1.1.1.cmml\" xref=\"S5.E11.m1.3.3.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E11.m1.3.3.1.1.1.1.1.1.1.cmml\" xref=\"S5.E11.m1.3.3.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S5.E11.m1.3.3.1.1.1.1.1.1.2.cmml\" xref=\"S5.E11.m1.3.3.1.1.1.1.1.1.2\">𝑒</ci><interval closure=\"open\" id=\"S5.E11.m1.2.2.2.3.cmml\" xref=\"S5.E11.m1.2.2.2.4\"><ci id=\"S5.E11.m1.1.1.1.1.cmml\" xref=\"S5.E11.m1.1.1.1.1\">ℎ</ci><ci id=\"S5.E11.m1.2.2.2.2.cmml\" xref=\"S5.E11.m1.2.2.2.2\">𝑟</ci></interval></apply><apply id=\"S5.E11.m1.3.3.1.1.2.2.2.2.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E11.m1.3.3.1.1.2.2.2.2.1.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.2.2.2\">subscript</csymbol><ci id=\"S5.E11.m1.3.3.1.1.2.2.2.2.2.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.2.2.2.2\">𝑒</ci><ci id=\"S5.E11.m1.3.3.1.1.2.2.2.2.3.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.2.2.2.3\">𝑡</ci></apply></interval></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E11.m1.3c\">s=f_{score}(e_{(h,r)},e_{t}),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E11.m1.3d\">italic_s = italic_f start_POSTSUBSCRIPT italic_s italic_c italic_o italic_r italic_e end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT ( italic_h , italic_r ) end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(11)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS2.SSS1.p4.7\">where <math alttext=\"f_{score}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p4.7.m1.1\"><semantics id=\"S5.SS2.SSS1.p4.7.m1.1a\"><msub id=\"S5.SS2.SSS1.p4.7.m1.1.1\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.cmml\"><mi id=\"S5.SS2.SSS1.p4.7.m1.1.1.2\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.2.cmml\">f</mi><mrow id=\"S5.SS2.SSS1.p4.7.m1.1.1.3\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.cmml\"><mi id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.2\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.2.cmml\">s</mi><mo id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.3\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.3.cmml\">c</mi><mo id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1a\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.4\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.4.cmml\">o</mi><mo id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1b\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.5\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.5.cmml\">r</mi><mo id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1c\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.6\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.6.cmml\">e</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p4.7.m1.1b\"><apply id=\"S5.SS2.SSS1.p4.7.m1.1.1.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS2.SSS1.p4.7.m1.1.1.1.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1\">subscript</csymbol><ci id=\"S5.SS2.SSS1.p4.7.m1.1.1.2.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.2\">𝑓</ci><apply id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3\"><times id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1\"></times><ci id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.2.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.2\">𝑠</ci><ci id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.3.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.3\">𝑐</ci><ci id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.4.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.4\">𝑜</ci><ci id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.5.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.5\">𝑟</ci><ci id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.6.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.6\">𝑒</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p4.7.m1.1c\">f_{score}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p4.7.m1.1d\">italic_f start_POSTSUBSCRIPT italic_s italic_c italic_o italic_r italic_e end_POSTSUBSCRIPT</annotation></semantics></math> denotes the score function like TransE.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS2.SSS1.p5\">\n<p class=\"ltx_p\" id=\"S5.SS2.SSS1.p5.1\">StAR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib143\" title=\"\">143</a>]</cite> applies Siamese-style textual encoders on their text, encoding them into separate contextualized representations. To avoid the combinatorial explosion of textual encoding approaches, e.g., KG-BERT, StAR employs a scoring module that involves both deterministic classifier and spatial measurement for representation and structure learning respectively, which also enhances structured knowledge by exploring the spatial characteristics. SimKGC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib144\" title=\"\">144</a>]</cite> is another instance of leveraging a Siamese textual encoder to encode textual representations. Following the encoding process, SimKGC applies contrastive learning techniques to these representations. This process involves computing the similarity between the encoded representations of a given triple and its positive and negative samples. In particular, the similarity between the encoded representation of the triple and the positive sample is maximized, while the similarity between the encoded representation of the triple and the negative sample is minimized. This enables SimKGC to learn a representation space that separates plausible and implausible triples. To avoid overfitting textural information, CSPromp-KG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib186\" title=\"\">186</a>]</cite> employs parameter-efficient prompt learning for KGC.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS2.SSS1.p6\">\n<p class=\"ltx_p\" id=\"S5.SS2.SSS1.p6.1\">LP-BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib145\" title=\"\">145</a>]</cite> is a hybrid KGC method that combines both MLM Encoding and Separated Encoding. This approach consists of two stages, namely pre-training and fine-tuning. During pre-training, the method utilizes the standard MLM mechanism to pre-train a LLM with KGC data. During the fine-tuning stage, the LLM encodes both parts and is optimized using a contrastive learning strategy (similar to SimKGC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib144\" title=\"\">144</a>]</cite>).\n</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S5.SS2.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.2.2 </span>LLM as Generators (PaG).</h4>\n<figure class=\"ltx_figure\" id=\"S5.F17\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"648\" id=\"S5.F17.g1\" src=\"x14.png\" width=\"664\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 17: </span>The general framework of adopting LLMs as decoders (PaG) for KG Completion. The En. and De. denote the encoder and decoder, respectively.</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS2.SSS2.p1\">\n<p class=\"ltx_p\" id=\"S5.SS2.SSS2.p1.2\">Recent works use LLMs as sequence-to-sequence generators in KGC. As presented in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.F17\" title=\"Figure 17 ‣ 5.2.2 LLM as Generators (PaG). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> (a) and (b), these approaches involve encoder-decoder or decoder-only LLMs. The LLMs receive a sequence text input of the query triple <math alttext=\"(h,r,?)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS2.p1.1.m1.3\"><semantics id=\"S5.SS2.SSS2.p1.1.m1.3a\"><mrow id=\"S5.SS2.SSS2.p1.1.m1.3.4.2\" xref=\"S5.SS2.SSS2.p1.1.m1.3.4.1.cmml\"><mo id=\"S5.SS2.SSS2.p1.1.m1.3.4.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS2.p1.1.m1.3.4.1.cmml\">(</mo><mi id=\"S5.SS2.SSS2.p1.1.m1.1.1\" xref=\"S5.SS2.SSS2.p1.1.m1.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS2.p1.1.m1.3.4.2.2\" xref=\"S5.SS2.SSS2.p1.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS2.p1.1.m1.2.2\" xref=\"S5.SS2.SSS2.p1.1.m1.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS2.p1.1.m1.3.4.2.3\" xref=\"S5.SS2.SSS2.p1.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS2.p1.1.m1.3.3\" mathvariant=\"normal\" xref=\"S5.SS2.SSS2.p1.1.m1.3.3.cmml\">?</mi><mo id=\"S5.SS2.SSS2.p1.1.m1.3.4.2.4\" stretchy=\"false\" xref=\"S5.SS2.SSS2.p1.1.m1.3.4.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS2.p1.1.m1.3b\"><vector id=\"S5.SS2.SSS2.p1.1.m1.3.4.1.cmml\" xref=\"S5.SS2.SSS2.p1.1.m1.3.4.2\"><ci id=\"S5.SS2.SSS2.p1.1.m1.1.1.cmml\" xref=\"S5.SS2.SSS2.p1.1.m1.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS2.p1.1.m1.2.2.cmml\" xref=\"S5.SS2.SSS2.p1.1.m1.2.2\">𝑟</ci><ci id=\"S5.SS2.SSS2.p1.1.m1.3.3.cmml\" xref=\"S5.SS2.SSS2.p1.1.m1.3.3\">?</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS2.p1.1.m1.3c\">(h,r,?)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS2.p1.1.m1.3d\">( italic_h , italic_r , ? )</annotation></semantics></math>, and generate the text of tail entity <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS2.p1.2.m2.1\"><semantics id=\"S5.SS2.SSS2.p1.2.m2.1a\"><mi id=\"S5.SS2.SSS2.p1.2.m2.1.1\" xref=\"S5.SS2.SSS2.p1.2.m2.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS2.p1.2.m2.1b\"><ci id=\"S5.SS2.SSS2.p1.2.m2.1.1.cmml\" xref=\"S5.SS2.SSS2.p1.2.m2.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS2.p1.2.m2.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS2.p1.2.m2.1d\">italic_t</annotation></semantics></math> directly.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS2.SSS2.p2\">\n<p class=\"ltx_p\" id=\"S5.SS2.SSS2.p2.2\">GenKGC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib96\" title=\"\">96</a>]</cite> uses the large language model BART <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib5\" title=\"\">5</a>]</cite> as the backbone model. Inspired by the in-context learning approach used in GPT-3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib59\" title=\"\">59</a>]</cite>, where the model concatenates relevant samples to learn correct output answers, GenKGC proposes a relation-guided demonstration technique that includes triples with the same relation to facilitating the model’s learning process. In addition, during generation, an entity-aware hierarchical decoding method is proposed to reduce the time complexity. KGT5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib146\" title=\"\">146</a>]</cite> introduces a novel KGC model that fulfils four key requirements of such models: scalability, quality, versatility, and simplicity. To address these objectives, the proposed model employs a straightforward T5 small architecture. The model is distinct from previous KGC methods, in which it is randomly initialized rather than using pre-trained models. KG-S2S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib147\" title=\"\">147</a>]</cite> is a comprehensive framework that can be applied to various types of KGC tasks, including Static KGC, Temporal KGC, and Few-shot KGC. To achieve this objective, KG-S2S reformulates the standard triple KG fact by introducing an additional element, forming a quadruple <math alttext=\"(h,r,t,m)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS2.p2.1.m1.4\"><semantics id=\"S5.SS2.SSS2.p2.1.m1.4a\"><mrow id=\"S5.SS2.SSS2.p2.1.m1.4.5.2\" xref=\"S5.SS2.SSS2.p2.1.m1.4.5.1.cmml\"><mo id=\"S5.SS2.SSS2.p2.1.m1.4.5.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS2.p2.1.m1.4.5.1.cmml\">(</mo><mi id=\"S5.SS2.SSS2.p2.1.m1.1.1\" xref=\"S5.SS2.SSS2.p2.1.m1.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS2.p2.1.m1.4.5.2.2\" xref=\"S5.SS2.SSS2.p2.1.m1.4.5.1.cmml\">,</mo><mi id=\"S5.SS2.SSS2.p2.1.m1.2.2\" xref=\"S5.SS2.SSS2.p2.1.m1.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS2.p2.1.m1.4.5.2.3\" xref=\"S5.SS2.SSS2.p2.1.m1.4.5.1.cmml\">,</mo><mi id=\"S5.SS2.SSS2.p2.1.m1.3.3\" xref=\"S5.SS2.SSS2.p2.1.m1.3.3.cmml\">t</mi><mo id=\"S5.SS2.SSS2.p2.1.m1.4.5.2.4\" xref=\"S5.SS2.SSS2.p2.1.m1.4.5.1.cmml\">,</mo><mi id=\"S5.SS2.SSS2.p2.1.m1.4.4\" xref=\"S5.SS2.SSS2.p2.1.m1.4.4.cmml\">m</mi><mo id=\"S5.SS2.SSS2.p2.1.m1.4.5.2.5\" stretchy=\"false\" xref=\"S5.SS2.SSS2.p2.1.m1.4.5.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS2.p2.1.m1.4b\"><vector id=\"S5.SS2.SSS2.p2.1.m1.4.5.1.cmml\" xref=\"S5.SS2.SSS2.p2.1.m1.4.5.2\"><ci id=\"S5.SS2.SSS2.p2.1.m1.1.1.cmml\" xref=\"S5.SS2.SSS2.p2.1.m1.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS2.p2.1.m1.2.2.cmml\" xref=\"S5.SS2.SSS2.p2.1.m1.2.2\">𝑟</ci><ci id=\"S5.SS2.SSS2.p2.1.m1.3.3.cmml\" xref=\"S5.SS2.SSS2.p2.1.m1.3.3\">𝑡</ci><ci id=\"S5.SS2.SSS2.p2.1.m1.4.4.cmml\" xref=\"S5.SS2.SSS2.p2.1.m1.4.4\">𝑚</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS2.p2.1.m1.4c\">(h,r,t,m)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS2.p2.1.m1.4d\">( italic_h , italic_r , italic_t , italic_m )</annotation></semantics></math>, where <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS2.p2.2.m2.1\"><semantics id=\"S5.SS2.SSS2.p2.2.m2.1a\"><mi id=\"S5.SS2.SSS2.p2.2.m2.1.1\" xref=\"S5.SS2.SSS2.p2.2.m2.1.1.cmml\">m</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS2.p2.2.m2.1b\"><ci id=\"S5.SS2.SSS2.p2.2.m2.1.1.cmml\" xref=\"S5.SS2.SSS2.p2.2.m2.1.1\">𝑚</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS2.p2.2.m2.1c\">m</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS2.p2.2.m2.1d\">italic_m</annotation></semantics></math> represents the additional ”condition” element. Although different KGC tasks may refer to different conditions, they typically have a similar textual format, which enables unification across different KGC tasks. The KG-S2S approach incorporates various techniques such as entity description, soft prompt, and Seq2Seq Dropout to improve the model’s performance. In addition, it utilizes constrained decoding to ensure the generated entities are valid. For closed-source LLMs (e.g., ChatGPT and GPT-4), AutoKG adopts prompt engineering to design customized prompts <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib93\" title=\"\">93</a>]</cite>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.F18\" title=\"Figure 18 ‣ 5.2.2 LLM as Generators (PaG). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>, these prompts contain the task description, few-shot examples, and test input, which instruct LLMs to predict the tail entity for KG completion.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F18\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"534\" id=\"S5.F18.g1\" src=\"x15.png\" width=\"581\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 18: </span>The framework of prompt-based PaG for KG Completion.</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS2.SSS2.p3\">\n<p class=\"ltx_p\" id=\"S5.SS2.SSS2.p3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.SS2.SSS2.p3.1.1\">Comparison between PaE and PaG.</span>\nLLMs as Encoders (PaE) applies an additional prediction head on the top of the representation encoded by LLMs. Therefore, the PaE framework is much easier to finetune since we can only optimize the prediction heads and freeze the LLMs. Moreover, the output of the prediction can be easily specified and integrated with existing KGC functions for different KGC tasks. However, during the inference stage, the PaE requires to compute a score for every candidate in KGs, which could be computationally expensive. Besides, they cannot generalize to unseen entities. Furthermore, the PaE requires the representation output of the LLMs, whereas some state-of-the-art LLMs (e.g. GPT-4<span class=\"ltx_note ltx_role_footnotemark\" id=\"footnotex1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_type\">footnotemark: </span><span class=\"ltx_tag ltx_tag_note\">1</span></span></span></span>) are closed sources and do not grant access to the representation output.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS2.SSS2.p4\">\n<p class=\"ltx_p\" id=\"S5.SS2.SSS2.p4.1\">LLMs as Generators (PaG), on the other hand, which does not need the prediction head, can be used without finetuning or access to representations. Therefore, the framework of PaG is suitable for all kinds of LLMs. In addition, PaG directly generates the tail entity, making it efficient in inference without ranking all the candidates and easily generalizing to unseen entities. But, the challenge of PaG is that the generated entities could be diverse and not lie in KGs. What is more, the time of a single inference is longer due to the auto-regressive generation. Last, how to design a powerful prompt that feeds KGs into LLMs is still an open question.\nConsequently, while PaG has demonstrated promising results for KGC tasks, the trade-off between model complexity and computational efficiency must be carefully considered when selecting an appropriate LLM-based KGC framework.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S5.SS2.SSS3\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.2.3 </span>Model Analysis</h4>\n<div class=\"ltx_para\" id=\"S5.SS2.SSS3.p1\">\n<p class=\"ltx_p\" id=\"S5.SS2.SSS3.p1.1\">Justin et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib187\" title=\"\">187</a>]</cite> provide a comprehensive analysis of KGC methods integrated with LLMs. Their research investigates the quality of LLM embeddings and finds that they are suboptimal for effective entity ranking. In response, they propose several techniques for processing embeddings to improve their suitability for candidate retrieval. The study also compares different model selection dimensions, such as Embedding Extraction, Query Entity Extraction, and Language Model Selection. Lastly, the authors propose a framework that effectively adapts LLM for knowledge graph completion.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.3 </span><span class=\"ltx_text ltx_font_italic\" id=\"S5.SS3.1.1\">LLM-augmented KG Construction</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS3.p1\">\n<p class=\"ltx_p\" id=\"S5.SS3.p1.1\">Knowledge graph construction involves creating a structured representation of knowledge within a specific domain. This includes identifying entities and their relationships with each other. The process of knowledge graph construction typically involves multiple stages, including <em class=\"ltx_emph ltx_font_italic\" id=\"S5.SS3.p1.1.1\">1) entity discovery</em>, <em class=\"ltx_emph ltx_font_italic\" id=\"S5.SS3.p1.1.2\">2) coreference resolution</em>, and <em class=\"ltx_emph ltx_font_italic\" id=\"S5.SS3.p1.1.3\">3) relation extraction</em>. Fig <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.F19\" title=\"Figure 19 ‣ 5.3.1 Entity Discovery ‣ 5.3 LLM-augmented KG Construction ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> presents the general framework of applying LLMs for each stage in KG construction. More recent approaches have explored <em class=\"ltx_emph ltx_font_italic\" id=\"S5.SS3.p1.1.4\">4) end-to-end knowledge graph construction</em>, which involves constructing a complete knowledge graph in one step or directly <em class=\"ltx_emph ltx_font_italic\" id=\"S5.SS3.p1.1.5\">5) distilling knowledge graphs from LLMs</em>.</p>\n</div>\n<section class=\"ltx_subsubsection\" id=\"S5.SS3.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.3.1 </span>Entity Discovery</h4>\n<div class=\"ltx_para\" id=\"S5.SS3.SSS1.p1\">\n<p class=\"ltx_p\" id=\"S5.SS3.SSS1.p1.1\">Entity discovery in KG construction refers to the process of identifying and extracting entities from unstructured data sources, such as text documents, web pages, or social media posts, and incorporating them to construct knowledge graphs.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS3.SSS1.p2\">\n<p class=\"ltx_p\" id=\"S5.SS3.SSS1.p2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.SS3.SSS1.p2.1.1\">Named Entity Recognition (NER)</span> involves identifying and tagging named entities in text data with their positions and classifications. The named entities include people, organizations, locations, and other types of entities. The state-of-the-art NER methods usually employ LLMs to leverage their contextual understanding and linguistic knowledge for accurate entity recognition and classification. There are three NER sub-tasks based on the types of NER spans identified, i.e., flat NER, nested NER, and discontinuous NER. <em class=\"ltx_emph ltx_font_italic\" id=\"S5.SS3.SSS1.p2.1.2\">1) Flat NER is to identify non-overlapping named entities from input text.</em> It is usually conceptualized as a sequence labelling problem where each token in the text is assigned a unique label based on its position in the sequence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib148\" title=\"\">148</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib188\" title=\"\">188</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib189\" title=\"\">189</a>]</cite>. <em class=\"ltx_emph ltx_font_italic\" id=\"S5.SS3.SSS1.p2.1.3\">2) Nested NER considers complex scenarios which allow a token to belong to multiple entities.</em> The span-based method <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib190\" title=\"\">190</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib191\" title=\"\">191</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib192\" title=\"\">192</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib193\" title=\"\">193</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib194\" title=\"\">194</a>]</cite> is a popular branch of nested NER which involves enumerating all candidate spans and classifying them into entity types (including a non-entity type). Parsing-based methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib195\" title=\"\">195</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib196\" title=\"\">196</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib197\" title=\"\">197</a>]</cite> reveal similarities between nested NER and constituency parsing tasks (predicting nested and non-overlapping spans), and propose to integrate the insights of constituency parsing into nested NER.\n<em class=\"ltx_emph ltx_font_italic\" id=\"S5.SS3.SSS1.p2.1.4\">3) Discontinuous NER identifies named entities that may not be contiguous in the text.</em> To address this challenge, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib198\" title=\"\">198</a>]</cite> uses the LLM output to identify entity fragments and determine whether they are overlapped or in succession.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS3.SSS1.p3\">\n<p class=\"ltx_p\" id=\"S5.SS3.SSS1.p3.1\">Unlike the task-specific methods, GenerativeNER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib149\" title=\"\">149</a>]</cite> uses a sequence-to-sequence LLM with a pointer mechanism to generate an entity sequence, which is capable of solving all three types of NER sub-tasks.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F19\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"693\" id=\"S5.F19.g1\" src=\"x16.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 19: </span>The general framework of LLM-based KG construction.</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS3.SSS1.p4\">\n<p class=\"ltx_p\" id=\"S5.SS3.SSS1.p4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.SS3.SSS1.p4.1.1\">Entity Typing (ET)</span> aims to provide fine-grained and ultra-grained type information for a given entity mentioned in context. These methods usually utilize LLM to encode mentions, context and types. LDET <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib150\" title=\"\">150</a>]</cite> applies pre-trained ELMo embeddings <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib148\" title=\"\">148</a>]</cite> for word representation and adopts LSTM as its sentence and mention encoders. BOX4Types <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib151\" title=\"\">151</a>]</cite> recognizes the importance of type dependency and uses BERT to represent the hidden vector and each type in a hyperrectangular (box) space. LRN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib199\" title=\"\">199</a>]</cite> considers extrinsic and intrinsic dependencies between labels. It encodes the context and entity with BERT and employs these output embeddings to conduct deductive and inductive reasoning. MLMET <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib200\" title=\"\">200</a>]</cite> uses predefined patterns to construct input samples for the BERT MLM and employs [MASK] to predict context-dependent hypernyms of the mention, which can be viewed as type labels. PL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib201\" title=\"\">201</a>]</cite> and DFET <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib202\" title=\"\">202</a>]</cite> utilize prompt learning for entity typing. LITE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib203\" title=\"\">203</a>]</cite> formulates entity typing as textual inference and uses RoBERTa-large-MNLI as the backbone network. </p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS3.SSS1.p5\">\n<p class=\"ltx_p\" id=\"S5.SS3.SSS1.p5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.SS3.SSS1.p5.1.1\">Entity Linking (EL)</span>, as known as entity disambiguation, involves linking entity mentions appearing in the text to their corresponding entities in a knowledge graph. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib204\" title=\"\">204</a>]</cite> proposed BERT-based end-to-end EL systems that jointly discover and link entities. ELQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib152\" title=\"\">152</a>]</cite> employs a fast bi-encoder architecture to jointly perform mention detection and linking in one pass for downstream question answering systems. Unlike previous models that frame EL as matching in vector space, GENRE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib205\" title=\"\">205</a>]</cite> formulates it as a sequence-to-sequence problem, autoregressively generating a version of the input markup-annotated with the unique identifiers of an entity expressed in natural language. GENRE is extended to its multilingual version mGENRE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib206\" title=\"\">206</a>]</cite>. Considering the efficiency challenges of generative EL approaches, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib207\" title=\"\">207</a>]</cite> parallelizes autoregressive linking across all potential mentions and relies on a shallow and efficient decoder. ReFinED <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib153\" title=\"\">153</a>]</cite> proposes an efficient zero-shot-capable EL approach by taking advantage of fine-grained entity types and entity descriptions which are processed by a LLM-based encoder. </p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S5.SS3.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.3.2 </span>Coreference Resolution (CR)</h4>\n<div class=\"ltx_para\" id=\"S5.SS3.SSS2.p1\">\n<p class=\"ltx_p\" id=\"S5.SS3.SSS2.p1.1\">Coreference resolution is to find all expressions (i.e., mentions) that refer to the same entity or event in a text.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS3.SSS2.p2\">\n<p class=\"ltx_p\" id=\"S5.SS3.SSS2.p2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.SS3.SSS2.p2.1.1\">Within-document CR</span> refers to the CR sub-task where all these mentions are in a single document. Mandar et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib154\" title=\"\">154</a>]</cite> initialize LLM-based coreferences resolution by replacing the previous LSTM encoder <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib208\" title=\"\">208</a>]</cite> with BERT. This work is followed by the introduction of SpanBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib155\" title=\"\">155</a>]</cite> which is pre-trained on BERT architecture with a span-based masked language model (MLM). Inspired by these works, Tuan Manh et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib209\" title=\"\">209</a>]</cite> present a strong baseline by incorporating the SpanBERT encoder into a non-LLM approach e2e-coref <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib208\" title=\"\">208</a>]</cite>. CorefBERT leverages Mention Reference Prediction (MRP) task which masks one or several mentions and requires the model to predict the masked mention’s corresponding referents. CorefQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib210\" title=\"\">210</a>]</cite> formulates coreference resolution as a question answering task, where contextual queries are generated for each candidate mention and the coreferent spans are extracted from the document using the queries.\nTuan Manh et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib211\" title=\"\">211</a>]</cite> introduce a gating mechanism and a noisy training method to extract information from event mentions using the SpanBERT encoder.\n</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS3.SSS2.p3\">\n<p class=\"ltx_p\" id=\"S5.SS3.SSS2.p3.1\">In order to reduce the large memory footprint faced by large LLM-based NER models, Yuval et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib212\" title=\"\">212</a>]</cite> and Raghuveer el al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib213\" title=\"\">213</a>]</cite> proposed start-to-end and approximation models, respectively, both utilizing bilinear functions to calculate mention and antecedent scores with reduced reliance on span-level representations.\n</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS3.SSS2.p4\">\n<p class=\"ltx_p\" id=\"S5.SS3.SSS2.p4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.SS3.SSS2.p4.1.1\">Cross-document CR</span> refers to the sub-task where the mentions refer to the same entity or event might be across multiple documents. CDML <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib156\" title=\"\">156</a>]</cite> proposes a cross document language modeling method which pre-trains a Longformer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib214\" title=\"\">214</a>]</cite> encoder on concatenated related documents and employs an MLP for binary classification to determine whether a pair of mentions is coreferent or not. CrossCR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib157\" title=\"\">157</a>]</cite> utilizes an end-to-end model for cross-document coreference resolution which pre-trained the mention scorer on gold mention spans and uses a pairwise scorer to compare mentions with all spans across all documents. CR-RL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib158\" title=\"\">158</a>]</cite> proposes an actor-critic deep reinforcement learning-based coreference resolver for cross-document CR.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S5.SS3.SSS3\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.3.3 </span>Relation Extraction (RE)</h4>\n<div class=\"ltx_para\" id=\"S5.SS3.SSS3.p1\">\n<p class=\"ltx_p\" id=\"S5.SS3.SSS3.p1.1\">Relation extraction involves identifying semantic relationships between entities mentioned in natural language text. There are two types of relation extraction methods, i.e. sentence-level RE and document-level RE, according to the scope of the text analyzed.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS3.SSS3.p2\">\n<p class=\"ltx_p\" id=\"S5.SS3.SSS3.p2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.SS3.SSS3.p2.1.1\">Sentence-level RE</span> focuses on identifying relations between entities within a single sentence. Peng et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib159\" title=\"\">159</a>]</cite> and TRE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib215\" title=\"\">215</a>]</cite> introduce LLM to improve the performance of relation extraction models. BERT-MTB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib216\" title=\"\">216</a>]</cite> learns relation representations based on BERT by performing the matching-the-blanks task and incorporating designed objectives for relation extraction. Curriculum-RE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib160\" title=\"\">160</a>]</cite> utilizes curriculum learning to improve relation extraction models by gradually increasing the difficulty of the data during training.\nRECENT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib217\" title=\"\">217</a>]</cite> introduces SpanBERT and exploits entity type restriction to reduce the noisy candidate relation types. Jiewen <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib218\" title=\"\">218</a>]</cite> extends RECENT by combining both the entity information and the label information into sentence-level embeddings, which enables the embedding to be entity-label aware.\n</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS3.SSS3.p3\">\n<p class=\"ltx_p\" id=\"S5.SS3.SSS3.p3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.SS3.SSS3.p3.1.1\">Document-level RE (DocRE)</span> aims to extract relations between entities across multiple sentences within a document. Hong et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib219\" title=\"\">219</a>]</cite> propose a strong baseline for DocRE by replacing the BiLSTM backbone with LLMs. HIN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib220\" title=\"\">220</a>]</cite> use LLM to encode and aggregate entity representation at different levels, including entity, sentence, and document levels. GLRE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib221\" title=\"\">221</a>]</cite> is a global-to-local network, which uses LLM to encode the document information in terms of entity global and local representations as well as context relation representations. SIRE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib222\" title=\"\">222</a>]</cite> uses two LLM-based encoders to extract intra-sentence and inter-sentence relations. LSR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib223\" title=\"\">223</a>]</cite> and GAIN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib224\" title=\"\">224</a>]</cite> propose graph-based approaches which induce graph structures on top of LLM to better extract relations. DocuNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib225\" title=\"\">225</a>]</cite> formulates DocRE as a semantic segmentation task and introduces a U-Net <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib226\" title=\"\">226</a>]</cite> on the LLM encoder to capture local and global dependencies between entities. ATLOP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib227\" title=\"\">227</a>]</cite> focuses on the multi-label problems in DocRE, which could be handled with two techniques, i.e., adaptive thresholding for classifier and localized context pooling for LLM. DREEAM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib161\" title=\"\">161</a>]</cite> further extends and improves ATLOP by incorporating evidence information.\n</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.SS3.SSS3.p4\">\n<p class=\"ltx_p\" id=\"S5.SS3.SSS3.p4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.SS3.SSS3.p4.1.1\">End-to-End KG Construction.</span>\nCurrently, researchers are exploring the use of LLMs for end-to-end KG construction. Kumar et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib95\" title=\"\">95</a>]</cite> propose a unified approach to build KGs from raw text, which contains two LLMs powered components. They first finetune a LLM on named entity recognition tasks to make it capable of recognizing entities in raw text. Then, they propose another “2-model BERT” for solving the relation extraction task, which contains two BERT-based classifiers. The first classifier learns the relation class whereas the second binary classifier learns the direction of the relations between the two entities. The predicted triples and relations are then used to construct the KG. Guo et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib162\" title=\"\">162</a>]</cite> propose an end-to-end knowledge extraction model based on BERT, which can be applied to construct KGs from Classical Chinese text.\nGrapher <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib41\" title=\"\">41</a>]</cite> presents a novel end-to-end multi-stage system. It first utilizes LLMs to generate KG entities, followed by a simple relation construction head, enabling efficient KG construction from the textual description. PiVE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib163\" title=\"\">163</a>]</cite> proposes a prompting with an iterative verification framework that utilizes a smaller LLM like T5 to correct the errors in KGs generated by a larger LLM (e.g., ChatGPT). To further explore advanced LLMs, AutoKG design several prompts for different KG construction tasks (e.g., entity typing, entity linking, and relation extraction). Then, it adopts the prompt to perform KG construction using ChatGPT and GPT-4.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F20\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"310\" id=\"S5.F20.g1\" src=\"x17.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 20: </span>The general framework of distilling KGs from LLMs.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S5.SS3.SSS4\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.3.4 </span>Distilling Knowledge Graphs from LLMs </h4>\n<div class=\"ltx_para\" id=\"S5.SS3.SSS4.p1\">\n<p class=\"ltx_p\" id=\"S5.SS3.SSS4.p1.1\">LLMs have been shown to implicitly encode massive knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a>]</cite>. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.F20\" title=\"Figure 20 ‣ 5.3.3 Relation Extraction (RE) ‣ 5.3 LLM-augmented KG Construction ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, some research aims to distill knowledge from LLMs to construct KGs. COMET <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib164\" title=\"\">164</a>]</cite> proposes a commonsense transformer model that constructs commonsense KGs by using existing tuples as a seed set of knowledge on which to train. Using this seed set, a LLM learns to adapt its learned representations to knowledge generation, and produces novel tuples that are high quality. Experimental results reveal that implicit knowledge from LLMs is transferred to generate explicit knowledge in commonsense KGs. BertNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib165\" title=\"\">165</a>]</cite> proposes a novel framework for automatic KG construction empowered by LLMs. It requires only the minimal definition of relations as inputs and automatically generates diverse prompts, and performs an efficient knowledge search within a given LLM for consistent outputs. The constructed KGs show competitive quality, diversity, and novelty with a richer set of new and complex relations, which cannot be extracted by previous methods. West et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib166\" title=\"\">166</a>]</cite> propose a symbolic knowledge distillation framework that distills symbolic knowledge from LLMs. They first finetune a small student LLM by distilling commonsense facts from a large LLM like GPT-3. Then, the student LLM is utilized to generate commonsense KGs.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.4 </span><span class=\"ltx_text ltx_font_italic\" id=\"S5.SS4.1.1\">LLM-augmented KG-to-text Generation</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS4.p1\">\n<p class=\"ltx_p\" id=\"S5.SS4.p1.1\">The goal of Knowledge-graph-to-text (KG-to-text) generation is to generate high-quality texts that accurately and consistently describe the input knowledge graph information <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib228\" title=\"\">228</a>]</cite>. KG-to-text generation connects knowledge graphs and texts, significantly improving the applicability of KG in more realistic NLG scenarios, including storytelling <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib229\" title=\"\">229</a>]</cite> and knowledge-grounded dialogue <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib230\" title=\"\">230</a>]</cite>. However, it is challenging and costly to collect large amounts of graph-text parallel data, resulting in insufficient training and poor generation quality. Thus, many research efforts resort to either: <em class=\"ltx_emph ltx_font_italic\" id=\"S5.SS4.p1.1.1\">1) leverage knowledge from LLMs</em> or <em class=\"ltx_emph ltx_font_italic\" id=\"S5.SS4.p1.1.2\">2) construct large-scale weakly-supervised KG-text corpus</em> to solve this issue.</p>\n</div>\n<section class=\"ltx_subsubsection\" id=\"S5.SS4.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.4.1 </span>Leveraging Knowledge from LLMs</h4>\n<div class=\"ltx_para\" id=\"S5.SS4.SSS1.p1\">\n<p class=\"ltx_p\" id=\"S5.SS4.SSS1.p1.1\">As pioneering research efforts in using LLMs for KG-to-Text generation, Ribeiro et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib167\" title=\"\">167</a>]</cite> and Kale and Rastogi <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib231\" title=\"\">231</a>]</cite> directly fine-tune various LLMs, including BART and T5, with the goal of transferring LLMs knowledge for this task. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.F21\" title=\"Figure 21 ‣ 5.4.1 Leveraging Knowledge from LLMs ‣ 5.4 LLM-augmented KG-to-text Generation ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, both works simply represent the input graph as a linear traversal and find that such a naive approach successfully outperforms many existing state-of-the-art KG-to-text generation systems. Interestingly, Ribeiro et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib167\" title=\"\">167</a>]</cite> also find that continue pre-training could further improve model performance. However, these methods are unable to <em class=\"ltx_emph ltx_font_italic\" id=\"S5.SS4.SSS1.p1.1.1\">explicitly</em> incorporate rich graph semantics in KGs. To enhance LLMs with KG structure information, JointGT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib42\" title=\"\">42</a>]</cite> proposes to inject KG structure-preserving representations into the Seq2Seq large language models. Given input sub-KGs and corresponding text, JointGT first represents the KG entities and their relations as a sequence of tokens, then concatenate them with the textual tokens which are fed into LLM. After the standard self-attention module, JointGT then uses a pooling layer to obtain the contextual semantic representations of knowledge entities and relations. Finally, these pooled KG representations are then aggregated in another structure-aware self-attention layer. JointGT also deploys additional pre-training objectives, including KG and text reconstruction tasks given masked inputs, to improve the alignment between text and graph information. Li et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib168\" title=\"\">168</a>]</cite> focus on the few-shot scenario. It first employs a novel breadth-first search (BFS) strategy to better traverse the input KG structure and feed the enhanced linearized graph representations into LLMs for high-quality generated outputs, then aligns the GCN-based and LLM-based KG entity representation. Colas et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib169\" title=\"\">169</a>]</cite> first transform the graph into its appropriate representation before linearizing the graph. Next, each KG node is encoded via a global attention mechanism, followed by a graph-aware attention module, ultimately being decoded into a sequence of tokens. Different from these works, KG-BART <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib37\" title=\"\">37</a>]</cite> keeps the structure of KGs and leverages the graph attention to aggregate the rich concept semantics in the sub-KG, which enhances the model generalization on unseen concept sets.\n</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F21\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"372\" id=\"S5.F21.g1\" src=\"x18.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 21: </span>The general framework of KG-to-text generation.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S5.SS4.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.4.2 </span>Constructing large weakly KG-text aligned Corpus</h4>\n<div class=\"ltx_para\" id=\"S5.SS4.SSS2.p1\">\n<p class=\"ltx_p\" id=\"S5.SS4.SSS2.p1.1\">Although LLMs have achieved remarkable empirical success, their unsupervised pre-training objectives are not necessarily aligned well with the task of KG-to-text generation, motivating researchers to develop large-scale KG-text aligned corpus. Jin et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib170\" title=\"\">170</a>]</cite> propose a 1.3M unsupervised KG-to-graph training data from Wikipedia. Specifically, they first detect the entities appearing in the text via hyperlinks and named entity detectors, and then only add text that shares a common set of entities with the corresponding knowledge graph, similar to the idea of distance supervision in the relation extraction task <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib232\" title=\"\">232</a>]</cite>.\nThey also provide a 1,000+ human annotated KG-to-Text test data to verify the effectiveness of the pre-trained KG-to-Text models. Similarly, Chen et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib171\" title=\"\">171</a>]</cite> also propose a KG-grounded text corpus collected from the English Wikidump. To ensure the connection between KG and text, they only extract sentences with at least two Wikipedia anchor links. Then, they use the entities from those links to query their surrounding neighbors in WikiData and calculate the lexical overlapping between these neighbors and the original sentences. Finally, only highly overlapped pairs are selected. The authors explore both graph-based and sequence-based encoders and identify their advantages in various different tasks and settings.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.5 </span><span class=\"ltx_text ltx_font_italic\" id=\"S5.SS5.1.1\">LLM-augmented KG Question Answering</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS5.p1\">\n<p class=\"ltx_p\" id=\"S5.SS5.p1.1\">Knowledge graph question answering (KGQA) aims to find answers to natural language questions based on the structured facts stored in knowledge graphs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib233\" title=\"\">233</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib234\" title=\"\">234</a>]</cite>. The inevitable challenge in KGQA is to retrieve related facts and extend the reasoning advantage of KGs to QA. Therefore, recent studies adopt LLMs to bridge the gap between natural language questions and structured knowledge graphs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib175\" title=\"\">175</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib235\" title=\"\">235</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib174\" title=\"\">174</a>]</cite>. The general framework of applying LLMs for KGQA is illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.F22\" title=\"Figure 22 ‣ 5.5.1 LLMs as Entity/relation Extractors ‣ 5.5 LLM-augmented KG Question Answering ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">22</span></a>, where LLMs can be used as 1) entity/relation extractors, and 2) answer reasoners.</p>\n</div>\n<section class=\"ltx_subsubsection\" id=\"S5.SS5.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.5.1 </span>LLMs as Entity/relation Extractors</h4>\n<div class=\"ltx_para\" id=\"S5.SS5.SSS1.p1\">\n<p class=\"ltx_p\" id=\"S5.SS5.SSS1.p1.12\">Entity/relation extractors are designed to identify entities and relationships mentioned in natural language questions and retrieve related facts in KGs. Given the proficiency in language comprehension, LLMs can be effectively utilized for this purpose. Lukovnikov et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib172\" title=\"\">172</a>]</cite> are the first to utilize LLMs as classifiers for relation prediction, resulting in a notable improvement in performance compared to shallow neural networks. Nan et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib174\" title=\"\">174</a>]</cite> introduce two LLM-based KGQA frameworks that adopt LLMs to detect mentioned entities and relations. Then, they query the answer in KGs using the extracted entity-relation pairs. QA-GNN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib131\" title=\"\">131</a>]</cite> uses LLMs to encode the question and candidate answer pairs, which are adopted to estimate the importance of relative KG entities. The entities are retrieved to form a subgraph, where an answer reasoning is conducted by a graph neural network.\nLuo et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib173\" title=\"\">173</a>]</cite> use LLMs to calculate the similarities between relations and questions to retrieve related facts, formulated as</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S5.E12\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"s(r,q)=\\text{LLM}(r)^{\\top}\\text{LLM}(q),\" class=\"ltx_Math\" display=\"block\" id=\"S5.E12.m1.5\"><semantics id=\"S5.E12.m1.5a\"><mrow id=\"S5.E12.m1.5.5.1\" xref=\"S5.E12.m1.5.5.1.1.cmml\"><mrow id=\"S5.E12.m1.5.5.1.1\" xref=\"S5.E12.m1.5.5.1.1.cmml\"><mrow id=\"S5.E12.m1.5.5.1.1.2\" xref=\"S5.E12.m1.5.5.1.1.2.cmml\"><mi id=\"S5.E12.m1.5.5.1.1.2.2\" xref=\"S5.E12.m1.5.5.1.1.2.2.cmml\">s</mi><mo id=\"S5.E12.m1.5.5.1.1.2.1\" xref=\"S5.E12.m1.5.5.1.1.2.1.cmml\">⁢</mo><mrow id=\"S5.E12.m1.5.5.1.1.2.3.2\" xref=\"S5.E12.m1.5.5.1.1.2.3.1.cmml\"><mo id=\"S5.E12.m1.5.5.1.1.2.3.2.1\" stretchy=\"false\" xref=\"S5.E12.m1.5.5.1.1.2.3.1.cmml\">(</mo><mi id=\"S5.E12.m1.1.1\" xref=\"S5.E12.m1.1.1.cmml\">r</mi><mo id=\"S5.E12.m1.5.5.1.1.2.3.2.2\" xref=\"S5.E12.m1.5.5.1.1.2.3.1.cmml\">,</mo><mi id=\"S5.E12.m1.2.2\" xref=\"S5.E12.m1.2.2.cmml\">q</mi><mo id=\"S5.E12.m1.5.5.1.1.2.3.2.3\" stretchy=\"false\" xref=\"S5.E12.m1.5.5.1.1.2.3.1.cmml\">)</mo></mrow></mrow><mo id=\"S5.E12.m1.5.5.1.1.1\" xref=\"S5.E12.m1.5.5.1.1.1.cmml\">=</mo><mrow id=\"S5.E12.m1.5.5.1.1.3\" xref=\"S5.E12.m1.5.5.1.1.3.cmml\"><mtext id=\"S5.E12.m1.5.5.1.1.3.2\" xref=\"S5.E12.m1.5.5.1.1.3.2a.cmml\">LLM</mtext><mo id=\"S5.E12.m1.5.5.1.1.3.1\" xref=\"S5.E12.m1.5.5.1.1.3.1.cmml\">⁢</mo><msup id=\"S5.E12.m1.5.5.1.1.3.3\" xref=\"S5.E12.m1.5.5.1.1.3.3.cmml\"><mrow id=\"S5.E12.m1.5.5.1.1.3.3.2.2\" xref=\"S5.E12.m1.5.5.1.1.3.3.cmml\"><mo id=\"S5.E12.m1.5.5.1.1.3.3.2.2.1\" stretchy=\"false\" xref=\"S5.E12.m1.5.5.1.1.3.3.cmml\">(</mo><mi id=\"S5.E12.m1.3.3\" xref=\"S5.E12.m1.3.3.cmml\">r</mi><mo id=\"S5.E12.m1.5.5.1.1.3.3.2.2.2\" stretchy=\"false\" xref=\"S5.E12.m1.5.5.1.1.3.3.cmml\">)</mo></mrow><mo id=\"S5.E12.m1.5.5.1.1.3.3.3\" xref=\"S5.E12.m1.5.5.1.1.3.3.3.cmml\">⊤</mo></msup><mo id=\"S5.E12.m1.5.5.1.1.3.1a\" xref=\"S5.E12.m1.5.5.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E12.m1.5.5.1.1.3.4\" xref=\"S5.E12.m1.5.5.1.1.3.4a.cmml\">LLM</mtext><mo id=\"S5.E12.m1.5.5.1.1.3.1b\" xref=\"S5.E12.m1.5.5.1.1.3.1.cmml\">⁢</mo><mrow id=\"S5.E12.m1.5.5.1.1.3.5.2\" xref=\"S5.E12.m1.5.5.1.1.3.cmml\"><mo id=\"S5.E12.m1.5.5.1.1.3.5.2.1\" stretchy=\"false\" xref=\"S5.E12.m1.5.5.1.1.3.cmml\">(</mo><mi id=\"S5.E12.m1.4.4\" xref=\"S5.E12.m1.4.4.cmml\">q</mi><mo id=\"S5.E12.m1.5.5.1.1.3.5.2.2\" stretchy=\"false\" xref=\"S5.E12.m1.5.5.1.1.3.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E12.m1.5.5.1.2\" xref=\"S5.E12.m1.5.5.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E12.m1.5b\"><apply id=\"S5.E12.m1.5.5.1.1.cmml\" xref=\"S5.E12.m1.5.5.1\"><eq id=\"S5.E12.m1.5.5.1.1.1.cmml\" xref=\"S5.E12.m1.5.5.1.1.1\"></eq><apply id=\"S5.E12.m1.5.5.1.1.2.cmml\" xref=\"S5.E12.m1.5.5.1.1.2\"><times id=\"S5.E12.m1.5.5.1.1.2.1.cmml\" xref=\"S5.E12.m1.5.5.1.1.2.1\"></times><ci id=\"S5.E12.m1.5.5.1.1.2.2.cmml\" xref=\"S5.E12.m1.5.5.1.1.2.2\">𝑠</ci><interval closure=\"open\" id=\"S5.E12.m1.5.5.1.1.2.3.1.cmml\" xref=\"S5.E12.m1.5.5.1.1.2.3.2\"><ci id=\"S5.E12.m1.1.1.cmml\" xref=\"S5.E12.m1.1.1\">𝑟</ci><ci id=\"S5.E12.m1.2.2.cmml\" xref=\"S5.E12.m1.2.2\">𝑞</ci></interval></apply><apply id=\"S5.E12.m1.5.5.1.1.3.cmml\" xref=\"S5.E12.m1.5.5.1.1.3\"><times id=\"S5.E12.m1.5.5.1.1.3.1.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.1\"></times><ci id=\"S5.E12.m1.5.5.1.1.3.2a.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.2\"><mtext id=\"S5.E12.m1.5.5.1.1.3.2.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.2\">LLM</mtext></ci><apply id=\"S5.E12.m1.5.5.1.1.3.3.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E12.m1.5.5.1.1.3.3.1.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.3\">superscript</csymbol><ci id=\"S5.E12.m1.3.3.cmml\" xref=\"S5.E12.m1.3.3\">𝑟</ci><csymbol cd=\"latexml\" id=\"S5.E12.m1.5.5.1.1.3.3.3.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.3.3\">top</csymbol></apply><ci id=\"S5.E12.m1.5.5.1.1.3.4a.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.4\"><mtext id=\"S5.E12.m1.5.5.1.1.3.4.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.4\">LLM</mtext></ci><ci id=\"S5.E12.m1.4.4.cmml\" xref=\"S5.E12.m1.4.4\">𝑞</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E12.m1.5c\">s(r,q)=\\text{LLM}(r)^{\\top}\\text{LLM}(q),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E12.m1.5d\">italic_s ( italic_r , italic_q ) = LLM ( italic_r ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT LLM ( italic_q ) ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(12)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS5.SSS1.p1.5\">where <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS1.p1.1.m1.1\"><semantics id=\"S5.SS5.SSS1.p1.1.m1.1a\"><mi id=\"S5.SS5.SSS1.p1.1.m1.1.1\" xref=\"S5.SS5.SSS1.p1.1.m1.1.1.cmml\">q</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.1.m1.1b\"><ci id=\"S5.SS5.SSS1.p1.1.m1.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.1.m1.1.1\">𝑞</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.1.m1.1c\">q</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.1.m1.1d\">italic_q</annotation></semantics></math> denotes the question, <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS1.p1.2.m2.1\"><semantics id=\"S5.SS5.SSS1.p1.2.m2.1a\"><mi id=\"S5.SS5.SSS1.p1.2.m2.1.1\" xref=\"S5.SS5.SSS1.p1.2.m2.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.2.m2.1b\"><ci id=\"S5.SS5.SSS1.p1.2.m2.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.2.m2.1.1\">𝑟</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.2.m2.1c\">r</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.2.m2.1d\">italic_r</annotation></semantics></math> denotes the relation, and <math alttext=\"\\text{LLM}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS1.p1.3.m3.1\"><semantics id=\"S5.SS5.SSS1.p1.3.m3.1a\"><mrow id=\"S5.SS5.SSS1.p1.3.m3.1.2\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.cmml\"><mtext id=\"S5.SS5.SSS1.p1.3.m3.1.2.2\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.2a.cmml\">LLM</mtext><mo id=\"S5.SS5.SSS1.p1.3.m3.1.2.1\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.1.cmml\">⁢</mo><mrow id=\"S5.SS5.SSS1.p1.3.m3.1.2.3.2\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.cmml\"><mo id=\"S5.SS5.SSS1.p1.3.m3.1.2.3.2.1\" stretchy=\"false\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.cmml\">(</mo><mo id=\"S5.SS5.SSS1.p1.3.m3.1.1\" lspace=\"0em\" rspace=\"0em\" xref=\"S5.SS5.SSS1.p1.3.m3.1.1.cmml\">⋅</mo><mo id=\"S5.SS5.SSS1.p1.3.m3.1.2.3.2.2\" stretchy=\"false\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.3.m3.1b\"><apply id=\"S5.SS5.SSS1.p1.3.m3.1.2.cmml\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2\"><times id=\"S5.SS5.SSS1.p1.3.m3.1.2.1.cmml\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.1\"></times><ci id=\"S5.SS5.SSS1.p1.3.m3.1.2.2a.cmml\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.2\"><mtext id=\"S5.SS5.SSS1.p1.3.m3.1.2.2.cmml\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.2\">LLM</mtext></ci><ci id=\"S5.SS5.SSS1.p1.3.m3.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.3.m3.1.1\">⋅</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.3.m3.1c\">\\text{LLM}(\\cdot)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.3.m3.1d\">LLM ( ⋅ )</annotation></semantics></math> would generate representation for <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS1.p1.4.m4.1\"><semantics id=\"S5.SS5.SSS1.p1.4.m4.1a\"><mi id=\"S5.SS5.SSS1.p1.4.m4.1.1\" xref=\"S5.SS5.SSS1.p1.4.m4.1.1.cmml\">q</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.4.m4.1b\"><ci id=\"S5.SS5.SSS1.p1.4.m4.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.4.m4.1.1\">𝑞</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.4.m4.1c\">q</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.4.m4.1d\">italic_q</annotation></semantics></math> and <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS1.p1.5.m5.1\"><semantics id=\"S5.SS5.SSS1.p1.5.m5.1a\"><mi id=\"S5.SS5.SSS1.p1.5.m5.1.1\" xref=\"S5.SS5.SSS1.p1.5.m5.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.5.m5.1b\"><ci id=\"S5.SS5.SSS1.p1.5.m5.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.5.m5.1.1\">𝑟</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.5.m5.1c\">r</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.5.m5.1d\">italic_r</annotation></semantics></math>, respectively. Furthermore, Zhang et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib236\" title=\"\">236</a>]</cite> propose a LLM-based path retriever to retrieve question-related relations hop-by-hop and construct several paths. The probability of each path can be calculated as</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S5.E13\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"P(p|q)=\\prod_{t=1}^{|p|}s(r_{t},q),\" class=\"ltx_Math\" display=\"block\" id=\"S5.E13.m1.3\"><semantics id=\"S5.E13.m1.3a\"><mrow id=\"S5.E13.m1.3.3.1\" xref=\"S5.E13.m1.3.3.1.1.cmml\"><mrow id=\"S5.E13.m1.3.3.1.1\" xref=\"S5.E13.m1.3.3.1.1.cmml\"><mrow id=\"S5.E13.m1.3.3.1.1.1\" xref=\"S5.E13.m1.3.3.1.1.1.cmml\"><mi id=\"S5.E13.m1.3.3.1.1.1.3\" xref=\"S5.E13.m1.3.3.1.1.1.3.cmml\">P</mi><mo id=\"S5.E13.m1.3.3.1.1.1.2\" xref=\"S5.E13.m1.3.3.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E13.m1.3.3.1.1.1.1.1\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.cmml\"><mo id=\"S5.E13.m1.3.3.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S5.E13.m1.3.3.1.1.1.1.1.1\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.cmml\"><mi id=\"S5.E13.m1.3.3.1.1.1.1.1.1.2\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.2.cmml\">p</mi><mo fence=\"false\" id=\"S5.E13.m1.3.3.1.1.1.1.1.1.1\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.1.cmml\">|</mo><mi id=\"S5.E13.m1.3.3.1.1.1.1.1.1.3\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.3.cmml\">q</mi></mrow><mo id=\"S5.E13.m1.3.3.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo id=\"S5.E13.m1.3.3.1.1.3\" rspace=\"0.111em\" xref=\"S5.E13.m1.3.3.1.1.3.cmml\">=</mo><mrow id=\"S5.E13.m1.3.3.1.1.2\" xref=\"S5.E13.m1.3.3.1.1.2.cmml\"><munderover id=\"S5.E13.m1.3.3.1.1.2.2\" xref=\"S5.E13.m1.3.3.1.1.2.2.cmml\"><mo id=\"S5.E13.m1.3.3.1.1.2.2.2.2\" movablelimits=\"false\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.2.cmml\">∏</mo><mrow id=\"S5.E13.m1.3.3.1.1.2.2.2.3\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3.cmml\"><mi id=\"S5.E13.m1.3.3.1.1.2.2.2.3.2\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3.2.cmml\">t</mi><mo id=\"S5.E13.m1.3.3.1.1.2.2.2.3.1\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3.1.cmml\">=</mo><mn id=\"S5.E13.m1.3.3.1.1.2.2.2.3.3\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3.3.cmml\">1</mn></mrow><mrow id=\"S5.E13.m1.1.1.1.3\" xref=\"S5.E13.m1.1.1.1.2.cmml\"><mo id=\"S5.E13.m1.1.1.1.3.1\" stretchy=\"false\" xref=\"S5.E13.m1.1.1.1.2.1.cmml\">|</mo><mi id=\"S5.E13.m1.1.1.1.1\" xref=\"S5.E13.m1.1.1.1.1.cmml\">p</mi><mo id=\"S5.E13.m1.1.1.1.3.2\" stretchy=\"false\" xref=\"S5.E13.m1.1.1.1.2.1.cmml\">|</mo></mrow></munderover><mrow id=\"S5.E13.m1.3.3.1.1.2.1\" xref=\"S5.E13.m1.3.3.1.1.2.1.cmml\"><mi id=\"S5.E13.m1.3.3.1.1.2.1.3\" xref=\"S5.E13.m1.3.3.1.1.2.1.3.cmml\">s</mi><mo id=\"S5.E13.m1.3.3.1.1.2.1.2\" xref=\"S5.E13.m1.3.3.1.1.2.1.2.cmml\">⁢</mo><mrow id=\"S5.E13.m1.3.3.1.1.2.1.1.1\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.2.cmml\"><mo id=\"S5.E13.m1.3.3.1.1.2.1.1.1.2\" stretchy=\"false\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.2.cmml\">(</mo><msub id=\"S5.E13.m1.3.3.1.1.2.1.1.1.1\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.cmml\"><mi id=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.2\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.2.cmml\">r</mi><mi id=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.3\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.3.cmml\">t</mi></msub><mo id=\"S5.E13.m1.3.3.1.1.2.1.1.1.3\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.2.cmml\">,</mo><mi id=\"S5.E13.m1.2.2\" xref=\"S5.E13.m1.2.2.cmml\">q</mi><mo id=\"S5.E13.m1.3.3.1.1.2.1.1.1.4\" stretchy=\"false\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.2.cmml\">)</mo></mrow></mrow></mrow></mrow><mo id=\"S5.E13.m1.3.3.1.2\" xref=\"S5.E13.m1.3.3.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E13.m1.3b\"><apply id=\"S5.E13.m1.3.3.1.1.cmml\" xref=\"S5.E13.m1.3.3.1\"><eq id=\"S5.E13.m1.3.3.1.1.3.cmml\" xref=\"S5.E13.m1.3.3.1.1.3\"></eq><apply id=\"S5.E13.m1.3.3.1.1.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.1\"><times id=\"S5.E13.m1.3.3.1.1.1.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.1.2\"></times><ci id=\"S5.E13.m1.3.3.1.1.1.3.cmml\" xref=\"S5.E13.m1.3.3.1.1.1.3\">𝑃</ci><apply id=\"S5.E13.m1.3.3.1.1.1.1.1.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.1.1.1\"><csymbol cd=\"latexml\" id=\"S5.E13.m1.3.3.1.1.1.1.1.1.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.1\">conditional</csymbol><ci id=\"S5.E13.m1.3.3.1.1.1.1.1.1.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.2\">𝑝</ci><ci id=\"S5.E13.m1.3.3.1.1.1.1.1.1.3.cmml\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.3\">𝑞</ci></apply></apply><apply id=\"S5.E13.m1.3.3.1.1.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2\"><apply id=\"S5.E13.m1.3.3.1.1.2.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E13.m1.3.3.1.1.2.2.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2\">superscript</csymbol><apply id=\"S5.E13.m1.3.3.1.1.2.2.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E13.m1.3.3.1.1.2.2.2.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2\">subscript</csymbol><csymbol cd=\"latexml\" id=\"S5.E13.m1.3.3.1.1.2.2.2.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.2\">product</csymbol><apply id=\"S5.E13.m1.3.3.1.1.2.2.2.3.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3\"><eq id=\"S5.E13.m1.3.3.1.1.2.2.2.3.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3.1\"></eq><ci id=\"S5.E13.m1.3.3.1.1.2.2.2.3.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3.2\">𝑡</ci><cn id=\"S5.E13.m1.3.3.1.1.2.2.2.3.3.cmml\" type=\"integer\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3.3\">1</cn></apply></apply><apply id=\"S5.E13.m1.1.1.1.2.cmml\" xref=\"S5.E13.m1.1.1.1.3\"><abs id=\"S5.E13.m1.1.1.1.2.1.cmml\" xref=\"S5.E13.m1.1.1.1.3.1\"></abs><ci id=\"S5.E13.m1.1.1.1.1.cmml\" xref=\"S5.E13.m1.1.1.1.1\">𝑝</ci></apply></apply><apply id=\"S5.E13.m1.3.3.1.1.2.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1\"><times id=\"S5.E13.m1.3.3.1.1.2.1.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1.2\"></times><ci id=\"S5.E13.m1.3.3.1.1.2.1.3.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1.3\">𝑠</ci><interval closure=\"open\" id=\"S5.E13.m1.3.3.1.1.2.1.1.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1\"><apply id=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1.1\">subscript</csymbol><ci id=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.2\">𝑟</ci><ci id=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.3.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.3\">𝑡</ci></apply><ci id=\"S5.E13.m1.2.2.cmml\" xref=\"S5.E13.m1.2.2\">𝑞</ci></interval></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E13.m1.3c\">P(p|q)=\\prod_{t=1}^{|p|}s(r_{t},q),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E13.m1.3d\">italic_P ( italic_p | italic_q ) = ∏ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_p | end_POSTSUPERSCRIPT italic_s ( italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_q ) ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(13)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS5.SSS1.p1.9\">where <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS1.p1.6.m1.1\"><semantics id=\"S5.SS5.SSS1.p1.6.m1.1a\"><mi id=\"S5.SS5.SSS1.p1.6.m1.1.1\" xref=\"S5.SS5.SSS1.p1.6.m1.1.1.cmml\">p</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.6.m1.1b\"><ci id=\"S5.SS5.SSS1.p1.6.m1.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.6.m1.1.1\">𝑝</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.6.m1.1c\">p</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.6.m1.1d\">italic_p</annotation></semantics></math> denotes the path, and <math alttext=\"r_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS1.p1.7.m2.1\"><semantics id=\"S5.SS5.SSS1.p1.7.m2.1a\"><msub id=\"S5.SS5.SSS1.p1.7.m2.1.1\" xref=\"S5.SS5.SSS1.p1.7.m2.1.1.cmml\"><mi id=\"S5.SS5.SSS1.p1.7.m2.1.1.2\" xref=\"S5.SS5.SSS1.p1.7.m2.1.1.2.cmml\">r</mi><mi id=\"S5.SS5.SSS1.p1.7.m2.1.1.3\" xref=\"S5.SS5.SSS1.p1.7.m2.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.7.m2.1b\"><apply id=\"S5.SS5.SSS1.p1.7.m2.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.7.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS5.SSS1.p1.7.m2.1.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.7.m2.1.1\">subscript</csymbol><ci id=\"S5.SS5.SSS1.p1.7.m2.1.1.2.cmml\" xref=\"S5.SS5.SSS1.p1.7.m2.1.1.2\">𝑟</ci><ci id=\"S5.SS5.SSS1.p1.7.m2.1.1.3.cmml\" xref=\"S5.SS5.SSS1.p1.7.m2.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.7.m2.1c\">r_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.7.m2.1d\">italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> denotes the relation at the <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS1.p1.8.m3.1\"><semantics id=\"S5.SS5.SSS1.p1.8.m3.1a\"><mi id=\"S5.SS5.SSS1.p1.8.m3.1.1\" xref=\"S5.SS5.SSS1.p1.8.m3.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.8.m3.1b\"><ci id=\"S5.SS5.SSS1.p1.8.m3.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.8.m3.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.8.m3.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.8.m3.1d\">italic_t</annotation></semantics></math>-th hop of <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS1.p1.9.m4.1\"><semantics id=\"S5.SS5.SSS1.p1.9.m4.1a\"><mi id=\"S5.SS5.SSS1.p1.9.m4.1.1\" xref=\"S5.SS5.SSS1.p1.9.m4.1.1.cmml\">p</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.9.m4.1b\"><ci id=\"S5.SS5.SSS1.p1.9.m4.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.9.m4.1.1\">𝑝</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.9.m4.1c\">p</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.9.m4.1d\">italic_p</annotation></semantics></math>. The retrieved relations and paths can be used as context knowledge to improve the performance of answer reasoners as</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S5.E14\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"P(a|q)=\\sum_{p\\in\\mathcal{P}}P(a|p)P(p|q),\" class=\"ltx_Math\" display=\"block\" id=\"S5.E14.m1.1\"><semantics id=\"S5.E14.m1.1a\"><mrow id=\"S5.E14.m1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.cmml\"><mrow id=\"S5.E14.m1.1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.cmml\"><mrow id=\"S5.E14.m1.1.1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.1.cmml\"><mi id=\"S5.E14.m1.1.1.1.1.1.3\" xref=\"S5.E14.m1.1.1.1.1.1.3.cmml\">P</mi><mo id=\"S5.E14.m1.1.1.1.1.1.2\" xref=\"S5.E14.m1.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E14.m1.1.1.1.1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S5.E14.m1.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S5.E14.m1.1.1.1.1.1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S5.E14.m1.1.1.1.1.1.1.1.1.2\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.2.cmml\">a</mi><mo fence=\"false\" id=\"S5.E14.m1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.1.cmml\">|</mo><mi id=\"S5.E14.m1.1.1.1.1.1.1.1.1.3\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.3.cmml\">q</mi></mrow><mo id=\"S5.E14.m1.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo id=\"S5.E14.m1.1.1.1.1.4\" rspace=\"0.111em\" xref=\"S5.E14.m1.1.1.1.1.4.cmml\">=</mo><mrow id=\"S5.E14.m1.1.1.1.1.3\" xref=\"S5.E14.m1.1.1.1.1.3.cmml\"><munder id=\"S5.E14.m1.1.1.1.1.3.3\" xref=\"S5.E14.m1.1.1.1.1.3.3.cmml\"><mo id=\"S5.E14.m1.1.1.1.1.3.3.2\" movablelimits=\"false\" xref=\"S5.E14.m1.1.1.1.1.3.3.2.cmml\">∑</mo><mrow id=\"S5.E14.m1.1.1.1.1.3.3.3\" xref=\"S5.E14.m1.1.1.1.1.3.3.3.cmml\"><mi id=\"S5.E14.m1.1.1.1.1.3.3.3.2\" xref=\"S5.E14.m1.1.1.1.1.3.3.3.2.cmml\">p</mi><mo id=\"S5.E14.m1.1.1.1.1.3.3.3.1\" xref=\"S5.E14.m1.1.1.1.1.3.3.3.1.cmml\">∈</mo><mi class=\"ltx_font_mathcaligraphic\" id=\"S5.E14.m1.1.1.1.1.3.3.3.3\" xref=\"S5.E14.m1.1.1.1.1.3.3.3.3.cmml\">𝒫</mi></mrow></munder><mrow id=\"S5.E14.m1.1.1.1.1.3.2\" xref=\"S5.E14.m1.1.1.1.1.3.2.cmml\"><mi id=\"S5.E14.m1.1.1.1.1.3.2.4\" xref=\"S5.E14.m1.1.1.1.1.3.2.4.cmml\">P</mi><mo id=\"S5.E14.m1.1.1.1.1.3.2.3\" xref=\"S5.E14.m1.1.1.1.1.3.2.3.cmml\">⁢</mo><mrow id=\"S5.E14.m1.1.1.1.1.2.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.cmml\"><mo id=\"S5.E14.m1.1.1.1.1.2.1.1.1.2\" stretchy=\"false\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.cmml\">(</mo><mrow id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.cmml\"><mi id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.2\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.2.cmml\">a</mi><mo fence=\"false\" id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.1.cmml\">|</mo><mi id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.3\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.3.cmml\">p</mi></mrow><mo id=\"S5.E14.m1.1.1.1.1.2.1.1.1.3\" stretchy=\"false\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.cmml\">)</mo></mrow><mo id=\"S5.E14.m1.1.1.1.1.3.2.3a\" xref=\"S5.E14.m1.1.1.1.1.3.2.3.cmml\">⁢</mo><mi id=\"S5.E14.m1.1.1.1.1.3.2.5\" xref=\"S5.E14.m1.1.1.1.1.3.2.5.cmml\">P</mi><mo id=\"S5.E14.m1.1.1.1.1.3.2.3b\" xref=\"S5.E14.m1.1.1.1.1.3.2.3.cmml\">⁢</mo><mrow id=\"S5.E14.m1.1.1.1.1.3.2.2.1\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.cmml\"><mo id=\"S5.E14.m1.1.1.1.1.3.2.2.1.2\" stretchy=\"false\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.cmml\">(</mo><mrow id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.cmml\"><mi id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.2\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.2.cmml\">p</mi><mo fence=\"false\" id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.1.cmml\">|</mo><mi id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.3\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.3.cmml\">q</mi></mrow><mo id=\"S5.E14.m1.1.1.1.1.3.2.2.1.3\" stretchy=\"false\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.cmml\">)</mo></mrow></mrow></mrow></mrow><mo id=\"S5.E14.m1.1.1.1.2\" xref=\"S5.E14.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E14.m1.1b\"><apply id=\"S5.E14.m1.1.1.1.1.cmml\" xref=\"S5.E14.m1.1.1.1\"><eq id=\"S5.E14.m1.1.1.1.1.4.cmml\" xref=\"S5.E14.m1.1.1.1.1.4\"></eq><apply id=\"S5.E14.m1.1.1.1.1.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.1\"><times id=\"S5.E14.m1.1.1.1.1.1.2.cmml\" xref=\"S5.E14.m1.1.1.1.1.1.2\"></times><ci id=\"S5.E14.m1.1.1.1.1.1.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.1.3\">𝑃</ci><apply id=\"S5.E14.m1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.1.1.1\"><csymbol cd=\"latexml\" id=\"S5.E14.m1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.1\">conditional</csymbol><ci id=\"S5.E14.m1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.2\">𝑎</ci><ci id=\"S5.E14.m1.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.3\">𝑞</ci></apply></apply><apply id=\"S5.E14.m1.1.1.1.1.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.3\"><apply id=\"S5.E14.m1.1.1.1.1.3.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E14.m1.1.1.1.1.3.3.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.3\">subscript</csymbol><sum id=\"S5.E14.m1.1.1.1.1.3.3.2.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.3.2\"></sum><apply id=\"S5.E14.m1.1.1.1.1.3.3.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.3.3\"><in id=\"S5.E14.m1.1.1.1.1.3.3.3.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.3.3.1\"></in><ci id=\"S5.E14.m1.1.1.1.1.3.3.3.2.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.3.3.2\">𝑝</ci><ci id=\"S5.E14.m1.1.1.1.1.3.3.3.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.3.3.3\">𝒫</ci></apply></apply><apply id=\"S5.E14.m1.1.1.1.1.3.2.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2\"><times id=\"S5.E14.m1.1.1.1.1.3.2.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2.3\"></times><ci id=\"S5.E14.m1.1.1.1.1.3.2.4.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2.4\">𝑃</ci><apply id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1\"><csymbol cd=\"latexml\" id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.1\">conditional</csymbol><ci id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.2.cmml\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.2\">𝑎</ci><ci id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.3\">𝑝</ci></apply><ci id=\"S5.E14.m1.1.1.1.1.3.2.5.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2.5\">𝑃</ci><apply id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1\"><csymbol cd=\"latexml\" id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.1\">conditional</csymbol><ci id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.2.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.2\">𝑝</ci><ci id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.3\">𝑞</ci></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E14.m1.1c\">P(a|q)=\\sum_{p\\in\\mathcal{P}}P(a|p)P(p|q),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E14.m1.1d\">italic_P ( italic_a | italic_q ) = ∑ start_POSTSUBSCRIPT italic_p ∈ caligraphic_P end_POSTSUBSCRIPT italic_P ( italic_a | italic_p ) italic_P ( italic_p | italic_q ) ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(14)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS5.SSS1.p1.11\">where <math alttext=\"\\mathcal{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS1.p1.10.m1.1\"><semantics id=\"S5.SS5.SSS1.p1.10.m1.1a\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S5.SS5.SSS1.p1.10.m1.1.1\" xref=\"S5.SS5.SSS1.p1.10.m1.1.1.cmml\">𝒫</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.10.m1.1b\"><ci id=\"S5.SS5.SSS1.p1.10.m1.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.10.m1.1.1\">𝒫</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.10.m1.1c\">\\mathcal{P}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.10.m1.1d\">caligraphic_P</annotation></semantics></math> denotes retrieved paths and <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS1.p1.11.m2.1\"><semantics id=\"S5.SS5.SSS1.p1.11.m2.1a\"><mi id=\"S5.SS5.SSS1.p1.11.m2.1.1\" xref=\"S5.SS5.SSS1.p1.11.m2.1.1.cmml\">a</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.11.m2.1b\"><ci id=\"S5.SS5.SSS1.p1.11.m2.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.11.m2.1.1\">𝑎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.11.m2.1c\">a</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.11.m2.1d\">italic_a</annotation></semantics></math> denotes the answer.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F22\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"536\" id=\"S5.F22.g1\" src=\"x19.png\" width=\"664\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 22: </span>The general framework of applying LLMs for knowledge graph question answering (KGQA).</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S5.SS5.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">5.5.2 </span>LLMs as Answer Reasoners</h4>\n<div class=\"ltx_para\" id=\"S5.SS5.SSS2.p1\">\n<p class=\"ltx_p\" id=\"S5.SS5.SSS2.p1.13\">Answer reasoners are designed to reason over the retrieved facts and generate answers. LLMs can be used as answer reasoners to generate answers directly. For example, as shown in Fig. 3 <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.F22\" title=\"Figure 22 ‣ 5.5.1 LLMs as Entity/relation Extractors ‣ 5.5 LLM-augmented KG Question Answering ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">22</span></a>, DEKCOR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib175\" title=\"\">175</a>]</cite> concatenates the retrieved facts with questions and candidate answers as</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S5.E15\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"x=\\texttt{[CLS]}\\ q\\ \\texttt{[SEP]}\\ \\text{Related Facts}\\ \\texttt{[SEP]}\\ a\\ %\n\\texttt{[SEP]},\" class=\"ltx_Math\" display=\"block\" id=\"S5.E15.m1.1\"><semantics id=\"S5.E15.m1.1a\"><mrow id=\"S5.E15.m1.1.1.1\" xref=\"S5.E15.m1.1.1.1.1.cmml\"><mrow id=\"S5.E15.m1.1.1.1.1\" xref=\"S5.E15.m1.1.1.1.1.cmml\"><mi id=\"S5.E15.m1.1.1.1.1.2\" xref=\"S5.E15.m1.1.1.1.1.2.cmml\">x</mi><mo id=\"S5.E15.m1.1.1.1.1.1\" xref=\"S5.E15.m1.1.1.1.1.1.cmml\">=</mo><mrow id=\"S5.E15.m1.1.1.1.1.3\" xref=\"S5.E15.m1.1.1.1.1.3.cmml\"><mtext id=\"S5.E15.m1.1.1.1.1.3.2\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.2a.cmml\">[CLS]</mtext><mo id=\"S5.E15.m1.1.1.1.1.3.1\" lspace=\"0.500em\" xref=\"S5.E15.m1.1.1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.E15.m1.1.1.1.1.3.3\" xref=\"S5.E15.m1.1.1.1.1.3.3.cmml\">q</mi><mo id=\"S5.E15.m1.1.1.1.1.3.1a\" lspace=\"0.500em\" xref=\"S5.E15.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E15.m1.1.1.1.1.3.4\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.4a.cmml\">[SEP]</mtext><mo id=\"S5.E15.m1.1.1.1.1.3.1b\" lspace=\"0.500em\" xref=\"S5.E15.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E15.m1.1.1.1.1.3.5\" xref=\"S5.E15.m1.1.1.1.1.3.5a.cmml\">Related Facts</mtext><mo id=\"S5.E15.m1.1.1.1.1.3.1c\" lspace=\"0.500em\" xref=\"S5.E15.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E15.m1.1.1.1.1.3.6\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.6a.cmml\">[SEP]</mtext><mo id=\"S5.E15.m1.1.1.1.1.3.1d\" lspace=\"0.500em\" xref=\"S5.E15.m1.1.1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.E15.m1.1.1.1.1.3.7\" xref=\"S5.E15.m1.1.1.1.1.3.7.cmml\">a</mi><mo id=\"S5.E15.m1.1.1.1.1.3.1e\" lspace=\"0.500em\" xref=\"S5.E15.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E15.m1.1.1.1.1.3.8\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.8a.cmml\">[SEP]</mtext></mrow></mrow><mo id=\"S5.E15.m1.1.1.1.2\" xref=\"S5.E15.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E15.m1.1b\"><apply id=\"S5.E15.m1.1.1.1.1.cmml\" xref=\"S5.E15.m1.1.1.1\"><eq id=\"S5.E15.m1.1.1.1.1.1.cmml\" xref=\"S5.E15.m1.1.1.1.1.1\"></eq><ci id=\"S5.E15.m1.1.1.1.1.2.cmml\" xref=\"S5.E15.m1.1.1.1.1.2\">𝑥</ci><apply id=\"S5.E15.m1.1.1.1.1.3.cmml\" xref=\"S5.E15.m1.1.1.1.1.3\"><times id=\"S5.E15.m1.1.1.1.1.3.1.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.1\"></times><ci id=\"S5.E15.m1.1.1.1.1.3.2a.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.2\"><mtext id=\"S5.E15.m1.1.1.1.1.3.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.2\">[CLS]</mtext></ci><ci id=\"S5.E15.m1.1.1.1.1.3.3.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.3\">𝑞</ci><ci id=\"S5.E15.m1.1.1.1.1.3.4a.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.4\"><mtext id=\"S5.E15.m1.1.1.1.1.3.4.cmml\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.4\">[SEP]</mtext></ci><ci id=\"S5.E15.m1.1.1.1.1.3.5a.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.5\"><mtext id=\"S5.E15.m1.1.1.1.1.3.5.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.5\">Related Facts</mtext></ci><ci id=\"S5.E15.m1.1.1.1.1.3.6a.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.6\"><mtext id=\"S5.E15.m1.1.1.1.1.3.6.cmml\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.6\">[SEP]</mtext></ci><ci id=\"S5.E15.m1.1.1.1.1.3.7.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.7\">𝑎</ci><ci id=\"S5.E15.m1.1.1.1.1.3.8a.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.8\"><mtext id=\"S5.E15.m1.1.1.1.1.3.8.cmml\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.8\">[SEP]</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E15.m1.1c\">x=\\texttt{[CLS]}\\ q\\ \\texttt{[SEP]}\\ \\text{Related Facts}\\ \\texttt{[SEP]}\\ a\\ %\n\\texttt{[SEP]},</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E15.m1.1d\">italic_x = [CLS] italic_q [SEP] Related Facts [SEP] italic_a [SEP] ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(15)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS5.SSS2.p1.6\">where <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS2.p1.1.m1.1\"><semantics id=\"S5.SS5.SSS2.p1.1.m1.1a\"><mi id=\"S5.SS5.SSS2.p1.1.m1.1.1\" xref=\"S5.SS5.SSS2.p1.1.m1.1.1.cmml\">a</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.1.m1.1b\"><ci id=\"S5.SS5.SSS2.p1.1.m1.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.1.m1.1.1\">𝑎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.1.m1.1c\">a</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.1.m1.1d\">italic_a</annotation></semantics></math> denotes candidate answers. Then, it feeds them into LLMs to predict answer scores. After utilizing LLMs to generate the representation of <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS2.p1.2.m2.1\"><semantics id=\"S5.SS5.SSS2.p1.2.m2.1a\"><mi id=\"S5.SS5.SSS2.p1.2.m2.1.1\" xref=\"S5.SS5.SSS2.p1.2.m2.1.1.cmml\">x</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.2.m2.1b\"><ci id=\"S5.SS5.SSS2.p1.2.m2.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.2.m2.1.1\">𝑥</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.2.m2.1c\">x</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.2.m2.1d\">italic_x</annotation></semantics></math> as QA context, DRLK <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib176\" title=\"\">176</a>]</cite> proposes a Dynamic Hierarchical Reasoner to capture the interactions between QA context and answers for answer prediction. Yan et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib235\" title=\"\">235</a>]</cite> propose a LLM-based KGQA framework consisting of two stages: (1) retrieve related facts from KGs and (2) generate answers based on the retrieved facts. The first stage is similar to the entity/relation extractors. Given a candidate answer entity <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS2.p1.3.m3.1\"><semantics id=\"S5.SS5.SSS2.p1.3.m3.1a\"><mi id=\"S5.SS5.SSS2.p1.3.m3.1.1\" xref=\"S5.SS5.SSS2.p1.3.m3.1.1.cmml\">a</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.3.m3.1b\"><ci id=\"S5.SS5.SSS2.p1.3.m3.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.3.m3.1.1\">𝑎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.3.m3.1c\">a</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.3.m3.1d\">italic_a</annotation></semantics></math>, it extracts a series of paths <math alttext=\"p_{1},\\ldots,p_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS2.p1.4.m4.3\"><semantics id=\"S5.SS5.SSS2.p1.4.m4.3a\"><mrow id=\"S5.SS5.SSS2.p1.4.m4.3.3.2\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.3.cmml\"><msub id=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1\" xref=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.cmml\"><mi id=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.2\" xref=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.2.cmml\">p</mi><mn id=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.3\" xref=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.3.cmml\">1</mn></msub><mo id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.3\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.3.cmml\">,</mo><mi id=\"S5.SS5.SSS2.p1.4.m4.1.1\" mathvariant=\"normal\" xref=\"S5.SS5.SSS2.p1.4.m4.1.1.cmml\">…</mi><mo id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.4\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.3.cmml\">,</mo><msub id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.cmml\"><mi id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.2\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.2.cmml\">p</mi><mi id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.3\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.3.cmml\">n</mi></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.4.m4.3b\"><list id=\"S5.SS5.SSS2.p1.4.m4.3.3.3.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2\"><apply id=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1\">subscript</csymbol><ci id=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.2.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.2\">𝑝</ci><cn id=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.3.cmml\" type=\"integer\" xref=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.3\">1</cn></apply><ci id=\"S5.SS5.SSS2.p1.4.m4.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.1.1\">…</ci><apply id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.1.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2\">subscript</csymbol><ci id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.2.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.2\">𝑝</ci><ci id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.3.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.3\">𝑛</ci></apply></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.4.m4.3c\">p_{1},\\ldots,p_{n}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.4.m4.3d\">italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> from KGs. But the second stage is a LLM-based answer reasoner. It first verbalizes the paths by using the entity names and relation names in KGs. Then, it concatenates the question <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS2.p1.5.m5.1\"><semantics id=\"S5.SS5.SSS2.p1.5.m5.1a\"><mi id=\"S5.SS5.SSS2.p1.5.m5.1.1\" xref=\"S5.SS5.SSS2.p1.5.m5.1.1.cmml\">q</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.5.m5.1b\"><ci id=\"S5.SS5.SSS2.p1.5.m5.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.5.m5.1.1\">𝑞</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.5.m5.1c\">q</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.5.m5.1d\">italic_q</annotation></semantics></math> and all paths <math alttext=\"p_{1},\\ldots,p_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS2.p1.6.m6.3\"><semantics id=\"S5.SS5.SSS2.p1.6.m6.3a\"><mrow id=\"S5.SS5.SSS2.p1.6.m6.3.3.2\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.3.cmml\"><msub id=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1\" xref=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.cmml\"><mi id=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.2\" xref=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.2.cmml\">p</mi><mn id=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.3\" xref=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.3.cmml\">1</mn></msub><mo id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.3\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.3.cmml\">,</mo><mi id=\"S5.SS5.SSS2.p1.6.m6.1.1\" mathvariant=\"normal\" xref=\"S5.SS5.SSS2.p1.6.m6.1.1.cmml\">…</mi><mo id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.4\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.3.cmml\">,</mo><msub id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.cmml\"><mi id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.2\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.2.cmml\">p</mi><mi id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.3\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.3.cmml\">n</mi></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.6.m6.3b\"><list id=\"S5.SS5.SSS2.p1.6.m6.3.3.3.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2\"><apply id=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1\">subscript</csymbol><ci id=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.2.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.2\">𝑝</ci><cn id=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.3.cmml\" type=\"integer\" xref=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.3\">1</cn></apply><ci id=\"S5.SS5.SSS2.p1.6.m6.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.1.1\">…</ci><apply id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.1.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2\">subscript</csymbol><ci id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.2.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.2\">𝑝</ci><ci id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.3.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.3\">𝑛</ci></apply></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.6.m6.3c\">p_{1},\\ldots,p_{n}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.6.m6.3d\">italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> to make an input sample as</p>\n<table class=\"ltx_equation ltx_eqn_table\" id=\"S5.E16\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"x=\\texttt{[CLS]}\\ q\\ \\texttt{[SEP]}\\ p_{1}\\ \\texttt{[SEP]}\\ \\cdots\\ \\texttt{[%\nSEP]}\\ p_{n}\\ \\texttt{[SEP]}.\" class=\"ltx_Math\" display=\"block\" id=\"S5.E16.m1.1\"><semantics id=\"S5.E16.m1.1a\"><mrow id=\"S5.E16.m1.1.1.1\" xref=\"S5.E16.m1.1.1.1.1.cmml\"><mrow id=\"S5.E16.m1.1.1.1.1\" xref=\"S5.E16.m1.1.1.1.1.cmml\"><mi id=\"S5.E16.m1.1.1.1.1.2\" xref=\"S5.E16.m1.1.1.1.1.2.cmml\">x</mi><mo id=\"S5.E16.m1.1.1.1.1.1\" xref=\"S5.E16.m1.1.1.1.1.1.cmml\">=</mo><mrow id=\"S5.E16.m1.1.1.1.1.3\" xref=\"S5.E16.m1.1.1.1.1.3.cmml\"><mtext id=\"S5.E16.m1.1.1.1.1.3.2\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.2a.cmml\">[CLS]</mtext><mo id=\"S5.E16.m1.1.1.1.1.3.1\" lspace=\"0.500em\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.E16.m1.1.1.1.1.3.3\" xref=\"S5.E16.m1.1.1.1.1.3.3.cmml\">q</mi><mo id=\"S5.E16.m1.1.1.1.1.3.1a\" lspace=\"0.500em\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E16.m1.1.1.1.1.3.4\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.4a.cmml\">[SEP]</mtext><mo id=\"S5.E16.m1.1.1.1.1.3.1b\" lspace=\"0.500em\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E16.m1.1.1.1.1.3.5\" xref=\"S5.E16.m1.1.1.1.1.3.5.cmml\"><mi id=\"S5.E16.m1.1.1.1.1.3.5.2\" xref=\"S5.E16.m1.1.1.1.1.3.5.2.cmml\">p</mi><mn id=\"S5.E16.m1.1.1.1.1.3.5.3\" xref=\"S5.E16.m1.1.1.1.1.3.5.3.cmml\">1</mn></msub><mo id=\"S5.E16.m1.1.1.1.1.3.1c\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E16.m1.1.1.1.1.3.6\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.6a.cmml\">[SEP]</mtext><mo id=\"S5.E16.m1.1.1.1.1.3.1d\" lspace=\"0.500em\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.E16.m1.1.1.1.1.3.7\" mathvariant=\"normal\" xref=\"S5.E16.m1.1.1.1.1.3.7.cmml\">⋯</mi><mo id=\"S5.E16.m1.1.1.1.1.3.1e\" lspace=\"0.500em\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E16.m1.1.1.1.1.3.8\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.8a.cmml\">[SEP]</mtext><mo id=\"S5.E16.m1.1.1.1.1.3.1f\" lspace=\"0.500em\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E16.m1.1.1.1.1.3.9\" xref=\"S5.E16.m1.1.1.1.1.3.9.cmml\"><mi id=\"S5.E16.m1.1.1.1.1.3.9.2\" xref=\"S5.E16.m1.1.1.1.1.3.9.2.cmml\">p</mi><mi id=\"S5.E16.m1.1.1.1.1.3.9.3\" xref=\"S5.E16.m1.1.1.1.1.3.9.3.cmml\">n</mi></msub><mo id=\"S5.E16.m1.1.1.1.1.3.1g\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E16.m1.1.1.1.1.3.10\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.10a.cmml\">[SEP]</mtext></mrow></mrow><mo id=\"S5.E16.m1.1.1.1.2\" lspace=\"0em\" xref=\"S5.E16.m1.1.1.1.1.cmml\">.</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E16.m1.1b\"><apply id=\"S5.E16.m1.1.1.1.1.cmml\" xref=\"S5.E16.m1.1.1.1\"><eq id=\"S5.E16.m1.1.1.1.1.1.cmml\" xref=\"S5.E16.m1.1.1.1.1.1\"></eq><ci id=\"S5.E16.m1.1.1.1.1.2.cmml\" xref=\"S5.E16.m1.1.1.1.1.2\">𝑥</ci><apply id=\"S5.E16.m1.1.1.1.1.3.cmml\" xref=\"S5.E16.m1.1.1.1.1.3\"><times id=\"S5.E16.m1.1.1.1.1.3.1.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.1\"></times><ci id=\"S5.E16.m1.1.1.1.1.3.2a.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.2\"><mtext id=\"S5.E16.m1.1.1.1.1.3.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.2\">[CLS]</mtext></ci><ci id=\"S5.E16.m1.1.1.1.1.3.3.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.3\">𝑞</ci><ci id=\"S5.E16.m1.1.1.1.1.3.4a.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.4\"><mtext id=\"S5.E16.m1.1.1.1.1.3.4.cmml\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.4\">[SEP]</mtext></ci><apply id=\"S5.E16.m1.1.1.1.1.3.5.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.5\"><csymbol cd=\"ambiguous\" id=\"S5.E16.m1.1.1.1.1.3.5.1.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.5\">subscript</csymbol><ci id=\"S5.E16.m1.1.1.1.1.3.5.2.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.5.2\">𝑝</ci><cn id=\"S5.E16.m1.1.1.1.1.3.5.3.cmml\" type=\"integer\" xref=\"S5.E16.m1.1.1.1.1.3.5.3\">1</cn></apply><ci id=\"S5.E16.m1.1.1.1.1.3.6a.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.6\"><mtext id=\"S5.E16.m1.1.1.1.1.3.6.cmml\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.6\">[SEP]</mtext></ci><ci id=\"S5.E16.m1.1.1.1.1.3.7.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.7\">⋯</ci><ci id=\"S5.E16.m1.1.1.1.1.3.8a.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.8\"><mtext id=\"S5.E16.m1.1.1.1.1.3.8.cmml\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.8\">[SEP]</mtext></ci><apply id=\"S5.E16.m1.1.1.1.1.3.9.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.9\"><csymbol cd=\"ambiguous\" id=\"S5.E16.m1.1.1.1.1.3.9.1.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.9\">subscript</csymbol><ci id=\"S5.E16.m1.1.1.1.1.3.9.2.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.9.2\">𝑝</ci><ci id=\"S5.E16.m1.1.1.1.1.3.9.3.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.9.3\">𝑛</ci></apply><ci id=\"S5.E16.m1.1.1.1.1.3.10a.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.10\"><mtext id=\"S5.E16.m1.1.1.1.1.3.10.cmml\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.10\">[SEP]</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E16.m1.1c\">x=\\texttt{[CLS]}\\ q\\ \\texttt{[SEP]}\\ p_{1}\\ \\texttt{[SEP]}\\ \\cdots\\ \\texttt{[%\nSEP]}\\ p_{n}\\ \\texttt{[SEP]}.</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E16.m1.1d\">italic_x = [CLS] italic_q [SEP] italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT [SEP] ⋯ [SEP] italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT [SEP] .</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(16)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS5.SSS2.p1.9\">These paths are regarded as the related facts for the candidate answer <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS2.p1.7.m1.1\"><semantics id=\"S5.SS5.SSS2.p1.7.m1.1a\"><mi id=\"S5.SS5.SSS2.p1.7.m1.1.1\" xref=\"S5.SS5.SSS2.p1.7.m1.1.1.cmml\">a</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.7.m1.1b\"><ci id=\"S5.SS5.SSS2.p1.7.m1.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.7.m1.1.1\">𝑎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.7.m1.1c\">a</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.7.m1.1d\">italic_a</annotation></semantics></math>. Finally, it uses LLMs to predict whether the hypothesis: “<math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS2.p1.8.m2.1\"><semantics id=\"S5.SS5.SSS2.p1.8.m2.1a\"><mi id=\"S5.SS5.SSS2.p1.8.m2.1.1\" xref=\"S5.SS5.SSS2.p1.8.m2.1.1.cmml\">a</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.8.m2.1b\"><ci id=\"S5.SS5.SSS2.p1.8.m2.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.8.m2.1.1\">𝑎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.8.m2.1c\">a</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.8.m2.1d\">italic_a</annotation></semantics></math> is the answer of <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS2.p1.9.m3.1\"><semantics id=\"S5.SS5.SSS2.p1.9.m3.1a\"><mi id=\"S5.SS5.SSS2.p1.9.m3.1.1\" xref=\"S5.SS5.SSS2.p1.9.m3.1.1.cmml\">q</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.9.m3.1b\"><ci id=\"S5.SS5.SSS2.p1.9.m3.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.9.m3.1.1\">𝑞</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.9.m3.1c\">q</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.9.m3.1d\">italic_q</annotation></semantics></math>” is supported by those facts, which is formulated as\n</p>\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table\" id=\"A1.EGx3\">\n<tbody id=\"S5.E17\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle e_{\\texttt{[CLS]}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.E17.m1.1\"><semantics id=\"S5.E17.m1.1a\"><msub id=\"S5.E17.m1.1.1\" xref=\"S5.E17.m1.1.1.cmml\"><mi id=\"S5.E17.m1.1.1.2\" xref=\"S5.E17.m1.1.1.2.cmml\">e</mi><mtext id=\"S5.E17.m1.1.1.3\" mathvariant=\"monospace\" xref=\"S5.E17.m1.1.1.3a.cmml\">[CLS]</mtext></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.E17.m1.1b\"><apply id=\"S5.E17.m1.1.1.cmml\" xref=\"S5.E17.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E17.m1.1.1.1.cmml\" xref=\"S5.E17.m1.1.1\">subscript</csymbol><ci id=\"S5.E17.m1.1.1.2.cmml\" xref=\"S5.E17.m1.1.1.2\">𝑒</ci><ci id=\"S5.E17.m1.1.1.3a.cmml\" xref=\"S5.E17.m1.1.1.3\"><mtext id=\"S5.E17.m1.1.1.3.cmml\" mathsize=\"70%\" mathvariant=\"monospace\" xref=\"S5.E17.m1.1.1.3\">[CLS]</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E17.m1.1c\">\\displaystyle e_{\\texttt{[CLS]}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E17.m1.1d\">italic_e start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\text{LLM}(x),\" class=\"ltx_Math\" display=\"inline\" id=\"S5.E17.m2.2\"><semantics id=\"S5.E17.m2.2a\"><mrow id=\"S5.E17.m2.2.2.1\" xref=\"S5.E17.m2.2.2.1.1.cmml\"><mrow id=\"S5.E17.m2.2.2.1.1\" xref=\"S5.E17.m2.2.2.1.1.cmml\"><mi id=\"S5.E17.m2.2.2.1.1.2\" xref=\"S5.E17.m2.2.2.1.1.2.cmml\"></mi><mo id=\"S5.E17.m2.2.2.1.1.1\" xref=\"S5.E17.m2.2.2.1.1.1.cmml\">=</mo><mrow id=\"S5.E17.m2.2.2.1.1.3\" xref=\"S5.E17.m2.2.2.1.1.3.cmml\"><mtext id=\"S5.E17.m2.2.2.1.1.3.2\" xref=\"S5.E17.m2.2.2.1.1.3.2a.cmml\">LLM</mtext><mo id=\"S5.E17.m2.2.2.1.1.3.1\" xref=\"S5.E17.m2.2.2.1.1.3.1.cmml\">⁢</mo><mrow id=\"S5.E17.m2.2.2.1.1.3.3.2\" xref=\"S5.E17.m2.2.2.1.1.3.cmml\"><mo id=\"S5.E17.m2.2.2.1.1.3.3.2.1\" stretchy=\"false\" xref=\"S5.E17.m2.2.2.1.1.3.cmml\">(</mo><mi id=\"S5.E17.m2.1.1\" xref=\"S5.E17.m2.1.1.cmml\">x</mi><mo id=\"S5.E17.m2.2.2.1.1.3.3.2.2\" stretchy=\"false\" xref=\"S5.E17.m2.2.2.1.1.3.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E17.m2.2.2.1.2\" xref=\"S5.E17.m2.2.2.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E17.m2.2b\"><apply id=\"S5.E17.m2.2.2.1.1.cmml\" xref=\"S5.E17.m2.2.2.1\"><eq id=\"S5.E17.m2.2.2.1.1.1.cmml\" xref=\"S5.E17.m2.2.2.1.1.1\"></eq><csymbol cd=\"latexml\" id=\"S5.E17.m2.2.2.1.1.2.cmml\" xref=\"S5.E17.m2.2.2.1.1.2\">absent</csymbol><apply id=\"S5.E17.m2.2.2.1.1.3.cmml\" xref=\"S5.E17.m2.2.2.1.1.3\"><times id=\"S5.E17.m2.2.2.1.1.3.1.cmml\" xref=\"S5.E17.m2.2.2.1.1.3.1\"></times><ci id=\"S5.E17.m2.2.2.1.1.3.2a.cmml\" xref=\"S5.E17.m2.2.2.1.1.3.2\"><mtext id=\"S5.E17.m2.2.2.1.1.3.2.cmml\" xref=\"S5.E17.m2.2.2.1.1.3.2\">LLM</mtext></ci><ci id=\"S5.E17.m2.1.1.cmml\" xref=\"S5.E17.m2.1.1\">𝑥</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E17.m2.2c\">\\displaystyle=\\text{LLM}(x),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E17.m2.2d\">= LLM ( italic_x ) ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(17)</span></td>\n</tr></tbody>\n<tbody id=\"S5.E18\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle s\" class=\"ltx_Math\" display=\"inline\" id=\"S5.E18.m1.1\"><semantics id=\"S5.E18.m1.1a\"><mi id=\"S5.E18.m1.1.1\" xref=\"S5.E18.m1.1.1.cmml\">s</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.E18.m1.1b\"><ci id=\"S5.E18.m1.1.1.cmml\" xref=\"S5.E18.m1.1.1\">𝑠</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E18.m1.1c\">\\displaystyle s</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E18.m1.1d\">italic_s</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\sigma(\\text{MLP}(e_{\\texttt{[CLS]}})),\" class=\"ltx_Math\" display=\"inline\" id=\"S5.E18.m2.1\"><semantics id=\"S5.E18.m2.1a\"><mrow id=\"S5.E18.m2.1.1.1\" xref=\"S5.E18.m2.1.1.1.1.cmml\"><mrow id=\"S5.E18.m2.1.1.1.1\" xref=\"S5.E18.m2.1.1.1.1.cmml\"><mi id=\"S5.E18.m2.1.1.1.1.3\" xref=\"S5.E18.m2.1.1.1.1.3.cmml\"></mi><mo id=\"S5.E18.m2.1.1.1.1.2\" xref=\"S5.E18.m2.1.1.1.1.2.cmml\">=</mo><mrow id=\"S5.E18.m2.1.1.1.1.1\" xref=\"S5.E18.m2.1.1.1.1.1.cmml\"><mi id=\"S5.E18.m2.1.1.1.1.1.3\" xref=\"S5.E18.m2.1.1.1.1.1.3.cmml\">σ</mi><mo id=\"S5.E18.m2.1.1.1.1.1.2\" xref=\"S5.E18.m2.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E18.m2.1.1.1.1.1.1.1\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S5.E18.m2.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S5.E18.m2.1.1.1.1.1.1.1.1\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.cmml\"><mtext id=\"S5.E18.m2.1.1.1.1.1.1.1.1.3\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.3a.cmml\">MLP</mtext><mo id=\"S5.E18.m2.1.1.1.1.1.1.1.1.2\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.cmml\">(</mo><msub id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.2\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.2.cmml\">e</mi><mtext id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.3\" mathvariant=\"monospace\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.3a.cmml\">[CLS]</mtext></msub><mo id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo id=\"S5.E18.m2.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E18.m2.1.1.1.2\" xref=\"S5.E18.m2.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E18.m2.1b\"><apply id=\"S5.E18.m2.1.1.1.1.cmml\" xref=\"S5.E18.m2.1.1.1\"><eq id=\"S5.E18.m2.1.1.1.1.2.cmml\" xref=\"S5.E18.m2.1.1.1.1.2\"></eq><csymbol cd=\"latexml\" id=\"S5.E18.m2.1.1.1.1.3.cmml\" xref=\"S5.E18.m2.1.1.1.1.3\">absent</csymbol><apply id=\"S5.E18.m2.1.1.1.1.1.cmml\" xref=\"S5.E18.m2.1.1.1.1.1\"><times id=\"S5.E18.m2.1.1.1.1.1.2.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.2\"></times><ci id=\"S5.E18.m2.1.1.1.1.1.3.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.3\">𝜎</ci><apply id=\"S5.E18.m2.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1\"><times id=\"S5.E18.m2.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.2\"></times><ci id=\"S5.E18.m2.1.1.1.1.1.1.1.1.3a.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.3\"><mtext id=\"S5.E18.m2.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.3\">MLP</mtext></ci><apply id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.2\">𝑒</ci><ci id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.3a.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.3\"><mtext id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.3.cmml\" mathsize=\"70%\" mathvariant=\"monospace\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.3\">[CLS]</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E18.m2.1c\">\\displaystyle=\\sigma(\\text{MLP}(e_{\\texttt{[CLS]}})),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E18.m2.1d\">= italic_σ ( MLP ( italic_e start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(18)</span></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\" id=\"S5.SS5.SSS2.p1.12\">where it encodes <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS2.p1.10.m1.1\"><semantics id=\"S5.SS5.SSS2.p1.10.m1.1a\"><mi id=\"S5.SS5.SSS2.p1.10.m1.1.1\" xref=\"S5.SS5.SSS2.p1.10.m1.1.1.cmml\">x</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.10.m1.1b\"><ci id=\"S5.SS5.SSS2.p1.10.m1.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.10.m1.1.1\">𝑥</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.10.m1.1c\">x</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.10.m1.1d\">italic_x</annotation></semantics></math> using a LLM and feeds representation corresponding to <span class=\"ltx_text ltx_markedasmath ltx_font_typewriter\" id=\"S5.SS5.SSS2.p1.12.1\">[CLS]</span> token for binary classification, and <math alttext=\"\\sigma(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.SSS2.p1.12.m3.1\"><semantics id=\"S5.SS5.SSS2.p1.12.m3.1a\"><mrow id=\"S5.SS5.SSS2.p1.12.m3.1.2\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.cmml\"><mi id=\"S5.SS5.SSS2.p1.12.m3.1.2.2\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.2.cmml\">σ</mi><mo id=\"S5.SS5.SSS2.p1.12.m3.1.2.1\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.1.cmml\">⁢</mo><mrow id=\"S5.SS5.SSS2.p1.12.m3.1.2.3.2\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.cmml\"><mo id=\"S5.SS5.SSS2.p1.12.m3.1.2.3.2.1\" stretchy=\"false\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.cmml\">(</mo><mo id=\"S5.SS5.SSS2.p1.12.m3.1.1\" lspace=\"0em\" rspace=\"0em\" xref=\"S5.SS5.SSS2.p1.12.m3.1.1.cmml\">⋅</mo><mo id=\"S5.SS5.SSS2.p1.12.m3.1.2.3.2.2\" stretchy=\"false\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.12.m3.1b\"><apply id=\"S5.SS5.SSS2.p1.12.m3.1.2.cmml\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2\"><times id=\"S5.SS5.SSS2.p1.12.m3.1.2.1.cmml\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.1\"></times><ci id=\"S5.SS5.SSS2.p1.12.m3.1.2.2.cmml\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.2\">𝜎</ci><ci id=\"S5.SS5.SSS2.p1.12.m3.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.12.m3.1.1\">⋅</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.12.m3.1c\">\\sigma(\\cdot)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.12.m3.1d\">italic_σ ( ⋅ )</annotation></semantics></math> denotes the sigmoid function.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS5.SSS2.p2\">\n<p class=\"ltx_p\" id=\"S5.SS5.SSS2.p2.1\">To better guide LLMs reason through KGs, OreoLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib177\" title=\"\">177</a>]</cite> proposes a Knowledge Interaction Layer (KIL) which is inserted amid LLM layers. KIL interacts with a KG reasoning module, where it discovers different reasoning paths, and then the reasoning module can reason over the paths to generate answers. GreaseLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib178\" title=\"\">178</a>]</cite> fuses the representations from LLMs and graph neural networks to effectively reason over KG facts and language context. UniKGQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib43\" title=\"\">43</a>]</cite> unifies the facts retrieval and reasoning into a unified framework. UniKGQA consists of two modules. The first module is a semantic matching module that uses a LLM to match questions with their corresponding relations semantically. The second module is a matching information propagation module, which propagates the matching information along directed edges on KGs for answer reasoning. Similarly, ReLMKG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib179\" title=\"\">179</a>]</cite> performs joint reasoning on a large language model and the associated knowledge graph. The question and verbalized paths are encoded by the language model, and different layers of the language model produce outputs that guide a graph neural network to perform message passing. This process utilizes the explicit knowledge contained in the structured knowledge graph for reasoning purposes. StructGPT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib237\" title=\"\">237</a>]</cite> adopts a customized interface to allow large language models (e.g., ChatGPT) directly reasoning on KGs to perform multi-step question answering.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S5.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE IV: </span>Summary of methods that synergize KGs and LLMs.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T4.1.1.1.1\">Task</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.1.1.1.2\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.1.1.1.3\">Year</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T4.1.2.1.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S5.T4.1.2.1.1.1\">Synergized Knowledge representation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.2.1.2\">JointGT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib42\" title=\"\">42</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.2.1.3\">2021</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.3.2.1\">KEPLER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.3.2.2\">2021</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.4.3.1\">DRAGON <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib44\" title=\"\">44</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.4.3.2\">2022</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.5.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.5.4.1\">HKLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib238\" title=\"\">238</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.5.4.2\">2023</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S5.T4.1.6.5.1\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S5.T4.1.6.5.1.1\">Synergized Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.6.5.2\">LARK <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib45\" title=\"\">45</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.6.5.3\">2023</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.7.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.7.6.1\">Siyuan et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib46\" title=\"\">46</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.7.6.2\">2023</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.8.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.8.7.1\">KSL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib239\" title=\"\">239</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.8.7.2\">2023</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.9.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.9.8.1\">StructGPT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib237\" title=\"\">237</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.9.8.2\">2023</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.10.9.1\">Think-on-graph <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib240\" title=\"\">240</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.10.9.2\">2023</td>\n</tr>\n</tbody>\n</table>\n</figure>\n</section>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S6\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">6 </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S6.1.1\">Synergized LLMs + KGs</span>\n</h2>\n<div class=\"ltx_para\" id=\"S6.p1\">\n<p class=\"ltx_p\" id=\"S6.p1.1\">The synergy of LLMs and KGs has attracted increasing attention these years, which marries the merits of LLMs and KGs to mutually enhance performance in various downstream applications. For example, LLMs can be used to understand natural language, while KGs are treated as a knowledge base, which provides factual knowledge. The unification of LLMs and KGs could result in a powerful model for knowledge representation and reasoning.</p>\n</div>\n<div class=\"ltx_para\" id=\"S6.p2\">\n<p class=\"ltx_p\" id=\"S6.p2.1\">In this section, we will discuss the state-of-the-art <span class=\"ltx_text ltx_font_italic\" id=\"S6.p2.1.1\">Synergized LLMs + KGs</span> from two perspectives: <em class=\"ltx_emph ltx_font_italic\" id=\"S6.p2.1.2\">1) Synergized Knowledge Representation</em>, and <em class=\"ltx_emph ltx_font_italic\" id=\"S6.p2.1.3\">2)</em> Synergized Reasoning. Representative works are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S5.T4\" title=\"TABLE IV ‣ 5.5.2 LLMs as Answer Reasoners ‣ 5.5 LLM-augmented KG Question Answering ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>.\n</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S6.F23\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"416\" id=\"S6.F23.g1\" src=\"x20.png\" width=\"581\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 23: </span>Synergized knowledge representation by additional KG fusion modules.</figcaption>\n</figure>\n<section class=\"ltx_subsection\" id=\"S6.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">6.1 </span><span class=\"ltx_text ltx_font_italic\" id=\"S6.SS1.1.1\">Synergized Knowledge Representation</span>\n</h3>\n<div class=\"ltx_para\" id=\"S6.SS1.p1\">\n<p class=\"ltx_p\" id=\"S6.SS1.p1.1\">Text corpus and knowledge graphs both contain enormous knowledge. However, the knowledge in text corpus is usually implicit and unstructured, while the knowledge in KGs is explicit and structured. Synergized Knowledge Representation aims to design a synergized model that can effectively represent knowledge from both LLMs and KGs. The synergized model can provide a better understanding of the knowledge from both sources, making it valuable for many downstream tasks.</p>\n</div>\n<div class=\"ltx_para\" id=\"S6.SS1.p2\">\n<p class=\"ltx_p\" id=\"S6.SS1.p2.1\">To jointly represent the knowledge, researchers propose the synergized models by introducing additional KG fusion modules, which are jointly trained with LLMs. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S6.F23\" title=\"Figure 23 ‣ 6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">23</span></a>, ERNIE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib35\" title=\"\">35</a>]</cite> proposes a textual-knowledge dual encoder architecture where a <em class=\"ltx_emph ltx_font_italic\" id=\"S6.SS1.p2.1.1\">T-encoder</em> first encodes the input sentences, then a <em class=\"ltx_emph ltx_font_italic\" id=\"S6.SS1.p2.1.2\">K-encoder</em> processes knowledge graphs which are fused them with the textual representation from the <em class=\"ltx_emph ltx_font_italic\" id=\"S6.SS1.p2.1.3\">T-encoder</em>. BERT-MK <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib241\" title=\"\">241</a>]</cite> employs a similar dual-encoder architecture but it introduces additional information of neighboring entities in the knowledge encoder component during the pre-training of LLMs. However, some of the neighboring entities in KGs may not be relevant to the input text, resulting in extra redundancy and noise. CokeBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib242\" title=\"\">242</a>]</cite> focuses on this issue and proposes a GNN-based module to filter out irrelevant KG entities using the input text. JAKET <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib243\" title=\"\">243</a>]</cite> proposes to fuse the entity information in the middle of the large language model.</p>\n</div>\n<div class=\"ltx_para\" id=\"S6.SS1.p3\">\n<p class=\"ltx_p\" id=\"S6.SS1.p3.1\">KEPLER <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite> presents a unified model for knowledge embedding and pre-trained language representation. In KEPLER, they encode textual entity descriptions with a LLM as their embeddings, and then jointly optimize the knowledge embedding and language modeling objectives. JointGT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib42\" title=\"\">42</a>]</cite> proposes a graph-text joint representation learning model, which proposes three pre-training tasks to align representations of graph and text. DRAGON <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib44\" title=\"\">44</a>]</cite> presents a self-supervised method to pre-train a joint language-knowledge foundation model from text and KG. It takes text segments and relevant KG subgraphs as input and bidirectionally fuses information from both modalities. Then, DRAGON utilizes two self-supervised reasoning tasks, i.e., masked language modeling and KG link prediction to optimize the model parameters. HKLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib238\" title=\"\">238</a>]</cite> introduces a unified LLM which incorporates KGs to learn representations of domain-specific knowledge.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S6.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">6.2 </span><span class=\"ltx_text ltx_font_italic\" id=\"S6.SS2.1.1\">Synergized Reasoning</span>\n</h3>\n<div class=\"ltx_para\" id=\"S6.SS2.p1\">\n<p class=\"ltx_p\" id=\"S6.SS2.p1.1\">To better utilize the knowledge from text corpus and knowledge graph reasoning, Synergized Reasoning aims to design a synergized model that can effectively conduct reasoning with both LLMs and KGs.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S6.SS2.p2\">\n<p class=\"ltx_p\" id=\"S6.SS2.p2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.SS2.p2.1.1\">LLM-KG Fusion Reasoning.</span>\nLLM-KG Fusion Reasoning leverages two separated LLM and KG encoders to process the text and relevant KG inputs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib244\" title=\"\">244</a>]</cite>. These two encoders are equally important and jointly fusing the knowledge from two sources for reasoning. To improve the interaction between text and knowledge, KagNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib38\" title=\"\">38</a>]</cite> proposes to first encode the input KG, and then augment the input textual representation. In contrast, MHGRN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib234\" title=\"\">234</a>]</cite> uses the final LLM outputs of the input text to guide the reasoning process on the KGs. Yet, both of them only design a single-direction interaction between the text and KGs. To tackle this issue, QA-GNN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib131\" title=\"\">131</a>]</cite> proposes to use a GNN-based model to jointly reason over input context and KG information via message passing. Specifically, QA-GNN represents the input textual information as a special node via a pooling operation and connects this node with other entities in KG. However, the textual inputs are only pooled into a single dense vector, limiting the information fusion performance. JointLK <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib245\" title=\"\">245</a>]</cite> then proposes a framework with fine-grained interaction between any tokens in the textual inputs and any KG entities through LM-to-KG and KG-to-LM bi-directional attention mechanism. As shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S6.F24\" title=\"Figure 24 ‣ 6.2 Synergized Reasoning ‣ 6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">24</span></a>, pairwise dot-product scores are calculated over all textual tokens and KG entities, the bi-directional attentive scores are computed separately. In addition, at each jointLK layer, the KGs are also dynamically pruned based on the attention score to allow later layers to focus on more important sub-KG structures. Despite being effective, in JointLK, the fusion process between the input text and KG still uses the final LLM outputs as the input text representations. GreaseLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib178\" title=\"\">178</a>]</cite> designs deep and rich interaction between the input text tokens and KG entities at each layer of the LLMs. The architecture and fusion approach is mostly similar to ERNIE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib35\" title=\"\">35</a>]</cite> discussed in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S6.SS1\" title=\"6.1 Synergized Knowledge Representation ‣ 6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">6.1</span></a>, except that GreaseLM does not use the text-only <em class=\"ltx_emph ltx_font_italic\" id=\"S6.SS2.p2.1.2\">T-encoder</em> to handle input text.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S6.F24\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"387\" id=\"S6.F24.g1\" src=\"x21.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 24: </span>The framework of LLM-KG Fusion Reasoning.</figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S6.SS2.p3\">\n<p class=\"ltx_p\" id=\"S6.SS2.p3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.SS2.p3.1.1\">LLMs as Agents Reasoning.</span>\nInstead using two encoders to fuse the knowledge, LLMs can also be treated as agents to interact with the KGs to conduct reasoning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib246\" title=\"\">246</a>]</cite>, as illustrated in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S6.F25\" title=\"Figure 25 ‣ 6.2 Synergized Reasoning ‣ 6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">25</span></a>. KD-CoT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib247\" title=\"\">247</a>]</cite> iteratively retrieves facts from KGs and produces faithful reasoning traces, which guide LLMs to generate answers.\nKSL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib239\" title=\"\">239</a>]</cite> teaches LLMs to search on KGs to retrieve relevant facts and then generate answers. StructGPT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib237\" title=\"\">237</a>]</cite> designs several API interfaces to allow LLMs to access the structural data and perform reasoning by traversing on KGs. Think-on-graph <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib240\" title=\"\">240</a>]</cite> provides a flexible plug-and-play framework where LLM agents iteratively execute beam searches on KGs to discover the reasoning paths and generate answers. To enhance the agent abilities, AgentTuning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib248\" title=\"\">248</a>]</cite> presents several instruction-tuning datasets to guide LLM agents to perform reasoning on KGs.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S6.F25\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"412\" id=\"S6.F25.g1\" src=\"x22.png\" width=\"581\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 25: </span>Using LLMs as agents for reasoning on KGs.</figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S6.SS2.p4\">\n<p class=\"ltx_p\" id=\"S6.SS2.p4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.SS2.p4.1.1\">Comparison and Discussion.</span>\nLLM-KG Fusion Reasoning combines the LLM encoder and KG encoder to represent knowledge in a unified manner. It then employs a synergized reasoning module to jointly reason the results. This framework allows for different encoders and reasoning modules, which are trained end-to-end to effectively utilize the knowledge and reasoning capabilities of LLMs and KGs. However, these additional modules may introduce extra parameters and computational costs while lacking interpretability. LLMs as Agents for KG reasoning provides a flexible framework for reasoning on KGs without additional training cost, which can be generalized to different LLMs and KGs. Meanwhile, the reasoning process is interpretable, which can be used to explain the results. Nevertheless, defining the actions and policies for LLM agents is also challenging. The synergy of LLMs and KGs is still an ongoing research topic, with the potential to have more powerful frameworks in the future.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S7\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">7 </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S7.1.1\">Future Directions and Milestones</span>\n</h2>\n<div class=\"ltx_para\" id=\"S7.p1\">\n<p class=\"ltx_p\" id=\"S7.p1.1\">In this section, we discuss the future directions and several milestones in the research area of unifying KGs and LLMs.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S7.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">7.1 </span><span class=\"ltx_text ltx_font_italic\" id=\"S7.SS1.1.1\">KGs for Hallucination Detection in LLMs</span>\n</h3>\n<div class=\"ltx_para\" id=\"S7.SS1.p1\">\n<p class=\"ltx_p\" id=\"S7.SS1.p1.1\">The hallucination problem in LLMs, which generates factually incorrect content, significantly hinders the reliability of LLMs. As discussed in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S4\" title=\"4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, existing studies try to utilize KGs to obtain more reliable LLMs through pre-training or KG-enhanced inference. Despite the efforts, the issue of hallucination may continue to persist in the realm of LLMs for the foreseeable future. Consequently, in order to gain the public’s trust and border applications, it is imperative to detect and assess instances of hallucination within LLMs and other forms of AI-generated content (AIGC). Existing methods strive to detect hallucination by training a neural classifier on a small set of documents <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib249\" title=\"\">249</a>]</cite>, which are neither robust nor powerful to handle ever-growing LLMs. Recently, researchers try to use KGs as an external source to validate LLMs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib250\" title=\"\">250</a>]</cite>. Further studies combine LLMs and KGs to achieve a generalized fact-checking model that can detect hallucinations across domains <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib251\" title=\"\">251</a>]</cite>. Therefore, it opens a new door to utilizing KGs for hallucination detection.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S7.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">7.2 </span><span class=\"ltx_text ltx_font_italic\" id=\"S7.SS2.1.1\">KGs for Editing Knowledge in LLMs</span>\n</h3>\n<div class=\"ltx_para\" id=\"S7.SS2.p1\">\n<p class=\"ltx_p\" id=\"S7.SS2.p1.1\">Although LLMs are capable of storing massive real-world knowledge, they cannot quickly update their internal knowledge updated as real-world situations change. There are some research efforts proposed for editing knowledge in LLMs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib252\" title=\"\">252</a>]</cite> without re-training the whole LLMs. Yet, such solutions still suffer from poor performance or computational overhead <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib253\" title=\"\">253</a>]</cite>. Existing studies <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib254\" title=\"\">254</a>]</cite> also reveal that edit a single fact would cause a ripple effect for other related knowledge. Therefore, it is necessary to develop a more efficient and effective method to edit knowledge in LLMs. Recently, researchers try to leverage KGs to edit knowledge in LLMs efficiently.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S7.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">7.3 </span><span class=\"ltx_text ltx_font_italic\" id=\"S7.SS3.1.1\">KGs for Black-box LLMs Knowledge Injection</span>\n</h3>\n<div class=\"ltx_para\" id=\"S7.SS3.p1\">\n<p class=\"ltx_p\" id=\"S7.SS3.p1.1\">Although pre-training and knowledge editing could update LLMs to catch up with the latest knowledge, they still need to access the internal structures and parameters of LLMs. However, many state-of-the-art large LLMs (e.g., ChatGPT) only provide APIs for users and developers to access, making themselves black-box to the public. Consequently, it is impossible to follow conventional KG injection approaches described <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib244\" title=\"\">244</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib38\" title=\"\">38</a>]</cite> that change LLM structure by adding additional knowledge fusion modules. Converting various types of knowledge into different text prompts seems to be a feasible solution. However, it is unclear whether these prompts can generalize well to new LLMs. Moreover, the prompt-based approach is limited to the length of input tokens of LLMs. Therefore, how to enable effective knowledge injection for black-box LLMs is still an open question for us to explore <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib255\" title=\"\">255</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib256\" title=\"\">256</a>]</cite>.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S7.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">7.4 </span><span class=\"ltx_text ltx_font_italic\" id=\"S7.SS4.1.1\">Multi-Modal LLMs for KGs</span>\n</h3>\n<div class=\"ltx_para\" id=\"S7.SS4.p1\">\n<p class=\"ltx_p\" id=\"S7.SS4.p1.1\">Current knowledge graphs typically rely on textual and graph structure to handle KG-related applications. However, real-world knowledge graphs are often constructed by data from diverse modalities <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib257\" title=\"\">257</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib258\" title=\"\">258</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib99\" title=\"\">99</a>]</cite>. Therefore, effectively leveraging representations from multiple modalities would be a significant challenge for future research in KGs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib259\" title=\"\">259</a>]</cite>. One potential solution is to develop methods that can accurately encode and align entities across different modalities. Recently, with the development of multi-modal LLMs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib260\" title=\"\">260</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib98\" title=\"\">98</a>]</cite>, leveraging LLMs for modality alignment holds promise in this regard. But, bridging the gap between multi-modal LLMs and KG structure remains a crucial challenge in this field, demanding further investigation and advancements.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S7.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">7.5 </span><span class=\"ltx_text ltx_font_italic\" id=\"S7.SS5.1.1\">LLMs for Understanding KG Structure</span>\n</h3>\n<div class=\"ltx_para\" id=\"S7.SS5.p1\">\n<p class=\"ltx_p\" id=\"S7.SS5.p1.1\">Conventional LLMs trained on plain text data are not designed to understand structured data like knowledge graphs. Thus, LLMs might not fully grasp or understand the information conveyed by the KG structure. A straightforward way is to linearize the structured data into a sentence that LLMs can understand. However, the scale of the KGs makes it impossible to linearize the whole KGs as input. Moreover, the linearization process may lose some underlying information in KGs. Therefore, it is necessary to develop LLMs that can directly understand the KG structure and reason over it <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib237\" title=\"\">237</a>]</cite>.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"S7.F26\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"270\" id=\"S7.F26.g1\" src=\"x23.png\" width=\"498\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 26: </span>The milestones of unifying KGs and LLMs.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"S7.SS6\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">7.6 </span><span class=\"ltx_text ltx_font_italic\" id=\"S7.SS6.1.1\">Synergized LLMs and KGs for Birectional Reasoning</span>\n</h3>\n<div class=\"ltx_para\" id=\"S7.SS6.p1\">\n<p class=\"ltx_p\" id=\"S7.SS6.p1.1\">KGs and LLMs are two complementary technologies that can synergize each other. However, the synergy of LLMs and KGs is less explored by existing researchers. A desired synergy of LLMs and KGs would involve leveraging the strengths of both technologies to overcome their individual limitations. LLMs, such as ChatGPT, excel in generating human-like text and understanding natural language, while KGs are structured databases that capture and represent knowledge in a structured manner. By combining their capabilities, we can create a powerful system that benefits from the contextual understanding of LLMs and the structured knowledge representation of KGs. To better unify LLMs and KGs, many advanced techniques need to be incorporated, such as multi-modal learning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib261\" title=\"\">261</a>]</cite>, graph neural network <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib262\" title=\"\">262</a>]</cite>, and continuous learning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib263\" title=\"\">263</a>]</cite>. Last, the synergy of LLMs and KGs can be applied to many real-world applications, such as search engines <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib100\" title=\"\">100</a>]</cite>, recommender systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib89\" title=\"\">89</a>]</cite>, and drug discovery.</p>\n</div>\n<div class=\"ltx_para\" id=\"S7.SS6.p2\">\n<p class=\"ltx_p\" id=\"S7.SS6.p2.1\">With a given application problem, we can apply a KG to perform a knowledge-driven search for potential goals and unseen data, and simultaneously start with LLMs to perform a data/text-driven inference to see what new data/goal items can be derived. When the knowledge-based search is combined with data/text-driven inference, they can mutually validate each other, resulting in efficient and effective solutions powered by dual-driving wheels.\nTherefore, we can anticipate increasing attention to unlock the potential of integrating KGs and LLMs for diverse downstream applications with both generative and reasoning capabilities in the near future.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S8\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">8 </span><span class=\"ltx_text ltx_font_smallcaps\" id=\"S8.1.1\">Conclusion</span>\n</h2>\n<div class=\"ltx_para\" id=\"S8.p1\">\n<p class=\"ltx_p\" id=\"S8.p1.1\">Unifying large language models (LLMs) and knowledge graphs (KGs) is an active research direction that has attracted increasing attention from both academia and industry. In this article, we provide a thorough overview of the recent research in this field. We first introduce different manners that integrate KGs to enhance LLMs. Then, we introduce existing methods that apply LLMs for KGs and establish taxonomy based on varieties of KG tasks. Finally, we discuss the challenges and future directions in this field.\nWe envision that there will be multiple stages (milestones) in the roadmap of unifying KGs and LLMs, as shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S7.F26\" title=\"Figure 26 ‣ 7.5 LLMs for Understanding KG Structure ‣ 7 Future Directions and Milestones ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">26</span></a>. In particular, we will anticipate increasing research on three stages: <em class=\"ltx_emph ltx_font_italic\" id=\"S8.p1.1.1\">Stage 1</em>: KG-enhanced LLMs, LLM-augmented KGs, <em class=\"ltx_emph ltx_font_italic\" id=\"S8.p1.1.2\">Stage 2:</em> Synergized LLMs + KGs, and <em class=\"ltx_emph ltx_font_italic\" id=\"S8.p1.1.3\">Stage 3:</em> Graph Structure Understanding, Multi-modality, Knowledge Updating. We hope that this article will provide a guideline to advance future research.\n</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"Sx1\">\n<h2 class=\"ltx_title ltx_font_smallcaps ltx_title_section\">Acknowledgments</h2>\n<div class=\"ltx_para\" id=\"Sx1.p1\">\n<p class=\"ltx_p\" id=\"Sx1.p1.1\">This research was supported by the Australian Research Council (ARC) under grants FT210100097 and DP240101547 and the National Natural Science Foundation of China (NSFC) under grant 62120106008.</p>\n</div>\n</section>\n<section class=\"ltx_bibliography\" id=\"bib\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n<ul class=\"ltx_biblist\">\n<li class=\"ltx_bibitem\" id=\"bib.bib1\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[1]</span>\n<span class=\"ltx_bibblock\">\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib1.1.1\">arXiv preprint arXiv:1810.04805</em>, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib2\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[2]</span>\n<span class=\"ltx_bibblock\">\nY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert pretraining approach,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib2.1.1\">arXiv preprint arXiv:1907.11692</em>, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib3\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[3]</span>\n<span class=\"ltx_bibblock\">\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib3.1.1\">The Journal of Machine Learning Research</em>, vol. 21, no. 1, pp. 5485–5551, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib4\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[4]</span>\n<span class=\"ltx_bibblock\">\nD. Su, Y. Xu, G. I. Winata, P. Xu, H. Kim, Z. Liu, and P. Fung, “Generalizing question answering system with pre-trained language model fine-tuning,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib4.1.1\">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</em>, 2019, pp. 203–211.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib5\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[5]</span>\n<span class=\"ltx_bibblock\">\nM. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib5.1.1\">ACL</em>, 2020, pp. 7871–7880.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib6\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[6]</span>\n<span class=\"ltx_bibblock\">\nJ. Li, T. Tang, W. X. Zhao, and J.-R. Wen, “Pretrained language models for text generation: A survey,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib6.1.1\">arXiv preprint arXiv:2105.10311</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib7\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[7]</span>\n<span class=\"ltx_bibblock\">\nJ. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib7.1.1\">et al.</em>, “Emergent abilities of large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib7.2.2\">Transactions on Machine Learning Research</em>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib8\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[8]</span>\n<span class=\"ltx_bibblock\">\nK. Malinka, M. Perešíni, A. Firc, O. Hujňák, and F. Januš, “On the educational impact of chatgpt: Is artificial intelligence ready to obtain a university degree?” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib8.1.1\">arXiv preprint arXiv:2303.11146</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib9\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[9]</span>\n<span class=\"ltx_bibblock\">\nZ. Li, C. Wang, Z. Liu, H. Wang, S. Wang, and C. Gao, “Cctest: Testing and repairing code completion systems,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib9.1.1\">ICSE</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib10\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[10]</span>\n<span class=\"ltx_bibblock\">\nJ. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang, “Is chatgpt a good recommender? a preliminary study,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib10.1.1\">arXiv preprint arXiv:2304.10149</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib11\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[11]</span>\n<span class=\"ltx_bibblock\">\nW. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib11.1.1\">et al.</em>, “A survey of large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib11.2.2\">arXiv preprint arXiv:2303.18223</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib12\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[12]</span>\n<span class=\"ltx_bibblock\">\nX. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained models for natural language processing: A survey,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib12.1.1\">Science China Technological Sciences</em>, vol. 63, no. 10, pp. 1872–1897, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib13\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[13]</span>\n<span class=\"ltx_bibblock\">\nJ. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, and X. Hu, “Harnessing the power of llms in practice: A survey on chatgpt and beyond,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib13.1.1\">arXiv preprint arXiv:2304.13712</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib14\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[14]</span>\n<span class=\"ltx_bibblock\">\nF. Petroni, T. Rocktäschel, S. Riedel, P. Lewis, A. Bakhtin, Y. Wu, and A. Miller, “Language models as knowledge bases?” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib14.1.1\">EMNLP-IJCNLP</em>, 2019, pp. 2463–2473.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib15\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[15]</span>\n<span class=\"ltx_bibblock\">\nZ. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung, “Survey of hallucination in natural language generation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib15.1.1\">ACM Computing Surveys</em>, vol. 55, no. 12, pp. 1–38, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib16\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[16]</span>\n<span class=\"ltx_bibblock\">\nH. Zhang, H. Song, S. Li, M. Zhou, and D. Song, “A survey of controllable text generation using transformer-based pre-trained language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib16.1.1\">arXiv preprint arXiv:2201.05337</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib17\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[17]</span>\n<span class=\"ltx_bibblock\">\nM. Danilevsky, K. Qian, R. Aharonov, Y. Katsis, B. Kawas, and P. Sen, “A survey of the state of explainable ai for natural language processing,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib17.1.1\">arXiv preprint arXiv:2010.00711</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib18\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[18]</span>\n<span class=\"ltx_bibblock\">\nJ. Wang, X. Hu, W. Hou, H. Chen, R. Zheng, Y. Wang, L. Yang, H. Huang, W. Ye, X. Geng <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib18.1.1\">et al.</em>, “On the robustness of chatgpt: An adversarial and out-of-distribution perspective,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib18.2.2\">arXiv preprint arXiv:2302.12095</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib19\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[19]</span>\n<span class=\"ltx_bibblock\">\nS. Ji, S. Pan, E. Cambria, P. Marttinen, and S. Y. Philip, “A survey on knowledge graphs: Representation, acquisition, and applications,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib19.1.1\">IEEE TNNLS</em>, vol. 33, no. 2, pp. 494–514, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib20\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[20]</span>\n<span class=\"ltx_bibblock\">\nD. Vrandečić and M. Krötzsch, “Wikidata: a free collaborative knowledgebase,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib20.1.1\">Communications of the ACM</em>, vol. 57, no. 10, pp. 78–85, 2014.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib21\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[21]</span>\n<span class=\"ltx_bibblock\">\nS. Hu, L. Zou, and X. Zhang, “A state-transition framework to answer complex questions over knowledge base,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib21.1.1\">EMNLP</em>, 2018, pp. 2098–2108.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib22\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[22]</span>\n<span class=\"ltx_bibblock\">\nJ. Zhang, B. Chen, L. Zhang, X. Ke, and H. Ding, “Neural, symbolic and neural-symbolic reasoning on knowledge graphs,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib22.1.1\">AI Open</em>, vol. 2, pp. 14–35, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib23\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[23]</span>\n<span class=\"ltx_bibblock\">\nB. Abu-Salih, “Domain-specific knowledge graphs: A survey,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib23.1.1\">Journal of Network and Computer Applications</em>, vol. 185, p. 103076, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib24\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[24]</span>\n<span class=\"ltx_bibblock\">\nT. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, B. Yang, J. Betteridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel, K. Jayant, L. Ni, M. Kathryn, M. Thahir, N. Ndapandula, P. Emmanouil, R. Alan, S. Mehdi, S. Burr, W. Derry, G. Abhinav, C. Xi, S. Abulhair, and W. Joel, “Never-ending learning,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib24.1.1\">Communications of the ACM</em>, vol. 61, no. 5, pp. 103–115, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib25\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[25]</span>\n<span class=\"ltx_bibblock\">\nL. Zhong, J. Wu, Q. Li, H. Peng, and X. Wu, “A comprehensive survey on automatic knowledge graph construction,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib25.1.1\">arXiv preprint arXiv:2302.05019</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib26\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[26]</span>\n<span class=\"ltx_bibblock\">\nL. Yao, C. Mao, and Y. Luo, “Kg-bert: Bert for knowledge graph completion,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib26.1.1\">arXiv preprint arXiv:1909.03193</em>, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib27\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[27]</span>\n<span class=\"ltx_bibblock\">\nL. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Normalizing flow-based neural process for few-shot knowledge graph completion,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib27.1.1\">SIGIR</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib28\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[28]</span>\n<span class=\"ltx_bibblock\">\nY. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib28.1.1\">et al.</em>, “A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib28.2.2\">arXiv preprint arXiv:2302.04023</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib29\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[29]</span>\n<span class=\"ltx_bibblock\">\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou, “Self-consistency improves chain of thought reasoning in language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib29.1.1\">arXiv preprint arXiv:2203.11171</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib30\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[30]</span>\n<span class=\"ltx_bibblock\">\nO. Golovneva, M. Chen, S. Poff, M. Corredor, L. Zettlemoyer, M. Fazel-Zarandi, and A. Celikyilmaz, “Roscoe: A suite of metrics for scoring step-by-step reasoning,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib30.1.1\">ICLR</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib31\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[31]</span>\n<span class=\"ltx_bibblock\">\nF. M. Suchanek, G. Kasneci, and G. Weikum, “Yago: a core of semantic knowledge,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib31.1.1\">WWW</em>, 2007, pp. 697–706.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib32\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[32]</span>\n<span class=\"ltx_bibblock\">\nA. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. Hruschka, and T. Mitchell, “Toward an architecture for never-ending language learning,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib32.1.1\">Proceedings of the AAAI conference on artificial intelligence</em>, vol. 24, no. 1, 2010, pp. 1306–1313.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib33\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[33]</span>\n<span class=\"ltx_bibblock\">\nA. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko, “Translating embeddings for modeling multi-relational data,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib33.1.1\">NeurIPS</em>, vol. 26, 2013.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib34\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[34]</span>\n<span class=\"ltx_bibblock\">\nG. Wan, S. Pan, C. Gong, C. Zhou, and G. Haffari, “Reasoning like human: Hierarchical reinforcement learning for knowledge graph reasoning,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib34.1.1\">AAAI</em>, 2021, pp. 1926–1932.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib35\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[35]</span>\n<span class=\"ltx_bibblock\">\nZ. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, “ERNIE: Enhanced language representation with informative entities,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib35.1.1\">ACL</em>, 2019, pp. 1441–1451.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib36\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[36]</span>\n<span class=\"ltx_bibblock\">\nW. Liu, P. Zhou, Z. Zhao, Z. Wang, Q. Ju, H. Deng, and P. Wang, “K-BERT: enabling language representation with knowledge graph,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib36.1.1\">AAAI</em>, 2020, pp. 2901–2908.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib37\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[37]</span>\n<span class=\"ltx_bibblock\">\nY. Liu, Y. Wan, L. He, H. Peng, and P. S. Yu, “KG-BART: knowledge graph-augmented BART for generative commonsense reasoning,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib37.1.1\">AAAI</em>, 2021, pp. 6418–6425.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib38\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[38]</span>\n<span class=\"ltx_bibblock\">\nB. Y. Lin, X. Chen, J. Chen, and X. Ren, “KagNet: Knowledge-aware graph networks for commonsense reasoning,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib38.1.1\">EMNLP-IJCNLP</em>, 2019, pp. 2829–2839.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib39\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[39]</span>\n<span class=\"ltx_bibblock\">\nD. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei, “Knowledge neurons in pretrained transformers,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib39.1.1\">arXiv preprint arXiv:2104.08696</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib40\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[40]</span>\n<span class=\"ltx_bibblock\">\nX. Wang, T. Gao, Z. Zhu, Z. Zhang, Z. Liu, J. Li, and J. Tang, “KEPLER: A unified model for knowledge embedding and pre-trained language representation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib40.1.1\">Transactions of the Association for Computational Linguistics</em>, vol. 9, pp. 176–194, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib41\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[41]</span>\n<span class=\"ltx_bibblock\">\nI. Melnyk, P. Dognin, and P. Das, “Grapher: Multi-stage knowledge graph construction using pretrained language models,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib41.1.1\">NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib42\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[42]</span>\n<span class=\"ltx_bibblock\">\nP. Ke, H. Ji, Y. Ran, X. Cui, L. Wang, L. Song, X. Zhu, and M. Huang, “JointGT: Graph-text joint representation learning for text generation from knowledge graphs,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib42.1.1\">ACL Finding</em>, 2021, pp. 2526–2538.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib43\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[43]</span>\n<span class=\"ltx_bibblock\">\nJ. Jiang, K. Zhou, W. X. Zhao, and J.-R. Wen, “Unikgqa: Unified retrieval and reasoning for solving multi-hop question answering over knowledge graph,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib43.1.1\">ICLR 2023</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib44\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[44]</span>\n<span class=\"ltx_bibblock\">\nM. Yasunaga, A. Bosselut, H. Ren, X. Zhang, C. D. Manning, P. S. Liang, and J. Leskovec, “Deep bidirectional language-knowledge graph pretraining,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib44.1.1\">NeurIPS</em>, vol. 35, pp. 37 309–37 323, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib45\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[45]</span>\n<span class=\"ltx_bibblock\">\nN. Choudhary and C. K. Reddy, “Complex logical reasoning over knowledge graphs using large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib45.1.1\">arXiv preprint arXiv:2305.01157</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib46\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[46]</span>\n<span class=\"ltx_bibblock\">\nS. Wang, Z. Wei, J. Xu, and Z. Fan, “Unifying structure reasoning and language model pre-training for complex reasoning,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib46.1.1\">arXiv preprint arXiv:2301.08913</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib47\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[47]</span>\n<span class=\"ltx_bibblock\">\nC. Zhen, Y. Shang, X. Liu, Y. Li, Y. Chen, and D. Zhang, “A survey on knowledge-enhanced pre-trained language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib47.1.1\">arXiv preprint arXiv:2212.13428</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib48\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[48]</span>\n<span class=\"ltx_bibblock\">\nX. Wei, S. Wang, D. Zhang, P. Bhatia, and A. Arnold, “Knowledge enhanced pretrained language models: A compreshensive survey,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib48.1.1\">arXiv preprint arXiv:2110.08455</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib49\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[49]</span>\n<span class=\"ltx_bibblock\">\nD. Yin, L. Dong, H. Cheng, X. Liu, K.-W. Chang, F. Wei, and J. Gao, “A survey of knowledge-intensive nlp with pre-trained language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib49.1.1\">arXiv preprint arXiv:2202.08772</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib50\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[50]</span>\n<span class=\"ltx_bibblock\">\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib50.1.1\">NeurIPS</em>, vol. 30, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib51\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[51]</span>\n<span class=\"ltx_bibblock\">\nZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “Albert: A lite bert for self-supervised learning of language representations,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib51.1.1\">ICLR</em>, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib52\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[52]</span>\n<span class=\"ltx_bibblock\">\nK. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “Electra: Pre-training text encoders as discriminators rather than generators,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib52.1.1\">arXiv preprint arXiv:2003.10555</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib53\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[53]</span>\n<span class=\"ltx_bibblock\">\nK. Hakala and S. Pyysalo, “Biomedical named entity recognition with multilingual bert,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib53.1.1\">Proceedings of the 5th workshop on BioNLP open shared tasks</em>, 2019, pp. 56–61.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib54\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[54]</span>\n<span class=\"ltx_bibblock\">\nY. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, D. Bahri, T. Schuster, S. Zheng <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib54.1.1\">et al.</em>, “Ul2: Unifying language learning paradigms,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib54.2.2\">ICLR</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib55\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[55]</span>\n<span class=\"ltx_bibblock\">\nV. Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib55.1.1\">et al.</em>, “Multitask prompted training enables zero-shot task generalization,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib55.2.2\">ICLR</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib56\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[56]</span>\n<span class=\"ltx_bibblock\">\nB. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus, “St-moe: Designing stable and transferable sparse expert models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib56.1.1\">URL https://arxiv. org/abs/2202.08906</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib57\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[57]</span>\n<span class=\"ltx_bibblock\">\nA. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, W. L. Tam, Z. Ma, Y. Xue, J. Zhai, W. Chen, Z. Liu, P. Zhang, Y. Dong, and J. Tang, “GLM-130b: An open bilingual pre-trained model,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib57.1.1\">ICLR</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib58\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[58]</span>\n<span class=\"ltx_bibblock\">\nL. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained text-to-text transformer,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib58.1.1\">NAACL</em>, 2021, pp. 483–498.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib59\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[59]</span>\n<span class=\"ltx_bibblock\">\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib59.1.1\">et al.</em>, “Language models are few-shot learners,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib59.2.2\">Advances in neural information processing systems</em>, vol. 33, pp. 1877–1901, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib60\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[60]</span>\n<span class=\"ltx_bibblock\">\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib60.1.1\">et al.</em>, “Training language models to follow instructions with human feedback,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib60.2.2\">NeurIPS</em>, vol. 35, pp. 27 730–27 744, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib61\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[61]</span>\n<span class=\"ltx_bibblock\">\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib61.1.1\">et al.</em>, “Llama: Open and efficient foundation language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib61.2.2\">arXiv preprint arXiv:2302.13971</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib62\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[62]</span>\n<span class=\"ltx_bibblock\">\nE. Saravia, “Prompt Engineering Guide,” <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/dair-ai/Prompt-Engineering-Guide\" title=\"\">https://github.com/dair-ai/Prompt-Engineering-Guide</a>, 2022, accessed: 2022-12.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib63\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[63]</span>\n<span class=\"ltx_bibblock\">\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib63.1.1\">et al.</em>, “Chain-of-thought prompting elicits reasoning in large language models,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib63.2.2\">NeurIPS</em>.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib64\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[64]</span>\n<span class=\"ltx_bibblock\">\nS. Li, Y. Gao, H. Jiang, Q. Yin, Z. Li, X. Yan, C. Zhang, and B. Yin, “Graph reasoning for question answering with triplet retrieval,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib64.1.1\">ACL</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib65\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[65]</span>\n<span class=\"ltx_bibblock\">\nY. Wen, Z. Wang, and J. Sun, “Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib65.1.1\">arXiv preprint arXiv:2308.09729</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib66\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[66]</span>\n<span class=\"ltx_bibblock\">\nK. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor, “Freebase: A collaboratively created graph database for structuring human knowledge,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib66.1.1\">SIGMOD</em>, 2008, pp. 1247–1250.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib67\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[67]</span>\n<span class=\"ltx_bibblock\">\nS. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives, “Dbpedia: A nucleus for a web of open data,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib67.1.1\">The Semantic Web: 6th International Semantic Web Conference</em>.   Springer, 2007, pp. 722–735.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib68\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[68]</span>\n<span class=\"ltx_bibblock\">\nB. Xu, Y. Xu, J. Liang, C. Xie, B. Liang, W. Cui, and Y. Xiao, “Cn-dbpedia: A never-ending chinese knowledge extraction system,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib68.1.1\">30th International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems</em>.   Springer, 2017, pp. 428–438.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib69\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[69]</span>\n<span class=\"ltx_bibblock\">\nP. Hai-Nyzhnyk, “Vikidia as a universal multilingual online encyclopedia for children,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib69.1.1\">The Encyclopedia Herald of Ukraine</em>, vol. 14, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib70\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[70]</span>\n<span class=\"ltx_bibblock\">\nF. Ilievski, P. Szekely, and B. Zhang, “Cskg: The commonsense knowledge graph,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib70.1.1\">Extended Semantic Web Conference (ESWC)</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib71\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[71]</span>\n<span class=\"ltx_bibblock\">\nR. Speer, J. Chin, and C. Havasi, “Conceptnet 5.5: An open multilingual graph of general knowledge,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib71.1.1\">Proceedings of the AAAI conference on artificial intelligence</em>, vol. 31, no. 1, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib72\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[72]</span>\n<span class=\"ltx_bibblock\">\nH. Ji, P. Ke, S. Huang, F. Wei, X. Zhu, and M. Huang, “Language generation with multi-hop reasoning on commonsense knowledge graph,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib72.1.1\">EMNLP</em>, 2020, pp. 725–736.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib73\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[73]</span>\n<span class=\"ltx_bibblock\">\nJ. D. Hwang, C. Bhagavatula, R. Le Bras, J. Da, K. Sakaguchi, A. Bosselut, and Y. Choi, “(comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib73.1.1\">AAAI</em>, vol. 35, no. 7, 2021, pp. 6384–6392.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib74\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[74]</span>\n<span class=\"ltx_bibblock\">\nH. Zhang, X. Liu, H. Pan, Y. Song, and C. W.-K. Leung, “Aser: A large-scale eventuality knowledge graph,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib74.1.1\">Proceedings of the web conference 2020</em>, 2020, pp. 201–211.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib75\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[75]</span>\n<span class=\"ltx_bibblock\">\nH. Zhang, D. Khashabi, Y. Song, and D. Roth, “Transomcs: from linguistic graphs to commonsense knowledge,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib75.1.1\">IJCAI</em>, 2021, pp. 4004–4010.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib76\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[76]</span>\n<span class=\"ltx_bibblock\">\nZ. Li, X. Ding, T. Liu, J. E. Hu, and B. Van Durme, “Guided generation of cause and effect,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib76.1.1\">IJCAI</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib77\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[77]</span>\n<span class=\"ltx_bibblock\">\nO. Bodenreider, “The unified medical language system (umls): integrating biomedical terminology,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib77.1.1\">Nucleic acids research</em>, vol. 32, no. suppl_1, pp. D267–D270, 2004.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib78\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[78]</span>\n<span class=\"ltx_bibblock\">\nY. Liu, Q. Zeng, J. Ordieres Meré, and H. Yang, “Anticipating stock market of the renowned companies: a knowledge graph approach,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib78.1.1\">Complexity</em>, vol. 2019, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib79\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[79]</span>\n<span class=\"ltx_bibblock\">\nY. Zhu, W. Zhou, Y. Xu, J. Liu, Y. Tan <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib79.1.1\">et al.</em>, “Intelligent learning for knowledge graph towards geological data,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib79.2.2\">Scientific Programming</em>, vol. 2017, 2017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib80\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[80]</span>\n<span class=\"ltx_bibblock\">\nW. Choi and H. Lee, “Inference of biomedical relations among chemicals, genes, diseases, and symptoms using knowledge representation learning,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib80.1.1\">IEEE Access</em>, vol. 7, pp. 179 373–179 384, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib81\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[81]</span>\n<span class=\"ltx_bibblock\">\nF. Farazi, M. Salamanca, S. Mosbach, J. Akroyd, A. Eibeck, L. K. Aditya, A. Chadzynski, K. Pan, X. Zhou, S. Zhang <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib81.1.1\">et al.</em>, “Knowledge graph approach to combustion chemistry and interoperability,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib81.2.2\">ACS omega</em>, vol. 5, no. 29, pp. 18 342–18 348, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib82\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[82]</span>\n<span class=\"ltx_bibblock\">\nX. Wu, T. Jiang, Y. Zhu, and C. Bu, “Knowledge graph for china’s genealogy,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib82.1.1\">IEEE TKDE</em>, vol. 35, no. 1, pp. 634–646, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib83\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[83]</span>\n<span class=\"ltx_bibblock\">\nX. Zhu, Z. Li, X. Wang, X. Jiang, P. Sun, X. Wang, Y. Xiao, and N. J. Yuan, “Multi-modal knowledge graph construction and application: A survey,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib83.1.1\">IEEE TKDE</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib84\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[84]</span>\n<span class=\"ltx_bibblock\">\nS. Ferrada, B. Bustos, and A. Hogan, “Imgpedia: a linked dataset with content-based analysis of wikimedia images,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib84.1.1\">The Semantic Web–ISWC 2017</em>.   Springer, 2017, pp. 84–93.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib85\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[85]</span>\n<span class=\"ltx_bibblock\">\nY. Liu, H. Li, A. Garcia-Duran, M. Niepert, D. Onoro-Rubio, and D. S. Rosenblum, “Mmkg: multi-modal knowledge graphs,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib85.1.1\">The Semantic Web: 16th International Conference, ESWC 2019, Portorož, Slovenia, June 2–6, 2019, Proceedings 16</em>.   Springer, 2019, pp. 459–474.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib86\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[86]</span>\n<span class=\"ltx_bibblock\">\nM. Wang, H. Wang, G. Qi, and Q. Zheng, “Richpedia: a large-scale, comprehensive multi-modal knowledge graph,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib86.1.1\">Big Data Research</em>, vol. 22, p. 100159, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib87\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[87]</span>\n<span class=\"ltx_bibblock\">\nB. Shi, L. Ji, P. Lu, Z. Niu, and N. Duan, “Knowledge aware semantic concept expansion for image-text matching.” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib87.1.1\">IJCAI</em>, vol. 1, 2019, p. 2.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib88\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[88]</span>\n<span class=\"ltx_bibblock\">\nS. Shah, A. Mishra, N. Yadati, and P. P. Talukdar, “Kvqa: Knowledge-aware visual question answering,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib88.1.1\">AAAI</em>, vol. 33, no. 01, 2019, pp. 8876–8884.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib89\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[89]</span>\n<span class=\"ltx_bibblock\">\nR. Sun, X. Cao, Y. Zhao, J. Wan, K. Zhou, F. Zhang, Z. Wang, and K. Zheng, “Multi-modal knowledge graphs for recommender systems,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib89.1.1\">CIKM</em>, 2020, pp. 1405–1414.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib90\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[90]</span>\n<span class=\"ltx_bibblock\">\nS. Deng, C. Wang, Z. Li, N. Zhang, Z. Dai, H. Chen, F. Xiong, M. Yan, Q. Chen, M. Chen, J. Chen, J. Z. Pan, B. Hooi, and H. Chen, “Construction and applications of billion-scale pre-trained multimodal business knowledge graph,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib90.1.1\">ICDE</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib91\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[91]</span>\n<span class=\"ltx_bibblock\">\nC. Rosset, C. Xiong, M. Phan, X. Song, P. Bennett, and S. Tiwary, “Knowledge-aware language model pretraining,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib91.1.1\">arXiv preprint arXiv:2007.00655</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib92\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[92]</span>\n<span class=\"ltx_bibblock\">\nP. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive nlp tasks,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib92.1.1\">NeurIPS</em>, vol. 33, 2020, pp. 9459–9474.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib93\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[93]</span>\n<span class=\"ltx_bibblock\">\nY. Zhu, X. Wang, J. Chen, S. Qiao, Y. Ou, Y. Yao, S. Deng, H. Chen, and N. Zhang, “Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib93.1.1\">arXiv preprint arXiv:2305.13168</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib94\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[94]</span>\n<span class=\"ltx_bibblock\">\nZ. Zhang, X. Liu, Y. Zhang, Q. Su, X. Sun, and B. He, “Pretrain-kge: learning knowledge representation from pretrained language models,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib94.1.1\">EMNLP Finding</em>, 2020, pp. 259–266.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib95\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[95]</span>\n<span class=\"ltx_bibblock\">\nA. Kumar, A. Pandey, R. Gadia, and M. Mishra, “Building knowledge graph using pre-trained language model for learning entity-aware relationships,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib95.1.1\">2020 IEEE International Conference on Computing, Power and Communication Technologies (GUCON)</em>.   IEEE, 2020, pp. 310–315.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib96\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[96]</span>\n<span class=\"ltx_bibblock\">\nX. Xie, N. Zhang, Z. Li, S. Deng, H. Chen, F. Xiong, M. Chen, and H. Chen, “From discrimination to generation: Knowledge graph completion with generative transformer,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib96.1.1\">WWW</em>, 2022, pp. 162–165.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib97\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[97]</span>\n<span class=\"ltx_bibblock\">\nZ. Chen, C. Xu, F. Su, Z. Huang, and Y. Dou, “Incorporating structured sentences with time-enhanced bert for fully-inductive temporal relation prediction,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib97.1.1\">SIGIR</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib98\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[98]</span>\n<span class=\"ltx_bibblock\">\nD. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4: Enhancing vision-language understanding with advanced large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib98.1.1\">arXiv preprint arXiv:2304.10592</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib99\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[99]</span>\n<span class=\"ltx_bibblock\">\nM. Warren, D. A. Shamma, and P. J. Hayes, “Knowledge engineering with image data in real-world settings,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib99.1.1\">AAAI</em>, ser. CEUR Workshop Proceedings, vol. 2846, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib100\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[100]</span>\n<span class=\"ltx_bibblock\">\nR. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib100.1.1\">et al.</em>, “Lamda: Language models for dialog applications,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib100.2.2\">arXiv preprint arXiv:2201.08239</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib101\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[101]</span>\n<span class=\"ltx_bibblock\">\nY. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib101.1.1\">et al.</em>, “Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib101.2.2\">arXiv preprint arXiv:2107.02137</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib102\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[102]</span>\n<span class=\"ltx_bibblock\">\nT. Shen, Y. Mao, P. He, G. Long, A. Trischler, and W. Chen, “Exploiting structured knowledge in text via graph-guided representation learning,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib102.1.1\">EMNLP</em>, 2020, pp. 8980–8994.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib103\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[103]</span>\n<span class=\"ltx_bibblock\">\nD. Zhang, Z. Yuan, Y. Liu, F. Zhuang, H. Chen, and H. Xiong, “E-bert: A phrase and product knowledge enhanced language model for e-commerce,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib103.1.1\">arXiv preprint arXiv:2009.02835</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib104\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[104]</span>\n<span class=\"ltx_bibblock\">\nS. Li, X. Li, L. Shang, C. Sun, B. Liu, Z. Ji, X. Jiang, and Q. Liu, “Pre-training language models with deterministic factual knowledge,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib104.1.1\">EMNLP</em>, 2022, pp. 11 118–11 131.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib105\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[105]</span>\n<span class=\"ltx_bibblock\">\nM. Kang, J. Baek, and S. J. Hwang, “Kala: Knowledge-augmented language model adaptation,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib105.1.1\">NAACL</em>, 2022, pp. 5144–5167.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib106\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[106]</span>\n<span class=\"ltx_bibblock\">\nW. Xiong, J. Du, W. Y. Wang, and V. Stoyanov, “Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib106.1.1\">ICLR</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib107\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[107]</span>\n<span class=\"ltx_bibblock\">\nT. Sun, Y. Shao, X. Qiu, Q. Guo, Y. Hu, X. Huang, and Z. Zhang, “CoLAKE: Contextualized language and knowledge embedding,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib107.1.1\">Proceedings of the 28th International Conference on Computational Linguistics</em>, 2020, pp. 3660–3670.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib108\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[108]</span>\n<span class=\"ltx_bibblock\">\nT. Zhang, C. Wang, N. Hu, M. Qiu, C. Tang, X. He, and J. Huang, “DKPLM: decomposable knowledge-enhanced pre-trained language model for natural language understanding,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib108.1.1\">AAAI</em>, 2022, pp. 11 703–11 711.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib109\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[109]</span>\n<span class=\"ltx_bibblock\">\nJ. Wang, W. Huang, M. Qiu, Q. Shi, H. Wang, X. Li, and M. Gao, “Knowledge prompting in pre-trained language model for natural language understanding,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib109.1.1\">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, 2022, pp. 3164–3177.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib110\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[110]</span>\n<span class=\"ltx_bibblock\">\nH. Ye, N. Zhang, S. Deng, X. Chen, H. Chen, F. Xiong, X. Chen, and H. Chen, “Ontology-enhanced prompt-tuning for few-shot learning,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib110.1.1\">Proceedings of the ACM Web Conference 2022</em>, 2022, pp. 778–787.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib111\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[111]</span>\n<span class=\"ltx_bibblock\">\nH. Luo, Z. Tang, S. Peng, Y. Guo, W. Zhang, C. Ma, G. Dong, M. Song, W. Lin <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib111.1.1\">et al.</em>, “Chatkbqa: A generate-then-retrieve framework for knowledge base question answering with fine-tuned large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib111.2.2\">arXiv preprint arXiv:2310.08975</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib112\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[112]</span>\n<span class=\"ltx_bibblock\">\nL. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faithful and interpretable large language model reasoning,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib112.1.1\">arXiv preprint arxiv:2310.01061</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib113\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[113]</span>\n<span class=\"ltx_bibblock\">\nR. Logan, N. F. Liu, M. E. Peters, M. Gardner, and S. Singh, “Barack’s wife hillary: Using knowledge graphs for fact-aware language modeling,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib113.1.1\">ACL</em>, 2019, pp. 5962–5971.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib114\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[114]</span>\n<span class=\"ltx_bibblock\">\nK. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang, “Realm: Retrieval-augmented language model pre-training,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib114.1.1\">ICML</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib115\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[115]</span>\n<span class=\"ltx_bibblock\">\nY. Wu, Y. Zhao, B. Hu, P. Minervini, P. Stenetorp, and S. Riedel, “An efficient memory-augmented transformer for knowledge-intensive NLP tasks,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib115.1.1\">EMNLP</em>, 2022, pp. 5184–5196.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib116\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[116]</span>\n<span class=\"ltx_bibblock\">\nL. Luo, J. Ju, B. Xiong, Y.-F. Li, G. Haffari, and S. Pan, “Chatrule: Mining logical rules with large language models for knowledge graph reasoning,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib116.1.1\">arXiv preprint arXiv:2309.01538</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib117\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[117]</span>\n<span class=\"ltx_bibblock\">\nJ. Wang, Q. Sun, N. Chen, X. Li, and M. Gao, “Boosting language models reasoning with chain-of-knowledge prompting,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib117.1.1\">arXiv preprint arXiv:2306.06427</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib118\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[118]</span>\n<span class=\"ltx_bibblock\">\nZ. Jiang, F. F. Xu, J. Araki, and G. Neubig, “How can we know what language models know?” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib118.1.1\">Transactions of the Association for Computational Linguistics</em>, vol. 8, pp. 423–438, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib119\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[119]</span>\n<span class=\"ltx_bibblock\">\nT. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh, “Autoprompt: Eliciting knowledge from language models with automatically generated prompts,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib119.1.1\">arXiv preprint arXiv:2010.15980</em>, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib120\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[120]</span>\n<span class=\"ltx_bibblock\">\nZ. Meng, F. Liu, E. Shareghi, Y. Su, C. Collins, and N. Collier, “Rewire-then-probe: A contrastive recipe for probing biomedical knowledge of pre-trained language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib120.1.1\">arXiv preprint arXiv:2110.08173</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib121\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[121]</span>\n<span class=\"ltx_bibblock\">\nL. Luo, T.-T. Vu, D. Phung, and G. Haffari, “Systematic assessment of factual knowledge in large language models,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib121.1.1\">EMNLP</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib122\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[122]</span>\n<span class=\"ltx_bibblock\">\nV. Swamy, A. Romanou, and M. Jaggi, “Interpreting language models through knowledge graph extraction,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib122.1.1\">arXiv preprint arXiv:2111.08546</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib123\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[123]</span>\n<span class=\"ltx_bibblock\">\nS. Li, X. Li, L. Shang, Z. Dong, C. Sun, B. Liu, Z. Ji, X. Jiang, and Q. Liu, “How pre-trained language models capture factual knowledge? a causal-inspired analysis,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib123.1.1\">arXiv preprint arXiv:2203.16747</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib124\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[124]</span>\n<span class=\"ltx_bibblock\">\nH. Tian, C. Gao, X. Xiao, H. Liu, B. He, H. Wu, H. Wang, and F. Wu, “SKEP: Sentiment knowledge enhanced pre-training for sentiment analysis,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib124.1.1\">ACL</em>, 2020, pp. 4067–4076.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib125\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[125]</span>\n<span class=\"ltx_bibblock\">\nW. Yu, C. Zhu, Y. Fang, D. Yu, S. Wang, Y. Xu, M. Zeng, and M. Jiang, “Dict-BERT: Enhancing language model pre-training with dictionary,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib125.1.1\">ACL</em>, 2022, pp. 1907–1918.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib126\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[126]</span>\n<span class=\"ltx_bibblock\">\nT. McCoy, E. Pavlick, and T. Linzen, “Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib126.1.1\">ACL</em>, 2019, pp. 3428–3448.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib127\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[127]</span>\n<span class=\"ltx_bibblock\">\nD. Wilmot and F. Keller, “Memory and knowledge augmented language models for inferring salience in long-form stories,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib127.1.1\">EMNLP</em>, 2021, pp. 851–865.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib128\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[128]</span>\n<span class=\"ltx_bibblock\">\nL. Adolphs, S. Dhuliawala, and T. Hofmann, “How to query language models?” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib128.1.1\">arXiv preprint arXiv:2108.01928</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib129\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[129]</span>\n<span class=\"ltx_bibblock\">\nM. Sung, J. Lee, S. Yi, M. Jeon, S. Kim, and J. Kang, “Can language models be biomedical knowledge bases?” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib129.1.1\">EMNLP</em>, 2021, pp. 4723–4734.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib130\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[130]</span>\n<span class=\"ltx_bibblock\">\nA. Mallen, A. Asai, V. Zhong, R. Das, H. Hajishirzi, and D. Khashabi, “When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib130.1.1\">arXiv preprint arXiv:2212.10511</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib131\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[131]</span>\n<span class=\"ltx_bibblock\">\nM. Yasunaga, H. Ren, A. Bosselut, P. Liang, and J. Leskovec, “QA-GNN: Reasoning with language models and knowledge graphs for question answering,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib131.1.1\">NAACL</em>, 2021, pp. 535–546.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib132\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[132]</span>\n<span class=\"ltx_bibblock\">\nM. Nayyeri, Z. Wang, M. Akter, M. M. Alam, M. R. A. H. Rony, J. Lehmann, S. Staab <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib132.1.1\">et al.</em>, “Integrating knowledge graph embedding and pretrained language models in hypercomplex spaces,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib132.2.2\">arXiv preprint arXiv:2208.02743</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib133\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[133]</span>\n<span class=\"ltx_bibblock\">\nN. Huang, Y. R. Deshpande, Y. Liu, H. Alberts, K. Cho, C. Vania, and I. Calixto, “Endowing language models with multimodal knowledge graph representations,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib133.1.1\">arXiv preprint arXiv:2206.13163</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib134\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[134]</span>\n<span class=\"ltx_bibblock\">\nM. M. Alam, M. R. A. H. Rony, M. Nayyeri, K. Mohiuddin, M. M. Akter, S. Vahdati, and J. Lehmann, “Language model guided knowledge graph embeddings,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib134.1.1\">IEEE Access</em>, vol. 10, pp. 76 008–76 020, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib135\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[135]</span>\n<span class=\"ltx_bibblock\">\nX. Wang, Q. He, J. Liang, and Y. Xiao, “Language models as knowledge embeddings,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib135.1.1\">arXiv preprint arXiv:2206.12617</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib136\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[136]</span>\n<span class=\"ltx_bibblock\">\nN. Zhang, X. Xie, X. Chen, S. Deng, C. Tan, F. Huang, X. Cheng, and H. Chen, “Reasoning through memorization: Nearest neighbor knowledge graph embeddings,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib136.1.1\">arXiv preprint arXiv:2201.05575</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib137\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[137]</span>\n<span class=\"ltx_bibblock\">\nX. Xie, Z. Li, X. Wang, Y. Zhu, N. Zhang, J. Zhang, S. Cheng, B. Tian, S. Deng, F. Xiong, and H. Chen, “Lambdakg: A library for pre-trained language model-based knowledge graph embeddings,” 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib138\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[138]</span>\n<span class=\"ltx_bibblock\">\nB. Kim, T. Hong, Y. Ko, and J. Seo, “Multi-task learning for knowledge graph completion with pre-trained language models,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib138.1.1\">COLING</em>, 2020, pp. 1737–1743.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib139\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[139]</span>\n<span class=\"ltx_bibblock\">\nX. Lv, Y. Lin, Y. Cao, L. Hou, J. Li, Z. Liu, P. Li, and J. Zhou, “Do pre-trained models benefit knowledge graph completion? A reliable evaluation and a reasonable approach,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib139.1.1\">ACL</em>, 2022, pp. 3570–3581.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib140\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[140]</span>\n<span class=\"ltx_bibblock\">\nJ. Shen, C. Wang, L. Gong, and D. Song, “Joint language semantic and structure embedding for knowledge graph completion,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib140.1.1\">COLING</em>, 2022, pp. 1965–1978.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib141\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[141]</span>\n<span class=\"ltx_bibblock\">\nB. Choi, D. Jang, and Y. Ko, “MEM-KGC: masked entity model for knowledge graph completion with pre-trained language model,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib141.1.1\">IEEE Access</em>, vol. 9, pp. 132 025–132 032, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib142\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[142]</span>\n<span class=\"ltx_bibblock\">\nB. Choi and Y. Ko, “Knowledge graph extension with a pre-trained language model via unified learning method,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib142.1.1\">Knowl. Based Syst.</em>, vol. 262, p. 110245, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib143\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[143]</span>\n<span class=\"ltx_bibblock\">\nB. Wang, T. Shen, G. Long, T. Zhou, Y. Wang, and Y. Chang, “Structure-augmented text representation learning for efficient knowledge graph completion,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib143.1.1\">WWW</em>, 2021, pp. 1737–1748.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib144\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[144]</span>\n<span class=\"ltx_bibblock\">\nL. Wang, W. Zhao, Z. Wei, and J. Liu, “Simkgc: Simple contrastive knowledge graph completion with pre-trained language models,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib144.1.1\">ACL</em>, 2022, pp. 4281–4294.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib145\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[145]</span>\n<span class=\"ltx_bibblock\">\nD. Li, M. Yi, and Y. He, “Lp-bert: Multi-task pre-training knowledge graph bert for link prediction,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib145.1.1\">arXiv preprint arXiv:2201.04843</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib146\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[146]</span>\n<span class=\"ltx_bibblock\">\nA. Saxena, A. Kochsiek, and R. Gemulla, “Sequence-to-sequence knowledge graph completion and question answering,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib146.1.1\">ACL</em>, 2022, pp. 2814–2828.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib147\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[147]</span>\n<span class=\"ltx_bibblock\">\nC. Chen, Y. Wang, B. Li, and K. Lam, “Knowledge is flat: A seq2seq generative framework for various knowledge graph completion,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib147.1.1\">COLING</em>, 2022, pp. 4005–4017.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib148\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[148]</span>\n<span class=\"ltx_bibblock\">\nM. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, “Deep contextualized word representations,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib148.1.1\">NAACL</em>, 2018, pp. 2227–2237.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib149\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[149]</span>\n<span class=\"ltx_bibblock\">\nH. Yan, T. Gui, J. Dai, Q. Guo, Z. Zhang, and X. Qiu, “A unified generative framework for various NER subtasks,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib149.1.1\">ACL</em>, 2021, pp. 5808–5822.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib150\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[150]</span>\n<span class=\"ltx_bibblock\">\nY. Onoe and G. Durrett, “Learning to denoise distantly-labeled data for entity typing,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib150.1.1\">NAACL</em>, 2019, pp. 2407–2417.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib151\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[151]</span>\n<span class=\"ltx_bibblock\">\nY. Onoe, M. Boratko, A. McCallum, and G. Durrett, “Modeling fine-grained entity types with box embeddings,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib151.1.1\">ACL</em>, 2021, pp. 2051–2064.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib152\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[152]</span>\n<span class=\"ltx_bibblock\">\nB. Z. Li, S. Min, S. Iyer, Y. Mehdad, and W. Yih, “Efficient one-pass end-to-end entity linking for questions,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib152.1.1\">EMNLP</em>, 2020, pp. 6433–6441.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib153\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[153]</span>\n<span class=\"ltx_bibblock\">\nT. Ayoola, S. Tyagi, J. Fisher, C. Christodoulopoulos, and A. Pierleoni, “Refined: An efficient zero-shot-capable approach to end-to-end entity linking,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib153.1.1\">NAACL</em>, 2022, pp. 209–220.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib154\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[154]</span>\n<span class=\"ltx_bibblock\">\nM. Joshi, O. Levy, L. Zettlemoyer, and D. S. Weld, “BERT for coreference resolution: Baselines and analysis,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib154.1.1\">EMNLP</em>, 2019, pp. 5802–5807.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib155\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[155]</span>\n<span class=\"ltx_bibblock\">\nM. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy, “Spanbert: Improving pre-training by representing and predicting spans,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib155.1.1\">Trans. Assoc. Comput. Linguistics</em>, vol. 8, pp. 64–77, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib156\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[156]</span>\n<span class=\"ltx_bibblock\">\nA. Caciularu, A. Cohan, I. Beltagy, M. E. Peters, A. Cattan, and I. Dagan, “CDLM: cross-document language modeling,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib156.1.1\">EMNLP</em>, 2021, pp. 2648–2662.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib157\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[157]</span>\n<span class=\"ltx_bibblock\">\nA. Cattan, A. Eirew, G. Stanovsky, M. Joshi, and I. Dagan, “Cross-document coreference resolution over predicted mentions,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib157.1.1\">ACL</em>, 2021, pp. 5100–5107.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib158\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[158]</span>\n<span class=\"ltx_bibblock\">\nY. Wang, Y. Shen, and H. Jin, “An end-to-end actor-critic-based neural coreference resolution system,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib158.1.1\">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021</em>, 2021, pp. 7848–7852.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib159\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[159]</span>\n<span class=\"ltx_bibblock\">\nP. Shi and J. Lin, “Simple BERT models for relation extraction and semantic role labeling,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib159.1.1\">CoRR</em>, vol. abs/1904.05255, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib160\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[160]</span>\n<span class=\"ltx_bibblock\">\nS. Park and H. Kim, “Improving sentence-level relation extraction through curriculum learning,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib160.1.1\">CoRR</em>, vol. abs/2107.09332, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib161\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[161]</span>\n<span class=\"ltx_bibblock\">\nY. Ma, A. Wang, and N. Okazaki, “DREEAM: guiding attention with evidence for improving document-level relation extraction,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib161.1.1\">EACL</em>, 2023, pp. 1963–1975.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib162\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[162]</span>\n<span class=\"ltx_bibblock\">\nQ. Guo, Y. Sun, G. Liu, Z. Wang, Z. Ji, Y. Shen, and X. Wang, “Constructing chinese historical literature knowledge graph based on bert,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib162.1.1\">Web Information Systems and Applications: 18th International Conference, WISA 2021, Kaifeng, China, September 24–26, 2021, Proceedings 18</em>.   Springer, 2021, pp. 323–334.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib163\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[163]</span>\n<span class=\"ltx_bibblock\">\nJ. Han, N. Collier, W. Buntine, and E. Shareghi, “Pive: Prompting with iterative verification improving graph-based generative capability of llms,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib163.1.1\">arXiv preprint arXiv:2305.12392</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib164\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[164]</span>\n<span class=\"ltx_bibblock\">\nA. Bosselut, H. Rashkin, M. Sap, C. Malaviya, A. Celikyilmaz, and Y. Choi, “Comet: Commonsense transformers for knowledge graph construction,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib164.1.1\">ACL</em>, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib165\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[165]</span>\n<span class=\"ltx_bibblock\">\nS. Hao, B. Tan, K. Tang, H. Zhang, E. P. Xing, and Z. Hu, “Bertnet: Harvesting knowledge graphs from pretrained language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib165.1.1\">arXiv preprint arXiv:2206.14268</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib166\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[166]</span>\n<span class=\"ltx_bibblock\">\nP. West, C. Bhagavatula, J. Hessel, J. Hwang, L. Jiang, R. Le Bras, X. Lu, S. Welleck, and Y. Choi, “Symbolic knowledge distillation: from general language models to commonsense models,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib166.1.1\">NAACL</em>, 2022, pp. 4602–4625.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib167\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[167]</span>\n<span class=\"ltx_bibblock\">\nL. F. R. Ribeiro, M. Schmitt, H. Schütze, and I. Gurevych, “Investigating pretrained language models for graph-to-text generation,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib167.1.1\">Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</em>, 2021, pp. 211–227.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib168\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[168]</span>\n<span class=\"ltx_bibblock\">\nJ. Li, T. Tang, W. X. Zhao, Z. Wei, N. J. Yuan, and J.-R. Wen, “Few-shot knowledge graph-to-text generation with pretrained language models,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib168.1.1\">ACL</em>, 2021, pp. 1558–1568.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib169\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[169]</span>\n<span class=\"ltx_bibblock\">\nA. Colas, M. Alvandipour, and D. Z. Wang, “GAP: A graph-aware language model framework for knowledge graph-to-text generation,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib169.1.1\">Proceedings of the 29th International Conference on Computational Linguistics</em>, 2022, pp. 5755–5769.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib170\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[170]</span>\n<span class=\"ltx_bibblock\">\nZ. Jin, Q. Guo, X. Qiu, and Z. Zhang, “GenWiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib170.1.1\">Proceedings of the 28th International Conference on Computational Linguistics</em>, 2020, pp. 2398–2409.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib171\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[171]</span>\n<span class=\"ltx_bibblock\">\nW. Chen, Y. Su, X. Yan, and W. Y. Wang, “KGPT: Knowledge-grounded pre-training for data-to-text generation,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib171.1.1\">EMNLP</em>, 2020, pp. 8635–8648.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib172\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[172]</span>\n<span class=\"ltx_bibblock\">\nD. Lukovnikov, A. Fischer, and J. Lehmann, “Pretrained transformers for simple question answering over knowledge graphs,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib172.1.1\">The Semantic Web–ISWC 2019: 18th International Semantic Web Conference, Auckland, New Zealand, October 26–30, 2019, Proceedings, Part I 18</em>.   Springer, 2019, pp. 470–486.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib173\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[173]</span>\n<span class=\"ltx_bibblock\">\nD. Luo, J. Su, and S. Yu, “A bert-based approach with relation-aware attention for knowledge base question answering,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib173.1.1\">IJCNN</em>.   IEEE, 2020, pp. 1–8.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib174\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[174]</span>\n<span class=\"ltx_bibblock\">\nN. Hu, Y. Wu, G. Qi, D. Min, J. Chen, J. Z. Pan, and Z. Ali, “An empirical study of pre-trained language models in simple knowledge graph question answering,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib174.1.1\">arXiv preprint arXiv:2303.10368</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib175\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[175]</span>\n<span class=\"ltx_bibblock\">\nY. Xu, C. Zhu, R. Xu, Y. Liu, M. Zeng, and X. Huang, “Fusing context into knowledge graph for commonsense question answering,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib175.1.1\">ACL</em>, 2021, pp. 1201–1207.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib176\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[176]</span>\n<span class=\"ltx_bibblock\">\nM. Zhang, R. Dai, M. Dong, and T. He, “Drlk: Dynamic hierarchical reasoning with language model and knowledge graph for question answering,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib176.1.1\">EMNLP</em>, 2022, pp. 5123–5133.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib177\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[177]</span>\n<span class=\"ltx_bibblock\">\nZ. Hu, Y. Xu, W. Yu, S. Wang, Z. Yang, C. Zhu, K.-W. Chang, and Y. Sun, “Empowering language models with knowledge graph reasoning for open-domain question answering,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib177.1.1\">EMNLP</em>, 2022, pp. 9562–9581.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib178\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[178]</span>\n<span class=\"ltx_bibblock\">\nX. Zhang, A. Bosselut, M. Yasunaga, H. Ren, P. Liang, C. D. Manning, and J. Leskovec, “Greaselm: Graph reasoning enhanced language models,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib178.1.1\">ICLR</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib179\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[179]</span>\n<span class=\"ltx_bibblock\">\nX. Cao and Y. Liu, “Relmkg: reasoning with pre-trained language models and knowledge graphs for complex question answering,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib179.1.1\">Applied Intelligence</em>, pp. 1–15, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib180\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[180]</span>\n<span class=\"ltx_bibblock\">\nX. Huang, J. Zhang, D. Li, and P. Li, “Knowledge graph embedding based question answering,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib180.1.1\">WSDM</em>, 2019, pp. 105–113.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib181\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[181]</span>\n<span class=\"ltx_bibblock\">\nH. Wang, F. Zhang, X. Xie, and M. Guo, “Dkn: Deep knowledge-aware network for news recommendation,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib181.1.1\">WWW</em>, 2018, pp. 1835–1844.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib182\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[182]</span>\n<span class=\"ltx_bibblock\">\nB. Yang, S. W.-t. Yih, X. He, J. Gao, and L. Deng, “Embedding entities and relations for learning and inference in knowledge bases,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib182.1.1\">ICLR</em>, 2015.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib183\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[183]</span>\n<span class=\"ltx_bibblock\">\nW. Xiong, M. Yu, S. Chang, X. Guo, and W. Y. Wang, “One-shot relational learning for knowledge graphs,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib183.1.1\">EMNLP</em>, 2018, pp. 1980–1990.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib184\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[184]</span>\n<span class=\"ltx_bibblock\">\nP. Wang, J. Han, C. Li, and R. Pan, “Logic attention based neighborhood aggregation for inductive knowledge graph embedding,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib184.1.1\">AAAI</em>, vol. 33, no. 01, 2019, pp. 7152–7159.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib185\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[185]</span>\n<span class=\"ltx_bibblock\">\nY. Lin, Z. Liu, M. Sun, Y. Liu, and X. Zhu, “Learning entity and relation embeddings for knowledge graph completion,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib185.1.1\">Proceedings of the AAAI conference on artificial intelligence</em>, vol. 29, no. 1, 2015.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib186\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[186]</span>\n<span class=\"ltx_bibblock\">\nC. Chen, Y. Wang, A. Sun, B. Li, and L. Kwok-Yan, “Dipping plms sauce: Bridging structure and text for effective knowledge graph completion via conditional soft prompting,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib186.1.1\">ACL</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib187\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[187]</span>\n<span class=\"ltx_bibblock\">\nJ. Lovelace and C. P. Rosé, “A framework for adapting pre-trained language models to knowledge graph completion,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib187.1.1\">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, 2022, pp. 5937–5955.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib188\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[188]</span>\n<span class=\"ltx_bibblock\">\nJ. Fu, L. Feng, Q. Zhang, X. Huang, and P. Liu, “Larger-context tagging: When and why does it work?” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib188.1.1\">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</em>, 2021, pp. 1463–1475.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib189\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[189]</span>\n<span class=\"ltx_bibblock\">\nX. Liu, K. Ji, Y. Fu, Z. Du, Z. Yang, and J. Tang, “P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib189.1.1\">CoRR</em>, vol. abs/2110.07602, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib190\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[190]</span>\n<span class=\"ltx_bibblock\">\nJ. Yu, B. Bohnet, and M. Poesio, “Named entity recognition as dependency parsing,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib190.1.1\">ACL</em>, 2020, pp. 6470–6476.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib191\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[191]</span>\n<span class=\"ltx_bibblock\">\nF. Li, Z. Lin, M. Zhang, and D. Ji, “A span-based model for joint overlapped and discontinuous named entity recognition,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib191.1.1\">ACL</em>, 2021, pp. 4814–4828.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib192\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[192]</span>\n<span class=\"ltx_bibblock\">\nC. Tan, W. Qiu, M. Chen, R. Wang, and F. Huang, “Boundary enhanced neural span classification for nested named entity recognition,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib192.1.1\">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020</em>, 2020, pp. 9016–9023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib193\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[193]</span>\n<span class=\"ltx_bibblock\">\nY. Xu, H. Huang, C. Feng, and Y. Hu, “A supervised multi-head self-attention network for nested named entity recognition,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib193.1.1\">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021</em>, 2021, pp. 14 185–14 193.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib194\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[194]</span>\n<span class=\"ltx_bibblock\">\nJ. Yu, B. Ji, S. Li, J. Ma, H. Liu, and H. Xu, “S-NER: A concise and efficient span-based model for named entity recognition,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib194.1.1\">Sensors</em>, vol. 22, no. 8, p. 2852, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib195\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[195]</span>\n<span class=\"ltx_bibblock\">\nY. Fu, C. Tan, M. Chen, S. Huang, and F. Huang, “Nested named entity recognition with partially-observed treecrfs,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib195.1.1\">AAAI</em>, 2021, pp. 12 839–12 847.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib196\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[196]</span>\n<span class=\"ltx_bibblock\">\nC. Lou, S. Yang, and K. Tu, “Nested named entity recognition as latent lexicalized constituency parsing,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib196.1.1\">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, 2022, pp. 6183–6198.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib197\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[197]</span>\n<span class=\"ltx_bibblock\">\nS. Yang and K. Tu, “Bottom-up constituency parsing and nested named entity recognition with pointer networks,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib197.1.1\">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, 2022, pp. 2403–2416.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib198\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[198]</span>\n<span class=\"ltx_bibblock\">\nF. Li, Z. Lin, M. Zhang, and D. Ji, “A span-based model for joint overlapped and discontinuous named entity recognition,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib198.1.1\">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021</em>, 2021, pp. 4814–4828.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib199\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[199]</span>\n<span class=\"ltx_bibblock\">\nQ. Liu, H. Lin, X. Xiao, X. Han, L. Sun, and H. Wu, “Fine-grained entity typing via label reasoning,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib199.1.1\">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>, 2021, pp. 4611–4622.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib200\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[200]</span>\n<span class=\"ltx_bibblock\">\nH. Dai, Y. Song, and H. Wang, “Ultra-fine entity typing with weak supervision from a masked language model,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib200.1.1\">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021</em>, 2021, pp. 1790–1799.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib201\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[201]</span>\n<span class=\"ltx_bibblock\">\nN. Ding, Y. Chen, X. Han, G. Xu, X. Wang, P. Xie, H. Zheng, Z. Liu, J. Li, and H. Kim, “Prompt-learning for fine-grained entity typing,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib201.1.1\">Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, 2022, pp. 6888–6901.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib202\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[202]</span>\n<span class=\"ltx_bibblock\">\nW. Pan, W. Wei, and F. Zhu, “Automatic noisy label correction for fine-grained entity typing,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib202.1.1\">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022</em>, 2022, pp. 4317–4323.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib203\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[203]</span>\n<span class=\"ltx_bibblock\">\nB. Li, W. Yin, and M. Chen, “Ultra-fine entity typing with indirect supervision from natural language inference,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib203.1.1\">Trans. Assoc. Comput. Linguistics</em>, vol. 10, pp. 607–622, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib204\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[204]</span>\n<span class=\"ltx_bibblock\">\nS. Broscheit, “Investigating entity knowledge in BERT with simple neural end-to-end entity linking,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib204.1.1\">CoRR</em>, vol. abs/2003.05473, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib205\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[205]</span>\n<span class=\"ltx_bibblock\">\nN. D. Cao, G. Izacard, S. Riedel, and F. Petroni, “Autoregressive entity retrieval,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib205.1.1\">9th ICLR, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib206\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[206]</span>\n<span class=\"ltx_bibblock\">\nN. D. Cao, L. Wu, K. Popat, M. Artetxe, N. Goyal, M. Plekhanov, L. Zettlemoyer, N. Cancedda, S. Riedel, and F. Petroni, “Multilingual autoregressive entity linking,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib206.1.1\">Trans. Assoc. Comput. Linguistics</em>, vol. 10, pp. 274–290, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib207\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[207]</span>\n<span class=\"ltx_bibblock\">\nN. D. Cao, W. Aziz, and I. Titov, “Highly parallel autoregressive entity linking with discriminative correction,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib207.1.1\">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>, 2021, pp. 7662–7669.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib208\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[208]</span>\n<span class=\"ltx_bibblock\">\nK. Lee, L. He, and L. Zettlemoyer, “Higher-order coreference resolution with coarse-to-fine inference,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib208.1.1\">NAACL</em>, 2018, pp. 687–692.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib209\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[209]</span>\n<span class=\"ltx_bibblock\">\nT. M. Lai, T. Bui, and D. S. Kim, “End-to-end neural coreference resolution revisited: A simple yet effective baseline,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib209.1.1\">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022</em>, 2022, pp. 8147–8151.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib210\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[210]</span>\n<span class=\"ltx_bibblock\">\nW. Wu, F. Wang, A. Yuan, F. Wu, and J. Li, “Corefqa: Coreference resolution as query-based span prediction,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib210.1.1\">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>, 2020, pp. 6953–6963.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib211\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[211]</span>\n<span class=\"ltx_bibblock\">\nT. M. Lai, H. Ji, T. Bui, Q. H. Tran, F. Dernoncourt, and W. Chang, “A context-dependent gated module for incorporating symbolic semantics into event coreference resolution,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib211.1.1\">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</em>, 2021, pp. 3491–3499.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib212\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[212]</span>\n<span class=\"ltx_bibblock\">\nY. Kirstain, O. Ram, and O. Levy, “Coreference resolution without span representations,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib212.1.1\">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual Event, August 1-6, 2021</em>, 2021, pp. 14–19.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib213\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[213]</span>\n<span class=\"ltx_bibblock\">\nR. Thirukovalluru, N. Monath, K. Shridhar, M. Zaheer, M. Sachan, and A. McCallum, “Scaling within document coreference to long texts,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib213.1.1\">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021</em>, ser. Findings of ACL, vol. ACL/IJCNLP 2021, 2021, pp. 3921–3931.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib214\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[214]</span>\n<span class=\"ltx_bibblock\">\nI. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-document transformer,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib214.1.1\">CoRR</em>, vol. abs/2004.05150, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib215\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[215]</span>\n<span class=\"ltx_bibblock\">\nC. Alt, M. Hübner, and L. Hennig, “Improving relation extraction by pre-trained language representations,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib215.1.1\">1st Conference on Automated Knowledge Base Construction, AKBC 2019, Amherst, MA, USA, May 20-22, 2019</em>, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib216\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[216]</span>\n<span class=\"ltx_bibblock\">\nL. B. Soares, N. FitzGerald, J. Ling, and T. Kwiatkowski, “Matching the blanks: Distributional similarity for relation learning,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib216.1.1\">ACL</em>, 2019, pp. 2895–2905.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib217\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[217]</span>\n<span class=\"ltx_bibblock\">\nS. Lyu and H. Chen, “Relation classification with entity type restriction,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib217.1.1\">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021</em>, ser. Findings of ACL, vol. ACL/IJCNLP 2021, 2021, pp. 390–395.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib218\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[218]</span>\n<span class=\"ltx_bibblock\">\nJ. Zheng and Z. Chen, “Sentence-level relation extraction via contrastive learning with descriptive relation prompts,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib218.1.1\">CoRR</em>, vol. abs/2304.04935, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib219\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[219]</span>\n<span class=\"ltx_bibblock\">\nH. Wang, C. Focke, R. Sylvester, N. Mishra, and W. Y. Wang, “Fine-tune bert for docred with two-step process,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib219.1.1\">CoRR</em>, vol. abs/1909.11898, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib220\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[220]</span>\n<span class=\"ltx_bibblock\">\nH. Tang, Y. Cao, Z. Zhang, J. Cao, F. Fang, S. Wang, and P. Yin, “HIN: hierarchical inference network for document-level relation extraction,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib220.1.1\">PAKDD</em>, ser. Lecture Notes in Computer Science, vol. 12084, 2020, pp. 197–209.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib221\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[221]</span>\n<span class=\"ltx_bibblock\">\nD. Wang, W. Hu, E. Cao, and W. Sun, “Global-to-local neural networks for document-level relation extraction,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib221.1.1\">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>, 2020, pp. 3711–3721.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib222\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[222]</span>\n<span class=\"ltx_bibblock\">\nS. Zeng, Y. Wu, and B. Chang, “SIRE: separate intra- and inter-sentential reasoning for document-level relation extraction,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib222.1.1\">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021</em>, ser. Findings of ACL, vol. ACL/IJCNLP 2021, 2021, pp. 524–534.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib223\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[223]</span>\n<span class=\"ltx_bibblock\">\nG. Nan, Z. Guo, I. Sekulic, and W. Lu, “Reasoning with latent structure refinement for document-level relation extraction,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib223.1.1\">ACL</em>, 2020, pp. 1546–1557.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib224\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[224]</span>\n<span class=\"ltx_bibblock\">\nS. Zeng, R. Xu, B. Chang, and L. Li, “Double graph based reasoning for document-level relation extraction,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib224.1.1\">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>, 2020, pp. 1630–1640.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib225\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[225]</span>\n<span class=\"ltx_bibblock\">\nN. Zhang, X. Chen, X. Xie, S. Deng, C. Tan, M. Chen, F. Huang, L. Si, and H. Chen, “Document-level relation extraction as semantic segmentation,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib225.1.1\">IJCAI</em>, 2021, pp. 3999–4006.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib226\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[226]</span>\n<span class=\"ltx_bibblock\">\nO. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib226.1.1\">Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015 - 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III</em>, ser. Lecture Notes in Computer Science, vol. 9351, 2015, pp. 234–241.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib227\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[227]</span>\n<span class=\"ltx_bibblock\">\nW. Zhou, K. Huang, T. Ma, and J. Huang, “Document-level relation extraction with adaptive thresholding and localized context pooling,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib227.1.1\">AAAI</em>, 2021, pp. 14 612–14 620.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib228\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[228]</span>\n<span class=\"ltx_bibblock\">\nC. Gardent, A. Shimorina, S. Narayan, and L. Perez-Beltrachini, “The WebNLG challenge: Generating text from RDF data,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib228.1.1\">Proceedings of the 10th International Conference on Natural Language Generation</em>, 2017, pp. 124–133.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib229\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[229]</span>\n<span class=\"ltx_bibblock\">\nJ. Guan, Y. Wang, and M. Huang, “Story ending generation with incremental encoding and commonsense knowledge,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib229.1.1\">AAAI</em>, 2019, pp. 6473–6480.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib230\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[230]</span>\n<span class=\"ltx_bibblock\">\nH. Zhou, T. Young, M. Huang, H. Zhao, J. Xu, and X. Zhu, “Commonsense knowledge aware conversation generation with graph attention,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib230.1.1\">IJCAI</em>, 2018, pp. 4623–4629.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib231\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[231]</span>\n<span class=\"ltx_bibblock\">\nM. Kale and A. Rastogi, “Text-to-text pre-training for data-to-text tasks,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib231.1.1\">Proceedings of the 13th International Conference on Natural Language Generation</em>, 2020, pp. 97–102.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib232\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[232]</span>\n<span class=\"ltx_bibblock\">\nM. Mintz, S. Bills, R. Snow, and D. Jurafsky, “Distant supervision for relation extraction without labeled data,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib232.1.1\">ACL</em>, 2009, pp. 1003–1011.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib233\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[233]</span>\n<span class=\"ltx_bibblock\">\nA. Saxena, A. Tripathi, and P. Talukdar, “Improving multi-hop question answering over knowledge graphs using knowledge base embeddings,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib233.1.1\">ACL</em>, 2020, pp. 4498–4507.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib234\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[234]</span>\n<span class=\"ltx_bibblock\">\nY. Feng, X. Chen, B. Y. Lin, P. Wang, J. Yan, and X. Ren, “Scalable multi-hop relational reasoning for knowledge-aware question answering,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib234.1.1\">EMNLP</em>, 2020, pp. 1295–1309.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib235\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[235]</span>\n<span class=\"ltx_bibblock\">\nY. Yan, R. Li, S. Wang, H. Zhang, Z. Daoguang, F. Zhang, W. Wu, and W. Xu, “Large-scale relation learning for question answering over knowledge bases with pre-trained language models,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib235.1.1\">EMNLP</em>, 2021, pp. 3653–3660.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib236\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[236]</span>\n<span class=\"ltx_bibblock\">\nJ. Zhang, X. Zhang, J. Yu, J. Tang, J. Tang, C. Li, and H. Chen, “Subgraph retrieval enhanced model for multi-hop knowledge base question answering,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib236.1.1\">ACL (Volume 1: Long Papers)</em>, 2022, pp. 5773–5784.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib237\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[237]</span>\n<span class=\"ltx_bibblock\">\nJ. Jiang, K. Zhou, Z. Dong, K. Ye, W. X. Zhao, and J.-R. Wen, “Structgpt: A general framework for large language model to reason over structured data,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib237.1.1\">arXiv preprint arXiv:2305.09645</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib238\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[238]</span>\n<span class=\"ltx_bibblock\">\nH. Zhu, H. Peng, Z. Lyu, L. Hou, J. Li, and J. Xiao, “Pre-training language model incorporating domain-specific heterogeneous knowledge into a unified representation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib238.1.1\">Expert Systems with Applications</em>, vol. 215, p. 119369, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib239\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[239]</span>\n<span class=\"ltx_bibblock\">\nC. Feng, X. Zhang, and Z. Fei, “Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib239.1.1\">arXiv preprint arXiv:2309.03118</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib240\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[240]</span>\n<span class=\"ltx_bibblock\">\nJ. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y. Gong, H.-Y. Shum, and J. Guo, “Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib240.1.1\">arXiv preprint arXiv:2307.07697</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib241\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[241]</span>\n<span class=\"ltx_bibblock\">\nB. He, D. Zhou, J. Xiao, X. Jiang, Q. Liu, N. J. Yuan, and T. Xu, “BERT-MK: Integrating graph contextualized knowledge into pre-trained language models,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib241.1.1\">EMNLP</em>, 2020, pp. 2281–2290.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib242\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[242]</span>\n<span class=\"ltx_bibblock\">\nY. Su, X. Han, Z. Zhang, Y. Lin, P. Li, Z. Liu, J. Zhou, and M. Sun, “Cokebert: Contextual knowledge selection and embedding towards enhanced pre-trained language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib242.1.1\">AI Open</em>, vol. 2, pp. 127–134, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib243\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[243]</span>\n<span class=\"ltx_bibblock\">\nD. Yu, C. Zhu, Y. Yang, and M. Zeng, “JAKET: joint pre-training of knowledge graph and language understanding,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib243.1.1\">AAAI</em>, 2022, pp. 11 630–11 638.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib244\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[244]</span>\n<span class=\"ltx_bibblock\">\nX. Wang, P. Kapanipathi, R. Musa, M. Yu, K. Talamadupula, I. Abdelaziz, M. Chang, A. Fokoue, B. Makni, N. Mattei, and M. Witbrock, “Improving natural language inference using external knowledge in the science questions domain,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib244.1.1\">AAAI</em>, 2019, pp. 7208–7215.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib245\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[245]</span>\n<span class=\"ltx_bibblock\">\nY. Sun, Q. Shi, L. Qi, and Y. Zhang, “JointLK: Joint reasoning with language models and knowledge graphs for commonsense question answering,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib245.1.1\">NAACL</em>, 2022, pp. 5049–5060.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib246\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[246]</span>\n<span class=\"ltx_bibblock\">\nX. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib246.1.1\">et al.</em>, “Agentbench: Evaluating llms as agents,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib246.2.2\">arXiv preprint arXiv:2308.03688</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib247\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[247]</span>\n<span class=\"ltx_bibblock\">\nY. Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr, “Knowledge graph prompting for multi-document question answering,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib247.1.1\">arXiv preprint arXiv:2308.11730</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib248\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[248]</span>\n<span class=\"ltx_bibblock\">\nA. Zeng, M. Liu, R. Lu, B. Wang, X. Liu, Y. Dong, and J. Tang, “Agenttuning: Enabling generalized agent abilities for llms,” 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib249\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[249]</span>\n<span class=\"ltx_bibblock\">\nW. Kryściński, B. McCann, C. Xiong, and R. Socher, “Evaluating the factual consistency of abstractive text summarization,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib249.1.1\">arXiv preprint arXiv:1910.12840</em>, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib250\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[250]</span>\n<span class=\"ltx_bibblock\">\nZ. Ji, Z. Liu, N. Lee, T. Yu, B. Wilie, M. Zeng, and P. Fung, “Rho (<math alttext=\"\\backslash\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"bib.bib250.1.m1.1\"><semantics id=\"bib.bib250.1.m1.1a\"><mrow id=\"bib.bib250.1.m1.1.1\" xref=\"bib.bib250.1.m1.1.1.cmml\"><mi id=\"bib.bib250.1.m1.1.1.2\" xref=\"bib.bib250.1.m1.1.1.2.cmml\"></mi><mo id=\"bib.bib250.1.m1.1.1.1\" lspace=\"0.222em\" rspace=\"0.222em\" xref=\"bib.bib250.1.m1.1.1.1.cmml\">\\</mo><mi id=\"bib.bib250.1.m1.1.1.3\" xref=\"bib.bib250.1.m1.1.1.3.cmml\">ρ</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"bib.bib250.1.m1.1b\"><apply id=\"bib.bib250.1.m1.1.1.cmml\" xref=\"bib.bib250.1.m1.1.1\"><ci id=\"bib.bib250.1.m1.1.1.1.cmml\" xref=\"bib.bib250.1.m1.1.1.1\">\\</ci><csymbol cd=\"latexml\" id=\"bib.bib250.1.m1.1.1.2.cmml\" xref=\"bib.bib250.1.m1.1.1.2\">absent</csymbol><ci id=\"bib.bib250.1.m1.1.1.3.cmml\" xref=\"bib.bib250.1.m1.1.1.3\">𝜌</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"bib.bib250.1.m1.1c\">\\backslash\\rho</annotation><annotation encoding=\"application/x-llamapun\" id=\"bib.bib250.1.m1.1d\">\\ italic_ρ</annotation></semantics></math>): Reducing hallucination in open-domain dialogues with knowledge grounding,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib250.2.1\">arXiv preprint arXiv:2212.01588</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib251\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[251]</span>\n<span class=\"ltx_bibblock\">\nS. Feng, V. Balachandran, Y. Bai, and Y. Tsvetkov, “Factkb: Generalizable factuality evaluation using language models enhanced with factual knowledge,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib251.1.1\">arXiv preprint arXiv:2305.08281</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib252\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[252]</span>\n<span class=\"ltx_bibblock\">\nY. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang, “Editing large language models: Problems, methods, and opportunities,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib252.1.1\">arXiv preprint arXiv:2305.13172</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib253\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[253]</span>\n<span class=\"ltx_bibblock\">\nZ. Li, N. Zhang, Y. Yao, M. Wang, X. Chen, and H. Chen, “Unveiling the pitfalls of knowledge editing for large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib253.1.1\">arXiv preprint arXiv:2310.02129</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib254\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[254]</span>\n<span class=\"ltx_bibblock\">\nR. Cohen, E. Biran, O. Yoran, A. Globerson, and M. Geva, “Evaluating the ripple effects of knowledge editing in language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib254.1.1\">arXiv preprint arXiv:2307.12976</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib255\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[255]</span>\n<span class=\"ltx_bibblock\">\nS. Diao, Z. Huang, R. Xu, X. Li, Y. Lin, X. Zhou, and T. Zhang, “Black-box prompt learning for pre-trained language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib255.1.1\">arXiv preprint arXiv:2201.08531</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib256\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[256]</span>\n<span class=\"ltx_bibblock\">\nT. Sun, Y. Shao, H. Qian, X. Huang, and X. Qiu, “Black-box tuning for language-model-as-a-service,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib256.1.1\">International Conference on Machine Learning</em>.   PMLR, 2022, pp. 20 841–20 855.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib257\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[257]</span>\n<span class=\"ltx_bibblock\">\nX. Chen, A. Shrivastava, and A. Gupta, “NEIL: extracting visual knowledge from web data,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib257.1.1\">IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013</em>, 2013, pp. 1409–1416.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib258\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[258]</span>\n<span class=\"ltx_bibblock\">\nM. Warren and P. J. Hayes, “Bounding ambiguity: Experiences with an image annotation system,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib258.1.1\">Proceedings of the 1st Workshop on Subjectivity, Ambiguity and Disagreement in Crowdsourcing</em>, ser. CEUR Workshop Proceedings, vol. 2276, 2018, pp. 41–54.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib259\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[259]</span>\n<span class=\"ltx_bibblock\">\nZ. Chen, Y. Huang, J. Chen, Y. Geng, Y. Fang, J. Z. Pan, N. Zhang, and W. Zhang, “Lako: Knowledge-driven visual estion answering via late knowledge-to-text injection,” 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib260\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[260]</span>\n<span class=\"ltx_bibblock\">\nR. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra, “Imagebind: One embedding space to bind them all,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib260.1.1\">ICCV</em>, 2023, pp. 15 180–15 190.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib261\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[261]</span>\n<span class=\"ltx_bibblock\">\nJ. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib261.1.1\">Information Fusion</em>, vol. 59, pp. 103–126, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib262\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[262]</span>\n<span class=\"ltx_bibblock\">\nH. Zhang, B. Wu, X. Yuan, S. Pan, H. Tong, and J. Pei, “Trustworthy graph neural networks: Aspects, methods and trends,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib262.1.1\">arXiv:2205.07424</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib263\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[263]</span>\n<span class=\"ltx_bibblock\">\nT. Wu, M. Caccia, Z. Li, Y.-F. Li, G. Qi, and G. Haffari, “Pretrained language model in continual learning: A comparative study,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib263.1.1\">ICLR</em>, 2022.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib264\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[264]</span>\n<span class=\"ltx_bibblock\">\nX. L. Li, A. Kuncoro, J. Hoffmann, C. de Masson d’Autume, P. Blunsom, and A. Nematzadeh, “A systematic investigation of commonsense knowledge in large language models,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib264.1.1\">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, 2022, pp. 11 838–11 855.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib265\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[265]</span>\n<span class=\"ltx_bibblock\">\nY. Zheng, H. Y. Koh, J. Ju, A. T. Nguyen, L. T. May, G. I. Webb, and S. Pan, “Large language models for scientific synthesis, inference and explanation,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib265.1.1\">arXiv preprint arXiv:2310.07984</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib266\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[266]</span>\n<span class=\"ltx_bibblock\">\nB. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heintz, and D. Roth, “Recent advances in natural language processing via large pre-trained language models: A survey,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib266.1.1\">ACM Computing Surveys</em>, vol. 56, no. 2, pp. 1–40, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib267\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[267]</span>\n<span class=\"ltx_bibblock\">\nJ. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, “Finetuned language models are zero-shot learners,” in <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib267.1.1\">International Conference on Learning Representations</em>, 2021.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib268\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[268]</span>\n<span class=\"ltx_bibblock\">\nY. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen, L. Wang, A. T. Luu, W. Bi, F. Shi, and S. Shi, “Siren’s song in the ai ocean: A survey on hallucination in large language models,” <em class=\"ltx_emph ltx_font_italic\" id=\"bib.bib268.1.1\">arXiv preprint arXiv:2309.01219</em>, 2023.\n\n</span>\n</li>\n</ul>\n</section>\n<section class=\"ltx_appendix\" id=\"A1\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Pros and Cons for LLMs and KGs</h2>\n<div class=\"ltx_para\" id=\"A1.p1\">\n<p class=\"ltx_p\" id=\"A1.p1.1\"><span class=\"ltx_text\" id=\"A1.p1.1.1\" style=\"color:#000000;\">In this section, we introduce the pros and cons of LLMs and KGs in detail. We summarize the pros and cons of LLMs and KGs in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, respectively.</span></p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.p2\">\n<p class=\"ltx_p\" id=\"A1.p2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.p2.1.1\" style=\"color:#000000;\">LLM pros.</span><span class=\"ltx_text\" id=\"A1.p2.1.2\" style=\"color:#000000;\"></span></p>\n<ul class=\"ltx_itemize\" id=\"A1.I1\">\n<li class=\"ltx_item\" id=\"A1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I1.i1.p1\">\n<p class=\"ltx_p\" id=\"A1.I1.i1.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I1.i1.p1.1.1\" style=\"color:#000000;\">General Knowledge</em><span class=\"ltx_text\" id=\"A1.I1.i1.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I1.i1.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib11\" title=\"\">11</a><span class=\"ltx_text\" id=\"A1.I1.i1.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I1.i1.p1.1.5\" style=\"color:#000000;\">: LLMs pre-trained on large-scale corpora, which contain a large amount of general knowledge, such as commonsense knowledge </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I1.i1.p1.1.6.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib264\" title=\"\">264</a><span class=\"ltx_text\" id=\"A1.I1.i1.p1.1.7.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I1.i1.p1.1.8\" style=\"color:#000000;\"> and factual knowledge </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I1.i1.p1.1.9.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" id=\"A1.I1.i1.p1.1.10.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I1.i1.p1.1.11\" style=\"color:#000000;\">. Such knowledge can be distilled from LLMs and used for downstream tasks </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I1.i1.p1.1.12.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib265\" title=\"\">265</a><span class=\"ltx_text\" id=\"A1.I1.i1.p1.1.13.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I1.i1.p1.1.14\" style=\"color:#000000;\">.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I1.i2.p1\">\n<p class=\"ltx_p\" id=\"A1.I1.i2.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I1.i2.p1.1.1\" style=\"color:#000000;\">Language Processing</em><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.5\" style=\"color:#000000;\">: LLMs have shown great performance in understanding natural language </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.6.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib266\" title=\"\">266</a><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.7.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.8\" style=\"color:#000000;\">. Therefore, LLMs can be used in many natural language processing tasks, such as question answering </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.9.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib4\" title=\"\">4</a><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.10.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.11\" style=\"color:#000000;\">, machine translation </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.12.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.13.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.14\" style=\"color:#000000;\">, and text generation </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.15.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.16.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I1.i2.p1.1.17\" style=\"color:#000000;\">.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I1.i3.p1\">\n<p class=\"ltx_p\" id=\"A1.I1.i3.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I1.i3.p1.1.1\" style=\"color:#000000;\">Generalizability</em><span class=\"ltx_text\" id=\"A1.I1.i3.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I1.i3.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" id=\"A1.I1.i3.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I1.i3.p1.1.5\" style=\"color:#000000;\">: LLMs enable great generalizability, which can be applied to various downstream tasks </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I1.i3.p1.1.6.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib267\" title=\"\">267</a><span class=\"ltx_text\" id=\"A1.I1.i3.p1.1.7.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I1.i3.p1.1.8\" style=\"color:#000000;\">. By providing few-shot examples </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I1.i3.p1.1.9.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib59\" title=\"\">59</a><span class=\"ltx_text\" id=\"A1.I1.i3.p1.1.10.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I1.i3.p1.1.11\" style=\"color:#000000;\"> or finetuning on multi-task data </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I1.i3.p1.1.12.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" id=\"A1.I1.i3.p1.1.13.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I1.i3.p1.1.14\" style=\"color:#000000;\">, LLMs achieve great performance on many tasks.</span></p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.p3\">\n<p class=\"ltx_p\" id=\"A1.p3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.p3.1.1\" style=\"color:#000000;\">LLM cons.</span><span class=\"ltx_text\" id=\"A1.p3.1.2\" style=\"color:#000000;\"></span></p>\n<ul class=\"ltx_itemize\" id=\"A1.I2\">\n<li class=\"ltx_item\" id=\"A1.I2.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I2.i1.p1\">\n<p class=\"ltx_p\" id=\"A1.I2.i1.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I2.i1.p1.1.1\" style=\"color:#000000;\">Implicit Knowledge</em><span class=\"ltx_text\" id=\"A1.I2.i1.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I2.i1.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" id=\"A1.I2.i1.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I2.i1.p1.1.5\" style=\"color:#000000;\">: LLMs represent knowledge implicitly in their parameters. It is difficult to interpret or validate the knowledge obtained by LLMs.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I2.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I2.i2.p1\">\n<p class=\"ltx_p\" id=\"A1.I2.i2.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I2.i2.p1.1.1\" style=\"color:#000000;\">Hallucination</em><span class=\"ltx_text\" id=\"A1.I2.i2.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I2.i2.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" id=\"A1.I2.i2.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I2.i2.p1.1.5\" style=\"color:#000000;\">: LLMs often experience hallucinations by generating content that while seemingly plausible but are factually incorrect </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I2.i2.p1.1.6.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib268\" title=\"\">268</a><span class=\"ltx_text\" id=\"A1.I2.i2.p1.1.7.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I2.i2.p1.1.8\" style=\"color:#000000;\">. This problem greatly reduces the trustworthiness of LLMs in real-world scenarios.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I2.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I2.i3.p1\">\n<p class=\"ltx_p\" id=\"A1.I2.i3.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I2.i3.p1.1.1\" style=\"color:#000000;\">Indecisiveness</em><span class=\"ltx_text\" id=\"A1.I2.i3.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I2.i3.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib16\" title=\"\">16</a><span class=\"ltx_text\" id=\"A1.I2.i3.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I2.i3.p1.1.5\" style=\"color:#000000;\">: LLMs perform reasoning by generating from a probability model, which is an indecisive process. The generated results are sampled from the probability distribution, which is difficult to control.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I2.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I2.i4.p1\">\n<p class=\"ltx_p\" id=\"A1.I2.i4.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I2.i4.p1.1.1\" style=\"color:#000000;\">Black-box</em><span class=\"ltx_text\" id=\"A1.I2.i4.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I2.i4.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib17\" title=\"\">17</a><span class=\"ltx_text\" id=\"A1.I2.i4.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I2.i4.p1.1.5\" style=\"color:#000000;\">: LLMs are criticized for their lack of interpretability. It is unclear to know the specific patterns and functions LLMs use to arrive at predictions or decisions.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I2.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I2.i5.p1\">\n<p class=\"ltx_p\" id=\"A1.I2.i5.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I2.i5.p1.1.1\" style=\"color:#000000;\">Lacking Domain-specific/New Knowledge</em><span class=\"ltx_text\" id=\"A1.I2.i5.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I2.i5.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" id=\"A1.I2.i5.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I2.i5.p1.1.5\" style=\"color:#000000;\">: LLMs trained on general corpus might not be able to generalize well to specific domains or new knowledge due to the lack of domain-specific knowledge or new training data.</span></p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.p4\">\n<p class=\"ltx_p\" id=\"A1.p4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.p4.1.1\" style=\"color:#000000;\">KG pros.</span><span class=\"ltx_text\" id=\"A1.p4.1.2\" style=\"color:#000000;\"></span></p>\n<ul class=\"ltx_itemize\" id=\"A1.I3\">\n<li class=\"ltx_item\" id=\"A1.I3.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I3.i1.p1\">\n<p class=\"ltx_p\" id=\"A1.I3.i1.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I3.i1.p1.1.1\" style=\"color:#000000;\">Structural Knowledge</em><span class=\"ltx_text\" id=\"A1.I3.i1.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I3.i1.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib19\" title=\"\">19</a><span class=\"ltx_text\" id=\"A1.I3.i1.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I3.i1.p1.1.5\" style=\"color:#000000;\">: KGs store facts in a structural format (i.e., triples), which can be understandable by both humans and machines.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I3.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I3.i2.p1\">\n<p class=\"ltx_p\" id=\"A1.I3.i2.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I3.i2.p1.1.1\" style=\"color:#000000;\">Accuracy</em><span class=\"ltx_text\" id=\"A1.I3.i2.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I3.i2.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib20\" title=\"\">20</a><span class=\"ltx_text\" id=\"A1.I3.i2.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I3.i2.p1.1.5\" style=\"color:#000000;\">: Facts in KGs are usually manually curated or validated by experts, which are more accurate and dependable than those in LLMs.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I3.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I3.i3.p1\">\n<p class=\"ltx_p\" id=\"A1.I3.i3.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I3.i3.p1.1.1\" style=\"color:#000000;\">Decisiveness</em><span class=\"ltx_text\" id=\"A1.I3.i3.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I3.i3.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib21\" title=\"\">21</a><span class=\"ltx_text\" id=\"A1.I3.i3.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I3.i3.p1.1.5\" style=\"color:#000000;\">: The factual knowledge in KGs is stored in a decisive manner. The reasoning algorithm in KGs is also deterministic, which can provide decisive results.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I3.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I3.i4.p1\">\n<p class=\"ltx_p\" id=\"A1.I3.i4.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I3.i4.p1.1.1\" style=\"color:#000000;\">Interpretability</em><span class=\"ltx_text\" id=\"A1.I3.i4.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I3.i4.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib22\" title=\"\">22</a><span class=\"ltx_text\" id=\"A1.I3.i4.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I3.i4.p1.1.5\" style=\"color:#000000;\">: KGs are renowned for their symbolic reasoning ability, which provides an interpretable reasoning process that can be understood by humans.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I3.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I3.i5.p1\">\n<p class=\"ltx_p\" id=\"A1.I3.i5.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I3.i5.p1.1.1\" style=\"color:#000000;\">Domain-specific Knowledge</em><span class=\"ltx_text\" id=\"A1.I3.i5.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I3.i5.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib23\" title=\"\">23</a><span class=\"ltx_text\" id=\"A1.I3.i5.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I3.i5.p1.1.5\" style=\"color:#000000;\">: Many domains can construct their KGs by experts to provide precise and dependable domain-specific knowledge.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I3.i6\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I3.i6.p1\">\n<p class=\"ltx_p\" id=\"A1.I3.i6.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I3.i6.p1.1.1\" style=\"color:#000000;\">Evolving Knowledge</em><span class=\"ltx_text\" id=\"A1.I3.i6.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I3.i6.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib24\" title=\"\">24</a><span class=\"ltx_text\" id=\"A1.I3.i6.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I3.i6.p1.1.5\" style=\"color:#000000;\">: The facts in KGs are continuously evolving. The KGs can be updated with new facts by inserting new triples and deleting outdated ones.</span></p>\n</div>\n</li>\n</ul>\n<p class=\"ltx_p\" id=\"A1.p4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.p4.2.1\" style=\"color:#000000;\">KG cons.</span><span class=\"ltx_text\" id=\"A1.p4.2.2\" style=\"color:#000000;\"></span></p>\n<ul class=\"ltx_itemize\" id=\"A1.I4\">\n<li class=\"ltx_item\" id=\"A1.I4.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I4.i1.p1\">\n<p class=\"ltx_p\" id=\"A1.I4.i1.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I4.i1.p1.1.1\" style=\"color:#000000;\">Incompleteness</em><span class=\"ltx_text\" id=\"A1.I4.i1.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I4.i1.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib25\" title=\"\">25</a><span class=\"ltx_text\" id=\"A1.I4.i1.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I4.i1.p1.1.5\" style=\"color:#000000;\">: KGs are hard to construct and often incomplete, which limits the ability of KGs to provide comprehensive knowledge.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I4.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I4.i2.p1\">\n<p class=\"ltx_p\" id=\"A1.I4.i2.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I4.i2.p1.1.1\" style=\"color:#000000;\">Lacking Language Understanding</em><span class=\"ltx_text\" id=\"A1.I4.i2.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I4.i2.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib33\" title=\"\">33</a><span class=\"ltx_text\" id=\"A1.I4.i2.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I4.i2.p1.1.5\" style=\"color:#000000;\">: Most studies on KGs model the structure of knowledge, but ignore the textual information in KGs. The textual information in KGs is often ignored in KG-related tasks, such as KG completion </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I4.i2.p1.1.6.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib26\" title=\"\">26</a><span class=\"ltx_text\" id=\"A1.I4.i2.p1.1.7.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I4.i2.p1.1.8\" style=\"color:#000000;\"> and KGQA </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I4.i2.p1.1.9.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib43\" title=\"\">43</a><span class=\"ltx_text\" id=\"A1.I4.i2.p1.1.10.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I4.i2.p1.1.11\" style=\"color:#000000;\">.</span></p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A1.I4.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">•</span>\n<div class=\"ltx_para\" id=\"A1.I4.i3.p1\">\n<p class=\"ltx_p\" id=\"A1.I4.i3.p1.1\"><em class=\"ltx_emph ltx_font_italic\" id=\"A1.I4.i3.p1.1.1\" style=\"color:#000000;\">Unseen Facts</em><span class=\"ltx_text\" id=\"A1.I4.i3.p1.1.2\" style=\"color:#000000;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"A1.I4.i3.p1.1.3.1\" style=\"color:#000000;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2306.08302v3#bib.bib27\" title=\"\">27</a><span class=\"ltx_text\" id=\"A1.I4.i3.p1.1.4.2\" style=\"color:#000000;\">]</span></cite><span class=\"ltx_text\" id=\"A1.I4.i3.p1.1.5\" style=\"color:#000000;\">: KGs are dynamically changing, which makes it difficult to model unseen entities and represent new facts.</span></p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n</article>\n</div>\n<footer class=\"ltx_page_footer\">\n<div class=\"ltx_page_logo\">Generated  on Thu Jan 25 00:42:33 2024 by <a class=\"ltx_LaTeXML_logo\" href=\"http://dlmf.nist.gov/LaTeXML/\"><span style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span style=\"font-size:70%;position:relative; bottom:2.2pt;\">A</span>T<span style=\"position:relative; bottom:-0.4ex;\">E</span></span><span class=\"ltx_font_smallcaps\">xml</span><img alt=\"[LOGO]\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==\"/></a>\n</div></footer>\n</div>\n</body>\n</html>\n","oembed":false,"readabilityObject":{"title":"Unifying Large Language Models and Knowledge Graphs: A Roadmap","content":"<div id=\"readability-page-1\" class=\"page\"><article>\n\n<p><span>\n<span>Shirui Pan, <em id=\"id1.1.id1\">Senior Member, IEEE</em>,&nbsp;Linhao&nbsp;Luo,\n<br>&nbsp;Yufei&nbsp;Wang,&nbsp;Chen&nbsp;Chen,&nbsp;Jiapu&nbsp;Wang,&nbsp;Xindong&nbsp;Wu, <span id=\"id2.2.id2\">Fellow, IEEE</span>\n</span><span>\nShirui Pan is with the School of Information and Communication Technology and Institute for Integrated and Intelligent Systems (IIIS), Griffith University, Queensland, Australia.\nEmail: s.pan@griffith.edu.au;\nLinhao Luo and Yufei&nbsp;Wang are with the Department of Data Science and AI, Monash University, Melbourne, Australia. E-mail: linhao.luo@monash.edu, garyyufei@gmail.com.\nChen Chen is with the Nanyang Technological University, Singapore. E-mail: s190009@ntu.edu.sg.\nJiapu Wang is with the Faculty of Information Technology, Beijing University of Technology, Beijing, China. E-mail: jpwang@emails.bjut.edu.cn.\nXindong Wu is with the Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Technology, Hefei, China, and also with the Research Center for Knowledge Engineering, Zhejiang Lab, Hangzhou, China.\nEmail: xwu@hfut.edu.cn.\nShirui Pan and Linhao Luo contributed equally to this work. Corresponding Author: Xindong Wu.\n</span></span>\n</p>\n<div>\n<h6>Abstract</h6>\n<p id=\"id3.id1\">Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages.\nIn this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, <span id=\"id3.id1.1\">1) KG-enhanced LLMs,</span> which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; <span id=\"id3.id1.2\">2) LLM-augmented KGs,</span> that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and <span id=\"id3.id1.3\">3) Synergized LLMs + KGs</span>, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.\n</p>\n</div>\n<div>\n<h6>Index Terms: </h6><p> Natural Language Processing, Large Language Models, Generative Pre-Training, Knowledge Graphs, Roadmap, Bidirectional Reasoning.\n\n</p></div>\n<span id=\"id1\"><sup>†</sup><span><span><sup>†</sup><span>publicationid: </span>pubid: 0000–0000/00$00.00&nbsp;©&nbsp;2023 IEEE</span></span></span>\n<section id=\"S1\">\n<h2>\n<span>1 </span><span id=\"S1.1.1\">Introduction</span>\n</h2>\n<p id=\"S1.p1.1\">Large language models (LLMs)<span id=\"footnote1\"><sup>1</sup><span><span><sup>1</sup><span>1</span>LLMs are also known as pre-trained language models (PLMs).</span></span></span> (e.g., BERT <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib1\" title=\"\">1</a>]</cite>, RoBERTA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib2\" title=\"\">2</a>]</cite>, and T5 <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib3\" title=\"\">3</a>]</cite>), pre-trained on the large-scale corpus, have shown great performance in various natural language processing (NLP) tasks, such as question answering <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib4\" title=\"\">4</a>]</cite>, machine translation <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib5\" title=\"\">5</a>]</cite>, and text generation <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib6\" title=\"\">6</a>]</cite>. Recently, the dramatically increasing model size further enables the LLMs with the emergent ability <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib7\" title=\"\">7</a>]</cite>, paving the road for applying LLMs as Artificial General Intelligence (AGI). Advanced LLMs like ChatGPT<span id=\"footnote2\"><sup>2</sup><span><span><sup>2</sup><span>2</span><a href=\"https://openai.com/blog/chatgpt\" title=\"\">https://openai.com/blog/chatgpt</a></span></span></span> and PaLM2<span id=\"footnote3\"><sup>3</sup><span><span><sup>3</sup><span>3</span><a href=\"https://ai.google/discover/palm2\" title=\"\">https://ai.google/discover/palm2</a></span></span></span>, with billions of parameters, exhibit great potential in many complex practical tasks, such as education <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib8\" title=\"\">8</a>]</cite>, code generation <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib9\" title=\"\">9</a>]</cite> and recommendation <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib10\" title=\"\">10</a>]</cite>.</p>\n<figure id=\"S1.F1\"><img alt=\"Refer to caption\" height=\"387\" id=\"S1.F1.g1\" src=\"extracted/5367551/figs/LLM_vs_KG.png\" width=\"598\">\n<figcaption><span>Figure 1: </span>Summarization of the pros and cons for LLMs and KGs. LLM pros: <em id=\"S1.F1.19.1\">General Knowledge</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib11\" title=\"\">11</a>]</cite>, <em id=\"S1.F1.20.2\">Language Processing</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib12\" title=\"\">12</a>]</cite>, <em id=\"S1.F1.21.3\">Generalizability</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib13\" title=\"\">13</a>]</cite>; LLM cons: <em id=\"S1.F1.22.4\">Implicit Knowledge</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a>]</cite>, <em id=\"S1.F1.23.5\">Hallucination</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib15\" title=\"\">15</a>]</cite>, <em id=\"S1.F1.24.6\">Indecisiveness</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib16\" title=\"\">16</a>]</cite>, <em id=\"S1.F1.25.7\">Black-box</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib17\" title=\"\">17</a>]</cite>, <em id=\"S1.F1.26.8\">Lacking Domain-specific/New Knowledge</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib18\" title=\"\">18</a>]</cite>. KG pros: <em id=\"S1.F1.27.9\">Structural Knowledge</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib19\" title=\"\">19</a>]</cite>, <em id=\"S1.F1.28.10\">Accuracy</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib20\" title=\"\">20</a>]</cite>, <em id=\"S1.F1.29.11\">Decisiveness</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib21\" title=\"\">21</a>]</cite>, <em id=\"S1.F1.30.12\">Interpretability</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib22\" title=\"\">22</a>]</cite>, <em id=\"S1.F1.31.13\">Domain-specific Knowledge</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib23\" title=\"\">23</a>]</cite>, <em id=\"S1.F1.32.14\">Evolving Knowledge</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib24\" title=\"\">24</a>]</cite>; KG cons: <em id=\"S1.F1.33.15\">Incompleteness</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib25\" title=\"\">25</a>]</cite>, <em id=\"S1.F1.34.16\">Lacking Language Understanding</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib26\" title=\"\">26</a>]</cite>, <em id=\"S1.F1.35.17\">Unseen Facts</em> <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib27\" title=\"\">27</a>]</cite>. Pros. and Cons. are selected based on their representativeness. Detailed discussion can be found in <em id=\"S1.F1.36.18\">Appendix <a href=\"https://arxiv.org/html/2306.08302v3#A1\" title=\"Appendix A Pros and Cons for LLMs and KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>A</span></a>.</em></figcaption>\n</figure>\n<p id=\"S1.p2.1\">Despite their success in many applications, LLMs have been criticized for their lack of factual knowledge. Specifically, LLMs memorize facts and knowledge contained in the training corpus <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a>]</cite>. However, further studies reveal that LLMs are not able to recall facts and often experience hallucinations by generating statements that are factually incorrect <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib28\" title=\"\">28</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib15\" title=\"\">15</a>]</cite>. For example, LLMs might say “Einstein discovered gravity in 1687” when asked, “When did Einstein discover gravity?”, which contradicts the fact that Isaac Newton formulated the gravitational theory. This issue severely impairs the trustworthiness of LLMs.</p>\n<p id=\"S1.p3.1\">As black-box models, LLMs are also criticized for their lack of interpretability. LLMs represent knowledge implicitly in their parameters. It is difficult to interpret or validate the knowledge obtained by LLMs. Moreover, LLMs perform reasoning by a probability model, which is an indecisive process <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib16\" title=\"\">16</a>]</cite>. The specific patterns and functions LLMs used to arrive at predictions or decisions are not directly accessible or explainable to humans <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib17\" title=\"\">17</a>]</cite>. Even though some LLMs are equipped to explain their predictions by applying chain-of-thought <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib29\" title=\"\">29</a>]</cite>, their reasoning explanations also suffer from the hallucination issue <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib30\" title=\"\">30</a>]</cite>. This severely impairs the application of LLMs in high-stakes scenarios, such as medical diagnosis and legal judgment. For instance, in a medical diagnosis scenario, LLMs may incorrectly diagnose a disease and provide explanations that contradict medical commonsense. This raises another issue that LLMs trained on general corpus might not be able to generalize well to specific domains or new knowledge due to the lack of domain-specific knowledge or new training data <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib18\" title=\"\">18</a>]</cite>.</p>\n<p id=\"S1.p4.1\">To address the above issues, a potential solution is to incorporate knowledge graphs (KGs) into LLMs. Knowledge graphs (KGs), storing enormous facts in the way of triples, i.e., <math alttext=\"(head~{}entity,relation,tail~{}entity)\" display=\"inline\" id=\"S1.p4.1.m1.3\"><semantics id=\"S1.p4.1.m1.3a\"><mrow id=\"S1.p4.1.m1.3.3.3\" xref=\"S1.p4.1.m1.3.3.4.cmml\"><mo id=\"S1.p4.1.m1.3.3.3.4\" stretchy=\"false\" xref=\"S1.p4.1.m1.3.3.4.cmml\">(</mo><mrow id=\"S1.p4.1.m1.1.1.1.1\" xref=\"S1.p4.1.m1.1.1.1.1.cmml\"><mi id=\"S1.p4.1.m1.1.1.1.1.2\" xref=\"S1.p4.1.m1.1.1.1.1.2.cmml\">h</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.3\" xref=\"S1.p4.1.m1.1.1.1.1.3.cmml\">e</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1a\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.4\" xref=\"S1.p4.1.m1.1.1.1.1.4.cmml\">a</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1b\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.5\" xref=\"S1.p4.1.m1.1.1.1.1.5.cmml\">d</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1c\" lspace=\"0.330em\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.6\" xref=\"S1.p4.1.m1.1.1.1.1.6.cmml\">e</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1d\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.7\" xref=\"S1.p4.1.m1.1.1.1.1.7.cmml\">n</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1e\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.8\" xref=\"S1.p4.1.m1.1.1.1.1.8.cmml\">t</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1f\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.9\" xref=\"S1.p4.1.m1.1.1.1.1.9.cmml\">i</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1g\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.10\" xref=\"S1.p4.1.m1.1.1.1.1.10.cmml\">t</mi><mo id=\"S1.p4.1.m1.1.1.1.1.1h\" xref=\"S1.p4.1.m1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.1.1.1.1.11\" xref=\"S1.p4.1.m1.1.1.1.1.11.cmml\">y</mi></mrow><mo id=\"S1.p4.1.m1.3.3.3.5\" xref=\"S1.p4.1.m1.3.3.4.cmml\">,</mo><mrow id=\"S1.p4.1.m1.2.2.2.2\" xref=\"S1.p4.1.m1.2.2.2.2.cmml\"><mi id=\"S1.p4.1.m1.2.2.2.2.2\" xref=\"S1.p4.1.m1.2.2.2.2.2.cmml\">r</mi><mo id=\"S1.p4.1.m1.2.2.2.2.1\" xref=\"S1.p4.1.m1.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.2.2.2.2.3\" xref=\"S1.p4.1.m1.2.2.2.2.3.cmml\">e</mi><mo id=\"S1.p4.1.m1.2.2.2.2.1a\" xref=\"S1.p4.1.m1.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.2.2.2.2.4\" xref=\"S1.p4.1.m1.2.2.2.2.4.cmml\">l</mi><mo id=\"S1.p4.1.m1.2.2.2.2.1b\" xref=\"S1.p4.1.m1.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.2.2.2.2.5\" xref=\"S1.p4.1.m1.2.2.2.2.5.cmml\">a</mi><mo id=\"S1.p4.1.m1.2.2.2.2.1c\" xref=\"S1.p4.1.m1.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.2.2.2.2.6\" xref=\"S1.p4.1.m1.2.2.2.2.6.cmml\">t</mi><mo id=\"S1.p4.1.m1.2.2.2.2.1d\" xref=\"S1.p4.1.m1.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.2.2.2.2.7\" xref=\"S1.p4.1.m1.2.2.2.2.7.cmml\">i</mi><mo id=\"S1.p4.1.m1.2.2.2.2.1e\" xref=\"S1.p4.1.m1.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.2.2.2.2.8\" xref=\"S1.p4.1.m1.2.2.2.2.8.cmml\">o</mi><mo id=\"S1.p4.1.m1.2.2.2.2.1f\" xref=\"S1.p4.1.m1.2.2.2.2.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.2.2.2.2.9\" xref=\"S1.p4.1.m1.2.2.2.2.9.cmml\">n</mi></mrow><mo id=\"S1.p4.1.m1.3.3.3.6\" xref=\"S1.p4.1.m1.3.3.4.cmml\">,</mo><mrow id=\"S1.p4.1.m1.3.3.3.3\" xref=\"S1.p4.1.m1.3.3.3.3.cmml\"><mi id=\"S1.p4.1.m1.3.3.3.3.2\" xref=\"S1.p4.1.m1.3.3.3.3.2.cmml\">t</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.3\" xref=\"S1.p4.1.m1.3.3.3.3.3.cmml\">a</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1a\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.4\" xref=\"S1.p4.1.m1.3.3.3.3.4.cmml\">i</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1b\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.5\" xref=\"S1.p4.1.m1.3.3.3.3.5.cmml\">l</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1c\" lspace=\"0.330em\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.6\" xref=\"S1.p4.1.m1.3.3.3.3.6.cmml\">e</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1d\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.7\" xref=\"S1.p4.1.m1.3.3.3.3.7.cmml\">n</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1e\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.8\" xref=\"S1.p4.1.m1.3.3.3.3.8.cmml\">t</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1f\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.9\" xref=\"S1.p4.1.m1.3.3.3.3.9.cmml\">i</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1g\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.10\" xref=\"S1.p4.1.m1.3.3.3.3.10.cmml\">t</mi><mo id=\"S1.p4.1.m1.3.3.3.3.1h\" xref=\"S1.p4.1.m1.3.3.3.3.1.cmml\">⁢</mo><mi id=\"S1.p4.1.m1.3.3.3.3.11\" xref=\"S1.p4.1.m1.3.3.3.3.11.cmml\">y</mi></mrow><mo id=\"S1.p4.1.m1.3.3.3.7\" stretchy=\"false\" xref=\"S1.p4.1.m1.3.3.4.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S1.p4.1.m1.3b\"><vector id=\"S1.p4.1.m1.3.3.4.cmml\" xref=\"S1.p4.1.m1.3.3.3\"><apply id=\"S1.p4.1.m1.1.1.1.1.cmml\" xref=\"S1.p4.1.m1.1.1.1.1\"><times id=\"S1.p4.1.m1.1.1.1.1.1.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.1\"></times><ci id=\"S1.p4.1.m1.1.1.1.1.2.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.2\">ℎ</ci><ci id=\"S1.p4.1.m1.1.1.1.1.3.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.3\">𝑒</ci><ci id=\"S1.p4.1.m1.1.1.1.1.4.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.4\">𝑎</ci><ci id=\"S1.p4.1.m1.1.1.1.1.5.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.5\">𝑑</ci><ci id=\"S1.p4.1.m1.1.1.1.1.6.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.6\">𝑒</ci><ci id=\"S1.p4.1.m1.1.1.1.1.7.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.7\">𝑛</ci><ci id=\"S1.p4.1.m1.1.1.1.1.8.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.8\">𝑡</ci><ci id=\"S1.p4.1.m1.1.1.1.1.9.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.9\">𝑖</ci><ci id=\"S1.p4.1.m1.1.1.1.1.10.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.10\">𝑡</ci><ci id=\"S1.p4.1.m1.1.1.1.1.11.cmml\" xref=\"S1.p4.1.m1.1.1.1.1.11\">𝑦</ci></apply><apply id=\"S1.p4.1.m1.2.2.2.2.cmml\" xref=\"S1.p4.1.m1.2.2.2.2\"><times id=\"S1.p4.1.m1.2.2.2.2.1.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.1\"></times><ci id=\"S1.p4.1.m1.2.2.2.2.2.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.2\">𝑟</ci><ci id=\"S1.p4.1.m1.2.2.2.2.3.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.3\">𝑒</ci><ci id=\"S1.p4.1.m1.2.2.2.2.4.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.4\">𝑙</ci><ci id=\"S1.p4.1.m1.2.2.2.2.5.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.5\">𝑎</ci><ci id=\"S1.p4.1.m1.2.2.2.2.6.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.6\">𝑡</ci><ci id=\"S1.p4.1.m1.2.2.2.2.7.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.7\">𝑖</ci><ci id=\"S1.p4.1.m1.2.2.2.2.8.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.8\">𝑜</ci><ci id=\"S1.p4.1.m1.2.2.2.2.9.cmml\" xref=\"S1.p4.1.m1.2.2.2.2.9\">𝑛</ci></apply><apply id=\"S1.p4.1.m1.3.3.3.3.cmml\" xref=\"S1.p4.1.m1.3.3.3.3\"><times id=\"S1.p4.1.m1.3.3.3.3.1.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.1\"></times><ci id=\"S1.p4.1.m1.3.3.3.3.2.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.2\">𝑡</ci><ci id=\"S1.p4.1.m1.3.3.3.3.3.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.3\">𝑎</ci><ci id=\"S1.p4.1.m1.3.3.3.3.4.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.4\">𝑖</ci><ci id=\"S1.p4.1.m1.3.3.3.3.5.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.5\">𝑙</ci><ci id=\"S1.p4.1.m1.3.3.3.3.6.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.6\">𝑒</ci><ci id=\"S1.p4.1.m1.3.3.3.3.7.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.7\">𝑛</ci><ci id=\"S1.p4.1.m1.3.3.3.3.8.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.8\">𝑡</ci><ci id=\"S1.p4.1.m1.3.3.3.3.9.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.9\">𝑖</ci><ci id=\"S1.p4.1.m1.3.3.3.3.10.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.10\">𝑡</ci><ci id=\"S1.p4.1.m1.3.3.3.3.11.cmml\" xref=\"S1.p4.1.m1.3.3.3.3.11\">𝑦</ci></apply></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.p4.1.m1.3c\">(head~{}entity,relation,tail~{}entity)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.p4.1.m1.3d\">( italic_h italic_e italic_a italic_d italic_e italic_n italic_t italic_i italic_t italic_y , italic_r italic_e italic_l italic_a italic_t italic_i italic_o italic_n , italic_t italic_a italic_i italic_l italic_e italic_n italic_t italic_i italic_t italic_y )</annotation></semantics></math>, are a structured and decisive manner of knowledge representation (e.g., Wikidata <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib20\" title=\"\">20</a>]</cite>, YAGO <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib31\" title=\"\">31</a>]</cite>, and NELL <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib32\" title=\"\">32</a>]</cite>). KGs are crucial for various applications as they offer accurate explicit knowledge <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib19\" title=\"\">19</a>]</cite>.\nBesides, they are renowned for their symbolic reasoning ability <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib22\" title=\"\">22</a>]</cite>, which generates interpretable results.\nKGs can also actively evolve with new knowledge continuously added in <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib24\" title=\"\">24</a>]</cite>. Additionally, experts can construct domain-specific KGs to provide precise and dependable domain-specific knowledge <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib23\" title=\"\">23</a>]</cite>.</p>\n<p id=\"S1.p5.1\">Nevertheless, KGs are difficult to construct <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib25\" title=\"\">25</a>]</cite>, and current approaches in KGs <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib33\" title=\"\">33</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib34\" title=\"\">34</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib27\" title=\"\">27</a>]</cite> are inadequate in handling the incomplete and dynamically changing nature of real-world KGs. These approaches fail to effectively model unseen entities and represent new facts. In addition, they often ignore the abundant textual information in KGs. Moreover, existing methods in KGs are often customized for specific KGs or tasks, which are not generalizable enough. Therefore, it is also necessary to utilize LLMs to address the challenges faced in KGs. We summarize the pros and cons of LLMs and KGs in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>1</span></a>, respectively.</p>\n<p id=\"S1.p6.1\">Recently, the possibility of unifying LLMs with KGs has attracted increasing attention from researchers and practitioners. LLMs and KGs are inherently interconnected and can mutually enhance each other. In <em id=\"S1.p6.1.1\">KG-enhanced LLMs</em>, KGs can not only be incorporated into the pre-training and inference stages of LLMs to provide external knowledge <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib35\" title=\"\">35</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib36\" title=\"\">36</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib37\" title=\"\">37</a>]</cite>, but also used for analyzing LLMs and providing interpretability <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib38\" title=\"\">38</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib39\" title=\"\">39</a>]</cite>. In <em id=\"S1.p6.1.2\">LLM-augmented KGs</em>, LLMs have been used in various KG-related tasks, e.g., KG embedding <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite>, KG completion <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib26\" title=\"\">26</a>]</cite>, KG construction <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib41\" title=\"\">41</a>]</cite>, KG-to-text generation <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib42\" title=\"\">42</a>]</cite>, and KGQA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib43\" title=\"\">43</a>]</cite>, to improve the performance and facilitate the application of KGs. In <em id=\"S1.p6.1.3\">Synergized LLM + KG</em>, researchers marries the merits of LLMs and KGs to mutually enhance performance in knowledge representation <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib44\" title=\"\">44</a>]</cite> and reasoning <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib45\" title=\"\">45</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib46\" title=\"\">46</a>]</cite>.\nAlthough there are some surveys on knowledge-enhanced LLMs <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib47\" title=\"\">47</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib48\" title=\"\">48</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib49\" title=\"\">49</a>]</cite>, which mainly focus on using KGs as an external knowledge to enhance LLMs, they ignore other possibilities of integrating KGs for LLMs and the potential role of LLMs in KG applications.\n</p>\n<div id=\"S1.p7\">\n<p id=\"S1.p7.1\">In this article, we present a forward-looking roadmap for unifying both LLMs and KGs, to leverage their respective strengths and overcome the limitations of each approach, for various downstream tasks. We propose detailed categorization, conduct comprehensive reviews, and pinpoint emerging directions in these fast-growing fields.\nOur main contributions are summarized as follows:</p>\n<ol id=\"S1.I1\">\n<li id=\"S1.I1.i1\">\n<span>1.</span>\n<p id=\"S1.I1.i1.p1.1\"><span id=\"S1.I1.i1.p1.1.1\">Roadmap.</span> We present a forward-looking roadmap for integrating LLMs and KGs. Our roadmap, consisting of three general frameworks to unify LLMs and KGs, namely, <span id=\"S1.I1.i1.p1.1.2\">KG-enhanced LLMs</span>, <span id=\"S1.I1.i1.p1.1.3\">LLM-augmented KGs</span>, and <span id=\"S1.I1.i1.p1.1.4\">Synergized LLMs + KGs</span>, provides guidelines for the unification of these two distinct but complementary technologies.\n</p>\n</li>\n<li id=\"S1.I1.i2\">\n<span>2.</span>\n<p id=\"S1.I1.i2.p1.1\"><span id=\"S1.I1.i2.p1.1.1\">Categorization and review.</span> For each integration framework of our roadmap, we present a detailed categorization and novel taxonomies of research on unifying LLMs and KGs. In each category, we review the research from the perspectives of different integration strategies and tasks, which provides more insights into each framework.</p>\n</li>\n<li id=\"S1.I1.i3\">\n<span>3.</span>\n<p id=\"S1.I1.i3.p1.1\"><span id=\"S1.I1.i3.p1.1.1\">Coverage of emerging advances.</span> We cover the advanced techniques in both LLMs and KGs. We include the discussion of state-of-the-art LLMs like ChatGPT and GPT-4 as well as the novel KGs e.g., multi-modal knowledge graphs.</p>\n</li>\n<li id=\"S1.I1.i4\">\n<span>4.</span>\n<p id=\"S1.I1.i4.p1.1\"><span id=\"S1.I1.i4.p1.1.1\">Summary of challenges and future directions.</span> We highlight the challenges in existing research and present several promising future research directions.</p>\n</li>\n</ol>\n</div>\n<p id=\"S1.p8.1\">The rest of this article is organized as follows. Section <a href=\"https://arxiv.org/html/2306.08302v3#S2\" title=\"2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>2</span></a> first explains the background of LLMs and KGs. Section <a href=\"https://arxiv.org/html/2306.08302v3#S3\" title=\"3 Roadmap &amp; Categorization ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>3</span></a> introduces the roadmap and the overall categorization of this article. Section <a href=\"https://arxiv.org/html/2306.08302v3#S4\" title=\"4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>4</span></a> presents the different KGs-enhanced LLM approaches. Section <a href=\"https://arxiv.org/html/2306.08302v3#S5\" title=\"5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>5</span></a> describes the possible LLM-augmented KG methods. Section <a href=\"https://arxiv.org/html/2306.08302v3#S6\" title=\"6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>6</span></a> shows the approaches of synergizing LLMs and KGs. Section <a href=\"https://arxiv.org/html/2306.08302v3#S7\" title=\"7 Future Directions and Milestones ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>7</span></a> discusses the challenges and future research directions. Finally, Section <a href=\"https://arxiv.org/html/2306.08302v3#S8\" title=\"8 Conclusion ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>8</span></a> concludes this paper.</p>\n</section>\n<section id=\"S2\">\n<h2>\n<span>2 </span><span id=\"S2.1.1\">Background</span>\n</h2>\n<p id=\"S2.p1.1\">In this section, we will first briefly introduce a few representative large language models (LLMs) and discuss the prompt engineering that efficiently uses LLMs for varieties of applications. Then, we illustrate the concept of knowledge graphs (KGs) and present different categories of KGs.\n</p>\n<figure id=\"S2.F2\"><img alt=\"Refer to caption\" height=\"376\" id=\"S2.F2.g1\" src=\"x1.png\" width=\"664\">\n<figcaption><span>Figure 2: </span>Representative large language models (LLMs) in recent years. Open-source models are represented by solid squares, while closed source models are represented by hollow squares.</figcaption>\n</figure>\n<figure id=\"S2.F3\"><img alt=\"Refer to caption\" height=\"308\" id=\"S2.F3.g1\" src=\"x2.png\" width=\"664\">\n<figcaption><span>Figure 3: </span>An illustration of the Transformer-based LLMs with self-attention mechanism.</figcaption>\n</figure>\n<section id=\"S2.SS1\">\n<h3>\n<span>2.1 </span><span id=\"S2.SS1.1.1\">Large Language models (LLMs)</span>\n</h3>\n<p id=\"S2.SS1.p1.1\">Large language models (LLMs) pre-trained on large-scale corpus have shown great potential in various NLP tasks <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib13\" title=\"\">13</a>]</cite>. As shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S2.F3\" title=\"Figure 3 ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>3</span></a>, most LLMs derive from the Transformer design <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib50\" title=\"\">50</a>]</cite>, which contains the encoder and decoder modules empowered by a self-attention mechanism. Based on the architecture structure, LLMs can be categorized into three groups: <em id=\"S2.SS1.p1.1.1\">1)&nbsp;encoder-only LLMs</em>,&nbsp;<em id=\"S2.SS1.p1.1.2\">2)&nbsp;encoder-decoder LLMs</em>,&nbsp;and <em id=\"S2.SS1.p1.1.3\">3)&nbsp;decoder-only LLMs</em>. As shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S2.F2\" title=\"Figure 2 ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>2</span></a>, we summarize several representative LLMs with different model architectures, model sizes, and open-source availabilities.</p>\n<section id=\"S2.SS1.SSS1\">\n<h4>\n<span>2.1.1 </span>Encoder-only LLMs.</h4>\n<p id=\"S2.SS1.SSS1.p1.1\">Encoder-only large language models only use the encoder to encode the sentence and understand the relationships between words. The common training paradigm for these model is to predict the mask words in an input sentence. This method is unsupervised and can be trained on the large-scale corpus. Encoder-only LLMs like BERT <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib1\" title=\"\">1</a>]</cite>, ALBERT <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib51\" title=\"\">51</a>]</cite>, RoBERTa <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib2\" title=\"\">2</a>]</cite>, and ELECTRA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib52\" title=\"\">52</a>]</cite> require adding an extra prediction head to resolve downstream tasks. These models are most effective for tasks that require understanding the entire sentence, such as text classification <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib26\" title=\"\">26</a>]</cite> and named entity recognition <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib53\" title=\"\">53</a>]</cite>.</p>\n</section>\n<section id=\"S2.SS1.SSS2\">\n<h4>\n<span>2.1.2 </span>Encoder-decoder LLMs.</h4>\n<p id=\"S2.SS1.SSS2.p1.1\">Encoder-decoder large language models adopt both the encoder and decoder module. The encoder module is responsible for encoding the input sentence into a hidden-space, and the decoder is used to generate the target output text. The training strategies in encoder-decoder LLMs can be more flexible. For example, T5 <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib3\" title=\"\">3</a>]</cite> is pre-trained by masking and predicting spans of masking words. UL2 <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib54\" title=\"\">54</a>]</cite> unifies several training targets such as different masking spans and masking frequencies. Encoder-decoder LLMs (e.g., T0 <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib55\" title=\"\">55</a>]</cite>, ST-MoE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib56\" title=\"\">56</a>]</cite>, and GLM-130B <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib57\" title=\"\">57</a>]</cite>) are able to directly resolve tasks that generate sentences based on some context, such as summariaztion, translation, and question answering <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib58\" title=\"\">58</a>]</cite>.</p>\n</section>\n<section id=\"S2.SS1.SSS3\">\n<h4>\n<span>2.1.3 </span>Decoder-only LLMs.</h4>\n<p id=\"S2.SS1.SSS3.p1.1\">Decoder-only large language models only adopt the decoder module to generate target output text. The training paradigm for these models is to predict the next word in the sentence. Large-scale decoder-only LLMs can generally perform downstream tasks from a few examples or simple instructions, without adding prediction heads or finetuning <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib59\" title=\"\">59</a>]</cite>. Many state-of-the-art LLMs (e.g., Chat-GPT <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib60\" title=\"\">60</a>]</cite> and GPT-4<span id=\"footnote4\"><sup>4</sup><span><span><sup>4</sup><span>4</span><a href=\"https://openai.com/product/gpt-4\" title=\"\">https://openai.com/product/gpt-4</a></span></span></span>) follow the decoder-only architecture. However, since these models are closed-source, it is challenging for academic researchers to conduct further research. Recently, Alpaca<span id=\"footnote5\"><sup>5</sup><span><span><sup>5</sup><span>5</span><a href=\"https://github.com/tatsu-lab/stanford_alpaca\" title=\"\">https://github.com/tatsu-lab/stanford_alpaca</a></span></span></span> and Vicuna<span id=\"footnote6\"><sup>6</sup><span><span><sup>6</sup><span>6</span><a href=\"https://lmsys.org/blog/2023-03-30-vicuna/\" title=\"\">https://lmsys.org/blog/2023-03-30-vicuna/</a></span></span></span> are released as open-source decoder-only LLMs. These models are finetuned based on LLaMA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib61\" title=\"\">61</a>]</cite> and achieve comparable performance with ChatGPT and GPT-4.</p>\n</section>\n<section id=\"S2.SS1.SSS4\">\n<h4>\n<span>2.1.4 </span>Prompt Engineering</h4>\n<p id=\"S2.SS1.SSS4.p1.1\">Prompt engineering is a novel field that focuses on creating and refining prompts to maximize the effectiveness of large language models (LLMs) across various applications and research areas <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib62\" title=\"\">62</a>]</cite>.\nAs shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S2.F4\" title=\"Figure 4 ‣ 2.1.4 Prompt Engineering ‣ 2.1 Large Language models (LLMs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>4</span></a>, a prompt is a sequence of natural language inputs for LLMs that are specified for the task, such as sentiment classification. A prompt could contain several elements, i.e., <em id=\"S2.SS1.SSS4.p1.1.1\">1)&nbsp;Instruction</em>, <em id=\"S2.SS1.SSS4.p1.1.2\">2)&nbsp;Context</em>, and <em id=\"S2.SS1.SSS4.p1.1.3\">3)&nbsp;Input Text</em>. <em id=\"S2.SS1.SSS4.p1.1.4\">Instruction</em> is a short sentence that instructs the model to perform a specific task. <em id=\"S2.SS1.SSS4.p1.1.5\">Context</em> provides the context for the input text or few-shot examples. <em id=\"S2.SS1.SSS4.p1.1.6\">Input Text</em> is the text that needs to be processed by the model.</p>\n<p id=\"S2.SS1.SSS4.p2.1\">Prompt engineering seeks to improve the capacity of large large language models (e.g., ChatGPT) in diverse complex tasks such as question answering, sentiment classification, and common sense reasoning. Chain-of-thought (CoT) prompt <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib63\" title=\"\">63</a>]</cite> enables complex reasoning capabilities through intermediate reasoning steps.\nPrompt engineering also enables the integration of structural data like knowledge graphs (KGs) into LLMs. Li et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib64\" title=\"\">64</a>]</cite> simply linearizes the KGs and uses templates to convert the KGs into passages.\nMindmap <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib65\" title=\"\">65</a>]</cite> designs a KG prompt to convert graph structure into a mind map that enables LLMs to perform reasoning on it.\nPrompt offers a simple way to utilize the potential of LLMs without finetuning. Proficiency in prompt engineering leads to a better understanding of the strengths and weaknesses of LLMs.</p>\n<figure id=\"S2.F4\"><img alt=\"Refer to caption\" height=\"565\" id=\"S2.F4.g1\" src=\"x3.png\" width=\"706\">\n<figcaption><span>Figure 4: </span>An example of sentiment classification prompt.</figcaption>\n</figure>\n<figure id=\"S2.F5\"><img alt=\"Refer to caption\" height=\"1020\" id=\"S2.F5.g1\" src=\"x4.png\" width=\"830\">\n<figcaption><span>Figure 5: </span>Examples of different categories’ knowledge graphs, i.e., <em id=\"S2.F5.5.1\">encyclopedic KGs</em>, <em id=\"S2.F5.6.2\">commonsense KGs</em>, <em id=\"S2.F5.7.3\">domain-specific KGs</em>, and <em id=\"S2.F5.8.4\">multi-modal KGs</em>.</figcaption>\n</figure>\n</section>\n</section>\n<section id=\"S2.SS2\">\n<h3>\n<span>2.2 </span><span id=\"S2.SS2.1.1\">Knowledge Graphs (KGs)</span>\n</h3>\n<p id=\"S2.SS2.p1.3\">Knowledge graphs (KGs) store structured knowledge as a collection of triples <math alttext=\"\\mathcal{KG}=\\{(h,r,t)\\subseteq\\mathcal{E}\\times\\mathcal{R}\\times\\mathcal{E}\\}\" display=\"inline\" id=\"S2.SS2.p1.1.m1.4\"><semantics id=\"S2.SS2.p1.1.m1.4a\"><mrow id=\"S2.SS2.p1.1.m1.4.4\" xref=\"S2.SS2.p1.1.m1.4.4.cmml\"><mrow id=\"S2.SS2.p1.1.m1.4.4.3\" xref=\"S2.SS2.p1.1.m1.4.4.3.cmml\"><mi id=\"S2.SS2.p1.1.m1.4.4.3.2\" xref=\"S2.SS2.p1.1.m1.4.4.3.2.cmml\">𝒦</mi><mo id=\"S2.SS2.p1.1.m1.4.4.3.1\" xref=\"S2.SS2.p1.1.m1.4.4.3.1.cmml\">⁢</mo><mi id=\"S2.SS2.p1.1.m1.4.4.3.3\" xref=\"S2.SS2.p1.1.m1.4.4.3.3.cmml\">𝒢</mi></mrow><mo id=\"S2.SS2.p1.1.m1.4.4.2\" xref=\"S2.SS2.p1.1.m1.4.4.2.cmml\">=</mo><mrow id=\"S2.SS2.p1.1.m1.4.4.1.1\" xref=\"S2.SS2.p1.1.m1.4.4.1.2.cmml\"><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.2\" stretchy=\"false\" xref=\"S2.SS2.p1.1.m1.4.4.1.2.cmml\">{</mo><mrow id=\"S2.SS2.p1.1.m1.4.4.1.1.1\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.cmml\"><mrow id=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.2\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.1.cmml\"><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.2.1\" stretchy=\"false\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.1.cmml\">(</mo><mi id=\"S2.SS2.p1.1.m1.1.1\" xref=\"S2.SS2.p1.1.m1.1.1.cmml\">h</mi><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.2.2\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.1.cmml\">,</mo><mi id=\"S2.SS2.p1.1.m1.2.2\" xref=\"S2.SS2.p1.1.m1.2.2.cmml\">r</mi><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.2.3\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.1.cmml\">,</mo><mi id=\"S2.SS2.p1.1.m1.3.3\" xref=\"S2.SS2.p1.1.m1.3.3.cmml\">t</mi><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.2.4\" stretchy=\"false\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.1.cmml\">)</mo></mrow><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.1.1\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.1.cmml\">⊆</mo><mrow id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.cmml\"><mi id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.2\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.2.cmml\">ℰ</mi><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.1\" lspace=\"0.222em\" rspace=\"0.222em\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.1.cmml\">×</mo><mi id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.3\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.3.cmml\">ℛ</mi><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.1a\" lspace=\"0.222em\" rspace=\"0.222em\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.1.cmml\">×</mo><mi id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.4\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.4.cmml\">ℰ</mi></mrow></mrow><mo id=\"S2.SS2.p1.1.m1.4.4.1.1.3\" stretchy=\"false\" xref=\"S2.SS2.p1.1.m1.4.4.1.2.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p1.1.m1.4b\"><apply id=\"S2.SS2.p1.1.m1.4.4.cmml\" xref=\"S2.SS2.p1.1.m1.4.4\"><eq id=\"S2.SS2.p1.1.m1.4.4.2.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.2\"></eq><apply id=\"S2.SS2.p1.1.m1.4.4.3.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.3\"><times id=\"S2.SS2.p1.1.m1.4.4.3.1.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.3.1\"></times><ci id=\"S2.SS2.p1.1.m1.4.4.3.2.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.3.2\">𝒦</ci><ci id=\"S2.SS2.p1.1.m1.4.4.3.3.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.3.3\">𝒢</ci></apply><set id=\"S2.SS2.p1.1.m1.4.4.1.2.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1\"><apply id=\"S2.SS2.p1.1.m1.4.4.1.1.1.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1\"><subset id=\"S2.SS2.p1.1.m1.4.4.1.1.1.1.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.1\"></subset><vector id=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.1.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.2.2\"><ci id=\"S2.SS2.p1.1.m1.1.1.cmml\" xref=\"S2.SS2.p1.1.m1.1.1\">ℎ</ci><ci id=\"S2.SS2.p1.1.m1.2.2.cmml\" xref=\"S2.SS2.p1.1.m1.2.2\">𝑟</ci><ci id=\"S2.SS2.p1.1.m1.3.3.cmml\" xref=\"S2.SS2.p1.1.m1.3.3\">𝑡</ci></vector><apply id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3\"><times id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.1.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.1\"></times><ci id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.2.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.2\">ℰ</ci><ci id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.3.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.3\">ℛ</ci><ci id=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.4.cmml\" xref=\"S2.SS2.p1.1.m1.4.4.1.1.1.3.4\">ℰ</ci></apply></apply></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p1.1.m1.4c\">\\mathcal{KG}=\\{(h,r,t)\\subseteq\\mathcal{E}\\times\\mathcal{R}\\times\\mathcal{E}\\}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p1.1.m1.4d\">caligraphic_K caligraphic_G = { ( italic_h , italic_r , italic_t ) ⊆ caligraphic_E × caligraphic_R × caligraphic_E }</annotation></semantics></math>, where <math alttext=\"\\mathcal{E}\" display=\"inline\" id=\"S2.SS2.p1.2.m2.1\"><semantics id=\"S2.SS2.p1.2.m2.1a\"><mi id=\"S2.SS2.p1.2.m2.1.1\" xref=\"S2.SS2.p1.2.m2.1.1.cmml\">ℰ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p1.2.m2.1b\"><ci id=\"S2.SS2.p1.2.m2.1.1.cmml\" xref=\"S2.SS2.p1.2.m2.1.1\">ℰ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p1.2.m2.1c\">\\mathcal{E}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p1.2.m2.1d\">caligraphic_E</annotation></semantics></math> and <math alttext=\"\\mathcal{R}\" display=\"inline\" id=\"S2.SS2.p1.3.m3.1\"><semantics id=\"S2.SS2.p1.3.m3.1a\"><mi id=\"S2.SS2.p1.3.m3.1.1\" xref=\"S2.SS2.p1.3.m3.1.1.cmml\">ℛ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.SS2.p1.3.m3.1b\"><ci id=\"S2.SS2.p1.3.m3.1.1.cmml\" xref=\"S2.SS2.p1.3.m3.1.1\">ℛ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.SS2.p1.3.m3.1c\">\\mathcal{R}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S2.SS2.p1.3.m3.1d\">caligraphic_R</annotation></semantics></math> respectively denote the set of entities and relations. Existing knowledge graphs (KGs) can be classified into four groups based on the stored information: <em id=\"S2.SS2.p1.3.1\">1)&nbsp;encyclopedic KGs</em>, <em id=\"S2.SS2.p1.3.2\">2)&nbsp;commonsense KGs</em>, <em id=\"S2.SS2.p1.3.3\">3)&nbsp;domain-specific KGs</em>, and <em id=\"S2.SS2.p1.3.4\">4)&nbsp;multi-modal KGs</em>. We illustrate the examples of KGs of different categories in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S2.F5\" title=\"Figure 5 ‣ 2.1.4 Prompt Engineering ‣ 2.1 Large Language models (LLMs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>5</span></a>.</p>\n<section id=\"S2.SS2.SSS1\">\n<h4>\n<span>2.2.1 </span>Encyclopedic Knowledge Graphs.</h4>\n<p id=\"S2.SS2.SSS1.p1.1\">Encyclopedic knowledge graphs are the most ubiquitous KGs, which represent the general knowledge in real-world. Encyclopedic knowledge graphs are often constructed by integrating information from diverse and extensive sources, including human experts, encyclopedias, and databases. Wikidata <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib20\" title=\"\">20</a>]</cite> is one of the most widely used encyclopedic knowledge graphs, which incorporates varieties of knowledge extracted from articles on Wikipedia. Other typical encyclopedic knowledge graphs, like Freebase <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib66\" title=\"\">66</a>]</cite>, Dbpedia <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib67\" title=\"\">67</a>]</cite>, and YAGO <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib31\" title=\"\">31</a>]</cite> are also derived from Wikipedia. In addition, NELL <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib32\" title=\"\">32</a>]</cite> is a continuously improving encyclopedic knowledge graph, which automatically extracts knowledge from the web, and uses that knowledge to improve its performance over time. There are several encyclopedic knowledge graphs available in languages other than English such as CN-DBpedia <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib68\" title=\"\">68</a>]</cite> and Vikidia <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib69\" title=\"\">69</a>]</cite>. The largest knowledge graph, named Knowledge Occean (KO)<span id=\"footnote7\"><sup>7</sup><span><span><sup>7</sup><span>7</span><a href=\"https://ko.zhonghuapu.com/\" title=\"\">https://ko.zhonghuapu.com/</a></span></span></span>, currently contains 4,8784,3636 entities and\n17,3115,8349 relations in both English and Chinese.</p>\n</section>\n<section id=\"S2.SS2.SSS2\">\n<h4>\n<span>2.2.2 </span>Commonsense Knowledge Graphs.</h4>\n<p id=\"S2.SS2.SSS2.p1.1\">Commonsense knowledge graphs formulate the knowledge about daily concepts, e.g., objects, and events, as well as their relationships <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib70\" title=\"\">70</a>]</cite>. Compared with encyclopedic knowledge graphs, commonsense knowledge graphs often model the tacit knowledge extracted from text such as <span id=\"S2.SS2.SSS2.p1.1.1\">(Car, UsedFor, Drive)</span>. ConceptNet <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib71\" title=\"\">71</a>]</cite> contains a wide range of commonsense concepts and relations, which can help computers understand the meanings of words people use. ATOMIC <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib72\" title=\"\">72</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib73\" title=\"\">73</a>]</cite> and ASER <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib74\" title=\"\">74</a>]</cite> focus on the causal effects between events, which can be used for commonsense reasoning. Some other commonsense knowledge graphs, such as TransOMCS <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib75\" title=\"\">75</a>]</cite> and CausalBanK <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib76\" title=\"\">76</a>]</cite> are automatically constructed to provide commonsense knowledge.\n</p>\n</section>\n<section id=\"S2.SS2.SSS3\">\n<h4>\n<span>2.2.3 </span>Domain-specific Knowledge Graphs</h4>\n<p id=\"S2.SS2.SSS3.p1.1\">Domain-specific knowledge graphs are often constructed to represent knowledge in a specific domain, e.g., medical, biology, and finance <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib23\" title=\"\">23</a>]</cite>. Compared with encyclopedic knowledge graphs, domain-specific knowledge graphs are often smaller in size, but more accurate and reliable. For example, UMLS <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib77\" title=\"\">77</a>]</cite> is a domain-specific knowledge graph in the medical domain, which contains biomedical concepts and their relationships. In addition, there are some domain-specific knowledge graphs in other domains, such as finance <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib78\" title=\"\">78</a>]</cite>, geology <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib79\" title=\"\">79</a>]</cite>, biology <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib80\" title=\"\">80</a>]</cite>, chemistry <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib81\" title=\"\">81</a>]</cite> and genealogy <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib82\" title=\"\">82</a>]</cite>.</p>\n<figure id=\"S2.F6\"><img alt=\"Refer to caption\" height=\"201\" id=\"S2.F6.g1\" src=\"x5.png\" width=\"747\">\n<figcaption><span>Figure 6: </span>The general roadmap of unifying KGs and LLMs. (a.) KG-enhanced LLMs. (b.) LLM-augmented KGs. (c.) Synergized LLMs + KGs.</figcaption>\n</figure>\n</section>\n<section id=\"S2.SS2.SSS4\">\n<h4>\n<span>2.2.4 </span>Multi-modal Knowledge Graphs.</h4>\n<p id=\"S2.SS2.SSS4.p1.1\">Unlike conventional knowledge graphs that only contain textual information, multi-modal knowledge graphs represent facts in multiple modalities such as images, sounds, and videos <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib83\" title=\"\">83</a>]</cite>. For example, IMGpedia <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib84\" title=\"\">84</a>]</cite>, MMKG <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib85\" title=\"\">85</a>]</cite>, and Richpedia <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib86\" title=\"\">86</a>]</cite> incorporate both the text and image information into the knowledge graphs. These knowledge graphs can be used for various multi-modal tasks such as image-text matching <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib87\" title=\"\">87</a>]</cite>, visual question answering <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib88\" title=\"\">88</a>]</cite>, and recommendation <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib89\" title=\"\">89</a>]</cite>.</p>\n<figure id=\"S2.T1\">\n<figcaption><span>TABLE I: </span>Representative applications of using LLMs and KGs.</figcaption>\n\n</figure>\n</section>\n</section>\n<section id=\"S2.SS3\">\n<h3>\n<span>2.3 </span><span id=\"S2.SS3.1.1\">Applications</span>\n</h3>\n<p id=\"S2.SS3.p1.1\">LLMs as KGs have been widely applied in various real-world applications. We summarize some representative applications of using LLMs and KGs in Table <a href=\"https://arxiv.org/html/2306.08302v3#S2.T1\" title=\"TABLE I ‣ 2.2.4 Multi-modal Knowledge Graphs. ‣ 2.2 Knowledge Graphs (KGs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>I</span></a>. ChatGPT/GPT-4 are LLM-based chatbots that can communicate with humans in a natural dialogue format. To improve knowledge awareness of LLMs, ERNIE 3.0 and Bard incorporate KGs into their chatbot applications. Instead of Chatbot. Firefly develops a photo editing application that allows users to edit photos by using natural language descriptions. Copilot, New Bing, and Shop.ai adopt LLMs to empower their applications in the areas of coding assistant, web search, and recommendation, respectively. Wikidata and KO are two representative knowledge graph applications that are used to provide external knowledge.\nOpenBG <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib90\" title=\"\">90</a>]</cite> is a knowledge graph designed for recommendation. Doctor.ai develops a health care assistant that incorporates LLMs and KGs to provide medical advice.</p>\n</section>\n</section>\n<section id=\"S3\">\n<h2>\n<span>3 </span><span id=\"S3.1.1\">Roadmap &amp; Categorization</span>\n</h2>\n<p id=\"S3.p1.1\">In this section, we first present a road map of explicit frameworks that unify LLMs and KGs. Then, we present the categorization of research on unifying LLMs and KGs.</p>\n<section id=\"S3.SS1\">\n<h3>\n<span>3.1 </span><span id=\"S3.SS1.1.1\">Roadmap</span>\n</h3>\n<p id=\"S3.SS1.p1.1\">The roadmap of unifying KGs and LLMs is illustrated in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S2.F6\" title=\"Figure 6 ‣ 2.2.3 Domain-specific Knowledge Graphs ‣ 2.2 Knowledge Graphs (KGs) ‣ 2 Background ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>6</span></a>. In the roadmap, we identify three frameworks for the unification of LLMs and KGs, including KG-enhanced LLMs, LLM-augmented KGs, and Synergized LLMs + KGs. The KG-enhanced LLMs and LLM-augmented KGs are two parallel frameworks that aim to enhance the capabilities of LLMs and KGs, respectively. Building upon these frameworks, Synergized LLMs + KGs is a unified framework that aims to synergize LLMs and KGs to mutually enhance each other.</p>\n<section id=\"S3.SS1.SSS1\">\n<h4>\n<span>3.1.1 </span>KG-enhanced LLMs</h4>\n<p id=\"S3.SS1.SSS1.p1.1\">LLMs are renowned for their ability to learn knowledge from large-scale corpus and achieve state-of-the-art performance in various NLP tasks. However, LLMs are often criticized for their hallucination issues <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib15\" title=\"\">15</a>]</cite>, and lacking of interpretability. To address these issues, researchers have proposed to enhance LLMs with knowledge graphs (KGs).</p>\n<p id=\"S3.SS1.SSS1.p2.1\">KGs store enormous knowledge in an explicit and structured way, which can be used to enhance the knowledge awareness of LLMs. Some researchers have proposed to incorporate KGs into LLMs during the pre-training stage, which can help LLMs learn knowledge from KGs <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib35\" title=\"\">35</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib91\" title=\"\">91</a>]</cite>. Other researchers have proposed to incorporate KGs into LLMs during the inference stage. By retrieving knowledge from KGs, it can significantly improve the performance of LLMs in accessing domain-specific knowledge <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib92\" title=\"\">92</a>]</cite>. To improve the interpretability of LLMs, researchers also utilize KGs to interpret the facts <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a>]</cite> and the reasoning process of LLMs <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib38\" title=\"\">38</a>]</cite>.</p>\n</section>\n<section id=\"S3.SS1.SSS2\">\n<h4>\n<span>3.1.2 </span>LLM-augmented KGs</h4>\n<p id=\"S3.SS1.SSS2.p1.1\">KGs store structure knowledge playing an essential role in many real-word applications <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib19\" title=\"\">19</a>]</cite>. Existing methods in KGs fall short of handling incomplete KGs <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib33\" title=\"\">33</a>]</cite> and processing text corpus to construct KGs <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib93\" title=\"\">93</a>]</cite>. With the generalizability of LLMs, many researchers are trying to harness the power of LLMs for addressing KG-related tasks.\n</p>\n<p id=\"S3.SS1.SSS2.p2.1\">The most straightforward way to apply LLMs as text encoders for KG-related tasks. Researchers take advantage of LLMs to process the textual corpus in the KGs and then use the representations of the text to enrich KGs representation <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib94\" title=\"\">94</a>]</cite>. Some studies also use LLMs to process the original corpus and extract relations and entities for KG construction <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib95\" title=\"\">95</a>]</cite>. Recent studies try to design a KG prompt that\ncan effectively convert structural KGs into a format that can be comprehended by LLMs. In this way, LLMs can be directly applied to KG-related tasks, e.g., KG completion <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib96\" title=\"\">96</a>]</cite> and KG reasoning <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib97\" title=\"\">97</a>]</cite>.</p>\n<figure id=\"S3.F7\"><img alt=\"Refer to caption\" height=\"315\" id=\"S3.F7.g1\" src=\"x6.png\" width=\"358\">\n<figcaption><span>Figure 7: </span>The general framework of the <span id=\"S3.F7.6.1\">Synergized LLMs + KGs</span>, which contains four layers: <em id=\"S3.F7.7.2\">1)&nbsp;Data</em>, <em id=\"S3.F7.8.3\">2)&nbsp;Synergized Model</em>, <em id=\"S3.F7.9.4\">3)&nbsp;Technique</em>, and <em id=\"S3.F7.10.5\">4)&nbsp;Application</em>.</figcaption>\n</figure>\n</section>\n<section id=\"S3.SS1.SSS3\">\n<h4>\n<span>3.1.3 </span>Synergized LLMs + KGs</h4>\n<p id=\"S3.SS1.SSS3.p1.1\">The synergy of LLMs and KGs has attracted increasing attention from researchers these years <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib42\" title=\"\">42</a>]</cite>. LLMs and KGs are two inherently complementary techniques, which should be unified into a general framework to mutually enhance each other.</p>\n<p id=\"S3.SS1.SSS3.p2.1\">To further explore the unification, we propose a unified framework of the synergized LLMs + KGs in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S3.F7\" title=\"Figure 7 ‣ 3.1.2 LLM-augmented KGs ‣ 3.1 Roadmap ‣ 3 Roadmap &amp; Categorization ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>7</span></a>. The unified framework contains four layers: <em id=\"S3.SS1.SSS3.p2.1.1\">1)&nbsp;Data</em>, <em id=\"S3.SS1.SSS3.p2.1.2\">2)&nbsp;Synergized Model</em>, <em id=\"S3.SS1.SSS3.p2.1.3\">3)&nbsp;Technique</em>, and <em id=\"S3.SS1.SSS3.p2.1.4\">4)&nbsp;Application</em>. In the <em id=\"S3.SS1.SSS3.p2.1.5\">Data</em> layer, LLMs and KGs are used to process the textual and structural data, respectively. With the development of multi-modal LLMs <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib98\" title=\"\">98</a>]</cite> and KGs <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib99\" title=\"\">99</a>]</cite>, this framework can be extended to process multi-modal data, such as video, audio, and images. In the <em id=\"S3.SS1.SSS3.p2.1.6\">Synergized Model</em> layer, LLMs and KGs could synergize with each other to improve their capabilities. In <em id=\"S3.SS1.SSS3.p2.1.7\">Technique</em> layer, related techniques that have been used in LLMs and KGs can be incorporated into this framework to further enhance the performance. In the <em id=\"S3.SS1.SSS3.p2.1.8\">Application</em> layer, LLMs and KGs can be integrated to address various real-world applications, such as search engines <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib100\" title=\"\">100</a>]</cite>, recommender systems <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib10\" title=\"\">10</a>]</cite>, and AI assistants <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib101\" title=\"\">101</a>]</cite>.</p>\n<figure id=\"S3.F8\"><img alt=\"Refer to caption\" height=\"437\" id=\"S3.F8.g1\" src=\"x7.png\" width=\"623\">\n<figcaption><span>Figure 8: </span>Fine-grained categorization of research on unifying large language models (LLMs) with knowledge graphs (KGs).</figcaption>\n</figure>\n</section>\n</section>\n<section id=\"S3.SS2\">\n<h3>\n<span>3.2 </span><span id=\"S3.SS2.1.1\">Categorization</span>\n</h3>\n<p id=\"S3.SS2.p1.1\">To better understand the research on unifying LLMs and KGs, we further provide a fine-grained categorization for each framework in the roadmap. Specifically, we focus on different ways of integrating KGs and LLMs, i.e., KG-enhanced LLMs, KG-augmented LLMs, and Synergized LLMs + KGs. The fine-grained categorization of the research is illustrated in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S3.F8\" title=\"Figure 8 ‣ 3.1.3 Synergized LLMs + KGs ‣ 3.1 Roadmap ‣ 3 Roadmap &amp; Categorization ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>8</span></a>.</p>\n<div id=\"S3.SS2.p2\">\n<p id=\"S3.SS2.p2.1\"><span id=\"S3.SS2.p2.1.1\">KG-enhanced LLMs.</span> Integrating KGs can enhance the performance and interpretability of LLMs in various downstream tasks. We categorize the research on KG-enhanced LLMs into three groups:</p>\n<ol id=\"S3.I1\">\n<li id=\"S3.I1.i1\">\n<span>1.</span>\n<p id=\"S3.I1.i1.p1.1\"><em id=\"S3.I1.i1.p1.1.1\">KG-enhanced LLM pre-training</em> includes works that apply KGs during the pre-training stage and improve the knowledge expression of LLMs.</p>\n</li>\n<li id=\"S3.I1.i2\">\n<span>2.</span>\n<p id=\"S3.I1.i2.p1.1\"><em id=\"S3.I1.i2.p1.1.1\">KG-enhanced LLM inference</em> includes research that utilizes KGs during the inference stage of LLMs, which enables LLMs to access the latest knowledge without retraining.</p>\n</li>\n<li id=\"S3.I1.i3\">\n<span>3.</span>\n<p id=\"S3.I1.i3.p1.1\"><em id=\"S3.I1.i3.p1.1.1\">KG-enhanced LLM interpretability</em> includes works that use KGs to understand the knowledge learned by LLMs and interpret the reasoning process of LLMs.</p>\n</li>\n</ol>\n</div>\n<div id=\"S3.SS2.p3\">\n<p id=\"S3.SS2.p3.1\"><span id=\"S3.SS2.p3.1.1\">LLM-augmented KGs.</span> LLMs can be applied to augment various KG-related tasks. We categorize the research on LLM-augmented KGs into five groups based on the task types:</p>\n<ol id=\"S3.I2\">\n<li id=\"S3.I2.i1\">\n<span>1.</span>\n<p id=\"S3.I2.i1.p1.1\"><em id=\"S3.I2.i1.p1.1.1\">LLM-augmented KG embedding</em> includes studies that apply LLMs to enrich representations of KGs by encoding the textual descriptions\nof entities and relations.</p>\n</li>\n<li id=\"S3.I2.i2\">\n<span>2.</span>\n<p id=\"S3.I2.i2.p1.1\"><em id=\"S3.I2.i2.p1.1.1\">LLM-augmented KG completion</em> includes papers that utilize LLMs to encode text or generate facts for better KGC performance.</p>\n</li>\n<li id=\"S3.I2.i3\">\n<span>3.</span>\n<p id=\"S3.I2.i3.p1.1\"><em id=\"S3.I2.i3.p1.1.1\">LLM-augmented KG construction</em> includes works that apply LLMs to address the entity discovery, coreference resolution, and relation extraction tasks for KG construction.</p>\n</li>\n<li id=\"S3.I2.i4\">\n<span>4.</span>\n<p id=\"S3.I2.i4.p1.1\"><em id=\"S3.I2.i4.p1.1.1\">LLM-augmented KG-to-text Generation</em> includes research that utilizes LLMs to generate natural language that describes the facts from KGs.</p>\n</li>\n<li id=\"S3.I2.i5\">\n<span>5.</span>\n<p id=\"S3.I2.i5.p1.1\"><em id=\"S3.I2.i5.p1.1.1\">LLM-augmented KG question answering</em> includes studies that apply LLMs to bridge the gap between natural language questions and retrieve answers from KGs.\n</p>\n</li>\n</ol>\n</div>\n<p id=\"S3.SS2.p4.1\"><span id=\"S3.SS2.p4.1.1\">Synergized LLMs + KGs.</span> The synergy of LLMs and KGs aims to integrate LLMs and KGs into a unified framework to mutually enhance each other. In this categorization, we review the recent attempts of Synergized LLMs + KGs from the perspectives of <em id=\"S3.SS2.p4.1.2\">knowledge representation</em> and <em id=\"S3.SS2.p4.1.3\">reasoning</em>.</p>\n<p id=\"S3.SS2.p5.1\">In the following sections (Sec <a href=\"https://arxiv.org/html/2306.08302v3#S4\" title=\"4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>4</span></a>, <a href=\"https://arxiv.org/html/2306.08302v3#S5\" title=\"5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>5</span></a>, and <a href=\"https://arxiv.org/html/2306.08302v3#S6\" title=\"6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>6</span></a>), we will provide details on these categorizations.</p>\n</section>\n</section>\n<section id=\"S4\">\n<h2>\n<span>4 </span><span id=\"S4.1.1\">KG-enhanced LLMs</span>\n</h2>\n<p id=\"S4.p1.1\">Large language models (LLMs) achieve promising results in many natural language processing tasks. However, LLMs have been criticized for their lack of practical knowledge and tendency to generate factual errors during inference. To address this issue, researchers have proposed integrating knowledge graphs (KGs) to enhance LLMs. In this section, we first introduce the KG-enhanced LLM pre-training, which aims to inject knowledge into LLMs during the pre-training stage. Then, we introduce the KG-enhanced LLM inference, which enables LLMs to consider the latest knowledge while generating sentences. Finally, we introduce the KG-enhanced LLM interpretability, which aims to improve the interpretability of LLMs by using KGs. Table <a href=\"https://arxiv.org/html/2306.08302v3#S4.T2\" title=\"TABLE II ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>II</span></a> summarizes the typical methods that integrate KGs for LLMs.</p>\n<figure id=\"S4.T2\">\n<figcaption><span>TABLE II: </span>Summary of KG-enhanced LLM methods.</figcaption>\n<p><span>\n<p id=\"S4.T2.1.1\">[b]\n\n<span id=\"S4.T2.1.1.1\">\n<span>\n<span id=\"S4.T2.1.1.1.1.1\">\n<span id=\"S4.T2.1.1.1.1.1.1\">Task</span>\n<span id=\"S4.T2.1.1.1.1.1.2\">Method</span>\n<span id=\"S4.T2.1.1.1.1.1.3\">Year</span>\n<span id=\"S4.T2.1.1.1.1.1.4\">KG</span>\n<span id=\"S4.T2.1.1.1.1.1.5\">Technique</span></span>\n<span id=\"S4.T2.1.1.1.2.2\">\n<span id=\"S4.T2.1.1.1.2.2.1\"><span id=\"S4.T2.1.1.1.2.2.1.1\">KG-enhanced LLM pre-training</span></span>\n<span id=\"S4.T2.1.1.1.2.2.2\">ERNIE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib35\" title=\"\">35</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.2.2.3\">2019</span>\n<span id=\"S4.T2.1.1.1.2.2.4\"><span id=\"S4.T2.1.1.1.2.2.4.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.2.2.5\">Integrating KGs into Training Objective</span></span>\n<span id=\"S4.T2.1.1.1.3.3\">\n<span id=\"S4.T2.1.1.1.3.3.1\">GLM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib102\" title=\"\">102</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.3.3.2\">2020</span>\n<span id=\"S4.T2.1.1.1.3.3.3\"><span id=\"S4.T2.1.1.1.3.3.3.1\">C</span></span>\n<span id=\"S4.T2.1.1.1.3.3.4\">Integrating KGs into Training Objective</span></span>\n<span id=\"S4.T2.1.1.1.4.4\">\n<span id=\"S4.T2.1.1.1.4.4.1\">Ebert <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib103\" title=\"\">103</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.4.4.2\">2020</span>\n<span id=\"S4.T2.1.1.1.4.4.3\"><span id=\"S4.T2.1.1.1.4.4.3.1\">D</span></span>\n<span id=\"S4.T2.1.1.1.4.4.4\">Integrating KGs into Training Objective</span></span>\n<span id=\"S4.T2.1.1.1.5.5\">\n<span id=\"S4.T2.1.1.1.5.5.1\">KEPLER <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.5.5.2\">2021</span>\n<span id=\"S4.T2.1.1.1.5.5.3\"><span id=\"S4.T2.1.1.1.5.5.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.5.5.4\">Integrating KGs into Training Objective</span></span>\n<span id=\"S4.T2.1.1.1.6.6\">\n<span id=\"S4.T2.1.1.1.6.6.1\">Deterministic LLM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib104\" title=\"\">104</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.6.6.2\">2022</span>\n<span id=\"S4.T2.1.1.1.6.6.3\"><span id=\"S4.T2.1.1.1.6.6.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.6.6.4\">Integrating KGs into Training Objective</span></span>\n<span id=\"S4.T2.1.1.1.7.7\">\n<span id=\"S4.T2.1.1.1.7.7.1\">KALA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib105\" title=\"\">105</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.7.7.2\">2022</span>\n<span id=\"S4.T2.1.1.1.7.7.3\"><span id=\"S4.T2.1.1.1.7.7.3.1\">D</span></span>\n<span id=\"S4.T2.1.1.1.7.7.4\">Integrating KGs into Training Objective</span></span>\n<span id=\"S4.T2.1.1.1.8.8\">\n<span id=\"S4.T2.1.1.1.8.8.1\">WKLM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib106\" title=\"\">106</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.8.8.2\">2020</span>\n<span id=\"S4.T2.1.1.1.8.8.3\"><span id=\"S4.T2.1.1.1.8.8.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.8.8.4\">Integrating KGs into Training Objective</span></span>\n<span id=\"S4.T2.1.1.1.9.9\">\n<span id=\"S4.T2.1.1.1.9.9.1\">K-BERT <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib36\" title=\"\">36</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.9.9.2\">2020</span>\n<span id=\"S4.T2.1.1.1.9.9.3\"><span id=\"S4.T2.1.1.1.9.9.3.1\">E</span> + <span id=\"S4.T2.1.1.1.9.9.3.2\">D</span></span>\n<span id=\"S4.T2.1.1.1.9.9.4\">Integrating KGs into Language Model Inputs</span></span>\n<span id=\"S4.T2.1.1.1.10.10\">\n<span id=\"S4.T2.1.1.1.10.10.1\">CoLAKE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib107\" title=\"\">107</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.10.10.2\">2020</span>\n<span id=\"S4.T2.1.1.1.10.10.3\"><span id=\"S4.T2.1.1.1.10.10.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.10.10.4\">Integrating KGs into Language Model Inputs</span></span>\n<span id=\"S4.T2.1.1.1.11.11\">\n<span id=\"S4.T2.1.1.1.11.11.1\">ERNIE3.0 <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib101\" title=\"\">101</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.11.11.2\">2021</span>\n<span id=\"S4.T2.1.1.1.11.11.3\"><span id=\"S4.T2.1.1.1.11.11.3.1\">E</span> + <span id=\"S4.T2.1.1.1.11.11.3.2\">D</span></span>\n<span id=\"S4.T2.1.1.1.11.11.4\">Integrating KGs into Language Model Inputs</span></span>\n<span id=\"S4.T2.1.1.1.12.12\">\n<span id=\"S4.T2.1.1.1.12.12.1\">DkLLM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib108\" title=\"\">108</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.12.12.2\">2022</span>\n<span id=\"S4.T2.1.1.1.12.12.3\"><span id=\"S4.T2.1.1.1.12.12.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.12.12.4\">Integrating KGs into Language Model Inputs</span></span>\n<span id=\"S4.T2.1.1.1.13.13\">\n<span id=\"S4.T2.1.1.1.13.13.1\">KP-PLM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib109\" title=\"\">109</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.13.13.2\">2022</span>\n<span id=\"S4.T2.1.1.1.13.13.3\"><span id=\"S4.T2.1.1.1.13.13.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.13.13.4\">KGs Instruction-tuning</span></span>\n<span id=\"S4.T2.1.1.1.14.14\">\n<span id=\"S4.T2.1.1.1.14.14.1\">OntoPrompt <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib110\" title=\"\">110</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.14.14.2\">2022</span>\n<span id=\"S4.T2.1.1.1.14.14.3\"><span id=\"S4.T2.1.1.1.14.14.3.1\">E</span> + <span id=\"S4.T2.1.1.1.14.14.3.2\">D</span></span>\n<span id=\"S4.T2.1.1.1.14.14.4\">KGs Instruction-tuning</span></span>\n<span id=\"S4.T2.1.1.1.15.15\">\n<span id=\"S4.T2.1.1.1.15.15.1\">ChatKBQA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib111\" title=\"\">111</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.15.15.2\">2023</span>\n<span id=\"S4.T2.1.1.1.15.15.3\"><span id=\"S4.T2.1.1.1.15.15.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.15.15.4\">KGs Instruction-tuning</span></span>\n<span id=\"S4.T2.1.1.1.16.16\">\n<span id=\"S4.T2.1.1.1.16.16.1\">RoG <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib112\" title=\"\">112</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.16.16.2\">2023</span>\n<span id=\"S4.T2.1.1.1.16.16.3\"><span id=\"S4.T2.1.1.1.16.16.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.16.16.4\">KGs Instruction-tuning</span></span>\n<span id=\"S4.T2.1.1.1.17.17\">\n<span id=\"S4.T2.1.1.1.17.17.1\"><span id=\"S4.T2.1.1.1.17.17.1.1\">KG-enhanced LLM inference</span></span>\n<span id=\"S4.T2.1.1.1.17.17.2\">KGLM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib113\" title=\"\">113</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.17.17.3\">2019</span>\n<span id=\"S4.T2.1.1.1.17.17.4\"><span id=\"S4.T2.1.1.1.17.17.4.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.17.17.5\">Retrival-augmented knowledge fusion</span></span>\n<span id=\"S4.T2.1.1.1.18.18\">\n<span id=\"S4.T2.1.1.1.18.18.1\">REALM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib114\" title=\"\">114</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.18.18.2\">2020</span>\n<span id=\"S4.T2.1.1.1.18.18.3\"><span id=\"S4.T2.1.1.1.18.18.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.18.18.4\">Retrival-augmented knowledge fusion</span></span>\n<span id=\"S4.T2.1.1.1.19.19\">\n<span id=\"S4.T2.1.1.1.19.19.1\">RAG <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib92\" title=\"\">92</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.19.19.2\">2020</span>\n<span id=\"S4.T2.1.1.1.19.19.3\"><span id=\"S4.T2.1.1.1.19.19.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.19.19.4\">Retrival-augmented knowledge fusion</span></span>\n<span id=\"S4.T2.1.1.1.20.20\">\n<span id=\"S4.T2.1.1.1.20.20.1\">EMAT <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib115\" title=\"\">115</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.20.20.2\">2022</span>\n<span id=\"S4.T2.1.1.1.20.20.3\"><span id=\"S4.T2.1.1.1.20.20.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.20.20.4\">Retrival-augmented knowledge fusion</span></span>\n<span id=\"S4.T2.1.1.1.21.21\">\n<span id=\"S4.T2.1.1.1.21.21.1\">Li et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib64\" title=\"\">64</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.21.21.2\">2023</span>\n<span id=\"S4.T2.1.1.1.21.21.3\"><span id=\"S4.T2.1.1.1.21.21.3.1\">C</span></span>\n<span id=\"S4.T2.1.1.1.21.21.4\">KGs Prompting</span></span>\n<span id=\"S4.T2.1.1.1.22.22\">\n<span id=\"S4.T2.1.1.1.22.22.1\">Mindmap <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib65\" title=\"\">65</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.22.22.2\">2023</span>\n<span id=\"S4.T2.1.1.1.22.22.3\"><span id=\"S4.T2.1.1.1.22.22.3.1\">E</span> + <span id=\"S4.T2.1.1.1.22.22.3.2\">D</span></span>\n<span id=\"S4.T2.1.1.1.22.22.4\">KGs Prompting</span></span>\n<span id=\"S4.T2.1.1.1.23.23\">\n<span id=\"S4.T2.1.1.1.23.23.1\">ChatRule <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib116\" title=\"\">116</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.23.23.2\">2023</span>\n<span id=\"S4.T2.1.1.1.23.23.3\"><span id=\"S4.T2.1.1.1.23.23.3.1\">E</span> + <span id=\"S4.T2.1.1.1.23.23.3.2\">D</span></span>\n<span id=\"S4.T2.1.1.1.23.23.4\">KGs Prompting</span></span>\n<span id=\"S4.T2.1.1.1.24.24\">\n<span id=\"S4.T2.1.1.1.24.24.1\">CoK <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib117\" title=\"\">117</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.24.24.2\">2023</span>\n<span id=\"S4.T2.1.1.1.24.24.3\"><span id=\"S4.T2.1.1.1.24.24.3.1\">E</span> + <span id=\"S4.T2.1.1.1.24.24.3.2\">C</span> + <span id=\"S4.T2.1.1.1.24.24.3.3\">D</span></span>\n<span id=\"S4.T2.1.1.1.24.24.4\">KGs Prompting</span></span>\n<span id=\"S4.T2.1.1.1.25.25\">\n<span id=\"S4.T2.1.1.1.25.25.1\"><span id=\"S4.T2.1.1.1.25.25.1.1\">KG-enhanced LLM interpretability</span></span>\n<span id=\"S4.T2.1.1.1.25.25.2\">LAMA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.25.25.3\">2019</span>\n<span id=\"S4.T2.1.1.1.25.25.4\"><span id=\"S4.T2.1.1.1.25.25.4.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.25.25.5\">KGs for LLM probing</span></span>\n<span id=\"S4.T2.1.1.1.26.26\">\n<span id=\"S4.T2.1.1.1.26.26.1\">LPAQA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib118\" title=\"\">118</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.26.26.2\">2020</span>\n<span id=\"S4.T2.1.1.1.26.26.3\"><span id=\"S4.T2.1.1.1.26.26.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.26.26.4\">KGs for LLM probing</span></span>\n<span id=\"S4.T2.1.1.1.27.27\">\n<span id=\"S4.T2.1.1.1.27.27.1\">Autoprompt <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib119\" title=\"\">119</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.27.27.2\">2020</span>\n<span id=\"S4.T2.1.1.1.27.27.3\"><span id=\"S4.T2.1.1.1.27.27.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.27.27.4\">KGs for LLM probing</span></span>\n<span id=\"S4.T2.1.1.1.28.28\">\n<span id=\"S4.T2.1.1.1.28.28.1\">MedLAMA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib120\" title=\"\">120</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.28.28.2\">2022</span>\n<span id=\"S4.T2.1.1.1.28.28.3\"><span id=\"S4.T2.1.1.1.28.28.3.1\">D</span></span>\n<span id=\"S4.T2.1.1.1.28.28.4\">KGs for LLM probing</span></span>\n<span id=\"S4.T2.1.1.1.29.29\">\n<span id=\"S4.T2.1.1.1.29.29.1\">LLM-facteval <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib121\" title=\"\">121</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.29.29.2\">2023</span>\n<span id=\"S4.T2.1.1.1.29.29.3\"><span id=\"S4.T2.1.1.1.29.29.3.1\">E</span> + <span id=\"S4.T2.1.1.1.29.29.3.2\">D</span></span>\n<span id=\"S4.T2.1.1.1.29.29.4\">KGs for LLM probing</span></span>\n<span id=\"S4.T2.1.1.1.30.30\">\n<span id=\"S4.T2.1.1.1.30.30.1\">KagNet <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib38\" title=\"\">38</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.30.30.2\">2019</span>\n<span id=\"S4.T2.1.1.1.30.30.3\"><span id=\"S4.T2.1.1.1.30.30.3.1\">C</span></span>\n<span id=\"S4.T2.1.1.1.30.30.4\">KGs for LLM analysis</span></span>\n<span id=\"S4.T2.1.1.1.31.31\">\n<span id=\"S4.T2.1.1.1.31.31.1\">Interpret-lm <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib122\" title=\"\">122</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.31.31.2\">2021</span>\n<span id=\"S4.T2.1.1.1.31.31.3\"><span id=\"S4.T2.1.1.1.31.31.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.31.31.4\">KGs for LLM analysis</span></span>\n<span id=\"S4.T2.1.1.1.32.32\">\n<span id=\"S4.T2.1.1.1.32.32.1\">knowledge-neurons <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib39\" title=\"\">39</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.32.32.2\">2021</span>\n<span id=\"S4.T2.1.1.1.32.32.3\"><span id=\"S4.T2.1.1.1.32.32.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.32.32.4\">KGs for LLM analysis</span></span>\n<span id=\"S4.T2.1.1.1.33.33\">\n<span id=\"S4.T2.1.1.1.33.33.1\">Shaobo et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib123\" title=\"\">123</a>]</cite></span>\n<span id=\"S4.T2.1.1.1.33.33.2\">2022</span>\n<span id=\"S4.T2.1.1.1.33.33.3\"><span id=\"S4.T2.1.1.1.33.33.3.1\">E</span></span>\n<span id=\"S4.T2.1.1.1.33.33.4\">KGs for LLM analysis</span></span>\n</span>\n</span></p>\n<ul id=\"S4.I1\">\n<li id=\"S4.I1.i1\">\n<span>•</span>\n<p id=\"S4.I1.i1.p1.1\"><span id=\"S4.I1.i1.p1.1.1\">E</span>: Encyclopedic Knowledge Graphs, <span id=\"S4.I1.i1.p1.1.2\">C</span>: Commonsense Knowledge Graphs, <span id=\"S4.I1.i1.p1.1.3\">D</span>: Domain-Specific Knowledge Graphs.</p>\n</li>\n</ul>\n</span></p>\n</figure>\n<section id=\"S4.SS1\">\n<h3>\n<span>4.1 </span><span id=\"S4.SS1.1.1\">KG-enhanced LLM Pre-training</span>\n</h3>\n<p id=\"S4.SS1.p1.1\">Existing large language models mostly rely on unsupervised training on the large-scale corpus. While these models may exhibit impressive performance on downstream tasks, they often lack practical knowledge relevant to the real world. Previous works that integrate KGs into large language models can be categorized into three parts: <em id=\"S4.SS1.p1.1.1\">1) Integrating KGs into training objective</em>, <em id=\"S4.SS1.p1.1.2\">2) Integrating KGs into LLM inputs</em>, and <em id=\"S4.SS1.p1.1.3\">3) KGs Instruction-tuning</em>.</p>\n<section id=\"S4.SS1.SSS1\">\n<h4>\n<span>4.1.1 </span>Integrating KGs into Training Objective</h4>\n<p id=\"S4.SS1.SSS1.p1.1\">The research efforts in this category focus on designing novel knowledge-aware training objectives. An intuitive idea is to expose more knowledge entities in the pre-training objective. GLM&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib102\" title=\"\">102</a>]</cite> leverages the knowledge graph structure to assign a masking probability. Specifically, entities that can be reached within a certain number of hops are considered to be the most important entities for learning, and they are given a higher masking probability during pre-training. Furthermore, E-BERT&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib103\" title=\"\">103</a>]</cite> further controls the balance between the token-level and entity-level training losses.\nThe training loss values are used as indications of the learning process for token and entity, which dynamically determines their ratio for the next training epochs. SKEP&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib124\" title=\"\">124</a>]</cite> also follows a similar fusion to inject sentiment knowledge during LLMs pre-training. SKEP first determines words with positive and negative sentiment by utilizing PMI along with a predefined set of seed sentiment words. Then, it assigns a higher masking probability to those identified sentiment words in the word masking objective.</p>\n<p id=\"S4.SS1.SSS1.p2.1\">The other line of work explicitly leverages the connections with knowledge and input text. As shown in Fig.&nbsp;<a href=\"https://arxiv.org/html/2306.08302v3#S4.F9\" title=\"Figure 9 ‣ 4.1.1 Integrating KGs into Training Objective ‣ 4.1 KG-enhanced LLM Pre-training ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>9</span></a>, ERNIE&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib35\" title=\"\">35</a>]</cite> proposes a novel word-entity alignment training objective as a pre-training objective. Specifically, ERNIE feeds both sentences and corresponding entities mentioned in the text into LLMs, and then trains the LLMs to predict alignment links between textual tokens and entities in knowledge graphs. Similarly, KALM&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib91\" title=\"\">91</a>]</cite> enhances the input tokens by incorporating entity embeddings and includes an entity prediction pre-training task in addition to the token-only pre-training objective. This approach aims to improve the ability of LLMs to capture knowledge related to entities. Finally, KEPLER&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite> directly employs both knowledge graph embedding training objective and Masked token pre-training objective into a shared transformer-based encoder. Deterministic LLM&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib104\" title=\"\">104</a>]</cite> focuses on pre-training language models to capture <em id=\"S4.SS1.SSS1.p2.1.1\">deterministic</em> factual knowledge. It only masks the span that has a deterministic entity as the question and introduces additional clue contrast learning and clue classification objective. WKLM&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib106\" title=\"\">106</a>]</cite> first replaces entities in the text with other same-type entities and then feeds them into LLMs. The model is further pre-trained to distinguish whether the entities have been replaced or not.</p>\n<figure id=\"S4.F9\"><img alt=\"Refer to caption\" height=\"377\" id=\"S4.F9.g1\" src=\"x8.png\" width=\"664\">\n<figcaption><span>Figure 9: </span>Injecting KG information into LLMs training objective via text-knowledge alignment loss, where <math alttext=\"h\" display=\"inline\" id=\"S4.F9.2.m1.1\"><semantics id=\"S4.F9.2.m1.1b\"><mi id=\"S4.F9.2.m1.1.1\" xref=\"S4.F9.2.m1.1.1.cmml\">h</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.F9.2.m1.1c\"><ci id=\"S4.F9.2.m1.1.1.cmml\" xref=\"S4.F9.2.m1.1.1\">ℎ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.F9.2.m1.1d\">h</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.F9.2.m1.1e\">italic_h</annotation></semantics></math> denotes the hidden representation generated by LLMs.</figcaption>\n</figure>\n</section>\n<section id=\"S4.SS1.SSS2\">\n<h4>\n<span>4.1.2 </span>Integrating KGs into LLM Inputs</h4>\n<p id=\"S4.SS1.SSS2.p1.1\">As shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S4.F10\" title=\"Figure 10 ‣ 4.1.2 Integrating KGs into LLM Inputs ‣ 4.1 KG-enhanced LLM Pre-training ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>10</span></a>, this kind of research focus on introducing relevant knowledge sub-graph into the inputs of LLMs. Given a knowledge graph triple and the corresponding sentences, ERNIE 3.0&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib101\" title=\"\">101</a>]</cite> represents the triple as a sequence of tokens and directly concatenates them with the sentences. It further randomly masks either the relation token in the triple or tokens in the sentences\nto better combine knowledge with textual representations. However, such\ndirect knowledge triple concatenation method allows the tokens in the sentence to intensively interact with the tokens in the knowledge sub-graph, which could result in <em id=\"S4.SS1.SSS2.p1.1.1\">Knowledge Noise</em>&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib36\" title=\"\">36</a>]</cite>. To solve this issue, K-BERT&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib36\" title=\"\">36</a>]</cite> takes the first step to inject the knowledge triple into the sentence via a <em id=\"S4.SS1.SSS2.p1.1.2\">visible matrix</em> where only the knowledge entities have access to the knowledge triple information, while the tokens in the sentences can only see each other in the self-attention module. To further reduce <em id=\"S4.SS1.SSS2.p1.1.3\">Knowledge Noise</em>, Colake&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib107\" title=\"\">107</a>]</cite> proposes a unified word-knowledge graph (shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S4.F10\" title=\"Figure 10 ‣ 4.1.2 Integrating KGs into LLM Inputs ‣ 4.1 KG-enhanced LLM Pre-training ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>10</span></a>) where the tokens in the input sentences form a fully connected word graph where tokens aligned with knowledge entities are connected with their neighboring entities.</p>\n<p id=\"S4.SS1.SSS2.p2.1\">The above methods can indeed inject a large amount of knowledge into LLMs. However, they mostly focus on popular entities and overlook the low-frequent and long-tail ones. DkLLM&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib108\" title=\"\">108</a>]</cite> aims to improve the LLMs representations towards those entities. DkLLM first proposes a novel measurement to determine long-tail entities and then replaces these selected entities in the text with pseudo token embedding as new input to the large language models. Furthermore, Dict-BERT&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib125\" title=\"\">125</a>]</cite> proposes to leverage external dictionaries to solve this issue. Specifically, Dict-BERT improves the representation quality of rare words by appending their definitions from the dictionary at the end of input text and trains the language model to locally align rare word representations in input sentences and dictionary definitions as well as to discriminate whether the input text and definition are correctly mapped.</p>\n<figure id=\"S4.F10\"><img alt=\"Refer to caption\" height=\"574\" id=\"S4.F10.g1\" src=\"x9.png\" width=\"581\">\n<figcaption><span>Figure 10: </span>Injecting KG information into LLMs inputs using graph structure.</figcaption>\n</figure>\n</section>\n<section id=\"S4.SS1.SSS3\">\n<h4>\n<span>4.1.3 </span>KGs Instruction-tuning</h4>\n<p id=\"S4.SS1.SSS3.p1.1\">Instead of injecting factual knowledge into LLMs, the KGs Instruction-tuning aims to fine-tune LLMs to better comprehend the structure of KGs and effectively follow user instructions to conduct complex tasks. KGs Instruction-tuning utilizes both facts and the structure of KGs to create instruction-tuning datasets. LLMs finetuned on these datasets can extract both factual and structural knowledge from KGs, enhancing the reasoning ability of LLMs.\nKP-PLM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib109\" title=\"\">109</a>]</cite> first designs several prompt templates to transfer structural graphs into natural language text. Then, two self-supervised tasks are proposed to finetune LLMs to further leverage the knowledge from these prompts. OntoPrompt <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib110\" title=\"\">110</a>]</cite> proposes an ontology-enhanced prompt-tuning that can place knowledge of entities into the context of LLMs, which are further finetuned on several downstream tasks. ChatKBQA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib111\" title=\"\">111</a>]</cite> finetunes LLMs on KG structure to generate logical queries, which can be executed on KGs to obtain answers. To better reason on graphs, RoG <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib112\" title=\"\">112</a>]</cite> presents a planning-retrieval-reasoning framework. RoG is finetuned on KG structure to generate relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning and generate interpretable results.</p>\n<p id=\"S4.SS1.SSS3.p2.1\">KGs Instruction-tuning can better leverage the knowledge from KGs for downstream tasks. However, it requires retraining the models, which is time-consuming and requires lots of resources.</p>\n</section>\n</section>\n<section id=\"S4.SS2\">\n<h3>\n<span>4.2 </span><span id=\"S4.SS2.1.1\">KG-enhanced LLM Inference</span>\n</h3>\n<p id=\"S4.SS2.p1.1\">The above methods could effectively fuse knowledge into LLMs. However, real-world knowledge is subject to change and the limitation of these approaches is that they do not permit updates to the incorporated knowledge without retraining the model. As a result, they may not generalize well to the unseen knowledge during inference&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib126\" title=\"\">126</a>]</cite>. Therefore, considerable research has been devoted to keeping the knowledge space and text space separate and injecting the knowledge while inference. These methods mostly focus on the Question Answering (QA) tasks, because QA requires the model to capture both textual semantic meanings and up-to-date real-world knowledge.</p>\n<section id=\"S4.SS2.SSS1\">\n<h4>\n<span>4.2.1 </span>Retrieval-Augmented Knowledge Fusion</h4>\n<p id=\"S4.SS2.SSS1.p1.1\">Retrieval-Augmented Knowledge Fusion is a popular method to inject knowledge into LLMs during inference. The key idea is to retrieve relevant knowledge from a large corpus and then fuse the retrieved knowledge into LLMs. As shown in Fig.&nbsp;<a href=\"https://arxiv.org/html/2306.08302v3#S4.F11\" title=\"Figure 11 ‣ 4.2.1 Retrieval-Augmented Knowledge Fusion ‣ 4.2 KG-enhanced LLM Inference ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>11</span></a>, RAG&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib92\" title=\"\">92</a>]</cite> proposes to combine non-parametric and parametric modules to handle the external knowledge. Given the input text, RAG first searches for relevant KG in the non-parametric module via MIPS to obtain several documents. RAG then treats these documents as hidden variables <math alttext=\"z\" display=\"inline\" id=\"S4.SS2.SSS1.p1.1.m1.1\"><semantics id=\"S4.SS2.SSS1.p1.1.m1.1a\"><mi id=\"S4.SS2.SSS1.p1.1.m1.1.1\" xref=\"S4.SS2.SSS1.p1.1.m1.1.1.cmml\">z</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.SSS1.p1.1.m1.1b\"><ci id=\"S4.SS2.SSS1.p1.1.m1.1.1.cmml\" xref=\"S4.SS2.SSS1.p1.1.m1.1.1\">𝑧</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.SSS1.p1.1.m1.1c\">z</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS2.SSS1.p1.1.m1.1d\">italic_z</annotation></semantics></math> and feeds them into the output generator, empowered by Seq2Seq LLMs, as additional context information. The research indicates that using different retrieved documents as conditions at different generation steps performs better than only using a single document to guide the whole generation process.\nThe experimental results show that RAG outperforms other parametric-only and non-parametric-only baseline models in open-domain QA. RAG can also generate more specific, diverse, and factual text than other parameter-only baselines. Story-fragments <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib127\" title=\"\">127</a>]</cite> further improves architecture by adding an additional module to determine salient knowledge entities and fuse them into the generator to improve the quality of generated long stories. EMAT&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib115\" title=\"\">115</a>]</cite> further improves the efficiency of such a system by encoding external knowledge into a key-value memory and exploiting the\nfast maximum inner product search for memory querying. REALM&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib114\" title=\"\">114</a>]</cite> proposes a novel knowledge retriever to help the model to retrieve and attend over documents from a large corpus during the pre-training stage and successfully improves the performance of open-domain question answering. KGLM&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib113\" title=\"\">113</a>]</cite> selects the facts from a knowledge graph using the current context to generate factual sentences. With the help of an external knowledge graph, KGLM could describe facts using out-of-domain words or phrases.</p>\n<figure id=\"S4.F11\"><img alt=\"Refer to caption\" height=\"339\" id=\"S4.F11.g1\" src=\"x10.png\" width=\"830\">\n<figcaption><span>Figure 11: </span>Retrieving external knowledge to enhance the LLM generation.</figcaption>\n</figure>\n</section>\n<section id=\"S4.SS2.SSS2\">\n<h4>\n<span>4.2.2 </span>KGs Prompting</h4>\n<p id=\"S4.SS2.SSS2.p1.1\">To better feed the KG structure into the LLM during inference, KGs prompting aims to design a crafted prompt that converts structured KGs into text sequences, which can be fed as context into LLMs. In this way, LLMs can better take advantage of the structure of KGs to perform reasoning. Li et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib64\" title=\"\">64</a>]</cite> adopt the pre-defined template to convert each triple into a short sentence, which can be understood by LLMs for reasoning. Mindmap <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib65\" title=\"\">65</a>]</cite> designs a KG prompt to convert graph structure into a mind map that enables LLMs to perform reasoning by consolidating the facts in KGs and the implicit knowledge from LLMs.\nChatRule <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib116\" title=\"\">116</a>]</cite> samples several relation paths from KGs, which are verbalized and fed into LLMs. Then, LLMs are prompted to generate meaningful logical rules that can be used for reasoning. CoK <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib117\" title=\"\">117</a>]</cite> proposes a chain-of-knowledge prompting that uses a sequence of triples to elicit the reasoning ability of LLMs to reach the final answer.</p>\n<p id=\"S4.SS2.SSS2.p2.1\">KGs prompting presents a simple way to synergize LLMs and KGs. By using the prompt, we can easily harness the power of LLMs to perform reasoning based on KGs without retraining the models. However, the prompt is usually designed manually, which requires lots of human effort.</p>\n</section>\n</section>\n<section id=\"S4.SS3\">\n<h3>\n<span>4.3 </span><span id=\"S4.SS3.1.1\">Comparison between KG-enhanced LLM Pre-training and Inference</span>\n</h3>\n<p id=\"S4.SS3.p1.1\">KG-enhanced LLM Pre-training methods commonly enrich large-amount of unlabeled corpus with semantically relevant real-world knowledge. These methods allow the knowledge representations to be aligned with appropriate linguistic context and explicitly train LLMs to leverage those knowledge from scratch. When applying the resulting LLMs to downstream knowledge-intensive tasks, they should achieve optimal performance. In contrast, KG-enhanced LLM inference methods only present the knowledge to LLMs in the inference stage and the underlying LLMs may not be trained to fully leverage these knowledge when conducting downstream tasks, potentially resulting in sub-optimal model performance.</p>\n<p id=\"S4.SS3.p2.1\">However, real-world knowledge is dynamic and requires frequent updates. Despite being effective, the KG-enhanced LLM Pre-training methods never permit knowledge updates or editing without model re-training. As a result, the KG-enhanced LLM Pre-training methods could generalize poorly to recent or unseen knowledge. KG-enhanced LLM inference methods can easily maintain knowledge updates by changing the inference inputs. These methods help improve LLMs performance on new knowledge and domains.</p>\n<p id=\"S4.SS3.p3.1\">In summary, when to use these methods depends on the application scenarios. If one wishes to apply LLMs to handle time-insensitive knowledge in particular domains (e.g., commonsense and reasoning knowledge), KG-enhanced LLM Pre-training methods should be considered. Otherwise, KG-enhanced LLM inference methods can be used to handle open-domain knowledge with frequent updates.</p>\n<figure id=\"S4.F12\"><img alt=\"Refer to caption\" height=\"241\" id=\"S4.F12.g1\" src=\"extracted/5367551/figs/LLM_probing.png\" width=\"419\">\n<figcaption><span>Figure 12: </span>The general framework of using knowledge graph for language model probing.</figcaption>\n</figure>\n</section>\n<section id=\"S4.SS4\">\n<h3>\n<span>4.4 </span><span id=\"S4.SS4.1.1\">KG-enhanced LLM Interpretability</span>\n</h3>\n<p id=\"S4.SS4.p1.1\">Although LLMs have achieved remarkable success in many NLP tasks, they are still criticized for their lack of interpretability. The large language model (LLM) interpretability refers to the understanding and explanation of the inner workings and decision-making processes of a large language model <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib17\" title=\"\">17</a>]</cite>. This can improve the trustworthiness of LLMs and facilitate their applications in high-stakes scenarios such as medical diagnosis and legal judgment.\nKnowledge graphs (KGs) represent the knowledge structurally and can provide good interpretability for the reasoning results. Therefore, researchers try to utilize KGs to improve the interpretability of LLMs, which can be roughly grouped into two categories: <em id=\"S4.SS4.p1.1.1\">1) KGs for language model probing</em>, and <em id=\"S4.SS4.p1.1.2\">2) KGs for language model analysis</em>.</p>\n<section id=\"S4.SS4.SSS1\">\n<h4>\n<span>4.4.1 </span>KGs for LLM Probing</h4>\n<p id=\"S4.SS4.SSS1.p1.1\">The large language model (LLM) probing aims to understand the knowledge stored in LLMs. LLMs, trained on large-scale corpus, are often known as containing enormous knowledge. However, LLMs store the knowledge in a hidden way, making it hard to figure out the stored knowledge. Moreover, LLMs suffer from the hallucination problem <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib15\" title=\"\">15</a>]</cite>, which results in generating statements that contradict facts. This issue significantly affects the reliability of LLMs. Therefore, it is necessary to probe and verify the knowledge stored in LLMs.</p>\n<p id=\"S4.SS4.SSS1.p2.1\">LAMA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a>]</cite> is the first work to probe the knowledge in LLMs by using KGs. As shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S4.F12\" title=\"Figure 12 ‣ 4.3 Comparison between KG-enhanced LLM Pre-training and Inference ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>12</span></a>, LAMA first converts the facts in KGs into cloze statements by a pre-defined prompt template and then uses LLMs to predict the missing entity. The prediction results are used to evaluate the knowledge stored in LLMs. For example, we try to probe whether LLMs know the fact <span id=\"S4.SS4.SSS1.p2.1.1\">(Obama, profession, president)</span>. We first convert the fact triple into a cloze question “Obama’s profession is <math alttext=\"\\_\" display=\"inline\" id=\"S4.SS4.SSS1.p2.1.m1.1\"><semantics id=\"S4.SS4.SSS1.p2.1.m1.1a\"><mi id=\"S4.SS4.SSS1.p2.1.m1.1.1\" mathvariant=\"normal\" xref=\"S4.SS4.SSS1.p2.1.m1.1.1.cmml\">_</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS4.SSS1.p2.1.m1.1b\"><ci id=\"S4.SS4.SSS1.p2.1.m1.1.1.cmml\" xref=\"S4.SS4.SSS1.p2.1.m1.1.1\">_</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS4.SSS1.p2.1.m1.1c\">\\_</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.SS4.SSS1.p2.1.m1.1d\">_</annotation></semantics></math>.” with the object masked. Then, we test if the LLMs can predict the object “president” correctly.</p>\n<p id=\"S4.SS4.SSS1.p3.1\">However, LAMA ignores the fact that the prompts are inappropriate. For example, the prompt <span id=\"S4.SS4.SSS1.p3.1.1\">“Obama worked as a _”</span> may be more favorable to the prediction of the blank by the language models than <span id=\"S4.SS4.SSS1.p3.1.2\">“Obama is a _ by profession”</span>. Thus, LPAQA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib118\" title=\"\">118</a>]</cite> proposes a mining and paraphrasing-based method to automatically generate high-quality and diverse prompts for a more accurate assessment of the knowledge contained in the language model. Moreover, Adolphs et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib128\" title=\"\">128</a>]</cite> attempt to use examples to make the language model understand the query, and experiments obtain substantial improvements for BERT-large on the T-REx data. Unlike using manually defined prompt templates, Autoprompt <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib119\" title=\"\">119</a>]</cite> proposes an automated method, which is based on the gradient-guided search to create prompts. LLM-facteval <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib121\" title=\"\">121</a>]</cite> designs a systematic framework that automatically generates probing questions from KGs. The generated questions are then used to evaluate the factual knowledge stored in LLMs.</p>\n<p id=\"S4.SS4.SSS1.p4.1\">Instead of probing the general knowledge by using the encyclopedic and commonsense knowledge graphs, BioLAMA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib129\" title=\"\">129</a>]</cite> and MedLAMA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib120\" title=\"\">120</a>]</cite> probe the medical knowledge in LLMs by using medical knowledge graphs.\nAlex et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib130\" title=\"\">130</a>]</cite> investigate the capacity of LLMs to retain less popular factual knowledge. They select unpopular facts from Wikidata knowledge graphs which have low-frequency clicked entities. These facts are then used for the evaluation, where the results indicate that LLMs encounter difficulties with such knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the tail.</p>\n<figure id=\"S4.F13\"><img alt=\"Refer to caption\" height=\"280\" id=\"S4.F13.g1\" src=\"extracted/5367551/figs/LLM_analysis.png\" width=\"419\">\n<figcaption><span>Figure 13: </span>The general framework of using knowledge graph for language model analysis.</figcaption>\n</figure>\n</section>\n<section id=\"S4.SS4.SSS2\">\n<h4>\n<span>4.4.2 </span>KGs for LLM Analysis</h4>\n<p id=\"S4.SS4.SSS2.p1.1\">Knowledge graphs (KGs) for pre-train language models (LLMs) analysis aims to answer the following questions such as “how do LLMs generate the results?”, and “how do the function and structure work in LLMs?”.\nTo analyze the inference process of LLMs, as shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S4.F13\" title=\"Figure 13 ‣ 4.4.1 KGs for LLM Probing ‣ 4.4 KG-enhanced LLM Interpretability ‣ 4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>13</span></a>, KagNet <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib38\" title=\"\">38</a>]</cite> and QA-GNN <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib131\" title=\"\">131</a>]</cite> make the results generated by LLMs at each reasoning step grounded by knowledge graphs. In this way, the reasoning process of LLMs can be explained by extracting the graph structure from KGs. Shaobo et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib123\" title=\"\">123</a>]</cite> investigate how LLMs generate the results correctly. They adopt the causal-inspired analysis from facts extracted from KGs. This analysis quantitatively measures the word patterns that LLMs depend on to generate the results. The results show that LLMs generate the missing factual more by the positionally closed words rather than the knowledge-dependent words. Thus, they claim that LLMs are inadequate to memorize factual knowledge because of the inaccurate dependence.\nTo interpret the training of LLMs, Swamy et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib122\" title=\"\">122</a>]</cite> adopt the language model during pre-training to generate knowledge graphs. The knowledge acquired by LLMs during training can be unveiled by the facts in KGs explicitly. To explore how implicit knowledge is stored in parameters of LLMs, Dai&nbsp;et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib39\" title=\"\">39</a>]</cite> propose the concept of <em id=\"S4.SS4.SSS2.p1.1.1\">knowledge neurons</em>. Specifically, activation of the identified knowledge neurons is highly correlated with knowledge expression. Thus, they explore the knowledge and facts represented by each neuron by suppressing and amplifying knowledge neurons.</p>\n</section>\n</section>\n</section>\n<section id=\"S5\">\n<h2>\n<span>5 </span><span id=\"S5.1.1\">LLM-augmented KGs</span>\n</h2>\n<p id=\"S5.p1.1\">Knowledge graphs are famous for representing knowledge in a structural manner. They have been applied in many downstream tasks such as question answering, recommendation, and web search. However, the conventional KGs are often incomplete and existing methods often lack considering textual information. To address these issues, recent research has explored integrating LLMs to augment KGs to consider the textual information and improve the performance in downstream tasks. In this section, we will introduce the recent research on LLM-augmented KGs. We will introduce the methods that integrate LLMs for KG embedding, KG completion, KG construction, KG-to-text generation, and KG question answering, respectively. Representative works are summarized in Table <a href=\"https://arxiv.org/html/2306.08302v3#S5.T3\" title=\"TABLE III ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>III</span></a>.</p>\n<figure id=\"S5.T3\">\n<figcaption><span>TABLE III: </span>Summary of representative LLM-augmented KG methods.</figcaption>\n<p><span>\n<p id=\"S5.T3.1.1\">[b]\n\n<span id=\"S5.T3.1.1.1\">\n<span>\n<span id=\"S5.T3.1.1.1.1.1\">\n<span id=\"S5.T3.1.1.1.1.1.1\">Task</span>\n<span id=\"S5.T3.1.1.1.1.1.2\">Method</span>\n<span id=\"S5.T3.1.1.1.1.1.3\">Year</span>\n<span id=\"S5.T3.1.1.1.1.1.4\">LLM</span>\n<span id=\"S5.T3.1.1.1.1.1.5\">Technique</span></span>\n<span id=\"S5.T3.1.1.1.2.2\">\n<span id=\"S5.T3.1.1.1.2.2.1\"><span id=\"S5.T3.1.1.1.2.2.1.1\">LLM-augmented KG embedding</span></span>\n<span id=\"S5.T3.1.1.1.2.2.2\">Pretrain-KGE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib94\" title=\"\">94</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.2.2.3\">2020</span>\n<span id=\"S5.T3.1.1.1.2.2.4\"><span id=\"S5.T3.1.1.1.2.2.4.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.2.2.5\">LLMs as Text Encoders</span></span>\n<span id=\"S5.T3.1.1.1.3.3\">\n<span id=\"S5.T3.1.1.1.3.3.1\">KEPLER <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.3.3.2\">2020</span>\n<span id=\"S5.T3.1.1.1.3.3.3\"><span id=\"S5.T3.1.1.1.3.3.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.3.3.4\">LLMs as Text Encoders</span></span>\n<span id=\"S5.T3.1.1.1.4.4\">\n<span id=\"S5.T3.1.1.1.4.4.1\">Nayyeri et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib132\" title=\"\">132</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.4.4.2\">2022</span>\n<span id=\"S5.T3.1.1.1.4.4.3\"><span id=\"S5.T3.1.1.1.4.4.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.4.4.4\">LLMs as Text Encoders</span></span>\n<span id=\"S5.T3.1.1.1.5.5\">\n<span id=\"S5.T3.1.1.1.5.5.1\">Huang et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib133\" title=\"\">133</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.5.5.2\">2022</span>\n<span id=\"S5.T3.1.1.1.5.5.3\"><span id=\"S5.T3.1.1.1.5.5.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.5.5.4\">LLMs as Text Encoders</span></span>\n<span id=\"S5.T3.1.1.1.6.6\">\n<span id=\"S5.T3.1.1.1.6.6.1\">CoDEx <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib134\" title=\"\">134</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.6.6.2\">2022</span>\n<span id=\"S5.T3.1.1.1.6.6.3\"><span id=\"S5.T3.1.1.1.6.6.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.6.6.4\">LLMs as Text Encoders</span></span>\n<span id=\"S5.T3.1.1.1.7.7\">\n<span id=\"S5.T3.1.1.1.7.7.1\">LMKE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib135\" title=\"\">135</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.7.7.2\">2022</span>\n<span id=\"S5.T3.1.1.1.7.7.3\"><span id=\"S5.T3.1.1.1.7.7.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.7.7.4\">LLMs for Joint Text and KG Embedding</span></span>\n<span id=\"S5.T3.1.1.1.8.8\">\n<span id=\"S5.T3.1.1.1.8.8.1\">kNN-KGE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib136\" title=\"\">136</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.8.8.2\">2022</span>\n<span id=\"S5.T3.1.1.1.8.8.3\"><span id=\"S5.T3.1.1.1.8.8.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.8.8.4\">LLMs for Joint Text and KG Embedding</span></span>\n<span id=\"S5.T3.1.1.1.9.9\">\n<span id=\"S5.T3.1.1.1.9.9.1\">LambdaKG <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib137\" title=\"\">137</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.9.9.2\">2023</span>\n<span id=\"S5.T3.1.1.1.9.9.3\"><span id=\"S5.T3.1.1.1.9.9.3.1\">E</span> + <span id=\"S5.T3.1.1.1.9.9.3.2\">D</span> + <span id=\"S5.T3.1.1.1.9.9.3.3\">ED</span></span>\n<span id=\"S5.T3.1.1.1.9.9.4\">LLMs for Joint Text and KG Embedding</span></span>\n<span id=\"S5.T3.1.1.1.10.10\">\n<span id=\"S5.T3.1.1.1.10.10.1\"><span id=\"S5.T3.1.1.1.10.10.1.1\">LLM-augmented KG completion</span></span>\n<span id=\"S5.T3.1.1.1.10.10.2\">KG-BERT&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib26\" title=\"\">26</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.10.10.3\">2019</span>\n<span id=\"S5.T3.1.1.1.10.10.4\"><span id=\"S5.T3.1.1.1.10.10.4.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.10.10.5\">Joint Encoding</span></span>\n<span id=\"S5.T3.1.1.1.11.11\">\n<span id=\"S5.T3.1.1.1.11.11.1\">MTL-KGC&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib138\" title=\"\">138</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.11.11.2\">2020</span>\n<span id=\"S5.T3.1.1.1.11.11.3\"><span id=\"S5.T3.1.1.1.11.11.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.11.11.4\">Joint Encoding</span></span>\n<span id=\"S5.T3.1.1.1.12.12\">\n<span id=\"S5.T3.1.1.1.12.12.1\">PKGC&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib139\" title=\"\">139</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.12.12.2\">2022</span>\n<span id=\"S5.T3.1.1.1.12.12.3\"><span id=\"S5.T3.1.1.1.12.12.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.12.12.4\">Joint Encoding</span></span>\n<span id=\"S5.T3.1.1.1.13.13\">\n<span id=\"S5.T3.1.1.1.13.13.1\">LASS&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib140\" title=\"\">140</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.13.13.2\">2022</span>\n<span id=\"S5.T3.1.1.1.13.13.3\"><span id=\"S5.T3.1.1.1.13.13.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.13.13.4\">Joint Encoding</span></span>\n<span id=\"S5.T3.1.1.1.14.14\">\n<span id=\"S5.T3.1.1.1.14.14.1\">MEM-KGC&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib141\" title=\"\">141</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.14.14.2\">2021</span>\n<span id=\"S5.T3.1.1.1.14.14.3\"><span id=\"S5.T3.1.1.1.14.14.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.14.14.4\">MLM Encoding</span></span>\n<span id=\"S5.T3.1.1.1.15.15\">\n<span id=\"S5.T3.1.1.1.15.15.1\">OpenWorld KGC <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib142\" title=\"\">142</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.15.15.2\">2023</span>\n<span id=\"S5.T3.1.1.1.15.15.3\"><span id=\"S5.T3.1.1.1.15.15.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.15.15.4\">MLM Encoding</span></span>\n<span id=\"S5.T3.1.1.1.16.16\">\n<span id=\"S5.T3.1.1.1.16.16.1\">StAR&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib143\" title=\"\">143</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.16.16.2\">2021</span>\n<span id=\"S5.T3.1.1.1.16.16.3\"><span id=\"S5.T3.1.1.1.16.16.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.16.16.4\">Separated Encoding</span></span>\n<span id=\"S5.T3.1.1.1.17.17\">\n<span id=\"S5.T3.1.1.1.17.17.1\">SimKGC&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib144\" title=\"\">144</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.17.17.2\">2022</span>\n<span id=\"S5.T3.1.1.1.17.17.3\"><span id=\"S5.T3.1.1.1.17.17.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.17.17.4\">Separated Encoding</span></span>\n<span id=\"S5.T3.1.1.1.18.18\">\n<span id=\"S5.T3.1.1.1.18.18.1\">LP-BERT <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib145\" title=\"\">145</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.18.18.2\">2022</span>\n<span id=\"S5.T3.1.1.1.18.18.3\"><span id=\"S5.T3.1.1.1.18.18.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.18.18.4\">Separated Encoding</span></span>\n<span id=\"S5.T3.1.1.1.19.19\">\n<span id=\"S5.T3.1.1.1.19.19.1\">GenKGC&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib96\" title=\"\">96</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.19.19.2\">2022</span>\n<span id=\"S5.T3.1.1.1.19.19.3\"><span id=\"S5.T3.1.1.1.19.19.3.1\">ED</span></span>\n<span id=\"S5.T3.1.1.1.19.19.4\">LLM as decoders</span></span>\n<span id=\"S5.T3.1.1.1.20.20\">\n<span id=\"S5.T3.1.1.1.20.20.1\">KGT5&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib146\" title=\"\">146</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.20.20.2\">2022</span>\n<span id=\"S5.T3.1.1.1.20.20.3\"><span id=\"S5.T3.1.1.1.20.20.3.1\">ED</span></span>\n<span id=\"S5.T3.1.1.1.20.20.4\">LLM as decoders</span></span>\n<span id=\"S5.T3.1.1.1.21.21\">\n<span id=\"S5.T3.1.1.1.21.21.1\">KG-S2S&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib147\" title=\"\">147</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.21.21.2\">2022</span>\n<span id=\"S5.T3.1.1.1.21.21.3\"><span id=\"S5.T3.1.1.1.21.21.3.1\">ED</span></span>\n<span id=\"S5.T3.1.1.1.21.21.4\">LLM as decoders</span></span>\n<span id=\"S5.T3.1.1.1.22.22\">\n<span id=\"S5.T3.1.1.1.22.22.1\"></span>\n<span id=\"S5.T3.1.1.1.22.22.2\">AutoKG&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib93\" title=\"\">93</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.22.22.3\">2023</span>\n<span id=\"S5.T3.1.1.1.22.22.4\"><span id=\"S5.T3.1.1.1.22.22.4.1\">D</span></span>\n<span id=\"S5.T3.1.1.1.22.22.5\">LLM as decoders</span></span>\n<span id=\"S5.T3.1.1.1.23.23\">\n<span id=\"S5.T3.1.1.1.23.23.1\"><span id=\"S5.T3.1.1.1.23.23.1.1\">LLM-augmented KG construction</span></span>\n<span id=\"S5.T3.1.1.1.23.23.2\">ELMO <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib148\" title=\"\">148</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.23.23.3\">2018</span>\n<span id=\"S5.T3.1.1.1.23.23.4\"><span id=\"S5.T3.1.1.1.23.23.4.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.23.23.5\">Named Entity Recognition</span></span>\n<span id=\"S5.T3.1.1.1.24.24\">\n<span id=\"S5.T3.1.1.1.24.24.1\">GenerativeNER <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib149\" title=\"\">149</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.24.24.2\">2021</span>\n<span id=\"S5.T3.1.1.1.24.24.3\"><span id=\"S5.T3.1.1.1.24.24.3.1\">ED</span></span>\n<span id=\"S5.T3.1.1.1.24.24.4\">Named Entity Recognition</span></span>\n<span id=\"S5.T3.1.1.1.25.25\">\n<span id=\"S5.T3.1.1.1.25.25.1\">LDET <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib150\" title=\"\">150</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.25.25.2\">2019</span>\n<span id=\"S5.T3.1.1.1.25.25.3\"><span id=\"S5.T3.1.1.1.25.25.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.25.25.4\">Entity Typing</span></span>\n<span id=\"S5.T3.1.1.1.26.26\">\n<span id=\"S5.T3.1.1.1.26.26.1\">BOX4Types <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib151\" title=\"\">151</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.26.26.2\">2021</span>\n<span id=\"S5.T3.1.1.1.26.26.3\"><span id=\"S5.T3.1.1.1.26.26.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.26.26.4\">Entity Typing</span></span>\n<span id=\"S5.T3.1.1.1.27.27\">\n<span id=\"S5.T3.1.1.1.27.27.1\">ELQ&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib152\" title=\"\">152</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.27.27.2\">2020</span>\n<span id=\"S5.T3.1.1.1.27.27.3\"><span id=\"S5.T3.1.1.1.27.27.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.27.27.4\">Entity Linking</span></span>\n<span id=\"S5.T3.1.1.1.28.28\">\n<span id=\"S5.T3.1.1.1.28.28.1\">ReFinED <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib153\" title=\"\">153</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.28.28.2\">2022</span>\n<span id=\"S5.T3.1.1.1.28.28.3\"><span id=\"S5.T3.1.1.1.28.28.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.28.28.4\">Entity Linking</span></span>\n<span id=\"S5.T3.1.1.1.29.29\">\n<span id=\"S5.T3.1.1.1.29.29.1\">BertCR <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib154\" title=\"\">154</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.29.29.2\">2019</span>\n<span id=\"S5.T3.1.1.1.29.29.3\"><span id=\"S5.T3.1.1.1.29.29.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.29.29.4\">CR (Within-document)</span></span>\n<span id=\"S5.T3.1.1.1.30.30\">\n<span id=\"S5.T3.1.1.1.30.30.1\">Spanbert <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib155\" title=\"\">155</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.30.30.2\">2020</span>\n<span id=\"S5.T3.1.1.1.30.30.3\"><span id=\"S5.T3.1.1.1.30.30.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.30.30.4\">CR (Within-document)</span></span>\n<span id=\"S5.T3.1.1.1.31.31\">\n<span id=\"S5.T3.1.1.1.31.31.1\">CDLM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib156\" title=\"\">156</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.31.31.2\">2021</span>\n<span id=\"S5.T3.1.1.1.31.31.3\"><span id=\"S5.T3.1.1.1.31.31.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.31.31.4\">CR (Cross-document)</span></span>\n<span id=\"S5.T3.1.1.1.32.32\">\n<span id=\"S5.T3.1.1.1.32.32.1\">CrossCR <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib157\" title=\"\">157</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.32.32.2\">2021</span>\n<span id=\"S5.T3.1.1.1.32.32.3\"><span id=\"S5.T3.1.1.1.32.32.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.32.32.4\">CR (Cross-document)</span></span>\n<span id=\"S5.T3.1.1.1.33.33\">\n<span id=\"S5.T3.1.1.1.33.33.1\">CR-RL <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib158\" title=\"\">158</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.33.33.2\">2021</span>\n<span id=\"S5.T3.1.1.1.33.33.3\"><span id=\"S5.T3.1.1.1.33.33.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.33.33.4\">CR (Cross-document)</span></span>\n<span id=\"S5.T3.1.1.1.34.34\">\n<span id=\"S5.T3.1.1.1.34.34.1\">SentRE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib159\" title=\"\">159</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.34.34.2\">2019</span>\n<span id=\"S5.T3.1.1.1.34.34.3\"><span id=\"S5.T3.1.1.1.34.34.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.34.34.4\">RE (Sentence-level)</span></span>\n<span id=\"S5.T3.1.1.1.35.35\">\n<span id=\"S5.T3.1.1.1.35.35.1\">Curriculum-RE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib160\" title=\"\">160</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.35.35.2\">2021</span>\n<span id=\"S5.T3.1.1.1.35.35.3\"><span id=\"S5.T3.1.1.1.35.35.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.35.35.4\">RE (Sentence-level)</span></span>\n<span id=\"S5.T3.1.1.1.36.36\">\n<span id=\"S5.T3.1.1.1.36.36.1\">DREEAM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib161\" title=\"\">161</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.36.36.2\">2023</span>\n<span id=\"S5.T3.1.1.1.36.36.3\"><span id=\"S5.T3.1.1.1.36.36.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.36.36.4\">RE (Document-level)</span></span>\n<span id=\"S5.T3.1.1.1.37.37\">\n<span id=\"S5.T3.1.1.1.37.37.1\">Kumar et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib95\" title=\"\">95</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.37.37.2\">2020</span>\n<span id=\"S5.T3.1.1.1.37.37.3\"><span id=\"S5.T3.1.1.1.37.37.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.37.37.4\">End-to-End Construction</span></span>\n<span id=\"S5.T3.1.1.1.38.38\">\n<span id=\"S5.T3.1.1.1.38.38.1\">Guo et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib162\" title=\"\">162</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.38.38.2\">2021</span>\n<span id=\"S5.T3.1.1.1.38.38.3\"><span id=\"S5.T3.1.1.1.38.38.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.38.38.4\">End-to-End Construction</span></span>\n<span id=\"S5.T3.1.1.1.39.39\">\n<span id=\"S5.T3.1.1.1.39.39.1\">Grapher <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib41\" title=\"\">41</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.39.39.2\">2021</span>\n<span id=\"S5.T3.1.1.1.39.39.3\"><span id=\"S5.T3.1.1.1.39.39.3.1\">ED</span></span>\n<span id=\"S5.T3.1.1.1.39.39.4\">End-to-End Construction</span></span>\n<span id=\"S5.T3.1.1.1.40.40\">\n<span id=\"S5.T3.1.1.1.40.40.1\">PiVE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib163\" title=\"\">163</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.40.40.2\">2023</span>\n<span id=\"S5.T3.1.1.1.40.40.3\"><span id=\"S5.T3.1.1.1.40.40.3.1\">D</span> + <span id=\"S5.T3.1.1.1.40.40.3.2\">ED</span></span>\n<span id=\"S5.T3.1.1.1.40.40.4\">End-to-End Construction</span></span>\n<span id=\"S5.T3.1.1.1.41.41\">\n<span id=\"S5.T3.1.1.1.41.41.1\"></span>\n<span id=\"S5.T3.1.1.1.41.41.2\">COMET <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib164\" title=\"\">164</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.41.41.3\">2019</span>\n<span id=\"S5.T3.1.1.1.41.41.4\"><span id=\"S5.T3.1.1.1.41.41.4.1\">D</span></span>\n<span id=\"S5.T3.1.1.1.41.41.5\">Distilling KGs from LLMs</span></span>\n<span id=\"S5.T3.1.1.1.42.42\">\n<span id=\"S5.T3.1.1.1.42.42.1\"></span>\n<span id=\"S5.T3.1.1.1.42.42.2\">BertNet <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib165\" title=\"\">165</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.42.42.3\">2022</span>\n<span id=\"S5.T3.1.1.1.42.42.4\"><span id=\"S5.T3.1.1.1.42.42.4.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.42.42.5\">Distilling KGs from LLMs</span></span>\n<span id=\"S5.T3.1.1.1.43.43\">\n<span id=\"S5.T3.1.1.1.43.43.1\"></span>\n<span id=\"S5.T3.1.1.1.43.43.2\">West et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib166\" title=\"\">166</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.43.43.3\">2022</span>\n<span id=\"S5.T3.1.1.1.43.43.4\"><span id=\"S5.T3.1.1.1.43.43.4.1\">D</span></span>\n<span id=\"S5.T3.1.1.1.43.43.5\">Distilling KGs from LLMs</span></span>\n<span id=\"S5.T3.1.1.1.44.44\">\n<span id=\"S5.T3.1.1.1.44.44.1\"><span id=\"S5.T3.1.1.1.44.44.1.1\">LLM-augmented KG-to-text Generation</span></span>\n<span id=\"S5.T3.1.1.1.44.44.2\">Ribeiro et al <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib167\" title=\"\">167</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.44.44.3\">2021</span>\n<span id=\"S5.T3.1.1.1.44.44.4\"><span id=\"S5.T3.1.1.1.44.44.4.1\">ED</span></span>\n<span id=\"S5.T3.1.1.1.44.44.5\">Leveraging Knowledge from LLMs</span></span>\n<span id=\"S5.T3.1.1.1.45.45\">\n<span id=\"S5.T3.1.1.1.45.45.1\">JointGT <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib42\" title=\"\">42</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.45.45.2\">2021</span>\n<span id=\"S5.T3.1.1.1.45.45.3\"><span id=\"S5.T3.1.1.1.45.45.3.1\">ED</span></span>\n<span id=\"S5.T3.1.1.1.45.45.4\">Leveraging Knowledge from LLMs</span></span>\n<span id=\"S5.T3.1.1.1.46.46\">\n<span id=\"S5.T3.1.1.1.46.46.1\">FSKG2Text <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib168\" title=\"\">168</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.46.46.2\">2021</span>\n<span id=\"S5.T3.1.1.1.46.46.3\"><span id=\"S5.T3.1.1.1.46.46.3.1\">D</span> + <span id=\"S5.T3.1.1.1.46.46.3.2\">ED</span></span>\n<span id=\"S5.T3.1.1.1.46.46.4\">Leveraging Knowledge from LLMs</span></span>\n<span id=\"S5.T3.1.1.1.47.47\">\n<span id=\"S5.T3.1.1.1.47.47.1\">GAP <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib169\" title=\"\">169</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.47.47.2\">2022</span>\n<span id=\"S5.T3.1.1.1.47.47.3\"><span id=\"S5.T3.1.1.1.47.47.3.1\">ED</span></span>\n<span id=\"S5.T3.1.1.1.47.47.4\">Leveraging Knowledge from LLMs</span></span>\n<span id=\"S5.T3.1.1.1.48.48\">\n<span id=\"S5.T3.1.1.1.48.48.1\">GenWiki <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib170\" title=\"\">170</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.48.48.2\">2020</span>\n<span id=\"S5.T3.1.1.1.48.48.3\">-</span>\n<span id=\"S5.T3.1.1.1.48.48.4\">Constructing KG-text aligned Corpus</span></span>\n<span id=\"S5.T3.1.1.1.49.49\">\n<span id=\"S5.T3.1.1.1.49.49.1\">KGPT <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib171\" title=\"\">171</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.49.49.2\">2020</span>\n<span id=\"S5.T3.1.1.1.49.49.3\"><span id=\"S5.T3.1.1.1.49.49.3.1\">ED</span></span>\n<span id=\"S5.T3.1.1.1.49.49.4\">Constructing KG-text aligned Corpus</span></span>\n<span id=\"S5.T3.1.1.1.50.50\">\n<span id=\"S5.T3.1.1.1.50.50.1\"><span id=\"S5.T3.1.1.1.50.50.1.1\">LLM-augmented KGQA</span></span>\n<span id=\"S5.T3.1.1.1.50.50.2\">Lukovnikov et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib172\" title=\"\">172</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.50.50.3\">2019</span>\n<span id=\"S5.T3.1.1.1.50.50.4\"><span id=\"S5.T3.1.1.1.50.50.4.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.50.50.5\">Entity/Relation Extractor</span></span>\n<span id=\"S5.T3.1.1.1.51.51\">\n<span id=\"S5.T3.1.1.1.51.51.1\">Luo et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib173\" title=\"\">173</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.51.51.2\">2020</span>\n<span id=\"S5.T3.1.1.1.51.51.3\"><span id=\"S5.T3.1.1.1.51.51.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.51.51.4\">Entity/Relation Extractor</span></span>\n<span id=\"S5.T3.1.1.1.52.52\">\n<span id=\"S5.T3.1.1.1.52.52.1\">QA-GNN <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib131\" title=\"\">131</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.52.52.2\">2021</span>\n<span id=\"S5.T3.1.1.1.52.52.3\"><span id=\"S5.T3.1.1.1.52.52.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.52.52.4\">Entity/Relation Extractor</span></span>\n<span id=\"S5.T3.1.1.1.53.53\">\n<span id=\"S5.T3.1.1.1.53.53.1\">Nan et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib174\" title=\"\">174</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.53.53.2\">2023</span>\n<span id=\"S5.T3.1.1.1.53.53.3\"><span id=\"S5.T3.1.1.1.53.53.3.1\">E</span> + <span id=\"S5.T3.1.1.1.53.53.3.2\">D</span> + <span id=\"S5.T3.1.1.1.53.53.3.3\">ED</span></span>\n<span id=\"S5.T3.1.1.1.53.53.4\">Entity/Relation Extractor</span></span>\n<span id=\"S5.T3.1.1.1.54.54\">\n<span id=\"S5.T3.1.1.1.54.54.1\">DEKCOR <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib175\" title=\"\">175</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.54.54.2\">2021</span>\n<span id=\"S5.T3.1.1.1.54.54.3\"><span id=\"S5.T3.1.1.1.54.54.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.54.54.4\">Answer Reasoner</span></span>\n<span id=\"S5.T3.1.1.1.55.55\">\n<span id=\"S5.T3.1.1.1.55.55.1\">DRLK <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib176\" title=\"\">176</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.55.55.2\">2022</span>\n<span id=\"S5.T3.1.1.1.55.55.3\"><span id=\"S5.T3.1.1.1.55.55.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.55.55.4\">Answer Reasoner</span></span>\n<span id=\"S5.T3.1.1.1.56.56\">\n<span id=\"S5.T3.1.1.1.56.56.1\">OreoLM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib177\" title=\"\">177</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.56.56.2\">2022</span>\n<span id=\"S5.T3.1.1.1.56.56.3\"><span id=\"S5.T3.1.1.1.56.56.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.56.56.4\">Answer Reasoner</span></span>\n<span id=\"S5.T3.1.1.1.57.57\">\n<span id=\"S5.T3.1.1.1.57.57.1\">GreaseLM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib178\" title=\"\">178</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.57.57.2\">2022</span>\n<span id=\"S5.T3.1.1.1.57.57.3\"><span id=\"S5.T3.1.1.1.57.57.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.57.57.4\">Answer Reasoner</span></span>\n<span id=\"S5.T3.1.1.1.58.58\">\n<span id=\"S5.T3.1.1.1.58.58.1\">ReLMKG <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib179\" title=\"\">179</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.58.58.2\">2022</span>\n<span id=\"S5.T3.1.1.1.58.58.3\"><span id=\"S5.T3.1.1.1.58.58.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.58.58.4\">Answer Reasoner</span></span>\n<span id=\"S5.T3.1.1.1.59.59\">\n<span id=\"S5.T3.1.1.1.59.59.1\">UniKGQA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib43\" title=\"\">43</a>]</cite></span>\n<span id=\"S5.T3.1.1.1.59.59.2\">2023</span>\n<span id=\"S5.T3.1.1.1.59.59.3\"><span id=\"S5.T3.1.1.1.59.59.3.1\">E</span></span>\n<span id=\"S5.T3.1.1.1.59.59.4\">Answer Reasoner</span></span>\n</span>\n</span></p>\n<ul id=\"S5.I1\">\n<li id=\"S5.I1.i1\">\n<span>•</span>\n<p id=\"S5.I1.i1.p1.1\"><span id=\"S5.I1.i1.p1.1.1\">E</span>: Encoder-only LLMs, <span id=\"S5.I1.i1.p1.1.2\">D</span>: Decoder-only LLMs, <span id=\"S5.I1.i1.p1.1.3\">ED</span>: Encoder-decoder LLMs.</p>\n</li>\n</ul>\n</span></p>\n</figure>\n<section id=\"S5.SS1\">\n<h3>\n<span>5.1 </span><span id=\"S5.SS1.1.1\">LLM-augmented KG Embedding</span>\n</h3>\n<p id=\"S5.SS1.p1.1\">Knowledge graph embedding (KGE) aims to map each entity and relation into a low-dimensional vector (embedding) space. These embeddings contain both semantic and structural information of KGs, which can be utilized for various tasks such as question answering <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib180\" title=\"\">180</a>]</cite>, reasoning <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib38\" title=\"\">38</a>]</cite>, and recommendation <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib181\" title=\"\">181</a>]</cite>. Conventional knowledge graph embedding methods mainly rely on the structural information of KGs to optimize a scoring function defined on embeddings (e.g., TransE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib33\" title=\"\">33</a>]</cite>, and DisMult <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib182\" title=\"\">182</a>]</cite>). However, these approaches often fall short in representing unseen entities and long-tailed relations due to their limited structural connectivity <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib183\" title=\"\">183</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib184\" title=\"\">184</a>]</cite>. To address this issue, as shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S5.F14\" title=\"Figure 14 ‣ 5.1 LLM-augmented KG Embedding ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>14</span></a>, recent research adopts LLMs to enrich representations of KGs by encoding the textual descriptions of entities and relations <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib94\" title=\"\">94</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite>.</p>\n<figure id=\"S5.F14\"><img alt=\"Refer to caption\" height=\"563\" id=\"S5.F14.g1\" src=\"x11.png\" width=\"581\">\n<figcaption><span>Figure 14: </span>LLMs as text encoder for knowledge graph embedding (KGE).</figcaption>\n</figure>\n<section id=\"S5.SS1.SSS1\">\n<h4>\n<span>5.1.1 </span>LLMs as Text Encoders</h4>\n<div id=\"S5.SS1.SSS1.p1\">\n<p id=\"S5.SS1.SSS1.p1.4\">Pretrain-KGE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib94\" title=\"\">94</a>]</cite> is a representative method that follows the framework shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S5.F14\" title=\"Figure 14 ‣ 5.1 LLM-augmented KG Embedding ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>14</span></a>. Given a triple <math alttext=\"(h,r,t)\" display=\"inline\" id=\"S5.SS1.SSS1.p1.1.m1.3\"><semantics id=\"S5.SS1.SSS1.p1.1.m1.3a\"><mrow id=\"S5.SS1.SSS1.p1.1.m1.3.4.2\" xref=\"S5.SS1.SSS1.p1.1.m1.3.4.1.cmml\"><mo id=\"S5.SS1.SSS1.p1.1.m1.3.4.2.1\" stretchy=\"false\" xref=\"S5.SS1.SSS1.p1.1.m1.3.4.1.cmml\">(</mo><mi id=\"S5.SS1.SSS1.p1.1.m1.1.1\" xref=\"S5.SS1.SSS1.p1.1.m1.1.1.cmml\">h</mi><mo id=\"S5.SS1.SSS1.p1.1.m1.3.4.2.2\" xref=\"S5.SS1.SSS1.p1.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS1.SSS1.p1.1.m1.2.2\" xref=\"S5.SS1.SSS1.p1.1.m1.2.2.cmml\">r</mi><mo id=\"S5.SS1.SSS1.p1.1.m1.3.4.2.3\" xref=\"S5.SS1.SSS1.p1.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS1.SSS1.p1.1.m1.3.3\" xref=\"S5.SS1.SSS1.p1.1.m1.3.3.cmml\">t</mi><mo id=\"S5.SS1.SSS1.p1.1.m1.3.4.2.4\" stretchy=\"false\" xref=\"S5.SS1.SSS1.p1.1.m1.3.4.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.1.m1.3b\"><vector id=\"S5.SS1.SSS1.p1.1.m1.3.4.1.cmml\" xref=\"S5.SS1.SSS1.p1.1.m1.3.4.2\"><ci id=\"S5.SS1.SSS1.p1.1.m1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.1.m1.1.1\">ℎ</ci><ci id=\"S5.SS1.SSS1.p1.1.m1.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.1.m1.2.2\">𝑟</ci><ci id=\"S5.SS1.SSS1.p1.1.m1.3.3.cmml\" xref=\"S5.SS1.SSS1.p1.1.m1.3.3\">𝑡</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.1.m1.3c\">(h,r,t)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.1.m1.3d\">( italic_h , italic_r , italic_t )</annotation></semantics></math> from KGs, it firsts uses a LLM encoder to encode the textual descriptions of entities <math alttext=\"h\" display=\"inline\" id=\"S5.SS1.SSS1.p1.2.m2.1\"><semantics id=\"S5.SS1.SSS1.p1.2.m2.1a\"><mi id=\"S5.SS1.SSS1.p1.2.m2.1.1\" xref=\"S5.SS1.SSS1.p1.2.m2.1.1.cmml\">h</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.2.m2.1b\"><ci id=\"S5.SS1.SSS1.p1.2.m2.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.2.m2.1.1\">ℎ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.2.m2.1c\">h</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.2.m2.1d\">italic_h</annotation></semantics></math>, <math alttext=\"t\" display=\"inline\" id=\"S5.SS1.SSS1.p1.3.m3.1\"><semantics id=\"S5.SS1.SSS1.p1.3.m3.1a\"><mi id=\"S5.SS1.SSS1.p1.3.m3.1.1\" xref=\"S5.SS1.SSS1.p1.3.m3.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.3.m3.1b\"><ci id=\"S5.SS1.SSS1.p1.3.m3.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.3.m3.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.3.m3.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.3.m3.1d\">italic_t</annotation></semantics></math>, and relations <math alttext=\"r\" display=\"inline\" id=\"S5.SS1.SSS1.p1.4.m4.1\"><semantics id=\"S5.SS1.SSS1.p1.4.m4.1a\"><mi id=\"S5.SS1.SSS1.p1.4.m4.1.1\" xref=\"S5.SS1.SSS1.p1.4.m4.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.4.m4.1b\"><ci id=\"S5.SS1.SSS1.p1.4.m4.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.4.m4.1.1\">𝑟</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.4.m4.1c\">r</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.4.m4.1d\">italic_r</annotation></semantics></math> into representations as</p>\n<table id=\"A1.EGx1\">\n<tbody id=\"S5.E1\"><tr>\n<td></td>\n<td><math alttext=\"\\displaystyle e_{h}=\\text{LLM}(\\text{Text}_{h}),e_{t}=\\text{LLM}(\\text{Text}_{%\nt}),e_{r}=\\text{LLM}(\\text{Text}_{r}),\" display=\"block\" id=\"S5.E1.m1.1\"><semantics id=\"S5.E1.m1.1a\"><mrow id=\"S5.E1.m1.1.1.1\"><mrow id=\"S5.E1.m1.1.1.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.3.cmml\"><mrow id=\"S5.E1.m1.1.1.1.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.1.1.cmml\"><msub id=\"S5.E1.m1.1.1.1.1.1.1.3\" xref=\"S5.E1.m1.1.1.1.1.1.1.3.cmml\"><mi id=\"S5.E1.m1.1.1.1.1.1.1.3.2\" xref=\"S5.E1.m1.1.1.1.1.1.1.3.2.cmml\">e</mi><mi id=\"S5.E1.m1.1.1.1.1.1.1.3.3\" xref=\"S5.E1.m1.1.1.1.1.1.1.3.3.cmml\">h</mi></msub><mo id=\"S5.E1.m1.1.1.1.1.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.1.1.2.cmml\">=</mo><mrow id=\"S5.E1.m1.1.1.1.1.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.cmml\"><mtext id=\"S5.E1.m1.1.1.1.1.1.1.1.3\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.3a.cmml\">LLM</mtext><mo id=\"S5.E1.m1.1.1.1.1.1.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml\">(</mo><msub id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml\"><mtext id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.2a.cmml\">Text</mtext><mi id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.3\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml\">h</mi></msub><mo id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E1.m1.1.1.1.1.2.3\" xref=\"S5.E1.m1.1.1.1.1.3a.cmml\">,</mo><mrow id=\"S5.E1.m1.1.1.1.1.2.2.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.3.cmml\"><mrow id=\"S5.E1.m1.1.1.1.1.2.2.1.1\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.cmml\"><msub id=\"S5.E1.m1.1.1.1.1.2.2.1.1.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.cmml\"><mi id=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.2.cmml\">e</mi><mi id=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.3.cmml\">t</mi></msub><mo id=\"S5.E1.m1.1.1.1.1.2.2.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.2.cmml\">=</mo><mrow id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.cmml\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.3a.cmml\">LLM</mtext><mo id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml\"><mo id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml\">(</mo><msub id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.2a.cmml\">Text</mtext><mi id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.3.cmml\">t</mi></msub><mo id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E1.m1.1.1.1.1.2.2.2.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.3a.cmml\">,</mo><mrow id=\"S5.E1.m1.1.1.1.1.2.2.2.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.cmml\"><msub id=\"S5.E1.m1.1.1.1.1.2.2.2.2.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.cmml\"><mi id=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.2.cmml\">e</mi><mi id=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.3.cmml\">r</mi></msub><mo id=\"S5.E1.m1.1.1.1.1.2.2.2.2.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.2.cmml\">=</mo><mrow id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.cmml\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.3a.cmml\">LLM</mtext><mo id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.2.cmml\">⁢</mo><mrow id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.cmml\"><mo id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.2\" stretchy=\"false\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.cmml\">(</mo><msub id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.cmml\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.2\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.2a.cmml\">Text</mtext><mi id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.3\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.3.cmml\">r</mi></msub><mo id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.3\" stretchy=\"false\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow></mrow></mrow><mo id=\"S5.E1.m1.1.1.1.2\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E1.m1.1b\"><apply id=\"S5.E1.m1.1.1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.3a.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.3\">formulae-sequence</csymbol><apply id=\"S5.E1.m1.1.1.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1\"><eq id=\"S5.E1.m1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.2\"></eq><apply id=\"S5.E1.m1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.1.1.3.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S5.E1.m1.1.1.1.1.1.1.3.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.3.2\">𝑒</ci><ci id=\"S5.E1.m1.1.1.1.1.1.1.3.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.3.3\">ℎ</ci></apply><apply id=\"S5.E1.m1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1\"><times id=\"S5.E1.m1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.2\"></times><ci id=\"S5.E1.m1.1.1.1.1.1.1.1.3a.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.3\"><mtext id=\"S5.E1.m1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.3\">LLM</mtext></ci><apply id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.2a.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.2\"><mtext id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.2\">Text</mtext></ci><ci id=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.1.1.1.1.1.3\">ℎ</ci></apply></apply></apply><apply id=\"S5.E1.m1.1.1.1.1.2.2.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.2.2.3a.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.3\">formulae-sequence</csymbol><apply id=\"S5.E1.m1.1.1.1.1.2.2.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1\"><eq id=\"S5.E1.m1.1.1.1.1.2.2.1.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.2\"></eq><apply id=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.3\">subscript</csymbol><ci id=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.2\">𝑒</ci><ci id=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.3.3\">𝑡</ci></apply><apply id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1\"><times id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.2\"></times><ci id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.3a.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.3\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.3\">LLM</mtext></ci><apply id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1\">subscript</csymbol><ci id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.2a.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.2\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.2\">Text</mtext></ci><ci id=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.1.1.1.1.1.1.3\">𝑡</ci></apply></apply></apply><apply id=\"S5.E1.m1.1.1.1.1.2.2.2.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2\"><eq id=\"S5.E1.m1.1.1.1.1.2.2.2.2.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.2\"></eq><apply id=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.3\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.3\">subscript</csymbol><ci id=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.2\">𝑒</ci><ci id=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.3.3\">𝑟</ci></apply><apply id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1\"><times id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.2\"></times><ci id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.3a.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.3\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.3\">LLM</mtext></ci><apply id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1\">subscript</csymbol><ci id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.2a.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.2\"><mtext id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.2.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.2\">Text</mtext></ci><ci id=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.1.1.2.2.2.2.1.1.1.1.3\">𝑟</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E1.m1.1c\">\\displaystyle e_{h}=\\text{LLM}(\\text{Text}_{h}),e_{t}=\\text{LLM}(\\text{Text}_{%\nt}),e_{r}=\\text{LLM}(\\text{Text}_{r}),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E1.m1.1d\">italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = LLM ( Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = LLM ( Text start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , italic_e start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = LLM ( Text start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(1)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS1.SSS1.p1.11\">where <math alttext=\"e_{h},e_{r},\" display=\"inline\" id=\"S5.SS1.SSS1.p1.5.m1.1\"><semantics id=\"S5.SS1.SSS1.p1.5.m1.1a\"><mrow id=\"S5.SS1.SSS1.p1.5.m1.1.1.1\"><mrow id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.3.cmml\"><msub id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.cmml\"><mi id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.2\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.2.cmml\">e</mi><mi id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.3\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.3.cmml\">h</mi></msub><mo id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.3\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.3.cmml\">,</mo><msub id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.cmml\"><mi id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.2\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.2.cmml\">e</mi><mi id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.3\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.3.cmml\">r</mi></msub></mrow><mo id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.2\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.5.m1.1b\"><list id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.3.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2\"><apply id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.2.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.2\">𝑒</ci><ci id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.3.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.1.1.3\">ℎ</ci></apply><apply id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.1.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2\">subscript</csymbol><ci id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.2\">𝑒</ci><ci id=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.5.m1.1.1.1.1.2.2.3\">𝑟</ci></apply></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.5.m1.1c\">e_{h},e_{r},</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.5.m1.1d\">italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ,</annotation></semantics></math> and <math alttext=\"e_{t}\" display=\"inline\" id=\"S5.SS1.SSS1.p1.6.m2.1\"><semantics id=\"S5.SS1.SSS1.p1.6.m2.1a\"><msub id=\"S5.SS1.SSS1.p1.6.m2.1.1\" xref=\"S5.SS1.SSS1.p1.6.m2.1.1.cmml\"><mi id=\"S5.SS1.SSS1.p1.6.m2.1.1.2\" xref=\"S5.SS1.SSS1.p1.6.m2.1.1.2.cmml\">e</mi><mi id=\"S5.SS1.SSS1.p1.6.m2.1.1.3\" xref=\"S5.SS1.SSS1.p1.6.m2.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.6.m2.1b\"><apply id=\"S5.SS1.SSS1.p1.6.m2.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.6.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.6.m2.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.6.m2.1.1\">subscript</csymbol><ci id=\"S5.SS1.SSS1.p1.6.m2.1.1.2.cmml\" xref=\"S5.SS1.SSS1.p1.6.m2.1.1.2\">𝑒</ci><ci id=\"S5.SS1.SSS1.p1.6.m2.1.1.3.cmml\" xref=\"S5.SS1.SSS1.p1.6.m2.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.6.m2.1c\">e_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.6.m2.1d\">italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> denotes the initial embeddings of entities <math alttext=\"h\" display=\"inline\" id=\"S5.SS1.SSS1.p1.7.m3.1\"><semantics id=\"S5.SS1.SSS1.p1.7.m3.1a\"><mi id=\"S5.SS1.SSS1.p1.7.m3.1.1\" xref=\"S5.SS1.SSS1.p1.7.m3.1.1.cmml\">h</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.7.m3.1b\"><ci id=\"S5.SS1.SSS1.p1.7.m3.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.7.m3.1.1\">ℎ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.7.m3.1c\">h</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.7.m3.1d\">italic_h</annotation></semantics></math>, <math alttext=\"t\" display=\"inline\" id=\"S5.SS1.SSS1.p1.8.m4.1\"><semantics id=\"S5.SS1.SSS1.p1.8.m4.1a\"><mi id=\"S5.SS1.SSS1.p1.8.m4.1.1\" xref=\"S5.SS1.SSS1.p1.8.m4.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.8.m4.1b\"><ci id=\"S5.SS1.SSS1.p1.8.m4.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.8.m4.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.8.m4.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.8.m4.1d\">italic_t</annotation></semantics></math>, and relations <math alttext=\"r\" display=\"inline\" id=\"S5.SS1.SSS1.p1.9.m5.1\"><semantics id=\"S5.SS1.SSS1.p1.9.m5.1a\"><mi id=\"S5.SS1.SSS1.p1.9.m5.1.1\" xref=\"S5.SS1.SSS1.p1.9.m5.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.9.m5.1b\"><ci id=\"S5.SS1.SSS1.p1.9.m5.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.9.m5.1.1\">𝑟</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.9.m5.1c\">r</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.9.m5.1d\">italic_r</annotation></semantics></math>, respectively. Pretrain-KGE uses the BERT as the LLM encoder in experiments. Then, the initial embeddings are fed into a KGE model to generate the final embeddings <math alttext=\"v_{h},v_{r}\" display=\"inline\" id=\"S5.SS1.SSS1.p1.10.m6.2\"><semantics id=\"S5.SS1.SSS1.p1.10.m6.2a\"><mrow id=\"S5.SS1.SSS1.p1.10.m6.2.2.2\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.3.cmml\"><msub id=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1\" xref=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.cmml\"><mi id=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.2\" xref=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.2.cmml\">v</mi><mi id=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.3\" xref=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.3.cmml\">h</mi></msub><mo id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.3\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.3.cmml\">,</mo><msub id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.cmml\"><mi id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.2\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.2.cmml\">v</mi><mi id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.3\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.3.cmml\">r</mi></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.10.m6.2b\"><list id=\"S5.SS1.SSS1.p1.10.m6.2.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2\"><apply id=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1\">subscript</csymbol><ci id=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.2.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.2\">𝑣</ci><ci id=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.3.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.1.1.1.1.3\">ℎ</ci></apply><apply id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.1.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2\">subscript</csymbol><ci id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.2\">𝑣</ci><ci id=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.10.m6.2.2.2.2.3\">𝑟</ci></apply></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.10.m6.2c\">v_{h},v_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.10.m6.2d\">italic_v start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext=\"v_{t}\" display=\"inline\" id=\"S5.SS1.SSS1.p1.11.m7.1\"><semantics id=\"S5.SS1.SSS1.p1.11.m7.1a\"><msub id=\"S5.SS1.SSS1.p1.11.m7.1.1\" xref=\"S5.SS1.SSS1.p1.11.m7.1.1.cmml\"><mi id=\"S5.SS1.SSS1.p1.11.m7.1.1.2\" xref=\"S5.SS1.SSS1.p1.11.m7.1.1.2.cmml\">v</mi><mi id=\"S5.SS1.SSS1.p1.11.m7.1.1.3\" xref=\"S5.SS1.SSS1.p1.11.m7.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.11.m7.1b\"><apply id=\"S5.SS1.SSS1.p1.11.m7.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.11.m7.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.11.m7.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.11.m7.1.1\">subscript</csymbol><ci id=\"S5.SS1.SSS1.p1.11.m7.1.1.2.cmml\" xref=\"S5.SS1.SSS1.p1.11.m7.1.1.2\">𝑣</ci><ci id=\"S5.SS1.SSS1.p1.11.m7.1.1.3.cmml\" xref=\"S5.SS1.SSS1.p1.11.m7.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.11.m7.1c\">v_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.11.m7.1d\">italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. During the KGE training phase, they optimize the KGE model by following the standard KGE loss function as\n</p>\n<table id=\"S5.E2\">\n<tbody><tr>\n<td></td>\n<td><math alttext=\"\\mathcal{L}=[\\gamma+f(v_{h},v_{r},v_{t})-f(v^{\\prime}_{h},v^{\\prime}_{r},v^{%\n\\prime}_{t})],\" display=\"block\" id=\"S5.E2.m1.1\"><semantics id=\"S5.E2.m1.1a\"><mrow id=\"S5.E2.m1.1.1.1\" xref=\"S5.E2.m1.1.1.1.1.cmml\"><mrow id=\"S5.E2.m1.1.1.1.1\" xref=\"S5.E2.m1.1.1.1.1.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.3\" xref=\"S5.E2.m1.1.1.1.1.3.cmml\">ℒ</mi><mo id=\"S5.E2.m1.1.1.1.1.2\" xref=\"S5.E2.m1.1.1.1.1.2.cmml\">=</mo><mrow id=\"S5.E2.m1.1.1.1.1.1.1\" xref=\"S5.E2.m1.1.1.1.1.1.2.cmml\"><mo id=\"S5.E2.m1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E2.m1.1.1.1.1.1.2.1.cmml\">[</mo><mrow id=\"S5.E2.m1.1.1.1.1.1.1.1\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.cmml\"><mrow id=\"S5.E2.m1.1.1.1.1.1.1.1.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.3.5\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.5.cmml\">γ</mi><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.3.4\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.4.cmml\">+</mo><mrow id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.5\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.5.cmml\">f</mi><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.4\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.4.cmml\">⁢</mo><mrow id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.4.cmml\"><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.4\" stretchy=\"false\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.4.cmml\">(</mo><msub id=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml\">v</mi><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml\">h</mi></msub><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.5\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.4.cmml\">,</mo><msub id=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml\">v</mi><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml\">r</mi></msub><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.6\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.4.cmml\">,</mo><msub id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.2.cmml\">v</mi><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.3.cmml\">t</mi></msub><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.7\" stretchy=\"false\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.4.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.7\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.7.cmml\">−</mo><mrow id=\"S5.E2.m1.1.1.1.1.1.1.1.6\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.6.5\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.5.cmml\">f</mi><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.6.4\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.4.cmml\">⁢</mo><mrow id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.4.cmml\"><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.4\" stretchy=\"false\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.4.cmml\">(</mo><msubsup id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.2.cmml\">v</mi><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.3.cmml\">h</mi><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.3.cmml\">′</mo></msubsup><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.5\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.4.cmml\">,</mo><msubsup id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.2.cmml\">v</mi><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.3.cmml\">r</mi><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.3.cmml\">′</mo></msubsup><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.6\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.4.cmml\">,</mo><msubsup id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.cmml\"><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.2\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.2.cmml\">v</mi><mi id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.3.cmml\">t</mi><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.3\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.3.cmml\">′</mo></msubsup><mo id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.7\" stretchy=\"false\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.4.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E2.m1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E2.m1.1.1.1.1.1.2.1.cmml\">]</mo></mrow></mrow><mo id=\"S5.E2.m1.1.1.1.2\" xref=\"S5.E2.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E2.m1.1b\"><apply id=\"S5.E2.m1.1.1.1.1.cmml\" xref=\"S5.E2.m1.1.1.1\"><eq id=\"S5.E2.m1.1.1.1.1.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.2\"></eq><ci id=\"S5.E2.m1.1.1.1.1.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.3\">ℒ</ci><apply id=\"S5.E2.m1.1.1.1.1.1.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1\"><csymbol cd=\"latexml\" id=\"S5.E2.m1.1.1.1.1.1.2.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.2\">delimited-[]</csymbol><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1\"><minus id=\"S5.E2.m1.1.1.1.1.1.1.1.7.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.7\"></minus><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3\"><plus id=\"S5.E2.m1.1.1.1.1.1.1.1.3.4.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.4\"></plus><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.3.5.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.5\">𝛾</ci><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3\"><times id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.4.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.4\"></times><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.5.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.5\">𝑓</ci><vector id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.4.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3\"><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2\">𝑣</ci><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3\">ℎ</ci></apply><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2\">subscript</csymbol><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.2\">𝑣</ci><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.2.2.2.2.2.3\">𝑟</ci></apply><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3\">subscript</csymbol><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.2\">𝑣</ci><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.3.3.3.3.3.3\">𝑡</ci></apply></vector></apply></apply><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.6.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6\"><times id=\"S5.E2.m1.1.1.1.1.1.1.1.6.4.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.4\"></times><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.6.5.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.5\">𝑓</ci><vector id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.4.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3\"><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1\">subscript</csymbol><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1\">superscript</csymbol><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.2\">𝑣</ci><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.2.3\">′</ci></apply><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.4.1.1.1.3\">ℎ</ci></apply><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2\">subscript</csymbol><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2\">superscript</csymbol><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.2\">𝑣</ci><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.2.3\">′</ci></apply><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.5.2.2.2.3\">𝑟</ci></apply><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3\">subscript</csymbol><apply id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3\">superscript</csymbol><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.2.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.2\">𝑣</ci><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.2.3\">′</ci></apply><ci id=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.3.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.1.1.6.3.3.3.3\">𝑡</ci></apply></vector></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E2.m1.1c\">\\mathcal{L}=[\\gamma+f(v_{h},v_{r},v_{t})-f(v^{\\prime}_{h},v^{\\prime}_{r},v^{%\n\\prime}_{t})],</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E2.m1.1d\">caligraphic_L = [ italic_γ + italic_f ( italic_v start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - italic_f ( italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ] ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(2)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS1.SSS1.p1.15\">where <math alttext=\"f\" display=\"inline\" id=\"S5.SS1.SSS1.p1.12.m1.1\"><semantics id=\"S5.SS1.SSS1.p1.12.m1.1a\"><mi id=\"S5.SS1.SSS1.p1.12.m1.1.1\" xref=\"S5.SS1.SSS1.p1.12.m1.1.1.cmml\">f</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.12.m1.1b\"><ci id=\"S5.SS1.SSS1.p1.12.m1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.12.m1.1.1\">𝑓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.12.m1.1c\">f</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.12.m1.1d\">italic_f</annotation></semantics></math> is the KGE scoring function, <math alttext=\"\\gamma\" display=\"inline\" id=\"S5.SS1.SSS1.p1.13.m2.1\"><semantics id=\"S5.SS1.SSS1.p1.13.m2.1a\"><mi id=\"S5.SS1.SSS1.p1.13.m2.1.1\" xref=\"S5.SS1.SSS1.p1.13.m2.1.1.cmml\">γ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.13.m2.1b\"><ci id=\"S5.SS1.SSS1.p1.13.m2.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.13.m2.1.1\">𝛾</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.13.m2.1c\">\\gamma</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.13.m2.1d\">italic_γ</annotation></semantics></math> is a margin hyperparameter, and <math alttext=\"v^{\\prime}_{h},v^{\\prime}_{r}\" display=\"inline\" id=\"S5.SS1.SSS1.p1.14.m3.2\"><semantics id=\"S5.SS1.SSS1.p1.14.m3.2a\"><mrow id=\"S5.SS1.SSS1.p1.14.m3.2.2.2\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.3.cmml\"><msubsup id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.cmml\"><mi id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.2\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.2.cmml\">v</mi><mi id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.3\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.3.cmml\">h</mi><mo id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.3\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.3.cmml\">′</mo></msubsup><mo id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.3\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.3.cmml\">,</mo><msubsup id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.cmml\"><mi id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.2\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.2.cmml\">v</mi><mi id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.3\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.3.cmml\">r</mi><mo id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.3\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.3.cmml\">′</mo></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.14.m3.2b\"><list id=\"S5.SS1.SSS1.p1.14.m3.2.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2\"><apply id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1\">subscript</csymbol><apply id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.1.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1\">superscript</csymbol><ci id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.2\">𝑣</ci><ci id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.2.3\">′</ci></apply><ci id=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.3.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.1.1.1.1.3\">ℎ</ci></apply><apply id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.1.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2\">subscript</csymbol><apply id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.1.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2\">superscript</csymbol><ci id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.2\">𝑣</ci><ci id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.2.3\">′</ci></apply><ci id=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.14.m3.2.2.2.2.3\">𝑟</ci></apply></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.14.m3.2c\">v^{\\prime}_{h},v^{\\prime}_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.14.m3.2d\">italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext=\"v^{\\prime}_{t}\" display=\"inline\" id=\"S5.SS1.SSS1.p1.15.m4.1\"><semantics id=\"S5.SS1.SSS1.p1.15.m4.1a\"><msubsup id=\"S5.SS1.SSS1.p1.15.m4.1.1\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1.cmml\"><mi id=\"S5.SS1.SSS1.p1.15.m4.1.1.2.2\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1.2.2.cmml\">v</mi><mi id=\"S5.SS1.SSS1.p1.15.m4.1.1.3\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1.3.cmml\">t</mi><mo id=\"S5.SS1.SSS1.p1.15.m4.1.1.2.3\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1.2.3.cmml\">′</mo></msubsup><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS1.p1.15.m4.1b\"><apply id=\"S5.SS1.SSS1.p1.15.m4.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.15.m4.1.1.1.cmml\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1\">subscript</csymbol><apply id=\"S5.SS1.SSS1.p1.15.m4.1.1.2.cmml\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.SSS1.p1.15.m4.1.1.2.1.cmml\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1\">superscript</csymbol><ci id=\"S5.SS1.SSS1.p1.15.m4.1.1.2.2.cmml\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1.2.2\">𝑣</ci><ci id=\"S5.SS1.SSS1.p1.15.m4.1.1.2.3.cmml\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1.2.3\">′</ci></apply><ci id=\"S5.SS1.SSS1.p1.15.m4.1.1.3.cmml\" xref=\"S5.SS1.SSS1.p1.15.m4.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS1.p1.15.m4.1c\">v^{\\prime}_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS1.p1.15.m4.1d\">italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> are the negative samples. In this way, the KGE model could learn adequate structure information, while reserving partial knowledge from LLM enabling better knowledge graph embedding. KEPLER <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite> offers a unified model for knowledge embedding and pre-trained language representation. This model not only generates effective text-enhanced knowledge embedding using powerful LLMs but also seamlessly integrates factual knowledge into LLMs. Nayyeri et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib132\" title=\"\">132</a>]</cite> use LLMs to generate the world-level, sentence-level, and document-level representations. They are integrated with graph structure embeddings into a unified vector by Dihedron and Quaternion representations of 4D hypercomplex numbers. Huang et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib133\" title=\"\">133</a>]</cite> combine LLMs with other vision and graph encoders to learn multi-modal knowledge graph embedding that enhances the performance of downstream tasks. CoDEx <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib134\" title=\"\">134</a>]</cite> presents a novel loss function empowered by LLMs that guides the KGE models in measuring the likelihood of triples by considering the textual information. The proposed loss function is agnostic to model structure that can be incorporated with any KGE model.</p>\n</div>\n</section>\n<section id=\"S5.SS1.SSS2\">\n<h4>\n<span>5.1.2 </span>LLMs for Joint Text and KG Embedding</h4>\n<div id=\"S5.SS1.SSS2.p1\">\n<p id=\"S5.SS1.SSS2.p1.3\">Instead of using KGE model to consider graph structure, another line of methods directly employs LLMs to incorporate both the graph structure and textual information into the embedding space simultaneously. As shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S5.F15\" title=\"Figure 15 ‣ 5.1.2 LLMs for Joint Text and KG Embedding ‣ 5.1 LLM-augmented KG Embedding ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>15</span></a>, <math alttext=\"k\" display=\"inline\" id=\"S5.SS1.SSS2.p1.1.m1.1\"><semantics id=\"S5.SS1.SSS2.p1.1.m1.1a\"><mi id=\"S5.SS1.SSS2.p1.1.m1.1.1\" xref=\"S5.SS1.SSS2.p1.1.m1.1.1.cmml\">k</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS2.p1.1.m1.1b\"><ci id=\"S5.SS1.SSS2.p1.1.m1.1.1.cmml\" xref=\"S5.SS1.SSS2.p1.1.m1.1.1\">𝑘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS2.p1.1.m1.1c\">k</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS2.p1.1.m1.1d\">italic_k</annotation></semantics></math>NN-KGE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib136\" title=\"\">136</a>]</cite> treats the entities and relations as special tokens in the LLM. During training, it transfers each triple <math alttext=\"(h,r,t)\" display=\"inline\" id=\"S5.SS1.SSS2.p1.2.m2.3\"><semantics id=\"S5.SS1.SSS2.p1.2.m2.3a\"><mrow id=\"S5.SS1.SSS2.p1.2.m2.3.4.2\" xref=\"S5.SS1.SSS2.p1.2.m2.3.4.1.cmml\"><mo id=\"S5.SS1.SSS2.p1.2.m2.3.4.2.1\" stretchy=\"false\" xref=\"S5.SS1.SSS2.p1.2.m2.3.4.1.cmml\">(</mo><mi id=\"S5.SS1.SSS2.p1.2.m2.1.1\" xref=\"S5.SS1.SSS2.p1.2.m2.1.1.cmml\">h</mi><mo id=\"S5.SS1.SSS2.p1.2.m2.3.4.2.2\" xref=\"S5.SS1.SSS2.p1.2.m2.3.4.1.cmml\">,</mo><mi id=\"S5.SS1.SSS2.p1.2.m2.2.2\" xref=\"S5.SS1.SSS2.p1.2.m2.2.2.cmml\">r</mi><mo id=\"S5.SS1.SSS2.p1.2.m2.3.4.2.3\" xref=\"S5.SS1.SSS2.p1.2.m2.3.4.1.cmml\">,</mo><mi id=\"S5.SS1.SSS2.p1.2.m2.3.3\" xref=\"S5.SS1.SSS2.p1.2.m2.3.3.cmml\">t</mi><mo id=\"S5.SS1.SSS2.p1.2.m2.3.4.2.4\" stretchy=\"false\" xref=\"S5.SS1.SSS2.p1.2.m2.3.4.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS2.p1.2.m2.3b\"><vector id=\"S5.SS1.SSS2.p1.2.m2.3.4.1.cmml\" xref=\"S5.SS1.SSS2.p1.2.m2.3.4.2\"><ci id=\"S5.SS1.SSS2.p1.2.m2.1.1.cmml\" xref=\"S5.SS1.SSS2.p1.2.m2.1.1\">ℎ</ci><ci id=\"S5.SS1.SSS2.p1.2.m2.2.2.cmml\" xref=\"S5.SS1.SSS2.p1.2.m2.2.2\">𝑟</ci><ci id=\"S5.SS1.SSS2.p1.2.m2.3.3.cmml\" xref=\"S5.SS1.SSS2.p1.2.m2.3.3\">𝑡</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS2.p1.2.m2.3c\">(h,r,t)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS2.p1.2.m2.3d\">( italic_h , italic_r , italic_t )</annotation></semantics></math> and corresponding text descriptions into a sentence <math alttext=\"x\" display=\"inline\" id=\"S5.SS1.SSS2.p1.3.m3.1\"><semantics id=\"S5.SS1.SSS2.p1.3.m3.1a\"><mi id=\"S5.SS1.SSS2.p1.3.m3.1.1\" xref=\"S5.SS1.SSS2.p1.3.m3.1.1.cmml\">x</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS2.p1.3.m3.1b\"><ci id=\"S5.SS1.SSS2.p1.3.m3.1.1.cmml\" xref=\"S5.SS1.SSS2.p1.3.m3.1.1\">𝑥</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS2.p1.3.m3.1c\">x</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS2.p1.3.m3.1d\">italic_x</annotation></semantics></math> as</p>\n<table id=\"S5.E3\">\n<tbody><tr>\n<td></td>\n<td><math alttext=\"x=\\texttt{[CLS]}\\ h\\ \\ \\text{Text}_{h}\\texttt{[SEP]}\\ r\\ \\texttt{[SEP]}\\ %\n\\texttt{[MASK]}\\ \\ \\text{Text}_{t}\\texttt{[SEP]},\" display=\"block\" id=\"S5.E3.m1.1\"><semantics id=\"S5.E3.m1.1a\"><mrow id=\"S5.E3.m1.1.1.1\" xref=\"S5.E3.m1.1.1.1.1.cmml\"><mrow id=\"S5.E3.m1.1.1.1.1\" xref=\"S5.E3.m1.1.1.1.1.cmml\"><mi id=\"S5.E3.m1.1.1.1.1.5\" xref=\"S5.E3.m1.1.1.1.1.5.cmml\">x</mi><mo id=\"S5.E3.m1.1.1.1.1.4\" xref=\"S5.E3.m1.1.1.1.1.4.cmml\">=</mo><mrow id=\"S5.E3.m1.1.1.1.1.3.3\" xref=\"S5.E3.m1.1.1.1.1.3.4.cmml\"><mrow id=\"S5.E3.m1.1.1.1.1.1.1.1\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.cmml\"><mtext id=\"S5.E3.m1.1.1.1.1.1.1.1.2\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.2a.cmml\">[CLS]</mtext><mo id=\"S5.E3.m1.1.1.1.1.1.1.1.1\" lspace=\"0.500em\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.1.cmml\">⁢</mo><mi id=\"S5.E3.m1.1.1.1.1.1.1.1.3\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.3.cmml\">h</mi></mrow><mspace id=\"S5.E3.m1.1.1.1.1.3.3.4\" width=\"1em\" xref=\"S5.E3.m1.1.1.1.1.3.4.cmml\"></mspace><mrow id=\"S5.E3.m1.1.1.1.1.2.2.2\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.cmml\"><msub id=\"S5.E3.m1.1.1.1.1.2.2.2.2\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2.cmml\"><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.2.2\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2.2a.cmml\">Text</mtext><mi id=\"S5.E3.m1.1.1.1.1.2.2.2.2.3\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2.3.cmml\">h</mi></msub><mo id=\"S5.E3.m1.1.1.1.1.2.2.2.1\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.1.cmml\">⁢</mo><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.3\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.3a.cmml\">[SEP]</mtext><mo id=\"S5.E3.m1.1.1.1.1.2.2.2.1a\" lspace=\"0.500em\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.1.cmml\">⁢</mo><mi id=\"S5.E3.m1.1.1.1.1.2.2.2.4\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.4.cmml\">r</mi><mo id=\"S5.E3.m1.1.1.1.1.2.2.2.1b\" lspace=\"0.500em\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.1.cmml\">⁢</mo><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.5\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.5a.cmml\">[SEP]</mtext><mo id=\"S5.E3.m1.1.1.1.1.2.2.2.1c\" lspace=\"0.500em\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.1.cmml\">⁢</mo><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.6\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.6a.cmml\">[MASK]</mtext></mrow><mspace id=\"S5.E3.m1.1.1.1.1.3.3.5\" width=\"1em\" xref=\"S5.E3.m1.1.1.1.1.3.4.cmml\"></mspace><mrow id=\"S5.E3.m1.1.1.1.1.3.3.3\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.cmml\"><msub id=\"S5.E3.m1.1.1.1.1.3.3.3.2\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2.cmml\"><mtext id=\"S5.E3.m1.1.1.1.1.3.3.3.2.2\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2.2a.cmml\">Text</mtext><mi id=\"S5.E3.m1.1.1.1.1.3.3.3.2.3\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2.3.cmml\">t</mi></msub><mo id=\"S5.E3.m1.1.1.1.1.3.3.3.1\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.1.cmml\">⁢</mo><mtext id=\"S5.E3.m1.1.1.1.1.3.3.3.3\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.3a.cmml\">[SEP]</mtext></mrow></mrow></mrow><mo id=\"S5.E3.m1.1.1.1.2\" xref=\"S5.E3.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E3.m1.1b\"><apply id=\"S5.E3.m1.1.1.1.1.cmml\" xref=\"S5.E3.m1.1.1.1\"><eq id=\"S5.E3.m1.1.1.1.1.4.cmml\" xref=\"S5.E3.m1.1.1.1.1.4\"></eq><ci id=\"S5.E3.m1.1.1.1.1.5.cmml\" xref=\"S5.E3.m1.1.1.1.1.5\">𝑥</ci><list id=\"S5.E3.m1.1.1.1.1.3.4.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3\"><apply id=\"S5.E3.m1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E3.m1.1.1.1.1.1.1.1\"><times id=\"S5.E3.m1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.1\"></times><ci id=\"S5.E3.m1.1.1.1.1.1.1.1.2a.cmml\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.2\"><mtext id=\"S5.E3.m1.1.1.1.1.1.1.1.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.2\">[CLS]</mtext></ci><ci id=\"S5.E3.m1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E3.m1.1.1.1.1.1.1.1.3\">ℎ</ci></apply><apply id=\"S5.E3.m1.1.1.1.1.2.2.2.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2\"><times id=\"S5.E3.m1.1.1.1.1.2.2.2.1.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.1\"></times><apply id=\"S5.E3.m1.1.1.1.1.2.2.2.2.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E3.m1.1.1.1.1.2.2.2.2.1.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2\">subscript</csymbol><ci id=\"S5.E3.m1.1.1.1.1.2.2.2.2.2a.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2.2\"><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.2.2.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2.2\">Text</mtext></ci><ci id=\"S5.E3.m1.1.1.1.1.2.2.2.2.3.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.2.3\">ℎ</ci></apply><ci id=\"S5.E3.m1.1.1.1.1.2.2.2.3a.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.3\"><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.3.cmml\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.3\">[SEP]</mtext></ci><ci id=\"S5.E3.m1.1.1.1.1.2.2.2.4.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.4\">𝑟</ci><ci id=\"S5.E3.m1.1.1.1.1.2.2.2.5a.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.5\"><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.5.cmml\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.5\">[SEP]</mtext></ci><ci id=\"S5.E3.m1.1.1.1.1.2.2.2.6a.cmml\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.6\"><mtext id=\"S5.E3.m1.1.1.1.1.2.2.2.6.cmml\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.2.2.2.6\">[MASK]</mtext></ci></apply><apply id=\"S5.E3.m1.1.1.1.1.3.3.3.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3\"><times id=\"S5.E3.m1.1.1.1.1.3.3.3.1.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.1\"></times><apply id=\"S5.E3.m1.1.1.1.1.3.3.3.2.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2\"><csymbol cd=\"ambiguous\" id=\"S5.E3.m1.1.1.1.1.3.3.3.2.1.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2\">subscript</csymbol><ci id=\"S5.E3.m1.1.1.1.1.3.3.3.2.2a.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2.2\"><mtext id=\"S5.E3.m1.1.1.1.1.3.3.3.2.2.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2.2\">Text</mtext></ci><ci id=\"S5.E3.m1.1.1.1.1.3.3.3.2.3.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.2.3\">𝑡</ci></apply><ci id=\"S5.E3.m1.1.1.1.1.3.3.3.3a.cmml\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.3\"><mtext id=\"S5.E3.m1.1.1.1.1.3.3.3.3.cmml\" mathvariant=\"monospace\" xref=\"S5.E3.m1.1.1.1.1.3.3.3.3\">[SEP]</mtext></ci></apply></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E3.m1.1c\">x=\\texttt{[CLS]}\\ h\\ \\ \\text{Text}_{h}\\texttt{[SEP]}\\ r\\ \\texttt{[SEP]}\\ %\n\\texttt{[MASK]}\\ \\ \\text{Text}_{t}\\texttt{[SEP]},</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E3.m1.1d\">italic_x = [CLS] italic_h Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] italic_r [SEP] [MASK] Text start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [SEP] ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(3)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS1.SSS2.p1.6\">where the tailed entities are replaced by <span id=\"S5.SS1.SSS2.p1.6.1\">[MASK]</span>. The sentence is fed into a LLM, which then finetunes the model to predict the masked entity, formulated as</p>\n<table id=\"S5.E4\">\n<tbody><tr>\n<td></td>\n<td><math alttext=\"P_{LLM}(t|h,r)=P(\\texttt{[MASK]=t}|x,\\Theta),\" display=\"block\" id=\"S5.E4.m1.5\"><semantics id=\"S5.E4.m1.5a\"><mrow id=\"S5.E4.m1.5.5.1\" xref=\"S5.E4.m1.5.5.1.1.cmml\"><mrow id=\"S5.E4.m1.5.5.1.1\" xref=\"S5.E4.m1.5.5.1.1.cmml\"><mrow id=\"S5.E4.m1.5.5.1.1.1\" xref=\"S5.E4.m1.5.5.1.1.1.cmml\"><msub id=\"S5.E4.m1.5.5.1.1.1.3\" xref=\"S5.E4.m1.5.5.1.1.1.3.cmml\"><mi id=\"S5.E4.m1.5.5.1.1.1.3.2\" xref=\"S5.E4.m1.5.5.1.1.1.3.2.cmml\">P</mi><mrow id=\"S5.E4.m1.5.5.1.1.1.3.3\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.cmml\"><mi id=\"S5.E4.m1.5.5.1.1.1.3.3.2\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.2.cmml\">L</mi><mo id=\"S5.E4.m1.5.5.1.1.1.3.3.1\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.1.cmml\">⁢</mo><mi id=\"S5.E4.m1.5.5.1.1.1.3.3.3\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.3.cmml\">L</mi><mo id=\"S5.E4.m1.5.5.1.1.1.3.3.1a\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.1.cmml\">⁢</mo><mi id=\"S5.E4.m1.5.5.1.1.1.3.3.4\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.4.cmml\">M</mi></mrow></msub><mo id=\"S5.E4.m1.5.5.1.1.1.2\" xref=\"S5.E4.m1.5.5.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E4.m1.5.5.1.1.1.1.1\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.cmml\"><mo id=\"S5.E4.m1.5.5.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S5.E4.m1.5.5.1.1.1.1.1.1\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.cmml\"><mi id=\"S5.E4.m1.5.5.1.1.1.1.1.1.2\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.2.cmml\">t</mi><mo fence=\"false\" id=\"S5.E4.m1.5.5.1.1.1.1.1.1.1\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.1.cmml\">|</mo><mrow id=\"S5.E4.m1.5.5.1.1.1.1.1.1.3.2\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.3.1.cmml\"><mi id=\"S5.E4.m1.1.1\" xref=\"S5.E4.m1.1.1.cmml\">h</mi><mo id=\"S5.E4.m1.5.5.1.1.1.1.1.1.3.2.1\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.3.1.cmml\">,</mo><mi id=\"S5.E4.m1.2.2\" xref=\"S5.E4.m1.2.2.cmml\">r</mi></mrow></mrow><mo id=\"S5.E4.m1.5.5.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo id=\"S5.E4.m1.5.5.1.1.3\" xref=\"S5.E4.m1.5.5.1.1.3.cmml\">=</mo><mrow id=\"S5.E4.m1.5.5.1.1.2\" xref=\"S5.E4.m1.5.5.1.1.2.cmml\"><mi id=\"S5.E4.m1.5.5.1.1.2.3\" xref=\"S5.E4.m1.5.5.1.1.2.3.cmml\">P</mi><mo id=\"S5.E4.m1.5.5.1.1.2.2\" xref=\"S5.E4.m1.5.5.1.1.2.2.cmml\">⁢</mo><mrow id=\"S5.E4.m1.5.5.1.1.2.1.1\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.cmml\"><mo id=\"S5.E4.m1.5.5.1.1.2.1.1.2\" stretchy=\"false\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.cmml\">(</mo><mrow id=\"S5.E4.m1.5.5.1.1.2.1.1.1\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.cmml\"><mtext id=\"S5.E4.m1.5.5.1.1.2.1.1.1.2\" mathvariant=\"monospace\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.2a.cmml\">[MASK]=t</mtext><mo fence=\"false\" id=\"S5.E4.m1.5.5.1.1.2.1.1.1.1\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.1.cmml\">|</mo><mrow id=\"S5.E4.m1.5.5.1.1.2.1.1.1.3.2\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.3.1.cmml\"><mi id=\"S5.E4.m1.3.3\" xref=\"S5.E4.m1.3.3.cmml\">x</mi><mo id=\"S5.E4.m1.5.5.1.1.2.1.1.1.3.2.1\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.3.1.cmml\">,</mo><mi id=\"S5.E4.m1.4.4\" mathvariant=\"normal\" xref=\"S5.E4.m1.4.4.cmml\">Θ</mi></mrow></mrow><mo id=\"S5.E4.m1.5.5.1.1.2.1.1.3\" stretchy=\"false\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E4.m1.5.5.1.2\" xref=\"S5.E4.m1.5.5.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E4.m1.5b\"><apply id=\"S5.E4.m1.5.5.1.1.cmml\" xref=\"S5.E4.m1.5.5.1\"><eq id=\"S5.E4.m1.5.5.1.1.3.cmml\" xref=\"S5.E4.m1.5.5.1.1.3\"></eq><apply id=\"S5.E4.m1.5.5.1.1.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.1\"><times id=\"S5.E4.m1.5.5.1.1.1.2.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.2\"></times><apply id=\"S5.E4.m1.5.5.1.1.1.3.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.E4.m1.5.5.1.1.1.3.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3\">subscript</csymbol><ci id=\"S5.E4.m1.5.5.1.1.1.3.2.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3.2\">𝑃</ci><apply id=\"S5.E4.m1.5.5.1.1.1.3.3.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3.3\"><times id=\"S5.E4.m1.5.5.1.1.1.3.3.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.1\"></times><ci id=\"S5.E4.m1.5.5.1.1.1.3.3.2.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.2\">𝐿</ci><ci id=\"S5.E4.m1.5.5.1.1.1.3.3.3.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.3\">𝐿</ci><ci id=\"S5.E4.m1.5.5.1.1.1.3.3.4.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.3.3.4\">𝑀</ci></apply></apply><apply id=\"S5.E4.m1.5.5.1.1.1.1.1.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.1.1\"><csymbol cd=\"latexml\" id=\"S5.E4.m1.5.5.1.1.1.1.1.1.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.1\">conditional</csymbol><ci id=\"S5.E4.m1.5.5.1.1.1.1.1.1.2.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.2\">𝑡</ci><list id=\"S5.E4.m1.5.5.1.1.1.1.1.1.3.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.1.1.1.1.3.2\"><ci id=\"S5.E4.m1.1.1.cmml\" xref=\"S5.E4.m1.1.1\">ℎ</ci><ci id=\"S5.E4.m1.2.2.cmml\" xref=\"S5.E4.m1.2.2\">𝑟</ci></list></apply></apply><apply id=\"S5.E4.m1.5.5.1.1.2.cmml\" xref=\"S5.E4.m1.5.5.1.1.2\"><times id=\"S5.E4.m1.5.5.1.1.2.2.cmml\" xref=\"S5.E4.m1.5.5.1.1.2.2\"></times><ci id=\"S5.E4.m1.5.5.1.1.2.3.cmml\" xref=\"S5.E4.m1.5.5.1.1.2.3\">𝑃</ci><apply id=\"S5.E4.m1.5.5.1.1.2.1.1.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.2.1.1\"><csymbol cd=\"latexml\" id=\"S5.E4.m1.5.5.1.1.2.1.1.1.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.1\">conditional</csymbol><ci id=\"S5.E4.m1.5.5.1.1.2.1.1.1.2a.cmml\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.2\"><mtext id=\"S5.E4.m1.5.5.1.1.2.1.1.1.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.2\">[MASK]=t</mtext></ci><list id=\"S5.E4.m1.5.5.1.1.2.1.1.1.3.1.cmml\" xref=\"S5.E4.m1.5.5.1.1.2.1.1.1.3.2\"><ci id=\"S5.E4.m1.3.3.cmml\" xref=\"S5.E4.m1.3.3\">𝑥</ci><ci id=\"S5.E4.m1.4.4.cmml\" xref=\"S5.E4.m1.4.4\">Θ</ci></list></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E4.m1.5c\">P_{LLM}(t|h,r)=P(\\texttt{[MASK]=t}|x,\\Theta),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E4.m1.5d\">italic_P start_POSTSUBSCRIPT italic_L italic_L italic_M end_POSTSUBSCRIPT ( italic_t | italic_h , italic_r ) = italic_P ( [MASK]=t | italic_x , roman_Θ ) ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(4)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS1.SSS2.p1.5\">where <math alttext=\"\\Theta\" display=\"inline\" id=\"S5.SS1.SSS2.p1.4.m1.1\"><semantics id=\"S5.SS1.SSS2.p1.4.m1.1a\"><mi id=\"S5.SS1.SSS2.p1.4.m1.1.1\" mathvariant=\"normal\" xref=\"S5.SS1.SSS2.p1.4.m1.1.1.cmml\">Θ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS2.p1.4.m1.1b\"><ci id=\"S5.SS1.SSS2.p1.4.m1.1.1.cmml\" xref=\"S5.SS1.SSS2.p1.4.m1.1.1\">Θ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS2.p1.4.m1.1c\">\\Theta</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS2.p1.4.m1.1d\">roman_Θ</annotation></semantics></math> denotes the parameters of the LLM. The LLM is optimized to maximize the probability of the correct entity <math alttext=\"t\" display=\"inline\" id=\"S5.SS1.SSS2.p1.5.m2.1\"><semantics id=\"S5.SS1.SSS2.p1.5.m2.1a\"><mi id=\"S5.SS1.SSS2.p1.5.m2.1.1\" xref=\"S5.SS1.SSS2.p1.5.m2.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.SSS2.p1.5.m2.1b\"><ci id=\"S5.SS1.SSS2.p1.5.m2.1.1.cmml\" xref=\"S5.SS1.SSS2.p1.5.m2.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.SSS2.p1.5.m2.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.SSS2.p1.5.m2.1d\">italic_t</annotation></semantics></math>. After training, the corresponding token representations in LLMs are used as embeddings for entities and relations. Similarly, LMKE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib135\" title=\"\">135</a>]</cite> proposes a contrastive learning method to improve the learning of embeddings generated by LLMs for KGE. Meanwhile, to better capture graph structure, LambdaKG <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib137\" title=\"\">137</a>]</cite> samples 1-hop neighbor entities and concatenates their tokens with the triple as a sentence feeding into LLMs.</p>\n</div>\n<figure id=\"S5.F15\"><img alt=\"Refer to caption\" height=\"378\" id=\"S5.F15.g1\" src=\"x12.png\" width=\"664\">\n<figcaption><span>Figure 15: </span>LLMs for joint text and knowledge graph embedding.</figcaption>\n</figure>\n</section>\n</section>\n<section id=\"S5.SS2\">\n<h3>\n<span>5.2 </span><span id=\"S5.SS2.1.1\">LLM-augmented KG Completion</span>\n</h3>\n<p id=\"S5.SS2.p1.1\">Knowledge Graph Completion (KGC) refers to the task of inferring missing facts in a given knowledge graph. Similar to KGE, conventional KGC methods mainly focused on the structure of the KG, without considering the extensive textual information. However, the recent integration of LLMs enables KGC methods to encode text or generate facts for better KGC performance. These methods fall into two distinct categories based on their utilization styles: <em id=\"S5.SS2.p1.1.1\">1) LLM as Encoders (PaE)</em>, and <em id=\"S5.SS2.p1.1.2\">2) LLM as Generators (PaG)</em>.</p>\n<section id=\"S5.SS2.SSS1\">\n<h4>\n<span>5.2.1 </span>LLM as Encoders (PaE).</h4>\n<p id=\"S5.SS2.SSS1.p1.1\">As shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S5.F16\" title=\"Figure 16 ‣ 5.2.1 LLM as Encoders (PaE). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>16</span></a> (a), (b), and (c), this line of work first uses encoder-only LLMs to encode textual information as well as KG facts. Then, they predict the plausibility of the triples or masked entities by feeding the encoded representation into a prediction head, which could be a simple MLP or conventional KG score function (e.g., TransE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib33\" title=\"\">33</a>]</cite> and TransR <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib185\" title=\"\">185</a>]</cite>).</p>\n<div id=\"S5.SS2.SSS1.p2\">\n<p id=\"S5.SS2.SSS1.p2.1\"><span id=\"S5.SS2.SSS1.p2.1.1\">Joint Encoding.</span>\nSince the encoder-only LLMs (e.g., Bert <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib1\" title=\"\">1</a>]</cite>) are well at encoding text sequences, KG-BERT&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib26\" title=\"\">26</a>]</cite> represents a triple <math alttext=\"(h,r,t)\" display=\"inline\" id=\"S5.SS2.SSS1.p2.1.m1.3\"><semantics id=\"S5.SS2.SSS1.p2.1.m1.3a\"><mrow id=\"S5.SS2.SSS1.p2.1.m1.3.4.2\" xref=\"S5.SS2.SSS1.p2.1.m1.3.4.1.cmml\"><mo id=\"S5.SS2.SSS1.p2.1.m1.3.4.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p2.1.m1.3.4.1.cmml\">(</mo><mi id=\"S5.SS2.SSS1.p2.1.m1.1.1\" xref=\"S5.SS2.SSS1.p2.1.m1.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS1.p2.1.m1.3.4.2.2\" xref=\"S5.SS2.SSS1.p2.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p2.1.m1.2.2\" xref=\"S5.SS2.SSS1.p2.1.m1.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS1.p2.1.m1.3.4.2.3\" xref=\"S5.SS2.SSS1.p2.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p2.1.m1.3.3\" xref=\"S5.SS2.SSS1.p2.1.m1.3.3.cmml\">t</mi><mo id=\"S5.SS2.SSS1.p2.1.m1.3.4.2.4\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p2.1.m1.3.4.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.1.m1.3b\"><vector id=\"S5.SS2.SSS1.p2.1.m1.3.4.1.cmml\" xref=\"S5.SS2.SSS1.p2.1.m1.3.4.2\"><ci id=\"S5.SS2.SSS1.p2.1.m1.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.1.m1.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS1.p2.1.m1.2.2.cmml\" xref=\"S5.SS2.SSS1.p2.1.m1.2.2\">𝑟</ci><ci id=\"S5.SS2.SSS1.p2.1.m1.3.3.cmml\" xref=\"S5.SS2.SSS1.p2.1.m1.3.3\">𝑡</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.1.m1.3c\">(h,r,t)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.1.m1.3d\">( italic_h , italic_r , italic_t )</annotation></semantics></math> as a text sequence and encodes it with LLM Fig.&nbsp;<a href=\"https://arxiv.org/html/2306.08302v3#S5.F16\" title=\"Figure 16 ‣ 5.2.1 LLM as Encoders (PaE). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>16</span></a>(a).</p>\n<table id=\"S5.E5\">\n<tbody><tr>\n<td></td>\n<td><math alttext=\"x=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}\\ \\texttt{[%\nSEP]}\\ \\text{Text}_{t}\\ \\texttt{[SEP]},\" display=\"block\" id=\"S5.E5.m1.1\"><semantics id=\"S5.E5.m1.1a\"><mrow id=\"S5.E5.m1.1.1.1\" xref=\"S5.E5.m1.1.1.1.1.cmml\"><mrow id=\"S5.E5.m1.1.1.1.1\" xref=\"S5.E5.m1.1.1.1.1.cmml\"><mi id=\"S5.E5.m1.1.1.1.1.2\" xref=\"S5.E5.m1.1.1.1.1.2.cmml\">x</mi><mo id=\"S5.E5.m1.1.1.1.1.1\" xref=\"S5.E5.m1.1.1.1.1.1.cmml\">=</mo><mrow id=\"S5.E5.m1.1.1.1.1.3\" xref=\"S5.E5.m1.1.1.1.1.3.cmml\"><mtext id=\"S5.E5.m1.1.1.1.1.3.2\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.2a.cmml\">[CLS]</mtext><mo id=\"S5.E5.m1.1.1.1.1.3.1\" lspace=\"0.500em\" xref=\"S5.E5.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E5.m1.1.1.1.1.3.3\" xref=\"S5.E5.m1.1.1.1.1.3.3.cmml\"><mtext id=\"S5.E5.m1.1.1.1.1.3.3.2\" xref=\"S5.E5.m1.1.1.1.1.3.3.2a.cmml\">Text</mtext><mi id=\"S5.E5.m1.1.1.1.1.3.3.3\" xref=\"S5.E5.m1.1.1.1.1.3.3.3.cmml\">h</mi></msub><mo id=\"S5.E5.m1.1.1.1.1.3.1a\" xref=\"S5.E5.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E5.m1.1.1.1.1.3.4\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.4a.cmml\">[SEP]</mtext><mo id=\"S5.E5.m1.1.1.1.1.3.1b\" lspace=\"0.500em\" xref=\"S5.E5.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E5.m1.1.1.1.1.3.5\" xref=\"S5.E5.m1.1.1.1.1.3.5.cmml\"><mtext id=\"S5.E5.m1.1.1.1.1.3.5.2\" xref=\"S5.E5.m1.1.1.1.1.3.5.2a.cmml\">Text</mtext><mi id=\"S5.E5.m1.1.1.1.1.3.5.3\" xref=\"S5.E5.m1.1.1.1.1.3.5.3.cmml\">r</mi></msub><mo id=\"S5.E5.m1.1.1.1.1.3.1c\" xref=\"S5.E5.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E5.m1.1.1.1.1.3.6\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.6a.cmml\">[SEP]</mtext><mo id=\"S5.E5.m1.1.1.1.1.3.1d\" lspace=\"0.500em\" xref=\"S5.E5.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E5.m1.1.1.1.1.3.7\" xref=\"S5.E5.m1.1.1.1.1.3.7.cmml\"><mtext id=\"S5.E5.m1.1.1.1.1.3.7.2\" xref=\"S5.E5.m1.1.1.1.1.3.7.2a.cmml\">Text</mtext><mi id=\"S5.E5.m1.1.1.1.1.3.7.3\" xref=\"S5.E5.m1.1.1.1.1.3.7.3.cmml\">t</mi></msub><mo id=\"S5.E5.m1.1.1.1.1.3.1e\" xref=\"S5.E5.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E5.m1.1.1.1.1.3.8\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.8a.cmml\">[SEP]</mtext></mrow></mrow><mo id=\"S5.E5.m1.1.1.1.2\" xref=\"S5.E5.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E5.m1.1b\"><apply id=\"S5.E5.m1.1.1.1.1.cmml\" xref=\"S5.E5.m1.1.1.1\"><eq id=\"S5.E5.m1.1.1.1.1.1.cmml\" xref=\"S5.E5.m1.1.1.1.1.1\"></eq><ci id=\"S5.E5.m1.1.1.1.1.2.cmml\" xref=\"S5.E5.m1.1.1.1.1.2\">𝑥</ci><apply id=\"S5.E5.m1.1.1.1.1.3.cmml\" xref=\"S5.E5.m1.1.1.1.1.3\"><times id=\"S5.E5.m1.1.1.1.1.3.1.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.1\"></times><ci id=\"S5.E5.m1.1.1.1.1.3.2a.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.2\"><mtext id=\"S5.E5.m1.1.1.1.1.3.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.2\">[CLS]</mtext></ci><apply id=\"S5.E5.m1.1.1.1.1.3.3.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E5.m1.1.1.1.1.3.3.1.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.3\">subscript</csymbol><ci id=\"S5.E5.m1.1.1.1.1.3.3.2a.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.3.2\"><mtext id=\"S5.E5.m1.1.1.1.1.3.3.2.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.3.2\">Text</mtext></ci><ci id=\"S5.E5.m1.1.1.1.1.3.3.3.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.3.3\">ℎ</ci></apply><ci id=\"S5.E5.m1.1.1.1.1.3.4a.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.4\"><mtext id=\"S5.E5.m1.1.1.1.1.3.4.cmml\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.4\">[SEP]</mtext></ci><apply id=\"S5.E5.m1.1.1.1.1.3.5.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.5\"><csymbol cd=\"ambiguous\" id=\"S5.E5.m1.1.1.1.1.3.5.1.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.5\">subscript</csymbol><ci id=\"S5.E5.m1.1.1.1.1.3.5.2a.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.5.2\"><mtext id=\"S5.E5.m1.1.1.1.1.3.5.2.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.5.2\">Text</mtext></ci><ci id=\"S5.E5.m1.1.1.1.1.3.5.3.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.5.3\">𝑟</ci></apply><ci id=\"S5.E5.m1.1.1.1.1.3.6a.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.6\"><mtext id=\"S5.E5.m1.1.1.1.1.3.6.cmml\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.6\">[SEP]</mtext></ci><apply id=\"S5.E5.m1.1.1.1.1.3.7.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.7\"><csymbol cd=\"ambiguous\" id=\"S5.E5.m1.1.1.1.1.3.7.1.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.7\">subscript</csymbol><ci id=\"S5.E5.m1.1.1.1.1.3.7.2a.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.7.2\"><mtext id=\"S5.E5.m1.1.1.1.1.3.7.2.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.7.2\">Text</mtext></ci><ci id=\"S5.E5.m1.1.1.1.1.3.7.3.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.7.3\">𝑡</ci></apply><ci id=\"S5.E5.m1.1.1.1.1.3.8a.cmml\" xref=\"S5.E5.m1.1.1.1.1.3.8\"><mtext id=\"S5.E5.m1.1.1.1.1.3.8.cmml\" mathvariant=\"monospace\" xref=\"S5.E5.m1.1.1.1.1.3.8\">[SEP]</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E5.m1.1c\">x=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}\\ \\texttt{[%\nSEP]}\\ \\text{Text}_{t}\\ \\texttt{[SEP]},</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E5.m1.1d\">italic_x = [CLS] Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] Text start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT [SEP] Text start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [SEP] ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(5)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS2.SSS1.p2.2\">The final hidden state of the <span id=\"S5.SS2.SSS1.p2.2.1\">[CLS]</span> token is fed into a classifier to predict the possibility of the triple, formulated as</p>\n<table id=\"S5.E6\">\n<tbody><tr>\n<td></td>\n<td><math alttext=\"s=\\sigma(\\text{MLP}(e_{\\texttt{[CLS]}})),\" display=\"block\" id=\"S5.E6.m1.1\"><semantics id=\"S5.E6.m1.1a\"><mrow id=\"S5.E6.m1.1.1.1\" xref=\"S5.E6.m1.1.1.1.1.cmml\"><mrow id=\"S5.E6.m1.1.1.1.1\" xref=\"S5.E6.m1.1.1.1.1.cmml\"><mi id=\"S5.E6.m1.1.1.1.1.3\" xref=\"S5.E6.m1.1.1.1.1.3.cmml\">s</mi><mo id=\"S5.E6.m1.1.1.1.1.2\" xref=\"S5.E6.m1.1.1.1.1.2.cmml\">=</mo><mrow id=\"S5.E6.m1.1.1.1.1.1\" xref=\"S5.E6.m1.1.1.1.1.1.cmml\"><mi id=\"S5.E6.m1.1.1.1.1.1.3\" xref=\"S5.E6.m1.1.1.1.1.1.3.cmml\">σ</mi><mo id=\"S5.E6.m1.1.1.1.1.1.2\" xref=\"S5.E6.m1.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E6.m1.1.1.1.1.1.1.1\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S5.E6.m1.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S5.E6.m1.1.1.1.1.1.1.1.1\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.cmml\"><mtext id=\"S5.E6.m1.1.1.1.1.1.1.1.1.3\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.3a.cmml\">MLP</mtext><mo id=\"S5.E6.m1.1.1.1.1.1.1.1.1.2\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml\">(</mo><msub id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml\">e</mi><mtext id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3\" mathvariant=\"monospace\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml\">[CLS]</mtext></msub><mo id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo id=\"S5.E6.m1.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E6.m1.1.1.1.2\" xref=\"S5.E6.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E6.m1.1b\"><apply id=\"S5.E6.m1.1.1.1.1.cmml\" xref=\"S5.E6.m1.1.1.1\"><eq id=\"S5.E6.m1.1.1.1.1.2.cmml\" xref=\"S5.E6.m1.1.1.1.1.2\"></eq><ci id=\"S5.E6.m1.1.1.1.1.3.cmml\" xref=\"S5.E6.m1.1.1.1.1.3\">𝑠</ci><apply id=\"S5.E6.m1.1.1.1.1.1.cmml\" xref=\"S5.E6.m1.1.1.1.1.1\"><times id=\"S5.E6.m1.1.1.1.1.1.2.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.2\"></times><ci id=\"S5.E6.m1.1.1.1.1.1.3.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.3\">𝜎</ci><apply id=\"S5.E6.m1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1\"><times id=\"S5.E6.m1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.2\"></times><ci id=\"S5.E6.m1.1.1.1.1.1.1.1.1.3a.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.3\"><mtext id=\"S5.E6.m1.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.3\">MLP</mtext></ci><apply id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2\">𝑒</ci><ci id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3\"><mtext id=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml\" mathsize=\"70%\" mathvariant=\"monospace\" xref=\"S5.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3\">[CLS]</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E6.m1.1c\">s=\\sigma(\\text{MLP}(e_{\\texttt{[CLS]}})),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E6.m1.1d\">italic_s = italic_σ ( MLP ( italic_e start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(6)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS2.SSS1.p2.10\">where <math alttext=\"\\sigma(\\cdot)\" display=\"inline\" id=\"S5.SS2.SSS1.p2.3.m1.1\"><semantics id=\"S5.SS2.SSS1.p2.3.m1.1a\"><mrow id=\"S5.SS2.SSS1.p2.3.m1.1.2\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.cmml\"><mi id=\"S5.SS2.SSS1.p2.3.m1.1.2.2\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.2.cmml\">σ</mi><mo id=\"S5.SS2.SSS1.p2.3.m1.1.2.1\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.1.cmml\">⁢</mo><mrow id=\"S5.SS2.SSS1.p2.3.m1.1.2.3.2\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.cmml\"><mo id=\"S5.SS2.SSS1.p2.3.m1.1.2.3.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.cmml\">(</mo><mo id=\"S5.SS2.SSS1.p2.3.m1.1.1\" lspace=\"0em\" rspace=\"0em\" xref=\"S5.SS2.SSS1.p2.3.m1.1.1.cmml\">⋅</mo><mo id=\"S5.SS2.SSS1.p2.3.m1.1.2.3.2.2\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.3.m1.1b\"><apply id=\"S5.SS2.SSS1.p2.3.m1.1.2.cmml\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2\"><times id=\"S5.SS2.SSS1.p2.3.m1.1.2.1.cmml\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.1\"></times><ci id=\"S5.SS2.SSS1.p2.3.m1.1.2.2.cmml\" xref=\"S5.SS2.SSS1.p2.3.m1.1.2.2\">𝜎</ci><ci id=\"S5.SS2.SSS1.p2.3.m1.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.3.m1.1.1\">⋅</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.3.m1.1c\">\\sigma(\\cdot)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.3.m1.1d\">italic_σ ( ⋅ )</annotation></semantics></math> denotes the sigmoid function and <math alttext=\"e_{\\texttt{[CLS]}}\" display=\"inline\" id=\"S5.SS2.SSS1.p2.4.m2.1\"><semantics id=\"S5.SS2.SSS1.p2.4.m2.1a\"><msub id=\"S5.SS2.SSS1.p2.4.m2.1.1\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1.cmml\"><mi id=\"S5.SS2.SSS1.p2.4.m2.1.1.2\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1.2.cmml\">e</mi><mtext id=\"S5.SS2.SSS1.p2.4.m2.1.1.3\" mathvariant=\"monospace\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1.3a.cmml\">[CLS]</mtext></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.4.m2.1b\"><apply id=\"S5.SS2.SSS1.p2.4.m2.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS2.SSS1.p2.4.m2.1.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1\">subscript</csymbol><ci id=\"S5.SS2.SSS1.p2.4.m2.1.1.2.cmml\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1.2\">𝑒</ci><ci id=\"S5.SS2.SSS1.p2.4.m2.1.1.3a.cmml\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1.3\"><mtext id=\"S5.SS2.SSS1.p2.4.m2.1.1.3.cmml\" mathsize=\"70%\" mathvariant=\"monospace\" xref=\"S5.SS2.SSS1.p2.4.m2.1.1.3\">[CLS]</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.4.m2.1c\">e_{\\texttt{[CLS]}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.4.m2.1d\">italic_e start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT</annotation></semantics></math> denotes the representation encoded by LLMs.\nTo improve the efficacy of KG-BERT, MTL-KGC&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib138\" title=\"\">138</a>]</cite> proposed a Multi-Task Learning for the KGC framework which incorporates additional auxiliary tasks into the model’s training, i.e. prediction (RP) and relevance ranking (RR). PKGC&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib139\" title=\"\">139</a>]</cite> assesses the validity of a triplet <math alttext=\"(h,r,t)\" display=\"inline\" id=\"S5.SS2.SSS1.p2.5.m3.3\"><semantics id=\"S5.SS2.SSS1.p2.5.m3.3a\"><mrow id=\"S5.SS2.SSS1.p2.5.m3.3.4.2\" xref=\"S5.SS2.SSS1.p2.5.m3.3.4.1.cmml\"><mo id=\"S5.SS2.SSS1.p2.5.m3.3.4.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p2.5.m3.3.4.1.cmml\">(</mo><mi id=\"S5.SS2.SSS1.p2.5.m3.1.1\" xref=\"S5.SS2.SSS1.p2.5.m3.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS1.p2.5.m3.3.4.2.2\" xref=\"S5.SS2.SSS1.p2.5.m3.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p2.5.m3.2.2\" xref=\"S5.SS2.SSS1.p2.5.m3.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS1.p2.5.m3.3.4.2.3\" xref=\"S5.SS2.SSS1.p2.5.m3.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p2.5.m3.3.3\" xref=\"S5.SS2.SSS1.p2.5.m3.3.3.cmml\">t</mi><mo id=\"S5.SS2.SSS1.p2.5.m3.3.4.2.4\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p2.5.m3.3.4.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.5.m3.3b\"><vector id=\"S5.SS2.SSS1.p2.5.m3.3.4.1.cmml\" xref=\"S5.SS2.SSS1.p2.5.m3.3.4.2\"><ci id=\"S5.SS2.SSS1.p2.5.m3.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.5.m3.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS1.p2.5.m3.2.2.cmml\" xref=\"S5.SS2.SSS1.p2.5.m3.2.2\">𝑟</ci><ci id=\"S5.SS2.SSS1.p2.5.m3.3.3.cmml\" xref=\"S5.SS2.SSS1.p2.5.m3.3.3\">𝑡</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.5.m3.3c\">(h,r,t)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.5.m3.3d\">( italic_h , italic_r , italic_t )</annotation></semantics></math> by transforming the triple and its supporting information into natural language sentences with pre-defined templates. These sentences are then processed by LLMs for binary classification. The supporting information of the triplet is derived from the attributes of <math alttext=\"h\" display=\"inline\" id=\"S5.SS2.SSS1.p2.6.m4.1\"><semantics id=\"S5.SS2.SSS1.p2.6.m4.1a\"><mi id=\"S5.SS2.SSS1.p2.6.m4.1.1\" xref=\"S5.SS2.SSS1.p2.6.m4.1.1.cmml\">h</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.6.m4.1b\"><ci id=\"S5.SS2.SSS1.p2.6.m4.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.6.m4.1.1\">ℎ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.6.m4.1c\">h</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.6.m4.1d\">italic_h</annotation></semantics></math> and <math alttext=\"t\" display=\"inline\" id=\"S5.SS2.SSS1.p2.7.m5.1\"><semantics id=\"S5.SS2.SSS1.p2.7.m5.1a\"><mi id=\"S5.SS2.SSS1.p2.7.m5.1.1\" xref=\"S5.SS2.SSS1.p2.7.m5.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.7.m5.1b\"><ci id=\"S5.SS2.SSS1.p2.7.m5.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.7.m5.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.7.m5.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.7.m5.1d\">italic_t</annotation></semantics></math> with a verbalizing function. For instance, if the triple is <span id=\"S5.SS2.SSS1.p2.10.1\">(Lebron James, member of sports team, Lakers)</span>, the information regarding Lebron James is verbalized as ”Lebron James: American basketball player”. LASS&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib140\" title=\"\">140</a>]</cite> observes that language semantics and graph structures are equally vital to KGC. As a result, LASS is proposed to jointly learn two types of embeddings: semantic embedding and structure embedding. In this method, the full text of a triple is forwarded to the LLM, and the mean pooling of the corresponding LLM outputs for <math alttext=\"h\" display=\"inline\" id=\"S5.SS2.SSS1.p2.8.m6.1\"><semantics id=\"S5.SS2.SSS1.p2.8.m6.1a\"><mi id=\"S5.SS2.SSS1.p2.8.m6.1.1\" xref=\"S5.SS2.SSS1.p2.8.m6.1.1.cmml\">h</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.8.m6.1b\"><ci id=\"S5.SS2.SSS1.p2.8.m6.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.8.m6.1.1\">ℎ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.8.m6.1c\">h</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.8.m6.1d\">italic_h</annotation></semantics></math>, <math alttext=\"r\" display=\"inline\" id=\"S5.SS2.SSS1.p2.9.m7.1\"><semantics id=\"S5.SS2.SSS1.p2.9.m7.1a\"><mi id=\"S5.SS2.SSS1.p2.9.m7.1.1\" xref=\"S5.SS2.SSS1.p2.9.m7.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.9.m7.1b\"><ci id=\"S5.SS2.SSS1.p2.9.m7.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.9.m7.1.1\">𝑟</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.9.m7.1c\">r</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.9.m7.1d\">italic_r</annotation></semantics></math>, and <math alttext=\"t\" display=\"inline\" id=\"S5.SS2.SSS1.p2.10.m8.1\"><semantics id=\"S5.SS2.SSS1.p2.10.m8.1a\"><mi id=\"S5.SS2.SSS1.p2.10.m8.1.1\" xref=\"S5.SS2.SSS1.p2.10.m8.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p2.10.m8.1b\"><ci id=\"S5.SS2.SSS1.p2.10.m8.1.1.cmml\" xref=\"S5.SS2.SSS1.p2.10.m8.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p2.10.m8.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p2.10.m8.1d\">italic_t</annotation></semantics></math> are separately calculated. These embeddings are then passed to a graph-based method, i.e. TransE, to reconstruct the KG structures.</p>\n</div>\n<figure id=\"S5.F16\"><img alt=\"Refer to caption\" height=\"705\" id=\"S5.F16.g1\" src=\"x13.png\" width=\"581\">\n<figcaption><span>Figure 16: </span>The general framework of adopting LLMs as encoders (PaE) for KG Completion.</figcaption>\n</figure>\n<div id=\"S5.SS2.SSS1.p3\">\n<p id=\"S5.SS2.SSS1.p3.3\"><span id=\"S5.SS2.SSS1.p3.3.1\">MLM Encoding.</span> Instead of encoding the full text of a triple, many works introduce the concept of Masked Language Model (MLM) to encode KG text (Fig.&nbsp;<a href=\"https://arxiv.org/html/2306.08302v3#S5.F16\" title=\"Figure 16 ‣ 5.2.1 LLM as Encoders (PaE). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>16</span></a>(b)). MEM-KGC&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib141\" title=\"\">141</a>]</cite> uses Masked Entity Model (MEM) classification mechanism to predict the masked entities of the triple. The input text is in the form of</p>\n<table id=\"S5.E7\">\n<tbody><tr>\n<td></td>\n<td><math alttext=\"x=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}\\ \\texttt{[%\nSEP]}\\ \\texttt{[MASK]}\\ \\texttt{[SEP]},\" display=\"block\" id=\"S5.E7.m1.1\"><semantics id=\"S5.E7.m1.1a\"><mrow id=\"S5.E7.m1.1.1.1\" xref=\"S5.E7.m1.1.1.1.1.cmml\"><mrow id=\"S5.E7.m1.1.1.1.1\" xref=\"S5.E7.m1.1.1.1.1.cmml\"><mi id=\"S5.E7.m1.1.1.1.1.2\" xref=\"S5.E7.m1.1.1.1.1.2.cmml\">x</mi><mo id=\"S5.E7.m1.1.1.1.1.1\" xref=\"S5.E7.m1.1.1.1.1.1.cmml\">=</mo><mrow id=\"S5.E7.m1.1.1.1.1.3\" xref=\"S5.E7.m1.1.1.1.1.3.cmml\"><mtext id=\"S5.E7.m1.1.1.1.1.3.2\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.2a.cmml\">[CLS]</mtext><mo id=\"S5.E7.m1.1.1.1.1.3.1\" lspace=\"0.500em\" xref=\"S5.E7.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E7.m1.1.1.1.1.3.3\" xref=\"S5.E7.m1.1.1.1.1.3.3.cmml\"><mtext id=\"S5.E7.m1.1.1.1.1.3.3.2\" xref=\"S5.E7.m1.1.1.1.1.3.3.2a.cmml\">Text</mtext><mi id=\"S5.E7.m1.1.1.1.1.3.3.3\" xref=\"S5.E7.m1.1.1.1.1.3.3.3.cmml\">h</mi></msub><mo id=\"S5.E7.m1.1.1.1.1.3.1a\" xref=\"S5.E7.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E7.m1.1.1.1.1.3.4\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.4a.cmml\">[SEP]</mtext><mo id=\"S5.E7.m1.1.1.1.1.3.1b\" lspace=\"0.500em\" xref=\"S5.E7.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E7.m1.1.1.1.1.3.5\" xref=\"S5.E7.m1.1.1.1.1.3.5.cmml\"><mtext id=\"S5.E7.m1.1.1.1.1.3.5.2\" xref=\"S5.E7.m1.1.1.1.1.3.5.2a.cmml\">Text</mtext><mi id=\"S5.E7.m1.1.1.1.1.3.5.3\" xref=\"S5.E7.m1.1.1.1.1.3.5.3.cmml\">r</mi></msub><mo id=\"S5.E7.m1.1.1.1.1.3.1c\" xref=\"S5.E7.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E7.m1.1.1.1.1.3.6\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.6a.cmml\">[SEP]</mtext><mo id=\"S5.E7.m1.1.1.1.1.3.1d\" lspace=\"0.500em\" xref=\"S5.E7.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E7.m1.1.1.1.1.3.7\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.7a.cmml\">[MASK]</mtext><mo id=\"S5.E7.m1.1.1.1.1.3.1e\" lspace=\"0.500em\" xref=\"S5.E7.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E7.m1.1.1.1.1.3.8\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.8a.cmml\">[SEP]</mtext></mrow></mrow><mo id=\"S5.E7.m1.1.1.1.2\" xref=\"S5.E7.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E7.m1.1b\"><apply id=\"S5.E7.m1.1.1.1.1.cmml\" xref=\"S5.E7.m1.1.1.1\"><eq id=\"S5.E7.m1.1.1.1.1.1.cmml\" xref=\"S5.E7.m1.1.1.1.1.1\"></eq><ci id=\"S5.E7.m1.1.1.1.1.2.cmml\" xref=\"S5.E7.m1.1.1.1.1.2\">𝑥</ci><apply id=\"S5.E7.m1.1.1.1.1.3.cmml\" xref=\"S5.E7.m1.1.1.1.1.3\"><times id=\"S5.E7.m1.1.1.1.1.3.1.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.1\"></times><ci id=\"S5.E7.m1.1.1.1.1.3.2a.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.2\"><mtext id=\"S5.E7.m1.1.1.1.1.3.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.2\">[CLS]</mtext></ci><apply id=\"S5.E7.m1.1.1.1.1.3.3.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E7.m1.1.1.1.1.3.3.1.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.3\">subscript</csymbol><ci id=\"S5.E7.m1.1.1.1.1.3.3.2a.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.3.2\"><mtext id=\"S5.E7.m1.1.1.1.1.3.3.2.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.3.2\">Text</mtext></ci><ci id=\"S5.E7.m1.1.1.1.1.3.3.3.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.3.3\">ℎ</ci></apply><ci id=\"S5.E7.m1.1.1.1.1.3.4a.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.4\"><mtext id=\"S5.E7.m1.1.1.1.1.3.4.cmml\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.4\">[SEP]</mtext></ci><apply id=\"S5.E7.m1.1.1.1.1.3.5.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.5\"><csymbol cd=\"ambiguous\" id=\"S5.E7.m1.1.1.1.1.3.5.1.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.5\">subscript</csymbol><ci id=\"S5.E7.m1.1.1.1.1.3.5.2a.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.5.2\"><mtext id=\"S5.E7.m1.1.1.1.1.3.5.2.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.5.2\">Text</mtext></ci><ci id=\"S5.E7.m1.1.1.1.1.3.5.3.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.5.3\">𝑟</ci></apply><ci id=\"S5.E7.m1.1.1.1.1.3.6a.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.6\"><mtext id=\"S5.E7.m1.1.1.1.1.3.6.cmml\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.6\">[SEP]</mtext></ci><ci id=\"S5.E7.m1.1.1.1.1.3.7a.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.7\"><mtext id=\"S5.E7.m1.1.1.1.1.3.7.cmml\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.7\">[MASK]</mtext></ci><ci id=\"S5.E7.m1.1.1.1.1.3.8a.cmml\" xref=\"S5.E7.m1.1.1.1.1.3.8\"><mtext id=\"S5.E7.m1.1.1.1.1.3.8.cmml\" mathvariant=\"monospace\" xref=\"S5.E7.m1.1.1.1.1.3.8\">[SEP]</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E7.m1.1c\">x=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}\\ \\texttt{[%\nSEP]}\\ \\texttt{[MASK]}\\ \\texttt{[SEP]},</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E7.m1.1d\">italic_x = [CLS] Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] Text start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT [SEP] [MASK] [SEP] ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(7)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS2.SSS1.p3.1\">Similar to Eq. <a href=\"https://arxiv.org/html/2306.08302v3#S5.E4\" title=\"4 ‣ 5.1.2 LLMs for Joint Text and KG Embedding ‣ 5.1 LLM-augmented KG Embedding ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>4</span></a>, it tries to maximize the probability that the masked entity is the correct entity <math alttext=\"t\" display=\"inline\" id=\"S5.SS2.SSS1.p3.1.m1.1\"><semantics id=\"S5.SS2.SSS1.p3.1.m1.1a\"><mi id=\"S5.SS2.SSS1.p3.1.m1.1.1\" xref=\"S5.SS2.SSS1.p3.1.m1.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p3.1.m1.1b\"><ci id=\"S5.SS2.SSS1.p3.1.m1.1.1.cmml\" xref=\"S5.SS2.SSS1.p3.1.m1.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p3.1.m1.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p3.1.m1.1d\">italic_t</annotation></semantics></math>.\nAdditionally, to enable the model to learn unseen entities, MEM-KGC integrates multitask learning for entities and super-class prediction based on the text description of entities:</p>\n<table id=\"S5.E8\">\n<tbody><tr>\n<td></td>\n<td><math alttext=\"x=\\texttt{[CLS]}\\ \\texttt{[MASK]}\\ \\texttt{[SEP]}\\ \\text{Text}_{h}\\ \\texttt{[%\nSEP]}.\" display=\"block\" id=\"S5.E8.m1.1\"><semantics id=\"S5.E8.m1.1a\"><mrow id=\"S5.E8.m1.1.1.1\" xref=\"S5.E8.m1.1.1.1.1.cmml\"><mrow id=\"S5.E8.m1.1.1.1.1\" xref=\"S5.E8.m1.1.1.1.1.cmml\"><mi id=\"S5.E8.m1.1.1.1.1.2\" xref=\"S5.E8.m1.1.1.1.1.2.cmml\">x</mi><mo id=\"S5.E8.m1.1.1.1.1.1\" xref=\"S5.E8.m1.1.1.1.1.1.cmml\">=</mo><mrow id=\"S5.E8.m1.1.1.1.1.3\" xref=\"S5.E8.m1.1.1.1.1.3.cmml\"><mtext id=\"S5.E8.m1.1.1.1.1.3.2\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.2a.cmml\">[CLS]</mtext><mo id=\"S5.E8.m1.1.1.1.1.3.1\" lspace=\"0.500em\" xref=\"S5.E8.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E8.m1.1.1.1.1.3.3\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.3a.cmml\">[MASK]</mtext><mo id=\"S5.E8.m1.1.1.1.1.3.1a\" lspace=\"0.500em\" xref=\"S5.E8.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E8.m1.1.1.1.1.3.4\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.4a.cmml\">[SEP]</mtext><mo id=\"S5.E8.m1.1.1.1.1.3.1b\" lspace=\"0.500em\" xref=\"S5.E8.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E8.m1.1.1.1.1.3.5\" xref=\"S5.E8.m1.1.1.1.1.3.5.cmml\"><mtext id=\"S5.E8.m1.1.1.1.1.3.5.2\" xref=\"S5.E8.m1.1.1.1.1.3.5.2a.cmml\">Text</mtext><mi id=\"S5.E8.m1.1.1.1.1.3.5.3\" xref=\"S5.E8.m1.1.1.1.1.3.5.3.cmml\">h</mi></msub><mo id=\"S5.E8.m1.1.1.1.1.3.1c\" xref=\"S5.E8.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E8.m1.1.1.1.1.3.6\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.6a.cmml\">[SEP]</mtext></mrow></mrow><mo id=\"S5.E8.m1.1.1.1.2\" lspace=\"0em\" xref=\"S5.E8.m1.1.1.1.1.cmml\">.</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E8.m1.1b\"><apply id=\"S5.E8.m1.1.1.1.1.cmml\" xref=\"S5.E8.m1.1.1.1\"><eq id=\"S5.E8.m1.1.1.1.1.1.cmml\" xref=\"S5.E8.m1.1.1.1.1.1\"></eq><ci id=\"S5.E8.m1.1.1.1.1.2.cmml\" xref=\"S5.E8.m1.1.1.1.1.2\">𝑥</ci><apply id=\"S5.E8.m1.1.1.1.1.3.cmml\" xref=\"S5.E8.m1.1.1.1.1.3\"><times id=\"S5.E8.m1.1.1.1.1.3.1.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.1\"></times><ci id=\"S5.E8.m1.1.1.1.1.3.2a.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.2\"><mtext id=\"S5.E8.m1.1.1.1.1.3.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.2\">[CLS]</mtext></ci><ci id=\"S5.E8.m1.1.1.1.1.3.3a.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.3\"><mtext id=\"S5.E8.m1.1.1.1.1.3.3.cmml\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.3\">[MASK]</mtext></ci><ci id=\"S5.E8.m1.1.1.1.1.3.4a.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.4\"><mtext id=\"S5.E8.m1.1.1.1.1.3.4.cmml\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.4\">[SEP]</mtext></ci><apply id=\"S5.E8.m1.1.1.1.1.3.5.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.5\"><csymbol cd=\"ambiguous\" id=\"S5.E8.m1.1.1.1.1.3.5.1.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.5\">subscript</csymbol><ci id=\"S5.E8.m1.1.1.1.1.3.5.2a.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.5.2\"><mtext id=\"S5.E8.m1.1.1.1.1.3.5.2.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.5.2\">Text</mtext></ci><ci id=\"S5.E8.m1.1.1.1.1.3.5.3.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.5.3\">ℎ</ci></apply><ci id=\"S5.E8.m1.1.1.1.1.3.6a.cmml\" xref=\"S5.E8.m1.1.1.1.1.3.6\"><mtext id=\"S5.E8.m1.1.1.1.1.3.6.cmml\" mathvariant=\"monospace\" xref=\"S5.E8.m1.1.1.1.1.3.6\">[SEP]</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E8.m1.1c\">x=\\texttt{[CLS]}\\ \\texttt{[MASK]}\\ \\texttt{[SEP]}\\ \\text{Text}_{h}\\ \\texttt{[%\nSEP]}.</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E8.m1.1d\">italic_x = [CLS] [MASK] [SEP] Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] .</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(8)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS2.SSS1.p3.2\">OpenWorld KGC <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib142\" title=\"\">142</a>]</cite> expands the MEM-KGC model to address the challenges of open-world KGC with a pipeline framework, where two sequential MLM-based modules are defined: Entity Description Prediction (EDP), an auxiliary module that predicts a corresponding entity with a given textual description; Incomplete Triple Prediction (ITP), the target module that predicts a plausible entity for a given incomplete triple <math alttext=\"(h,r,?)\" display=\"inline\" id=\"S5.SS2.SSS1.p3.2.m1.3\"><semantics id=\"S5.SS2.SSS1.p3.2.m1.3a\"><mrow id=\"S5.SS2.SSS1.p3.2.m1.3.4.2\" xref=\"S5.SS2.SSS1.p3.2.m1.3.4.1.cmml\"><mo id=\"S5.SS2.SSS1.p3.2.m1.3.4.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p3.2.m1.3.4.1.cmml\">(</mo><mi id=\"S5.SS2.SSS1.p3.2.m1.1.1\" xref=\"S5.SS2.SSS1.p3.2.m1.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS1.p3.2.m1.3.4.2.2\" xref=\"S5.SS2.SSS1.p3.2.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p3.2.m1.2.2\" xref=\"S5.SS2.SSS1.p3.2.m1.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS1.p3.2.m1.3.4.2.3\" xref=\"S5.SS2.SSS1.p3.2.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p3.2.m1.3.3\" mathvariant=\"normal\" xref=\"S5.SS2.SSS1.p3.2.m1.3.3.cmml\">?</mi><mo id=\"S5.SS2.SSS1.p3.2.m1.3.4.2.4\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p3.2.m1.3.4.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p3.2.m1.3b\"><vector id=\"S5.SS2.SSS1.p3.2.m1.3.4.1.cmml\" xref=\"S5.SS2.SSS1.p3.2.m1.3.4.2\"><ci id=\"S5.SS2.SSS1.p3.2.m1.1.1.cmml\" xref=\"S5.SS2.SSS1.p3.2.m1.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS1.p3.2.m1.2.2.cmml\" xref=\"S5.SS2.SSS1.p3.2.m1.2.2\">𝑟</ci><ci id=\"S5.SS2.SSS1.p3.2.m1.3.3.cmml\" xref=\"S5.SS2.SSS1.p3.2.m1.3.3\">?</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p3.2.m1.3c\">(h,r,?)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p3.2.m1.3d\">( italic_h , italic_r , ? )</annotation></semantics></math>. EDP first encodes the triple with Eq.&nbsp;<a href=\"https://arxiv.org/html/2306.08302v3#S5.E8\" title=\"8 ‣ 5.2.1 LLM as Encoders (PaE). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>8</span></a> and generates the final hidden state, which is then forwarded into ITP as an embedding of the head entity in Eq.&nbsp;<a href=\"https://arxiv.org/html/2306.08302v3#S5.E7\" title=\"7 ‣ 5.2.1 LLM as Encoders (PaE). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>7</span></a> to predict target entities.</p>\n</div>\n<div id=\"S5.SS2.SSS1.p4\">\n<p id=\"S5.SS2.SSS1.p4.3\"><span id=\"S5.SS2.SSS1.p4.3.1\">Separated Encoding.</span> As shown in Fig.&nbsp;<a href=\"https://arxiv.org/html/2306.08302v3#S5.F16\" title=\"Figure 16 ‣ 5.2.1 LLM as Encoders (PaE). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>16</span></a>(c), these methods involve partitioning a triple <math alttext=\"(h,r,t)\" display=\"inline\" id=\"S5.SS2.SSS1.p4.1.m1.3\"><semantics id=\"S5.SS2.SSS1.p4.1.m1.3a\"><mrow id=\"S5.SS2.SSS1.p4.1.m1.3.4.2\" xref=\"S5.SS2.SSS1.p4.1.m1.3.4.1.cmml\"><mo id=\"S5.SS2.SSS1.p4.1.m1.3.4.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p4.1.m1.3.4.1.cmml\">(</mo><mi id=\"S5.SS2.SSS1.p4.1.m1.1.1\" xref=\"S5.SS2.SSS1.p4.1.m1.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS1.p4.1.m1.3.4.2.2\" xref=\"S5.SS2.SSS1.p4.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p4.1.m1.2.2\" xref=\"S5.SS2.SSS1.p4.1.m1.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS1.p4.1.m1.3.4.2.3\" xref=\"S5.SS2.SSS1.p4.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p4.1.m1.3.3\" xref=\"S5.SS2.SSS1.p4.1.m1.3.3.cmml\">t</mi><mo id=\"S5.SS2.SSS1.p4.1.m1.3.4.2.4\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p4.1.m1.3.4.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p4.1.m1.3b\"><vector id=\"S5.SS2.SSS1.p4.1.m1.3.4.1.cmml\" xref=\"S5.SS2.SSS1.p4.1.m1.3.4.2\"><ci id=\"S5.SS2.SSS1.p4.1.m1.1.1.cmml\" xref=\"S5.SS2.SSS1.p4.1.m1.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS1.p4.1.m1.2.2.cmml\" xref=\"S5.SS2.SSS1.p4.1.m1.2.2\">𝑟</ci><ci id=\"S5.SS2.SSS1.p4.1.m1.3.3.cmml\" xref=\"S5.SS2.SSS1.p4.1.m1.3.3\">𝑡</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p4.1.m1.3c\">(h,r,t)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p4.1.m1.3d\">( italic_h , italic_r , italic_t )</annotation></semantics></math> into two distinct parts, i.e. <math alttext=\"(h,r)\" display=\"inline\" id=\"S5.SS2.SSS1.p4.2.m2.2\"><semantics id=\"S5.SS2.SSS1.p4.2.m2.2a\"><mrow id=\"S5.SS2.SSS1.p4.2.m2.2.3.2\" xref=\"S5.SS2.SSS1.p4.2.m2.2.3.1.cmml\"><mo id=\"S5.SS2.SSS1.p4.2.m2.2.3.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p4.2.m2.2.3.1.cmml\">(</mo><mi id=\"S5.SS2.SSS1.p4.2.m2.1.1\" xref=\"S5.SS2.SSS1.p4.2.m2.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS1.p4.2.m2.2.3.2.2\" xref=\"S5.SS2.SSS1.p4.2.m2.2.3.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p4.2.m2.2.2\" xref=\"S5.SS2.SSS1.p4.2.m2.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS1.p4.2.m2.2.3.2.3\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p4.2.m2.2.3.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p4.2.m2.2b\"><interval closure=\"open\" id=\"S5.SS2.SSS1.p4.2.m2.2.3.1.cmml\" xref=\"S5.SS2.SSS1.p4.2.m2.2.3.2\"><ci id=\"S5.SS2.SSS1.p4.2.m2.1.1.cmml\" xref=\"S5.SS2.SSS1.p4.2.m2.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS1.p4.2.m2.2.2.cmml\" xref=\"S5.SS2.SSS1.p4.2.m2.2.2\">𝑟</ci></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p4.2.m2.2c\">(h,r)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p4.2.m2.2d\">( italic_h , italic_r )</annotation></semantics></math> and <math alttext=\"t\" display=\"inline\" id=\"S5.SS2.SSS1.p4.3.m3.1\"><semantics id=\"S5.SS2.SSS1.p4.3.m3.1a\"><mi id=\"S5.SS2.SSS1.p4.3.m3.1.1\" xref=\"S5.SS2.SSS1.p4.3.m3.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p4.3.m3.1b\"><ci id=\"S5.SS2.SSS1.p4.3.m3.1.1.cmml\" xref=\"S5.SS2.SSS1.p4.3.m3.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p4.3.m3.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p4.3.m3.1d\">italic_t</annotation></semantics></math>, which can be expressed as</p>\n<table id=\"A1.EGx2\">\n<tbody id=\"S5.E9\"><tr>\n<td></td>\n<td><math alttext=\"\\displaystyle x_{(h,r)}\" display=\"inline\" id=\"S5.E9.m1.2\"><semantics id=\"S5.E9.m1.2a\"><msub id=\"S5.E9.m1.2.3\" xref=\"S5.E9.m1.2.3.cmml\"><mi id=\"S5.E9.m1.2.3.2\" xref=\"S5.E9.m1.2.3.2.cmml\">x</mi><mrow id=\"S5.E9.m1.2.2.2.4\" xref=\"S5.E9.m1.2.2.2.3.cmml\"><mo id=\"S5.E9.m1.2.2.2.4.1\" stretchy=\"false\" xref=\"S5.E9.m1.2.2.2.3.cmml\">(</mo><mi id=\"S5.E9.m1.1.1.1.1\" xref=\"S5.E9.m1.1.1.1.1.cmml\">h</mi><mo id=\"S5.E9.m1.2.2.2.4.2\" xref=\"S5.E9.m1.2.2.2.3.cmml\">,</mo><mi id=\"S5.E9.m1.2.2.2.2\" xref=\"S5.E9.m1.2.2.2.2.cmml\">r</mi><mo id=\"S5.E9.m1.2.2.2.4.3\" stretchy=\"false\" xref=\"S5.E9.m1.2.2.2.3.cmml\">)</mo></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.E9.m1.2b\"><apply id=\"S5.E9.m1.2.3.cmml\" xref=\"S5.E9.m1.2.3\"><csymbol cd=\"ambiguous\" id=\"S5.E9.m1.2.3.1.cmml\" xref=\"S5.E9.m1.2.3\">subscript</csymbol><ci id=\"S5.E9.m1.2.3.2.cmml\" xref=\"S5.E9.m1.2.3.2\">𝑥</ci><interval closure=\"open\" id=\"S5.E9.m1.2.2.2.3.cmml\" xref=\"S5.E9.m1.2.2.2.4\"><ci id=\"S5.E9.m1.1.1.1.1.cmml\" xref=\"S5.E9.m1.1.1.1.1\">ℎ</ci><ci id=\"S5.E9.m1.2.2.2.2.cmml\" xref=\"S5.E9.m1.2.2.2.2\">𝑟</ci></interval></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E9.m1.2c\">\\displaystyle x_{(h,r)}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E9.m1.2d\">italic_x start_POSTSUBSCRIPT ( italic_h , italic_r ) end_POSTSUBSCRIPT</annotation></semantics></math></td>\n<td><math alttext=\"\\displaystyle=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}%\n\\ \\texttt{[SEP]},\" display=\"inline\" id=\"S5.E9.m2.1\"><semantics id=\"S5.E9.m2.1a\"><mrow id=\"S5.E9.m2.1.1.1\" xref=\"S5.E9.m2.1.1.1.1.cmml\"><mrow id=\"S5.E9.m2.1.1.1.1\" xref=\"S5.E9.m2.1.1.1.1.cmml\"><mi id=\"S5.E9.m2.1.1.1.1.2\" xref=\"S5.E9.m2.1.1.1.1.2.cmml\"></mi><mo id=\"S5.E9.m2.1.1.1.1.1\" xref=\"S5.E9.m2.1.1.1.1.1.cmml\">=</mo><mrow id=\"S5.E9.m2.1.1.1.1.3\" xref=\"S5.E9.m2.1.1.1.1.3.cmml\"><mtext id=\"S5.E9.m2.1.1.1.1.3.2\" mathvariant=\"monospace\" xref=\"S5.E9.m2.1.1.1.1.3.2a.cmml\">[CLS]</mtext><mo id=\"S5.E9.m2.1.1.1.1.3.1\" lspace=\"0.500em\" xref=\"S5.E9.m2.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E9.m2.1.1.1.1.3.3\" xref=\"S5.E9.m2.1.1.1.1.3.3.cmml\"><mtext id=\"S5.E9.m2.1.1.1.1.3.3.2\" xref=\"S5.E9.m2.1.1.1.1.3.3.2a.cmml\">Text</mtext><mi id=\"S5.E9.m2.1.1.1.1.3.3.3\" xref=\"S5.E9.m2.1.1.1.1.3.3.3.cmml\">h</mi></msub><mo id=\"S5.E9.m2.1.1.1.1.3.1a\" xref=\"S5.E9.m2.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E9.m2.1.1.1.1.3.4\" mathvariant=\"monospace\" xref=\"S5.E9.m2.1.1.1.1.3.4a.cmml\">[SEP]</mtext><mo id=\"S5.E9.m2.1.1.1.1.3.1b\" lspace=\"0.500em\" xref=\"S5.E9.m2.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E9.m2.1.1.1.1.3.5\" xref=\"S5.E9.m2.1.1.1.1.3.5.cmml\"><mtext id=\"S5.E9.m2.1.1.1.1.3.5.2\" xref=\"S5.E9.m2.1.1.1.1.3.5.2a.cmml\">Text</mtext><mi id=\"S5.E9.m2.1.1.1.1.3.5.3\" xref=\"S5.E9.m2.1.1.1.1.3.5.3.cmml\">r</mi></msub><mo id=\"S5.E9.m2.1.1.1.1.3.1c\" xref=\"S5.E9.m2.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E9.m2.1.1.1.1.3.6\" mathvariant=\"monospace\" xref=\"S5.E9.m2.1.1.1.1.3.6a.cmml\">[SEP]</mtext></mrow></mrow><mo id=\"S5.E9.m2.1.1.1.2\" xref=\"S5.E9.m2.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E9.m2.1b\"><apply id=\"S5.E9.m2.1.1.1.1.cmml\" xref=\"S5.E9.m2.1.1.1\"><eq id=\"S5.E9.m2.1.1.1.1.1.cmml\" xref=\"S5.E9.m2.1.1.1.1.1\"></eq><csymbol cd=\"latexml\" id=\"S5.E9.m2.1.1.1.1.2.cmml\" xref=\"S5.E9.m2.1.1.1.1.2\">absent</csymbol><apply id=\"S5.E9.m2.1.1.1.1.3.cmml\" xref=\"S5.E9.m2.1.1.1.1.3\"><times id=\"S5.E9.m2.1.1.1.1.3.1.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.1\"></times><ci id=\"S5.E9.m2.1.1.1.1.3.2a.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.2\"><mtext id=\"S5.E9.m2.1.1.1.1.3.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E9.m2.1.1.1.1.3.2\">[CLS]</mtext></ci><apply id=\"S5.E9.m2.1.1.1.1.3.3.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E9.m2.1.1.1.1.3.3.1.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.3\">subscript</csymbol><ci id=\"S5.E9.m2.1.1.1.1.3.3.2a.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.3.2\"><mtext id=\"S5.E9.m2.1.1.1.1.3.3.2.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.3.2\">Text</mtext></ci><ci id=\"S5.E9.m2.1.1.1.1.3.3.3.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.3.3\">ℎ</ci></apply><ci id=\"S5.E9.m2.1.1.1.1.3.4a.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.4\"><mtext id=\"S5.E9.m2.1.1.1.1.3.4.cmml\" mathvariant=\"monospace\" xref=\"S5.E9.m2.1.1.1.1.3.4\">[SEP]</mtext></ci><apply id=\"S5.E9.m2.1.1.1.1.3.5.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.5\"><csymbol cd=\"ambiguous\" id=\"S5.E9.m2.1.1.1.1.3.5.1.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.5\">subscript</csymbol><ci id=\"S5.E9.m2.1.1.1.1.3.5.2a.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.5.2\"><mtext id=\"S5.E9.m2.1.1.1.1.3.5.2.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.5.2\">Text</mtext></ci><ci id=\"S5.E9.m2.1.1.1.1.3.5.3.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.5.3\">𝑟</ci></apply><ci id=\"S5.E9.m2.1.1.1.1.3.6a.cmml\" xref=\"S5.E9.m2.1.1.1.1.3.6\"><mtext id=\"S5.E9.m2.1.1.1.1.3.6.cmml\" mathvariant=\"monospace\" xref=\"S5.E9.m2.1.1.1.1.3.6\">[SEP]</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E9.m2.1c\">\\displaystyle=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}%\n\\ \\texttt{[SEP]},</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E9.m2.1d\">= [CLS] Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] Text start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT [SEP] ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(9)</span></td>\n</tr></tbody>\n<tbody id=\"S5.E10\"><tr>\n<td></td>\n<td><math alttext=\"\\displaystyle x_{t}\" display=\"inline\" id=\"S5.E10.m1.1\"><semantics id=\"S5.E10.m1.1a\"><msub id=\"S5.E10.m1.1.1\" xref=\"S5.E10.m1.1.1.cmml\"><mi id=\"S5.E10.m1.1.1.2\" xref=\"S5.E10.m1.1.1.2.cmml\">x</mi><mi id=\"S5.E10.m1.1.1.3\" xref=\"S5.E10.m1.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.E10.m1.1b\"><apply id=\"S5.E10.m1.1.1.cmml\" xref=\"S5.E10.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E10.m1.1.1.1.cmml\" xref=\"S5.E10.m1.1.1\">subscript</csymbol><ci id=\"S5.E10.m1.1.1.2.cmml\" xref=\"S5.E10.m1.1.1.2\">𝑥</ci><ci id=\"S5.E10.m1.1.1.3.cmml\" xref=\"S5.E10.m1.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E10.m1.1c\">\\displaystyle x_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E10.m1.1d\">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math></td>\n<td><math alttext=\"\\displaystyle=\\texttt{[CLS]}\\ \\text{Text}_{t}\\ \\texttt{[SEP]}.\" display=\"inline\" id=\"S5.E10.m2.1\"><semantics id=\"S5.E10.m2.1a\"><mrow id=\"S5.E10.m2.1.1.1\" xref=\"S5.E10.m2.1.1.1.1.cmml\"><mrow id=\"S5.E10.m2.1.1.1.1\" xref=\"S5.E10.m2.1.1.1.1.cmml\"><mi id=\"S5.E10.m2.1.1.1.1.2\" xref=\"S5.E10.m2.1.1.1.1.2.cmml\"></mi><mo id=\"S5.E10.m2.1.1.1.1.1\" xref=\"S5.E10.m2.1.1.1.1.1.cmml\">=</mo><mrow id=\"S5.E10.m2.1.1.1.1.3\" xref=\"S5.E10.m2.1.1.1.1.3.cmml\"><mtext id=\"S5.E10.m2.1.1.1.1.3.2\" mathvariant=\"monospace\" xref=\"S5.E10.m2.1.1.1.1.3.2a.cmml\">[CLS]</mtext><mo id=\"S5.E10.m2.1.1.1.1.3.1\" lspace=\"0.500em\" xref=\"S5.E10.m2.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E10.m2.1.1.1.1.3.3\" xref=\"S5.E10.m2.1.1.1.1.3.3.cmml\"><mtext id=\"S5.E10.m2.1.1.1.1.3.3.2\" xref=\"S5.E10.m2.1.1.1.1.3.3.2a.cmml\">Text</mtext><mi id=\"S5.E10.m2.1.1.1.1.3.3.3\" xref=\"S5.E10.m2.1.1.1.1.3.3.3.cmml\">t</mi></msub><mo id=\"S5.E10.m2.1.1.1.1.3.1a\" xref=\"S5.E10.m2.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E10.m2.1.1.1.1.3.4\" mathvariant=\"monospace\" xref=\"S5.E10.m2.1.1.1.1.3.4a.cmml\">[SEP]</mtext></mrow></mrow><mo id=\"S5.E10.m2.1.1.1.2\" lspace=\"0em\" xref=\"S5.E10.m2.1.1.1.1.cmml\">.</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E10.m2.1b\"><apply id=\"S5.E10.m2.1.1.1.1.cmml\" xref=\"S5.E10.m2.1.1.1\"><eq id=\"S5.E10.m2.1.1.1.1.1.cmml\" xref=\"S5.E10.m2.1.1.1.1.1\"></eq><csymbol cd=\"latexml\" id=\"S5.E10.m2.1.1.1.1.2.cmml\" xref=\"S5.E10.m2.1.1.1.1.2\">absent</csymbol><apply id=\"S5.E10.m2.1.1.1.1.3.cmml\" xref=\"S5.E10.m2.1.1.1.1.3\"><times id=\"S5.E10.m2.1.1.1.1.3.1.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.1\"></times><ci id=\"S5.E10.m2.1.1.1.1.3.2a.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.2\"><mtext id=\"S5.E10.m2.1.1.1.1.3.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E10.m2.1.1.1.1.3.2\">[CLS]</mtext></ci><apply id=\"S5.E10.m2.1.1.1.1.3.3.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E10.m2.1.1.1.1.3.3.1.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.3\">subscript</csymbol><ci id=\"S5.E10.m2.1.1.1.1.3.3.2a.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.3.2\"><mtext id=\"S5.E10.m2.1.1.1.1.3.3.2.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.3.2\">Text</mtext></ci><ci id=\"S5.E10.m2.1.1.1.1.3.3.3.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.3.3\">𝑡</ci></apply><ci id=\"S5.E10.m2.1.1.1.1.3.4a.cmml\" xref=\"S5.E10.m2.1.1.1.1.3.4\"><mtext id=\"S5.E10.m2.1.1.1.1.3.4.cmml\" mathvariant=\"monospace\" xref=\"S5.E10.m2.1.1.1.1.3.4\">[SEP]</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E10.m2.1c\">\\displaystyle=\\texttt{[CLS]}\\ \\text{Text}_{t}\\ \\texttt{[SEP]}.</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E10.m2.1d\">= [CLS] Text start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [SEP] .</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(10)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS2.SSS1.p4.6\">Then the two parts are encoded separately by LLMs, and the final hidden states of the <span id=\"S5.SS2.SSS1.p4.6.1\">[CLS]</span> tokens are used as the representations of <math alttext=\"(h,r)\" display=\"inline\" id=\"S5.SS2.SSS1.p4.5.m2.2\"><semantics id=\"S5.SS2.SSS1.p4.5.m2.2a\"><mrow id=\"S5.SS2.SSS1.p4.5.m2.2.3.2\" xref=\"S5.SS2.SSS1.p4.5.m2.2.3.1.cmml\"><mo id=\"S5.SS2.SSS1.p4.5.m2.2.3.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p4.5.m2.2.3.1.cmml\">(</mo><mi id=\"S5.SS2.SSS1.p4.5.m2.1.1\" xref=\"S5.SS2.SSS1.p4.5.m2.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS1.p4.5.m2.2.3.2.2\" xref=\"S5.SS2.SSS1.p4.5.m2.2.3.1.cmml\">,</mo><mi id=\"S5.SS2.SSS1.p4.5.m2.2.2\" xref=\"S5.SS2.SSS1.p4.5.m2.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS1.p4.5.m2.2.3.2.3\" stretchy=\"false\" xref=\"S5.SS2.SSS1.p4.5.m2.2.3.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p4.5.m2.2b\"><interval closure=\"open\" id=\"S5.SS2.SSS1.p4.5.m2.2.3.1.cmml\" xref=\"S5.SS2.SSS1.p4.5.m2.2.3.2\"><ci id=\"S5.SS2.SSS1.p4.5.m2.1.1.cmml\" xref=\"S5.SS2.SSS1.p4.5.m2.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS1.p4.5.m2.2.2.cmml\" xref=\"S5.SS2.SSS1.p4.5.m2.2.2\">𝑟</ci></interval></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p4.5.m2.2c\">(h,r)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p4.5.m2.2d\">( italic_h , italic_r )</annotation></semantics></math> and <math alttext=\"t\" display=\"inline\" id=\"S5.SS2.SSS1.p4.6.m3.1\"><semantics id=\"S5.SS2.SSS1.p4.6.m3.1a\"><mi id=\"S5.SS2.SSS1.p4.6.m3.1.1\" xref=\"S5.SS2.SSS1.p4.6.m3.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p4.6.m3.1b\"><ci id=\"S5.SS2.SSS1.p4.6.m3.1.1.cmml\" xref=\"S5.SS2.SSS1.p4.6.m3.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p4.6.m3.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p4.6.m3.1d\">italic_t</annotation></semantics></math>, respectively. The representations are then fed into a scoring function to predict the possibility of the triple, formulated as</p>\n<table id=\"S5.E11\">\n<tbody><tr>\n<td></td>\n<td><math alttext=\"s=f_{score}(e_{(h,r)},e_{t}),\" display=\"block\" id=\"S5.E11.m1.3\"><semantics id=\"S5.E11.m1.3a\"><mrow id=\"S5.E11.m1.3.3.1\" xref=\"S5.E11.m1.3.3.1.1.cmml\"><mrow id=\"S5.E11.m1.3.3.1.1\" xref=\"S5.E11.m1.3.3.1.1.cmml\"><mi id=\"S5.E11.m1.3.3.1.1.4\" xref=\"S5.E11.m1.3.3.1.1.4.cmml\">s</mi><mo id=\"S5.E11.m1.3.3.1.1.3\" xref=\"S5.E11.m1.3.3.1.1.3.cmml\">=</mo><mrow id=\"S5.E11.m1.3.3.1.1.2\" xref=\"S5.E11.m1.3.3.1.1.2.cmml\"><msub id=\"S5.E11.m1.3.3.1.1.2.4\" xref=\"S5.E11.m1.3.3.1.1.2.4.cmml\"><mi id=\"S5.E11.m1.3.3.1.1.2.4.2\" xref=\"S5.E11.m1.3.3.1.1.2.4.2.cmml\">f</mi><mrow id=\"S5.E11.m1.3.3.1.1.2.4.3\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.cmml\"><mi id=\"S5.E11.m1.3.3.1.1.2.4.3.2\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.2.cmml\">s</mi><mo id=\"S5.E11.m1.3.3.1.1.2.4.3.1\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.1.cmml\">⁢</mo><mi id=\"S5.E11.m1.3.3.1.1.2.4.3.3\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.3.cmml\">c</mi><mo id=\"S5.E11.m1.3.3.1.1.2.4.3.1a\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.1.cmml\">⁢</mo><mi id=\"S5.E11.m1.3.3.1.1.2.4.3.4\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.4.cmml\">o</mi><mo id=\"S5.E11.m1.3.3.1.1.2.4.3.1b\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.1.cmml\">⁢</mo><mi id=\"S5.E11.m1.3.3.1.1.2.4.3.5\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.5.cmml\">r</mi><mo id=\"S5.E11.m1.3.3.1.1.2.4.3.1c\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.1.cmml\">⁢</mo><mi id=\"S5.E11.m1.3.3.1.1.2.4.3.6\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.6.cmml\">e</mi></mrow></msub><mo id=\"S5.E11.m1.3.3.1.1.2.3\" xref=\"S5.E11.m1.3.3.1.1.2.3.cmml\">⁢</mo><mrow id=\"S5.E11.m1.3.3.1.1.2.2.2\" xref=\"S5.E11.m1.3.3.1.1.2.2.3.cmml\"><mo id=\"S5.E11.m1.3.3.1.1.2.2.2.3\" stretchy=\"false\" xref=\"S5.E11.m1.3.3.1.1.2.2.3.cmml\">(</mo><msub id=\"S5.E11.m1.3.3.1.1.1.1.1.1\" xref=\"S5.E11.m1.3.3.1.1.1.1.1.1.cmml\"><mi id=\"S5.E11.m1.3.3.1.1.1.1.1.1.2\" xref=\"S5.E11.m1.3.3.1.1.1.1.1.1.2.cmml\">e</mi><mrow id=\"S5.E11.m1.2.2.2.4\" xref=\"S5.E11.m1.2.2.2.3.cmml\"><mo id=\"S5.E11.m1.2.2.2.4.1\" stretchy=\"false\" xref=\"S5.E11.m1.2.2.2.3.cmml\">(</mo><mi id=\"S5.E11.m1.1.1.1.1\" xref=\"S5.E11.m1.1.1.1.1.cmml\">h</mi><mo id=\"S5.E11.m1.2.2.2.4.2\" xref=\"S5.E11.m1.2.2.2.3.cmml\">,</mo><mi id=\"S5.E11.m1.2.2.2.2\" xref=\"S5.E11.m1.2.2.2.2.cmml\">r</mi><mo id=\"S5.E11.m1.2.2.2.4.3\" stretchy=\"false\" xref=\"S5.E11.m1.2.2.2.3.cmml\">)</mo></mrow></msub><mo id=\"S5.E11.m1.3.3.1.1.2.2.2.4\" xref=\"S5.E11.m1.3.3.1.1.2.2.3.cmml\">,</mo><msub id=\"S5.E11.m1.3.3.1.1.2.2.2.2\" xref=\"S5.E11.m1.3.3.1.1.2.2.2.2.cmml\"><mi id=\"S5.E11.m1.3.3.1.1.2.2.2.2.2\" xref=\"S5.E11.m1.3.3.1.1.2.2.2.2.2.cmml\">e</mi><mi id=\"S5.E11.m1.3.3.1.1.2.2.2.2.3\" xref=\"S5.E11.m1.3.3.1.1.2.2.2.2.3.cmml\">t</mi></msub><mo id=\"S5.E11.m1.3.3.1.1.2.2.2.5\" stretchy=\"false\" xref=\"S5.E11.m1.3.3.1.1.2.2.3.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E11.m1.3.3.1.2\" xref=\"S5.E11.m1.3.3.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E11.m1.3b\"><apply id=\"S5.E11.m1.3.3.1.1.cmml\" xref=\"S5.E11.m1.3.3.1\"><eq id=\"S5.E11.m1.3.3.1.1.3.cmml\" xref=\"S5.E11.m1.3.3.1.1.3\"></eq><ci id=\"S5.E11.m1.3.3.1.1.4.cmml\" xref=\"S5.E11.m1.3.3.1.1.4\">𝑠</ci><apply id=\"S5.E11.m1.3.3.1.1.2.cmml\" xref=\"S5.E11.m1.3.3.1.1.2\"><times id=\"S5.E11.m1.3.3.1.1.2.3.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.3\"></times><apply id=\"S5.E11.m1.3.3.1.1.2.4.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4\"><csymbol cd=\"ambiguous\" id=\"S5.E11.m1.3.3.1.1.2.4.1.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4\">subscript</csymbol><ci id=\"S5.E11.m1.3.3.1.1.2.4.2.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.2\">𝑓</ci><apply id=\"S5.E11.m1.3.3.1.1.2.4.3.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.3\"><times id=\"S5.E11.m1.3.3.1.1.2.4.3.1.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.1\"></times><ci id=\"S5.E11.m1.3.3.1.1.2.4.3.2.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.2\">𝑠</ci><ci id=\"S5.E11.m1.3.3.1.1.2.4.3.3.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.3\">𝑐</ci><ci id=\"S5.E11.m1.3.3.1.1.2.4.3.4.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.4\">𝑜</ci><ci id=\"S5.E11.m1.3.3.1.1.2.4.3.5.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.5\">𝑟</ci><ci id=\"S5.E11.m1.3.3.1.1.2.4.3.6.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.4.3.6\">𝑒</ci></apply></apply><interval closure=\"open\" id=\"S5.E11.m1.3.3.1.1.2.2.3.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.2.2\"><apply id=\"S5.E11.m1.3.3.1.1.1.1.1.1.cmml\" xref=\"S5.E11.m1.3.3.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E11.m1.3.3.1.1.1.1.1.1.1.cmml\" xref=\"S5.E11.m1.3.3.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S5.E11.m1.3.3.1.1.1.1.1.1.2.cmml\" xref=\"S5.E11.m1.3.3.1.1.1.1.1.1.2\">𝑒</ci><interval closure=\"open\" id=\"S5.E11.m1.2.2.2.3.cmml\" xref=\"S5.E11.m1.2.2.2.4\"><ci id=\"S5.E11.m1.1.1.1.1.cmml\" xref=\"S5.E11.m1.1.1.1.1\">ℎ</ci><ci id=\"S5.E11.m1.2.2.2.2.cmml\" xref=\"S5.E11.m1.2.2.2.2\">𝑟</ci></interval></apply><apply id=\"S5.E11.m1.3.3.1.1.2.2.2.2.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E11.m1.3.3.1.1.2.2.2.2.1.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.2.2.2\">subscript</csymbol><ci id=\"S5.E11.m1.3.3.1.1.2.2.2.2.2.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.2.2.2.2\">𝑒</ci><ci id=\"S5.E11.m1.3.3.1.1.2.2.2.2.3.cmml\" xref=\"S5.E11.m1.3.3.1.1.2.2.2.2.3\">𝑡</ci></apply></interval></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E11.m1.3c\">s=f_{score}(e_{(h,r)},e_{t}),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E11.m1.3d\">italic_s = italic_f start_POSTSUBSCRIPT italic_s italic_c italic_o italic_r italic_e end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT ( italic_h , italic_r ) end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(11)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS2.SSS1.p4.7\">where <math alttext=\"f_{score}\" display=\"inline\" id=\"S5.SS2.SSS1.p4.7.m1.1\"><semantics id=\"S5.SS2.SSS1.p4.7.m1.1a\"><msub id=\"S5.SS2.SSS1.p4.7.m1.1.1\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.cmml\"><mi id=\"S5.SS2.SSS1.p4.7.m1.1.1.2\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.2.cmml\">f</mi><mrow id=\"S5.SS2.SSS1.p4.7.m1.1.1.3\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.cmml\"><mi id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.2\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.2.cmml\">s</mi><mo id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.3\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.3.cmml\">c</mi><mo id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1a\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.4\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.4.cmml\">o</mi><mo id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1b\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.5\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.5.cmml\">r</mi><mo id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1c\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.6\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.6.cmml\">e</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS1.p4.7.m1.1b\"><apply id=\"S5.SS2.SSS1.p4.7.m1.1.1.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS2.SSS1.p4.7.m1.1.1.1.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1\">subscript</csymbol><ci id=\"S5.SS2.SSS1.p4.7.m1.1.1.2.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.2\">𝑓</ci><apply id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3\"><times id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.1\"></times><ci id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.2.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.2\">𝑠</ci><ci id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.3.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.3\">𝑐</ci><ci id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.4.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.4\">𝑜</ci><ci id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.5.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.5\">𝑟</ci><ci id=\"S5.SS2.SSS1.p4.7.m1.1.1.3.6.cmml\" xref=\"S5.SS2.SSS1.p4.7.m1.1.1.3.6\">𝑒</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS1.p4.7.m1.1c\">f_{score}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS1.p4.7.m1.1d\">italic_f start_POSTSUBSCRIPT italic_s italic_c italic_o italic_r italic_e end_POSTSUBSCRIPT</annotation></semantics></math> denotes the score function like TransE.</p>\n</div>\n<p id=\"S5.SS2.SSS1.p5.1\">StAR&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib143\" title=\"\">143</a>]</cite> applies Siamese-style textual encoders on their text, encoding them into separate contextualized representations. To avoid the combinatorial explosion of textual encoding approaches, e.g., KG-BERT, StAR employs a scoring module that involves both deterministic classifier and spatial measurement for representation and structure learning respectively, which also enhances structured knowledge by exploring the spatial characteristics. SimKGC&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib144\" title=\"\">144</a>]</cite> is another instance of leveraging a Siamese textual encoder to encode textual representations. Following the encoding process, SimKGC applies contrastive learning techniques to these representations. This process involves computing the similarity between the encoded representations of a given triple and its positive and negative samples. In particular, the similarity between the encoded representation of the triple and the positive sample is maximized, while the similarity between the encoded representation of the triple and the negative sample is minimized. This enables SimKGC to learn a representation space that separates plausible and implausible triples. To avoid overfitting textural information, CSPromp-KG&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib186\" title=\"\">186</a>]</cite> employs parameter-efficient prompt learning for KGC.</p>\n<p id=\"S5.SS2.SSS1.p6.1\">LP-BERT <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib145\" title=\"\">145</a>]</cite> is a hybrid KGC method that combines both MLM Encoding and Separated Encoding. This approach consists of two stages, namely pre-training and fine-tuning. During pre-training, the method utilizes the standard MLM mechanism to pre-train a LLM with KGC data. During the fine-tuning stage, the LLM encodes both parts and is optimized using a contrastive learning strategy (similar to SimKGC&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib144\" title=\"\">144</a>]</cite>).\n</p>\n</section>\n<section id=\"S5.SS2.SSS2\">\n<h4>\n<span>5.2.2 </span>LLM as Generators (PaG).</h4>\n<figure id=\"S5.F17\"><img alt=\"Refer to caption\" height=\"648\" id=\"S5.F17.g1\" src=\"x14.png\" width=\"664\">\n<figcaption><span>Figure 17: </span>The general framework of adopting LLMs as decoders (PaG) for KG Completion. The En. and De. denote the encoder and decoder, respectively.</figcaption>\n</figure>\n<p id=\"S5.SS2.SSS2.p1.2\">Recent works use LLMs as sequence-to-sequence generators in KGC. As presented in Fig.&nbsp;<a href=\"https://arxiv.org/html/2306.08302v3#S5.F17\" title=\"Figure 17 ‣ 5.2.2 LLM as Generators (PaG). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>17</span></a> (a) and (b), these approaches involve encoder-decoder or decoder-only LLMs. The LLMs receive a sequence text input of the query triple <math alttext=\"(h,r,?)\" display=\"inline\" id=\"S5.SS2.SSS2.p1.1.m1.3\"><semantics id=\"S5.SS2.SSS2.p1.1.m1.3a\"><mrow id=\"S5.SS2.SSS2.p1.1.m1.3.4.2\" xref=\"S5.SS2.SSS2.p1.1.m1.3.4.1.cmml\"><mo id=\"S5.SS2.SSS2.p1.1.m1.3.4.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS2.p1.1.m1.3.4.1.cmml\">(</mo><mi id=\"S5.SS2.SSS2.p1.1.m1.1.1\" xref=\"S5.SS2.SSS2.p1.1.m1.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS2.p1.1.m1.3.4.2.2\" xref=\"S5.SS2.SSS2.p1.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS2.p1.1.m1.2.2\" xref=\"S5.SS2.SSS2.p1.1.m1.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS2.p1.1.m1.3.4.2.3\" xref=\"S5.SS2.SSS2.p1.1.m1.3.4.1.cmml\">,</mo><mi id=\"S5.SS2.SSS2.p1.1.m1.3.3\" mathvariant=\"normal\" xref=\"S5.SS2.SSS2.p1.1.m1.3.3.cmml\">?</mi><mo id=\"S5.SS2.SSS2.p1.1.m1.3.4.2.4\" stretchy=\"false\" xref=\"S5.SS2.SSS2.p1.1.m1.3.4.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS2.p1.1.m1.3b\"><vector id=\"S5.SS2.SSS2.p1.1.m1.3.4.1.cmml\" xref=\"S5.SS2.SSS2.p1.1.m1.3.4.2\"><ci id=\"S5.SS2.SSS2.p1.1.m1.1.1.cmml\" xref=\"S5.SS2.SSS2.p1.1.m1.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS2.p1.1.m1.2.2.cmml\" xref=\"S5.SS2.SSS2.p1.1.m1.2.2\">𝑟</ci><ci id=\"S5.SS2.SSS2.p1.1.m1.3.3.cmml\" xref=\"S5.SS2.SSS2.p1.1.m1.3.3\">?</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS2.p1.1.m1.3c\">(h,r,?)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS2.p1.1.m1.3d\">( italic_h , italic_r , ? )</annotation></semantics></math>, and generate the text of tail entity <math alttext=\"t\" display=\"inline\" id=\"S5.SS2.SSS2.p1.2.m2.1\"><semantics id=\"S5.SS2.SSS2.p1.2.m2.1a\"><mi id=\"S5.SS2.SSS2.p1.2.m2.1.1\" xref=\"S5.SS2.SSS2.p1.2.m2.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS2.p1.2.m2.1b\"><ci id=\"S5.SS2.SSS2.p1.2.m2.1.1.cmml\" xref=\"S5.SS2.SSS2.p1.2.m2.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS2.p1.2.m2.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS2.p1.2.m2.1d\">italic_t</annotation></semantics></math> directly.</p>\n<p id=\"S5.SS2.SSS2.p2.2\">GenKGC&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib96\" title=\"\">96</a>]</cite> uses the large language model BART&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib5\" title=\"\">5</a>]</cite> as the backbone model. Inspired by the in-context learning approach used in GPT-3&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib59\" title=\"\">59</a>]</cite>, where the model concatenates relevant samples to learn correct output answers, GenKGC proposes a relation-guided demonstration technique that includes triples with the same relation to facilitating the model’s learning process. In addition, during generation, an entity-aware hierarchical decoding method is proposed to reduce the time complexity. KGT5&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib146\" title=\"\">146</a>]</cite> introduces a novel KGC model that fulfils four key requirements of such models: scalability, quality, versatility, and simplicity. To address these objectives, the proposed model employs a straightforward T5 small architecture. The model is distinct from previous KGC methods, in which it is randomly initialized rather than using pre-trained models. KG-S2S&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib147\" title=\"\">147</a>]</cite> is a comprehensive framework that can be applied to various types of KGC tasks, including Static KGC, Temporal KGC, and Few-shot KGC. To achieve this objective, KG-S2S reformulates the standard triple KG fact by introducing an additional element, forming a quadruple <math alttext=\"(h,r,t,m)\" display=\"inline\" id=\"S5.SS2.SSS2.p2.1.m1.4\"><semantics id=\"S5.SS2.SSS2.p2.1.m1.4a\"><mrow id=\"S5.SS2.SSS2.p2.1.m1.4.5.2\" xref=\"S5.SS2.SSS2.p2.1.m1.4.5.1.cmml\"><mo id=\"S5.SS2.SSS2.p2.1.m1.4.5.2.1\" stretchy=\"false\" xref=\"S5.SS2.SSS2.p2.1.m1.4.5.1.cmml\">(</mo><mi id=\"S5.SS2.SSS2.p2.1.m1.1.1\" xref=\"S5.SS2.SSS2.p2.1.m1.1.1.cmml\">h</mi><mo id=\"S5.SS2.SSS2.p2.1.m1.4.5.2.2\" xref=\"S5.SS2.SSS2.p2.1.m1.4.5.1.cmml\">,</mo><mi id=\"S5.SS2.SSS2.p2.1.m1.2.2\" xref=\"S5.SS2.SSS2.p2.1.m1.2.2.cmml\">r</mi><mo id=\"S5.SS2.SSS2.p2.1.m1.4.5.2.3\" xref=\"S5.SS2.SSS2.p2.1.m1.4.5.1.cmml\">,</mo><mi id=\"S5.SS2.SSS2.p2.1.m1.3.3\" xref=\"S5.SS2.SSS2.p2.1.m1.3.3.cmml\">t</mi><mo id=\"S5.SS2.SSS2.p2.1.m1.4.5.2.4\" xref=\"S5.SS2.SSS2.p2.1.m1.4.5.1.cmml\">,</mo><mi id=\"S5.SS2.SSS2.p2.1.m1.4.4\" xref=\"S5.SS2.SSS2.p2.1.m1.4.4.cmml\">m</mi><mo id=\"S5.SS2.SSS2.p2.1.m1.4.5.2.5\" stretchy=\"false\" xref=\"S5.SS2.SSS2.p2.1.m1.4.5.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS2.p2.1.m1.4b\"><vector id=\"S5.SS2.SSS2.p2.1.m1.4.5.1.cmml\" xref=\"S5.SS2.SSS2.p2.1.m1.4.5.2\"><ci id=\"S5.SS2.SSS2.p2.1.m1.1.1.cmml\" xref=\"S5.SS2.SSS2.p2.1.m1.1.1\">ℎ</ci><ci id=\"S5.SS2.SSS2.p2.1.m1.2.2.cmml\" xref=\"S5.SS2.SSS2.p2.1.m1.2.2\">𝑟</ci><ci id=\"S5.SS2.SSS2.p2.1.m1.3.3.cmml\" xref=\"S5.SS2.SSS2.p2.1.m1.3.3\">𝑡</ci><ci id=\"S5.SS2.SSS2.p2.1.m1.4.4.cmml\" xref=\"S5.SS2.SSS2.p2.1.m1.4.4\">𝑚</ci></vector></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS2.p2.1.m1.4c\">(h,r,t,m)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS2.p2.1.m1.4d\">( italic_h , italic_r , italic_t , italic_m )</annotation></semantics></math>, where <math alttext=\"m\" display=\"inline\" id=\"S5.SS2.SSS2.p2.2.m2.1\"><semantics id=\"S5.SS2.SSS2.p2.2.m2.1a\"><mi id=\"S5.SS2.SSS2.p2.2.m2.1.1\" xref=\"S5.SS2.SSS2.p2.2.m2.1.1.cmml\">m</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS2.SSS2.p2.2.m2.1b\"><ci id=\"S5.SS2.SSS2.p2.2.m2.1.1.cmml\" xref=\"S5.SS2.SSS2.p2.2.m2.1.1\">𝑚</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS2.SSS2.p2.2.m2.1c\">m</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS2.SSS2.p2.2.m2.1d\">italic_m</annotation></semantics></math> represents the additional ”condition” element. Although different KGC tasks may refer to different conditions, they typically have a similar textual format, which enables unification across different KGC tasks. The KG-S2S approach incorporates various techniques such as entity description, soft prompt, and Seq2Seq Dropout to improve the model’s performance. In addition, it utilizes constrained decoding to ensure the generated entities are valid. For closed-source LLMs (e.g., ChatGPT and GPT-4), AutoKG adopts prompt engineering to design customized prompts <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib93\" title=\"\">93</a>]</cite>. As shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S5.F18\" title=\"Figure 18 ‣ 5.2.2 LLM as Generators (PaG). ‣ 5.2 LLM-augmented KG Completion ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>18</span></a>, these prompts contain the task description, few-shot examples, and test input, which instruct LLMs to predict the tail entity for KG completion.</p>\n<figure id=\"S5.F18\"><img alt=\"Refer to caption\" height=\"534\" id=\"S5.F18.g1\" src=\"x15.png\" width=\"581\">\n<figcaption><span>Figure 18: </span>The framework of prompt-based PaG for KG Completion.</figcaption>\n</figure>\n<p id=\"S5.SS2.SSS2.p3.1\"><span id=\"S5.SS2.SSS2.p3.1.1\">Comparison between PaE and PaG.</span>\nLLMs as Encoders (PaE) applies an additional prediction head on the top of the representation encoded by LLMs. Therefore, the PaE framework is much easier to finetune since we can only optimize the prediction heads and freeze the LLMs. Moreover, the output of the prediction can be easily specified and integrated with existing KGC functions for different KGC tasks. However, during the inference stage, the PaE requires to compute a score for every candidate in KGs, which could be computationally expensive. Besides, they cannot generalize to unseen entities. Furthermore, the PaE requires the representation output of the LLMs, whereas some state-of-the-art LLMs (e.g. GPT-4<span id=\"footnotex1\"><sup>1</sup><span><span><sup>1</sup><span>footnotemark: </span><span>1</span></span></span></span>) are closed sources and do not grant access to the representation output.</p>\n<p id=\"S5.SS2.SSS2.p4.1\">LLMs as Generators (PaG), on the other hand, which does not need the prediction head, can be used without finetuning or access to representations. Therefore, the framework of PaG is suitable for all kinds of LLMs. In addition, PaG directly generates the tail entity, making it efficient in inference without ranking all the candidates and easily generalizing to unseen entities. But, the challenge of PaG is that the generated entities could be diverse and not lie in KGs. What is more, the time of a single inference is longer due to the auto-regressive generation. Last, how to design a powerful prompt that feeds KGs into LLMs is still an open question.\nConsequently, while PaG has demonstrated promising results for KGC tasks, the trade-off between model complexity and computational efficiency must be carefully considered when selecting an appropriate LLM-based KGC framework.</p>\n</section>\n<section id=\"S5.SS2.SSS3\">\n<h4>\n<span>5.2.3 </span>Model Analysis</h4>\n<p id=\"S5.SS2.SSS3.p1.1\">Justin et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib187\" title=\"\">187</a>]</cite> provide a comprehensive analysis of KGC methods integrated with LLMs. Their research investigates the quality of LLM embeddings and finds that they are suboptimal for effective entity ranking. In response, they propose several techniques for processing embeddings to improve their suitability for candidate retrieval. The study also compares different model selection dimensions, such as Embedding Extraction, Query Entity Extraction, and Language Model Selection. Lastly, the authors propose a framework that effectively adapts LLM for knowledge graph completion.</p>\n</section>\n</section>\n<section id=\"S5.SS3\">\n<h3>\n<span>5.3 </span><span id=\"S5.SS3.1.1\">LLM-augmented KG Construction</span>\n</h3>\n<p id=\"S5.SS3.p1.1\">Knowledge graph construction involves creating a structured representation of knowledge within a specific domain. This includes identifying entities and their relationships with each other. The process of knowledge graph construction typically involves multiple stages, including <em id=\"S5.SS3.p1.1.1\">1) entity discovery</em>, <em id=\"S5.SS3.p1.1.2\">2) coreference resolution</em>, and <em id=\"S5.SS3.p1.1.3\">3) relation extraction</em>. Fig&nbsp;<a href=\"https://arxiv.org/html/2306.08302v3#S5.F19\" title=\"Figure 19 ‣ 5.3.1 Entity Discovery ‣ 5.3 LLM-augmented KG Construction ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>19</span></a> presents the general framework of applying LLMs for each stage in KG construction. More recent approaches have explored <em id=\"S5.SS3.p1.1.4\">4) end-to-end knowledge graph construction</em>, which involves constructing a complete knowledge graph in one step or directly <em id=\"S5.SS3.p1.1.5\">5) distilling knowledge graphs from LLMs</em>.</p>\n<section id=\"S5.SS3.SSS1\">\n<h4>\n<span>5.3.1 </span>Entity Discovery</h4>\n<p id=\"S5.SS3.SSS1.p1.1\">Entity discovery in KG construction refers to the process of identifying and extracting entities from unstructured data sources, such as text documents, web pages, or social media posts, and incorporating them to construct knowledge graphs.</p>\n<p id=\"S5.SS3.SSS1.p2.1\"><span id=\"S5.SS3.SSS1.p2.1.1\">Named Entity Recognition (NER)</span> involves identifying and tagging named entities in text data with their positions and classifications. The named entities include people, organizations, locations, and other types of entities. The state-of-the-art NER methods usually employ LLMs to leverage their contextual understanding and linguistic knowledge for accurate entity recognition and classification. There are three NER sub-tasks based on the types of NER spans identified, i.e., flat NER, nested NER, and discontinuous NER. <em id=\"S5.SS3.SSS1.p2.1.2\">1) Flat NER is to identify non-overlapping named entities from input text.</em> It is usually conceptualized as a sequence labelling problem where each token in the text is assigned a unique label based on its position in the sequence&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib148\" title=\"\">148</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib1\" title=\"\">1</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib188\" title=\"\">188</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib189\" title=\"\">189</a>]</cite>. <em id=\"S5.SS3.SSS1.p2.1.3\">2) Nested NER considers complex scenarios which allow a token to belong to multiple entities.</em> The span-based method&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib190\" title=\"\">190</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib191\" title=\"\">191</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib192\" title=\"\">192</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib193\" title=\"\">193</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib194\" title=\"\">194</a>]</cite> is a popular branch of nested NER which involves enumerating all candidate spans and classifying them into entity types (including a non-entity type). Parsing-based methods&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib195\" title=\"\">195</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib196\" title=\"\">196</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib197\" title=\"\">197</a>]</cite> reveal similarities between nested NER and constituency parsing tasks (predicting nested and non-overlapping spans), and propose to integrate the insights of constituency parsing into nested NER.\n<em id=\"S5.SS3.SSS1.p2.1.4\">3) Discontinuous NER identifies named entities that may not be contiguous in the text.</em> To address this challenge, <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib198\" title=\"\">198</a>]</cite> uses the LLM output to identify entity fragments and determine whether they are overlapped or in succession.</p>\n<p id=\"S5.SS3.SSS1.p3.1\">Unlike the task-specific methods, GenerativeNER <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib149\" title=\"\">149</a>]</cite> uses a sequence-to-sequence LLM with a pointer mechanism to generate an entity sequence, which is capable of solving all three types of NER sub-tasks.</p>\n<figure id=\"S5.F19\"><img alt=\"Refer to caption\" height=\"693\" id=\"S5.F19.g1\" src=\"x16.png\" width=\"830\">\n<figcaption><span>Figure 19: </span>The general framework of LLM-based KG construction.</figcaption>\n</figure>\n<p id=\"S5.SS3.SSS1.p4.1\"><span id=\"S5.SS3.SSS1.p4.1.1\">Entity Typing (ET)</span> aims to provide fine-grained and ultra-grained type information for a given entity mentioned in context. These methods usually utilize LLM to encode mentions, context and types. LDET&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib150\" title=\"\">150</a>]</cite> applies pre-trained ELMo embeddings&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib148\" title=\"\">148</a>]</cite> for word representation and adopts LSTM as its sentence and mention encoders. BOX4Types&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib151\" title=\"\">151</a>]</cite> recognizes the importance of type dependency and uses BERT to represent the hidden vector and each type in a hyperrectangular (box) space. LRN&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib199\" title=\"\">199</a>]</cite> considers extrinsic and intrinsic dependencies between labels. It encodes the context and entity with BERT and employs these output embeddings to conduct deductive and inductive reasoning. MLMET&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib200\" title=\"\">200</a>]</cite> uses predefined patterns to construct input samples for the BERT MLM and employs [MASK] to predict context-dependent hypernyms of the mention, which can be viewed as type labels. PL&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib201\" title=\"\">201</a>]</cite> and DFET&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib202\" title=\"\">202</a>]</cite> utilize prompt learning for entity typing. LITE&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib203\" title=\"\">203</a>]</cite> formulates entity typing as textual inference and uses RoBERTa-large-MNLI as the backbone network. </p>\n<p id=\"S5.SS3.SSS1.p5.1\"><span id=\"S5.SS3.SSS1.p5.1.1\">Entity Linking (EL)</span>, as known as entity disambiguation, involves linking entity mentions appearing in the text to their corresponding entities in a knowledge graph. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib204\" title=\"\">204</a>]</cite> proposed BERT-based end-to-end EL systems that jointly discover and link entities. ELQ&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib152\" title=\"\">152</a>]</cite> employs a fast bi-encoder architecture to jointly perform mention detection and linking in one pass for downstream question answering systems. Unlike previous models that frame EL as matching in vector space, GENRE&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib205\" title=\"\">205</a>]</cite> formulates it as a sequence-to-sequence problem, autoregressively generating a version of the input markup-annotated with the unique identifiers of an entity expressed in natural language. GENRE is extended to its multilingual version mGENRE&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib206\" title=\"\">206</a>]</cite>. Considering the efficiency challenges of generative EL approaches, <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib207\" title=\"\">207</a>]</cite> parallelizes autoregressive linking across all potential mentions and relies on a shallow and efficient decoder. ReFinED&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib153\" title=\"\">153</a>]</cite> proposes an efficient zero-shot-capable EL approach by taking advantage of fine-grained entity types and entity descriptions which are processed by a LLM-based encoder. </p>\n</section>\n<section id=\"S5.SS3.SSS2\">\n<h4>\n<span>5.3.2 </span>Coreference Resolution (CR)</h4>\n<p id=\"S5.SS3.SSS2.p1.1\">Coreference resolution is to find all expressions (i.e., mentions) that refer to the same entity or event in a text.</p>\n<p id=\"S5.SS3.SSS2.p2.1\"><span id=\"S5.SS3.SSS2.p2.1.1\">Within-document CR</span> refers to the CR sub-task where all these mentions are in a single document. Mandar et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib154\" title=\"\">154</a>]</cite> initialize LLM-based coreferences resolution by replacing the previous LSTM encoder&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib208\" title=\"\">208</a>]</cite> with BERT. This work is followed by the introduction of SpanBERT&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib155\" title=\"\">155</a>]</cite> which is pre-trained on BERT architecture with a span-based masked language model (MLM). Inspired by these works, Tuan Manh et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib209\" title=\"\">209</a>]</cite> present a strong baseline by incorporating the SpanBERT encoder into a non-LLM approach e2e-coref&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib208\" title=\"\">208</a>]</cite>. CorefBERT leverages Mention Reference Prediction (MRP) task which masks one or several mentions and requires the model to predict the masked mention’s corresponding referents. CorefQA&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib210\" title=\"\">210</a>]</cite> formulates coreference resolution as a question answering task, where contextual queries are generated for each candidate mention and the coreferent spans are extracted from the document using the queries.\nTuan Manh et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib211\" title=\"\">211</a>]</cite> introduce a gating mechanism and a noisy training method to extract information from event mentions using the SpanBERT encoder.\n</p>\n<p id=\"S5.SS3.SSS2.p3.1\">In order to reduce the large memory footprint faced by large LLM-based NER models, Yuval et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib212\" title=\"\">212</a>]</cite> and Raghuveer el al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib213\" title=\"\">213</a>]</cite> proposed start-to-end and approximation models, respectively, both utilizing bilinear functions to calculate mention and antecedent scores with reduced reliance on span-level representations.\n</p>\n<p id=\"S5.SS3.SSS2.p4.1\"><span id=\"S5.SS3.SSS2.p4.1.1\">Cross-document CR</span> refers to the sub-task where the mentions refer to the same entity or event might be across multiple documents. CDML <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib156\" title=\"\">156</a>]</cite> proposes a cross document language modeling method which pre-trains a Longformer&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib214\" title=\"\">214</a>]</cite> encoder on concatenated related documents and employs an MLP for binary classification to determine whether a pair of mentions is coreferent or not. CrossCR <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib157\" title=\"\">157</a>]</cite> utilizes an end-to-end model for cross-document coreference resolution which pre-trained the mention scorer on gold mention spans and uses a pairwise scorer to compare mentions with all spans across all documents. CR-RL <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib158\" title=\"\">158</a>]</cite> proposes an actor-critic deep reinforcement learning-based coreference resolver for cross-document CR.</p>\n</section>\n<section id=\"S5.SS3.SSS3\">\n<h4>\n<span>5.3.3 </span>Relation Extraction (RE)</h4>\n<p id=\"S5.SS3.SSS3.p1.1\">Relation extraction involves identifying semantic relationships between entities mentioned in natural language text. There are two types of relation extraction methods, i.e. sentence-level RE and document-level RE, according to the scope of the text analyzed.</p>\n<p id=\"S5.SS3.SSS3.p2.1\"><span id=\"S5.SS3.SSS3.p2.1.1\">Sentence-level RE</span> focuses on identifying relations between entities within a single sentence. Peng et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib159\" title=\"\">159</a>]</cite> and TRE&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib215\" title=\"\">215</a>]</cite> introduce LLM to improve the performance of relation extraction models. BERT-MTB&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib216\" title=\"\">216</a>]</cite> learns relation representations based on BERT by performing the matching-the-blanks task and incorporating designed objectives for relation extraction. Curriculum-RE&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib160\" title=\"\">160</a>]</cite> utilizes curriculum learning to improve relation extraction models by gradually increasing the difficulty of the data during training.\nRECENT&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib217\" title=\"\">217</a>]</cite> introduces SpanBERT and exploits entity type restriction to reduce the noisy candidate relation types. Jiewen&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib218\" title=\"\">218</a>]</cite> extends RECENT by combining both the entity information and the label information into sentence-level embeddings, which enables the embedding to be entity-label aware.\n</p>\n<p id=\"S5.SS3.SSS3.p3.1\"><span id=\"S5.SS3.SSS3.p3.1.1\">Document-level RE (DocRE)</span> aims to extract relations between entities across multiple sentences within a document. Hong et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib219\" title=\"\">219</a>]</cite> propose a strong baseline for DocRE by replacing the BiLSTM backbone with LLMs. HIN&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib220\" title=\"\">220</a>]</cite> use LLM to encode and aggregate entity representation at different levels, including entity, sentence, and document levels. GLRE&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib221\" title=\"\">221</a>]</cite> is a global-to-local network, which uses LLM to encode the document information in terms of entity global and local representations as well as context relation representations. SIRE&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib222\" title=\"\">222</a>]</cite> uses two LLM-based encoders to extract intra-sentence and inter-sentence relations. LSR&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib223\" title=\"\">223</a>]</cite> and GAIN&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib224\" title=\"\">224</a>]</cite> propose graph-based approaches which induce graph structures on top of LLM to better extract relations. DocuNet&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib225\" title=\"\">225</a>]</cite> formulates DocRE as a semantic segmentation task and introduces a U-Net&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib226\" title=\"\">226</a>]</cite> on the LLM encoder to capture local and global dependencies between entities. ATLOP <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib227\" title=\"\">227</a>]</cite> focuses on the multi-label problems in DocRE, which could be handled with two techniques, i.e., adaptive thresholding for classifier and localized context pooling for LLM. DREEAM&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib161\" title=\"\">161</a>]</cite> further extends and improves ATLOP by incorporating evidence information.\n</p>\n<p id=\"S5.SS3.SSS3.p4.1\"><span id=\"S5.SS3.SSS3.p4.1.1\">End-to-End KG Construction.</span>\nCurrently, researchers are exploring the use of LLMs for end-to-end KG construction. Kumar et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib95\" title=\"\">95</a>]</cite> propose a unified approach to build KGs from raw text, which contains two LLMs powered components. They first finetune a LLM on named entity recognition tasks to make it capable of recognizing entities in raw text. Then, they propose another “2-model BERT” for solving the relation extraction task, which contains two BERT-based classifiers. The first classifier learns the relation class whereas the second binary classifier learns the direction of the relations between the two entities. The predicted triples and relations are then used to construct the KG. Guo et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib162\" title=\"\">162</a>]</cite> propose an end-to-end knowledge extraction model based on BERT, which can be applied to construct KGs from Classical Chinese text.\nGrapher <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib41\" title=\"\">41</a>]</cite> presents a novel end-to-end multi-stage system. It first utilizes LLMs to generate KG entities, followed by a simple relation construction head, enabling efficient KG construction from the textual description. PiVE <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib163\" title=\"\">163</a>]</cite> proposes a prompting with an iterative verification framework that utilizes a smaller LLM like T5 to correct the errors in KGs generated by a larger LLM (e.g., ChatGPT). To further explore advanced LLMs, AutoKG design several prompts for different KG construction tasks (e.g., entity typing, entity linking, and relation extraction). Then, it adopts the prompt to perform KG construction using ChatGPT and GPT-4.</p>\n<figure id=\"S5.F20\"><img alt=\"Refer to caption\" height=\"310\" id=\"S5.F20.g1\" src=\"x17.png\" width=\"830\">\n<figcaption><span>Figure 20: </span>The general framework of distilling KGs from LLMs.</figcaption>\n</figure>\n</section>\n<section id=\"S5.SS3.SSS4\">\n<h4>\n<span>5.3.4 </span>Distilling Knowledge Graphs from LLMs </h4>\n<p id=\"S5.SS3.SSS4.p1.1\">LLMs have been shown to implicitly encode massive knowledge <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a>]</cite>. As shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S5.F20\" title=\"Figure 20 ‣ 5.3.3 Relation Extraction (RE) ‣ 5.3 LLM-augmented KG Construction ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>20</span></a>, some research aims to distill knowledge from LLMs to construct KGs. COMET <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib164\" title=\"\">164</a>]</cite> proposes a commonsense transformer model that constructs commonsense KGs by using existing tuples as a seed set of knowledge on which to train. Using this seed set, a LLM learns to adapt its learned representations to knowledge generation, and produces novel tuples that are high quality. Experimental results reveal that implicit knowledge from LLMs is transferred to generate explicit knowledge in commonsense KGs. BertNet <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib165\" title=\"\">165</a>]</cite> proposes a novel framework for automatic KG construction empowered by LLMs. It requires only the minimal definition of relations as inputs and automatically generates diverse prompts, and performs an efficient knowledge search within a given LLM for consistent outputs. The constructed KGs show competitive quality, diversity, and novelty with a richer set of new and complex relations, which cannot be extracted by previous methods. West et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib166\" title=\"\">166</a>]</cite> propose a symbolic knowledge distillation framework that distills symbolic knowledge from LLMs. They first finetune a small student LLM by distilling commonsense facts from a large LLM like GPT-3. Then, the student LLM is utilized to generate commonsense KGs.</p>\n</section>\n</section>\n<section id=\"S5.SS4\">\n<h3>\n<span>5.4 </span><span id=\"S5.SS4.1.1\">LLM-augmented KG-to-text Generation</span>\n</h3>\n<p id=\"S5.SS4.p1.1\">The goal of Knowledge-graph-to-text (KG-to-text) generation is to generate high-quality texts that accurately and consistently describe the input knowledge graph information&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib228\" title=\"\">228</a>]</cite>. KG-to-text generation connects knowledge graphs and texts, significantly improving the applicability of KG in more realistic NLG scenarios, including storytelling&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib229\" title=\"\">229</a>]</cite> and knowledge-grounded dialogue&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib230\" title=\"\">230</a>]</cite>. However, it is challenging and costly to collect large amounts of graph-text parallel data, resulting in insufficient training and poor generation quality. Thus, many research efforts resort to either: <em id=\"S5.SS4.p1.1.1\">1) leverage knowledge from LLMs</em> or <em id=\"S5.SS4.p1.1.2\">2) construct large-scale weakly-supervised KG-text corpus</em> to solve this issue.</p>\n<section id=\"S5.SS4.SSS1\">\n<h4>\n<span>5.4.1 </span>Leveraging Knowledge from LLMs</h4>\n<p id=\"S5.SS4.SSS1.p1.1\">As pioneering research efforts in using LLMs for KG-to-Text generation, Ribeiro et al.&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib167\" title=\"\">167</a>]</cite> and Kale and Rastogi&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib231\" title=\"\">231</a>]</cite> directly fine-tune various LLMs, including BART and T5, with the goal of transferring LLMs knowledge for this task. As shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S5.F21\" title=\"Figure 21 ‣ 5.4.1 Leveraging Knowledge from LLMs ‣ 5.4 LLM-augmented KG-to-text Generation ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>21</span></a>, both works simply represent the input graph as a linear traversal and find that such a naive approach successfully outperforms many existing state-of-the-art KG-to-text generation systems. Interestingly, Ribeiro et al.&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib167\" title=\"\">167</a>]</cite> also find that continue pre-training could further improve model performance. However, these methods are unable to <em id=\"S5.SS4.SSS1.p1.1.1\">explicitly</em> incorporate rich graph semantics in KGs. To enhance LLMs with KG structure information, JointGT&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib42\" title=\"\">42</a>]</cite> proposes to inject KG structure-preserving representations into the Seq2Seq large language models. Given input sub-KGs and corresponding text, JointGT first represents the KG entities and their relations as a sequence of tokens, then concatenate them with the textual tokens which are fed into LLM. After the standard self-attention module, JointGT then uses a pooling layer to obtain the contextual semantic representations of knowledge entities and relations. Finally, these pooled KG representations are then aggregated in another structure-aware self-attention layer. JointGT also deploys additional pre-training objectives, including KG and text reconstruction tasks given masked inputs, to improve the alignment between text and graph information. Li et al.&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib168\" title=\"\">168</a>]</cite> focus on the few-shot scenario. It first employs a novel breadth-first search (BFS) strategy to better traverse the input KG structure and feed the enhanced linearized graph representations into LLMs for high-quality generated outputs, then aligns the GCN-based and LLM-based KG entity representation. Colas et al.&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib169\" title=\"\">169</a>]</cite> first transform the graph into its appropriate representation before linearizing the graph. Next, each KG node is encoded via a global attention mechanism, followed by a graph-aware attention module, ultimately being decoded into a sequence of tokens. Different from these works, KG-BART&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib37\" title=\"\">37</a>]</cite> keeps the structure of KGs and leverages the graph attention to aggregate the rich concept semantics in the sub-KG, which enhances the model generalization on unseen concept sets.\n</p>\n<figure id=\"S5.F21\"><img alt=\"Refer to caption\" height=\"372\" id=\"S5.F21.g1\" src=\"x18.png\" width=\"830\">\n<figcaption><span>Figure 21: </span>The general framework of KG-to-text generation.</figcaption>\n</figure>\n</section>\n<section id=\"S5.SS4.SSS2\">\n<h4>\n<span>5.4.2 </span>Constructing large weakly KG-text aligned Corpus</h4>\n<p id=\"S5.SS4.SSS2.p1.1\">Although LLMs have achieved remarkable empirical success, their unsupervised pre-training objectives are not necessarily aligned well with the task of KG-to-text generation, motivating researchers to develop large-scale KG-text aligned corpus. Jin et al.&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib170\" title=\"\">170</a>]</cite> propose a 1.3M unsupervised KG-to-graph training data from Wikipedia. Specifically, they first detect the entities appearing in the text via hyperlinks and named entity detectors, and then only add text that shares a common set of entities with the corresponding knowledge graph, similar to the idea of distance supervision in the relation extraction task&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib232\" title=\"\">232</a>]</cite>.\nThey also provide a 1,000+ human annotated KG-to-Text test data to verify the effectiveness of the pre-trained KG-to-Text models. Similarly, Chen et al.&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib171\" title=\"\">171</a>]</cite> also propose a KG-grounded text corpus collected from the English Wikidump. To ensure the connection between KG and text, they only extract sentences with at least two Wikipedia anchor links. Then, they use the entities from those links to query their surrounding neighbors in WikiData and calculate the lexical overlapping between these neighbors and the original sentences. Finally, only highly overlapped pairs are selected. The authors explore both graph-based and sequence-based encoders and identify their advantages in various different tasks and settings.</p>\n</section>\n</section>\n<section id=\"S5.SS5\">\n<h3>\n<span>5.5 </span><span id=\"S5.SS5.1.1\">LLM-augmented KG Question Answering</span>\n</h3>\n<p id=\"S5.SS5.p1.1\">Knowledge graph question answering (KGQA) aims to find answers to natural language questions based on the structured facts stored in knowledge graphs <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib233\" title=\"\">233</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib234\" title=\"\">234</a>]</cite>. The inevitable challenge in KGQA is to retrieve related facts and extend the reasoning advantage of KGs to QA. Therefore, recent studies adopt LLMs to bridge the gap between natural language questions and structured knowledge graphs <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib175\" title=\"\">175</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib235\" title=\"\">235</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib174\" title=\"\">174</a>]</cite>. The general framework of applying LLMs for KGQA is illustrated in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S5.F22\" title=\"Figure 22 ‣ 5.5.1 LLMs as Entity/relation Extractors ‣ 5.5 LLM-augmented KG Question Answering ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>22</span></a>, where LLMs can be used as 1) entity/relation extractors, and 2) answer reasoners.</p>\n<section id=\"S5.SS5.SSS1\">\n<h4>\n<span>5.5.1 </span>LLMs as Entity/relation Extractors</h4>\n<div id=\"S5.SS5.SSS1.p1\">\n<p id=\"S5.SS5.SSS1.p1.12\">Entity/relation extractors are designed to identify entities and relationships mentioned in natural language questions and retrieve related facts in KGs. Given the proficiency in language comprehension, LLMs can be effectively utilized for this purpose. Lukovnikov et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib172\" title=\"\">172</a>]</cite> are the first to utilize LLMs as classifiers for relation prediction, resulting in a notable improvement in performance compared to shallow neural networks. Nan et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib174\" title=\"\">174</a>]</cite> introduce two LLM-based KGQA frameworks that adopt LLMs to detect mentioned entities and relations. Then, they query the answer in KGs using the extracted entity-relation pairs. QA-GNN <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib131\" title=\"\">131</a>]</cite> uses LLMs to encode the question and candidate answer pairs, which are adopted to estimate the importance of relative KG entities. The entities are retrieved to form a subgraph, where an answer reasoning is conducted by a graph neural network.\nLuo et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib173\" title=\"\">173</a>]</cite> use LLMs to calculate the similarities between relations and questions to retrieve related facts, formulated as</p>\n<table id=\"S5.E12\">\n<tbody><tr>\n<td></td>\n<td><math alttext=\"s(r,q)=\\text{LLM}(r)^{\\top}\\text{LLM}(q),\" display=\"block\" id=\"S5.E12.m1.5\"><semantics id=\"S5.E12.m1.5a\"><mrow id=\"S5.E12.m1.5.5.1\" xref=\"S5.E12.m1.5.5.1.1.cmml\"><mrow id=\"S5.E12.m1.5.5.1.1\" xref=\"S5.E12.m1.5.5.1.1.cmml\"><mrow id=\"S5.E12.m1.5.5.1.1.2\" xref=\"S5.E12.m1.5.5.1.1.2.cmml\"><mi id=\"S5.E12.m1.5.5.1.1.2.2\" xref=\"S5.E12.m1.5.5.1.1.2.2.cmml\">s</mi><mo id=\"S5.E12.m1.5.5.1.1.2.1\" xref=\"S5.E12.m1.5.5.1.1.2.1.cmml\">⁢</mo><mrow id=\"S5.E12.m1.5.5.1.1.2.3.2\" xref=\"S5.E12.m1.5.5.1.1.2.3.1.cmml\"><mo id=\"S5.E12.m1.5.5.1.1.2.3.2.1\" stretchy=\"false\" xref=\"S5.E12.m1.5.5.1.1.2.3.1.cmml\">(</mo><mi id=\"S5.E12.m1.1.1\" xref=\"S5.E12.m1.1.1.cmml\">r</mi><mo id=\"S5.E12.m1.5.5.1.1.2.3.2.2\" xref=\"S5.E12.m1.5.5.1.1.2.3.1.cmml\">,</mo><mi id=\"S5.E12.m1.2.2\" xref=\"S5.E12.m1.2.2.cmml\">q</mi><mo id=\"S5.E12.m1.5.5.1.1.2.3.2.3\" stretchy=\"false\" xref=\"S5.E12.m1.5.5.1.1.2.3.1.cmml\">)</mo></mrow></mrow><mo id=\"S5.E12.m1.5.5.1.1.1\" xref=\"S5.E12.m1.5.5.1.1.1.cmml\">=</mo><mrow id=\"S5.E12.m1.5.5.1.1.3\" xref=\"S5.E12.m1.5.5.1.1.3.cmml\"><mtext id=\"S5.E12.m1.5.5.1.1.3.2\" xref=\"S5.E12.m1.5.5.1.1.3.2a.cmml\">LLM</mtext><mo id=\"S5.E12.m1.5.5.1.1.3.1\" xref=\"S5.E12.m1.5.5.1.1.3.1.cmml\">⁢</mo><msup id=\"S5.E12.m1.5.5.1.1.3.3\" xref=\"S5.E12.m1.5.5.1.1.3.3.cmml\"><mrow id=\"S5.E12.m1.5.5.1.1.3.3.2.2\" xref=\"S5.E12.m1.5.5.1.1.3.3.cmml\"><mo id=\"S5.E12.m1.5.5.1.1.3.3.2.2.1\" stretchy=\"false\" xref=\"S5.E12.m1.5.5.1.1.3.3.cmml\">(</mo><mi id=\"S5.E12.m1.3.3\" xref=\"S5.E12.m1.3.3.cmml\">r</mi><mo id=\"S5.E12.m1.5.5.1.1.3.3.2.2.2\" stretchy=\"false\" xref=\"S5.E12.m1.5.5.1.1.3.3.cmml\">)</mo></mrow><mo id=\"S5.E12.m1.5.5.1.1.3.3.3\" xref=\"S5.E12.m1.5.5.1.1.3.3.3.cmml\">⊤</mo></msup><mo id=\"S5.E12.m1.5.5.1.1.3.1a\" xref=\"S5.E12.m1.5.5.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E12.m1.5.5.1.1.3.4\" xref=\"S5.E12.m1.5.5.1.1.3.4a.cmml\">LLM</mtext><mo id=\"S5.E12.m1.5.5.1.1.3.1b\" xref=\"S5.E12.m1.5.5.1.1.3.1.cmml\">⁢</mo><mrow id=\"S5.E12.m1.5.5.1.1.3.5.2\" xref=\"S5.E12.m1.5.5.1.1.3.cmml\"><mo id=\"S5.E12.m1.5.5.1.1.3.5.2.1\" stretchy=\"false\" xref=\"S5.E12.m1.5.5.1.1.3.cmml\">(</mo><mi id=\"S5.E12.m1.4.4\" xref=\"S5.E12.m1.4.4.cmml\">q</mi><mo id=\"S5.E12.m1.5.5.1.1.3.5.2.2\" stretchy=\"false\" xref=\"S5.E12.m1.5.5.1.1.3.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E12.m1.5.5.1.2\" xref=\"S5.E12.m1.5.5.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E12.m1.5b\"><apply id=\"S5.E12.m1.5.5.1.1.cmml\" xref=\"S5.E12.m1.5.5.1\"><eq id=\"S5.E12.m1.5.5.1.1.1.cmml\" xref=\"S5.E12.m1.5.5.1.1.1\"></eq><apply id=\"S5.E12.m1.5.5.1.1.2.cmml\" xref=\"S5.E12.m1.5.5.1.1.2\"><times id=\"S5.E12.m1.5.5.1.1.2.1.cmml\" xref=\"S5.E12.m1.5.5.1.1.2.1\"></times><ci id=\"S5.E12.m1.5.5.1.1.2.2.cmml\" xref=\"S5.E12.m1.5.5.1.1.2.2\">𝑠</ci><interval closure=\"open\" id=\"S5.E12.m1.5.5.1.1.2.3.1.cmml\" xref=\"S5.E12.m1.5.5.1.1.2.3.2\"><ci id=\"S5.E12.m1.1.1.cmml\" xref=\"S5.E12.m1.1.1\">𝑟</ci><ci id=\"S5.E12.m1.2.2.cmml\" xref=\"S5.E12.m1.2.2\">𝑞</ci></interval></apply><apply id=\"S5.E12.m1.5.5.1.1.3.cmml\" xref=\"S5.E12.m1.5.5.1.1.3\"><times id=\"S5.E12.m1.5.5.1.1.3.1.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.1\"></times><ci id=\"S5.E12.m1.5.5.1.1.3.2a.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.2\"><mtext id=\"S5.E12.m1.5.5.1.1.3.2.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.2\">LLM</mtext></ci><apply id=\"S5.E12.m1.5.5.1.1.3.3.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E12.m1.5.5.1.1.3.3.1.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.3\">superscript</csymbol><ci id=\"S5.E12.m1.3.3.cmml\" xref=\"S5.E12.m1.3.3\">𝑟</ci><csymbol cd=\"latexml\" id=\"S5.E12.m1.5.5.1.1.3.3.3.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.3.3\">top</csymbol></apply><ci id=\"S5.E12.m1.5.5.1.1.3.4a.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.4\"><mtext id=\"S5.E12.m1.5.5.1.1.3.4.cmml\" xref=\"S5.E12.m1.5.5.1.1.3.4\">LLM</mtext></ci><ci id=\"S5.E12.m1.4.4.cmml\" xref=\"S5.E12.m1.4.4\">𝑞</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E12.m1.5c\">s(r,q)=\\text{LLM}(r)^{\\top}\\text{LLM}(q),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E12.m1.5d\">italic_s ( italic_r , italic_q ) = LLM ( italic_r ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT LLM ( italic_q ) ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(12)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS5.SSS1.p1.5\">where <math alttext=\"q\" display=\"inline\" id=\"S5.SS5.SSS1.p1.1.m1.1\"><semantics id=\"S5.SS5.SSS1.p1.1.m1.1a\"><mi id=\"S5.SS5.SSS1.p1.1.m1.1.1\" xref=\"S5.SS5.SSS1.p1.1.m1.1.1.cmml\">q</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.1.m1.1b\"><ci id=\"S5.SS5.SSS1.p1.1.m1.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.1.m1.1.1\">𝑞</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.1.m1.1c\">q</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.1.m1.1d\">italic_q</annotation></semantics></math> denotes the question, <math alttext=\"r\" display=\"inline\" id=\"S5.SS5.SSS1.p1.2.m2.1\"><semantics id=\"S5.SS5.SSS1.p1.2.m2.1a\"><mi id=\"S5.SS5.SSS1.p1.2.m2.1.1\" xref=\"S5.SS5.SSS1.p1.2.m2.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.2.m2.1b\"><ci id=\"S5.SS5.SSS1.p1.2.m2.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.2.m2.1.1\">𝑟</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.2.m2.1c\">r</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.2.m2.1d\">italic_r</annotation></semantics></math> denotes the relation, and <math alttext=\"\\text{LLM}(\\cdot)\" display=\"inline\" id=\"S5.SS5.SSS1.p1.3.m3.1\"><semantics id=\"S5.SS5.SSS1.p1.3.m3.1a\"><mrow id=\"S5.SS5.SSS1.p1.3.m3.1.2\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.cmml\"><mtext id=\"S5.SS5.SSS1.p1.3.m3.1.2.2\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.2a.cmml\">LLM</mtext><mo id=\"S5.SS5.SSS1.p1.3.m3.1.2.1\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.1.cmml\">⁢</mo><mrow id=\"S5.SS5.SSS1.p1.3.m3.1.2.3.2\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.cmml\"><mo id=\"S5.SS5.SSS1.p1.3.m3.1.2.3.2.1\" stretchy=\"false\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.cmml\">(</mo><mo id=\"S5.SS5.SSS1.p1.3.m3.1.1\" lspace=\"0em\" rspace=\"0em\" xref=\"S5.SS5.SSS1.p1.3.m3.1.1.cmml\">⋅</mo><mo id=\"S5.SS5.SSS1.p1.3.m3.1.2.3.2.2\" stretchy=\"false\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.3.m3.1b\"><apply id=\"S5.SS5.SSS1.p1.3.m3.1.2.cmml\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2\"><times id=\"S5.SS5.SSS1.p1.3.m3.1.2.1.cmml\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.1\"></times><ci id=\"S5.SS5.SSS1.p1.3.m3.1.2.2a.cmml\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.2\"><mtext id=\"S5.SS5.SSS1.p1.3.m3.1.2.2.cmml\" xref=\"S5.SS5.SSS1.p1.3.m3.1.2.2\">LLM</mtext></ci><ci id=\"S5.SS5.SSS1.p1.3.m3.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.3.m3.1.1\">⋅</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.3.m3.1c\">\\text{LLM}(\\cdot)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.3.m3.1d\">LLM ( ⋅ )</annotation></semantics></math> would generate representation for <math alttext=\"q\" display=\"inline\" id=\"S5.SS5.SSS1.p1.4.m4.1\"><semantics id=\"S5.SS5.SSS1.p1.4.m4.1a\"><mi id=\"S5.SS5.SSS1.p1.4.m4.1.1\" xref=\"S5.SS5.SSS1.p1.4.m4.1.1.cmml\">q</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.4.m4.1b\"><ci id=\"S5.SS5.SSS1.p1.4.m4.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.4.m4.1.1\">𝑞</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.4.m4.1c\">q</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.4.m4.1d\">italic_q</annotation></semantics></math> and <math alttext=\"r\" display=\"inline\" id=\"S5.SS5.SSS1.p1.5.m5.1\"><semantics id=\"S5.SS5.SSS1.p1.5.m5.1a\"><mi id=\"S5.SS5.SSS1.p1.5.m5.1.1\" xref=\"S5.SS5.SSS1.p1.5.m5.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.5.m5.1b\"><ci id=\"S5.SS5.SSS1.p1.5.m5.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.5.m5.1.1\">𝑟</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.5.m5.1c\">r</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.5.m5.1d\">italic_r</annotation></semantics></math>, respectively. Furthermore, Zhang et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib236\" title=\"\">236</a>]</cite> propose a LLM-based path retriever to retrieve question-related relations hop-by-hop and construct several paths. The probability of each path can be calculated as</p>\n<table id=\"S5.E13\">\n<tbody><tr>\n<td></td>\n<td><math alttext=\"P(p|q)=\\prod_{t=1}^{|p|}s(r_{t},q),\" display=\"block\" id=\"S5.E13.m1.3\"><semantics id=\"S5.E13.m1.3a\"><mrow id=\"S5.E13.m1.3.3.1\" xref=\"S5.E13.m1.3.3.1.1.cmml\"><mrow id=\"S5.E13.m1.3.3.1.1\" xref=\"S5.E13.m1.3.3.1.1.cmml\"><mrow id=\"S5.E13.m1.3.3.1.1.1\" xref=\"S5.E13.m1.3.3.1.1.1.cmml\"><mi id=\"S5.E13.m1.3.3.1.1.1.3\" xref=\"S5.E13.m1.3.3.1.1.1.3.cmml\">P</mi><mo id=\"S5.E13.m1.3.3.1.1.1.2\" xref=\"S5.E13.m1.3.3.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E13.m1.3.3.1.1.1.1.1\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.cmml\"><mo id=\"S5.E13.m1.3.3.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S5.E13.m1.3.3.1.1.1.1.1.1\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.cmml\"><mi id=\"S5.E13.m1.3.3.1.1.1.1.1.1.2\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.2.cmml\">p</mi><mo fence=\"false\" id=\"S5.E13.m1.3.3.1.1.1.1.1.1.1\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.1.cmml\">|</mo><mi id=\"S5.E13.m1.3.3.1.1.1.1.1.1.3\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.3.cmml\">q</mi></mrow><mo id=\"S5.E13.m1.3.3.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo id=\"S5.E13.m1.3.3.1.1.3\" rspace=\"0.111em\" xref=\"S5.E13.m1.3.3.1.1.3.cmml\">=</mo><mrow id=\"S5.E13.m1.3.3.1.1.2\" xref=\"S5.E13.m1.3.3.1.1.2.cmml\"><munderover id=\"S5.E13.m1.3.3.1.1.2.2\" xref=\"S5.E13.m1.3.3.1.1.2.2.cmml\"><mo id=\"S5.E13.m1.3.3.1.1.2.2.2.2\" movablelimits=\"false\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.2.cmml\">∏</mo><mrow id=\"S5.E13.m1.3.3.1.1.2.2.2.3\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3.cmml\"><mi id=\"S5.E13.m1.3.3.1.1.2.2.2.3.2\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3.2.cmml\">t</mi><mo id=\"S5.E13.m1.3.3.1.1.2.2.2.3.1\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3.1.cmml\">=</mo><mn id=\"S5.E13.m1.3.3.1.1.2.2.2.3.3\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3.3.cmml\">1</mn></mrow><mrow id=\"S5.E13.m1.1.1.1.3\" xref=\"S5.E13.m1.1.1.1.2.cmml\"><mo id=\"S5.E13.m1.1.1.1.3.1\" stretchy=\"false\" xref=\"S5.E13.m1.1.1.1.2.1.cmml\">|</mo><mi id=\"S5.E13.m1.1.1.1.1\" xref=\"S5.E13.m1.1.1.1.1.cmml\">p</mi><mo id=\"S5.E13.m1.1.1.1.3.2\" stretchy=\"false\" xref=\"S5.E13.m1.1.1.1.2.1.cmml\">|</mo></mrow></munderover><mrow id=\"S5.E13.m1.3.3.1.1.2.1\" xref=\"S5.E13.m1.3.3.1.1.2.1.cmml\"><mi id=\"S5.E13.m1.3.3.1.1.2.1.3\" xref=\"S5.E13.m1.3.3.1.1.2.1.3.cmml\">s</mi><mo id=\"S5.E13.m1.3.3.1.1.2.1.2\" xref=\"S5.E13.m1.3.3.1.1.2.1.2.cmml\">⁢</mo><mrow id=\"S5.E13.m1.3.3.1.1.2.1.1.1\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.2.cmml\"><mo id=\"S5.E13.m1.3.3.1.1.2.1.1.1.2\" stretchy=\"false\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.2.cmml\">(</mo><msub id=\"S5.E13.m1.3.3.1.1.2.1.1.1.1\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.cmml\"><mi id=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.2\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.2.cmml\">r</mi><mi id=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.3\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.3.cmml\">t</mi></msub><mo id=\"S5.E13.m1.3.3.1.1.2.1.1.1.3\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.2.cmml\">,</mo><mi id=\"S5.E13.m1.2.2\" xref=\"S5.E13.m1.2.2.cmml\">q</mi><mo id=\"S5.E13.m1.3.3.1.1.2.1.1.1.4\" stretchy=\"false\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.2.cmml\">)</mo></mrow></mrow></mrow></mrow><mo id=\"S5.E13.m1.3.3.1.2\" xref=\"S5.E13.m1.3.3.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E13.m1.3b\"><apply id=\"S5.E13.m1.3.3.1.1.cmml\" xref=\"S5.E13.m1.3.3.1\"><eq id=\"S5.E13.m1.3.3.1.1.3.cmml\" xref=\"S5.E13.m1.3.3.1.1.3\"></eq><apply id=\"S5.E13.m1.3.3.1.1.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.1\"><times id=\"S5.E13.m1.3.3.1.1.1.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.1.2\"></times><ci id=\"S5.E13.m1.3.3.1.1.1.3.cmml\" xref=\"S5.E13.m1.3.3.1.1.1.3\">𝑃</ci><apply id=\"S5.E13.m1.3.3.1.1.1.1.1.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.1.1.1\"><csymbol cd=\"latexml\" id=\"S5.E13.m1.3.3.1.1.1.1.1.1.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.1\">conditional</csymbol><ci id=\"S5.E13.m1.3.3.1.1.1.1.1.1.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.2\">𝑝</ci><ci id=\"S5.E13.m1.3.3.1.1.1.1.1.1.3.cmml\" xref=\"S5.E13.m1.3.3.1.1.1.1.1.1.3\">𝑞</ci></apply></apply><apply id=\"S5.E13.m1.3.3.1.1.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2\"><apply id=\"S5.E13.m1.3.3.1.1.2.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E13.m1.3.3.1.1.2.2.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2\">superscript</csymbol><apply id=\"S5.E13.m1.3.3.1.1.2.2.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.E13.m1.3.3.1.1.2.2.2.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2\">subscript</csymbol><csymbol cd=\"latexml\" id=\"S5.E13.m1.3.3.1.1.2.2.2.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.2\">product</csymbol><apply id=\"S5.E13.m1.3.3.1.1.2.2.2.3.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3\"><eq id=\"S5.E13.m1.3.3.1.1.2.2.2.3.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3.1\"></eq><ci id=\"S5.E13.m1.3.3.1.1.2.2.2.3.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3.2\">𝑡</ci><cn id=\"S5.E13.m1.3.3.1.1.2.2.2.3.3.cmml\" type=\"integer\" xref=\"S5.E13.m1.3.3.1.1.2.2.2.3.3\">1</cn></apply></apply><apply id=\"S5.E13.m1.1.1.1.2.cmml\" xref=\"S5.E13.m1.1.1.1.3\"><abs id=\"S5.E13.m1.1.1.1.2.1.cmml\" xref=\"S5.E13.m1.1.1.1.3.1\"></abs><ci id=\"S5.E13.m1.1.1.1.1.cmml\" xref=\"S5.E13.m1.1.1.1.1\">𝑝</ci></apply></apply><apply id=\"S5.E13.m1.3.3.1.1.2.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1\"><times id=\"S5.E13.m1.3.3.1.1.2.1.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1.2\"></times><ci id=\"S5.E13.m1.3.3.1.1.2.1.3.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1.3\">𝑠</ci><interval closure=\"open\" id=\"S5.E13.m1.3.3.1.1.2.1.1.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1\"><apply id=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.1.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1.1\">subscript</csymbol><ci id=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.2.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.2\">𝑟</ci><ci id=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.3.cmml\" xref=\"S5.E13.m1.3.3.1.1.2.1.1.1.1.3\">𝑡</ci></apply><ci id=\"S5.E13.m1.2.2.cmml\" xref=\"S5.E13.m1.2.2\">𝑞</ci></interval></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E13.m1.3c\">P(p|q)=\\prod_{t=1}^{|p|}s(r_{t},q),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E13.m1.3d\">italic_P ( italic_p | italic_q ) = ∏ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_p | end_POSTSUPERSCRIPT italic_s ( italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_q ) ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(13)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS5.SSS1.p1.9\">where <math alttext=\"p\" display=\"inline\" id=\"S5.SS5.SSS1.p1.6.m1.1\"><semantics id=\"S5.SS5.SSS1.p1.6.m1.1a\"><mi id=\"S5.SS5.SSS1.p1.6.m1.1.1\" xref=\"S5.SS5.SSS1.p1.6.m1.1.1.cmml\">p</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.6.m1.1b\"><ci id=\"S5.SS5.SSS1.p1.6.m1.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.6.m1.1.1\">𝑝</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.6.m1.1c\">p</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.6.m1.1d\">italic_p</annotation></semantics></math> denotes the path, and <math alttext=\"r_{t}\" display=\"inline\" id=\"S5.SS5.SSS1.p1.7.m2.1\"><semantics id=\"S5.SS5.SSS1.p1.7.m2.1a\"><msub id=\"S5.SS5.SSS1.p1.7.m2.1.1\" xref=\"S5.SS5.SSS1.p1.7.m2.1.1.cmml\"><mi id=\"S5.SS5.SSS1.p1.7.m2.1.1.2\" xref=\"S5.SS5.SSS1.p1.7.m2.1.1.2.cmml\">r</mi><mi id=\"S5.SS5.SSS1.p1.7.m2.1.1.3\" xref=\"S5.SS5.SSS1.p1.7.m2.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.7.m2.1b\"><apply id=\"S5.SS5.SSS1.p1.7.m2.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.7.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS5.SSS1.p1.7.m2.1.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.7.m2.1.1\">subscript</csymbol><ci id=\"S5.SS5.SSS1.p1.7.m2.1.1.2.cmml\" xref=\"S5.SS5.SSS1.p1.7.m2.1.1.2\">𝑟</ci><ci id=\"S5.SS5.SSS1.p1.7.m2.1.1.3.cmml\" xref=\"S5.SS5.SSS1.p1.7.m2.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.7.m2.1c\">r_{t}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.7.m2.1d\">italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> denotes the relation at the <math alttext=\"t\" display=\"inline\" id=\"S5.SS5.SSS1.p1.8.m3.1\"><semantics id=\"S5.SS5.SSS1.p1.8.m3.1a\"><mi id=\"S5.SS5.SSS1.p1.8.m3.1.1\" xref=\"S5.SS5.SSS1.p1.8.m3.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.8.m3.1b\"><ci id=\"S5.SS5.SSS1.p1.8.m3.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.8.m3.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.8.m3.1c\">t</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.8.m3.1d\">italic_t</annotation></semantics></math>-th hop of <math alttext=\"p\" display=\"inline\" id=\"S5.SS5.SSS1.p1.9.m4.1\"><semantics id=\"S5.SS5.SSS1.p1.9.m4.1a\"><mi id=\"S5.SS5.SSS1.p1.9.m4.1.1\" xref=\"S5.SS5.SSS1.p1.9.m4.1.1.cmml\">p</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.9.m4.1b\"><ci id=\"S5.SS5.SSS1.p1.9.m4.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.9.m4.1.1\">𝑝</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.9.m4.1c\">p</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.9.m4.1d\">italic_p</annotation></semantics></math>. The retrieved relations and paths can be used as context knowledge to improve the performance of answer reasoners as</p>\n<table id=\"S5.E14\">\n<tbody><tr>\n<td></td>\n<td><math alttext=\"P(a|q)=\\sum_{p\\in\\mathcal{P}}P(a|p)P(p|q),\" display=\"block\" id=\"S5.E14.m1.1\"><semantics id=\"S5.E14.m1.1a\"><mrow id=\"S5.E14.m1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.cmml\"><mrow id=\"S5.E14.m1.1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.cmml\"><mrow id=\"S5.E14.m1.1.1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.1.cmml\"><mi id=\"S5.E14.m1.1.1.1.1.1.3\" xref=\"S5.E14.m1.1.1.1.1.1.3.cmml\">P</mi><mo id=\"S5.E14.m1.1.1.1.1.1.2\" xref=\"S5.E14.m1.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E14.m1.1.1.1.1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S5.E14.m1.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S5.E14.m1.1.1.1.1.1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S5.E14.m1.1.1.1.1.1.1.1.1.2\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.2.cmml\">a</mi><mo fence=\"false\" id=\"S5.E14.m1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.1.cmml\">|</mo><mi id=\"S5.E14.m1.1.1.1.1.1.1.1.1.3\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.3.cmml\">q</mi></mrow><mo id=\"S5.E14.m1.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo id=\"S5.E14.m1.1.1.1.1.4\" rspace=\"0.111em\" xref=\"S5.E14.m1.1.1.1.1.4.cmml\">=</mo><mrow id=\"S5.E14.m1.1.1.1.1.3\" xref=\"S5.E14.m1.1.1.1.1.3.cmml\"><munder id=\"S5.E14.m1.1.1.1.1.3.3\" xref=\"S5.E14.m1.1.1.1.1.3.3.cmml\"><mo id=\"S5.E14.m1.1.1.1.1.3.3.2\" movablelimits=\"false\" xref=\"S5.E14.m1.1.1.1.1.3.3.2.cmml\">∑</mo><mrow id=\"S5.E14.m1.1.1.1.1.3.3.3\" xref=\"S5.E14.m1.1.1.1.1.3.3.3.cmml\"><mi id=\"S5.E14.m1.1.1.1.1.3.3.3.2\" xref=\"S5.E14.m1.1.1.1.1.3.3.3.2.cmml\">p</mi><mo id=\"S5.E14.m1.1.1.1.1.3.3.3.1\" xref=\"S5.E14.m1.1.1.1.1.3.3.3.1.cmml\">∈</mo><mi id=\"S5.E14.m1.1.1.1.1.3.3.3.3\" xref=\"S5.E14.m1.1.1.1.1.3.3.3.3.cmml\">𝒫</mi></mrow></munder><mrow id=\"S5.E14.m1.1.1.1.1.3.2\" xref=\"S5.E14.m1.1.1.1.1.3.2.cmml\"><mi id=\"S5.E14.m1.1.1.1.1.3.2.4\" xref=\"S5.E14.m1.1.1.1.1.3.2.4.cmml\">P</mi><mo id=\"S5.E14.m1.1.1.1.1.3.2.3\" xref=\"S5.E14.m1.1.1.1.1.3.2.3.cmml\">⁢</mo><mrow id=\"S5.E14.m1.1.1.1.1.2.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.cmml\"><mo id=\"S5.E14.m1.1.1.1.1.2.1.1.1.2\" stretchy=\"false\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.cmml\">(</mo><mrow id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.cmml\"><mi id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.2\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.2.cmml\">a</mi><mo fence=\"false\" id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.1.cmml\">|</mo><mi id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.3\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.3.cmml\">p</mi></mrow><mo id=\"S5.E14.m1.1.1.1.1.2.1.1.1.3\" stretchy=\"false\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.cmml\">)</mo></mrow><mo id=\"S5.E14.m1.1.1.1.1.3.2.3a\" xref=\"S5.E14.m1.1.1.1.1.3.2.3.cmml\">⁢</mo><mi id=\"S5.E14.m1.1.1.1.1.3.2.5\" xref=\"S5.E14.m1.1.1.1.1.3.2.5.cmml\">P</mi><mo id=\"S5.E14.m1.1.1.1.1.3.2.3b\" xref=\"S5.E14.m1.1.1.1.1.3.2.3.cmml\">⁢</mo><mrow id=\"S5.E14.m1.1.1.1.1.3.2.2.1\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.cmml\"><mo id=\"S5.E14.m1.1.1.1.1.3.2.2.1.2\" stretchy=\"false\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.cmml\">(</mo><mrow id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.cmml\"><mi id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.2\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.2.cmml\">p</mi><mo fence=\"false\" id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.1\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.1.cmml\">|</mo><mi id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.3\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.3.cmml\">q</mi></mrow><mo id=\"S5.E14.m1.1.1.1.1.3.2.2.1.3\" stretchy=\"false\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.cmml\">)</mo></mrow></mrow></mrow></mrow><mo id=\"S5.E14.m1.1.1.1.2\" xref=\"S5.E14.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E14.m1.1b\"><apply id=\"S5.E14.m1.1.1.1.1.cmml\" xref=\"S5.E14.m1.1.1.1\"><eq id=\"S5.E14.m1.1.1.1.1.4.cmml\" xref=\"S5.E14.m1.1.1.1.1.4\"></eq><apply id=\"S5.E14.m1.1.1.1.1.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.1\"><times id=\"S5.E14.m1.1.1.1.1.1.2.cmml\" xref=\"S5.E14.m1.1.1.1.1.1.2\"></times><ci id=\"S5.E14.m1.1.1.1.1.1.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.1.3\">𝑃</ci><apply id=\"S5.E14.m1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.1.1.1\"><csymbol cd=\"latexml\" id=\"S5.E14.m1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.1\">conditional</csymbol><ci id=\"S5.E14.m1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.2\">𝑎</ci><ci id=\"S5.E14.m1.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.1.1.1.1.3\">𝑞</ci></apply></apply><apply id=\"S5.E14.m1.1.1.1.1.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.3\"><apply id=\"S5.E14.m1.1.1.1.1.3.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.3\"><csymbol cd=\"ambiguous\" id=\"S5.E14.m1.1.1.1.1.3.3.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.3\">subscript</csymbol><sum id=\"S5.E14.m1.1.1.1.1.3.3.2.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.3.2\"></sum><apply id=\"S5.E14.m1.1.1.1.1.3.3.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.3.3\"><in id=\"S5.E14.m1.1.1.1.1.3.3.3.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.3.3.1\"></in><ci id=\"S5.E14.m1.1.1.1.1.3.3.3.2.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.3.3.2\">𝑝</ci><ci id=\"S5.E14.m1.1.1.1.1.3.3.3.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.3.3.3\">𝒫</ci></apply></apply><apply id=\"S5.E14.m1.1.1.1.1.3.2.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2\"><times id=\"S5.E14.m1.1.1.1.1.3.2.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2.3\"></times><ci id=\"S5.E14.m1.1.1.1.1.3.2.4.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2.4\">𝑃</ci><apply id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1\"><csymbol cd=\"latexml\" id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.1\">conditional</csymbol><ci id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.2.cmml\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.2\">𝑎</ci><ci id=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.2.1.1.1.1.3\">𝑝</ci></apply><ci id=\"S5.E14.m1.1.1.1.1.3.2.5.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2.5\">𝑃</ci><apply id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1\"><csymbol cd=\"latexml\" id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.1.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.1\">conditional</csymbol><ci id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.2.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.2\">𝑝</ci><ci id=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.3.cmml\" xref=\"S5.E14.m1.1.1.1.1.3.2.2.1.1.3\">𝑞</ci></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E14.m1.1c\">P(a|q)=\\sum_{p\\in\\mathcal{P}}P(a|p)P(p|q),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E14.m1.1d\">italic_P ( italic_a | italic_q ) = ∑ start_POSTSUBSCRIPT italic_p ∈ caligraphic_P end_POSTSUBSCRIPT italic_P ( italic_a | italic_p ) italic_P ( italic_p | italic_q ) ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(14)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS5.SSS1.p1.11\">where <math alttext=\"\\mathcal{P}\" display=\"inline\" id=\"S5.SS5.SSS1.p1.10.m1.1\"><semantics id=\"S5.SS5.SSS1.p1.10.m1.1a\"><mi id=\"S5.SS5.SSS1.p1.10.m1.1.1\" xref=\"S5.SS5.SSS1.p1.10.m1.1.1.cmml\">𝒫</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.10.m1.1b\"><ci id=\"S5.SS5.SSS1.p1.10.m1.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.10.m1.1.1\">𝒫</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.10.m1.1c\">\\mathcal{P}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.10.m1.1d\">caligraphic_P</annotation></semantics></math> denotes retrieved paths and <math alttext=\"a\" display=\"inline\" id=\"S5.SS5.SSS1.p1.11.m2.1\"><semantics id=\"S5.SS5.SSS1.p1.11.m2.1a\"><mi id=\"S5.SS5.SSS1.p1.11.m2.1.1\" xref=\"S5.SS5.SSS1.p1.11.m2.1.1.cmml\">a</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS1.p1.11.m2.1b\"><ci id=\"S5.SS5.SSS1.p1.11.m2.1.1.cmml\" xref=\"S5.SS5.SSS1.p1.11.m2.1.1\">𝑎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS1.p1.11.m2.1c\">a</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS1.p1.11.m2.1d\">italic_a</annotation></semantics></math> denotes the answer.</p>\n</div>\n<figure id=\"S5.F22\"><img alt=\"Refer to caption\" height=\"536\" id=\"S5.F22.g1\" src=\"x19.png\" width=\"664\">\n<figcaption><span>Figure 22: </span>The general framework of applying LLMs for knowledge graph question answering (KGQA).</figcaption>\n</figure>\n</section>\n<section id=\"S5.SS5.SSS2\">\n<h4>\n<span>5.5.2 </span>LLMs as Answer Reasoners</h4>\n<div id=\"S5.SS5.SSS2.p1\">\n<p id=\"S5.SS5.SSS2.p1.13\">Answer reasoners are designed to reason over the retrieved facts and generate answers. LLMs can be used as answer reasoners to generate answers directly. For example, as shown in Fig. 3 <a href=\"https://arxiv.org/html/2306.08302v3#S5.F22\" title=\"Figure 22 ‣ 5.5.1 LLMs as Entity/relation Extractors ‣ 5.5 LLM-augmented KG Question Answering ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>22</span></a>, DEKCOR <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib175\" title=\"\">175</a>]</cite> concatenates the retrieved facts with questions and candidate answers as</p>\n<table id=\"S5.E15\">\n<tbody><tr>\n<td></td>\n<td><math alttext=\"x=\\texttt{[CLS]}\\ q\\ \\texttt{[SEP]}\\ \\text{Related Facts}\\ \\texttt{[SEP]}\\ a\\ %\n\\texttt{[SEP]},\" display=\"block\" id=\"S5.E15.m1.1\"><semantics id=\"S5.E15.m1.1a\"><mrow id=\"S5.E15.m1.1.1.1\" xref=\"S5.E15.m1.1.1.1.1.cmml\"><mrow id=\"S5.E15.m1.1.1.1.1\" xref=\"S5.E15.m1.1.1.1.1.cmml\"><mi id=\"S5.E15.m1.1.1.1.1.2\" xref=\"S5.E15.m1.1.1.1.1.2.cmml\">x</mi><mo id=\"S5.E15.m1.1.1.1.1.1\" xref=\"S5.E15.m1.1.1.1.1.1.cmml\">=</mo><mrow id=\"S5.E15.m1.1.1.1.1.3\" xref=\"S5.E15.m1.1.1.1.1.3.cmml\"><mtext id=\"S5.E15.m1.1.1.1.1.3.2\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.2a.cmml\">[CLS]</mtext><mo id=\"S5.E15.m1.1.1.1.1.3.1\" lspace=\"0.500em\" xref=\"S5.E15.m1.1.1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.E15.m1.1.1.1.1.3.3\" xref=\"S5.E15.m1.1.1.1.1.3.3.cmml\">q</mi><mo id=\"S5.E15.m1.1.1.1.1.3.1a\" lspace=\"0.500em\" xref=\"S5.E15.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E15.m1.1.1.1.1.3.4\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.4a.cmml\">[SEP]</mtext><mo id=\"S5.E15.m1.1.1.1.1.3.1b\" lspace=\"0.500em\" xref=\"S5.E15.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E15.m1.1.1.1.1.3.5\" xref=\"S5.E15.m1.1.1.1.1.3.5a.cmml\">Related Facts</mtext><mo id=\"S5.E15.m1.1.1.1.1.3.1c\" lspace=\"0.500em\" xref=\"S5.E15.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E15.m1.1.1.1.1.3.6\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.6a.cmml\">[SEP]</mtext><mo id=\"S5.E15.m1.1.1.1.1.3.1d\" lspace=\"0.500em\" xref=\"S5.E15.m1.1.1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.E15.m1.1.1.1.1.3.7\" xref=\"S5.E15.m1.1.1.1.1.3.7.cmml\">a</mi><mo id=\"S5.E15.m1.1.1.1.1.3.1e\" lspace=\"0.500em\" xref=\"S5.E15.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E15.m1.1.1.1.1.3.8\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.8a.cmml\">[SEP]</mtext></mrow></mrow><mo id=\"S5.E15.m1.1.1.1.2\" xref=\"S5.E15.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E15.m1.1b\"><apply id=\"S5.E15.m1.1.1.1.1.cmml\" xref=\"S5.E15.m1.1.1.1\"><eq id=\"S5.E15.m1.1.1.1.1.1.cmml\" xref=\"S5.E15.m1.1.1.1.1.1\"></eq><ci id=\"S5.E15.m1.1.1.1.1.2.cmml\" xref=\"S5.E15.m1.1.1.1.1.2\">𝑥</ci><apply id=\"S5.E15.m1.1.1.1.1.3.cmml\" xref=\"S5.E15.m1.1.1.1.1.3\"><times id=\"S5.E15.m1.1.1.1.1.3.1.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.1\"></times><ci id=\"S5.E15.m1.1.1.1.1.3.2a.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.2\"><mtext id=\"S5.E15.m1.1.1.1.1.3.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.2\">[CLS]</mtext></ci><ci id=\"S5.E15.m1.1.1.1.1.3.3.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.3\">𝑞</ci><ci id=\"S5.E15.m1.1.1.1.1.3.4a.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.4\"><mtext id=\"S5.E15.m1.1.1.1.1.3.4.cmml\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.4\">[SEP]</mtext></ci><ci id=\"S5.E15.m1.1.1.1.1.3.5a.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.5\"><mtext id=\"S5.E15.m1.1.1.1.1.3.5.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.5\">Related Facts</mtext></ci><ci id=\"S5.E15.m1.1.1.1.1.3.6a.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.6\"><mtext id=\"S5.E15.m1.1.1.1.1.3.6.cmml\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.6\">[SEP]</mtext></ci><ci id=\"S5.E15.m1.1.1.1.1.3.7.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.7\">𝑎</ci><ci id=\"S5.E15.m1.1.1.1.1.3.8a.cmml\" xref=\"S5.E15.m1.1.1.1.1.3.8\"><mtext id=\"S5.E15.m1.1.1.1.1.3.8.cmml\" mathvariant=\"monospace\" xref=\"S5.E15.m1.1.1.1.1.3.8\">[SEP]</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E15.m1.1c\">x=\\texttt{[CLS]}\\ q\\ \\texttt{[SEP]}\\ \\text{Related Facts}\\ \\texttt{[SEP]}\\ a\\ %\n\\texttt{[SEP]},</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E15.m1.1d\">italic_x = [CLS] italic_q [SEP] Related Facts [SEP] italic_a [SEP] ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(15)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS5.SSS2.p1.6\">where <math alttext=\"a\" display=\"inline\" id=\"S5.SS5.SSS2.p1.1.m1.1\"><semantics id=\"S5.SS5.SSS2.p1.1.m1.1a\"><mi id=\"S5.SS5.SSS2.p1.1.m1.1.1\" xref=\"S5.SS5.SSS2.p1.1.m1.1.1.cmml\">a</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.1.m1.1b\"><ci id=\"S5.SS5.SSS2.p1.1.m1.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.1.m1.1.1\">𝑎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.1.m1.1c\">a</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.1.m1.1d\">italic_a</annotation></semantics></math> denotes candidate answers. Then, it feeds them into LLMs to predict answer scores. After utilizing LLMs to generate the representation of <math alttext=\"x\" display=\"inline\" id=\"S5.SS5.SSS2.p1.2.m2.1\"><semantics id=\"S5.SS5.SSS2.p1.2.m2.1a\"><mi id=\"S5.SS5.SSS2.p1.2.m2.1.1\" xref=\"S5.SS5.SSS2.p1.2.m2.1.1.cmml\">x</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.2.m2.1b\"><ci id=\"S5.SS5.SSS2.p1.2.m2.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.2.m2.1.1\">𝑥</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.2.m2.1c\">x</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.2.m2.1d\">italic_x</annotation></semantics></math> as QA context, DRLK <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib176\" title=\"\">176</a>]</cite> proposes a Dynamic Hierarchical Reasoner to capture the interactions between QA context and answers for answer prediction. Yan et al. <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib235\" title=\"\">235</a>]</cite> propose a LLM-based KGQA framework consisting of two stages: (1) retrieve related facts from KGs and (2) generate answers based on the retrieved facts. The first stage is similar to the entity/relation extractors. Given a candidate answer entity <math alttext=\"a\" display=\"inline\" id=\"S5.SS5.SSS2.p1.3.m3.1\"><semantics id=\"S5.SS5.SSS2.p1.3.m3.1a\"><mi id=\"S5.SS5.SSS2.p1.3.m3.1.1\" xref=\"S5.SS5.SSS2.p1.3.m3.1.1.cmml\">a</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.3.m3.1b\"><ci id=\"S5.SS5.SSS2.p1.3.m3.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.3.m3.1.1\">𝑎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.3.m3.1c\">a</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.3.m3.1d\">italic_a</annotation></semantics></math>, it extracts a series of paths <math alttext=\"p_{1},\\ldots,p_{n}\" display=\"inline\" id=\"S5.SS5.SSS2.p1.4.m4.3\"><semantics id=\"S5.SS5.SSS2.p1.4.m4.3a\"><mrow id=\"S5.SS5.SSS2.p1.4.m4.3.3.2\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.3.cmml\"><msub id=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1\" xref=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.cmml\"><mi id=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.2\" xref=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.2.cmml\">p</mi><mn id=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.3\" xref=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.3.cmml\">1</mn></msub><mo id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.3\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.3.cmml\">,</mo><mi id=\"S5.SS5.SSS2.p1.4.m4.1.1\" mathvariant=\"normal\" xref=\"S5.SS5.SSS2.p1.4.m4.1.1.cmml\">…</mi><mo id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.4\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.3.cmml\">,</mo><msub id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.cmml\"><mi id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.2\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.2.cmml\">p</mi><mi id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.3\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.3.cmml\">n</mi></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.4.m4.3b\"><list id=\"S5.SS5.SSS2.p1.4.m4.3.3.3.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2\"><apply id=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1\">subscript</csymbol><ci id=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.2.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.2\">𝑝</ci><cn id=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.3.cmml\" type=\"integer\" xref=\"S5.SS5.SSS2.p1.4.m4.2.2.1.1.3\">1</cn></apply><ci id=\"S5.SS5.SSS2.p1.4.m4.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.1.1\">…</ci><apply id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.1.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2\">subscript</csymbol><ci id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.2.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.2\">𝑝</ci><ci id=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.3.cmml\" xref=\"S5.SS5.SSS2.p1.4.m4.3.3.2.2.3\">𝑛</ci></apply></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.4.m4.3c\">p_{1},\\ldots,p_{n}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.4.m4.3d\">italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> from KGs. But the second stage is a LLM-based answer reasoner. It first verbalizes the paths by using the entity names and relation names in KGs. Then, it concatenates the question <math alttext=\"q\" display=\"inline\" id=\"S5.SS5.SSS2.p1.5.m5.1\"><semantics id=\"S5.SS5.SSS2.p1.5.m5.1a\"><mi id=\"S5.SS5.SSS2.p1.5.m5.1.1\" xref=\"S5.SS5.SSS2.p1.5.m5.1.1.cmml\">q</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.5.m5.1b\"><ci id=\"S5.SS5.SSS2.p1.5.m5.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.5.m5.1.1\">𝑞</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.5.m5.1c\">q</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.5.m5.1d\">italic_q</annotation></semantics></math> and all paths <math alttext=\"p_{1},\\ldots,p_{n}\" display=\"inline\" id=\"S5.SS5.SSS2.p1.6.m6.3\"><semantics id=\"S5.SS5.SSS2.p1.6.m6.3a\"><mrow id=\"S5.SS5.SSS2.p1.6.m6.3.3.2\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.3.cmml\"><msub id=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1\" xref=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.cmml\"><mi id=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.2\" xref=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.2.cmml\">p</mi><mn id=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.3\" xref=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.3.cmml\">1</mn></msub><mo id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.3\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.3.cmml\">,</mo><mi id=\"S5.SS5.SSS2.p1.6.m6.1.1\" mathvariant=\"normal\" xref=\"S5.SS5.SSS2.p1.6.m6.1.1.cmml\">…</mi><mo id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.4\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.3.cmml\">,</mo><msub id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.cmml\"><mi id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.2\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.2.cmml\">p</mi><mi id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.3\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.3.cmml\">n</mi></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.6.m6.3b\"><list id=\"S5.SS5.SSS2.p1.6.m6.3.3.3.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2\"><apply id=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1\">subscript</csymbol><ci id=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.2.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.2\">𝑝</ci><cn id=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.3.cmml\" type=\"integer\" xref=\"S5.SS5.SSS2.p1.6.m6.2.2.1.1.3\">1</cn></apply><ci id=\"S5.SS5.SSS2.p1.6.m6.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.1.1\">…</ci><apply id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.1.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2\">subscript</csymbol><ci id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.2.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.2\">𝑝</ci><ci id=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.3.cmml\" xref=\"S5.SS5.SSS2.p1.6.m6.3.3.2.2.3\">𝑛</ci></apply></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.6.m6.3c\">p_{1},\\ldots,p_{n}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.6.m6.3d\">italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> to make an input sample as</p>\n<table id=\"S5.E16\">\n<tbody><tr>\n<td></td>\n<td><math alttext=\"x=\\texttt{[CLS]}\\ q\\ \\texttt{[SEP]}\\ p_{1}\\ \\texttt{[SEP]}\\ \\cdots\\ \\texttt{[%\nSEP]}\\ p_{n}\\ \\texttt{[SEP]}.\" display=\"block\" id=\"S5.E16.m1.1\"><semantics id=\"S5.E16.m1.1a\"><mrow id=\"S5.E16.m1.1.1.1\" xref=\"S5.E16.m1.1.1.1.1.cmml\"><mrow id=\"S5.E16.m1.1.1.1.1\" xref=\"S5.E16.m1.1.1.1.1.cmml\"><mi id=\"S5.E16.m1.1.1.1.1.2\" xref=\"S5.E16.m1.1.1.1.1.2.cmml\">x</mi><mo id=\"S5.E16.m1.1.1.1.1.1\" xref=\"S5.E16.m1.1.1.1.1.1.cmml\">=</mo><mrow id=\"S5.E16.m1.1.1.1.1.3\" xref=\"S5.E16.m1.1.1.1.1.3.cmml\"><mtext id=\"S5.E16.m1.1.1.1.1.3.2\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.2a.cmml\">[CLS]</mtext><mo id=\"S5.E16.m1.1.1.1.1.3.1\" lspace=\"0.500em\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.E16.m1.1.1.1.1.3.3\" xref=\"S5.E16.m1.1.1.1.1.3.3.cmml\">q</mi><mo id=\"S5.E16.m1.1.1.1.1.3.1a\" lspace=\"0.500em\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E16.m1.1.1.1.1.3.4\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.4a.cmml\">[SEP]</mtext><mo id=\"S5.E16.m1.1.1.1.1.3.1b\" lspace=\"0.500em\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E16.m1.1.1.1.1.3.5\" xref=\"S5.E16.m1.1.1.1.1.3.5.cmml\"><mi id=\"S5.E16.m1.1.1.1.1.3.5.2\" xref=\"S5.E16.m1.1.1.1.1.3.5.2.cmml\">p</mi><mn id=\"S5.E16.m1.1.1.1.1.3.5.3\" xref=\"S5.E16.m1.1.1.1.1.3.5.3.cmml\">1</mn></msub><mo id=\"S5.E16.m1.1.1.1.1.3.1c\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E16.m1.1.1.1.1.3.6\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.6a.cmml\">[SEP]</mtext><mo id=\"S5.E16.m1.1.1.1.1.3.1d\" lspace=\"0.500em\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><mi id=\"S5.E16.m1.1.1.1.1.3.7\" mathvariant=\"normal\" xref=\"S5.E16.m1.1.1.1.1.3.7.cmml\">⋯</mi><mo id=\"S5.E16.m1.1.1.1.1.3.1e\" lspace=\"0.500em\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E16.m1.1.1.1.1.3.8\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.8a.cmml\">[SEP]</mtext><mo id=\"S5.E16.m1.1.1.1.1.3.1f\" lspace=\"0.500em\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><msub id=\"S5.E16.m1.1.1.1.1.3.9\" xref=\"S5.E16.m1.1.1.1.1.3.9.cmml\"><mi id=\"S5.E16.m1.1.1.1.1.3.9.2\" xref=\"S5.E16.m1.1.1.1.1.3.9.2.cmml\">p</mi><mi id=\"S5.E16.m1.1.1.1.1.3.9.3\" xref=\"S5.E16.m1.1.1.1.1.3.9.3.cmml\">n</mi></msub><mo id=\"S5.E16.m1.1.1.1.1.3.1g\" xref=\"S5.E16.m1.1.1.1.1.3.1.cmml\">⁢</mo><mtext id=\"S5.E16.m1.1.1.1.1.3.10\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.10a.cmml\">[SEP]</mtext></mrow></mrow><mo id=\"S5.E16.m1.1.1.1.2\" lspace=\"0em\" xref=\"S5.E16.m1.1.1.1.1.cmml\">.</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E16.m1.1b\"><apply id=\"S5.E16.m1.1.1.1.1.cmml\" xref=\"S5.E16.m1.1.1.1\"><eq id=\"S5.E16.m1.1.1.1.1.1.cmml\" xref=\"S5.E16.m1.1.1.1.1.1\"></eq><ci id=\"S5.E16.m1.1.1.1.1.2.cmml\" xref=\"S5.E16.m1.1.1.1.1.2\">𝑥</ci><apply id=\"S5.E16.m1.1.1.1.1.3.cmml\" xref=\"S5.E16.m1.1.1.1.1.3\"><times id=\"S5.E16.m1.1.1.1.1.3.1.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.1\"></times><ci id=\"S5.E16.m1.1.1.1.1.3.2a.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.2\"><mtext id=\"S5.E16.m1.1.1.1.1.3.2.cmml\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.2\">[CLS]</mtext></ci><ci id=\"S5.E16.m1.1.1.1.1.3.3.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.3\">𝑞</ci><ci id=\"S5.E16.m1.1.1.1.1.3.4a.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.4\"><mtext id=\"S5.E16.m1.1.1.1.1.3.4.cmml\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.4\">[SEP]</mtext></ci><apply id=\"S5.E16.m1.1.1.1.1.3.5.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.5\"><csymbol cd=\"ambiguous\" id=\"S5.E16.m1.1.1.1.1.3.5.1.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.5\">subscript</csymbol><ci id=\"S5.E16.m1.1.1.1.1.3.5.2.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.5.2\">𝑝</ci><cn id=\"S5.E16.m1.1.1.1.1.3.5.3.cmml\" type=\"integer\" xref=\"S5.E16.m1.1.1.1.1.3.5.3\">1</cn></apply><ci id=\"S5.E16.m1.1.1.1.1.3.6a.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.6\"><mtext id=\"S5.E16.m1.1.1.1.1.3.6.cmml\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.6\">[SEP]</mtext></ci><ci id=\"S5.E16.m1.1.1.1.1.3.7.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.7\">⋯</ci><ci id=\"S5.E16.m1.1.1.1.1.3.8a.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.8\"><mtext id=\"S5.E16.m1.1.1.1.1.3.8.cmml\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.8\">[SEP]</mtext></ci><apply id=\"S5.E16.m1.1.1.1.1.3.9.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.9\"><csymbol cd=\"ambiguous\" id=\"S5.E16.m1.1.1.1.1.3.9.1.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.9\">subscript</csymbol><ci id=\"S5.E16.m1.1.1.1.1.3.9.2.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.9.2\">𝑝</ci><ci id=\"S5.E16.m1.1.1.1.1.3.9.3.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.9.3\">𝑛</ci></apply><ci id=\"S5.E16.m1.1.1.1.1.3.10a.cmml\" xref=\"S5.E16.m1.1.1.1.1.3.10\"><mtext id=\"S5.E16.m1.1.1.1.1.3.10.cmml\" mathvariant=\"monospace\" xref=\"S5.E16.m1.1.1.1.1.3.10\">[SEP]</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E16.m1.1c\">x=\\texttt{[CLS]}\\ q\\ \\texttt{[SEP]}\\ p_{1}\\ \\texttt{[SEP]}\\ \\cdots\\ \\texttt{[%\nSEP]}\\ p_{n}\\ \\texttt{[SEP]}.</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E16.m1.1d\">italic_x = [CLS] italic_q [SEP] italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT [SEP] ⋯ [SEP] italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT [SEP] .</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(16)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS5.SSS2.p1.9\">These paths are regarded as the related facts for the candidate answer <math alttext=\"a\" display=\"inline\" id=\"S5.SS5.SSS2.p1.7.m1.1\"><semantics id=\"S5.SS5.SSS2.p1.7.m1.1a\"><mi id=\"S5.SS5.SSS2.p1.7.m1.1.1\" xref=\"S5.SS5.SSS2.p1.7.m1.1.1.cmml\">a</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.7.m1.1b\"><ci id=\"S5.SS5.SSS2.p1.7.m1.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.7.m1.1.1\">𝑎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.7.m1.1c\">a</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.7.m1.1d\">italic_a</annotation></semantics></math>. Finally, it uses LLMs to predict whether the hypothesis: “<math alttext=\"a\" display=\"inline\" id=\"S5.SS5.SSS2.p1.8.m2.1\"><semantics id=\"S5.SS5.SSS2.p1.8.m2.1a\"><mi id=\"S5.SS5.SSS2.p1.8.m2.1.1\" xref=\"S5.SS5.SSS2.p1.8.m2.1.1.cmml\">a</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.8.m2.1b\"><ci id=\"S5.SS5.SSS2.p1.8.m2.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.8.m2.1.1\">𝑎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.8.m2.1c\">a</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.8.m2.1d\">italic_a</annotation></semantics></math> is the answer of <math alttext=\"q\" display=\"inline\" id=\"S5.SS5.SSS2.p1.9.m3.1\"><semantics id=\"S5.SS5.SSS2.p1.9.m3.1a\"><mi id=\"S5.SS5.SSS2.p1.9.m3.1.1\" xref=\"S5.SS5.SSS2.p1.9.m3.1.1.cmml\">q</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.9.m3.1b\"><ci id=\"S5.SS5.SSS2.p1.9.m3.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.9.m3.1.1\">𝑞</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.9.m3.1c\">q</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.9.m3.1d\">italic_q</annotation></semantics></math>” is supported by those facts, which is formulated as\n</p>\n<table id=\"A1.EGx3\">\n<tbody id=\"S5.E17\"><tr>\n<td></td>\n<td><math alttext=\"\\displaystyle e_{\\texttt{[CLS]}}\" display=\"inline\" id=\"S5.E17.m1.1\"><semantics id=\"S5.E17.m1.1a\"><msub id=\"S5.E17.m1.1.1\" xref=\"S5.E17.m1.1.1.cmml\"><mi id=\"S5.E17.m1.1.1.2\" xref=\"S5.E17.m1.1.1.2.cmml\">e</mi><mtext id=\"S5.E17.m1.1.1.3\" mathvariant=\"monospace\" xref=\"S5.E17.m1.1.1.3a.cmml\">[CLS]</mtext></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.E17.m1.1b\"><apply id=\"S5.E17.m1.1.1.cmml\" xref=\"S5.E17.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E17.m1.1.1.1.cmml\" xref=\"S5.E17.m1.1.1\">subscript</csymbol><ci id=\"S5.E17.m1.1.1.2.cmml\" xref=\"S5.E17.m1.1.1.2\">𝑒</ci><ci id=\"S5.E17.m1.1.1.3a.cmml\" xref=\"S5.E17.m1.1.1.3\"><mtext id=\"S5.E17.m1.1.1.3.cmml\" mathsize=\"70%\" mathvariant=\"monospace\" xref=\"S5.E17.m1.1.1.3\">[CLS]</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E17.m1.1c\">\\displaystyle e_{\\texttt{[CLS]}}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E17.m1.1d\">italic_e start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT</annotation></semantics></math></td>\n<td><math alttext=\"\\displaystyle=\\text{LLM}(x),\" display=\"inline\" id=\"S5.E17.m2.2\"><semantics id=\"S5.E17.m2.2a\"><mrow id=\"S5.E17.m2.2.2.1\" xref=\"S5.E17.m2.2.2.1.1.cmml\"><mrow id=\"S5.E17.m2.2.2.1.1\" xref=\"S5.E17.m2.2.2.1.1.cmml\"><mi id=\"S5.E17.m2.2.2.1.1.2\" xref=\"S5.E17.m2.2.2.1.1.2.cmml\"></mi><mo id=\"S5.E17.m2.2.2.1.1.1\" xref=\"S5.E17.m2.2.2.1.1.1.cmml\">=</mo><mrow id=\"S5.E17.m2.2.2.1.1.3\" xref=\"S5.E17.m2.2.2.1.1.3.cmml\"><mtext id=\"S5.E17.m2.2.2.1.1.3.2\" xref=\"S5.E17.m2.2.2.1.1.3.2a.cmml\">LLM</mtext><mo id=\"S5.E17.m2.2.2.1.1.3.1\" xref=\"S5.E17.m2.2.2.1.1.3.1.cmml\">⁢</mo><mrow id=\"S5.E17.m2.2.2.1.1.3.3.2\" xref=\"S5.E17.m2.2.2.1.1.3.cmml\"><mo id=\"S5.E17.m2.2.2.1.1.3.3.2.1\" stretchy=\"false\" xref=\"S5.E17.m2.2.2.1.1.3.cmml\">(</mo><mi id=\"S5.E17.m2.1.1\" xref=\"S5.E17.m2.1.1.cmml\">x</mi><mo id=\"S5.E17.m2.2.2.1.1.3.3.2.2\" stretchy=\"false\" xref=\"S5.E17.m2.2.2.1.1.3.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E17.m2.2.2.1.2\" xref=\"S5.E17.m2.2.2.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E17.m2.2b\"><apply id=\"S5.E17.m2.2.2.1.1.cmml\" xref=\"S5.E17.m2.2.2.1\"><eq id=\"S5.E17.m2.2.2.1.1.1.cmml\" xref=\"S5.E17.m2.2.2.1.1.1\"></eq><csymbol cd=\"latexml\" id=\"S5.E17.m2.2.2.1.1.2.cmml\" xref=\"S5.E17.m2.2.2.1.1.2\">absent</csymbol><apply id=\"S5.E17.m2.2.2.1.1.3.cmml\" xref=\"S5.E17.m2.2.2.1.1.3\"><times id=\"S5.E17.m2.2.2.1.1.3.1.cmml\" xref=\"S5.E17.m2.2.2.1.1.3.1\"></times><ci id=\"S5.E17.m2.2.2.1.1.3.2a.cmml\" xref=\"S5.E17.m2.2.2.1.1.3.2\"><mtext id=\"S5.E17.m2.2.2.1.1.3.2.cmml\" xref=\"S5.E17.m2.2.2.1.1.3.2\">LLM</mtext></ci><ci id=\"S5.E17.m2.1.1.cmml\" xref=\"S5.E17.m2.1.1\">𝑥</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E17.m2.2c\">\\displaystyle=\\text{LLM}(x),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E17.m2.2d\">= LLM ( italic_x ) ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(17)</span></td>\n</tr></tbody>\n<tbody id=\"S5.E18\"><tr>\n<td></td>\n<td><math alttext=\"\\displaystyle s\" display=\"inline\" id=\"S5.E18.m1.1\"><semantics id=\"S5.E18.m1.1a\"><mi id=\"S5.E18.m1.1.1\" xref=\"S5.E18.m1.1.1.cmml\">s</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.E18.m1.1b\"><ci id=\"S5.E18.m1.1.1.cmml\" xref=\"S5.E18.m1.1.1\">𝑠</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E18.m1.1c\">\\displaystyle s</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E18.m1.1d\">italic_s</annotation></semantics></math></td>\n<td><math alttext=\"\\displaystyle=\\sigma(\\text{MLP}(e_{\\texttt{[CLS]}})),\" display=\"inline\" id=\"S5.E18.m2.1\"><semantics id=\"S5.E18.m2.1a\"><mrow id=\"S5.E18.m2.1.1.1\" xref=\"S5.E18.m2.1.1.1.1.cmml\"><mrow id=\"S5.E18.m2.1.1.1.1\" xref=\"S5.E18.m2.1.1.1.1.cmml\"><mi id=\"S5.E18.m2.1.1.1.1.3\" xref=\"S5.E18.m2.1.1.1.1.3.cmml\"></mi><mo id=\"S5.E18.m2.1.1.1.1.2\" xref=\"S5.E18.m2.1.1.1.1.2.cmml\">=</mo><mrow id=\"S5.E18.m2.1.1.1.1.1\" xref=\"S5.E18.m2.1.1.1.1.1.cmml\"><mi id=\"S5.E18.m2.1.1.1.1.1.3\" xref=\"S5.E18.m2.1.1.1.1.1.3.cmml\">σ</mi><mo id=\"S5.E18.m2.1.1.1.1.1.2\" xref=\"S5.E18.m2.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E18.m2.1.1.1.1.1.1.1\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S5.E18.m2.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S5.E18.m2.1.1.1.1.1.1.1.1\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.cmml\"><mtext id=\"S5.E18.m2.1.1.1.1.1.1.1.1.3\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.3a.cmml\">MLP</mtext><mo id=\"S5.E18.m2.1.1.1.1.1.1.1.1.2\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.2.cmml\">⁢</mo><mrow id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.cmml\"><mo id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.2\" stretchy=\"false\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.cmml\">(</mo><msub id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.2\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.2.cmml\">e</mi><mtext id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.3\" mathvariant=\"monospace\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.3a.cmml\">[CLS]</mtext></msub><mo id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo id=\"S5.E18.m2.1.1.1.1.1.1.1.3\" stretchy=\"false\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S5.E18.m2.1.1.1.2\" xref=\"S5.E18.m2.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E18.m2.1b\"><apply id=\"S5.E18.m2.1.1.1.1.cmml\" xref=\"S5.E18.m2.1.1.1\"><eq id=\"S5.E18.m2.1.1.1.1.2.cmml\" xref=\"S5.E18.m2.1.1.1.1.2\"></eq><csymbol cd=\"latexml\" id=\"S5.E18.m2.1.1.1.1.3.cmml\" xref=\"S5.E18.m2.1.1.1.1.3\">absent</csymbol><apply id=\"S5.E18.m2.1.1.1.1.1.cmml\" xref=\"S5.E18.m2.1.1.1.1.1\"><times id=\"S5.E18.m2.1.1.1.1.1.2.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.2\"></times><ci id=\"S5.E18.m2.1.1.1.1.1.3.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.3\">𝜎</ci><apply id=\"S5.E18.m2.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1\"><times id=\"S5.E18.m2.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.2\"></times><ci id=\"S5.E18.m2.1.1.1.1.1.1.1.1.3a.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.3\"><mtext id=\"S5.E18.m2.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.3\">MLP</mtext></ci><apply id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.2\">𝑒</ci><ci id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.3a.cmml\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.3\"><mtext id=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.3.cmml\" mathsize=\"70%\" mathvariant=\"monospace\" xref=\"S5.E18.m2.1.1.1.1.1.1.1.1.1.1.1.3\">[CLS]</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E18.m2.1c\">\\displaystyle=\\sigma(\\text{MLP}(e_{\\texttt{[CLS]}})),</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.E18.m2.1d\">= italic_σ ( MLP ( italic_e start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>\n<td></td>\n<td rowspan=\"1\"><span>(18)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S5.SS5.SSS2.p1.12\">where it encodes <math alttext=\"x\" display=\"inline\" id=\"S5.SS5.SSS2.p1.10.m1.1\"><semantics id=\"S5.SS5.SSS2.p1.10.m1.1a\"><mi id=\"S5.SS5.SSS2.p1.10.m1.1.1\" xref=\"S5.SS5.SSS2.p1.10.m1.1.1.cmml\">x</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.10.m1.1b\"><ci id=\"S5.SS5.SSS2.p1.10.m1.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.10.m1.1.1\">𝑥</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.10.m1.1c\">x</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.10.m1.1d\">italic_x</annotation></semantics></math> using a LLM and feeds representation corresponding to <span id=\"S5.SS5.SSS2.p1.12.1\">[CLS]</span> token for binary classification, and <math alttext=\"\\sigma(\\cdot)\" display=\"inline\" id=\"S5.SS5.SSS2.p1.12.m3.1\"><semantics id=\"S5.SS5.SSS2.p1.12.m3.1a\"><mrow id=\"S5.SS5.SSS2.p1.12.m3.1.2\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.cmml\"><mi id=\"S5.SS5.SSS2.p1.12.m3.1.2.2\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.2.cmml\">σ</mi><mo id=\"S5.SS5.SSS2.p1.12.m3.1.2.1\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.1.cmml\">⁢</mo><mrow id=\"S5.SS5.SSS2.p1.12.m3.1.2.3.2\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.cmml\"><mo id=\"S5.SS5.SSS2.p1.12.m3.1.2.3.2.1\" stretchy=\"false\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.cmml\">(</mo><mo id=\"S5.SS5.SSS2.p1.12.m3.1.1\" lspace=\"0em\" rspace=\"0em\" xref=\"S5.SS5.SSS2.p1.12.m3.1.1.cmml\">⋅</mo><mo id=\"S5.SS5.SSS2.p1.12.m3.1.2.3.2.2\" stretchy=\"false\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS5.SSS2.p1.12.m3.1b\"><apply id=\"S5.SS5.SSS2.p1.12.m3.1.2.cmml\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2\"><times id=\"S5.SS5.SSS2.p1.12.m3.1.2.1.cmml\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.1\"></times><ci id=\"S5.SS5.SSS2.p1.12.m3.1.2.2.cmml\" xref=\"S5.SS5.SSS2.p1.12.m3.1.2.2\">𝜎</ci><ci id=\"S5.SS5.SSS2.p1.12.m3.1.1.cmml\" xref=\"S5.SS5.SSS2.p1.12.m3.1.1\">⋅</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS5.SSS2.p1.12.m3.1c\">\\sigma(\\cdot)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.SS5.SSS2.p1.12.m3.1d\">italic_σ ( ⋅ )</annotation></semantics></math> denotes the sigmoid function.</p>\n</div>\n<p id=\"S5.SS5.SSS2.p2.1\">To better guide LLMs reason through KGs, OreoLM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib177\" title=\"\">177</a>]</cite> proposes a Knowledge Interaction Layer (KIL) which is inserted amid LLM layers. KIL interacts with a KG reasoning module, where it discovers different reasoning paths, and then the reasoning module can reason over the paths to generate answers. GreaseLM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib178\" title=\"\">178</a>]</cite> fuses the representations from LLMs and graph neural networks to effectively reason over KG facts and language context. UniKGQA <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib43\" title=\"\">43</a>]</cite> unifies the facts retrieval and reasoning into a unified framework. UniKGQA consists of two modules. The first module is a semantic matching module that uses a LLM to match questions with their corresponding relations semantically. The second module is a matching information propagation module, which propagates the matching information along directed edges on KGs for answer reasoning. Similarly, ReLMKG <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib179\" title=\"\">179</a>]</cite> performs joint reasoning on a large language model and the associated knowledge graph. The question and verbalized paths are encoded by the language model, and different layers of the language model produce outputs that guide a graph neural network to perform message passing. This process utilizes the explicit knowledge contained in the structured knowledge graph for reasoning purposes. StructGPT <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib237\" title=\"\">237</a>]</cite> adopts a customized interface to allow large language models (e.g., ChatGPT) directly reasoning on KGs to perform multi-step question answering.</p>\n<figure id=\"S5.T4\">\n<figcaption><span>TABLE IV: </span>Summary of methods that synergize KGs and LLMs.</figcaption>\n\n</figure>\n</section>\n</section>\n</section>\n<section id=\"S6\">\n<h2>\n<span>6 </span><span id=\"S6.1.1\">Synergized LLMs + KGs</span>\n</h2>\n<p id=\"S6.p1.1\">The synergy of LLMs and KGs has attracted increasing attention these years, which marries the merits of LLMs and KGs to mutually enhance performance in various downstream applications. For example, LLMs can be used to understand natural language, while KGs are treated as a knowledge base, which provides factual knowledge. The unification of LLMs and KGs could result in a powerful model for knowledge representation and reasoning.</p>\n<p id=\"S6.p2.1\">In this section, we will discuss the state-of-the-art <span id=\"S6.p2.1.1\">Synergized LLMs + KGs</span> from two perspectives: <em id=\"S6.p2.1.2\">1) Synergized Knowledge Representation</em>, and <em id=\"S6.p2.1.3\">2)</em> Synergized Reasoning. Representative works are summarized in Table <a href=\"https://arxiv.org/html/2306.08302v3#S5.T4\" title=\"TABLE IV ‣ 5.5.2 LLMs as Answer Reasoners ‣ 5.5 LLM-augmented KG Question Answering ‣ 5 LLM-augmented KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>IV</span></a>.\n</p>\n<figure id=\"S6.F23\"><img alt=\"Refer to caption\" height=\"416\" id=\"S6.F23.g1\" src=\"x20.png\" width=\"581\">\n<figcaption><span>Figure 23: </span>Synergized knowledge representation by additional KG fusion modules.</figcaption>\n</figure>\n<section id=\"S6.SS1\">\n<h3>\n<span>6.1 </span><span id=\"S6.SS1.1.1\">Synergized Knowledge Representation</span>\n</h3>\n<p id=\"S6.SS1.p1.1\">Text corpus and knowledge graphs both contain enormous knowledge. However, the knowledge in text corpus is usually implicit and unstructured, while the knowledge in KGs is explicit and structured. Synergized Knowledge Representation aims to design a synergized model that can effectively represent knowledge from both LLMs and KGs. The synergized model can provide a better understanding of the knowledge from both sources, making it valuable for many downstream tasks.</p>\n<p id=\"S6.SS1.p2.1\">To jointly represent the knowledge, researchers propose the synergized models by introducing additional KG fusion modules, which are jointly trained with LLMs. As shown in Fig.&nbsp;<a href=\"https://arxiv.org/html/2306.08302v3#S6.F23\" title=\"Figure 23 ‣ 6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>23</span></a>, ERNIE&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib35\" title=\"\">35</a>]</cite> proposes a textual-knowledge dual encoder architecture where a <em id=\"S6.SS1.p2.1.1\">T-encoder</em> first encodes the input sentences, then a <em id=\"S6.SS1.p2.1.2\">K-encoder</em> processes knowledge graphs which are fused them with the textual representation from the <em id=\"S6.SS1.p2.1.3\">T-encoder</em>. BERT-MK&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib241\" title=\"\">241</a>]</cite> employs a similar dual-encoder architecture but it introduces additional information of neighboring entities in the knowledge encoder component during the pre-training of LLMs. However, some of the neighboring entities in KGs may not be relevant to the input text, resulting in extra redundancy and noise. CokeBERT&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib242\" title=\"\">242</a>]</cite> focuses on this issue and proposes a GNN-based module to filter out irrelevant KG entities using the input text. JAKET&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib243\" title=\"\">243</a>]</cite> proposes to fuse the entity information in the middle of the large language model.</p>\n<p id=\"S6.SS1.p3.1\">KEPLER <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib40\" title=\"\">40</a>]</cite> presents a unified model for knowledge embedding and pre-trained language representation. In KEPLER, they encode textual entity descriptions with a LLM as their embeddings, and then jointly optimize the knowledge embedding and language modeling objectives. JointGT&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib42\" title=\"\">42</a>]</cite> proposes a graph-text joint representation learning model, which proposes three pre-training tasks to align representations of graph and text. DRAGON <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib44\" title=\"\">44</a>]</cite> presents a self-supervised method to pre-train a joint language-knowledge foundation model from text and KG. It takes text segments and relevant KG subgraphs as input and bidirectionally fuses information from both modalities. Then, DRAGON utilizes two self-supervised reasoning tasks, i.e., masked language modeling and KG link prediction to optimize the model parameters. HKLM <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib238\" title=\"\">238</a>]</cite> introduces a unified LLM which incorporates KGs to learn representations of domain-specific knowledge.</p>\n</section>\n<section id=\"S6.SS2\">\n<h3>\n<span>6.2 </span><span id=\"S6.SS2.1.1\">Synergized Reasoning</span>\n</h3>\n<p id=\"S6.SS2.p1.1\">To better utilize the knowledge from text corpus and knowledge graph reasoning, Synergized Reasoning aims to design a synergized model that can effectively conduct reasoning with both LLMs and KGs.</p>\n<p id=\"S6.SS2.p2.1\"><span id=\"S6.SS2.p2.1.1\">LLM-KG Fusion Reasoning.</span>\nLLM-KG Fusion Reasoning leverages two separated LLM and KG encoders to process the text and relevant KG inputs&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib244\" title=\"\">244</a>]</cite>. These two encoders are equally important and jointly fusing the knowledge from two sources for reasoning. To improve the interaction between text and knowledge, KagNet&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib38\" title=\"\">38</a>]</cite> proposes to first encode the input KG, and then augment the input textual representation. In contrast, MHGRN&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib234\" title=\"\">234</a>]</cite> uses the final LLM outputs of the input text to guide the reasoning process on the KGs. Yet, both of them only design a single-direction interaction between the text and KGs. To tackle this issue, QA-GNN&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib131\" title=\"\">131</a>]</cite> proposes to use a GNN-based model to jointly reason over input context and KG information via message passing. Specifically, QA-GNN represents the input textual information as a special node via a pooling operation and connects this node with other entities in KG. However, the textual inputs are only pooled into a single dense vector, limiting the information fusion performance. JointLK&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib245\" title=\"\">245</a>]</cite> then proposes a framework with fine-grained interaction between any tokens in the textual inputs and any KG entities through LM-to-KG and KG-to-LM bi-directional attention mechanism. As shown in Fig.&nbsp;<a href=\"https://arxiv.org/html/2306.08302v3#S6.F24\" title=\"Figure 24 ‣ 6.2 Synergized Reasoning ‣ 6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>24</span></a>, pairwise dot-product scores are calculated over all textual tokens and KG entities, the bi-directional attentive scores are computed separately. In addition, at each jointLK layer, the KGs are also dynamically pruned based on the attention score to allow later layers to focus on more important sub-KG structures. Despite being effective, in JointLK, the fusion process between the input text and KG still uses the final LLM outputs as the input text representations. GreaseLM&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib178\" title=\"\">178</a>]</cite> designs deep and rich interaction between the input text tokens and KG entities at each layer of the LLMs. The architecture and fusion approach is mostly similar to ERNIE&nbsp;<cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib35\" title=\"\">35</a>]</cite> discussed in Section&nbsp;<a href=\"https://arxiv.org/html/2306.08302v3#S6.SS1\" title=\"6.1 Synergized Knowledge Representation ‣ 6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>6.1</span></a>, except that GreaseLM does not use the text-only <em id=\"S6.SS2.p2.1.2\">T-encoder</em> to handle input text.</p>\n<figure id=\"S6.F24\"><img alt=\"Refer to caption\" height=\"387\" id=\"S6.F24.g1\" src=\"x21.png\" width=\"830\">\n<figcaption><span>Figure 24: </span>The framework of LLM-KG Fusion Reasoning.</figcaption>\n</figure>\n<p id=\"S6.SS2.p3.1\"><span id=\"S6.SS2.p3.1.1\">LLMs as Agents Reasoning.</span>\nInstead using two encoders to fuse the knowledge, LLMs can also be treated as agents to interact with the KGs to conduct reasoning <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib246\" title=\"\">246</a>]</cite>, as illustrated in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S6.F25\" title=\"Figure 25 ‣ 6.2 Synergized Reasoning ‣ 6 Synergized LLMs + KGs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>25</span></a>. KD-CoT <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib247\" title=\"\">247</a>]</cite> iteratively retrieves facts from KGs and produces faithful reasoning traces, which guide LLMs to generate answers.\nKSL <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib239\" title=\"\">239</a>]</cite> teaches LLMs to search on KGs to retrieve relevant facts and then generate answers. StructGPT <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib237\" title=\"\">237</a>]</cite> designs several API interfaces to allow LLMs to access the structural data and perform reasoning by traversing on KGs. Think-on-graph <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib240\" title=\"\">240</a>]</cite> provides a flexible plug-and-play framework where LLM agents iteratively execute beam searches on KGs to discover the reasoning paths and generate answers. To enhance the agent abilities, AgentTuning <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib248\" title=\"\">248</a>]</cite> presents several instruction-tuning datasets to guide LLM agents to perform reasoning on KGs.</p>\n<figure id=\"S6.F25\"><img alt=\"Refer to caption\" height=\"412\" id=\"S6.F25.g1\" src=\"x22.png\" width=\"581\">\n<figcaption><span>Figure 25: </span>Using LLMs as agents for reasoning on KGs.</figcaption>\n</figure>\n<p id=\"S6.SS2.p4.1\"><span id=\"S6.SS2.p4.1.1\">Comparison and Discussion.</span>\nLLM-KG Fusion Reasoning combines the LLM encoder and KG encoder to represent knowledge in a unified manner. It then employs a synergized reasoning module to jointly reason the results. This framework allows for different encoders and reasoning modules, which are trained end-to-end to effectively utilize the knowledge and reasoning capabilities of LLMs and KGs. However, these additional modules may introduce extra parameters and computational costs while lacking interpretability. LLMs as Agents for KG reasoning provides a flexible framework for reasoning on KGs without additional training cost, which can be generalized to different LLMs and KGs. Meanwhile, the reasoning process is interpretable, which can be used to explain the results. Nevertheless, defining the actions and policies for LLM agents is also challenging. The synergy of LLMs and KGs is still an ongoing research topic, with the potential to have more powerful frameworks in the future.</p>\n</section>\n</section>\n<section id=\"S7\">\n<h2>\n<span>7 </span><span id=\"S7.1.1\">Future Directions and Milestones</span>\n</h2>\n<p id=\"S7.p1.1\">In this section, we discuss the future directions and several milestones in the research area of unifying KGs and LLMs.</p>\n<section id=\"S7.SS1\">\n<h3>\n<span>7.1 </span><span id=\"S7.SS1.1.1\">KGs for Hallucination Detection in LLMs</span>\n</h3>\n<p id=\"S7.SS1.p1.1\">The hallucination problem in LLMs, which generates factually incorrect content, significantly hinders the reliability of LLMs. As discussed in Section <a href=\"https://arxiv.org/html/2306.08302v3#S4\" title=\"4 KG-enhanced LLMs ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>4</span></a>, existing studies try to utilize KGs to obtain more reliable LLMs through pre-training or KG-enhanced inference. Despite the efforts, the issue of hallucination may continue to persist in the realm of LLMs for the foreseeable future. Consequently, in order to gain the public’s trust and border applications, it is imperative to detect and assess instances of hallucination within LLMs and other forms of AI-generated content (AIGC). Existing methods strive to detect hallucination by training a neural classifier on a small set of documents <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib249\" title=\"\">249</a>]</cite>, which are neither robust nor powerful to handle ever-growing LLMs. Recently, researchers try to use KGs as an external source to validate LLMs <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib250\" title=\"\">250</a>]</cite>. Further studies combine LLMs and KGs to achieve a generalized fact-checking model that can detect hallucinations across domains <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib251\" title=\"\">251</a>]</cite>. Therefore, it opens a new door to utilizing KGs for hallucination detection.</p>\n</section>\n<section id=\"S7.SS2\">\n<h3>\n<span>7.2 </span><span id=\"S7.SS2.1.1\">KGs for Editing Knowledge in LLMs</span>\n</h3>\n<p id=\"S7.SS2.p1.1\">Although LLMs are capable of storing massive real-world knowledge, they cannot quickly update their internal knowledge updated as real-world situations change. There are some research efforts proposed for editing knowledge in LLMs <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib252\" title=\"\">252</a>]</cite> without re-training the whole LLMs. Yet, such solutions still suffer from poor performance or computational overhead <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib253\" title=\"\">253</a>]</cite>. Existing studies <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib254\" title=\"\">254</a>]</cite> also reveal that edit a single fact would cause a ripple effect for other related knowledge. Therefore, it is necessary to develop a more efficient and effective method to edit knowledge in LLMs. Recently, researchers try to leverage KGs to edit knowledge in LLMs efficiently.</p>\n</section>\n<section id=\"S7.SS3\">\n<h3>\n<span>7.3 </span><span id=\"S7.SS3.1.1\">KGs for Black-box LLMs Knowledge Injection</span>\n</h3>\n<p id=\"S7.SS3.p1.1\">Although pre-training and knowledge editing could update LLMs to catch up with the latest knowledge, they still need to access the internal structures and parameters of LLMs. However, many state-of-the-art large LLMs (e.g., ChatGPT) only provide APIs for users and developers to access, making themselves black-box to the public. Consequently, it is impossible to follow conventional KG injection approaches described <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib244\" title=\"\">244</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib38\" title=\"\">38</a>]</cite> that change LLM structure by adding additional knowledge fusion modules. Converting various types of knowledge into different text prompts seems to be a feasible solution. However, it is unclear whether these prompts can generalize well to new LLMs. Moreover, the prompt-based approach is limited to the length of input tokens of LLMs. Therefore, how to enable effective knowledge injection for black-box LLMs is still an open question for us to explore <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib255\" title=\"\">255</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib256\" title=\"\">256</a>]</cite>.</p>\n</section>\n<section id=\"S7.SS4\">\n<h3>\n<span>7.4 </span><span id=\"S7.SS4.1.1\">Multi-Modal LLMs for KGs</span>\n</h3>\n<p id=\"S7.SS4.p1.1\">Current knowledge graphs typically rely on textual and graph structure to handle KG-related applications. However, real-world knowledge graphs are often constructed by data from diverse modalities <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib257\" title=\"\">257</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib258\" title=\"\">258</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib99\" title=\"\">99</a>]</cite>. Therefore, effectively leveraging representations from multiple modalities would be a significant challenge for future research in KGs <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib259\" title=\"\">259</a>]</cite>. One potential solution is to develop methods that can accurately encode and align entities across different modalities. Recently, with the development of multi-modal LLMs <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib260\" title=\"\">260</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib98\" title=\"\">98</a>]</cite>, leveraging LLMs for modality alignment holds promise in this regard. But, bridging the gap between multi-modal LLMs and KG structure remains a crucial challenge in this field, demanding further investigation and advancements.</p>\n</section>\n<section id=\"S7.SS5\">\n<h3>\n<span>7.5 </span><span id=\"S7.SS5.1.1\">LLMs for Understanding KG Structure</span>\n</h3>\n<p id=\"S7.SS5.p1.1\">Conventional LLMs trained on plain text data are not designed to understand structured data like knowledge graphs. Thus, LLMs might not fully grasp or understand the information conveyed by the KG structure. A straightforward way is to linearize the structured data into a sentence that LLMs can understand. However, the scale of the KGs makes it impossible to linearize the whole KGs as input. Moreover, the linearization process may lose some underlying information in KGs. Therefore, it is necessary to develop LLMs that can directly understand the KG structure and reason over it <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib237\" title=\"\">237</a>]</cite>.</p>\n<figure id=\"S7.F26\"><img alt=\"Refer to caption\" height=\"270\" id=\"S7.F26.g1\" src=\"x23.png\" width=\"498\">\n<figcaption><span>Figure 26: </span>The milestones of unifying KGs and LLMs.</figcaption>\n</figure>\n</section>\n<section id=\"S7.SS6\">\n<h3>\n<span>7.6 </span><span id=\"S7.SS6.1.1\">Synergized LLMs and KGs for Birectional Reasoning</span>\n</h3>\n<p id=\"S7.SS6.p1.1\">KGs and LLMs are two complementary technologies that can synergize each other. However, the synergy of LLMs and KGs is less explored by existing researchers. A desired synergy of LLMs and KGs would involve leveraging the strengths of both technologies to overcome their individual limitations. LLMs, such as ChatGPT, excel in generating human-like text and understanding natural language, while KGs are structured databases that capture and represent knowledge in a structured manner. By combining their capabilities, we can create a powerful system that benefits from the contextual understanding of LLMs and the structured knowledge representation of KGs. To better unify LLMs and KGs, many advanced techniques need to be incorporated, such as multi-modal learning <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib261\" title=\"\">261</a>]</cite>, graph neural network <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib262\" title=\"\">262</a>]</cite>, and continuous learning <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib263\" title=\"\">263</a>]</cite>. Last, the synergy of LLMs and KGs can be applied to many real-world applications, such as search engines <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib100\" title=\"\">100</a>]</cite>, recommender systems <cite>[<a href=\"https://arxiv.org/html/2306.08302v3#bib.bib10\" title=\"\">10</a>, <a href=\"https://arxiv.org/html/2306.08302v3#bib.bib89\" title=\"\">89</a>]</cite>, and drug discovery.</p>\n<p id=\"S7.SS6.p2.1\">With a given application problem, we can apply a KG to perform a knowledge-driven search for potential goals and unseen data, and simultaneously start with LLMs to perform a data/text-driven inference to see what new data/goal items can be derived. When the knowledge-based search is combined with data/text-driven inference, they can mutually validate each other, resulting in efficient and effective solutions powered by dual-driving wheels.\nTherefore, we can anticipate increasing attention to unlock the potential of integrating KGs and LLMs for diverse downstream applications with both generative and reasoning capabilities in the near future.</p>\n</section>\n</section>\n<section id=\"S8\">\n<h2>\n<span>8 </span><span id=\"S8.1.1\">Conclusion</span>\n</h2>\n<p id=\"S8.p1.1\">Unifying large language models (LLMs) and knowledge graphs (KGs) is an active research direction that has attracted increasing attention from both academia and industry. In this article, we provide a thorough overview of the recent research in this field. We first introduce different manners that integrate KGs to enhance LLMs. Then, we introduce existing methods that apply LLMs for KGs and establish taxonomy based on varieties of KG tasks. Finally, we discuss the challenges and future directions in this field.\nWe envision that there will be multiple stages (milestones) in the roadmap of unifying KGs and LLMs, as shown in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S7.F26\" title=\"Figure 26 ‣ 7.5 LLMs for Understanding KG Structure ‣ 7 Future Directions and Milestones ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>26</span></a>. In particular, we will anticipate increasing research on three stages: <em id=\"S8.p1.1.1\">Stage 1</em>: KG-enhanced LLMs, LLM-augmented KGs, <em id=\"S8.p1.1.2\">Stage 2:</em> Synergized LLMs + KGs, and <em id=\"S8.p1.1.3\">Stage 3:</em> Graph Structure Understanding, Multi-modality, Knowledge Updating. We hope that this article will provide a guideline to advance future research.\n</p>\n</section>\n<section id=\"Sx1\">\n<h2>Acknowledgments</h2>\n<p id=\"Sx1.p1.1\">This research was supported by the Australian Research Council (ARC) under grants FT210100097 and DP240101547 and the National Natural Science Foundation of China (NSFC) under grant 62120106008.</p>\n</section>\n<section id=\"bib\">\n<h2>References</h2>\n<ul>\n<li id=\"bib.bib1\">\n<span>[1]</span>\n<span>\nJ.&nbsp;Devlin, M.-W. Chang, K.&nbsp;Lee, and K.&nbsp;Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” <em id=\"bib.bib1.1.1\">arXiv preprint arXiv:1810.04805</em>, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib2\">\n<span>[2]</span>\n<span>\nY.&nbsp;Liu, M.&nbsp;Ott, N.&nbsp;Goyal, J.&nbsp;Du, M.&nbsp;Joshi, D.&nbsp;Chen, O.&nbsp;Levy, M.&nbsp;Lewis, L.&nbsp;Zettlemoyer, and V.&nbsp;Stoyanov, “Roberta: A robustly optimized bert pretraining approach,” <em id=\"bib.bib2.1.1\">arXiv preprint arXiv:1907.11692</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib3\">\n<span>[3]</span>\n<span>\nC.&nbsp;Raffel, N.&nbsp;Shazeer, A.&nbsp;Roberts, K.&nbsp;Lee, S.&nbsp;Narang, M.&nbsp;Matena, Y.&nbsp;Zhou, W.&nbsp;Li, and P.&nbsp;J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer,” <em id=\"bib.bib3.1.1\">The Journal of Machine Learning Research</em>, vol.&nbsp;21, no.&nbsp;1, pp. 5485–5551, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib4\">\n<span>[4]</span>\n<span>\nD.&nbsp;Su, Y.&nbsp;Xu, G.&nbsp;I. Winata, P.&nbsp;Xu, H.&nbsp;Kim, Z.&nbsp;Liu, and P.&nbsp;Fung, “Generalizing question answering system with pre-trained language model fine-tuning,” in <em id=\"bib.bib4.1.1\">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</em>, 2019, pp. 203–211.\n\n</span>\n</li>\n<li id=\"bib.bib5\">\n<span>[5]</span>\n<span>\nM.&nbsp;Lewis, Y.&nbsp;Liu, N.&nbsp;Goyal, M.&nbsp;Ghazvininejad, A.&nbsp;Mohamed, O.&nbsp;Levy, V.&nbsp;Stoyanov, and L.&nbsp;Zettlemoyer, “Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,” in <em id=\"bib.bib5.1.1\">ACL</em>, 2020, pp. 7871–7880.\n\n</span>\n</li>\n<li id=\"bib.bib6\">\n<span>[6]</span>\n<span>\nJ.&nbsp;Li, T.&nbsp;Tang, W.&nbsp;X. Zhao, and J.-R. Wen, “Pretrained language models for text generation: A survey,” <em id=\"bib.bib6.1.1\">arXiv preprint arXiv:2105.10311</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib7\">\n<span>[7]</span>\n<span>\nJ.&nbsp;Wei, Y.&nbsp;Tay, R.&nbsp;Bommasani, C.&nbsp;Raffel, B.&nbsp;Zoph, S.&nbsp;Borgeaud, D.&nbsp;Yogatama, M.&nbsp;Bosma, D.&nbsp;Zhou, D.&nbsp;Metzler <em id=\"bib.bib7.1.1\">et&nbsp;al.</em>, “Emergent abilities of large language models,” <em id=\"bib.bib7.2.2\">Transactions on Machine Learning Research</em>.\n\n</span>\n</li>\n<li id=\"bib.bib8\">\n<span>[8]</span>\n<span>\nK.&nbsp;Malinka, M.&nbsp;Perešíni, A.&nbsp;Firc, O.&nbsp;Hujňák, and F.&nbsp;Januš, “On the educational impact of chatgpt: Is artificial intelligence ready to obtain a university degree?” <em id=\"bib.bib8.1.1\">arXiv preprint arXiv:2303.11146</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib9\">\n<span>[9]</span>\n<span>\nZ.&nbsp;Li, C.&nbsp;Wang, Z.&nbsp;Liu, H.&nbsp;Wang, S.&nbsp;Wang, and C.&nbsp;Gao, “Cctest: Testing and repairing code completion systems,” <em id=\"bib.bib9.1.1\">ICSE</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib10\">\n<span>[10]</span>\n<span>\nJ.&nbsp;Liu, C.&nbsp;Liu, R.&nbsp;Lv, K.&nbsp;Zhou, and Y.&nbsp;Zhang, “Is chatgpt a good recommender? a preliminary study,” <em id=\"bib.bib10.1.1\">arXiv preprint arXiv:2304.10149</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib11\">\n<span>[11]</span>\n<span>\nW.&nbsp;X. Zhao, K.&nbsp;Zhou, J.&nbsp;Li, T.&nbsp;Tang, X.&nbsp;Wang, Y.&nbsp;Hou, Y.&nbsp;Min, B.&nbsp;Zhang, J.&nbsp;Zhang, Z.&nbsp;Dong <em id=\"bib.bib11.1.1\">et&nbsp;al.</em>, “A survey of large language models,” <em id=\"bib.bib11.2.2\">arXiv preprint arXiv:2303.18223</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib12\">\n<span>[12]</span>\n<span>\nX.&nbsp;Qiu, T.&nbsp;Sun, Y.&nbsp;Xu, Y.&nbsp;Shao, N.&nbsp;Dai, and X.&nbsp;Huang, “Pre-trained models for natural language processing: A survey,” <em id=\"bib.bib12.1.1\">Science China Technological Sciences</em>, vol.&nbsp;63, no.&nbsp;10, pp. 1872–1897, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib13\">\n<span>[13]</span>\n<span>\nJ.&nbsp;Yang, H.&nbsp;Jin, R.&nbsp;Tang, X.&nbsp;Han, Q.&nbsp;Feng, H.&nbsp;Jiang, B.&nbsp;Yin, and X.&nbsp;Hu, “Harnessing the power of llms in practice: A survey on chatgpt and beyond,” <em id=\"bib.bib13.1.1\">arXiv preprint arXiv:2304.13712</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib14\">\n<span>[14]</span>\n<span>\nF.&nbsp;Petroni, T.&nbsp;Rocktäschel, S.&nbsp;Riedel, P.&nbsp;Lewis, A.&nbsp;Bakhtin, Y.&nbsp;Wu, and A.&nbsp;Miller, “Language models as knowledge bases?” in <em id=\"bib.bib14.1.1\">EMNLP-IJCNLP</em>, 2019, pp. 2463–2473.\n\n</span>\n</li>\n<li id=\"bib.bib15\">\n<span>[15]</span>\n<span>\nZ.&nbsp;Ji, N.&nbsp;Lee, R.&nbsp;Frieske, T.&nbsp;Yu, D.&nbsp;Su, Y.&nbsp;Xu, E.&nbsp;Ishii, Y.&nbsp;J. Bang, A.&nbsp;Madotto, and P.&nbsp;Fung, “Survey of hallucination in natural language generation,” <em id=\"bib.bib15.1.1\">ACM Computing Surveys</em>, vol.&nbsp;55, no.&nbsp;12, pp. 1–38, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib16\">\n<span>[16]</span>\n<span>\nH.&nbsp;Zhang, H.&nbsp;Song, S.&nbsp;Li, M.&nbsp;Zhou, and D.&nbsp;Song, “A survey of controllable text generation using transformer-based pre-trained language models,” <em id=\"bib.bib16.1.1\">arXiv preprint arXiv:2201.05337</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib17\">\n<span>[17]</span>\n<span>\nM.&nbsp;Danilevsky, K.&nbsp;Qian, R.&nbsp;Aharonov, Y.&nbsp;Katsis, B.&nbsp;Kawas, and P.&nbsp;Sen, “A survey of the state of explainable ai for natural language processing,” <em id=\"bib.bib17.1.1\">arXiv preprint arXiv:2010.00711</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib18\">\n<span>[18]</span>\n<span>\nJ.&nbsp;Wang, X.&nbsp;Hu, W.&nbsp;Hou, H.&nbsp;Chen, R.&nbsp;Zheng, Y.&nbsp;Wang, L.&nbsp;Yang, H.&nbsp;Huang, W.&nbsp;Ye, X.&nbsp;Geng <em id=\"bib.bib18.1.1\">et&nbsp;al.</em>, “On the robustness of chatgpt: An adversarial and out-of-distribution perspective,” <em id=\"bib.bib18.2.2\">arXiv preprint arXiv:2302.12095</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib19\">\n<span>[19]</span>\n<span>\nS.&nbsp;Ji, S.&nbsp;Pan, E.&nbsp;Cambria, P.&nbsp;Marttinen, and S.&nbsp;Y. Philip, “A survey on knowledge graphs: Representation, acquisition, and applications,” <em id=\"bib.bib19.1.1\">IEEE TNNLS</em>, vol.&nbsp;33, no.&nbsp;2, pp. 494–514, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib20\">\n<span>[20]</span>\n<span>\nD.&nbsp;Vrandečić and M.&nbsp;Krötzsch, “Wikidata: a free collaborative knowledgebase,” <em id=\"bib.bib20.1.1\">Communications of the ACM</em>, vol.&nbsp;57, no.&nbsp;10, pp. 78–85, 2014.\n\n</span>\n</li>\n<li id=\"bib.bib21\">\n<span>[21]</span>\n<span>\nS.&nbsp;Hu, L.&nbsp;Zou, and X.&nbsp;Zhang, “A state-transition framework to answer complex questions over knowledge base,” in <em id=\"bib.bib21.1.1\">EMNLP</em>, 2018, pp. 2098–2108.\n\n</span>\n</li>\n<li id=\"bib.bib22\">\n<span>[22]</span>\n<span>\nJ.&nbsp;Zhang, B.&nbsp;Chen, L.&nbsp;Zhang, X.&nbsp;Ke, and H.&nbsp;Ding, “Neural, symbolic and neural-symbolic reasoning on knowledge graphs,” <em id=\"bib.bib22.1.1\">AI Open</em>, vol.&nbsp;2, pp. 14–35, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib23\">\n<span>[23]</span>\n<span>\nB.&nbsp;Abu-Salih, “Domain-specific knowledge graphs: A survey,” <em id=\"bib.bib23.1.1\">Journal of Network and Computer Applications</em>, vol. 185, p. 103076, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib24\">\n<span>[24]</span>\n<span>\nT.&nbsp;Mitchell, W.&nbsp;Cohen, E.&nbsp;Hruschka, P.&nbsp;Talukdar, B.&nbsp;Yang, J.&nbsp;Betteridge, A.&nbsp;Carlson, B.&nbsp;Dalvi, M.&nbsp;Gardner, B.&nbsp;Kisiel, K.&nbsp;Jayant, L.&nbsp;Ni, M.&nbsp;Kathryn, M.&nbsp;Thahir, N.&nbsp;Ndapandula, P.&nbsp;Emmanouil, R.&nbsp;Alan, S.&nbsp;Mehdi, S.&nbsp;Burr, W.&nbsp;Derry, G.&nbsp;Abhinav, C.&nbsp;Xi, S.&nbsp;Abulhair, and W.&nbsp;Joel, “Never-ending learning,” <em id=\"bib.bib24.1.1\">Communications of the ACM</em>, vol.&nbsp;61, no.&nbsp;5, pp. 103–115, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib25\">\n<span>[25]</span>\n<span>\nL.&nbsp;Zhong, J.&nbsp;Wu, Q.&nbsp;Li, H.&nbsp;Peng, and X.&nbsp;Wu, “A comprehensive survey on automatic knowledge graph construction,” <em id=\"bib.bib25.1.1\">arXiv preprint arXiv:2302.05019</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib26\">\n<span>[26]</span>\n<span>\nL.&nbsp;Yao, C.&nbsp;Mao, and Y.&nbsp;Luo, “Kg-bert: Bert for knowledge graph completion,” <em id=\"bib.bib26.1.1\">arXiv preprint arXiv:1909.03193</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib27\">\n<span>[27]</span>\n<span>\nL.&nbsp;Luo, Y.-F. Li, G.&nbsp;Haffari, and S.&nbsp;Pan, “Normalizing flow-based neural process for few-shot knowledge graph completion,” <em id=\"bib.bib27.1.1\">SIGIR</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib28\">\n<span>[28]</span>\n<span>\nY.&nbsp;Bang, S.&nbsp;Cahyawijaya, N.&nbsp;Lee, W.&nbsp;Dai, D.&nbsp;Su, B.&nbsp;Wilie, H.&nbsp;Lovenia, Z.&nbsp;Ji, T.&nbsp;Yu, W.&nbsp;Chung <em id=\"bib.bib28.1.1\">et&nbsp;al.</em>, “A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity,” <em id=\"bib.bib28.2.2\">arXiv preprint arXiv:2302.04023</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib29\">\n<span>[29]</span>\n<span>\nX.&nbsp;Wang, J.&nbsp;Wei, D.&nbsp;Schuurmans, Q.&nbsp;Le, E.&nbsp;Chi, and D.&nbsp;Zhou, “Self-consistency improves chain of thought reasoning in language models,” <em id=\"bib.bib29.1.1\">arXiv preprint arXiv:2203.11171</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib30\">\n<span>[30]</span>\n<span>\nO.&nbsp;Golovneva, M.&nbsp;Chen, S.&nbsp;Poff, M.&nbsp;Corredor, L.&nbsp;Zettlemoyer, M.&nbsp;Fazel-Zarandi, and A.&nbsp;Celikyilmaz, “Roscoe: A suite of metrics for scoring step-by-step reasoning,” <em id=\"bib.bib30.1.1\">ICLR</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib31\">\n<span>[31]</span>\n<span>\nF.&nbsp;M. Suchanek, G.&nbsp;Kasneci, and G.&nbsp;Weikum, “Yago: a core of semantic knowledge,” in <em id=\"bib.bib31.1.1\">WWW</em>, 2007, pp. 697–706.\n\n</span>\n</li>\n<li id=\"bib.bib32\">\n<span>[32]</span>\n<span>\nA.&nbsp;Carlson, J.&nbsp;Betteridge, B.&nbsp;Kisiel, B.&nbsp;Settles, E.&nbsp;Hruschka, and T.&nbsp;Mitchell, “Toward an architecture for never-ending language learning,” in <em id=\"bib.bib32.1.1\">Proceedings of the AAAI conference on artificial intelligence</em>, vol.&nbsp;24, no.&nbsp;1, 2010, pp. 1306–1313.\n\n</span>\n</li>\n<li id=\"bib.bib33\">\n<span>[33]</span>\n<span>\nA.&nbsp;Bordes, N.&nbsp;Usunier, A.&nbsp;Garcia-Duran, J.&nbsp;Weston, and O.&nbsp;Yakhnenko, “Translating embeddings for modeling multi-relational data,” <em id=\"bib.bib33.1.1\">NeurIPS</em>, vol.&nbsp;26, 2013.\n\n</span>\n</li>\n<li id=\"bib.bib34\">\n<span>[34]</span>\n<span>\nG.&nbsp;Wan, S.&nbsp;Pan, C.&nbsp;Gong, C.&nbsp;Zhou, and G.&nbsp;Haffari, “Reasoning like human: Hierarchical reinforcement learning for knowledge graph reasoning,” in <em id=\"bib.bib34.1.1\">AAAI</em>, 2021, pp. 1926–1932.\n\n</span>\n</li>\n<li id=\"bib.bib35\">\n<span>[35]</span>\n<span>\nZ.&nbsp;Zhang, X.&nbsp;Han, Z.&nbsp;Liu, X.&nbsp;Jiang, M.&nbsp;Sun, and Q.&nbsp;Liu, “ERNIE: Enhanced language representation with informative entities,” in <em id=\"bib.bib35.1.1\">ACL</em>, 2019, pp. 1441–1451.\n\n</span>\n</li>\n<li id=\"bib.bib36\">\n<span>[36]</span>\n<span>\nW.&nbsp;Liu, P.&nbsp;Zhou, Z.&nbsp;Zhao, Z.&nbsp;Wang, Q.&nbsp;Ju, H.&nbsp;Deng, and P.&nbsp;Wang, “K-BERT: enabling language representation with knowledge graph,” in <em id=\"bib.bib36.1.1\">AAAI</em>, 2020, pp. 2901–2908.\n\n</span>\n</li>\n<li id=\"bib.bib37\">\n<span>[37]</span>\n<span>\nY.&nbsp;Liu, Y.&nbsp;Wan, L.&nbsp;He, H.&nbsp;Peng, and P.&nbsp;S. Yu, “KG-BART: knowledge graph-augmented BART for generative commonsense reasoning,” in <em id=\"bib.bib37.1.1\">AAAI</em>, 2021, pp. 6418–6425.\n\n</span>\n</li>\n<li id=\"bib.bib38\">\n<span>[38]</span>\n<span>\nB.&nbsp;Y. Lin, X.&nbsp;Chen, J.&nbsp;Chen, and X.&nbsp;Ren, “KagNet: Knowledge-aware graph networks for commonsense reasoning,” in <em id=\"bib.bib38.1.1\">EMNLP-IJCNLP</em>, 2019, pp. 2829–2839.\n\n</span>\n</li>\n<li id=\"bib.bib39\">\n<span>[39]</span>\n<span>\nD.&nbsp;Dai, L.&nbsp;Dong, Y.&nbsp;Hao, Z.&nbsp;Sui, B.&nbsp;Chang, and F.&nbsp;Wei, “Knowledge neurons in pretrained transformers,” <em id=\"bib.bib39.1.1\">arXiv preprint arXiv:2104.08696</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib40\">\n<span>[40]</span>\n<span>\nX.&nbsp;Wang, T.&nbsp;Gao, Z.&nbsp;Zhu, Z.&nbsp;Zhang, Z.&nbsp;Liu, J.&nbsp;Li, and J.&nbsp;Tang, “KEPLER: A unified model for knowledge embedding and pre-trained language representation,” <em id=\"bib.bib40.1.1\">Transactions of the Association for Computational Linguistics</em>, vol.&nbsp;9, pp. 176–194, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib41\">\n<span>[41]</span>\n<span>\nI.&nbsp;Melnyk, P.&nbsp;Dognin, and P.&nbsp;Das, “Grapher: Multi-stage knowledge graph construction using pretrained language models,” in <em id=\"bib.bib41.1.1\">NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib42\">\n<span>[42]</span>\n<span>\nP.&nbsp;Ke, H.&nbsp;Ji, Y.&nbsp;Ran, X.&nbsp;Cui, L.&nbsp;Wang, L.&nbsp;Song, X.&nbsp;Zhu, and M.&nbsp;Huang, “JointGT: Graph-text joint representation learning for text generation from knowledge graphs,” in <em id=\"bib.bib42.1.1\">ACL Finding</em>, 2021, pp. 2526–2538.\n\n</span>\n</li>\n<li id=\"bib.bib43\">\n<span>[43]</span>\n<span>\nJ.&nbsp;Jiang, K.&nbsp;Zhou, W.&nbsp;X. Zhao, and J.-R. Wen, “Unikgqa: Unified retrieval and reasoning for solving multi-hop question answering over knowledge graph,” <em id=\"bib.bib43.1.1\">ICLR 2023</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib44\">\n<span>[44]</span>\n<span>\nM.&nbsp;Yasunaga, A.&nbsp;Bosselut, H.&nbsp;Ren, X.&nbsp;Zhang, C.&nbsp;D. Manning, P.&nbsp;S. Liang, and J.&nbsp;Leskovec, “Deep bidirectional language-knowledge graph pretraining,” <em id=\"bib.bib44.1.1\">NeurIPS</em>, vol.&nbsp;35, pp. 37 309–37 323, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib45\">\n<span>[45]</span>\n<span>\nN.&nbsp;Choudhary and C.&nbsp;K. Reddy, “Complex logical reasoning over knowledge graphs using large language models,” <em id=\"bib.bib45.1.1\">arXiv preprint arXiv:2305.01157</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib46\">\n<span>[46]</span>\n<span>\nS.&nbsp;Wang, Z.&nbsp;Wei, J.&nbsp;Xu, and Z.&nbsp;Fan, “Unifying structure reasoning and language model pre-training for complex reasoning,” <em id=\"bib.bib46.1.1\">arXiv preprint arXiv:2301.08913</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib47\">\n<span>[47]</span>\n<span>\nC.&nbsp;Zhen, Y.&nbsp;Shang, X.&nbsp;Liu, Y.&nbsp;Li, Y.&nbsp;Chen, and D.&nbsp;Zhang, “A survey on knowledge-enhanced pre-trained language models,” <em id=\"bib.bib47.1.1\">arXiv preprint arXiv:2212.13428</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib48\">\n<span>[48]</span>\n<span>\nX.&nbsp;Wei, S.&nbsp;Wang, D.&nbsp;Zhang, P.&nbsp;Bhatia, and A.&nbsp;Arnold, “Knowledge enhanced pretrained language models: A compreshensive survey,” <em id=\"bib.bib48.1.1\">arXiv preprint arXiv:2110.08455</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib49\">\n<span>[49]</span>\n<span>\nD.&nbsp;Yin, L.&nbsp;Dong, H.&nbsp;Cheng, X.&nbsp;Liu, K.-W. Chang, F.&nbsp;Wei, and J.&nbsp;Gao, “A survey of knowledge-intensive nlp with pre-trained language models,” <em id=\"bib.bib49.1.1\">arXiv preprint arXiv:2202.08772</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib50\">\n<span>[50]</span>\n<span>\nA.&nbsp;Vaswani, N.&nbsp;Shazeer, N.&nbsp;Parmar, J.&nbsp;Uszkoreit, L.&nbsp;Jones, A.&nbsp;N. Gomez, Ł.&nbsp;Kaiser, and I.&nbsp;Polosukhin, “Attention is all you need,” <em id=\"bib.bib50.1.1\">NeurIPS</em>, vol.&nbsp;30, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib51\">\n<span>[51]</span>\n<span>\nZ.&nbsp;Lan, M.&nbsp;Chen, S.&nbsp;Goodman, K.&nbsp;Gimpel, P.&nbsp;Sharma, and R.&nbsp;Soricut, “Albert: A lite bert for self-supervised learning of language representations,” in <em id=\"bib.bib51.1.1\">ICLR</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib52\">\n<span>[52]</span>\n<span>\nK.&nbsp;Clark, M.-T. Luong, Q.&nbsp;V. Le, and C.&nbsp;D. Manning, “Electra: Pre-training text encoders as discriminators rather than generators,” <em id=\"bib.bib52.1.1\">arXiv preprint arXiv:2003.10555</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib53\">\n<span>[53]</span>\n<span>\nK.&nbsp;Hakala and S.&nbsp;Pyysalo, “Biomedical named entity recognition with multilingual bert,” in <em id=\"bib.bib53.1.1\">Proceedings of the 5th workshop on BioNLP open shared tasks</em>, 2019, pp. 56–61.\n\n</span>\n</li>\n<li id=\"bib.bib54\">\n<span>[54]</span>\n<span>\nY.&nbsp;Tay, M.&nbsp;Dehghani, V.&nbsp;Q. Tran, X.&nbsp;Garcia, J.&nbsp;Wei, X.&nbsp;Wang, H.&nbsp;W. Chung, D.&nbsp;Bahri, T.&nbsp;Schuster, S.&nbsp;Zheng <em id=\"bib.bib54.1.1\">et&nbsp;al.</em>, “Ul2: Unifying language learning paradigms,” in <em id=\"bib.bib54.2.2\">ICLR</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib55\">\n<span>[55]</span>\n<span>\nV.&nbsp;Sanh, A.&nbsp;Webson, C.&nbsp;Raffel, S.&nbsp;Bach, L.&nbsp;Sutawika, Z.&nbsp;Alyafeai, A.&nbsp;Chaffin, A.&nbsp;Stiegler, A.&nbsp;Raja, M.&nbsp;Dey <em id=\"bib.bib55.1.1\">et&nbsp;al.</em>, “Multitask prompted training enables zero-shot task generalization,” in <em id=\"bib.bib55.2.2\">ICLR</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib56\">\n<span>[56]</span>\n<span>\nB.&nbsp;Zoph, I.&nbsp;Bello, S.&nbsp;Kumar, N.&nbsp;Du, Y.&nbsp;Huang, J.&nbsp;Dean, N.&nbsp;Shazeer, and W.&nbsp;Fedus, “St-moe: Designing stable and transferable sparse expert models,” <em id=\"bib.bib56.1.1\">URL https://arxiv. org/abs/2202.08906</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib57\">\n<span>[57]</span>\n<span>\nA.&nbsp;Zeng, X.&nbsp;Liu, Z.&nbsp;Du, Z.&nbsp;Wang, H.&nbsp;Lai, M.&nbsp;Ding, Z.&nbsp;Yang, Y.&nbsp;Xu, W.&nbsp;Zheng, X.&nbsp;Xia, W.&nbsp;L. Tam, Z.&nbsp;Ma, Y.&nbsp;Xue, J.&nbsp;Zhai, W.&nbsp;Chen, Z.&nbsp;Liu, P.&nbsp;Zhang, Y.&nbsp;Dong, and J.&nbsp;Tang, “GLM-130b: An open bilingual pre-trained model,” in <em id=\"bib.bib57.1.1\">ICLR</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib58\">\n<span>[58]</span>\n<span>\nL.&nbsp;Xue, N.&nbsp;Constant, A.&nbsp;Roberts, M.&nbsp;Kale, R.&nbsp;Al-Rfou, A.&nbsp;Siddhant, A.&nbsp;Barua, and C.&nbsp;Raffel, “mt5: A massively multilingual pre-trained text-to-text transformer,” in <em id=\"bib.bib58.1.1\">NAACL</em>, 2021, pp. 483–498.\n\n</span>\n</li>\n<li id=\"bib.bib59\">\n<span>[59]</span>\n<span>\nT.&nbsp;Brown, B.&nbsp;Mann, N.&nbsp;Ryder, M.&nbsp;Subbiah, J.&nbsp;D. Kaplan, P.&nbsp;Dhariwal, A.&nbsp;Neelakantan, P.&nbsp;Shyam, G.&nbsp;Sastry, A.&nbsp;Askell <em id=\"bib.bib59.1.1\">et&nbsp;al.</em>, “Language models are few-shot learners,” <em id=\"bib.bib59.2.2\">Advances in neural information processing systems</em>, vol.&nbsp;33, pp. 1877–1901, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib60\">\n<span>[60]</span>\n<span>\nL.&nbsp;Ouyang, J.&nbsp;Wu, X.&nbsp;Jiang, D.&nbsp;Almeida, C.&nbsp;Wainwright, P.&nbsp;Mishkin, C.&nbsp;Zhang, S.&nbsp;Agarwal, K.&nbsp;Slama, A.&nbsp;Ray <em id=\"bib.bib60.1.1\">et&nbsp;al.</em>, “Training language models to follow instructions with human feedback,” <em id=\"bib.bib60.2.2\">NeurIPS</em>, vol.&nbsp;35, pp. 27 730–27 744, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib61\">\n<span>[61]</span>\n<span>\nH.&nbsp;Touvron, T.&nbsp;Lavril, G.&nbsp;Izacard, X.&nbsp;Martinet, M.-A. Lachaux, T.&nbsp;Lacroix, B.&nbsp;Rozière, N.&nbsp;Goyal, E.&nbsp;Hambro, F.&nbsp;Azhar <em id=\"bib.bib61.1.1\">et&nbsp;al.</em>, “Llama: Open and efficient foundation language models,” <em id=\"bib.bib61.2.2\">arXiv preprint arXiv:2302.13971</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib62\">\n<span>[62]</span>\n<span>\nE.&nbsp;Saravia, “Prompt Engineering Guide,” <a href=\"https://github.com/dair-ai/Prompt-Engineering-Guide\" title=\"\">https://github.com/dair-ai/Prompt-Engineering-Guide</a>, 2022, accessed: 2022-12.\n\n</span>\n</li>\n<li id=\"bib.bib63\">\n<span>[63]</span>\n<span>\nJ.&nbsp;Wei, X.&nbsp;Wang, D.&nbsp;Schuurmans, M.&nbsp;Bosma, F.&nbsp;Xia, E.&nbsp;H. Chi, Q.&nbsp;V. Le, D.&nbsp;Zhou <em id=\"bib.bib63.1.1\">et&nbsp;al.</em>, “Chain-of-thought prompting elicits reasoning in large language models,” in <em id=\"bib.bib63.2.2\">NeurIPS</em>.\n\n</span>\n</li>\n<li id=\"bib.bib64\">\n<span>[64]</span>\n<span>\nS.&nbsp;Li, Y.&nbsp;Gao, H.&nbsp;Jiang, Q.&nbsp;Yin, Z.&nbsp;Li, X.&nbsp;Yan, C.&nbsp;Zhang, and B.&nbsp;Yin, “Graph reasoning for question answering with triplet retrieval,” in <em id=\"bib.bib64.1.1\">ACL</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib65\">\n<span>[65]</span>\n<span>\nY.&nbsp;Wen, Z.&nbsp;Wang, and J.&nbsp;Sun, “Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models,” <em id=\"bib.bib65.1.1\">arXiv preprint arXiv:2308.09729</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib66\">\n<span>[66]</span>\n<span>\nK.&nbsp;Bollacker, C.&nbsp;Evans, P.&nbsp;Paritosh, T.&nbsp;Sturge, and J.&nbsp;Taylor, “Freebase: A collaboratively created graph database for structuring human knowledge,” in <em id=\"bib.bib66.1.1\">SIGMOD</em>, 2008, pp. 1247–1250.\n\n</span>\n</li>\n<li id=\"bib.bib67\">\n<span>[67]</span>\n<span>\nS.&nbsp;Auer, C.&nbsp;Bizer, G.&nbsp;Kobilarov, J.&nbsp;Lehmann, R.&nbsp;Cyganiak, and Z.&nbsp;Ives, “Dbpedia: A nucleus for a web of open data,” in <em id=\"bib.bib67.1.1\">The Semantic Web: 6th International Semantic Web Conference</em>.&nbsp;&nbsp;&nbsp;Springer, 2007, pp. 722–735.\n\n</span>\n</li>\n<li id=\"bib.bib68\">\n<span>[68]</span>\n<span>\nB.&nbsp;Xu, Y.&nbsp;Xu, J.&nbsp;Liang, C.&nbsp;Xie, B.&nbsp;Liang, W.&nbsp;Cui, and Y.&nbsp;Xiao, “Cn-dbpedia: A never-ending chinese knowledge extraction system,” in <em id=\"bib.bib68.1.1\">30th International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems</em>.&nbsp;&nbsp;&nbsp;Springer, 2017, pp. 428–438.\n\n</span>\n</li>\n<li id=\"bib.bib69\">\n<span>[69]</span>\n<span>\nP.&nbsp;Hai-Nyzhnyk, “Vikidia as a universal multilingual online encyclopedia for children,” <em id=\"bib.bib69.1.1\">The Encyclopedia Herald of Ukraine</em>, vol.&nbsp;14, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib70\">\n<span>[70]</span>\n<span>\nF.&nbsp;Ilievski, P.&nbsp;Szekely, and B.&nbsp;Zhang, “Cskg: The commonsense knowledge graph,” <em id=\"bib.bib70.1.1\">Extended Semantic Web Conference (ESWC)</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib71\">\n<span>[71]</span>\n<span>\nR.&nbsp;Speer, J.&nbsp;Chin, and C.&nbsp;Havasi, “Conceptnet 5.5: An open multilingual graph of general knowledge,” in <em id=\"bib.bib71.1.1\">Proceedings of the AAAI conference on artificial intelligence</em>, vol.&nbsp;31, no.&nbsp;1, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib72\">\n<span>[72]</span>\n<span>\nH.&nbsp;Ji, P.&nbsp;Ke, S.&nbsp;Huang, F.&nbsp;Wei, X.&nbsp;Zhu, and M.&nbsp;Huang, “Language generation with multi-hop reasoning on commonsense knowledge graph,” in <em id=\"bib.bib72.1.1\">EMNLP</em>, 2020, pp. 725–736.\n\n</span>\n</li>\n<li id=\"bib.bib73\">\n<span>[73]</span>\n<span>\nJ.&nbsp;D. Hwang, C.&nbsp;Bhagavatula, R.&nbsp;Le&nbsp;Bras, J.&nbsp;Da, K.&nbsp;Sakaguchi, A.&nbsp;Bosselut, and Y.&nbsp;Choi, “(comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs,” in <em id=\"bib.bib73.1.1\">AAAI</em>, vol.&nbsp;35, no.&nbsp;7, 2021, pp. 6384–6392.\n\n</span>\n</li>\n<li id=\"bib.bib74\">\n<span>[74]</span>\n<span>\nH.&nbsp;Zhang, X.&nbsp;Liu, H.&nbsp;Pan, Y.&nbsp;Song, and C.&nbsp;W.-K. Leung, “Aser: A large-scale eventuality knowledge graph,” in <em id=\"bib.bib74.1.1\">Proceedings of the web conference 2020</em>, 2020, pp. 201–211.\n\n</span>\n</li>\n<li id=\"bib.bib75\">\n<span>[75]</span>\n<span>\nH.&nbsp;Zhang, D.&nbsp;Khashabi, Y.&nbsp;Song, and D.&nbsp;Roth, “Transomcs: from linguistic graphs to commonsense knowledge,” in <em id=\"bib.bib75.1.1\">IJCAI</em>, 2021, pp. 4004–4010.\n\n</span>\n</li>\n<li id=\"bib.bib76\">\n<span>[76]</span>\n<span>\nZ.&nbsp;Li, X.&nbsp;Ding, T.&nbsp;Liu, J.&nbsp;E. Hu, and B.&nbsp;Van&nbsp;Durme, “Guided generation of cause and effect,” in <em id=\"bib.bib76.1.1\">IJCAI</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib77\">\n<span>[77]</span>\n<span>\nO.&nbsp;Bodenreider, “The unified medical language system (umls): integrating biomedical terminology,” <em id=\"bib.bib77.1.1\">Nucleic acids research</em>, vol.&nbsp;32, no. suppl_1, pp. D267–D270, 2004.\n\n</span>\n</li>\n<li id=\"bib.bib78\">\n<span>[78]</span>\n<span>\nY.&nbsp;Liu, Q.&nbsp;Zeng, J.&nbsp;Ordieres&nbsp;Meré, and H.&nbsp;Yang, “Anticipating stock market of the renowned companies: a knowledge graph approach,” <em id=\"bib.bib78.1.1\">Complexity</em>, vol. 2019, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib79\">\n<span>[79]</span>\n<span>\nY.&nbsp;Zhu, W.&nbsp;Zhou, Y.&nbsp;Xu, J.&nbsp;Liu, Y.&nbsp;Tan <em id=\"bib.bib79.1.1\">et&nbsp;al.</em>, “Intelligent learning for knowledge graph towards geological data,” <em id=\"bib.bib79.2.2\">Scientific Programming</em>, vol. 2017, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib80\">\n<span>[80]</span>\n<span>\nW.&nbsp;Choi and H.&nbsp;Lee, “Inference of biomedical relations among chemicals, genes, diseases, and symptoms using knowledge representation learning,” <em id=\"bib.bib80.1.1\">IEEE Access</em>, vol.&nbsp;7, pp. 179 373–179 384, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib81\">\n<span>[81]</span>\n<span>\nF.&nbsp;Farazi, M.&nbsp;Salamanca, S.&nbsp;Mosbach, J.&nbsp;Akroyd, A.&nbsp;Eibeck, L.&nbsp;K. Aditya, A.&nbsp;Chadzynski, K.&nbsp;Pan, X.&nbsp;Zhou, S.&nbsp;Zhang <em id=\"bib.bib81.1.1\">et&nbsp;al.</em>, “Knowledge graph approach to combustion chemistry and interoperability,” <em id=\"bib.bib81.2.2\">ACS omega</em>, vol.&nbsp;5, no.&nbsp;29, pp. 18 342–18 348, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib82\">\n<span>[82]</span>\n<span>\nX.&nbsp;Wu, T.&nbsp;Jiang, Y.&nbsp;Zhu, and C.&nbsp;Bu, “Knowledge graph for china’s genealogy,” <em id=\"bib.bib82.1.1\">IEEE TKDE</em>, vol.&nbsp;35, no.&nbsp;1, pp. 634–646, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib83\">\n<span>[83]</span>\n<span>\nX.&nbsp;Zhu, Z.&nbsp;Li, X.&nbsp;Wang, X.&nbsp;Jiang, P.&nbsp;Sun, X.&nbsp;Wang, Y.&nbsp;Xiao, and N.&nbsp;J. Yuan, “Multi-modal knowledge graph construction and application: A survey,” <em id=\"bib.bib83.1.1\">IEEE TKDE</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib84\">\n<span>[84]</span>\n<span>\nS.&nbsp;Ferrada, B.&nbsp;Bustos, and A.&nbsp;Hogan, “Imgpedia: a linked dataset with content-based analysis of wikimedia images,” in <em id=\"bib.bib84.1.1\">The Semantic Web–ISWC 2017</em>.&nbsp;&nbsp;&nbsp;Springer, 2017, pp. 84–93.\n\n</span>\n</li>\n<li id=\"bib.bib85\">\n<span>[85]</span>\n<span>\nY.&nbsp;Liu, H.&nbsp;Li, A.&nbsp;Garcia-Duran, M.&nbsp;Niepert, D.&nbsp;Onoro-Rubio, and D.&nbsp;S. Rosenblum, “Mmkg: multi-modal knowledge graphs,” in <em id=\"bib.bib85.1.1\">The Semantic Web: 16th International Conference, ESWC 2019, Portorož, Slovenia, June 2–6, 2019, Proceedings 16</em>.&nbsp;&nbsp;&nbsp;Springer, 2019, pp. 459–474.\n\n</span>\n</li>\n<li id=\"bib.bib86\">\n<span>[86]</span>\n<span>\nM.&nbsp;Wang, H.&nbsp;Wang, G.&nbsp;Qi, and Q.&nbsp;Zheng, “Richpedia: a large-scale, comprehensive multi-modal knowledge graph,” <em id=\"bib.bib86.1.1\">Big Data Research</em>, vol.&nbsp;22, p. 100159, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib87\">\n<span>[87]</span>\n<span>\nB.&nbsp;Shi, L.&nbsp;Ji, P.&nbsp;Lu, Z.&nbsp;Niu, and N.&nbsp;Duan, “Knowledge aware semantic concept expansion for image-text matching.” in <em id=\"bib.bib87.1.1\">IJCAI</em>, vol.&nbsp;1, 2019, p.&nbsp;2.\n\n</span>\n</li>\n<li id=\"bib.bib88\">\n<span>[88]</span>\n<span>\nS.&nbsp;Shah, A.&nbsp;Mishra, N.&nbsp;Yadati, and P.&nbsp;P. Talukdar, “Kvqa: Knowledge-aware visual question answering,” in <em id=\"bib.bib88.1.1\">AAAI</em>, vol.&nbsp;33, no.&nbsp;01, 2019, pp. 8876–8884.\n\n</span>\n</li>\n<li id=\"bib.bib89\">\n<span>[89]</span>\n<span>\nR.&nbsp;Sun, X.&nbsp;Cao, Y.&nbsp;Zhao, J.&nbsp;Wan, K.&nbsp;Zhou, F.&nbsp;Zhang, Z.&nbsp;Wang, and K.&nbsp;Zheng, “Multi-modal knowledge graphs for recommender systems,” in <em id=\"bib.bib89.1.1\">CIKM</em>, 2020, pp. 1405–1414.\n\n</span>\n</li>\n<li id=\"bib.bib90\">\n<span>[90]</span>\n<span>\nS.&nbsp;Deng, C.&nbsp;Wang, Z.&nbsp;Li, N.&nbsp;Zhang, Z.&nbsp;Dai, H.&nbsp;Chen, F.&nbsp;Xiong, M.&nbsp;Yan, Q.&nbsp;Chen, M.&nbsp;Chen, J.&nbsp;Chen, J.&nbsp;Z. Pan, B.&nbsp;Hooi, and H.&nbsp;Chen, “Construction and applications of billion-scale pre-trained multimodal business knowledge graph,” in <em id=\"bib.bib90.1.1\">ICDE</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib91\">\n<span>[91]</span>\n<span>\nC.&nbsp;Rosset, C.&nbsp;Xiong, M.&nbsp;Phan, X.&nbsp;Song, P.&nbsp;Bennett, and S.&nbsp;Tiwary, “Knowledge-aware language model pretraining,” <em id=\"bib.bib91.1.1\">arXiv preprint arXiv:2007.00655</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib92\">\n<span>[92]</span>\n<span>\nP.&nbsp;Lewis, E.&nbsp;Perez, A.&nbsp;Piktus, F.&nbsp;Petroni, V.&nbsp;Karpukhin, N.&nbsp;Goyal, H.&nbsp;Küttler, M.&nbsp;Lewis, W.-t. Yih, T.&nbsp;Rocktäschel, S.&nbsp;Riedel, and D.&nbsp;Kiela, “Retrieval-augmented generation for knowledge-intensive nlp tasks,” in <em id=\"bib.bib92.1.1\">NeurIPS</em>, vol.&nbsp;33, 2020, pp. 9459–9474.\n\n</span>\n</li>\n<li id=\"bib.bib93\">\n<span>[93]</span>\n<span>\nY.&nbsp;Zhu, X.&nbsp;Wang, J.&nbsp;Chen, S.&nbsp;Qiao, Y.&nbsp;Ou, Y.&nbsp;Yao, S.&nbsp;Deng, H.&nbsp;Chen, and N.&nbsp;Zhang, “Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities,” <em id=\"bib.bib93.1.1\">arXiv preprint arXiv:2305.13168</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib94\">\n<span>[94]</span>\n<span>\nZ.&nbsp;Zhang, X.&nbsp;Liu, Y.&nbsp;Zhang, Q.&nbsp;Su, X.&nbsp;Sun, and B.&nbsp;He, “Pretrain-kge: learning knowledge representation from pretrained language models,” in <em id=\"bib.bib94.1.1\">EMNLP Finding</em>, 2020, pp. 259–266.\n\n</span>\n</li>\n<li id=\"bib.bib95\">\n<span>[95]</span>\n<span>\nA.&nbsp;Kumar, A.&nbsp;Pandey, R.&nbsp;Gadia, and M.&nbsp;Mishra, “Building knowledge graph using pre-trained language model for learning entity-aware relationships,” in <em id=\"bib.bib95.1.1\">2020 IEEE International Conference on Computing, Power and Communication Technologies (GUCON)</em>.&nbsp;&nbsp;&nbsp;IEEE, 2020, pp. 310–315.\n\n</span>\n</li>\n<li id=\"bib.bib96\">\n<span>[96]</span>\n<span>\nX.&nbsp;Xie, N.&nbsp;Zhang, Z.&nbsp;Li, S.&nbsp;Deng, H.&nbsp;Chen, F.&nbsp;Xiong, M.&nbsp;Chen, and H.&nbsp;Chen, “From discrimination to generation: Knowledge graph completion with generative transformer,” in <em id=\"bib.bib96.1.1\">WWW</em>, 2022, pp. 162–165.\n\n</span>\n</li>\n<li id=\"bib.bib97\">\n<span>[97]</span>\n<span>\nZ.&nbsp;Chen, C.&nbsp;Xu, F.&nbsp;Su, Z.&nbsp;Huang, and Y.&nbsp;Dou, “Incorporating structured sentences with time-enhanced bert for fully-inductive temporal relation prediction,” <em id=\"bib.bib97.1.1\">SIGIR</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib98\">\n<span>[98]</span>\n<span>\nD.&nbsp;Zhu, J.&nbsp;Chen, X.&nbsp;Shen, X.&nbsp;Li, and M.&nbsp;Elhoseiny, “Minigpt-4: Enhancing vision-language understanding with advanced large language models,” <em id=\"bib.bib98.1.1\">arXiv preprint arXiv:2304.10592</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib99\">\n<span>[99]</span>\n<span>\nM.&nbsp;Warren, D.&nbsp;A. Shamma, and P.&nbsp;J. Hayes, “Knowledge engineering with image data in real-world settings,” in <em id=\"bib.bib99.1.1\">AAAI</em>, ser. CEUR Workshop Proceedings, vol. 2846, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib100\">\n<span>[100]</span>\n<span>\nR.&nbsp;Thoppilan, D.&nbsp;De&nbsp;Freitas, J.&nbsp;Hall, N.&nbsp;Shazeer, A.&nbsp;Kulshreshtha, H.-T. Cheng, A.&nbsp;Jin, T.&nbsp;Bos, L.&nbsp;Baker, Y.&nbsp;Du <em id=\"bib.bib100.1.1\">et&nbsp;al.</em>, “Lamda: Language models for dialog applications,” <em id=\"bib.bib100.2.2\">arXiv preprint arXiv:2201.08239</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib101\">\n<span>[101]</span>\n<span>\nY.&nbsp;Sun, S.&nbsp;Wang, S.&nbsp;Feng, S.&nbsp;Ding, C.&nbsp;Pang, J.&nbsp;Shang, J.&nbsp;Liu, X.&nbsp;Chen, Y.&nbsp;Zhao, Y.&nbsp;Lu <em id=\"bib.bib101.1.1\">et&nbsp;al.</em>, “Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation,” <em id=\"bib.bib101.2.2\">arXiv preprint arXiv:2107.02137</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib102\">\n<span>[102]</span>\n<span>\nT.&nbsp;Shen, Y.&nbsp;Mao, P.&nbsp;He, G.&nbsp;Long, A.&nbsp;Trischler, and W.&nbsp;Chen, “Exploiting structured knowledge in text via graph-guided representation learning,” in <em id=\"bib.bib102.1.1\">EMNLP</em>, 2020, pp. 8980–8994.\n\n</span>\n</li>\n<li id=\"bib.bib103\">\n<span>[103]</span>\n<span>\nD.&nbsp;Zhang, Z.&nbsp;Yuan, Y.&nbsp;Liu, F.&nbsp;Zhuang, H.&nbsp;Chen, and H.&nbsp;Xiong, “E-bert: A phrase and product knowledge enhanced language model for e-commerce,” <em id=\"bib.bib103.1.1\">arXiv preprint arXiv:2009.02835</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib104\">\n<span>[104]</span>\n<span>\nS.&nbsp;Li, X.&nbsp;Li, L.&nbsp;Shang, C.&nbsp;Sun, B.&nbsp;Liu, Z.&nbsp;Ji, X.&nbsp;Jiang, and Q.&nbsp;Liu, “Pre-training language models with deterministic factual knowledge,” in <em id=\"bib.bib104.1.1\">EMNLP</em>, 2022, pp. 11 118–11 131.\n\n</span>\n</li>\n<li id=\"bib.bib105\">\n<span>[105]</span>\n<span>\nM.&nbsp;Kang, J.&nbsp;Baek, and S.&nbsp;J. Hwang, “Kala: Knowledge-augmented language model adaptation,” in <em id=\"bib.bib105.1.1\">NAACL</em>, 2022, pp. 5144–5167.\n\n</span>\n</li>\n<li id=\"bib.bib106\">\n<span>[106]</span>\n<span>\nW.&nbsp;Xiong, J.&nbsp;Du, W.&nbsp;Y. Wang, and V.&nbsp;Stoyanov, “Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model,” in <em id=\"bib.bib106.1.1\">ICLR</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib107\">\n<span>[107]</span>\n<span>\nT.&nbsp;Sun, Y.&nbsp;Shao, X.&nbsp;Qiu, Q.&nbsp;Guo, Y.&nbsp;Hu, X.&nbsp;Huang, and Z.&nbsp;Zhang, “CoLAKE: Contextualized language and knowledge embedding,” in <em id=\"bib.bib107.1.1\">Proceedings of the 28th International Conference on Computational Linguistics</em>, 2020, pp. 3660–3670.\n\n</span>\n</li>\n<li id=\"bib.bib108\">\n<span>[108]</span>\n<span>\nT.&nbsp;Zhang, C.&nbsp;Wang, N.&nbsp;Hu, M.&nbsp;Qiu, C.&nbsp;Tang, X.&nbsp;He, and J.&nbsp;Huang, “DKPLM: decomposable knowledge-enhanced pre-trained language model for natural language understanding,” in <em id=\"bib.bib108.1.1\">AAAI</em>, 2022, pp. 11 703–11 711.\n\n</span>\n</li>\n<li id=\"bib.bib109\">\n<span>[109]</span>\n<span>\nJ.&nbsp;Wang, W.&nbsp;Huang, M.&nbsp;Qiu, Q.&nbsp;Shi, H.&nbsp;Wang, X.&nbsp;Li, and M.&nbsp;Gao, “Knowledge prompting in pre-trained language model for natural language understanding,” in <em id=\"bib.bib109.1.1\">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, 2022, pp. 3164–3177.\n\n</span>\n</li>\n<li id=\"bib.bib110\">\n<span>[110]</span>\n<span>\nH.&nbsp;Ye, N.&nbsp;Zhang, S.&nbsp;Deng, X.&nbsp;Chen, H.&nbsp;Chen, F.&nbsp;Xiong, X.&nbsp;Chen, and H.&nbsp;Chen, “Ontology-enhanced prompt-tuning for few-shot learning,” in <em id=\"bib.bib110.1.1\">Proceedings of the ACM Web Conference 2022</em>, 2022, pp. 778–787.\n\n</span>\n</li>\n<li id=\"bib.bib111\">\n<span>[111]</span>\n<span>\nH.&nbsp;Luo, Z.&nbsp;Tang, S.&nbsp;Peng, Y.&nbsp;Guo, W.&nbsp;Zhang, C.&nbsp;Ma, G.&nbsp;Dong, M.&nbsp;Song, W.&nbsp;Lin <em id=\"bib.bib111.1.1\">et&nbsp;al.</em>, “Chatkbqa: A generate-then-retrieve framework for knowledge base question answering with fine-tuned large language models,” <em id=\"bib.bib111.2.2\">arXiv preprint arXiv:2310.08975</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib112\">\n<span>[112]</span>\n<span>\nL.&nbsp;Luo, Y.-F. Li, G.&nbsp;Haffari, and S.&nbsp;Pan, “Reasoning on graphs: Faithful and interpretable large language model reasoning,” <em id=\"bib.bib112.1.1\">arXiv preprint arxiv:2310.01061</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib113\">\n<span>[113]</span>\n<span>\nR.&nbsp;Logan, N.&nbsp;F. Liu, M.&nbsp;E. Peters, M.&nbsp;Gardner, and S.&nbsp;Singh, “Barack’s wife hillary: Using knowledge graphs for fact-aware language modeling,” in <em id=\"bib.bib113.1.1\">ACL</em>, 2019, pp. 5962–5971.\n\n</span>\n</li>\n<li id=\"bib.bib114\">\n<span>[114]</span>\n<span>\nK.&nbsp;Guu, K.&nbsp;Lee, Z.&nbsp;Tung, P.&nbsp;Pasupat, and M.-W. Chang, “Realm: Retrieval-augmented language model pre-training,” in <em id=\"bib.bib114.1.1\">ICML</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib115\">\n<span>[115]</span>\n<span>\nY.&nbsp;Wu, Y.&nbsp;Zhao, B.&nbsp;Hu, P.&nbsp;Minervini, P.&nbsp;Stenetorp, and S.&nbsp;Riedel, “An efficient memory-augmented transformer for knowledge-intensive NLP tasks,” in <em id=\"bib.bib115.1.1\">EMNLP</em>, 2022, pp. 5184–5196.\n\n</span>\n</li>\n<li id=\"bib.bib116\">\n<span>[116]</span>\n<span>\nL.&nbsp;Luo, J.&nbsp;Ju, B.&nbsp;Xiong, Y.-F. Li, G.&nbsp;Haffari, and S.&nbsp;Pan, “Chatrule: Mining logical rules with large language models for knowledge graph reasoning,” <em id=\"bib.bib116.1.1\">arXiv preprint arXiv:2309.01538</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib117\">\n<span>[117]</span>\n<span>\nJ.&nbsp;Wang, Q.&nbsp;Sun, N.&nbsp;Chen, X.&nbsp;Li, and M.&nbsp;Gao, “Boosting language models reasoning with chain-of-knowledge prompting,” <em id=\"bib.bib117.1.1\">arXiv preprint arXiv:2306.06427</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib118\">\n<span>[118]</span>\n<span>\nZ.&nbsp;Jiang, F.&nbsp;F. Xu, J.&nbsp;Araki, and G.&nbsp;Neubig, “How can we know what language models know?” <em id=\"bib.bib118.1.1\">Transactions of the Association for Computational Linguistics</em>, vol.&nbsp;8, pp. 423–438, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib119\">\n<span>[119]</span>\n<span>\nT.&nbsp;Shin, Y.&nbsp;Razeghi, R.&nbsp;L. Logan&nbsp;IV, E.&nbsp;Wallace, and S.&nbsp;Singh, “Autoprompt: Eliciting knowledge from language models with automatically generated prompts,” <em id=\"bib.bib119.1.1\">arXiv preprint arXiv:2010.15980</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib120\">\n<span>[120]</span>\n<span>\nZ.&nbsp;Meng, F.&nbsp;Liu, E.&nbsp;Shareghi, Y.&nbsp;Su, C.&nbsp;Collins, and N.&nbsp;Collier, “Rewire-then-probe: A contrastive recipe for probing biomedical knowledge of pre-trained language models,” <em id=\"bib.bib120.1.1\">arXiv preprint arXiv:2110.08173</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib121\">\n<span>[121]</span>\n<span>\nL.&nbsp;Luo, T.-T. Vu, D.&nbsp;Phung, and G.&nbsp;Haffari, “Systematic assessment of factual knowledge in large language models,” in <em id=\"bib.bib121.1.1\">EMNLP</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib122\">\n<span>[122]</span>\n<span>\nV.&nbsp;Swamy, A.&nbsp;Romanou, and M.&nbsp;Jaggi, “Interpreting language models through knowledge graph extraction,” <em id=\"bib.bib122.1.1\">arXiv preprint arXiv:2111.08546</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib123\">\n<span>[123]</span>\n<span>\nS.&nbsp;Li, X.&nbsp;Li, L.&nbsp;Shang, Z.&nbsp;Dong, C.&nbsp;Sun, B.&nbsp;Liu, Z.&nbsp;Ji, X.&nbsp;Jiang, and Q.&nbsp;Liu, “How pre-trained language models capture factual knowledge? a causal-inspired analysis,” <em id=\"bib.bib123.1.1\">arXiv preprint arXiv:2203.16747</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib124\">\n<span>[124]</span>\n<span>\nH.&nbsp;Tian, C.&nbsp;Gao, X.&nbsp;Xiao, H.&nbsp;Liu, B.&nbsp;He, H.&nbsp;Wu, H.&nbsp;Wang, and F.&nbsp;Wu, “SKEP: Sentiment knowledge enhanced pre-training for sentiment analysis,” in <em id=\"bib.bib124.1.1\">ACL</em>, 2020, pp. 4067–4076.\n\n</span>\n</li>\n<li id=\"bib.bib125\">\n<span>[125]</span>\n<span>\nW.&nbsp;Yu, C.&nbsp;Zhu, Y.&nbsp;Fang, D.&nbsp;Yu, S.&nbsp;Wang, Y.&nbsp;Xu, M.&nbsp;Zeng, and M.&nbsp;Jiang, “Dict-BERT: Enhancing language model pre-training with dictionary,” in <em id=\"bib.bib125.1.1\">ACL</em>, 2022, pp. 1907–1918.\n\n</span>\n</li>\n<li id=\"bib.bib126\">\n<span>[126]</span>\n<span>\nT.&nbsp;McCoy, E.&nbsp;Pavlick, and T.&nbsp;Linzen, “Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference,” in <em id=\"bib.bib126.1.1\">ACL</em>, 2019, pp. 3428–3448.\n\n</span>\n</li>\n<li id=\"bib.bib127\">\n<span>[127]</span>\n<span>\nD.&nbsp;Wilmot and F.&nbsp;Keller, “Memory and knowledge augmented language models for inferring salience in long-form stories,” in <em id=\"bib.bib127.1.1\">EMNLP</em>, 2021, pp. 851–865.\n\n</span>\n</li>\n<li id=\"bib.bib128\">\n<span>[128]</span>\n<span>\nL.&nbsp;Adolphs, S.&nbsp;Dhuliawala, and T.&nbsp;Hofmann, “How to query language models?” <em id=\"bib.bib128.1.1\">arXiv preprint arXiv:2108.01928</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib129\">\n<span>[129]</span>\n<span>\nM.&nbsp;Sung, J.&nbsp;Lee, S.&nbsp;Yi, M.&nbsp;Jeon, S.&nbsp;Kim, and J.&nbsp;Kang, “Can language models be biomedical knowledge bases?” in <em id=\"bib.bib129.1.1\">EMNLP</em>, 2021, pp. 4723–4734.\n\n</span>\n</li>\n<li id=\"bib.bib130\">\n<span>[130]</span>\n<span>\nA.&nbsp;Mallen, A.&nbsp;Asai, V.&nbsp;Zhong, R.&nbsp;Das, H.&nbsp;Hajishirzi, and D.&nbsp;Khashabi, “When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories,” <em id=\"bib.bib130.1.1\">arXiv preprint arXiv:2212.10511</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib131\">\n<span>[131]</span>\n<span>\nM.&nbsp;Yasunaga, H.&nbsp;Ren, A.&nbsp;Bosselut, P.&nbsp;Liang, and J.&nbsp;Leskovec, “QA-GNN: Reasoning with language models and knowledge graphs for question answering,” in <em id=\"bib.bib131.1.1\">NAACL</em>, 2021, pp. 535–546.\n\n</span>\n</li>\n<li id=\"bib.bib132\">\n<span>[132]</span>\n<span>\nM.&nbsp;Nayyeri, Z.&nbsp;Wang, M.&nbsp;Akter, M.&nbsp;M. Alam, M.&nbsp;R. A.&nbsp;H. Rony, J.&nbsp;Lehmann, S.&nbsp;Staab <em id=\"bib.bib132.1.1\">et&nbsp;al.</em>, “Integrating knowledge graph embedding and pretrained language models in hypercomplex spaces,” <em id=\"bib.bib132.2.2\">arXiv preprint arXiv:2208.02743</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib133\">\n<span>[133]</span>\n<span>\nN.&nbsp;Huang, Y.&nbsp;R. Deshpande, Y.&nbsp;Liu, H.&nbsp;Alberts, K.&nbsp;Cho, C.&nbsp;Vania, and I.&nbsp;Calixto, “Endowing language models with multimodal knowledge graph representations,” <em id=\"bib.bib133.1.1\">arXiv preprint arXiv:2206.13163</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib134\">\n<span>[134]</span>\n<span>\nM.&nbsp;M. Alam, M.&nbsp;R. A.&nbsp;H. Rony, M.&nbsp;Nayyeri, K.&nbsp;Mohiuddin, M.&nbsp;M. Akter, S.&nbsp;Vahdati, and J.&nbsp;Lehmann, “Language model guided knowledge graph embeddings,” <em id=\"bib.bib134.1.1\">IEEE Access</em>, vol.&nbsp;10, pp. 76 008–76 020, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib135\">\n<span>[135]</span>\n<span>\nX.&nbsp;Wang, Q.&nbsp;He, J.&nbsp;Liang, and Y.&nbsp;Xiao, “Language models as knowledge embeddings,” <em id=\"bib.bib135.1.1\">arXiv preprint arXiv:2206.12617</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib136\">\n<span>[136]</span>\n<span>\nN.&nbsp;Zhang, X.&nbsp;Xie, X.&nbsp;Chen, S.&nbsp;Deng, C.&nbsp;Tan, F.&nbsp;Huang, X.&nbsp;Cheng, and H.&nbsp;Chen, “Reasoning through memorization: Nearest neighbor knowledge graph embeddings,” <em id=\"bib.bib136.1.1\">arXiv preprint arXiv:2201.05575</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib137\">\n<span>[137]</span>\n<span>\nX.&nbsp;Xie, Z.&nbsp;Li, X.&nbsp;Wang, Y.&nbsp;Zhu, N.&nbsp;Zhang, J.&nbsp;Zhang, S.&nbsp;Cheng, B.&nbsp;Tian, S.&nbsp;Deng, F.&nbsp;Xiong, and H.&nbsp;Chen, “Lambdakg: A library for pre-trained language model-based knowledge graph embeddings,” 2022.\n\n</span>\n</li>\n<li id=\"bib.bib138\">\n<span>[138]</span>\n<span>\nB.&nbsp;Kim, T.&nbsp;Hong, Y.&nbsp;Ko, and J.&nbsp;Seo, “Multi-task learning for knowledge graph completion with pre-trained language models,” in <em id=\"bib.bib138.1.1\">COLING</em>, 2020, pp. 1737–1743.\n\n</span>\n</li>\n<li id=\"bib.bib139\">\n<span>[139]</span>\n<span>\nX.&nbsp;Lv, Y.&nbsp;Lin, Y.&nbsp;Cao, L.&nbsp;Hou, J.&nbsp;Li, Z.&nbsp;Liu, P.&nbsp;Li, and J.&nbsp;Zhou, “Do pre-trained models benefit knowledge graph completion? A reliable evaluation and a reasonable approach,” in <em id=\"bib.bib139.1.1\">ACL</em>, 2022, pp. 3570–3581.\n\n</span>\n</li>\n<li id=\"bib.bib140\">\n<span>[140]</span>\n<span>\nJ.&nbsp;Shen, C.&nbsp;Wang, L.&nbsp;Gong, and D.&nbsp;Song, “Joint language semantic and structure embedding for knowledge graph completion,” in <em id=\"bib.bib140.1.1\">COLING</em>, 2022, pp. 1965–1978.\n\n</span>\n</li>\n<li id=\"bib.bib141\">\n<span>[141]</span>\n<span>\nB.&nbsp;Choi, D.&nbsp;Jang, and Y.&nbsp;Ko, “MEM-KGC: masked entity model for knowledge graph completion with pre-trained language model,” <em id=\"bib.bib141.1.1\">IEEE Access</em>, vol.&nbsp;9, pp. 132 025–132 032, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib142\">\n<span>[142]</span>\n<span>\nB.&nbsp;Choi and Y.&nbsp;Ko, “Knowledge graph extension with a pre-trained language model via unified learning method,” <em id=\"bib.bib142.1.1\">Knowl. Based Syst.</em>, vol. 262, p. 110245, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib143\">\n<span>[143]</span>\n<span>\nB.&nbsp;Wang, T.&nbsp;Shen, G.&nbsp;Long, T.&nbsp;Zhou, Y.&nbsp;Wang, and Y.&nbsp;Chang, “Structure-augmented text representation learning for efficient knowledge graph completion,” in <em id=\"bib.bib143.1.1\">WWW</em>, 2021, pp. 1737–1748.\n\n</span>\n</li>\n<li id=\"bib.bib144\">\n<span>[144]</span>\n<span>\nL.&nbsp;Wang, W.&nbsp;Zhao, Z.&nbsp;Wei, and J.&nbsp;Liu, “Simkgc: Simple contrastive knowledge graph completion with pre-trained language models,” in <em id=\"bib.bib144.1.1\">ACL</em>, 2022, pp. 4281–4294.\n\n</span>\n</li>\n<li id=\"bib.bib145\">\n<span>[145]</span>\n<span>\nD.&nbsp;Li, M.&nbsp;Yi, and Y.&nbsp;He, “Lp-bert: Multi-task pre-training knowledge graph bert for link prediction,” <em id=\"bib.bib145.1.1\">arXiv preprint arXiv:2201.04843</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib146\">\n<span>[146]</span>\n<span>\nA.&nbsp;Saxena, A.&nbsp;Kochsiek, and R.&nbsp;Gemulla, “Sequence-to-sequence knowledge graph completion and question answering,” in <em id=\"bib.bib146.1.1\">ACL</em>, 2022, pp. 2814–2828.\n\n</span>\n</li>\n<li id=\"bib.bib147\">\n<span>[147]</span>\n<span>\nC.&nbsp;Chen, Y.&nbsp;Wang, B.&nbsp;Li, and K.&nbsp;Lam, “Knowledge is flat: A seq2seq generative framework for various knowledge graph completion,” in <em id=\"bib.bib147.1.1\">COLING</em>, 2022, pp. 4005–4017.\n\n</span>\n</li>\n<li id=\"bib.bib148\">\n<span>[148]</span>\n<span>\nM.&nbsp;E. Peters, M.&nbsp;Neumann, M.&nbsp;Iyyer, M.&nbsp;Gardner, C.&nbsp;Clark, K.&nbsp;Lee, and L.&nbsp;Zettlemoyer, “Deep contextualized word representations,” in <em id=\"bib.bib148.1.1\">NAACL</em>, 2018, pp. 2227–2237.\n\n</span>\n</li>\n<li id=\"bib.bib149\">\n<span>[149]</span>\n<span>\nH.&nbsp;Yan, T.&nbsp;Gui, J.&nbsp;Dai, Q.&nbsp;Guo, Z.&nbsp;Zhang, and X.&nbsp;Qiu, “A unified generative framework for various NER subtasks,” in <em id=\"bib.bib149.1.1\">ACL</em>, 2021, pp. 5808–5822.\n\n</span>\n</li>\n<li id=\"bib.bib150\">\n<span>[150]</span>\n<span>\nY.&nbsp;Onoe and G.&nbsp;Durrett, “Learning to denoise distantly-labeled data for entity typing,” in <em id=\"bib.bib150.1.1\">NAACL</em>, 2019, pp. 2407–2417.\n\n</span>\n</li>\n<li id=\"bib.bib151\">\n<span>[151]</span>\n<span>\nY.&nbsp;Onoe, M.&nbsp;Boratko, A.&nbsp;McCallum, and G.&nbsp;Durrett, “Modeling fine-grained entity types with box embeddings,” in <em id=\"bib.bib151.1.1\">ACL</em>, 2021, pp. 2051–2064.\n\n</span>\n</li>\n<li id=\"bib.bib152\">\n<span>[152]</span>\n<span>\nB.&nbsp;Z. Li, S.&nbsp;Min, S.&nbsp;Iyer, Y.&nbsp;Mehdad, and W.&nbsp;Yih, “Efficient one-pass end-to-end entity linking for questions,” in <em id=\"bib.bib152.1.1\">EMNLP</em>, 2020, pp. 6433–6441.\n\n</span>\n</li>\n<li id=\"bib.bib153\">\n<span>[153]</span>\n<span>\nT.&nbsp;Ayoola, S.&nbsp;Tyagi, J.&nbsp;Fisher, C.&nbsp;Christodoulopoulos, and A.&nbsp;Pierleoni, “Refined: An efficient zero-shot-capable approach to end-to-end entity linking,” in <em id=\"bib.bib153.1.1\">NAACL</em>, 2022, pp. 209–220.\n\n</span>\n</li>\n<li id=\"bib.bib154\">\n<span>[154]</span>\n<span>\nM.&nbsp;Joshi, O.&nbsp;Levy, L.&nbsp;Zettlemoyer, and D.&nbsp;S. Weld, “BERT for coreference resolution: Baselines and analysis,” in <em id=\"bib.bib154.1.1\">EMNLP</em>, 2019, pp. 5802–5807.\n\n</span>\n</li>\n<li id=\"bib.bib155\">\n<span>[155]</span>\n<span>\nM.&nbsp;Joshi, D.&nbsp;Chen, Y.&nbsp;Liu, D.&nbsp;S. Weld, L.&nbsp;Zettlemoyer, and O.&nbsp;Levy, “Spanbert: Improving pre-training by representing and predicting spans,” <em id=\"bib.bib155.1.1\">Trans. Assoc. Comput. Linguistics</em>, vol.&nbsp;8, pp. 64–77, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib156\">\n<span>[156]</span>\n<span>\nA.&nbsp;Caciularu, A.&nbsp;Cohan, I.&nbsp;Beltagy, M.&nbsp;E. Peters, A.&nbsp;Cattan, and I.&nbsp;Dagan, “CDLM: cross-document language modeling,” in <em id=\"bib.bib156.1.1\">EMNLP</em>, 2021, pp. 2648–2662.\n\n</span>\n</li>\n<li id=\"bib.bib157\">\n<span>[157]</span>\n<span>\nA.&nbsp;Cattan, A.&nbsp;Eirew, G.&nbsp;Stanovsky, M.&nbsp;Joshi, and I.&nbsp;Dagan, “Cross-document coreference resolution over predicted mentions,” in <em id=\"bib.bib157.1.1\">ACL</em>, 2021, pp. 5100–5107.\n\n</span>\n</li>\n<li id=\"bib.bib158\">\n<span>[158]</span>\n<span>\nY.&nbsp;Wang, Y.&nbsp;Shen, and H.&nbsp;Jin, “An end-to-end actor-critic-based neural coreference resolution system,” in <em id=\"bib.bib158.1.1\">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021</em>, 2021, pp. 7848–7852.\n\n</span>\n</li>\n<li id=\"bib.bib159\">\n<span>[159]</span>\n<span>\nP.&nbsp;Shi and J.&nbsp;Lin, “Simple BERT models for relation extraction and semantic role labeling,” <em id=\"bib.bib159.1.1\">CoRR</em>, vol. abs/1904.05255, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib160\">\n<span>[160]</span>\n<span>\nS.&nbsp;Park and H.&nbsp;Kim, “Improving sentence-level relation extraction through curriculum learning,” <em id=\"bib.bib160.1.1\">CoRR</em>, vol. abs/2107.09332, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib161\">\n<span>[161]</span>\n<span>\nY.&nbsp;Ma, A.&nbsp;Wang, and N.&nbsp;Okazaki, “DREEAM: guiding attention with evidence for improving document-level relation extraction,” in <em id=\"bib.bib161.1.1\">EACL</em>, 2023, pp. 1963–1975.\n\n</span>\n</li>\n<li id=\"bib.bib162\">\n<span>[162]</span>\n<span>\nQ.&nbsp;Guo, Y.&nbsp;Sun, G.&nbsp;Liu, Z.&nbsp;Wang, Z.&nbsp;Ji, Y.&nbsp;Shen, and X.&nbsp;Wang, “Constructing chinese historical literature knowledge graph based on bert,” in <em id=\"bib.bib162.1.1\">Web Information Systems and Applications: 18th International Conference, WISA 2021, Kaifeng, China, September 24–26, 2021, Proceedings 18</em>.&nbsp;&nbsp;&nbsp;Springer, 2021, pp. 323–334.\n\n</span>\n</li>\n<li id=\"bib.bib163\">\n<span>[163]</span>\n<span>\nJ.&nbsp;Han, N.&nbsp;Collier, W.&nbsp;Buntine, and E.&nbsp;Shareghi, “Pive: Prompting with iterative verification improving graph-based generative capability of llms,” <em id=\"bib.bib163.1.1\">arXiv preprint arXiv:2305.12392</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib164\">\n<span>[164]</span>\n<span>\nA.&nbsp;Bosselut, H.&nbsp;Rashkin, M.&nbsp;Sap, C.&nbsp;Malaviya, A.&nbsp;Celikyilmaz, and Y.&nbsp;Choi, “Comet: Commonsense transformers for knowledge graph construction,” in <em id=\"bib.bib164.1.1\">ACL</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib165\">\n<span>[165]</span>\n<span>\nS.&nbsp;Hao, B.&nbsp;Tan, K.&nbsp;Tang, H.&nbsp;Zhang, E.&nbsp;P. Xing, and Z.&nbsp;Hu, “Bertnet: Harvesting knowledge graphs from pretrained language models,” <em id=\"bib.bib165.1.1\">arXiv preprint arXiv:2206.14268</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib166\">\n<span>[166]</span>\n<span>\nP.&nbsp;West, C.&nbsp;Bhagavatula, J.&nbsp;Hessel, J.&nbsp;Hwang, L.&nbsp;Jiang, R.&nbsp;Le&nbsp;Bras, X.&nbsp;Lu, S.&nbsp;Welleck, and Y.&nbsp;Choi, “Symbolic knowledge distillation: from general language models to commonsense models,” in <em id=\"bib.bib166.1.1\">NAACL</em>, 2022, pp. 4602–4625.\n\n</span>\n</li>\n<li id=\"bib.bib167\">\n<span>[167]</span>\n<span>\nL.&nbsp;F.&nbsp;R. Ribeiro, M.&nbsp;Schmitt, H.&nbsp;Schütze, and I.&nbsp;Gurevych, “Investigating pretrained language models for graph-to-text generation,” in <em id=\"bib.bib167.1.1\">Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</em>, 2021, pp. 211–227.\n\n</span>\n</li>\n<li id=\"bib.bib168\">\n<span>[168]</span>\n<span>\nJ.&nbsp;Li, T.&nbsp;Tang, W.&nbsp;X. Zhao, Z.&nbsp;Wei, N.&nbsp;J. Yuan, and J.-R. Wen, “Few-shot knowledge graph-to-text generation with pretrained language models,” in <em id=\"bib.bib168.1.1\">ACL</em>, 2021, pp. 1558–1568.\n\n</span>\n</li>\n<li id=\"bib.bib169\">\n<span>[169]</span>\n<span>\nA.&nbsp;Colas, M.&nbsp;Alvandipour, and D.&nbsp;Z. Wang, “GAP: A graph-aware language model framework for knowledge graph-to-text generation,” in <em id=\"bib.bib169.1.1\">Proceedings of the 29th International Conference on Computational Linguistics</em>, 2022, pp. 5755–5769.\n\n</span>\n</li>\n<li id=\"bib.bib170\">\n<span>[170]</span>\n<span>\nZ.&nbsp;Jin, Q.&nbsp;Guo, X.&nbsp;Qiu, and Z.&nbsp;Zhang, “GenWiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation,” in <em id=\"bib.bib170.1.1\">Proceedings of the 28th International Conference on Computational Linguistics</em>, 2020, pp. 2398–2409.\n\n</span>\n</li>\n<li id=\"bib.bib171\">\n<span>[171]</span>\n<span>\nW.&nbsp;Chen, Y.&nbsp;Su, X.&nbsp;Yan, and W.&nbsp;Y. Wang, “KGPT: Knowledge-grounded pre-training for data-to-text generation,” in <em id=\"bib.bib171.1.1\">EMNLP</em>, 2020, pp. 8635–8648.\n\n</span>\n</li>\n<li id=\"bib.bib172\">\n<span>[172]</span>\n<span>\nD.&nbsp;Lukovnikov, A.&nbsp;Fischer, and J.&nbsp;Lehmann, “Pretrained transformers for simple question answering over knowledge graphs,” in <em id=\"bib.bib172.1.1\">The Semantic Web–ISWC 2019: 18th International Semantic Web Conference, Auckland, New Zealand, October 26–30, 2019, Proceedings, Part I 18</em>.&nbsp;&nbsp;&nbsp;Springer, 2019, pp. 470–486.\n\n</span>\n</li>\n<li id=\"bib.bib173\">\n<span>[173]</span>\n<span>\nD.&nbsp;Luo, J.&nbsp;Su, and S.&nbsp;Yu, “A bert-based approach with relation-aware attention for knowledge base question answering,” in <em id=\"bib.bib173.1.1\">IJCNN</em>.&nbsp;&nbsp;&nbsp;IEEE, 2020, pp. 1–8.\n\n</span>\n</li>\n<li id=\"bib.bib174\">\n<span>[174]</span>\n<span>\nN.&nbsp;Hu, Y.&nbsp;Wu, G.&nbsp;Qi, D.&nbsp;Min, J.&nbsp;Chen, J.&nbsp;Z. Pan, and Z.&nbsp;Ali, “An empirical study of pre-trained language models in simple knowledge graph question answering,” <em id=\"bib.bib174.1.1\">arXiv preprint arXiv:2303.10368</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib175\">\n<span>[175]</span>\n<span>\nY.&nbsp;Xu, C.&nbsp;Zhu, R.&nbsp;Xu, Y.&nbsp;Liu, M.&nbsp;Zeng, and X.&nbsp;Huang, “Fusing context into knowledge graph for commonsense question answering,” in <em id=\"bib.bib175.1.1\">ACL</em>, 2021, pp. 1201–1207.\n\n</span>\n</li>\n<li id=\"bib.bib176\">\n<span>[176]</span>\n<span>\nM.&nbsp;Zhang, R.&nbsp;Dai, M.&nbsp;Dong, and T.&nbsp;He, “Drlk: Dynamic hierarchical reasoning with language model and knowledge graph for question answering,” in <em id=\"bib.bib176.1.1\">EMNLP</em>, 2022, pp. 5123–5133.\n\n</span>\n</li>\n<li id=\"bib.bib177\">\n<span>[177]</span>\n<span>\nZ.&nbsp;Hu, Y.&nbsp;Xu, W.&nbsp;Yu, S.&nbsp;Wang, Z.&nbsp;Yang, C.&nbsp;Zhu, K.-W. Chang, and Y.&nbsp;Sun, “Empowering language models with knowledge graph reasoning for open-domain question answering,” in <em id=\"bib.bib177.1.1\">EMNLP</em>, 2022, pp. 9562–9581.\n\n</span>\n</li>\n<li id=\"bib.bib178\">\n<span>[178]</span>\n<span>\nX.&nbsp;Zhang, A.&nbsp;Bosselut, M.&nbsp;Yasunaga, H.&nbsp;Ren, P.&nbsp;Liang, C.&nbsp;D. Manning, and J.&nbsp;Leskovec, “Greaselm: Graph reasoning enhanced language models,” in <em id=\"bib.bib178.1.1\">ICLR</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib179\">\n<span>[179]</span>\n<span>\nX.&nbsp;Cao and Y.&nbsp;Liu, “Relmkg: reasoning with pre-trained language models and knowledge graphs for complex question answering,” <em id=\"bib.bib179.1.1\">Applied Intelligence</em>, pp. 1–15, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib180\">\n<span>[180]</span>\n<span>\nX.&nbsp;Huang, J.&nbsp;Zhang, D.&nbsp;Li, and P.&nbsp;Li, “Knowledge graph embedding based question answering,” in <em id=\"bib.bib180.1.1\">WSDM</em>, 2019, pp. 105–113.\n\n</span>\n</li>\n<li id=\"bib.bib181\">\n<span>[181]</span>\n<span>\nH.&nbsp;Wang, F.&nbsp;Zhang, X.&nbsp;Xie, and M.&nbsp;Guo, “Dkn: Deep knowledge-aware network for news recommendation,” in <em id=\"bib.bib181.1.1\">WWW</em>, 2018, pp. 1835–1844.\n\n</span>\n</li>\n<li id=\"bib.bib182\">\n<span>[182]</span>\n<span>\nB.&nbsp;Yang, S.&nbsp;W.-t. Yih, X.&nbsp;He, J.&nbsp;Gao, and L.&nbsp;Deng, “Embedding entities and relations for learning and inference in knowledge bases,” in <em id=\"bib.bib182.1.1\">ICLR</em>, 2015.\n\n</span>\n</li>\n<li id=\"bib.bib183\">\n<span>[183]</span>\n<span>\nW.&nbsp;Xiong, M.&nbsp;Yu, S.&nbsp;Chang, X.&nbsp;Guo, and W.&nbsp;Y. Wang, “One-shot relational learning for knowledge graphs,” in <em id=\"bib.bib183.1.1\">EMNLP</em>, 2018, pp. 1980–1990.\n\n</span>\n</li>\n<li id=\"bib.bib184\">\n<span>[184]</span>\n<span>\nP.&nbsp;Wang, J.&nbsp;Han, C.&nbsp;Li, and R.&nbsp;Pan, “Logic attention based neighborhood aggregation for inductive knowledge graph embedding,” in <em id=\"bib.bib184.1.1\">AAAI</em>, vol.&nbsp;33, no.&nbsp;01, 2019, pp. 7152–7159.\n\n</span>\n</li>\n<li id=\"bib.bib185\">\n<span>[185]</span>\n<span>\nY.&nbsp;Lin, Z.&nbsp;Liu, M.&nbsp;Sun, Y.&nbsp;Liu, and X.&nbsp;Zhu, “Learning entity and relation embeddings for knowledge graph completion,” in <em id=\"bib.bib185.1.1\">Proceedings of the AAAI conference on artificial intelligence</em>, vol.&nbsp;29, no.&nbsp;1, 2015.\n\n</span>\n</li>\n<li id=\"bib.bib186\">\n<span>[186]</span>\n<span>\nC.&nbsp;Chen, Y.&nbsp;Wang, A.&nbsp;Sun, B.&nbsp;Li, and L.&nbsp;Kwok-Yan, “Dipping plms sauce: Bridging structure and text for effective knowledge graph completion via conditional soft prompting,” in <em id=\"bib.bib186.1.1\">ACL</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib187\">\n<span>[187]</span>\n<span>\nJ.&nbsp;Lovelace and C.&nbsp;P. Rosé, “A framework for adapting pre-trained language models to knowledge graph completion,” in <em id=\"bib.bib187.1.1\">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, 2022, pp. 5937–5955.\n\n</span>\n</li>\n<li id=\"bib.bib188\">\n<span>[188]</span>\n<span>\nJ.&nbsp;Fu, L.&nbsp;Feng, Q.&nbsp;Zhang, X.&nbsp;Huang, and P.&nbsp;Liu, “Larger-context tagging: When and why does it work?” in <em id=\"bib.bib188.1.1\">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</em>, 2021, pp. 1463–1475.\n\n</span>\n</li>\n<li id=\"bib.bib189\">\n<span>[189]</span>\n<span>\nX.&nbsp;Liu, K.&nbsp;Ji, Y.&nbsp;Fu, Z.&nbsp;Du, Z.&nbsp;Yang, and J.&nbsp;Tang, “P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks,” <em id=\"bib.bib189.1.1\">CoRR</em>, vol. abs/2110.07602, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib190\">\n<span>[190]</span>\n<span>\nJ.&nbsp;Yu, B.&nbsp;Bohnet, and M.&nbsp;Poesio, “Named entity recognition as dependency parsing,” in <em id=\"bib.bib190.1.1\">ACL</em>, 2020, pp. 6470–6476.\n\n</span>\n</li>\n<li id=\"bib.bib191\">\n<span>[191]</span>\n<span>\nF.&nbsp;Li, Z.&nbsp;Lin, M.&nbsp;Zhang, and D.&nbsp;Ji, “A span-based model for joint overlapped and discontinuous named entity recognition,” in <em id=\"bib.bib191.1.1\">ACL</em>, 2021, pp. 4814–4828.\n\n</span>\n</li>\n<li id=\"bib.bib192\">\n<span>[192]</span>\n<span>\nC.&nbsp;Tan, W.&nbsp;Qiu, M.&nbsp;Chen, R.&nbsp;Wang, and F.&nbsp;Huang, “Boundary enhanced neural span classification for nested named entity recognition,” in <em id=\"bib.bib192.1.1\">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020</em>, 2020, pp. 9016–9023.\n\n</span>\n</li>\n<li id=\"bib.bib193\">\n<span>[193]</span>\n<span>\nY.&nbsp;Xu, H.&nbsp;Huang, C.&nbsp;Feng, and Y.&nbsp;Hu, “A supervised multi-head self-attention network for nested named entity recognition,” in <em id=\"bib.bib193.1.1\">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021</em>, 2021, pp. 14 185–14 193.\n\n</span>\n</li>\n<li id=\"bib.bib194\">\n<span>[194]</span>\n<span>\nJ.&nbsp;Yu, B.&nbsp;Ji, S.&nbsp;Li, J.&nbsp;Ma, H.&nbsp;Liu, and H.&nbsp;Xu, “S-NER: A concise and efficient span-based model for named entity recognition,” <em id=\"bib.bib194.1.1\">Sensors</em>, vol.&nbsp;22, no.&nbsp;8, p. 2852, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib195\">\n<span>[195]</span>\n<span>\nY.&nbsp;Fu, C.&nbsp;Tan, M.&nbsp;Chen, S.&nbsp;Huang, and F.&nbsp;Huang, “Nested named entity recognition with partially-observed treecrfs,” in <em id=\"bib.bib195.1.1\">AAAI</em>, 2021, pp. 12 839–12 847.\n\n</span>\n</li>\n<li id=\"bib.bib196\">\n<span>[196]</span>\n<span>\nC.&nbsp;Lou, S.&nbsp;Yang, and K.&nbsp;Tu, “Nested named entity recognition as latent lexicalized constituency parsing,” in <em id=\"bib.bib196.1.1\">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, 2022, pp. 6183–6198.\n\n</span>\n</li>\n<li id=\"bib.bib197\">\n<span>[197]</span>\n<span>\nS.&nbsp;Yang and K.&nbsp;Tu, “Bottom-up constituency parsing and nested named entity recognition with pointer networks,” in <em id=\"bib.bib197.1.1\">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, 2022, pp. 2403–2416.\n\n</span>\n</li>\n<li id=\"bib.bib198\">\n<span>[198]</span>\n<span>\nF.&nbsp;Li, Z.&nbsp;Lin, M.&nbsp;Zhang, and D.&nbsp;Ji, “A span-based model for joint overlapped and discontinuous named entity recognition,” in <em id=\"bib.bib198.1.1\">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021</em>, 2021, pp. 4814–4828.\n\n</span>\n</li>\n<li id=\"bib.bib199\">\n<span>[199]</span>\n<span>\nQ.&nbsp;Liu, H.&nbsp;Lin, X.&nbsp;Xiao, X.&nbsp;Han, L.&nbsp;Sun, and H.&nbsp;Wu, “Fine-grained entity typing via label reasoning,” in <em id=\"bib.bib199.1.1\">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>, 2021, pp. 4611–4622.\n\n</span>\n</li>\n<li id=\"bib.bib200\">\n<span>[200]</span>\n<span>\nH.&nbsp;Dai, Y.&nbsp;Song, and H.&nbsp;Wang, “Ultra-fine entity typing with weak supervision from a masked language model,” in <em id=\"bib.bib200.1.1\">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021</em>, 2021, pp. 1790–1799.\n\n</span>\n</li>\n<li id=\"bib.bib201\">\n<span>[201]</span>\n<span>\nN.&nbsp;Ding, Y.&nbsp;Chen, X.&nbsp;Han, G.&nbsp;Xu, X.&nbsp;Wang, P.&nbsp;Xie, H.&nbsp;Zheng, Z.&nbsp;Liu, J.&nbsp;Li, and H.&nbsp;Kim, “Prompt-learning for fine-grained entity typing,” in <em id=\"bib.bib201.1.1\">Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, 2022, pp. 6888–6901.\n\n</span>\n</li>\n<li id=\"bib.bib202\">\n<span>[202]</span>\n<span>\nW.&nbsp;Pan, W.&nbsp;Wei, and F.&nbsp;Zhu, “Automatic noisy label correction for fine-grained entity typing,” in <em id=\"bib.bib202.1.1\">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022</em>, 2022, pp. 4317–4323.\n\n</span>\n</li>\n<li id=\"bib.bib203\">\n<span>[203]</span>\n<span>\nB.&nbsp;Li, W.&nbsp;Yin, and M.&nbsp;Chen, “Ultra-fine entity typing with indirect supervision from natural language inference,” <em id=\"bib.bib203.1.1\">Trans. Assoc. Comput. Linguistics</em>, vol.&nbsp;10, pp. 607–622, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib204\">\n<span>[204]</span>\n<span>\nS.&nbsp;Broscheit, “Investigating entity knowledge in BERT with simple neural end-to-end entity linking,” <em id=\"bib.bib204.1.1\">CoRR</em>, vol. abs/2003.05473, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib205\">\n<span>[205]</span>\n<span>\nN.&nbsp;D. Cao, G.&nbsp;Izacard, S.&nbsp;Riedel, and F.&nbsp;Petroni, “Autoregressive entity retrieval,” in <em id=\"bib.bib205.1.1\">9th ICLR, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib206\">\n<span>[206]</span>\n<span>\nN.&nbsp;D. Cao, L.&nbsp;Wu, K.&nbsp;Popat, M.&nbsp;Artetxe, N.&nbsp;Goyal, M.&nbsp;Plekhanov, L.&nbsp;Zettlemoyer, N.&nbsp;Cancedda, S.&nbsp;Riedel, and F.&nbsp;Petroni, “Multilingual autoregressive entity linking,” <em id=\"bib.bib206.1.1\">Trans. Assoc. Comput. Linguistics</em>, vol.&nbsp;10, pp. 274–290, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib207\">\n<span>[207]</span>\n<span>\nN.&nbsp;D. Cao, W.&nbsp;Aziz, and I.&nbsp;Titov, “Highly parallel autoregressive entity linking with discriminative correction,” in <em id=\"bib.bib207.1.1\">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>, 2021, pp. 7662–7669.\n\n</span>\n</li>\n<li id=\"bib.bib208\">\n<span>[208]</span>\n<span>\nK.&nbsp;Lee, L.&nbsp;He, and L.&nbsp;Zettlemoyer, “Higher-order coreference resolution with coarse-to-fine inference,” in <em id=\"bib.bib208.1.1\">NAACL</em>, 2018, pp. 687–692.\n\n</span>\n</li>\n<li id=\"bib.bib209\">\n<span>[209]</span>\n<span>\nT.&nbsp;M. Lai, T.&nbsp;Bui, and D.&nbsp;S. Kim, “End-to-end neural coreference resolution revisited: A simple yet effective baseline,” in <em id=\"bib.bib209.1.1\">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022</em>, 2022, pp. 8147–8151.\n\n</span>\n</li>\n<li id=\"bib.bib210\">\n<span>[210]</span>\n<span>\nW.&nbsp;Wu, F.&nbsp;Wang, A.&nbsp;Yuan, F.&nbsp;Wu, and J.&nbsp;Li, “Corefqa: Coreference resolution as query-based span prediction,” in <em id=\"bib.bib210.1.1\">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>, 2020, pp. 6953–6963.\n\n</span>\n</li>\n<li id=\"bib.bib211\">\n<span>[211]</span>\n<span>\nT.&nbsp;M. Lai, H.&nbsp;Ji, T.&nbsp;Bui, Q.&nbsp;H. Tran, F.&nbsp;Dernoncourt, and W.&nbsp;Chang, “A context-dependent gated module for incorporating symbolic semantics into event coreference resolution,” in <em id=\"bib.bib211.1.1\">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</em>, 2021, pp. 3491–3499.\n\n</span>\n</li>\n<li id=\"bib.bib212\">\n<span>[212]</span>\n<span>\nY.&nbsp;Kirstain, O.&nbsp;Ram, and O.&nbsp;Levy, “Coreference resolution without span representations,” in <em id=\"bib.bib212.1.1\">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual Event, August 1-6, 2021</em>, 2021, pp. 14–19.\n\n</span>\n</li>\n<li id=\"bib.bib213\">\n<span>[213]</span>\n<span>\nR.&nbsp;Thirukovalluru, N.&nbsp;Monath, K.&nbsp;Shridhar, M.&nbsp;Zaheer, M.&nbsp;Sachan, and A.&nbsp;McCallum, “Scaling within document coreference to long texts,” in <em id=\"bib.bib213.1.1\">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021</em>, ser. Findings of ACL, vol. ACL/IJCNLP 2021, 2021, pp. 3921–3931.\n\n</span>\n</li>\n<li id=\"bib.bib214\">\n<span>[214]</span>\n<span>\nI.&nbsp;Beltagy, M.&nbsp;E. Peters, and A.&nbsp;Cohan, “Longformer: The long-document transformer,” <em id=\"bib.bib214.1.1\">CoRR</em>, vol. abs/2004.05150, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib215\">\n<span>[215]</span>\n<span>\nC.&nbsp;Alt, M.&nbsp;Hübner, and L.&nbsp;Hennig, “Improving relation extraction by pre-trained language representations,” in <em id=\"bib.bib215.1.1\">1st Conference on Automated Knowledge Base Construction, AKBC 2019, Amherst, MA, USA, May 20-22, 2019</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib216\">\n<span>[216]</span>\n<span>\nL.&nbsp;B. Soares, N.&nbsp;FitzGerald, J.&nbsp;Ling, and T.&nbsp;Kwiatkowski, “Matching the blanks: Distributional similarity for relation learning,” in <em id=\"bib.bib216.1.1\">ACL</em>, 2019, pp. 2895–2905.\n\n</span>\n</li>\n<li id=\"bib.bib217\">\n<span>[217]</span>\n<span>\nS.&nbsp;Lyu and H.&nbsp;Chen, “Relation classification with entity type restriction,” in <em id=\"bib.bib217.1.1\">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021</em>, ser. Findings of ACL, vol. ACL/IJCNLP 2021, 2021, pp. 390–395.\n\n</span>\n</li>\n<li id=\"bib.bib218\">\n<span>[218]</span>\n<span>\nJ.&nbsp;Zheng and Z.&nbsp;Chen, “Sentence-level relation extraction via contrastive learning with descriptive relation prompts,” <em id=\"bib.bib218.1.1\">CoRR</em>, vol. abs/2304.04935, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib219\">\n<span>[219]</span>\n<span>\nH.&nbsp;Wang, C.&nbsp;Focke, R.&nbsp;Sylvester, N.&nbsp;Mishra, and W.&nbsp;Y. Wang, “Fine-tune bert for docred with two-step process,” <em id=\"bib.bib219.1.1\">CoRR</em>, vol. abs/1909.11898, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib220\">\n<span>[220]</span>\n<span>\nH.&nbsp;Tang, Y.&nbsp;Cao, Z.&nbsp;Zhang, J.&nbsp;Cao, F.&nbsp;Fang, S.&nbsp;Wang, and P.&nbsp;Yin, “HIN: hierarchical inference network for document-level relation extraction,” in <em id=\"bib.bib220.1.1\">PAKDD</em>, ser. Lecture Notes in Computer Science, vol. 12084, 2020, pp. 197–209.\n\n</span>\n</li>\n<li id=\"bib.bib221\">\n<span>[221]</span>\n<span>\nD.&nbsp;Wang, W.&nbsp;Hu, E.&nbsp;Cao, and W.&nbsp;Sun, “Global-to-local neural networks for document-level relation extraction,” in <em id=\"bib.bib221.1.1\">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>, 2020, pp. 3711–3721.\n\n</span>\n</li>\n<li id=\"bib.bib222\">\n<span>[222]</span>\n<span>\nS.&nbsp;Zeng, Y.&nbsp;Wu, and B.&nbsp;Chang, “SIRE: separate intra- and inter-sentential reasoning for document-level relation extraction,” in <em id=\"bib.bib222.1.1\">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021</em>, ser. Findings of ACL, vol. ACL/IJCNLP 2021, 2021, pp. 524–534.\n\n</span>\n</li>\n<li id=\"bib.bib223\">\n<span>[223]</span>\n<span>\nG.&nbsp;Nan, Z.&nbsp;Guo, I.&nbsp;Sekulic, and W.&nbsp;Lu, “Reasoning with latent structure refinement for document-level relation extraction,” in <em id=\"bib.bib223.1.1\">ACL</em>, 2020, pp. 1546–1557.\n\n</span>\n</li>\n<li id=\"bib.bib224\">\n<span>[224]</span>\n<span>\nS.&nbsp;Zeng, R.&nbsp;Xu, B.&nbsp;Chang, and L.&nbsp;Li, “Double graph based reasoning for document-level relation extraction,” in <em id=\"bib.bib224.1.1\">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>, 2020, pp. 1630–1640.\n\n</span>\n</li>\n<li id=\"bib.bib225\">\n<span>[225]</span>\n<span>\nN.&nbsp;Zhang, X.&nbsp;Chen, X.&nbsp;Xie, S.&nbsp;Deng, C.&nbsp;Tan, M.&nbsp;Chen, F.&nbsp;Huang, L.&nbsp;Si, and H.&nbsp;Chen, “Document-level relation extraction as semantic segmentation,” in <em id=\"bib.bib225.1.1\">IJCAI</em>, 2021, pp. 3999–4006.\n\n</span>\n</li>\n<li id=\"bib.bib226\">\n<span>[226]</span>\n<span>\nO.&nbsp;Ronneberger, P.&nbsp;Fischer, and T.&nbsp;Brox, “U-net: Convolutional networks for biomedical image segmentation,” in <em id=\"bib.bib226.1.1\">Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015 - 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III</em>, ser. Lecture Notes in Computer Science, vol. 9351, 2015, pp. 234–241.\n\n</span>\n</li>\n<li id=\"bib.bib227\">\n<span>[227]</span>\n<span>\nW.&nbsp;Zhou, K.&nbsp;Huang, T.&nbsp;Ma, and J.&nbsp;Huang, “Document-level relation extraction with adaptive thresholding and localized context pooling,” in <em id=\"bib.bib227.1.1\">AAAI</em>, 2021, pp. 14 612–14 620.\n\n</span>\n</li>\n<li id=\"bib.bib228\">\n<span>[228]</span>\n<span>\nC.&nbsp;Gardent, A.&nbsp;Shimorina, S.&nbsp;Narayan, and L.&nbsp;Perez-Beltrachini, “The WebNLG challenge: Generating text from RDF data,” in <em id=\"bib.bib228.1.1\">Proceedings of the 10th International Conference on Natural Language Generation</em>, 2017, pp. 124–133.\n\n</span>\n</li>\n<li id=\"bib.bib229\">\n<span>[229]</span>\n<span>\nJ.&nbsp;Guan, Y.&nbsp;Wang, and M.&nbsp;Huang, “Story ending generation with incremental encoding and commonsense knowledge,” in <em id=\"bib.bib229.1.1\">AAAI</em>, 2019, pp. 6473–6480.\n\n</span>\n</li>\n<li id=\"bib.bib230\">\n<span>[230]</span>\n<span>\nH.&nbsp;Zhou, T.&nbsp;Young, M.&nbsp;Huang, H.&nbsp;Zhao, J.&nbsp;Xu, and X.&nbsp;Zhu, “Commonsense knowledge aware conversation generation with graph attention,” in <em id=\"bib.bib230.1.1\">IJCAI</em>, 2018, pp. 4623–4629.\n\n</span>\n</li>\n<li id=\"bib.bib231\">\n<span>[231]</span>\n<span>\nM.&nbsp;Kale and A.&nbsp;Rastogi, “Text-to-text pre-training for data-to-text tasks,” in <em id=\"bib.bib231.1.1\">Proceedings of the 13th International Conference on Natural Language Generation</em>, 2020, pp. 97–102.\n\n</span>\n</li>\n<li id=\"bib.bib232\">\n<span>[232]</span>\n<span>\nM.&nbsp;Mintz, S.&nbsp;Bills, R.&nbsp;Snow, and D.&nbsp;Jurafsky, “Distant supervision for relation extraction without labeled data,” in <em id=\"bib.bib232.1.1\">ACL</em>, 2009, pp. 1003–1011.\n\n</span>\n</li>\n<li id=\"bib.bib233\">\n<span>[233]</span>\n<span>\nA.&nbsp;Saxena, A.&nbsp;Tripathi, and P.&nbsp;Talukdar, “Improving multi-hop question answering over knowledge graphs using knowledge base embeddings,” in <em id=\"bib.bib233.1.1\">ACL</em>, 2020, pp. 4498–4507.\n\n</span>\n</li>\n<li id=\"bib.bib234\">\n<span>[234]</span>\n<span>\nY.&nbsp;Feng, X.&nbsp;Chen, B.&nbsp;Y. Lin, P.&nbsp;Wang, J.&nbsp;Yan, and X.&nbsp;Ren, “Scalable multi-hop relational reasoning for knowledge-aware question answering,” in <em id=\"bib.bib234.1.1\">EMNLP</em>, 2020, pp. 1295–1309.\n\n</span>\n</li>\n<li id=\"bib.bib235\">\n<span>[235]</span>\n<span>\nY.&nbsp;Yan, R.&nbsp;Li, S.&nbsp;Wang, H.&nbsp;Zhang, Z.&nbsp;Daoguang, F.&nbsp;Zhang, W.&nbsp;Wu, and W.&nbsp;Xu, “Large-scale relation learning for question answering over knowledge bases with pre-trained language models,” in <em id=\"bib.bib235.1.1\">EMNLP</em>, 2021, pp. 3653–3660.\n\n</span>\n</li>\n<li id=\"bib.bib236\">\n<span>[236]</span>\n<span>\nJ.&nbsp;Zhang, X.&nbsp;Zhang, J.&nbsp;Yu, J.&nbsp;Tang, J.&nbsp;Tang, C.&nbsp;Li, and H.&nbsp;Chen, “Subgraph retrieval enhanced model for multi-hop knowledge base question answering,” in <em id=\"bib.bib236.1.1\">ACL (Volume 1: Long Papers)</em>, 2022, pp. 5773–5784.\n\n</span>\n</li>\n<li id=\"bib.bib237\">\n<span>[237]</span>\n<span>\nJ.&nbsp;Jiang, K.&nbsp;Zhou, Z.&nbsp;Dong, K.&nbsp;Ye, W.&nbsp;X. Zhao, and J.-R. Wen, “Structgpt: A general framework for large language model to reason over structured data,” <em id=\"bib.bib237.1.1\">arXiv preprint arXiv:2305.09645</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib238\">\n<span>[238]</span>\n<span>\nH.&nbsp;Zhu, H.&nbsp;Peng, Z.&nbsp;Lyu, L.&nbsp;Hou, J.&nbsp;Li, and J.&nbsp;Xiao, “Pre-training language model incorporating domain-specific heterogeneous knowledge into a unified representation,” <em id=\"bib.bib238.1.1\">Expert Systems with Applications</em>, vol. 215, p. 119369, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib239\">\n<span>[239]</span>\n<span>\nC.&nbsp;Feng, X.&nbsp;Zhang, and Z.&nbsp;Fei, “Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs,” <em id=\"bib.bib239.1.1\">arXiv preprint arXiv:2309.03118</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib240\">\n<span>[240]</span>\n<span>\nJ.&nbsp;Sun, C.&nbsp;Xu, L.&nbsp;Tang, S.&nbsp;Wang, C.&nbsp;Lin, Y.&nbsp;Gong, H.-Y. Shum, and J.&nbsp;Guo, “Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph,” <em id=\"bib.bib240.1.1\">arXiv preprint arXiv:2307.07697</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib241\">\n<span>[241]</span>\n<span>\nB.&nbsp;He, D.&nbsp;Zhou, J.&nbsp;Xiao, X.&nbsp;Jiang, Q.&nbsp;Liu, N.&nbsp;J. Yuan, and T.&nbsp;Xu, “BERT-MK: Integrating graph contextualized knowledge into pre-trained language models,” in <em id=\"bib.bib241.1.1\">EMNLP</em>, 2020, pp. 2281–2290.\n\n</span>\n</li>\n<li id=\"bib.bib242\">\n<span>[242]</span>\n<span>\nY.&nbsp;Su, X.&nbsp;Han, Z.&nbsp;Zhang, Y.&nbsp;Lin, P.&nbsp;Li, Z.&nbsp;Liu, J.&nbsp;Zhou, and M.&nbsp;Sun, “Cokebert: Contextual knowledge selection and embedding towards enhanced pre-trained language models,” <em id=\"bib.bib242.1.1\">AI Open</em>, vol.&nbsp;2, pp. 127–134, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib243\">\n<span>[243]</span>\n<span>\nD.&nbsp;Yu, C.&nbsp;Zhu, Y.&nbsp;Yang, and M.&nbsp;Zeng, “JAKET: joint pre-training of knowledge graph and language understanding,” in <em id=\"bib.bib243.1.1\">AAAI</em>, 2022, pp. 11 630–11 638.\n\n</span>\n</li>\n<li id=\"bib.bib244\">\n<span>[244]</span>\n<span>\nX.&nbsp;Wang, P.&nbsp;Kapanipathi, R.&nbsp;Musa, M.&nbsp;Yu, K.&nbsp;Talamadupula, I.&nbsp;Abdelaziz, M.&nbsp;Chang, A.&nbsp;Fokoue, B.&nbsp;Makni, N.&nbsp;Mattei, and M.&nbsp;Witbrock, “Improving natural language inference using external knowledge in the science questions domain,” in <em id=\"bib.bib244.1.1\">AAAI</em>, 2019, pp. 7208–7215.\n\n</span>\n</li>\n<li id=\"bib.bib245\">\n<span>[245]</span>\n<span>\nY.&nbsp;Sun, Q.&nbsp;Shi, L.&nbsp;Qi, and Y.&nbsp;Zhang, “JointLK: Joint reasoning with language models and knowledge graphs for commonsense question answering,” in <em id=\"bib.bib245.1.1\">NAACL</em>, 2022, pp. 5049–5060.\n\n</span>\n</li>\n<li id=\"bib.bib246\">\n<span>[246]</span>\n<span>\nX.&nbsp;Liu, H.&nbsp;Yu, H.&nbsp;Zhang, Y.&nbsp;Xu, X.&nbsp;Lei, H.&nbsp;Lai, Y.&nbsp;Gu, H.&nbsp;Ding, K.&nbsp;Men, K.&nbsp;Yang <em id=\"bib.bib246.1.1\">et&nbsp;al.</em>, “Agentbench: Evaluating llms as agents,” <em id=\"bib.bib246.2.2\">arXiv preprint arXiv:2308.03688</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib247\">\n<span>[247]</span>\n<span>\nY.&nbsp;Wang, N.&nbsp;Lipka, R.&nbsp;A. Rossi, A.&nbsp;Siu, R.&nbsp;Zhang, and T.&nbsp;Derr, “Knowledge graph prompting for multi-document question answering,” <em id=\"bib.bib247.1.1\">arXiv preprint arXiv:2308.11730</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib248\">\n<span>[248]</span>\n<span>\nA.&nbsp;Zeng, M.&nbsp;Liu, R.&nbsp;Lu, B.&nbsp;Wang, X.&nbsp;Liu, Y.&nbsp;Dong, and J.&nbsp;Tang, “Agenttuning: Enabling generalized agent abilities for llms,” 2023.\n\n</span>\n</li>\n<li id=\"bib.bib249\">\n<span>[249]</span>\n<span>\nW.&nbsp;Kryściński, B.&nbsp;McCann, C.&nbsp;Xiong, and R.&nbsp;Socher, “Evaluating the factual consistency of abstractive text summarization,” <em id=\"bib.bib249.1.1\">arXiv preprint arXiv:1910.12840</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib250\">\n<span>[250]</span>\n<span>\nZ.&nbsp;Ji, Z.&nbsp;Liu, N.&nbsp;Lee, T.&nbsp;Yu, B.&nbsp;Wilie, M.&nbsp;Zeng, and P.&nbsp;Fung, “Rho (<math alttext=\"\\backslash\\rho\" display=\"inline\" id=\"bib.bib250.1.m1.1\"><semantics id=\"bib.bib250.1.m1.1a\"><mrow id=\"bib.bib250.1.m1.1.1\" xref=\"bib.bib250.1.m1.1.1.cmml\"><mi id=\"bib.bib250.1.m1.1.1.2\" xref=\"bib.bib250.1.m1.1.1.2.cmml\"></mi><mo id=\"bib.bib250.1.m1.1.1.1\" lspace=\"0.222em\" rspace=\"0.222em\" xref=\"bib.bib250.1.m1.1.1.1.cmml\">\\</mo><mi id=\"bib.bib250.1.m1.1.1.3\" xref=\"bib.bib250.1.m1.1.1.3.cmml\">ρ</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"bib.bib250.1.m1.1b\"><apply id=\"bib.bib250.1.m1.1.1.cmml\" xref=\"bib.bib250.1.m1.1.1\"><ci id=\"bib.bib250.1.m1.1.1.1.cmml\" xref=\"bib.bib250.1.m1.1.1.1\">\\</ci><csymbol cd=\"latexml\" id=\"bib.bib250.1.m1.1.1.2.cmml\" xref=\"bib.bib250.1.m1.1.1.2\">absent</csymbol><ci id=\"bib.bib250.1.m1.1.1.3.cmml\" xref=\"bib.bib250.1.m1.1.1.3\">𝜌</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"bib.bib250.1.m1.1c\">\\backslash\\rho</annotation><annotation encoding=\"application/x-llamapun\" id=\"bib.bib250.1.m1.1d\">\\ italic_ρ</annotation></semantics></math>): Reducing hallucination in open-domain dialogues with knowledge grounding,” <em id=\"bib.bib250.2.1\">arXiv preprint arXiv:2212.01588</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib251\">\n<span>[251]</span>\n<span>\nS.&nbsp;Feng, V.&nbsp;Balachandran, Y.&nbsp;Bai, and Y.&nbsp;Tsvetkov, “Factkb: Generalizable factuality evaluation using language models enhanced with factual knowledge,” <em id=\"bib.bib251.1.1\">arXiv preprint arXiv:2305.08281</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib252\">\n<span>[252]</span>\n<span>\nY.&nbsp;Yao, P.&nbsp;Wang, B.&nbsp;Tian, S.&nbsp;Cheng, Z.&nbsp;Li, S.&nbsp;Deng, H.&nbsp;Chen, and N.&nbsp;Zhang, “Editing large language models: Problems, methods, and opportunities,” <em id=\"bib.bib252.1.1\">arXiv preprint arXiv:2305.13172</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib253\">\n<span>[253]</span>\n<span>\nZ.&nbsp;Li, N.&nbsp;Zhang, Y.&nbsp;Yao, M.&nbsp;Wang, X.&nbsp;Chen, and H.&nbsp;Chen, “Unveiling the pitfalls of knowledge editing for large language models,” <em id=\"bib.bib253.1.1\">arXiv preprint arXiv:2310.02129</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib254\">\n<span>[254]</span>\n<span>\nR.&nbsp;Cohen, E.&nbsp;Biran, O.&nbsp;Yoran, A.&nbsp;Globerson, and M.&nbsp;Geva, “Evaluating the ripple effects of knowledge editing in language models,” <em id=\"bib.bib254.1.1\">arXiv preprint arXiv:2307.12976</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib255\">\n<span>[255]</span>\n<span>\nS.&nbsp;Diao, Z.&nbsp;Huang, R.&nbsp;Xu, X.&nbsp;Li, Y.&nbsp;Lin, X.&nbsp;Zhou, and T.&nbsp;Zhang, “Black-box prompt learning for pre-trained language models,” <em id=\"bib.bib255.1.1\">arXiv preprint arXiv:2201.08531</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib256\">\n<span>[256]</span>\n<span>\nT.&nbsp;Sun, Y.&nbsp;Shao, H.&nbsp;Qian, X.&nbsp;Huang, and X.&nbsp;Qiu, “Black-box tuning for language-model-as-a-service,” in <em id=\"bib.bib256.1.1\">International Conference on Machine Learning</em>.&nbsp;&nbsp;&nbsp;PMLR, 2022, pp. 20 841–20 855.\n\n</span>\n</li>\n<li id=\"bib.bib257\">\n<span>[257]</span>\n<span>\nX.&nbsp;Chen, A.&nbsp;Shrivastava, and A.&nbsp;Gupta, “NEIL: extracting visual knowledge from web data,” in <em id=\"bib.bib257.1.1\">IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013</em>, 2013, pp. 1409–1416.\n\n</span>\n</li>\n<li id=\"bib.bib258\">\n<span>[258]</span>\n<span>\nM.&nbsp;Warren and P.&nbsp;J. Hayes, “Bounding ambiguity: Experiences with an image annotation system,” in <em id=\"bib.bib258.1.1\">Proceedings of the 1st Workshop on Subjectivity, Ambiguity and Disagreement in Crowdsourcing</em>, ser. CEUR Workshop Proceedings, vol. 2276, 2018, pp. 41–54.\n\n</span>\n</li>\n<li id=\"bib.bib259\">\n<span>[259]</span>\n<span>\nZ.&nbsp;Chen, Y.&nbsp;Huang, J.&nbsp;Chen, Y.&nbsp;Geng, Y.&nbsp;Fang, J.&nbsp;Z. Pan, N.&nbsp;Zhang, and W.&nbsp;Zhang, “Lako: Knowledge-driven visual estion answering via late knowledge-to-text injection,” 2022.\n\n</span>\n</li>\n<li id=\"bib.bib260\">\n<span>[260]</span>\n<span>\nR.&nbsp;Girdhar, A.&nbsp;El-Nouby, Z.&nbsp;Liu, M.&nbsp;Singh, K.&nbsp;V. Alwala, A.&nbsp;Joulin, and I.&nbsp;Misra, “Imagebind: One embedding space to bind them all,” in <em id=\"bib.bib260.1.1\">ICCV</em>, 2023, pp. 15 180–15 190.\n\n</span>\n</li>\n<li id=\"bib.bib261\">\n<span>[261]</span>\n<span>\nJ.&nbsp;Zhang, Z.&nbsp;Yin, P.&nbsp;Chen, and S.&nbsp;Nichele, “Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review,” <em id=\"bib.bib261.1.1\">Information Fusion</em>, vol.&nbsp;59, pp. 103–126, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib262\">\n<span>[262]</span>\n<span>\nH.&nbsp;Zhang, B.&nbsp;Wu, X.&nbsp;Yuan, S.&nbsp;Pan, H.&nbsp;Tong, and J.&nbsp;Pei, “Trustworthy graph neural networks: Aspects, methods and trends,” <em id=\"bib.bib262.1.1\">arXiv:2205.07424</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib263\">\n<span>[263]</span>\n<span>\nT.&nbsp;Wu, M.&nbsp;Caccia, Z.&nbsp;Li, Y.-F. Li, G.&nbsp;Qi, and G.&nbsp;Haffari, “Pretrained language model in continual learning: A comparative study,” in <em id=\"bib.bib263.1.1\">ICLR</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib264\">\n<span>[264]</span>\n<span>\nX.&nbsp;L. Li, A.&nbsp;Kuncoro, J.&nbsp;Hoffmann, C.&nbsp;de&nbsp;Masson&nbsp;d’Autume, P.&nbsp;Blunsom, and A.&nbsp;Nematzadeh, “A systematic investigation of commonsense knowledge in large language models,” in <em id=\"bib.bib264.1.1\">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, 2022, pp. 11 838–11 855.\n\n</span>\n</li>\n<li id=\"bib.bib265\">\n<span>[265]</span>\n<span>\nY.&nbsp;Zheng, H.&nbsp;Y. Koh, J.&nbsp;Ju, A.&nbsp;T. Nguyen, L.&nbsp;T. May, G.&nbsp;I. Webb, and S.&nbsp;Pan, “Large language models for scientific synthesis, inference and explanation,” <em id=\"bib.bib265.1.1\">arXiv preprint arXiv:2310.07984</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib266\">\n<span>[266]</span>\n<span>\nB.&nbsp;Min, H.&nbsp;Ross, E.&nbsp;Sulem, A.&nbsp;P.&nbsp;B. Veyseh, T.&nbsp;H. Nguyen, O.&nbsp;Sainz, E.&nbsp;Agirre, I.&nbsp;Heintz, and D.&nbsp;Roth, “Recent advances in natural language processing via large pre-trained language models: A survey,” <em id=\"bib.bib266.1.1\">ACM Computing Surveys</em>, vol.&nbsp;56, no.&nbsp;2, pp. 1–40, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib267\">\n<span>[267]</span>\n<span>\nJ.&nbsp;Wei, M.&nbsp;Bosma, V.&nbsp;Zhao, K.&nbsp;Guu, A.&nbsp;W. Yu, B.&nbsp;Lester, N.&nbsp;Du, A.&nbsp;M. Dai, and Q.&nbsp;V. Le, “Finetuned language models are zero-shot learners,” in <em id=\"bib.bib267.1.1\">International Conference on Learning Representations</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib268\">\n<span>[268]</span>\n<span>\nY.&nbsp;Zhang, Y.&nbsp;Li, L.&nbsp;Cui, D.&nbsp;Cai, L.&nbsp;Liu, T.&nbsp;Fu, X.&nbsp;Huang, E.&nbsp;Zhao, Y.&nbsp;Zhang, Y.&nbsp;Chen, L.&nbsp;Wang, A.&nbsp;T. Luu, W.&nbsp;Bi, F.&nbsp;Shi, and S.&nbsp;Shi, “Siren’s song in the ai ocean: A survey on hallucination in large language models,” <em id=\"bib.bib268.1.1\">arXiv preprint arXiv:2309.01219</em>, 2023.\n\n</span>\n</li>\n</ul>\n</section>\n<section id=\"A1\">\n<h2>\n<span>Appendix A </span>Pros and Cons for LLMs and KGs</h2>\n<p id=\"A1.p1.1\"><span id=\"A1.p1.1.1\">In this section, we introduce the pros and cons of LLMs and KGs in detail. We summarize the pros and cons of LLMs and KGs in Fig. <a href=\"https://arxiv.org/html/2306.08302v3#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ Unifying Large Language Models and Knowledge Graphs: A Roadmap\"><span>1</span></a>, respectively.</span></p>\n<div id=\"A1.p2\">\n<p id=\"A1.p2.1\"><span id=\"A1.p2.1.1\">LLM pros.</span><span id=\"A1.p2.1.2\"></span></p>\n<ul id=\"A1.I1\">\n<li id=\"A1.I1.i1\">\n<span>•</span>\n<p id=\"A1.I1.i1.p1.1\"><em id=\"A1.I1.i1.p1.1.1\">General Knowledge</em><span id=\"A1.I1.i1.p1.1.2\"> </span><cite><span id=\"A1.I1.i1.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib11\" title=\"\">11</a><span id=\"A1.I1.i1.p1.1.4.2\">]</span></cite><span id=\"A1.I1.i1.p1.1.5\">: LLMs pre-trained on large-scale corpora, which contain a large amount of general knowledge, such as commonsense knowledge </span><cite><span id=\"A1.I1.i1.p1.1.6.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib264\" title=\"\">264</a><span id=\"A1.I1.i1.p1.1.7.2\">]</span></cite><span id=\"A1.I1.i1.p1.1.8\"> and factual knowledge </span><cite><span id=\"A1.I1.i1.p1.1.9.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a><span id=\"A1.I1.i1.p1.1.10.2\">]</span></cite><span id=\"A1.I1.i1.p1.1.11\">. Such knowledge can be distilled from LLMs and used for downstream tasks </span><cite><span id=\"A1.I1.i1.p1.1.12.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib265\" title=\"\">265</a><span id=\"A1.I1.i1.p1.1.13.2\">]</span></cite><span id=\"A1.I1.i1.p1.1.14\">.</span></p>\n</li>\n<li id=\"A1.I1.i2\">\n<span>•</span>\n<p id=\"A1.I1.i2.p1.1\"><em id=\"A1.I1.i2.p1.1.1\">Language Processing</em><span id=\"A1.I1.i2.p1.1.2\"> </span><cite><span id=\"A1.I1.i2.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib12\" title=\"\">12</a><span id=\"A1.I1.i2.p1.1.4.2\">]</span></cite><span id=\"A1.I1.i2.p1.1.5\">: LLMs have shown great performance in understanding natural language </span><cite><span id=\"A1.I1.i2.p1.1.6.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib266\" title=\"\">266</a><span id=\"A1.I1.i2.p1.1.7.2\">]</span></cite><span id=\"A1.I1.i2.p1.1.8\">. Therefore, LLMs can be used in many natural language processing tasks, such as question answering </span><cite><span id=\"A1.I1.i2.p1.1.9.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib4\" title=\"\">4</a><span id=\"A1.I1.i2.p1.1.10.2\">]</span></cite><span id=\"A1.I1.i2.p1.1.11\">, machine translation </span><cite><span id=\"A1.I1.i2.p1.1.12.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib5\" title=\"\">5</a><span id=\"A1.I1.i2.p1.1.13.2\">]</span></cite><span id=\"A1.I1.i2.p1.1.14\">, and text generation </span><cite><span id=\"A1.I1.i2.p1.1.15.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib6\" title=\"\">6</a><span id=\"A1.I1.i2.p1.1.16.2\">]</span></cite><span id=\"A1.I1.i2.p1.1.17\">.</span></p>\n</li>\n<li id=\"A1.I1.i3\">\n<span>•</span>\n<p id=\"A1.I1.i3.p1.1\"><em id=\"A1.I1.i3.p1.1.1\">Generalizability</em><span id=\"A1.I1.i3.p1.1.2\"> </span><cite><span id=\"A1.I1.i3.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib13\" title=\"\">13</a><span id=\"A1.I1.i3.p1.1.4.2\">]</span></cite><span id=\"A1.I1.i3.p1.1.5\">: LLMs enable great generalizability, which can be applied to various downstream tasks </span><cite><span id=\"A1.I1.i3.p1.1.6.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib267\" title=\"\">267</a><span id=\"A1.I1.i3.p1.1.7.2\">]</span></cite><span id=\"A1.I1.i3.p1.1.8\">. By providing few-shot examples </span><cite><span id=\"A1.I1.i3.p1.1.9.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib59\" title=\"\">59</a><span id=\"A1.I1.i3.p1.1.10.2\">]</span></cite><span id=\"A1.I1.i3.p1.1.11\"> or finetuning on multi-task data </span><cite><span id=\"A1.I1.i3.p1.1.12.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib3\" title=\"\">3</a><span id=\"A1.I1.i3.p1.1.13.2\">]</span></cite><span id=\"A1.I1.i3.p1.1.14\">, LLMs achieve great performance on many tasks.</span></p>\n</li>\n</ul>\n</div>\n<div id=\"A1.p3\">\n<p id=\"A1.p3.1\"><span id=\"A1.p3.1.1\">LLM cons.</span><span id=\"A1.p3.1.2\"></span></p>\n<ul id=\"A1.I2\">\n<li id=\"A1.I2.i1\">\n<span>•</span>\n<p id=\"A1.I2.i1.p1.1\"><em id=\"A1.I2.i1.p1.1.1\">Implicit Knowledge</em><span id=\"A1.I2.i1.p1.1.2\"> </span><cite><span id=\"A1.I2.i1.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib14\" title=\"\">14</a><span id=\"A1.I2.i1.p1.1.4.2\">]</span></cite><span id=\"A1.I2.i1.p1.1.5\">: LLMs represent knowledge implicitly in their parameters. It is difficult to interpret or validate the knowledge obtained by LLMs.</span></p>\n</li>\n<li id=\"A1.I2.i2\">\n<span>•</span>\n<p id=\"A1.I2.i2.p1.1\"><em id=\"A1.I2.i2.p1.1.1\">Hallucination</em><span id=\"A1.I2.i2.p1.1.2\"> </span><cite><span id=\"A1.I2.i2.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib15\" title=\"\">15</a><span id=\"A1.I2.i2.p1.1.4.2\">]</span></cite><span id=\"A1.I2.i2.p1.1.5\">: LLMs often experience hallucinations by generating content that while seemingly plausible but are factually incorrect </span><cite><span id=\"A1.I2.i2.p1.1.6.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib268\" title=\"\">268</a><span id=\"A1.I2.i2.p1.1.7.2\">]</span></cite><span id=\"A1.I2.i2.p1.1.8\">. This problem greatly reduces the trustworthiness of LLMs in real-world scenarios.</span></p>\n</li>\n<li id=\"A1.I2.i3\">\n<span>•</span>\n<p id=\"A1.I2.i3.p1.1\"><em id=\"A1.I2.i3.p1.1.1\">Indecisiveness</em><span id=\"A1.I2.i3.p1.1.2\"> </span><cite><span id=\"A1.I2.i3.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib16\" title=\"\">16</a><span id=\"A1.I2.i3.p1.1.4.2\">]</span></cite><span id=\"A1.I2.i3.p1.1.5\">: LLMs perform reasoning by generating from a probability model, which is an indecisive process. The generated results are sampled from the probability distribution, which is difficult to control.</span></p>\n</li>\n<li id=\"A1.I2.i4\">\n<span>•</span>\n<p id=\"A1.I2.i4.p1.1\"><em id=\"A1.I2.i4.p1.1.1\">Black-box</em><span id=\"A1.I2.i4.p1.1.2\"> </span><cite><span id=\"A1.I2.i4.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib17\" title=\"\">17</a><span id=\"A1.I2.i4.p1.1.4.2\">]</span></cite><span id=\"A1.I2.i4.p1.1.5\">: LLMs are criticized for their lack of interpretability. It is unclear to know the specific patterns and functions LLMs use to arrive at predictions or decisions.</span></p>\n</li>\n<li id=\"A1.I2.i5\">\n<span>•</span>\n<p id=\"A1.I2.i5.p1.1\"><em id=\"A1.I2.i5.p1.1.1\">Lacking Domain-specific/New Knowledge</em><span id=\"A1.I2.i5.p1.1.2\"> </span><cite><span id=\"A1.I2.i5.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib18\" title=\"\">18</a><span id=\"A1.I2.i5.p1.1.4.2\">]</span></cite><span id=\"A1.I2.i5.p1.1.5\">: LLMs trained on general corpus might not be able to generalize well to specific domains or new knowledge due to the lack of domain-specific knowledge or new training data.</span></p>\n</li>\n</ul>\n</div>\n<div id=\"A1.p4\">\n<p id=\"A1.p4.1\"><span id=\"A1.p4.1.1\">KG pros.</span><span id=\"A1.p4.1.2\"></span></p>\n<ul id=\"A1.I3\">\n<li id=\"A1.I3.i1\">\n<span>•</span>\n<p id=\"A1.I3.i1.p1.1\"><em id=\"A1.I3.i1.p1.1.1\">Structural Knowledge</em><span id=\"A1.I3.i1.p1.1.2\"> </span><cite><span id=\"A1.I3.i1.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib19\" title=\"\">19</a><span id=\"A1.I3.i1.p1.1.4.2\">]</span></cite><span id=\"A1.I3.i1.p1.1.5\">: KGs store facts in a structural format (i.e., triples), which can be understandable by both humans and machines.</span></p>\n</li>\n<li id=\"A1.I3.i2\">\n<span>•</span>\n<p id=\"A1.I3.i2.p1.1\"><em id=\"A1.I3.i2.p1.1.1\">Accuracy</em><span id=\"A1.I3.i2.p1.1.2\"> </span><cite><span id=\"A1.I3.i2.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib20\" title=\"\">20</a><span id=\"A1.I3.i2.p1.1.4.2\">]</span></cite><span id=\"A1.I3.i2.p1.1.5\">: Facts in KGs are usually manually curated or validated by experts, which are more accurate and dependable than those in LLMs.</span></p>\n</li>\n<li id=\"A1.I3.i3\">\n<span>•</span>\n<p id=\"A1.I3.i3.p1.1\"><em id=\"A1.I3.i3.p1.1.1\">Decisiveness</em><span id=\"A1.I3.i3.p1.1.2\"> </span><cite><span id=\"A1.I3.i3.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib21\" title=\"\">21</a><span id=\"A1.I3.i3.p1.1.4.2\">]</span></cite><span id=\"A1.I3.i3.p1.1.5\">: The factual knowledge in KGs is stored in a decisive manner. The reasoning algorithm in KGs is also deterministic, which can provide decisive results.</span></p>\n</li>\n<li id=\"A1.I3.i4\">\n<span>•</span>\n<p id=\"A1.I3.i4.p1.1\"><em id=\"A1.I3.i4.p1.1.1\">Interpretability</em><span id=\"A1.I3.i4.p1.1.2\"> </span><cite><span id=\"A1.I3.i4.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib22\" title=\"\">22</a><span id=\"A1.I3.i4.p1.1.4.2\">]</span></cite><span id=\"A1.I3.i4.p1.1.5\">: KGs are renowned for their symbolic reasoning ability, which provides an interpretable reasoning process that can be understood by humans.</span></p>\n</li>\n<li id=\"A1.I3.i5\">\n<span>•</span>\n<p id=\"A1.I3.i5.p1.1\"><em id=\"A1.I3.i5.p1.1.1\">Domain-specific Knowledge</em><span id=\"A1.I3.i5.p1.1.2\"> </span><cite><span id=\"A1.I3.i5.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib23\" title=\"\">23</a><span id=\"A1.I3.i5.p1.1.4.2\">]</span></cite><span id=\"A1.I3.i5.p1.1.5\">: Many domains can construct their KGs by experts to provide precise and dependable domain-specific knowledge.</span></p>\n</li>\n<li id=\"A1.I3.i6\">\n<span>•</span>\n<p id=\"A1.I3.i6.p1.1\"><em id=\"A1.I3.i6.p1.1.1\">Evolving Knowledge</em><span id=\"A1.I3.i6.p1.1.2\"> </span><cite><span id=\"A1.I3.i6.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib24\" title=\"\">24</a><span id=\"A1.I3.i6.p1.1.4.2\">]</span></cite><span id=\"A1.I3.i6.p1.1.5\">: The facts in KGs are continuously evolving. The KGs can be updated with new facts by inserting new triples and deleting outdated ones.</span></p>\n</li>\n</ul>\n<p id=\"A1.p4.2\"><span id=\"A1.p4.2.1\">KG cons.</span><span id=\"A1.p4.2.2\"></span></p>\n<ul id=\"A1.I4\">\n<li id=\"A1.I4.i1\">\n<span>•</span>\n<p id=\"A1.I4.i1.p1.1\"><em id=\"A1.I4.i1.p1.1.1\">Incompleteness</em><span id=\"A1.I4.i1.p1.1.2\"> </span><cite><span id=\"A1.I4.i1.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib25\" title=\"\">25</a><span id=\"A1.I4.i1.p1.1.4.2\">]</span></cite><span id=\"A1.I4.i1.p1.1.5\">: KGs are hard to construct and often incomplete, which limits the ability of KGs to provide comprehensive knowledge.</span></p>\n</li>\n<li id=\"A1.I4.i2\">\n<span>•</span>\n<p id=\"A1.I4.i2.p1.1\"><em id=\"A1.I4.i2.p1.1.1\">Lacking Language Understanding</em><span id=\"A1.I4.i2.p1.1.2\"> </span><cite><span id=\"A1.I4.i2.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib33\" title=\"\">33</a><span id=\"A1.I4.i2.p1.1.4.2\">]</span></cite><span id=\"A1.I4.i2.p1.1.5\">: Most studies on KGs model the structure of knowledge, but ignore the textual information in KGs. The textual information in KGs is often ignored in KG-related tasks, such as KG completion </span><cite><span id=\"A1.I4.i2.p1.1.6.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib26\" title=\"\">26</a><span id=\"A1.I4.i2.p1.1.7.2\">]</span></cite><span id=\"A1.I4.i2.p1.1.8\"> and KGQA </span><cite><span id=\"A1.I4.i2.p1.1.9.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib43\" title=\"\">43</a><span id=\"A1.I4.i2.p1.1.10.2\">]</span></cite><span id=\"A1.I4.i2.p1.1.11\">.</span></p>\n</li>\n<li id=\"A1.I4.i3\">\n<span>•</span>\n<p id=\"A1.I4.i3.p1.1\"><em id=\"A1.I4.i3.p1.1.1\">Unseen Facts</em><span id=\"A1.I4.i3.p1.1.2\"> </span><cite><span id=\"A1.I4.i3.p1.1.3.1\">[</span><a href=\"https://arxiv.org/html/2306.08302v3#bib.bib27\" title=\"\">27</a><span id=\"A1.I4.i3.p1.1.4.2\">]</span></cite><span id=\"A1.I4.i3.p1.1.5\">: KGs are dynamically changing, which makes it difficult to model unseen entities and represent new facts.</span></p>\n</li>\n</ul>\n</div>\n</section>\n</article></div>","textContent":"\n\n\nShirui Pan, Senior Member, IEEE, Linhao Luo,\n Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu, Fellow, IEEE\n\nShirui Pan is with the School of Information and Communication Technology and Institute for Integrated and Intelligent Systems (IIIS), Griffith University, Queensland, Australia.\nEmail: s.pan@griffith.edu.au;\nLinhao Luo and Yufei Wang are with the Department of Data Science and AI, Monash University, Melbourne, Australia. E-mail: linhao.luo@monash.edu, garyyufei@gmail.com.\nChen Chen is with the Nanyang Technological University, Singapore. E-mail: s190009@ntu.edu.sg.\nJiapu Wang is with the Faculty of Information Technology, Beijing University of Technology, Beijing, China. E-mail: jpwang@emails.bjut.edu.cn.\nXindong Wu is with the Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Technology, Hefei, China, and also with the Research Center for Knowledge Engineering, Zhejiang Lab, Hangzhou, China.\nEmail: xwu@hfut.edu.cn.\nShirui Pan and Linhao Luo contributed equally to this work. Corresponding Author: Xindong Wu.\n\n\n\nAbstract\nLarge language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages.\nIn this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.\n\n\n\nIndex Terms:  Natural Language Processing, Large Language Models, Generative Pre-Training, Knowledge Graphs, Roadmap, Bidirectional Reasoning.\n\n\n††publicationid: pubid: 0000–0000/00$00.00 © 2023 IEEE\n\n\n1 Introduction\n\nLarge language models (LLMs)111LLMs are also known as pre-trained language models (PLMs). (e.g., BERT [1], RoBERTA [2], and T5 [3]), pre-trained on the large-scale corpus, have shown great performance in various natural language processing (NLP) tasks, such as question answering [4], machine translation [5], and text generation [6]. Recently, the dramatically increasing model size further enables the LLMs with the emergent ability [7], paving the road for applying LLMs as Artificial General Intelligence (AGI). Advanced LLMs like ChatGPT222https://openai.com/blog/chatgpt and PaLM2333https://ai.google/discover/palm2, with billions of parameters, exhibit great potential in many complex practical tasks, such as education [8], code generation [9] and recommendation [10].\n\nFigure 1: Summarization of the pros and cons for LLMs and KGs. LLM pros: General Knowledge [11], Language Processing [12], Generalizability [13]; LLM cons: Implicit Knowledge [14], Hallucination [15], Indecisiveness [16], Black-box [17], Lacking Domain-specific/New Knowledge [18]. KG pros: Structural Knowledge [19], Accuracy [20], Decisiveness [21], Interpretability [22], Domain-specific Knowledge [23], Evolving Knowledge [24]; KG cons: Incompleteness [25], Lacking Language Understanding [26], Unseen Facts [27]. Pros. and Cons. are selected based on their representativeness. Detailed discussion can be found in Appendix A.\n\nDespite their success in many applications, LLMs have been criticized for their lack of factual knowledge. Specifically, LLMs memorize facts and knowledge contained in the training corpus [14]. However, further studies reveal that LLMs are not able to recall facts and often experience hallucinations by generating statements that are factually incorrect [28, 15]. For example, LLMs might say “Einstein discovered gravity in 1687” when asked, “When did Einstein discover gravity?”, which contradicts the fact that Isaac Newton formulated the gravitational theory. This issue severely impairs the trustworthiness of LLMs.\nAs black-box models, LLMs are also criticized for their lack of interpretability. LLMs represent knowledge implicitly in their parameters. It is difficult to interpret or validate the knowledge obtained by LLMs. Moreover, LLMs perform reasoning by a probability model, which is an indecisive process [16]. The specific patterns and functions LLMs used to arrive at predictions or decisions are not directly accessible or explainable to humans [17]. Even though some LLMs are equipped to explain their predictions by applying chain-of-thought [29], their reasoning explanations also suffer from the hallucination issue [30]. This severely impairs the application of LLMs in high-stakes scenarios, such as medical diagnosis and legal judgment. For instance, in a medical diagnosis scenario, LLMs may incorrectly diagnose a disease and provide explanations that contradict medical commonsense. This raises another issue that LLMs trained on general corpus might not be able to generalize well to specific domains or new knowledge due to the lack of domain-specific knowledge or new training data [18].\nTo address the above issues, a potential solution is to incorporate knowledge graphs (KGs) into LLMs. Knowledge graphs (KGs), storing enormous facts in the way of triples, i.e., (h⁢e⁢a⁢d⁢e⁢n⁢t⁢i⁢t⁢y,r⁢e⁢l⁢a⁢t⁢i⁢o⁢n,t⁢a⁢i⁢l⁢e⁢n⁢t⁢i⁢t⁢y)ℎ𝑒𝑎𝑑𝑒𝑛𝑡𝑖𝑡𝑦𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛𝑡𝑎𝑖𝑙𝑒𝑛𝑡𝑖𝑡𝑦(head~{}entity,relation,tail~{}entity)( italic_h italic_e italic_a italic_d italic_e italic_n italic_t italic_i italic_t italic_y , italic_r italic_e italic_l italic_a italic_t italic_i italic_o italic_n , italic_t italic_a italic_i italic_l italic_e italic_n italic_t italic_i italic_t italic_y ), are a structured and decisive manner of knowledge representation (e.g., Wikidata [20], YAGO [31], and NELL [32]). KGs are crucial for various applications as they offer accurate explicit knowledge [19].\nBesides, they are renowned for their symbolic reasoning ability [22], which generates interpretable results.\nKGs can also actively evolve with new knowledge continuously added in [24]. Additionally, experts can construct domain-specific KGs to provide precise and dependable domain-specific knowledge [23].\nNevertheless, KGs are difficult to construct [25], and current approaches in KGs [33, 34, 27] are inadequate in handling the incomplete and dynamically changing nature of real-world KGs. These approaches fail to effectively model unseen entities and represent new facts. In addition, they often ignore the abundant textual information in KGs. Moreover, existing methods in KGs are often customized for specific KGs or tasks, which are not generalizable enough. Therefore, it is also necessary to utilize LLMs to address the challenges faced in KGs. We summarize the pros and cons of LLMs and KGs in Fig. 1, respectively.\nRecently, the possibility of unifying LLMs with KGs has attracted increasing attention from researchers and practitioners. LLMs and KGs are inherently interconnected and can mutually enhance each other. In KG-enhanced LLMs, KGs can not only be incorporated into the pre-training and inference stages of LLMs to provide external knowledge [35, 36, 37], but also used for analyzing LLMs and providing interpretability [14, 38, 39]. In LLM-augmented KGs, LLMs have been used in various KG-related tasks, e.g., KG embedding [40], KG completion [26], KG construction [41], KG-to-text generation [42], and KGQA [43], to improve the performance and facilitate the application of KGs. In Synergized LLM + KG, researchers marries the merits of LLMs and KGs to mutually enhance performance in knowledge representation [44] and reasoning [45, 46].\nAlthough there are some surveys on knowledge-enhanced LLMs [47, 48, 49], which mainly focus on using KGs as an external knowledge to enhance LLMs, they ignore other possibilities of integrating KGs for LLMs and the potential role of LLMs in KG applications.\n\n\nIn this article, we present a forward-looking roadmap for unifying both LLMs and KGs, to leverage their respective strengths and overcome the limitations of each approach, for various downstream tasks. We propose detailed categorization, conduct comprehensive reviews, and pinpoint emerging directions in these fast-growing fields.\nOur main contributions are summarized as follows:\n\n\n1.\nRoadmap. We present a forward-looking roadmap for integrating LLMs and KGs. Our roadmap, consisting of three general frameworks to unify LLMs and KGs, namely, KG-enhanced LLMs, LLM-augmented KGs, and Synergized LLMs + KGs, provides guidelines for the unification of these two distinct but complementary technologies.\n\n\n\n2.\nCategorization and review. For each integration framework of our roadmap, we present a detailed categorization and novel taxonomies of research on unifying LLMs and KGs. In each category, we review the research from the perspectives of different integration strategies and tasks, which provides more insights into each framework.\n\n\n3.\nCoverage of emerging advances. We cover the advanced techniques in both LLMs and KGs. We include the discussion of state-of-the-art LLMs like ChatGPT and GPT-4 as well as the novel KGs e.g., multi-modal knowledge graphs.\n\n\n4.\nSummary of challenges and future directions. We highlight the challenges in existing research and present several promising future research directions.\n\n\n\nThe rest of this article is organized as follows. Section 2 first explains the background of LLMs and KGs. Section 3 introduces the roadmap and the overall categorization of this article. Section 4 presents the different KGs-enhanced LLM approaches. Section 5 describes the possible LLM-augmented KG methods. Section 6 shows the approaches of synergizing LLMs and KGs. Section 7 discusses the challenges and future research directions. Finally, Section 8 concludes this paper.\n\n\n\n2 Background\n\nIn this section, we will first briefly introduce a few representative large language models (LLMs) and discuss the prompt engineering that efficiently uses LLMs for varieties of applications. Then, we illustrate the concept of knowledge graphs (KGs) and present different categories of KGs.\n\n\nFigure 2: Representative large language models (LLMs) in recent years. Open-source models are represented by solid squares, while closed source models are represented by hollow squares.\n\n\nFigure 3: An illustration of the Transformer-based LLMs with self-attention mechanism.\n\n\n\n2.1 Large Language models (LLMs)\n\nLarge language models (LLMs) pre-trained on large-scale corpus have shown great potential in various NLP tasks [13]. As shown in Fig. 3, most LLMs derive from the Transformer design [50], which contains the encoder and decoder modules empowered by a self-attention mechanism. Based on the architecture structure, LLMs can be categorized into three groups: 1) encoder-only LLMs, 2) encoder-decoder LLMs, and 3) decoder-only LLMs. As shown in Fig. 2, we summarize several representative LLMs with different model architectures, model sizes, and open-source availabilities.\n\n\n2.1.1 Encoder-only LLMs.\nEncoder-only large language models only use the encoder to encode the sentence and understand the relationships between words. The common training paradigm for these model is to predict the mask words in an input sentence. This method is unsupervised and can be trained on the large-scale corpus. Encoder-only LLMs like BERT [1], ALBERT [51], RoBERTa [2], and ELECTRA [52] require adding an extra prediction head to resolve downstream tasks. These models are most effective for tasks that require understanding the entire sentence, such as text classification [26] and named entity recognition [53].\n\n\n\n2.1.2 Encoder-decoder LLMs.\nEncoder-decoder large language models adopt both the encoder and decoder module. The encoder module is responsible for encoding the input sentence into a hidden-space, and the decoder is used to generate the target output text. The training strategies in encoder-decoder LLMs can be more flexible. For example, T5 [3] is pre-trained by masking and predicting spans of masking words. UL2 [54] unifies several training targets such as different masking spans and masking frequencies. Encoder-decoder LLMs (e.g., T0 [55], ST-MoE [56], and GLM-130B [57]) are able to directly resolve tasks that generate sentences based on some context, such as summariaztion, translation, and question answering [58].\n\n\n\n2.1.3 Decoder-only LLMs.\nDecoder-only large language models only adopt the decoder module to generate target output text. The training paradigm for these models is to predict the next word in the sentence. Large-scale decoder-only LLMs can generally perform downstream tasks from a few examples or simple instructions, without adding prediction heads or finetuning [59]. Many state-of-the-art LLMs (e.g., Chat-GPT [60] and GPT-4444https://openai.com/product/gpt-4) follow the decoder-only architecture. However, since these models are closed-source, it is challenging for academic researchers to conduct further research. Recently, Alpaca555https://github.com/tatsu-lab/stanford_alpaca and Vicuna666https://lmsys.org/blog/2023-03-30-vicuna/ are released as open-source decoder-only LLMs. These models are finetuned based on LLaMA [61] and achieve comparable performance with ChatGPT and GPT-4.\n\n\n\n2.1.4 Prompt Engineering\nPrompt engineering is a novel field that focuses on creating and refining prompts to maximize the effectiveness of large language models (LLMs) across various applications and research areas [62].\nAs shown in Fig. 4, a prompt is a sequence of natural language inputs for LLMs that are specified for the task, such as sentiment classification. A prompt could contain several elements, i.e., 1) Instruction, 2) Context, and 3) Input Text. Instruction is a short sentence that instructs the model to perform a specific task. Context provides the context for the input text or few-shot examples. Input Text is the text that needs to be processed by the model.\nPrompt engineering seeks to improve the capacity of large large language models (e.g., ChatGPT) in diverse complex tasks such as question answering, sentiment classification, and common sense reasoning. Chain-of-thought (CoT) prompt [63] enables complex reasoning capabilities through intermediate reasoning steps.\nPrompt engineering also enables the integration of structural data like knowledge graphs (KGs) into LLMs. Li et al. [64] simply linearizes the KGs and uses templates to convert the KGs into passages.\nMindmap [65] designs a KG prompt to convert graph structure into a mind map that enables LLMs to perform reasoning on it.\nPrompt offers a simple way to utilize the potential of LLMs without finetuning. Proficiency in prompt engineering leads to a better understanding of the strengths and weaknesses of LLMs.\n\nFigure 4: An example of sentiment classification prompt.\n\n\nFigure 5: Examples of different categories’ knowledge graphs, i.e., encyclopedic KGs, commonsense KGs, domain-specific KGs, and multi-modal KGs.\n\n\n\n\n\n2.2 Knowledge Graphs (KGs)\n\nKnowledge graphs (KGs) store structured knowledge as a collection of triples 𝒦⁢𝒢={(h,r,t)⊆ℰ×ℛ×ℰ}𝒦𝒢ℎ𝑟𝑡ℰℛℰ\\mathcal{KG}=\\{(h,r,t)\\subseteq\\mathcal{E}\\times\\mathcal{R}\\times\\mathcal{E}\\}caligraphic_K caligraphic_G = { ( italic_h , italic_r , italic_t ) ⊆ caligraphic_E × caligraphic_R × caligraphic_E }, where ℰℰ\\mathcal{E}caligraphic_E and ℛℛ\\mathcal{R}caligraphic_R respectively denote the set of entities and relations. Existing knowledge graphs (KGs) can be classified into four groups based on the stored information: 1) encyclopedic KGs, 2) commonsense KGs, 3) domain-specific KGs, and 4) multi-modal KGs. We illustrate the examples of KGs of different categories in Fig. 5.\n\n\n2.2.1 Encyclopedic Knowledge Graphs.\nEncyclopedic knowledge graphs are the most ubiquitous KGs, which represent the general knowledge in real-world. Encyclopedic knowledge graphs are often constructed by integrating information from diverse and extensive sources, including human experts, encyclopedias, and databases. Wikidata [20] is one of the most widely used encyclopedic knowledge graphs, which incorporates varieties of knowledge extracted from articles on Wikipedia. Other typical encyclopedic knowledge graphs, like Freebase [66], Dbpedia [67], and YAGO [31] are also derived from Wikipedia. In addition, NELL [32] is a continuously improving encyclopedic knowledge graph, which automatically extracts knowledge from the web, and uses that knowledge to improve its performance over time. There are several encyclopedic knowledge graphs available in languages other than English such as CN-DBpedia [68] and Vikidia [69]. The largest knowledge graph, named Knowledge Occean (KO)777https://ko.zhonghuapu.com/, currently contains 4,8784,3636 entities and\n17,3115,8349 relations in both English and Chinese.\n\n\n\n2.2.2 Commonsense Knowledge Graphs.\nCommonsense knowledge graphs formulate the knowledge about daily concepts, e.g., objects, and events, as well as their relationships [70]. Compared with encyclopedic knowledge graphs, commonsense knowledge graphs often model the tacit knowledge extracted from text such as (Car, UsedFor, Drive). ConceptNet [71] contains a wide range of commonsense concepts and relations, which can help computers understand the meanings of words people use. ATOMIC [72, 73] and ASER [74] focus on the causal effects between events, which can be used for commonsense reasoning. Some other commonsense knowledge graphs, such as TransOMCS [75] and CausalBanK [76] are automatically constructed to provide commonsense knowledge.\n\n\n\n\n2.2.3 Domain-specific Knowledge Graphs\nDomain-specific knowledge graphs are often constructed to represent knowledge in a specific domain, e.g., medical, biology, and finance [23]. Compared with encyclopedic knowledge graphs, domain-specific knowledge graphs are often smaller in size, but more accurate and reliable. For example, UMLS [77] is a domain-specific knowledge graph in the medical domain, which contains biomedical concepts and their relationships. In addition, there are some domain-specific knowledge graphs in other domains, such as finance [78], geology [79], biology [80], chemistry [81] and genealogy [82].\n\nFigure 6: The general roadmap of unifying KGs and LLMs. (a.) KG-enhanced LLMs. (b.) LLM-augmented KGs. (c.) Synergized LLMs + KGs.\n\n\n\n\n2.2.4 Multi-modal Knowledge Graphs.\nUnlike conventional knowledge graphs that only contain textual information, multi-modal knowledge graphs represent facts in multiple modalities such as images, sounds, and videos [83]. For example, IMGpedia [84], MMKG [85], and Richpedia [86] incorporate both the text and image information into the knowledge graphs. These knowledge graphs can be used for various multi-modal tasks such as image-text matching [87], visual question answering [88], and recommendation [89].\n\nTABLE I: Representative applications of using LLMs and KGs.\n\n\n\n\n\n\n2.3 Applications\n\nLLMs as KGs have been widely applied in various real-world applications. We summarize some representative applications of using LLMs and KGs in Table I. ChatGPT/GPT-4 are LLM-based chatbots that can communicate with humans in a natural dialogue format. To improve knowledge awareness of LLMs, ERNIE 3.0 and Bard incorporate KGs into their chatbot applications. Instead of Chatbot. Firefly develops a photo editing application that allows users to edit photos by using natural language descriptions. Copilot, New Bing, and Shop.ai adopt LLMs to empower their applications in the areas of coding assistant, web search, and recommendation, respectively. Wikidata and KO are two representative knowledge graph applications that are used to provide external knowledge.\nOpenBG [90] is a knowledge graph designed for recommendation. Doctor.ai develops a health care assistant that incorporates LLMs and KGs to provide medical advice.\n\n\n\n\n3 Roadmap & Categorization\n\nIn this section, we first present a road map of explicit frameworks that unify LLMs and KGs. Then, we present the categorization of research on unifying LLMs and KGs.\n\n\n3.1 Roadmap\n\nThe roadmap of unifying KGs and LLMs is illustrated in Fig. 6. In the roadmap, we identify three frameworks for the unification of LLMs and KGs, including KG-enhanced LLMs, LLM-augmented KGs, and Synergized LLMs + KGs. The KG-enhanced LLMs and LLM-augmented KGs are two parallel frameworks that aim to enhance the capabilities of LLMs and KGs, respectively. Building upon these frameworks, Synergized LLMs + KGs is a unified framework that aims to synergize LLMs and KGs to mutually enhance each other.\n\n\n3.1.1 KG-enhanced LLMs\nLLMs are renowned for their ability to learn knowledge from large-scale corpus and achieve state-of-the-art performance in various NLP tasks. However, LLMs are often criticized for their hallucination issues [15], and lacking of interpretability. To address these issues, researchers have proposed to enhance LLMs with knowledge graphs (KGs).\nKGs store enormous knowledge in an explicit and structured way, which can be used to enhance the knowledge awareness of LLMs. Some researchers have proposed to incorporate KGs into LLMs during the pre-training stage, which can help LLMs learn knowledge from KGs [35, 91]. Other researchers have proposed to incorporate KGs into LLMs during the inference stage. By retrieving knowledge from KGs, it can significantly improve the performance of LLMs in accessing domain-specific knowledge [92]. To improve the interpretability of LLMs, researchers also utilize KGs to interpret the facts [14] and the reasoning process of LLMs [38].\n\n\n\n3.1.2 LLM-augmented KGs\nKGs store structure knowledge playing an essential role in many real-word applications [19]. Existing methods in KGs fall short of handling incomplete KGs [33] and processing text corpus to construct KGs [93]. With the generalizability of LLMs, many researchers are trying to harness the power of LLMs for addressing KG-related tasks.\n\nThe most straightforward way to apply LLMs as text encoders for KG-related tasks. Researchers take advantage of LLMs to process the textual corpus in the KGs and then use the representations of the text to enrich KGs representation [94]. Some studies also use LLMs to process the original corpus and extract relations and entities for KG construction [95]. Recent studies try to design a KG prompt that\ncan effectively convert structural KGs into a format that can be comprehended by LLMs. In this way, LLMs can be directly applied to KG-related tasks, e.g., KG completion [96] and KG reasoning [97].\n\nFigure 7: The general framework of the Synergized LLMs + KGs, which contains four layers: 1) Data, 2) Synergized Model, 3) Technique, and 4) Application.\n\n\n\n\n3.1.3 Synergized LLMs + KGs\nThe synergy of LLMs and KGs has attracted increasing attention from researchers these years [40, 42]. LLMs and KGs are two inherently complementary techniques, which should be unified into a general framework to mutually enhance each other.\nTo further explore the unification, we propose a unified framework of the synergized LLMs + KGs in Fig. 7. The unified framework contains four layers: 1) Data, 2) Synergized Model, 3) Technique, and 4) Application. In the Data layer, LLMs and KGs are used to process the textual and structural data, respectively. With the development of multi-modal LLMs [98] and KGs [99], this framework can be extended to process multi-modal data, such as video, audio, and images. In the Synergized Model layer, LLMs and KGs could synergize with each other to improve their capabilities. In Technique layer, related techniques that have been used in LLMs and KGs can be incorporated into this framework to further enhance the performance. In the Application layer, LLMs and KGs can be integrated to address various real-world applications, such as search engines [100], recommender systems [10], and AI assistants [101].\n\nFigure 8: Fine-grained categorization of research on unifying large language models (LLMs) with knowledge graphs (KGs).\n\n\n\n\n\n3.2 Categorization\n\nTo better understand the research on unifying LLMs and KGs, we further provide a fine-grained categorization for each framework in the roadmap. Specifically, we focus on different ways of integrating KGs and LLMs, i.e., KG-enhanced LLMs, KG-augmented LLMs, and Synergized LLMs + KGs. The fine-grained categorization of the research is illustrated in Fig. 8.\n\nKG-enhanced LLMs. Integrating KGs can enhance the performance and interpretability of LLMs in various downstream tasks. We categorize the research on KG-enhanced LLMs into three groups:\n\n\n1.\nKG-enhanced LLM pre-training includes works that apply KGs during the pre-training stage and improve the knowledge expression of LLMs.\n\n\n2.\nKG-enhanced LLM inference includes research that utilizes KGs during the inference stage of LLMs, which enables LLMs to access the latest knowledge without retraining.\n\n\n3.\nKG-enhanced LLM interpretability includes works that use KGs to understand the knowledge learned by LLMs and interpret the reasoning process of LLMs.\n\n\n\n\nLLM-augmented KGs. LLMs can be applied to augment various KG-related tasks. We categorize the research on LLM-augmented KGs into five groups based on the task types:\n\n\n1.\nLLM-augmented KG embedding includes studies that apply LLMs to enrich representations of KGs by encoding the textual descriptions\nof entities and relations.\n\n\n2.\nLLM-augmented KG completion includes papers that utilize LLMs to encode text or generate facts for better KGC performance.\n\n\n3.\nLLM-augmented KG construction includes works that apply LLMs to address the entity discovery, coreference resolution, and relation extraction tasks for KG construction.\n\n\n4.\nLLM-augmented KG-to-text Generation includes research that utilizes LLMs to generate natural language that describes the facts from KGs.\n\n\n5.\nLLM-augmented KG question answering includes studies that apply LLMs to bridge the gap between natural language questions and retrieve answers from KGs.\n\n\n\n\nSynergized LLMs + KGs. The synergy of LLMs and KGs aims to integrate LLMs and KGs into a unified framework to mutually enhance each other. In this categorization, we review the recent attempts of Synergized LLMs + KGs from the perspectives of knowledge representation and reasoning.\nIn the following sections (Sec 4, 5, and 6), we will provide details on these categorizations.\n\n\n\n\n4 KG-enhanced LLMs\n\nLarge language models (LLMs) achieve promising results in many natural language processing tasks. However, LLMs have been criticized for their lack of practical knowledge and tendency to generate factual errors during inference. To address this issue, researchers have proposed integrating knowledge graphs (KGs) to enhance LLMs. In this section, we first introduce the KG-enhanced LLM pre-training, which aims to inject knowledge into LLMs during the pre-training stage. Then, we introduce the KG-enhanced LLM inference, which enables LLMs to consider the latest knowledge while generating sentences. Finally, we introduce the KG-enhanced LLM interpretability, which aims to improve the interpretability of LLMs by using KGs. Table II summarizes the typical methods that integrate KGs for LLMs.\n\nTABLE II: Summary of KG-enhanced LLM methods.\n\n[b]\n\n\n\n\nTask\nMethod\nYear\nKG\nTechnique\n\nKG-enhanced LLM pre-training\nERNIE [35]\n2019\nE\nIntegrating KGs into Training Objective\n\nGLM [102]\n2020\nC\nIntegrating KGs into Training Objective\n\nEbert [103]\n2020\nD\nIntegrating KGs into Training Objective\n\nKEPLER [40]\n2021\nE\nIntegrating KGs into Training Objective\n\nDeterministic LLM [104]\n2022\nE\nIntegrating KGs into Training Objective\n\nKALA [105]\n2022\nD\nIntegrating KGs into Training Objective\n\nWKLM [106]\n2020\nE\nIntegrating KGs into Training Objective\n\nK-BERT [36]\n2020\nE + D\nIntegrating KGs into Language Model Inputs\n\nCoLAKE [107]\n2020\nE\nIntegrating KGs into Language Model Inputs\n\nERNIE3.0 [101]\n2021\nE + D\nIntegrating KGs into Language Model Inputs\n\nDkLLM [108]\n2022\nE\nIntegrating KGs into Language Model Inputs\n\nKP-PLM [109]\n2022\nE\nKGs Instruction-tuning\n\nOntoPrompt [110]\n2022\nE + D\nKGs Instruction-tuning\n\nChatKBQA [111]\n2023\nE\nKGs Instruction-tuning\n\nRoG [112]\n2023\nE\nKGs Instruction-tuning\n\nKG-enhanced LLM inference\nKGLM [113]\n2019\nE\nRetrival-augmented knowledge fusion\n\nREALM [114]\n2020\nE\nRetrival-augmented knowledge fusion\n\nRAG [92]\n2020\nE\nRetrival-augmented knowledge fusion\n\nEMAT [115]\n2022\nE\nRetrival-augmented knowledge fusion\n\nLi et al. [64]\n2023\nC\nKGs Prompting\n\nMindmap [65]\n2023\nE + D\nKGs Prompting\n\nChatRule [116]\n2023\nE + D\nKGs Prompting\n\nCoK [117]\n2023\nE + C + D\nKGs Prompting\n\nKG-enhanced LLM interpretability\nLAMA [14]\n2019\nE\nKGs for LLM probing\n\nLPAQA [118]\n2020\nE\nKGs for LLM probing\n\nAutoprompt [119]\n2020\nE\nKGs for LLM probing\n\nMedLAMA [120]\n2022\nD\nKGs for LLM probing\n\nLLM-facteval [121]\n2023\nE + D\nKGs for LLM probing\n\nKagNet [38]\n2019\nC\nKGs for LLM analysis\n\nInterpret-lm [122]\n2021\nE\nKGs for LLM analysis\n\nknowledge-neurons [39]\n2021\nE\nKGs for LLM analysis\n\nShaobo et al. [123]\n2022\nE\nKGs for LLM analysis\n\n\n\n\n•\nE: Encyclopedic Knowledge Graphs, C: Commonsense Knowledge Graphs, D: Domain-Specific Knowledge Graphs.\n\n\n\n\n\n\n4.1 KG-enhanced LLM Pre-training\n\nExisting large language models mostly rely on unsupervised training on the large-scale corpus. While these models may exhibit impressive performance on downstream tasks, they often lack practical knowledge relevant to the real world. Previous works that integrate KGs into large language models can be categorized into three parts: 1) Integrating KGs into training objective, 2) Integrating KGs into LLM inputs, and 3) KGs Instruction-tuning.\n\n\n4.1.1 Integrating KGs into Training Objective\nThe research efforts in this category focus on designing novel knowledge-aware training objectives. An intuitive idea is to expose more knowledge entities in the pre-training objective. GLM [102] leverages the knowledge graph structure to assign a masking probability. Specifically, entities that can be reached within a certain number of hops are considered to be the most important entities for learning, and they are given a higher masking probability during pre-training. Furthermore, E-BERT [103] further controls the balance between the token-level and entity-level training losses.\nThe training loss values are used as indications of the learning process for token and entity, which dynamically determines their ratio for the next training epochs. SKEP [124] also follows a similar fusion to inject sentiment knowledge during LLMs pre-training. SKEP first determines words with positive and negative sentiment by utilizing PMI along with a predefined set of seed sentiment words. Then, it assigns a higher masking probability to those identified sentiment words in the word masking objective.\nThe other line of work explicitly leverages the connections with knowledge and input text. As shown in Fig. 9, ERNIE [35] proposes a novel word-entity alignment training objective as a pre-training objective. Specifically, ERNIE feeds both sentences and corresponding entities mentioned in the text into LLMs, and then trains the LLMs to predict alignment links between textual tokens and entities in knowledge graphs. Similarly, KALM [91] enhances the input tokens by incorporating entity embeddings and includes an entity prediction pre-training task in addition to the token-only pre-training objective. This approach aims to improve the ability of LLMs to capture knowledge related to entities. Finally, KEPLER [40] directly employs both knowledge graph embedding training objective and Masked token pre-training objective into a shared transformer-based encoder. Deterministic LLM [104] focuses on pre-training language models to capture deterministic factual knowledge. It only masks the span that has a deterministic entity as the question and introduces additional clue contrast learning and clue classification objective. WKLM [106] first replaces entities in the text with other same-type entities and then feeds them into LLMs. The model is further pre-trained to distinguish whether the entities have been replaced or not.\n\nFigure 9: Injecting KG information into LLMs training objective via text-knowledge alignment loss, where hℎhitalic_h denotes the hidden representation generated by LLMs.\n\n\n\n\n4.1.2 Integrating KGs into LLM Inputs\nAs shown in Fig. 10, this kind of research focus on introducing relevant knowledge sub-graph into the inputs of LLMs. Given a knowledge graph triple and the corresponding sentences, ERNIE 3.0 [101] represents the triple as a sequence of tokens and directly concatenates them with the sentences. It further randomly masks either the relation token in the triple or tokens in the sentences\nto better combine knowledge with textual representations. However, such\ndirect knowledge triple concatenation method allows the tokens in the sentence to intensively interact with the tokens in the knowledge sub-graph, which could result in Knowledge Noise [36]. To solve this issue, K-BERT [36] takes the first step to inject the knowledge triple into the sentence via a visible matrix where only the knowledge entities have access to the knowledge triple information, while the tokens in the sentences can only see each other in the self-attention module. To further reduce Knowledge Noise, Colake [107] proposes a unified word-knowledge graph (shown in Fig. 10) where the tokens in the input sentences form a fully connected word graph where tokens aligned with knowledge entities are connected with their neighboring entities.\nThe above methods can indeed inject a large amount of knowledge into LLMs. However, they mostly focus on popular entities and overlook the low-frequent and long-tail ones. DkLLM [108] aims to improve the LLMs representations towards those entities. DkLLM first proposes a novel measurement to determine long-tail entities and then replaces these selected entities in the text with pseudo token embedding as new input to the large language models. Furthermore, Dict-BERT [125] proposes to leverage external dictionaries to solve this issue. Specifically, Dict-BERT improves the representation quality of rare words by appending their definitions from the dictionary at the end of input text and trains the language model to locally align rare word representations in input sentences and dictionary definitions as well as to discriminate whether the input text and definition are correctly mapped.\n\nFigure 10: Injecting KG information into LLMs inputs using graph structure.\n\n\n\n\n4.1.3 KGs Instruction-tuning\nInstead of injecting factual knowledge into LLMs, the KGs Instruction-tuning aims to fine-tune LLMs to better comprehend the structure of KGs and effectively follow user instructions to conduct complex tasks. KGs Instruction-tuning utilizes both facts and the structure of KGs to create instruction-tuning datasets. LLMs finetuned on these datasets can extract both factual and structural knowledge from KGs, enhancing the reasoning ability of LLMs.\nKP-PLM [109] first designs several prompt templates to transfer structural graphs into natural language text. Then, two self-supervised tasks are proposed to finetune LLMs to further leverage the knowledge from these prompts. OntoPrompt [110] proposes an ontology-enhanced prompt-tuning that can place knowledge of entities into the context of LLMs, which are further finetuned on several downstream tasks. ChatKBQA [111] finetunes LLMs on KG structure to generate logical queries, which can be executed on KGs to obtain answers. To better reason on graphs, RoG [112] presents a planning-retrieval-reasoning framework. RoG is finetuned on KG structure to generate relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning and generate interpretable results.\nKGs Instruction-tuning can better leverage the knowledge from KGs for downstream tasks. However, it requires retraining the models, which is time-consuming and requires lots of resources.\n\n\n\n\n4.2 KG-enhanced LLM Inference\n\nThe above methods could effectively fuse knowledge into LLMs. However, real-world knowledge is subject to change and the limitation of these approaches is that they do not permit updates to the incorporated knowledge without retraining the model. As a result, they may not generalize well to the unseen knowledge during inference [126]. Therefore, considerable research has been devoted to keeping the knowledge space and text space separate and injecting the knowledge while inference. These methods mostly focus on the Question Answering (QA) tasks, because QA requires the model to capture both textual semantic meanings and up-to-date real-world knowledge.\n\n\n4.2.1 Retrieval-Augmented Knowledge Fusion\nRetrieval-Augmented Knowledge Fusion is a popular method to inject knowledge into LLMs during inference. The key idea is to retrieve relevant knowledge from a large corpus and then fuse the retrieved knowledge into LLMs. As shown in Fig. 11, RAG [92] proposes to combine non-parametric and parametric modules to handle the external knowledge. Given the input text, RAG first searches for relevant KG in the non-parametric module via MIPS to obtain several documents. RAG then treats these documents as hidden variables z𝑧zitalic_z and feeds them into the output generator, empowered by Seq2Seq LLMs, as additional context information. The research indicates that using different retrieved documents as conditions at different generation steps performs better than only using a single document to guide the whole generation process.\nThe experimental results show that RAG outperforms other parametric-only and non-parametric-only baseline models in open-domain QA. RAG can also generate more specific, diverse, and factual text than other parameter-only baselines. Story-fragments [127] further improves architecture by adding an additional module to determine salient knowledge entities and fuse them into the generator to improve the quality of generated long stories. EMAT [115] further improves the efficiency of such a system by encoding external knowledge into a key-value memory and exploiting the\nfast maximum inner product search for memory querying. REALM [114] proposes a novel knowledge retriever to help the model to retrieve and attend over documents from a large corpus during the pre-training stage and successfully improves the performance of open-domain question answering. KGLM [113] selects the facts from a knowledge graph using the current context to generate factual sentences. With the help of an external knowledge graph, KGLM could describe facts using out-of-domain words or phrases.\n\nFigure 11: Retrieving external knowledge to enhance the LLM generation.\n\n\n\n\n4.2.2 KGs Prompting\nTo better feed the KG structure into the LLM during inference, KGs prompting aims to design a crafted prompt that converts structured KGs into text sequences, which can be fed as context into LLMs. In this way, LLMs can better take advantage of the structure of KGs to perform reasoning. Li et al. [64] adopt the pre-defined template to convert each triple into a short sentence, which can be understood by LLMs for reasoning. Mindmap [65] designs a KG prompt to convert graph structure into a mind map that enables LLMs to perform reasoning by consolidating the facts in KGs and the implicit knowledge from LLMs.\nChatRule [116] samples several relation paths from KGs, which are verbalized and fed into LLMs. Then, LLMs are prompted to generate meaningful logical rules that can be used for reasoning. CoK [117] proposes a chain-of-knowledge prompting that uses a sequence of triples to elicit the reasoning ability of LLMs to reach the final answer.\nKGs prompting presents a simple way to synergize LLMs and KGs. By using the prompt, we can easily harness the power of LLMs to perform reasoning based on KGs without retraining the models. However, the prompt is usually designed manually, which requires lots of human effort.\n\n\n\n\n4.3 Comparison between KG-enhanced LLM Pre-training and Inference\n\nKG-enhanced LLM Pre-training methods commonly enrich large-amount of unlabeled corpus with semantically relevant real-world knowledge. These methods allow the knowledge representations to be aligned with appropriate linguistic context and explicitly train LLMs to leverage those knowledge from scratch. When applying the resulting LLMs to downstream knowledge-intensive tasks, they should achieve optimal performance. In contrast, KG-enhanced LLM inference methods only present the knowledge to LLMs in the inference stage and the underlying LLMs may not be trained to fully leverage these knowledge when conducting downstream tasks, potentially resulting in sub-optimal model performance.\nHowever, real-world knowledge is dynamic and requires frequent updates. Despite being effective, the KG-enhanced LLM Pre-training methods never permit knowledge updates or editing without model re-training. As a result, the KG-enhanced LLM Pre-training methods could generalize poorly to recent or unseen knowledge. KG-enhanced LLM inference methods can easily maintain knowledge updates by changing the inference inputs. These methods help improve LLMs performance on new knowledge and domains.\nIn summary, when to use these methods depends on the application scenarios. If one wishes to apply LLMs to handle time-insensitive knowledge in particular domains (e.g., commonsense and reasoning knowledge), KG-enhanced LLM Pre-training methods should be considered. Otherwise, KG-enhanced LLM inference methods can be used to handle open-domain knowledge with frequent updates.\n\nFigure 12: The general framework of using knowledge graph for language model probing.\n\n\n\n\n4.4 KG-enhanced LLM Interpretability\n\nAlthough LLMs have achieved remarkable success in many NLP tasks, they are still criticized for their lack of interpretability. The large language model (LLM) interpretability refers to the understanding and explanation of the inner workings and decision-making processes of a large language model [17]. This can improve the trustworthiness of LLMs and facilitate their applications in high-stakes scenarios such as medical diagnosis and legal judgment.\nKnowledge graphs (KGs) represent the knowledge structurally and can provide good interpretability for the reasoning results. Therefore, researchers try to utilize KGs to improve the interpretability of LLMs, which can be roughly grouped into two categories: 1) KGs for language model probing, and 2) KGs for language model analysis.\n\n\n4.4.1 KGs for LLM Probing\nThe large language model (LLM) probing aims to understand the knowledge stored in LLMs. LLMs, trained on large-scale corpus, are often known as containing enormous knowledge. However, LLMs store the knowledge in a hidden way, making it hard to figure out the stored knowledge. Moreover, LLMs suffer from the hallucination problem [15], which results in generating statements that contradict facts. This issue significantly affects the reliability of LLMs. Therefore, it is necessary to probe and verify the knowledge stored in LLMs.\nLAMA [14] is the first work to probe the knowledge in LLMs by using KGs. As shown in Fig. 12, LAMA first converts the facts in KGs into cloze statements by a pre-defined prompt template and then uses LLMs to predict the missing entity. The prediction results are used to evaluate the knowledge stored in LLMs. For example, we try to probe whether LLMs know the fact (Obama, profession, president). We first convert the fact triple into a cloze question “Obama’s profession is __\\__.” with the object masked. Then, we test if the LLMs can predict the object “president” correctly.\nHowever, LAMA ignores the fact that the prompts are inappropriate. For example, the prompt “Obama worked as a _” may be more favorable to the prediction of the blank by the language models than “Obama is a _ by profession”. Thus, LPAQA [118] proposes a mining and paraphrasing-based method to automatically generate high-quality and diverse prompts for a more accurate assessment of the knowledge contained in the language model. Moreover, Adolphs et al. [128] attempt to use examples to make the language model understand the query, and experiments obtain substantial improvements for BERT-large on the T-REx data. Unlike using manually defined prompt templates, Autoprompt [119] proposes an automated method, which is based on the gradient-guided search to create prompts. LLM-facteval [121] designs a systematic framework that automatically generates probing questions from KGs. The generated questions are then used to evaluate the factual knowledge stored in LLMs.\nInstead of probing the general knowledge by using the encyclopedic and commonsense knowledge graphs, BioLAMA [129] and MedLAMA [120] probe the medical knowledge in LLMs by using medical knowledge graphs.\nAlex et al. [130] investigate the capacity of LLMs to retain less popular factual knowledge. They select unpopular facts from Wikidata knowledge graphs which have low-frequency clicked entities. These facts are then used for the evaluation, where the results indicate that LLMs encounter difficulties with such knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the tail.\n\nFigure 13: The general framework of using knowledge graph for language model analysis.\n\n\n\n\n4.4.2 KGs for LLM Analysis\nKnowledge graphs (KGs) for pre-train language models (LLMs) analysis aims to answer the following questions such as “how do LLMs generate the results?”, and “how do the function and structure work in LLMs?”.\nTo analyze the inference process of LLMs, as shown in Fig. 13, KagNet [38] and QA-GNN [131] make the results generated by LLMs at each reasoning step grounded by knowledge graphs. In this way, the reasoning process of LLMs can be explained by extracting the graph structure from KGs. Shaobo et al. [123] investigate how LLMs generate the results correctly. They adopt the causal-inspired analysis from facts extracted from KGs. This analysis quantitatively measures the word patterns that LLMs depend on to generate the results. The results show that LLMs generate the missing factual more by the positionally closed words rather than the knowledge-dependent words. Thus, they claim that LLMs are inadequate to memorize factual knowledge because of the inaccurate dependence.\nTo interpret the training of LLMs, Swamy et al. [122] adopt the language model during pre-training to generate knowledge graphs. The knowledge acquired by LLMs during training can be unveiled by the facts in KGs explicitly. To explore how implicit knowledge is stored in parameters of LLMs, Dai et al. [39] propose the concept of knowledge neurons. Specifically, activation of the identified knowledge neurons is highly correlated with knowledge expression. Thus, they explore the knowledge and facts represented by each neuron by suppressing and amplifying knowledge neurons.\n\n\n\n\n\n5 LLM-augmented KGs\n\nKnowledge graphs are famous for representing knowledge in a structural manner. They have been applied in many downstream tasks such as question answering, recommendation, and web search. However, the conventional KGs are often incomplete and existing methods often lack considering textual information. To address these issues, recent research has explored integrating LLMs to augment KGs to consider the textual information and improve the performance in downstream tasks. In this section, we will introduce the recent research on LLM-augmented KGs. We will introduce the methods that integrate LLMs for KG embedding, KG completion, KG construction, KG-to-text generation, and KG question answering, respectively. Representative works are summarized in Table III.\n\nTABLE III: Summary of representative LLM-augmented KG methods.\n\n[b]\n\n\n\n\nTask\nMethod\nYear\nLLM\nTechnique\n\nLLM-augmented KG embedding\nPretrain-KGE [94]\n2020\nE\nLLMs as Text Encoders\n\nKEPLER [40]\n2020\nE\nLLMs as Text Encoders\n\nNayyeri et al. [132]\n2022\nE\nLLMs as Text Encoders\n\nHuang et al. [133]\n2022\nE\nLLMs as Text Encoders\n\nCoDEx [134]\n2022\nE\nLLMs as Text Encoders\n\nLMKE [135]\n2022\nE\nLLMs for Joint Text and KG Embedding\n\nkNN-KGE [136]\n2022\nE\nLLMs for Joint Text and KG Embedding\n\nLambdaKG [137]\n2023\nE + D + ED\nLLMs for Joint Text and KG Embedding\n\nLLM-augmented KG completion\nKG-BERT [26]\n2019\nE\nJoint Encoding\n\nMTL-KGC [138]\n2020\nE\nJoint Encoding\n\nPKGC [139]\n2022\nE\nJoint Encoding\n\nLASS [140]\n2022\nE\nJoint Encoding\n\nMEM-KGC [141]\n2021\nE\nMLM Encoding\n\nOpenWorld KGC [142]\n2023\nE\nMLM Encoding\n\nStAR [143]\n2021\nE\nSeparated Encoding\n\nSimKGC [144]\n2022\nE\nSeparated Encoding\n\nLP-BERT [145]\n2022\nE\nSeparated Encoding\n\nGenKGC [96]\n2022\nED\nLLM as decoders\n\nKGT5 [146]\n2022\nED\nLLM as decoders\n\nKG-S2S [147]\n2022\nED\nLLM as decoders\n\n\nAutoKG [93]\n2023\nD\nLLM as decoders\n\nLLM-augmented KG construction\nELMO [148]\n2018\nE\nNamed Entity Recognition\n\nGenerativeNER [149]\n2021\nED\nNamed Entity Recognition\n\nLDET [150]\n2019\nE\nEntity Typing\n\nBOX4Types [151]\n2021\nE\nEntity Typing\n\nELQ [152]\n2020\nE\nEntity Linking\n\nReFinED [153]\n2022\nE\nEntity Linking\n\nBertCR [154]\n2019\nE\nCR (Within-document)\n\nSpanbert [155]\n2020\nE\nCR (Within-document)\n\nCDLM [156]\n2021\nE\nCR (Cross-document)\n\nCrossCR [157]\n2021\nE\nCR (Cross-document)\n\nCR-RL [158]\n2021\nE\nCR (Cross-document)\n\nSentRE [159]\n2019\nE\nRE (Sentence-level)\n\nCurriculum-RE [160]\n2021\nE\nRE (Sentence-level)\n\nDREEAM [161]\n2023\nE\nRE (Document-level)\n\nKumar et al. [95]\n2020\nE\nEnd-to-End Construction\n\nGuo et al. [162]\n2021\nE\nEnd-to-End Construction\n\nGrapher [41]\n2021\nED\nEnd-to-End Construction\n\nPiVE [163]\n2023\nD + ED\nEnd-to-End Construction\n\n\nCOMET [164]\n2019\nD\nDistilling KGs from LLMs\n\n\nBertNet [165]\n2022\nE\nDistilling KGs from LLMs\n\n\nWest et al. [166]\n2022\nD\nDistilling KGs from LLMs\n\nLLM-augmented KG-to-text Generation\nRibeiro et al [167]\n2021\nED\nLeveraging Knowledge from LLMs\n\nJointGT [42]\n2021\nED\nLeveraging Knowledge from LLMs\n\nFSKG2Text [168]\n2021\nD + ED\nLeveraging Knowledge from LLMs\n\nGAP [169]\n2022\nED\nLeveraging Knowledge from LLMs\n\nGenWiki [170]\n2020\n-\nConstructing KG-text aligned Corpus\n\nKGPT [171]\n2020\nED\nConstructing KG-text aligned Corpus\n\nLLM-augmented KGQA\nLukovnikov et al. [172]\n2019\nE\nEntity/Relation Extractor\n\nLuo et al. [173]\n2020\nE\nEntity/Relation Extractor\n\nQA-GNN [131]\n2021\nE\nEntity/Relation Extractor\n\nNan et al. [174]\n2023\nE + D + ED\nEntity/Relation Extractor\n\nDEKCOR [175]\n2021\nE\nAnswer Reasoner\n\nDRLK [176]\n2022\nE\nAnswer Reasoner\n\nOreoLM [177]\n2022\nE\nAnswer Reasoner\n\nGreaseLM [178]\n2022\nE\nAnswer Reasoner\n\nReLMKG [179]\n2022\nE\nAnswer Reasoner\n\nUniKGQA [43]\n2023\nE\nAnswer Reasoner\n\n\n\n\n•\nE: Encoder-only LLMs, D: Decoder-only LLMs, ED: Encoder-decoder LLMs.\n\n\n\n\n\n\n5.1 LLM-augmented KG Embedding\n\nKnowledge graph embedding (KGE) aims to map each entity and relation into a low-dimensional vector (embedding) space. These embeddings contain both semantic and structural information of KGs, which can be utilized for various tasks such as question answering [180], reasoning [38], and recommendation [181]. Conventional knowledge graph embedding methods mainly rely on the structural information of KGs to optimize a scoring function defined on embeddings (e.g., TransE [33], and DisMult [182]). However, these approaches often fall short in representing unseen entities and long-tailed relations due to their limited structural connectivity [183, 184]. To address this issue, as shown in Fig. 14, recent research adopts LLMs to enrich representations of KGs by encoding the textual descriptions of entities and relations [94, 40].\n\nFigure 14: LLMs as text encoder for knowledge graph embedding (KGE).\n\n\n\n5.1.1 LLMs as Text Encoders\n\nPretrain-KGE [94] is a representative method that follows the framework shown in Fig. 14. Given a triple (h,r,t)ℎ𝑟𝑡(h,r,t)( italic_h , italic_r , italic_t ) from KGs, it firsts uses a LLM encoder to encode the textual descriptions of entities hℎhitalic_h, t𝑡titalic_t, and relations r𝑟ritalic_r into representations as\n\n\n\neh=LLM⁢(Texth),et=LLM⁢(Textt),er=LLM⁢(Textr),formulae-sequencesubscript𝑒ℎLLMsubscriptTextℎformulae-sequencesubscript𝑒𝑡LLMsubscriptText𝑡subscript𝑒𝑟LLMsubscriptText𝑟\\displaystyle e_{h}=\\text{LLM}(\\text{Text}_{h}),e_{t}=\\text{LLM}(\\text{Text}_{%\nt}),e_{r}=\\text{LLM}(\\text{Text}_{r}),italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = LLM ( Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = LLM ( Text start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , italic_e start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = LLM ( Text start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) ,\n\n(1)\n\n\nwhere eh,er,subscript𝑒ℎsubscript𝑒𝑟e_{h},e_{r},italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , and etsubscript𝑒𝑡e_{t}italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT denotes the initial embeddings of entities hℎhitalic_h, t𝑡titalic_t, and relations r𝑟ritalic_r, respectively. Pretrain-KGE uses the BERT as the LLM encoder in experiments. Then, the initial embeddings are fed into a KGE model to generate the final embeddings vh,vrsubscript𝑣ℎsubscript𝑣𝑟v_{h},v_{r}italic_v start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, and vtsubscript𝑣𝑡v_{t}italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. During the KGE training phase, they optimize the KGE model by following the standard KGE loss function as\n\n\n\n\nℒ=[γ+f⁢(vh,vr,vt)−f⁢(vh′,vr′,vt′)],ℒdelimited-[]𝛾𝑓subscript𝑣ℎsubscript𝑣𝑟subscript𝑣𝑡𝑓subscriptsuperscript𝑣′ℎsubscriptsuperscript𝑣′𝑟subscriptsuperscript𝑣′𝑡\\mathcal{L}=[\\gamma+f(v_{h},v_{r},v_{t})-f(v^{\\prime}_{h},v^{\\prime}_{r},v^{%\n\\prime}_{t})],caligraphic_L = [ italic_γ + italic_f ( italic_v start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - italic_f ( italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ] ,\n\n(2)\n\n\nwhere f𝑓fitalic_f is the KGE scoring function, γ𝛾\\gammaitalic_γ is a margin hyperparameter, and vh′,vr′subscriptsuperscript𝑣′ℎsubscriptsuperscript𝑣′𝑟v^{\\prime}_{h},v^{\\prime}_{r}italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, and vt′subscriptsuperscript𝑣′𝑡v^{\\prime}_{t}italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are the negative samples. In this way, the KGE model could learn adequate structure information, while reserving partial knowledge from LLM enabling better knowledge graph embedding. KEPLER [40] offers a unified model for knowledge embedding and pre-trained language representation. This model not only generates effective text-enhanced knowledge embedding using powerful LLMs but also seamlessly integrates factual knowledge into LLMs. Nayyeri et al. [132] use LLMs to generate the world-level, sentence-level, and document-level representations. They are integrated with graph structure embeddings into a unified vector by Dihedron and Quaternion representations of 4D hypercomplex numbers. Huang et al. [133] combine LLMs with other vision and graph encoders to learn multi-modal knowledge graph embedding that enhances the performance of downstream tasks. CoDEx [134] presents a novel loss function empowered by LLMs that guides the KGE models in measuring the likelihood of triples by considering the textual information. The proposed loss function is agnostic to model structure that can be incorporated with any KGE model.\n\n\n\n\n5.1.2 LLMs for Joint Text and KG Embedding\n\nInstead of using KGE model to consider graph structure, another line of methods directly employs LLMs to incorporate both the graph structure and textual information into the embedding space simultaneously. As shown in Fig. 15, k𝑘kitalic_kNN-KGE [136] treats the entities and relations as special tokens in the LLM. During training, it transfers each triple (h,r,t)ℎ𝑟𝑡(h,r,t)( italic_h , italic_r , italic_t ) and corresponding text descriptions into a sentence x𝑥xitalic_x as\n\n\n\nx=[CLS]⁢hTexth⁢[SEP]⁢r⁢[SEP]⁢[MASK]Textt⁢[SEP],𝑥[CLS]ℎsubscriptTextℎ[SEP]𝑟[SEP][MASK]subscriptText𝑡[SEP]x=\\texttt{[CLS]}\\ h\\ \\ \\text{Text}_{h}\\texttt{[SEP]}\\ r\\ \\texttt{[SEP]}\\ %\n\\texttt{[MASK]}\\ \\ \\text{Text}_{t}\\texttt{[SEP]},italic_x = [CLS] italic_h Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] italic_r [SEP] [MASK] Text start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [SEP] ,\n\n(3)\n\n\nwhere the tailed entities are replaced by [MASK]. The sentence is fed into a LLM, which then finetunes the model to predict the masked entity, formulated as\n\n\n\nPL⁢L⁢M⁢(t|h,r)=P⁢([MASK]=t|x,Θ),subscript𝑃𝐿𝐿𝑀conditional𝑡ℎ𝑟𝑃conditional[MASK]=t𝑥ΘP_{LLM}(t|h,r)=P(\\texttt{[MASK]=t}|x,\\Theta),italic_P start_POSTSUBSCRIPT italic_L italic_L italic_M end_POSTSUBSCRIPT ( italic_t | italic_h , italic_r ) = italic_P ( [MASK]=t | italic_x , roman_Θ ) ,\n\n(4)\n\n\nwhere ΘΘ\\Thetaroman_Θ denotes the parameters of the LLM. The LLM is optimized to maximize the probability of the correct entity t𝑡titalic_t. After training, the corresponding token representations in LLMs are used as embeddings for entities and relations. Similarly, LMKE [135] proposes a contrastive learning method to improve the learning of embeddings generated by LLMs for KGE. Meanwhile, to better capture graph structure, LambdaKG [137] samples 1-hop neighbor entities and concatenates their tokens with the triple as a sentence feeding into LLMs.\n\n\nFigure 15: LLMs for joint text and knowledge graph embedding.\n\n\n\n\n\n5.2 LLM-augmented KG Completion\n\nKnowledge Graph Completion (KGC) refers to the task of inferring missing facts in a given knowledge graph. Similar to KGE, conventional KGC methods mainly focused on the structure of the KG, without considering the extensive textual information. However, the recent integration of LLMs enables KGC methods to encode text or generate facts for better KGC performance. These methods fall into two distinct categories based on their utilization styles: 1) LLM as Encoders (PaE), and 2) LLM as Generators (PaG).\n\n\n5.2.1 LLM as Encoders (PaE).\nAs shown in Fig. 16 (a), (b), and (c), this line of work first uses encoder-only LLMs to encode textual information as well as KG facts. Then, they predict the plausibility of the triples or masked entities by feeding the encoded representation into a prediction head, which could be a simple MLP or conventional KG score function (e.g., TransE [33] and TransR [185]).\n\nJoint Encoding.\nSince the encoder-only LLMs (e.g., Bert [1]) are well at encoding text sequences, KG-BERT [26] represents a triple (h,r,t)ℎ𝑟𝑡(h,r,t)( italic_h , italic_r , italic_t ) as a text sequence and encodes it with LLM Fig. 16(a).\n\n\n\nx=[CLS]⁢Texth⁢[SEP]⁢Textr⁢[SEP]⁢Textt⁢[SEP],𝑥[CLS]subscriptTextℎ[SEP]subscriptText𝑟[SEP]subscriptText𝑡[SEP]x=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}\\ \\texttt{[%\nSEP]}\\ \\text{Text}_{t}\\ \\texttt{[SEP]},italic_x = [CLS] Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] Text start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT [SEP] Text start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [SEP] ,\n\n(5)\n\n\nThe final hidden state of the [CLS] token is fed into a classifier to predict the possibility of the triple, formulated as\n\n\n\ns=σ⁢(MLP⁢(e[CLS])),𝑠𝜎MLPsubscript𝑒[CLS]s=\\sigma(\\text{MLP}(e_{\\texttt{[CLS]}})),italic_s = italic_σ ( MLP ( italic_e start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT ) ) ,\n\n(6)\n\n\nwhere σ⁢(⋅)𝜎⋅\\sigma(\\cdot)italic_σ ( ⋅ ) denotes the sigmoid function and e[CLS]subscript𝑒[CLS]e_{\\texttt{[CLS]}}italic_e start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT denotes the representation encoded by LLMs.\nTo improve the efficacy of KG-BERT, MTL-KGC [138] proposed a Multi-Task Learning for the KGC framework which incorporates additional auxiliary tasks into the model’s training, i.e. prediction (RP) and relevance ranking (RR). PKGC [139] assesses the validity of a triplet (h,r,t)ℎ𝑟𝑡(h,r,t)( italic_h , italic_r , italic_t ) by transforming the triple and its supporting information into natural language sentences with pre-defined templates. These sentences are then processed by LLMs for binary classification. The supporting information of the triplet is derived from the attributes of hℎhitalic_h and t𝑡titalic_t with a verbalizing function. For instance, if the triple is (Lebron James, member of sports team, Lakers), the information regarding Lebron James is verbalized as ”Lebron James: American basketball player”. LASS [140] observes that language semantics and graph structures are equally vital to KGC. As a result, LASS is proposed to jointly learn two types of embeddings: semantic embedding and structure embedding. In this method, the full text of a triple is forwarded to the LLM, and the mean pooling of the corresponding LLM outputs for hℎhitalic_h, r𝑟ritalic_r, and t𝑡titalic_t are separately calculated. These embeddings are then passed to a graph-based method, i.e. TransE, to reconstruct the KG structures.\n\n\nFigure 16: The general framework of adopting LLMs as encoders (PaE) for KG Completion.\n\n\nMLM Encoding. Instead of encoding the full text of a triple, many works introduce the concept of Masked Language Model (MLM) to encode KG text (Fig. 16(b)). MEM-KGC [141] uses Masked Entity Model (MEM) classification mechanism to predict the masked entities of the triple. The input text is in the form of\n\n\n\nx=[CLS]⁢Texth⁢[SEP]⁢Textr⁢[SEP]⁢[MASK]⁢[SEP],𝑥[CLS]subscriptTextℎ[SEP]subscriptText𝑟[SEP][MASK][SEP]x=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}\\ \\texttt{[%\nSEP]}\\ \\texttt{[MASK]}\\ \\texttt{[SEP]},italic_x = [CLS] Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] Text start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT [SEP] [MASK] [SEP] ,\n\n(7)\n\n\nSimilar to Eq. 4, it tries to maximize the probability that the masked entity is the correct entity t𝑡titalic_t.\nAdditionally, to enable the model to learn unseen entities, MEM-KGC integrates multitask learning for entities and super-class prediction based on the text description of entities:\n\n\n\nx=[CLS]⁢[MASK]⁢[SEP]⁢Texth⁢[SEP].𝑥[CLS][MASK][SEP]subscriptTextℎ[SEP]x=\\texttt{[CLS]}\\ \\texttt{[MASK]}\\ \\texttt{[SEP]}\\ \\text{Text}_{h}\\ \\texttt{[%\nSEP]}.italic_x = [CLS] [MASK] [SEP] Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] .\n\n(8)\n\n\nOpenWorld KGC [142] expands the MEM-KGC model to address the challenges of open-world KGC with a pipeline framework, where two sequential MLM-based modules are defined: Entity Description Prediction (EDP), an auxiliary module that predicts a corresponding entity with a given textual description; Incomplete Triple Prediction (ITP), the target module that predicts a plausible entity for a given incomplete triple (h,r,?)ℎ𝑟?(h,r,?)( italic_h , italic_r , ? ). EDP first encodes the triple with Eq. 8 and generates the final hidden state, which is then forwarded into ITP as an embedding of the head entity in Eq. 7 to predict target entities.\n\n\nSeparated Encoding. As shown in Fig. 16(c), these methods involve partitioning a triple (h,r,t)ℎ𝑟𝑡(h,r,t)( italic_h , italic_r , italic_t ) into two distinct parts, i.e. (h,r)ℎ𝑟(h,r)( italic_h , italic_r ) and t𝑡titalic_t, which can be expressed as\n\n\n\nx(h,r)subscript𝑥ℎ𝑟\\displaystyle x_{(h,r)}italic_x start_POSTSUBSCRIPT ( italic_h , italic_r ) end_POSTSUBSCRIPT\n=[CLS]⁢Texth⁢[SEP]⁢Textr⁢[SEP],absent[CLS]subscriptTextℎ[SEP]subscriptText𝑟[SEP]\\displaystyle=\\texttt{[CLS]}\\ \\text{Text}_{h}\\ \\texttt{[SEP]}\\ \\text{Text}_{r}%\n\\ \\texttt{[SEP]},= [CLS] Text start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT [SEP] Text start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT [SEP] ,\n\n(9)\n\n\n\nxtsubscript𝑥𝑡\\displaystyle x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT\n=[CLS]⁢Textt⁢[SEP].absent[CLS]subscriptText𝑡[SEP]\\displaystyle=\\texttt{[CLS]}\\ \\text{Text}_{t}\\ \\texttt{[SEP]}.= [CLS] Text start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [SEP] .\n\n(10)\n\n\nThen the two parts are encoded separately by LLMs, and the final hidden states of the [CLS] tokens are used as the representations of (h,r)ℎ𝑟(h,r)( italic_h , italic_r ) and t𝑡titalic_t, respectively. The representations are then fed into a scoring function to predict the possibility of the triple, formulated as\n\n\n\ns=fs⁢c⁢o⁢r⁢e⁢(e(h,r),et),𝑠subscript𝑓𝑠𝑐𝑜𝑟𝑒subscript𝑒ℎ𝑟subscript𝑒𝑡s=f_{score}(e_{(h,r)},e_{t}),italic_s = italic_f start_POSTSUBSCRIPT italic_s italic_c italic_o italic_r italic_e end_POSTSUBSCRIPT ( italic_e start_POSTSUBSCRIPT ( italic_h , italic_r ) end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ,\n\n(11)\n\n\nwhere fs⁢c⁢o⁢r⁢esubscript𝑓𝑠𝑐𝑜𝑟𝑒f_{score}italic_f start_POSTSUBSCRIPT italic_s italic_c italic_o italic_r italic_e end_POSTSUBSCRIPT denotes the score function like TransE.\n\nStAR [143] applies Siamese-style textual encoders on their text, encoding them into separate contextualized representations. To avoid the combinatorial explosion of textual encoding approaches, e.g., KG-BERT, StAR employs a scoring module that involves both deterministic classifier and spatial measurement for representation and structure learning respectively, which also enhances structured knowledge by exploring the spatial characteristics. SimKGC [144] is another instance of leveraging a Siamese textual encoder to encode textual representations. Following the encoding process, SimKGC applies contrastive learning techniques to these representations. This process involves computing the similarity between the encoded representations of a given triple and its positive and negative samples. In particular, the similarity between the encoded representation of the triple and the positive sample is maximized, while the similarity between the encoded representation of the triple and the negative sample is minimized. This enables SimKGC to learn a representation space that separates plausible and implausible triples. To avoid overfitting textural information, CSPromp-KG [186] employs parameter-efficient prompt learning for KGC.\nLP-BERT [145] is a hybrid KGC method that combines both MLM Encoding and Separated Encoding. This approach consists of two stages, namely pre-training and fine-tuning. During pre-training, the method utilizes the standard MLM mechanism to pre-train a LLM with KGC data. During the fine-tuning stage, the LLM encodes both parts and is optimized using a contrastive learning strategy (similar to SimKGC [144]).\n\n\n\n\n5.2.2 LLM as Generators (PaG).\n\nFigure 17: The general framework of adopting LLMs as decoders (PaG) for KG Completion. The En. and De. denote the encoder and decoder, respectively.\n\nRecent works use LLMs as sequence-to-sequence generators in KGC. As presented in Fig. 17 (a) and (b), these approaches involve encoder-decoder or decoder-only LLMs. The LLMs receive a sequence text input of the query triple (h,r,?)ℎ𝑟?(h,r,?)( italic_h , italic_r , ? ), and generate the text of tail entity t𝑡titalic_t directly.\nGenKGC [96] uses the large language model BART [5] as the backbone model. Inspired by the in-context learning approach used in GPT-3 [59], where the model concatenates relevant samples to learn correct output answers, GenKGC proposes a relation-guided demonstration technique that includes triples with the same relation to facilitating the model’s learning process. In addition, during generation, an entity-aware hierarchical decoding method is proposed to reduce the time complexity. KGT5 [146] introduces a novel KGC model that fulfils four key requirements of such models: scalability, quality, versatility, and simplicity. To address these objectives, the proposed model employs a straightforward T5 small architecture. The model is distinct from previous KGC methods, in which it is randomly initialized rather than using pre-trained models. KG-S2S [147] is a comprehensive framework that can be applied to various types of KGC tasks, including Static KGC, Temporal KGC, and Few-shot KGC. To achieve this objective, KG-S2S reformulates the standard triple KG fact by introducing an additional element, forming a quadruple (h,r,t,m)ℎ𝑟𝑡𝑚(h,r,t,m)( italic_h , italic_r , italic_t , italic_m ), where m𝑚mitalic_m represents the additional ”condition” element. Although different KGC tasks may refer to different conditions, they typically have a similar textual format, which enables unification across different KGC tasks. The KG-S2S approach incorporates various techniques such as entity description, soft prompt, and Seq2Seq Dropout to improve the model’s performance. In addition, it utilizes constrained decoding to ensure the generated entities are valid. For closed-source LLMs (e.g., ChatGPT and GPT-4), AutoKG adopts prompt engineering to design customized prompts [93]. As shown in Fig. 18, these prompts contain the task description, few-shot examples, and test input, which instruct LLMs to predict the tail entity for KG completion.\n\nFigure 18: The framework of prompt-based PaG for KG Completion.\n\nComparison between PaE and PaG.\nLLMs as Encoders (PaE) applies an additional prediction head on the top of the representation encoded by LLMs. Therefore, the PaE framework is much easier to finetune since we can only optimize the prediction heads and freeze the LLMs. Moreover, the output of the prediction can be easily specified and integrated with existing KGC functions for different KGC tasks. However, during the inference stage, the PaE requires to compute a score for every candidate in KGs, which could be computationally expensive. Besides, they cannot generalize to unseen entities. Furthermore, the PaE requires the representation output of the LLMs, whereas some state-of-the-art LLMs (e.g. GPT-411footnotemark: 1) are closed sources and do not grant access to the representation output.\nLLMs as Generators (PaG), on the other hand, which does not need the prediction head, can be used without finetuning or access to representations. Therefore, the framework of PaG is suitable for all kinds of LLMs. In addition, PaG directly generates the tail entity, making it efficient in inference without ranking all the candidates and easily generalizing to unseen entities. But, the challenge of PaG is that the generated entities could be diverse and not lie in KGs. What is more, the time of a single inference is longer due to the auto-regressive generation. Last, how to design a powerful prompt that feeds KGs into LLMs is still an open question.\nConsequently, while PaG has demonstrated promising results for KGC tasks, the trade-off between model complexity and computational efficiency must be carefully considered when selecting an appropriate LLM-based KGC framework.\n\n\n\n5.2.3 Model Analysis\nJustin et al. [187] provide a comprehensive analysis of KGC methods integrated with LLMs. Their research investigates the quality of LLM embeddings and finds that they are suboptimal for effective entity ranking. In response, they propose several techniques for processing embeddings to improve their suitability for candidate retrieval. The study also compares different model selection dimensions, such as Embedding Extraction, Query Entity Extraction, and Language Model Selection. Lastly, the authors propose a framework that effectively adapts LLM for knowledge graph completion.\n\n\n\n\n5.3 LLM-augmented KG Construction\n\nKnowledge graph construction involves creating a structured representation of knowledge within a specific domain. This includes identifying entities and their relationships with each other. The process of knowledge graph construction typically involves multiple stages, including 1) entity discovery, 2) coreference resolution, and 3) relation extraction. Fig 19 presents the general framework of applying LLMs for each stage in KG construction. More recent approaches have explored 4) end-to-end knowledge graph construction, which involves constructing a complete knowledge graph in one step or directly 5) distilling knowledge graphs from LLMs.\n\n\n5.3.1 Entity Discovery\nEntity discovery in KG construction refers to the process of identifying and extracting entities from unstructured data sources, such as text documents, web pages, or social media posts, and incorporating them to construct knowledge graphs.\nNamed Entity Recognition (NER) involves identifying and tagging named entities in text data with their positions and classifications. The named entities include people, organizations, locations, and other types of entities. The state-of-the-art NER methods usually employ LLMs to leverage their contextual understanding and linguistic knowledge for accurate entity recognition and classification. There are three NER sub-tasks based on the types of NER spans identified, i.e., flat NER, nested NER, and discontinuous NER. 1) Flat NER is to identify non-overlapping named entities from input text. It is usually conceptualized as a sequence labelling problem where each token in the text is assigned a unique label based on its position in the sequence [148, 1, 188, 189]. 2) Nested NER considers complex scenarios which allow a token to belong to multiple entities. The span-based method [190, 191, 192, 193, 194] is a popular branch of nested NER which involves enumerating all candidate spans and classifying them into entity types (including a non-entity type). Parsing-based methods [195, 196, 197] reveal similarities between nested NER and constituency parsing tasks (predicting nested and non-overlapping spans), and propose to integrate the insights of constituency parsing into nested NER.\n3) Discontinuous NER identifies named entities that may not be contiguous in the text. To address this challenge, [198] uses the LLM output to identify entity fragments and determine whether they are overlapped or in succession.\nUnlike the task-specific methods, GenerativeNER [149] uses a sequence-to-sequence LLM with a pointer mechanism to generate an entity sequence, which is capable of solving all three types of NER sub-tasks.\n\nFigure 19: The general framework of LLM-based KG construction.\n\nEntity Typing (ET) aims to provide fine-grained and ultra-grained type information for a given entity mentioned in context. These methods usually utilize LLM to encode mentions, context and types. LDET [150] applies pre-trained ELMo embeddings [148] for word representation and adopts LSTM as its sentence and mention encoders. BOX4Types [151] recognizes the importance of type dependency and uses BERT to represent the hidden vector and each type in a hyperrectangular (box) space. LRN [199] considers extrinsic and intrinsic dependencies between labels. It encodes the context and entity with BERT and employs these output embeddings to conduct deductive and inductive reasoning. MLMET [200] uses predefined patterns to construct input samples for the BERT MLM and employs [MASK] to predict context-dependent hypernyms of the mention, which can be viewed as type labels. PL [201] and DFET [202] utilize prompt learning for entity typing. LITE [203] formulates entity typing as textual inference and uses RoBERTa-large-MNLI as the backbone network. \nEntity Linking (EL), as known as entity disambiguation, involves linking entity mentions appearing in the text to their corresponding entities in a knowledge graph. [204] proposed BERT-based end-to-end EL systems that jointly discover and link entities. ELQ [152] employs a fast bi-encoder architecture to jointly perform mention detection and linking in one pass for downstream question answering systems. Unlike previous models that frame EL as matching in vector space, GENRE [205] formulates it as a sequence-to-sequence problem, autoregressively generating a version of the input markup-annotated with the unique identifiers of an entity expressed in natural language. GENRE is extended to its multilingual version mGENRE [206]. Considering the efficiency challenges of generative EL approaches, [207] parallelizes autoregressive linking across all potential mentions and relies on a shallow and efficient decoder. ReFinED [153] proposes an efficient zero-shot-capable EL approach by taking advantage of fine-grained entity types and entity descriptions which are processed by a LLM-based encoder. \n\n\n\n5.3.2 Coreference Resolution (CR)\nCoreference resolution is to find all expressions (i.e., mentions) that refer to the same entity or event in a text.\nWithin-document CR refers to the CR sub-task where all these mentions are in a single document. Mandar et al. [154] initialize LLM-based coreferences resolution by replacing the previous LSTM encoder [208] with BERT. This work is followed by the introduction of SpanBERT [155] which is pre-trained on BERT architecture with a span-based masked language model (MLM). Inspired by these works, Tuan Manh et al. [209] present a strong baseline by incorporating the SpanBERT encoder into a non-LLM approach e2e-coref [208]. CorefBERT leverages Mention Reference Prediction (MRP) task which masks one or several mentions and requires the model to predict the masked mention’s corresponding referents. CorefQA [210] formulates coreference resolution as a question answering task, where contextual queries are generated for each candidate mention and the coreferent spans are extracted from the document using the queries.\nTuan Manh et al. [211] introduce a gating mechanism and a noisy training method to extract information from event mentions using the SpanBERT encoder.\n\nIn order to reduce the large memory footprint faced by large LLM-based NER models, Yuval et al. [212] and Raghuveer el al. [213] proposed start-to-end and approximation models, respectively, both utilizing bilinear functions to calculate mention and antecedent scores with reduced reliance on span-level representations.\n\nCross-document CR refers to the sub-task where the mentions refer to the same entity or event might be across multiple documents. CDML [156] proposes a cross document language modeling method which pre-trains a Longformer [214] encoder on concatenated related documents and employs an MLP for binary classification to determine whether a pair of mentions is coreferent or not. CrossCR [157] utilizes an end-to-end model for cross-document coreference resolution which pre-trained the mention scorer on gold mention spans and uses a pairwise scorer to compare mentions with all spans across all documents. CR-RL [158] proposes an actor-critic deep reinforcement learning-based coreference resolver for cross-document CR.\n\n\n\n5.3.3 Relation Extraction (RE)\nRelation extraction involves identifying semantic relationships between entities mentioned in natural language text. There are two types of relation extraction methods, i.e. sentence-level RE and document-level RE, according to the scope of the text analyzed.\nSentence-level RE focuses on identifying relations between entities within a single sentence. Peng et al. [159] and TRE [215] introduce LLM to improve the performance of relation extraction models. BERT-MTB [216] learns relation representations based on BERT by performing the matching-the-blanks task and incorporating designed objectives for relation extraction. Curriculum-RE [160] utilizes curriculum learning to improve relation extraction models by gradually increasing the difficulty of the data during training.\nRECENT [217] introduces SpanBERT and exploits entity type restriction to reduce the noisy candidate relation types. Jiewen [218] extends RECENT by combining both the entity information and the label information into sentence-level embeddings, which enables the embedding to be entity-label aware.\n\nDocument-level RE (DocRE) aims to extract relations between entities across multiple sentences within a document. Hong et al. [219] propose a strong baseline for DocRE by replacing the BiLSTM backbone with LLMs. HIN [220] use LLM to encode and aggregate entity representation at different levels, including entity, sentence, and document levels. GLRE [221] is a global-to-local network, which uses LLM to encode the document information in terms of entity global and local representations as well as context relation representations. SIRE [222] uses two LLM-based encoders to extract intra-sentence and inter-sentence relations. LSR [223] and GAIN [224] propose graph-based approaches which induce graph structures on top of LLM to better extract relations. DocuNet [225] formulates DocRE as a semantic segmentation task and introduces a U-Net [226] on the LLM encoder to capture local and global dependencies between entities. ATLOP [227] focuses on the multi-label problems in DocRE, which could be handled with two techniques, i.e., adaptive thresholding for classifier and localized context pooling for LLM. DREEAM [161] further extends and improves ATLOP by incorporating evidence information.\n\nEnd-to-End KG Construction.\nCurrently, researchers are exploring the use of LLMs for end-to-end KG construction. Kumar et al. [95] propose a unified approach to build KGs from raw text, which contains two LLMs powered components. They first finetune a LLM on named entity recognition tasks to make it capable of recognizing entities in raw text. Then, they propose another “2-model BERT” for solving the relation extraction task, which contains two BERT-based classifiers. The first classifier learns the relation class whereas the second binary classifier learns the direction of the relations between the two entities. The predicted triples and relations are then used to construct the KG. Guo et al. [162] propose an end-to-end knowledge extraction model based on BERT, which can be applied to construct KGs from Classical Chinese text.\nGrapher [41] presents a novel end-to-end multi-stage system. It first utilizes LLMs to generate KG entities, followed by a simple relation construction head, enabling efficient KG construction from the textual description. PiVE [163] proposes a prompting with an iterative verification framework that utilizes a smaller LLM like T5 to correct the errors in KGs generated by a larger LLM (e.g., ChatGPT). To further explore advanced LLMs, AutoKG design several prompts for different KG construction tasks (e.g., entity typing, entity linking, and relation extraction). Then, it adopts the prompt to perform KG construction using ChatGPT and GPT-4.\n\nFigure 20: The general framework of distilling KGs from LLMs.\n\n\n\n\n5.3.4 Distilling Knowledge Graphs from LLMs \nLLMs have been shown to implicitly encode massive knowledge [14]. As shown in Fig. 20, some research aims to distill knowledge from LLMs to construct KGs. COMET [164] proposes a commonsense transformer model that constructs commonsense KGs by using existing tuples as a seed set of knowledge on which to train. Using this seed set, a LLM learns to adapt its learned representations to knowledge generation, and produces novel tuples that are high quality. Experimental results reveal that implicit knowledge from LLMs is transferred to generate explicit knowledge in commonsense KGs. BertNet [165] proposes a novel framework for automatic KG construction empowered by LLMs. It requires only the minimal definition of relations as inputs and automatically generates diverse prompts, and performs an efficient knowledge search within a given LLM for consistent outputs. The constructed KGs show competitive quality, diversity, and novelty with a richer set of new and complex relations, which cannot be extracted by previous methods. West et al. [166] propose a symbolic knowledge distillation framework that distills symbolic knowledge from LLMs. They first finetune a small student LLM by distilling commonsense facts from a large LLM like GPT-3. Then, the student LLM is utilized to generate commonsense KGs.\n\n\n\n\n5.4 LLM-augmented KG-to-text Generation\n\nThe goal of Knowledge-graph-to-text (KG-to-text) generation is to generate high-quality texts that accurately and consistently describe the input knowledge graph information [228]. KG-to-text generation connects knowledge graphs and texts, significantly improving the applicability of KG in more realistic NLG scenarios, including storytelling [229] and knowledge-grounded dialogue [230]. However, it is challenging and costly to collect large amounts of graph-text parallel data, resulting in insufficient training and poor generation quality. Thus, many research efforts resort to either: 1) leverage knowledge from LLMs or 2) construct large-scale weakly-supervised KG-text corpus to solve this issue.\n\n\n5.4.1 Leveraging Knowledge from LLMs\nAs pioneering research efforts in using LLMs for KG-to-Text generation, Ribeiro et al. [167] and Kale and Rastogi [231] directly fine-tune various LLMs, including BART and T5, with the goal of transferring LLMs knowledge for this task. As shown in Fig. 21, both works simply represent the input graph as a linear traversal and find that such a naive approach successfully outperforms many existing state-of-the-art KG-to-text generation systems. Interestingly, Ribeiro et al. [167] also find that continue pre-training could further improve model performance. However, these methods are unable to explicitly incorporate rich graph semantics in KGs. To enhance LLMs with KG structure information, JointGT [42] proposes to inject KG structure-preserving representations into the Seq2Seq large language models. Given input sub-KGs and corresponding text, JointGT first represents the KG entities and their relations as a sequence of tokens, then concatenate them with the textual tokens which are fed into LLM. After the standard self-attention module, JointGT then uses a pooling layer to obtain the contextual semantic representations of knowledge entities and relations. Finally, these pooled KG representations are then aggregated in another structure-aware self-attention layer. JointGT also deploys additional pre-training objectives, including KG and text reconstruction tasks given masked inputs, to improve the alignment between text and graph information. Li et al. [168] focus on the few-shot scenario. It first employs a novel breadth-first search (BFS) strategy to better traverse the input KG structure and feed the enhanced linearized graph representations into LLMs for high-quality generated outputs, then aligns the GCN-based and LLM-based KG entity representation. Colas et al. [169] first transform the graph into its appropriate representation before linearizing the graph. Next, each KG node is encoded via a global attention mechanism, followed by a graph-aware attention module, ultimately being decoded into a sequence of tokens. Different from these works, KG-BART [37] keeps the structure of KGs and leverages the graph attention to aggregate the rich concept semantics in the sub-KG, which enhances the model generalization on unseen concept sets.\n\n\nFigure 21: The general framework of KG-to-text generation.\n\n\n\n\n5.4.2 Constructing large weakly KG-text aligned Corpus\nAlthough LLMs have achieved remarkable empirical success, their unsupervised pre-training objectives are not necessarily aligned well with the task of KG-to-text generation, motivating researchers to develop large-scale KG-text aligned corpus. Jin et al. [170] propose a 1.3M unsupervised KG-to-graph training data from Wikipedia. Specifically, they first detect the entities appearing in the text via hyperlinks and named entity detectors, and then only add text that shares a common set of entities with the corresponding knowledge graph, similar to the idea of distance supervision in the relation extraction task [232].\nThey also provide a 1,000+ human annotated KG-to-Text test data to verify the effectiveness of the pre-trained KG-to-Text models. Similarly, Chen et al. [171] also propose a KG-grounded text corpus collected from the English Wikidump. To ensure the connection between KG and text, they only extract sentences with at least two Wikipedia anchor links. Then, they use the entities from those links to query their surrounding neighbors in WikiData and calculate the lexical overlapping between these neighbors and the original sentences. Finally, only highly overlapped pairs are selected. The authors explore both graph-based and sequence-based encoders and identify their advantages in various different tasks and settings.\n\n\n\n\n5.5 LLM-augmented KG Question Answering\n\nKnowledge graph question answering (KGQA) aims to find answers to natural language questions based on the structured facts stored in knowledge graphs [233, 234]. The inevitable challenge in KGQA is to retrieve related facts and extend the reasoning advantage of KGs to QA. Therefore, recent studies adopt LLMs to bridge the gap between natural language questions and structured knowledge graphs [175, 235, 174]. The general framework of applying LLMs for KGQA is illustrated in Fig. 22, where LLMs can be used as 1) entity/relation extractors, and 2) answer reasoners.\n\n\n5.5.1 LLMs as Entity/relation Extractors\n\nEntity/relation extractors are designed to identify entities and relationships mentioned in natural language questions and retrieve related facts in KGs. Given the proficiency in language comprehension, LLMs can be effectively utilized for this purpose. Lukovnikov et al. [172] are the first to utilize LLMs as classifiers for relation prediction, resulting in a notable improvement in performance compared to shallow neural networks. Nan et al. [174] introduce two LLM-based KGQA frameworks that adopt LLMs to detect mentioned entities and relations. Then, they query the answer in KGs using the extracted entity-relation pairs. QA-GNN [131] uses LLMs to encode the question and candidate answer pairs, which are adopted to estimate the importance of relative KG entities. The entities are retrieved to form a subgraph, where an answer reasoning is conducted by a graph neural network.\nLuo et al. [173] use LLMs to calculate the similarities between relations and questions to retrieve related facts, formulated as\n\n\n\ns⁢(r,q)=LLM⁢(r)⊤⁢LLM⁢(q),𝑠𝑟𝑞LLMsuperscript𝑟topLLM𝑞s(r,q)=\\text{LLM}(r)^{\\top}\\text{LLM}(q),italic_s ( italic_r , italic_q ) = LLM ( italic_r ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT LLM ( italic_q ) ,\n\n(12)\n\n\nwhere q𝑞qitalic_q denotes the question, r𝑟ritalic_r denotes the relation, and LLM⁢(⋅)LLM⋅\\text{LLM}(\\cdot)LLM ( ⋅ ) would generate representation for q𝑞qitalic_q and r𝑟ritalic_r, respectively. Furthermore, Zhang et al. [236] propose a LLM-based path retriever to retrieve question-related relations hop-by-hop and construct several paths. The probability of each path can be calculated as\n\n\n\nP⁢(p|q)=∏t=1|p|s⁢(rt,q),𝑃conditional𝑝𝑞superscriptsubscriptproduct𝑡1𝑝𝑠subscript𝑟𝑡𝑞P(p|q)=\\prod_{t=1}^{|p|}s(r_{t},q),italic_P ( italic_p | italic_q ) = ∏ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_p | end_POSTSUPERSCRIPT italic_s ( italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_q ) ,\n\n(13)\n\n\nwhere p𝑝pitalic_p denotes the path, and rtsubscript𝑟𝑡r_{t}italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT denotes the relation at the t𝑡titalic_t-th hop of p𝑝pitalic_p. The retrieved relations and paths can be used as context knowledge to improve the performance of answer reasoners as\n\n\n\nP⁢(a|q)=∑p∈𝒫P⁢(a|p)⁢P⁢(p|q),𝑃conditional𝑎𝑞subscript𝑝𝒫𝑃conditional𝑎𝑝𝑃conditional𝑝𝑞P(a|q)=\\sum_{p\\in\\mathcal{P}}P(a|p)P(p|q),italic_P ( italic_a | italic_q ) = ∑ start_POSTSUBSCRIPT italic_p ∈ caligraphic_P end_POSTSUBSCRIPT italic_P ( italic_a | italic_p ) italic_P ( italic_p | italic_q ) ,\n\n(14)\n\n\nwhere 𝒫𝒫\\mathcal{P}caligraphic_P denotes retrieved paths and a𝑎aitalic_a denotes the answer.\n\n\nFigure 22: The general framework of applying LLMs for knowledge graph question answering (KGQA).\n\n\n\n\n5.5.2 LLMs as Answer Reasoners\n\nAnswer reasoners are designed to reason over the retrieved facts and generate answers. LLMs can be used as answer reasoners to generate answers directly. For example, as shown in Fig. 3 22, DEKCOR [175] concatenates the retrieved facts with questions and candidate answers as\n\n\n\nx=[CLS]⁢q⁢[SEP]⁢Related Facts⁢[SEP]⁢a⁢[SEP],𝑥[CLS]𝑞[SEP]Related Facts[SEP]𝑎[SEP]x=\\texttt{[CLS]}\\ q\\ \\texttt{[SEP]}\\ \\text{Related Facts}\\ \\texttt{[SEP]}\\ a\\ %\n\\texttt{[SEP]},italic_x = [CLS] italic_q [SEP] Related Facts [SEP] italic_a [SEP] ,\n\n(15)\n\n\nwhere a𝑎aitalic_a denotes candidate answers. Then, it feeds them into LLMs to predict answer scores. After utilizing LLMs to generate the representation of x𝑥xitalic_x as QA context, DRLK [176] proposes a Dynamic Hierarchical Reasoner to capture the interactions between QA context and answers for answer prediction. Yan et al. [235] propose a LLM-based KGQA framework consisting of two stages: (1) retrieve related facts from KGs and (2) generate answers based on the retrieved facts. The first stage is similar to the entity/relation extractors. Given a candidate answer entity a𝑎aitalic_a, it extracts a series of paths p1,…,pnsubscript𝑝1…subscript𝑝𝑛p_{1},\\ldots,p_{n}italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT from KGs. But the second stage is a LLM-based answer reasoner. It first verbalizes the paths by using the entity names and relation names in KGs. Then, it concatenates the question q𝑞qitalic_q and all paths p1,…,pnsubscript𝑝1…subscript𝑝𝑛p_{1},\\ldots,p_{n}italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT to make an input sample as\n\n\n\nx=[CLS]⁢q⁢[SEP]⁢p1⁢[SEP]⁢⋯⁢[SEP]⁢pn⁢[SEP].𝑥[CLS]𝑞[SEP]subscript𝑝1[SEP]⋯[SEP]subscript𝑝𝑛[SEP]x=\\texttt{[CLS]}\\ q\\ \\texttt{[SEP]}\\ p_{1}\\ \\texttt{[SEP]}\\ \\cdots\\ \\texttt{[%\nSEP]}\\ p_{n}\\ \\texttt{[SEP]}.italic_x = [CLS] italic_q [SEP] italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT [SEP] ⋯ [SEP] italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT [SEP] .\n\n(16)\n\n\nThese paths are regarded as the related facts for the candidate answer a𝑎aitalic_a. Finally, it uses LLMs to predict whether the hypothesis: “a𝑎aitalic_a is the answer of q𝑞qitalic_q” is supported by those facts, which is formulated as\n\n\n\n\ne[CLS]subscript𝑒[CLS]\\displaystyle e_{\\texttt{[CLS]}}italic_e start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT\n=LLM⁢(x),absentLLM𝑥\\displaystyle=\\text{LLM}(x),= LLM ( italic_x ) ,\n\n(17)\n\n\n\ns𝑠\\displaystyle sitalic_s\n=σ⁢(MLP⁢(e[CLS])),absent𝜎MLPsubscript𝑒[CLS]\\displaystyle=\\sigma(\\text{MLP}(e_{\\texttt{[CLS]}})),= italic_σ ( MLP ( italic_e start_POSTSUBSCRIPT [CLS] end_POSTSUBSCRIPT ) ) ,\n\n(18)\n\n\nwhere it encodes x𝑥xitalic_x using a LLM and feeds representation corresponding to [CLS] token for binary classification, and σ⁢(⋅)𝜎⋅\\sigma(\\cdot)italic_σ ( ⋅ ) denotes the sigmoid function.\n\nTo better guide LLMs reason through KGs, OreoLM [177] proposes a Knowledge Interaction Layer (KIL) which is inserted amid LLM layers. KIL interacts with a KG reasoning module, where it discovers different reasoning paths, and then the reasoning module can reason over the paths to generate answers. GreaseLM [178] fuses the representations from LLMs and graph neural networks to effectively reason over KG facts and language context. UniKGQA [43] unifies the facts retrieval and reasoning into a unified framework. UniKGQA consists of two modules. The first module is a semantic matching module that uses a LLM to match questions with their corresponding relations semantically. The second module is a matching information propagation module, which propagates the matching information along directed edges on KGs for answer reasoning. Similarly, ReLMKG [179] performs joint reasoning on a large language model and the associated knowledge graph. The question and verbalized paths are encoded by the language model, and different layers of the language model produce outputs that guide a graph neural network to perform message passing. This process utilizes the explicit knowledge contained in the structured knowledge graph for reasoning purposes. StructGPT [237] adopts a customized interface to allow large language models (e.g., ChatGPT) directly reasoning on KGs to perform multi-step question answering.\n\nTABLE IV: Summary of methods that synergize KGs and LLMs.\n\n\n\n\n\n\n\n6 Synergized LLMs + KGs\n\nThe synergy of LLMs and KGs has attracted increasing attention these years, which marries the merits of LLMs and KGs to mutually enhance performance in various downstream applications. For example, LLMs can be used to understand natural language, while KGs are treated as a knowledge base, which provides factual knowledge. The unification of LLMs and KGs could result in a powerful model for knowledge representation and reasoning.\nIn this section, we will discuss the state-of-the-art Synergized LLMs + KGs from two perspectives: 1) Synergized Knowledge Representation, and 2) Synergized Reasoning. Representative works are summarized in Table IV.\n\n\nFigure 23: Synergized knowledge representation by additional KG fusion modules.\n\n\n\n6.1 Synergized Knowledge Representation\n\nText corpus and knowledge graphs both contain enormous knowledge. However, the knowledge in text corpus is usually implicit and unstructured, while the knowledge in KGs is explicit and structured. Synergized Knowledge Representation aims to design a synergized model that can effectively represent knowledge from both LLMs and KGs. The synergized model can provide a better understanding of the knowledge from both sources, making it valuable for many downstream tasks.\nTo jointly represent the knowledge, researchers propose the synergized models by introducing additional KG fusion modules, which are jointly trained with LLMs. As shown in Fig. 23, ERNIE [35] proposes a textual-knowledge dual encoder architecture where a T-encoder first encodes the input sentences, then a K-encoder processes knowledge graphs which are fused them with the textual representation from the T-encoder. BERT-MK [241] employs a similar dual-encoder architecture but it introduces additional information of neighboring entities in the knowledge encoder component during the pre-training of LLMs. However, some of the neighboring entities in KGs may not be relevant to the input text, resulting in extra redundancy and noise. CokeBERT [242] focuses on this issue and proposes a GNN-based module to filter out irrelevant KG entities using the input text. JAKET [243] proposes to fuse the entity information in the middle of the large language model.\nKEPLER [40] presents a unified model for knowledge embedding and pre-trained language representation. In KEPLER, they encode textual entity descriptions with a LLM as their embeddings, and then jointly optimize the knowledge embedding and language modeling objectives. JointGT [42] proposes a graph-text joint representation learning model, which proposes three pre-training tasks to align representations of graph and text. DRAGON [44] presents a self-supervised method to pre-train a joint language-knowledge foundation model from text and KG. It takes text segments and relevant KG subgraphs as input and bidirectionally fuses information from both modalities. Then, DRAGON utilizes two self-supervised reasoning tasks, i.e., masked language modeling and KG link prediction to optimize the model parameters. HKLM [238] introduces a unified LLM which incorporates KGs to learn representations of domain-specific knowledge.\n\n\n\n6.2 Synergized Reasoning\n\nTo better utilize the knowledge from text corpus and knowledge graph reasoning, Synergized Reasoning aims to design a synergized model that can effectively conduct reasoning with both LLMs and KGs.\nLLM-KG Fusion Reasoning.\nLLM-KG Fusion Reasoning leverages two separated LLM and KG encoders to process the text and relevant KG inputs [244]. These two encoders are equally important and jointly fusing the knowledge from two sources for reasoning. To improve the interaction between text and knowledge, KagNet [38] proposes to first encode the input KG, and then augment the input textual representation. In contrast, MHGRN [234] uses the final LLM outputs of the input text to guide the reasoning process on the KGs. Yet, both of them only design a single-direction interaction between the text and KGs. To tackle this issue, QA-GNN [131] proposes to use a GNN-based model to jointly reason over input context and KG information via message passing. Specifically, QA-GNN represents the input textual information as a special node via a pooling operation and connects this node with other entities in KG. However, the textual inputs are only pooled into a single dense vector, limiting the information fusion performance. JointLK [245] then proposes a framework with fine-grained interaction between any tokens in the textual inputs and any KG entities through LM-to-KG and KG-to-LM bi-directional attention mechanism. As shown in Fig. 24, pairwise dot-product scores are calculated over all textual tokens and KG entities, the bi-directional attentive scores are computed separately. In addition, at each jointLK layer, the KGs are also dynamically pruned based on the attention score to allow later layers to focus on more important sub-KG structures. Despite being effective, in JointLK, the fusion process between the input text and KG still uses the final LLM outputs as the input text representations. GreaseLM [178] designs deep and rich interaction between the input text tokens and KG entities at each layer of the LLMs. The architecture and fusion approach is mostly similar to ERNIE [35] discussed in Section 6.1, except that GreaseLM does not use the text-only T-encoder to handle input text.\n\nFigure 24: The framework of LLM-KG Fusion Reasoning.\n\nLLMs as Agents Reasoning.\nInstead using two encoders to fuse the knowledge, LLMs can also be treated as agents to interact with the KGs to conduct reasoning [246], as illustrated in Fig. 25. KD-CoT [247] iteratively retrieves facts from KGs and produces faithful reasoning traces, which guide LLMs to generate answers.\nKSL [239] teaches LLMs to search on KGs to retrieve relevant facts and then generate answers. StructGPT [237] designs several API interfaces to allow LLMs to access the structural data and perform reasoning by traversing on KGs. Think-on-graph [240] provides a flexible plug-and-play framework where LLM agents iteratively execute beam searches on KGs to discover the reasoning paths and generate answers. To enhance the agent abilities, AgentTuning [248] presents several instruction-tuning datasets to guide LLM agents to perform reasoning on KGs.\n\nFigure 25: Using LLMs as agents for reasoning on KGs.\n\nComparison and Discussion.\nLLM-KG Fusion Reasoning combines the LLM encoder and KG encoder to represent knowledge in a unified manner. It then employs a synergized reasoning module to jointly reason the results. This framework allows for different encoders and reasoning modules, which are trained end-to-end to effectively utilize the knowledge and reasoning capabilities of LLMs and KGs. However, these additional modules may introduce extra parameters and computational costs while lacking interpretability. LLMs as Agents for KG reasoning provides a flexible framework for reasoning on KGs without additional training cost, which can be generalized to different LLMs and KGs. Meanwhile, the reasoning process is interpretable, which can be used to explain the results. Nevertheless, defining the actions and policies for LLM agents is also challenging. The synergy of LLMs and KGs is still an ongoing research topic, with the potential to have more powerful frameworks in the future.\n\n\n\n\n7 Future Directions and Milestones\n\nIn this section, we discuss the future directions and several milestones in the research area of unifying KGs and LLMs.\n\n\n7.1 KGs for Hallucination Detection in LLMs\n\nThe hallucination problem in LLMs, which generates factually incorrect content, significantly hinders the reliability of LLMs. As discussed in Section 4, existing studies try to utilize KGs to obtain more reliable LLMs through pre-training or KG-enhanced inference. Despite the efforts, the issue of hallucination may continue to persist in the realm of LLMs for the foreseeable future. Consequently, in order to gain the public’s trust and border applications, it is imperative to detect and assess instances of hallucination within LLMs and other forms of AI-generated content (AIGC). Existing methods strive to detect hallucination by training a neural classifier on a small set of documents [249], which are neither robust nor powerful to handle ever-growing LLMs. Recently, researchers try to use KGs as an external source to validate LLMs [250]. Further studies combine LLMs and KGs to achieve a generalized fact-checking model that can detect hallucinations across domains [251]. Therefore, it opens a new door to utilizing KGs for hallucination detection.\n\n\n\n7.2 KGs for Editing Knowledge in LLMs\n\nAlthough LLMs are capable of storing massive real-world knowledge, they cannot quickly update their internal knowledge updated as real-world situations change. There are some research efforts proposed for editing knowledge in LLMs [252] without re-training the whole LLMs. Yet, such solutions still suffer from poor performance or computational overhead [253]. Existing studies [254] also reveal that edit a single fact would cause a ripple effect for other related knowledge. Therefore, it is necessary to develop a more efficient and effective method to edit knowledge in LLMs. Recently, researchers try to leverage KGs to edit knowledge in LLMs efficiently.\n\n\n\n7.3 KGs for Black-box LLMs Knowledge Injection\n\nAlthough pre-training and knowledge editing could update LLMs to catch up with the latest knowledge, they still need to access the internal structures and parameters of LLMs. However, many state-of-the-art large LLMs (e.g., ChatGPT) only provide APIs for users and developers to access, making themselves black-box to the public. Consequently, it is impossible to follow conventional KG injection approaches described [244, 38] that change LLM structure by adding additional knowledge fusion modules. Converting various types of knowledge into different text prompts seems to be a feasible solution. However, it is unclear whether these prompts can generalize well to new LLMs. Moreover, the prompt-based approach is limited to the length of input tokens of LLMs. Therefore, how to enable effective knowledge injection for black-box LLMs is still an open question for us to explore [255, 256].\n\n\n\n7.4 Multi-Modal LLMs for KGs\n\nCurrent knowledge graphs typically rely on textual and graph structure to handle KG-related applications. However, real-world knowledge graphs are often constructed by data from diverse modalities [257, 258, 99]. Therefore, effectively leveraging representations from multiple modalities would be a significant challenge for future research in KGs [259]. One potential solution is to develop methods that can accurately encode and align entities across different modalities. Recently, with the development of multi-modal LLMs [260, 98], leveraging LLMs for modality alignment holds promise in this regard. But, bridging the gap between multi-modal LLMs and KG structure remains a crucial challenge in this field, demanding further investigation and advancements.\n\n\n\n7.5 LLMs for Understanding KG Structure\n\nConventional LLMs trained on plain text data are not designed to understand structured data like knowledge graphs. Thus, LLMs might not fully grasp or understand the information conveyed by the KG structure. A straightforward way is to linearize the structured data into a sentence that LLMs can understand. However, the scale of the KGs makes it impossible to linearize the whole KGs as input. Moreover, the linearization process may lose some underlying information in KGs. Therefore, it is necessary to develop LLMs that can directly understand the KG structure and reason over it [237].\n\nFigure 26: The milestones of unifying KGs and LLMs.\n\n\n\n\n7.6 Synergized LLMs and KGs for Birectional Reasoning\n\nKGs and LLMs are two complementary technologies that can synergize each other. However, the synergy of LLMs and KGs is less explored by existing researchers. A desired synergy of LLMs and KGs would involve leveraging the strengths of both technologies to overcome their individual limitations. LLMs, such as ChatGPT, excel in generating human-like text and understanding natural language, while KGs are structured databases that capture and represent knowledge in a structured manner. By combining their capabilities, we can create a powerful system that benefits from the contextual understanding of LLMs and the structured knowledge representation of KGs. To better unify LLMs and KGs, many advanced techniques need to be incorporated, such as multi-modal learning [261], graph neural network [262], and continuous learning [263]. Last, the synergy of LLMs and KGs can be applied to many real-world applications, such as search engines [100], recommender systems [10, 89], and drug discovery.\nWith a given application problem, we can apply a KG to perform a knowledge-driven search for potential goals and unseen data, and simultaneously start with LLMs to perform a data/text-driven inference to see what new data/goal items can be derived. When the knowledge-based search is combined with data/text-driven inference, they can mutually validate each other, resulting in efficient and effective solutions powered by dual-driving wheels.\nTherefore, we can anticipate increasing attention to unlock the potential of integrating KGs and LLMs for diverse downstream applications with both generative and reasoning capabilities in the near future.\n\n\n\n\n8 Conclusion\n\nUnifying large language models (LLMs) and knowledge graphs (KGs) is an active research direction that has attracted increasing attention from both academia and industry. In this article, we provide a thorough overview of the recent research in this field. We first introduce different manners that integrate KGs to enhance LLMs. Then, we introduce existing methods that apply LLMs for KGs and establish taxonomy based on varieties of KG tasks. Finally, we discuss the challenges and future directions in this field.\nWe envision that there will be multiple stages (milestones) in the roadmap of unifying KGs and LLMs, as shown in Fig. 26. In particular, we will anticipate increasing research on three stages: Stage 1: KG-enhanced LLMs, LLM-augmented KGs, Stage 2: Synergized LLMs + KGs, and Stage 3: Graph Structure Understanding, Multi-modality, Knowledge Updating. We hope that this article will provide a guideline to advance future research.\n\n\n\nAcknowledgments\nThis research was supported by the Australian Research Council (ARC) under grants FT210100097 and DP240101547 and the National Natural Science Foundation of China (NSFC) under grant 62120106008.\n\n\nReferences\n\n\n[1]\n\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” arXiv preprint arXiv:1810.04805, 2018.\n\n\n\n\n[2]\n\nY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert pretraining approach,” arXiv preprint arXiv:1907.11692, 2019.\n\n\n\n\n[3]\n\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer,” The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485–5551, 2020.\n\n\n\n\n[4]\n\nD. Su, Y. Xu, G. I. Winata, P. Xu, H. Kim, Z. Liu, and P. Fung, “Generalizing question answering system with pre-trained language model fine-tuning,” in Proceedings of the 2nd Workshop on Machine Reading for Question Answering, 2019, pp. 203–211.\n\n\n\n\n[5]\n\nM. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,” in ACL, 2020, pp. 7871–7880.\n\n\n\n\n[6]\n\nJ. Li, T. Tang, W. X. Zhao, and J.-R. Wen, “Pretrained language models for text generation: A survey,” arXiv preprint arXiv:2105.10311, 2021.\n\n\n\n\n[7]\n\nJ. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler et al., “Emergent abilities of large language models,” Transactions on Machine Learning Research.\n\n\n\n\n[8]\n\nK. Malinka, M. Perešíni, A. Firc, O. Hujňák, and F. Januš, “On the educational impact of chatgpt: Is artificial intelligence ready to obtain a university degree?” arXiv preprint arXiv:2303.11146, 2023.\n\n\n\n\n[9]\n\nZ. Li, C. Wang, Z. Liu, H. Wang, S. Wang, and C. Gao, “Cctest: Testing and repairing code completion systems,” ICSE, 2023.\n\n\n\n\n[10]\n\nJ. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang, “Is chatgpt a good recommender? a preliminary study,” arXiv preprint arXiv:2304.10149, 2023.\n\n\n\n\n[11]\n\nW. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., “A survey of large language models,” arXiv preprint arXiv:2303.18223, 2023.\n\n\n\n\n[12]\n\nX. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained models for natural language processing: A survey,” Science China Technological Sciences, vol. 63, no. 10, pp. 1872–1897, 2020.\n\n\n\n\n[13]\n\nJ. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, and X. Hu, “Harnessing the power of llms in practice: A survey on chatgpt and beyond,” arXiv preprint arXiv:2304.13712, 2023.\n\n\n\n\n[14]\n\nF. Petroni, T. Rocktäschel, S. Riedel, P. Lewis, A. Bakhtin, Y. Wu, and A. Miller, “Language models as knowledge bases?” in EMNLP-IJCNLP, 2019, pp. 2463–2473.\n\n\n\n\n[15]\n\nZ. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung, “Survey of hallucination in natural language generation,” ACM Computing Surveys, vol. 55, no. 12, pp. 1–38, 2023.\n\n\n\n\n[16]\n\nH. Zhang, H. Song, S. Li, M. Zhou, and D. Song, “A survey of controllable text generation using transformer-based pre-trained language models,” arXiv preprint arXiv:2201.05337, 2022.\n\n\n\n\n[17]\n\nM. Danilevsky, K. Qian, R. Aharonov, Y. Katsis, B. Kawas, and P. Sen, “A survey of the state of explainable ai for natural language processing,” arXiv preprint arXiv:2010.00711, 2020.\n\n\n\n\n[18]\n\nJ. Wang, X. Hu, W. Hou, H. Chen, R. Zheng, Y. Wang, L. Yang, H. Huang, W. Ye, X. Geng et al., “On the robustness of chatgpt: An adversarial and out-of-distribution perspective,” arXiv preprint arXiv:2302.12095, 2023.\n\n\n\n\n[19]\n\nS. Ji, S. Pan, E. Cambria, P. Marttinen, and S. Y. Philip, “A survey on knowledge graphs: Representation, acquisition, and applications,” IEEE TNNLS, vol. 33, no. 2, pp. 494–514, 2021.\n\n\n\n\n[20]\n\nD. Vrandečić and M. Krötzsch, “Wikidata: a free collaborative knowledgebase,” Communications of the ACM, vol. 57, no. 10, pp. 78–85, 2014.\n\n\n\n\n[21]\n\nS. Hu, L. Zou, and X. Zhang, “A state-transition framework to answer complex questions over knowledge base,” in EMNLP, 2018, pp. 2098–2108.\n\n\n\n\n[22]\n\nJ. Zhang, B. Chen, L. Zhang, X. Ke, and H. Ding, “Neural, symbolic and neural-symbolic reasoning on knowledge graphs,” AI Open, vol. 2, pp. 14–35, 2021.\n\n\n\n\n[23]\n\nB. Abu-Salih, “Domain-specific knowledge graphs: A survey,” Journal of Network and Computer Applications, vol. 185, p. 103076, 2021.\n\n\n\n\n[24]\n\nT. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, B. Yang, J. Betteridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel, K. Jayant, L. Ni, M. Kathryn, M. Thahir, N. Ndapandula, P. Emmanouil, R. Alan, S. Mehdi, S. Burr, W. Derry, G. Abhinav, C. Xi, S. Abulhair, and W. Joel, “Never-ending learning,” Communications of the ACM, vol. 61, no. 5, pp. 103–115, 2018.\n\n\n\n\n[25]\n\nL. Zhong, J. Wu, Q. Li, H. Peng, and X. Wu, “A comprehensive survey on automatic knowledge graph construction,” arXiv preprint arXiv:2302.05019, 2023.\n\n\n\n\n[26]\n\nL. Yao, C. Mao, and Y. Luo, “Kg-bert: Bert for knowledge graph completion,” arXiv preprint arXiv:1909.03193, 2019.\n\n\n\n\n[27]\n\nL. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Normalizing flow-based neural process for few-shot knowledge graph completion,” SIGIR, 2023.\n\n\n\n\n[28]\n\nY. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung et al., “A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity,” arXiv preprint arXiv:2302.04023, 2023.\n\n\n\n\n[29]\n\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou, “Self-consistency improves chain of thought reasoning in language models,” arXiv preprint arXiv:2203.11171, 2022.\n\n\n\n\n[30]\n\nO. Golovneva, M. Chen, S. Poff, M. Corredor, L. Zettlemoyer, M. Fazel-Zarandi, and A. Celikyilmaz, “Roscoe: A suite of metrics for scoring step-by-step reasoning,” ICLR, 2023.\n\n\n\n\n[31]\n\nF. M. Suchanek, G. Kasneci, and G. Weikum, “Yago: a core of semantic knowledge,” in WWW, 2007, pp. 697–706.\n\n\n\n\n[32]\n\nA. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. Hruschka, and T. Mitchell, “Toward an architecture for never-ending language learning,” in Proceedings of the AAAI conference on artificial intelligence, vol. 24, no. 1, 2010, pp. 1306–1313.\n\n\n\n\n[33]\n\nA. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko, “Translating embeddings for modeling multi-relational data,” NeurIPS, vol. 26, 2013.\n\n\n\n\n[34]\n\nG. Wan, S. Pan, C. Gong, C. Zhou, and G. Haffari, “Reasoning like human: Hierarchical reinforcement learning for knowledge graph reasoning,” in AAAI, 2021, pp. 1926–1932.\n\n\n\n\n[35]\n\nZ. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, “ERNIE: Enhanced language representation with informative entities,” in ACL, 2019, pp. 1441–1451.\n\n\n\n\n[36]\n\nW. Liu, P. Zhou, Z. Zhao, Z. Wang, Q. Ju, H. Deng, and P. Wang, “K-BERT: enabling language representation with knowledge graph,” in AAAI, 2020, pp. 2901–2908.\n\n\n\n\n[37]\n\nY. Liu, Y. Wan, L. He, H. Peng, and P. S. Yu, “KG-BART: knowledge graph-augmented BART for generative commonsense reasoning,” in AAAI, 2021, pp. 6418–6425.\n\n\n\n\n[38]\n\nB. Y. Lin, X. Chen, J. Chen, and X. Ren, “KagNet: Knowledge-aware graph networks for commonsense reasoning,” in EMNLP-IJCNLP, 2019, pp. 2829–2839.\n\n\n\n\n[39]\n\nD. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei, “Knowledge neurons in pretrained transformers,” arXiv preprint arXiv:2104.08696, 2021.\n\n\n\n\n[40]\n\nX. Wang, T. Gao, Z. Zhu, Z. Zhang, Z. Liu, J. Li, and J. Tang, “KEPLER: A unified model for knowledge embedding and pre-trained language representation,” Transactions of the Association for Computational Linguistics, vol. 9, pp. 176–194, 2021.\n\n\n\n\n[41]\n\nI. Melnyk, P. Dognin, and P. Das, “Grapher: Multi-stage knowledge graph construction using pretrained language models,” in NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.\n\n\n\n\n[42]\n\nP. Ke, H. Ji, Y. Ran, X. Cui, L. Wang, L. Song, X. Zhu, and M. Huang, “JointGT: Graph-text joint representation learning for text generation from knowledge graphs,” in ACL Finding, 2021, pp. 2526–2538.\n\n\n\n\n[43]\n\nJ. Jiang, K. Zhou, W. X. Zhao, and J.-R. Wen, “Unikgqa: Unified retrieval and reasoning for solving multi-hop question answering over knowledge graph,” ICLR 2023, 2023.\n\n\n\n\n[44]\n\nM. Yasunaga, A. Bosselut, H. Ren, X. Zhang, C. D. Manning, P. S. Liang, and J. Leskovec, “Deep bidirectional language-knowledge graph pretraining,” NeurIPS, vol. 35, pp. 37 309–37 323, 2022.\n\n\n\n\n[45]\n\nN. Choudhary and C. K. Reddy, “Complex logical reasoning over knowledge graphs using large language models,” arXiv preprint arXiv:2305.01157, 2023.\n\n\n\n\n[46]\n\nS. Wang, Z. Wei, J. Xu, and Z. Fan, “Unifying structure reasoning and language model pre-training for complex reasoning,” arXiv preprint arXiv:2301.08913, 2023.\n\n\n\n\n[47]\n\nC. Zhen, Y. Shang, X. Liu, Y. Li, Y. Chen, and D. Zhang, “A survey on knowledge-enhanced pre-trained language models,” arXiv preprint arXiv:2212.13428, 2022.\n\n\n\n\n[48]\n\nX. Wei, S. Wang, D. Zhang, P. Bhatia, and A. Arnold, “Knowledge enhanced pretrained language models: A compreshensive survey,” arXiv preprint arXiv:2110.08455, 2021.\n\n\n\n\n[49]\n\nD. Yin, L. Dong, H. Cheng, X. Liu, K.-W. Chang, F. Wei, and J. Gao, “A survey of knowledge-intensive nlp with pre-trained language models,” arXiv preprint arXiv:2202.08772, 2022.\n\n\n\n\n[50]\n\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” NeurIPS, vol. 30, 2017.\n\n\n\n\n[51]\n\nZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “Albert: A lite bert for self-supervised learning of language representations,” in ICLR, 2019.\n\n\n\n\n[52]\n\nK. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “Electra: Pre-training text encoders as discriminators rather than generators,” arXiv preprint arXiv:2003.10555, 2020.\n\n\n\n\n[53]\n\nK. Hakala and S. Pyysalo, “Biomedical named entity recognition with multilingual bert,” in Proceedings of the 5th workshop on BioNLP open shared tasks, 2019, pp. 56–61.\n\n\n\n\n[54]\n\nY. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, D. Bahri, T. Schuster, S. Zheng et al., “Ul2: Unifying language learning paradigms,” in ICLR, 2022.\n\n\n\n\n[55]\n\nV. Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey et al., “Multitask prompted training enables zero-shot task generalization,” in ICLR, 2022.\n\n\n\n\n[56]\n\nB. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus, “St-moe: Designing stable and transferable sparse expert models,” URL https://arxiv. org/abs/2202.08906, 2022.\n\n\n\n\n[57]\n\nA. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, W. L. Tam, Z. Ma, Y. Xue, J. Zhai, W. Chen, Z. Liu, P. Zhang, Y. Dong, and J. Tang, “GLM-130b: An open bilingual pre-trained model,” in ICLR, 2023.\n\n\n\n\n[58]\n\nL. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained text-to-text transformer,” in NAACL, 2021, pp. 483–498.\n\n\n\n\n[59]\n\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models are few-shot learners,” Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020.\n\n\n\n\n[60]\n\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language models to follow instructions with human feedback,” NeurIPS, vol. 35, pp. 27 730–27 744, 2022.\n\n\n\n\n[61]\n\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023.\n\n\n\n\n[62]\n\nE. Saravia, “Prompt Engineering Guide,” https://github.com/dair-ai/Prompt-Engineering-Guide, 2022, accessed: 2022-12.\n\n\n\n\n[63]\n\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou et al., “Chain-of-thought prompting elicits reasoning in large language models,” in NeurIPS.\n\n\n\n\n[64]\n\nS. Li, Y. Gao, H. Jiang, Q. Yin, Z. Li, X. Yan, C. Zhang, and B. Yin, “Graph reasoning for question answering with triplet retrieval,” in ACL, 2023.\n\n\n\n\n[65]\n\nY. Wen, Z. Wang, and J. Sun, “Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models,” arXiv preprint arXiv:2308.09729, 2023.\n\n\n\n\n[66]\n\nK. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor, “Freebase: A collaboratively created graph database for structuring human knowledge,” in SIGMOD, 2008, pp. 1247–1250.\n\n\n\n\n[67]\n\nS. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives, “Dbpedia: A nucleus for a web of open data,” in The Semantic Web: 6th International Semantic Web Conference.   Springer, 2007, pp. 722–735.\n\n\n\n\n[68]\n\nB. Xu, Y. Xu, J. Liang, C. Xie, B. Liang, W. Cui, and Y. Xiao, “Cn-dbpedia: A never-ending chinese knowledge extraction system,” in 30th International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems.   Springer, 2017, pp. 428–438.\n\n\n\n\n[69]\n\nP. Hai-Nyzhnyk, “Vikidia as a universal multilingual online encyclopedia for children,” The Encyclopedia Herald of Ukraine, vol. 14, 2022.\n\n\n\n\n[70]\n\nF. Ilievski, P. Szekely, and B. Zhang, “Cskg: The commonsense knowledge graph,” Extended Semantic Web Conference (ESWC), 2021.\n\n\n\n\n[71]\n\nR. Speer, J. Chin, and C. Havasi, “Conceptnet 5.5: An open multilingual graph of general knowledge,” in Proceedings of the AAAI conference on artificial intelligence, vol. 31, no. 1, 2017.\n\n\n\n\n[72]\n\nH. Ji, P. Ke, S. Huang, F. Wei, X. Zhu, and M. Huang, “Language generation with multi-hop reasoning on commonsense knowledge graph,” in EMNLP, 2020, pp. 725–736.\n\n\n\n\n[73]\n\nJ. D. Hwang, C. Bhagavatula, R. Le Bras, J. Da, K. Sakaguchi, A. Bosselut, and Y. Choi, “(comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs,” in AAAI, vol. 35, no. 7, 2021, pp. 6384–6392.\n\n\n\n\n[74]\n\nH. Zhang, X. Liu, H. Pan, Y. Song, and C. W.-K. Leung, “Aser: A large-scale eventuality knowledge graph,” in Proceedings of the web conference 2020, 2020, pp. 201–211.\n\n\n\n\n[75]\n\nH. Zhang, D. Khashabi, Y. Song, and D. Roth, “Transomcs: from linguistic graphs to commonsense knowledge,” in IJCAI, 2021, pp. 4004–4010.\n\n\n\n\n[76]\n\nZ. Li, X. Ding, T. Liu, J. E. Hu, and B. Van Durme, “Guided generation of cause and effect,” in IJCAI, 2020.\n\n\n\n\n[77]\n\nO. Bodenreider, “The unified medical language system (umls): integrating biomedical terminology,” Nucleic acids research, vol. 32, no. suppl_1, pp. D267–D270, 2004.\n\n\n\n\n[78]\n\nY. Liu, Q. Zeng, J. Ordieres Meré, and H. Yang, “Anticipating stock market of the renowned companies: a knowledge graph approach,” Complexity, vol. 2019, 2019.\n\n\n\n\n[79]\n\nY. Zhu, W. Zhou, Y. Xu, J. Liu, Y. Tan et al., “Intelligent learning for knowledge graph towards geological data,” Scientific Programming, vol. 2017, 2017.\n\n\n\n\n[80]\n\nW. Choi and H. Lee, “Inference of biomedical relations among chemicals, genes, diseases, and symptoms using knowledge representation learning,” IEEE Access, vol. 7, pp. 179 373–179 384, 2019.\n\n\n\n\n[81]\n\nF. Farazi, M. Salamanca, S. Mosbach, J. Akroyd, A. Eibeck, L. K. Aditya, A. Chadzynski, K. Pan, X. Zhou, S. Zhang et al., “Knowledge graph approach to combustion chemistry and interoperability,” ACS omega, vol. 5, no. 29, pp. 18 342–18 348, 2020.\n\n\n\n\n[82]\n\nX. Wu, T. Jiang, Y. Zhu, and C. Bu, “Knowledge graph for china’s genealogy,” IEEE TKDE, vol. 35, no. 1, pp. 634–646, 2023.\n\n\n\n\n[83]\n\nX. Zhu, Z. Li, X. Wang, X. Jiang, P. Sun, X. Wang, Y. Xiao, and N. J. Yuan, “Multi-modal knowledge graph construction and application: A survey,” IEEE TKDE, 2022.\n\n\n\n\n[84]\n\nS. Ferrada, B. Bustos, and A. Hogan, “Imgpedia: a linked dataset with content-based analysis of wikimedia images,” in The Semantic Web–ISWC 2017.   Springer, 2017, pp. 84–93.\n\n\n\n\n[85]\n\nY. Liu, H. Li, A. Garcia-Duran, M. Niepert, D. Onoro-Rubio, and D. S. Rosenblum, “Mmkg: multi-modal knowledge graphs,” in The Semantic Web: 16th International Conference, ESWC 2019, Portorož, Slovenia, June 2–6, 2019, Proceedings 16.   Springer, 2019, pp. 459–474.\n\n\n\n\n[86]\n\nM. Wang, H. Wang, G. Qi, and Q. Zheng, “Richpedia: a large-scale, comprehensive multi-modal knowledge graph,” Big Data Research, vol. 22, p. 100159, 2020.\n\n\n\n\n[87]\n\nB. Shi, L. Ji, P. Lu, Z. Niu, and N. Duan, “Knowledge aware semantic concept expansion for image-text matching.” in IJCAI, vol. 1, 2019, p. 2.\n\n\n\n\n[88]\n\nS. Shah, A. Mishra, N. Yadati, and P. P. Talukdar, “Kvqa: Knowledge-aware visual question answering,” in AAAI, vol. 33, no. 01, 2019, pp. 8876–8884.\n\n\n\n\n[89]\n\nR. Sun, X. Cao, Y. Zhao, J. Wan, K. Zhou, F. Zhang, Z. Wang, and K. Zheng, “Multi-modal knowledge graphs for recommender systems,” in CIKM, 2020, pp. 1405–1414.\n\n\n\n\n[90]\n\nS. Deng, C. Wang, Z. Li, N. Zhang, Z. Dai, H. Chen, F. Xiong, M. Yan, Q. Chen, M. Chen, J. Chen, J. Z. Pan, B. Hooi, and H. Chen, “Construction and applications of billion-scale pre-trained multimodal business knowledge graph,” in ICDE, 2023.\n\n\n\n\n[91]\n\nC. Rosset, C. Xiong, M. Phan, X. Song, P. Bennett, and S. Tiwary, “Knowledge-aware language model pretraining,” arXiv preprint arXiv:2007.00655, 2020.\n\n\n\n\n[92]\n\nP. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive nlp tasks,” in NeurIPS, vol. 33, 2020, pp. 9459–9474.\n\n\n\n\n[93]\n\nY. Zhu, X. Wang, J. Chen, S. Qiao, Y. Ou, Y. Yao, S. Deng, H. Chen, and N. Zhang, “Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities,” arXiv preprint arXiv:2305.13168, 2023.\n\n\n\n\n[94]\n\nZ. Zhang, X. Liu, Y. Zhang, Q. Su, X. Sun, and B. He, “Pretrain-kge: learning knowledge representation from pretrained language models,” in EMNLP Finding, 2020, pp. 259–266.\n\n\n\n\n[95]\n\nA. Kumar, A. Pandey, R. Gadia, and M. Mishra, “Building knowledge graph using pre-trained language model for learning entity-aware relationships,” in 2020 IEEE International Conference on Computing, Power and Communication Technologies (GUCON).   IEEE, 2020, pp. 310–315.\n\n\n\n\n[96]\n\nX. Xie, N. Zhang, Z. Li, S. Deng, H. Chen, F. Xiong, M. Chen, and H. Chen, “From discrimination to generation: Knowledge graph completion with generative transformer,” in WWW, 2022, pp. 162–165.\n\n\n\n\n[97]\n\nZ. Chen, C. Xu, F. Su, Z. Huang, and Y. Dou, “Incorporating structured sentences with time-enhanced bert for fully-inductive temporal relation prediction,” SIGIR, 2023.\n\n\n\n\n[98]\n\nD. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4: Enhancing vision-language understanding with advanced large language models,” arXiv preprint arXiv:2304.10592, 2023.\n\n\n\n\n[99]\n\nM. Warren, D. A. Shamma, and P. J. Hayes, “Knowledge engineering with image data in real-world settings,” in AAAI, ser. CEUR Workshop Proceedings, vol. 2846, 2021.\n\n\n\n\n[100]\n\nR. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al., “Lamda: Language models for dialog applications,” arXiv preprint arXiv:2201.08239, 2022.\n\n\n\n\n[101]\n\nY. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu et al., “Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation,” arXiv preprint arXiv:2107.02137, 2021.\n\n\n\n\n[102]\n\nT. Shen, Y. Mao, P. He, G. Long, A. Trischler, and W. Chen, “Exploiting structured knowledge in text via graph-guided representation learning,” in EMNLP, 2020, pp. 8980–8994.\n\n\n\n\n[103]\n\nD. Zhang, Z. Yuan, Y. Liu, F. Zhuang, H. Chen, and H. Xiong, “E-bert: A phrase and product knowledge enhanced language model for e-commerce,” arXiv preprint arXiv:2009.02835, 2020.\n\n\n\n\n[104]\n\nS. Li, X. Li, L. Shang, C. Sun, B. Liu, Z. Ji, X. Jiang, and Q. Liu, “Pre-training language models with deterministic factual knowledge,” in EMNLP, 2022, pp. 11 118–11 131.\n\n\n\n\n[105]\n\nM. Kang, J. Baek, and S. J. Hwang, “Kala: Knowledge-augmented language model adaptation,” in NAACL, 2022, pp. 5144–5167.\n\n\n\n\n[106]\n\nW. Xiong, J. Du, W. Y. Wang, and V. Stoyanov, “Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model,” in ICLR, 2020.\n\n\n\n\n[107]\n\nT. Sun, Y. Shao, X. Qiu, Q. Guo, Y. Hu, X. Huang, and Z. Zhang, “CoLAKE: Contextualized language and knowledge embedding,” in Proceedings of the 28th International Conference on Computational Linguistics, 2020, pp. 3660–3670.\n\n\n\n\n[108]\n\nT. Zhang, C. Wang, N. Hu, M. Qiu, C. Tang, X. He, and J. Huang, “DKPLM: decomposable knowledge-enhanced pre-trained language model for natural language understanding,” in AAAI, 2022, pp. 11 703–11 711.\n\n\n\n\n[109]\n\nJ. Wang, W. Huang, M. Qiu, Q. Shi, H. Wang, X. Li, and M. Gao, “Knowledge prompting in pre-trained language model for natural language understanding,” in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022, pp. 3164–3177.\n\n\n\n\n[110]\n\nH. Ye, N. Zhang, S. Deng, X. Chen, H. Chen, F. Xiong, X. Chen, and H. Chen, “Ontology-enhanced prompt-tuning for few-shot learning,” in Proceedings of the ACM Web Conference 2022, 2022, pp. 778–787.\n\n\n\n\n[111]\n\nH. Luo, Z. Tang, S. Peng, Y. Guo, W. Zhang, C. Ma, G. Dong, M. Song, W. Lin et al., “Chatkbqa: A generate-then-retrieve framework for knowledge base question answering with fine-tuned large language models,” arXiv preprint arXiv:2310.08975, 2023.\n\n\n\n\n[112]\n\nL. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faithful and interpretable large language model reasoning,” arXiv preprint arxiv:2310.01061, 2023.\n\n\n\n\n[113]\n\nR. Logan, N. F. Liu, M. E. Peters, M. Gardner, and S. Singh, “Barack’s wife hillary: Using knowledge graphs for fact-aware language modeling,” in ACL, 2019, pp. 5962–5971.\n\n\n\n\n[114]\n\nK. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang, “Realm: Retrieval-augmented language model pre-training,” in ICML, 2020.\n\n\n\n\n[115]\n\nY. Wu, Y. Zhao, B. Hu, P. Minervini, P. Stenetorp, and S. Riedel, “An efficient memory-augmented transformer for knowledge-intensive NLP tasks,” in EMNLP, 2022, pp. 5184–5196.\n\n\n\n\n[116]\n\nL. Luo, J. Ju, B. Xiong, Y.-F. Li, G. Haffari, and S. Pan, “Chatrule: Mining logical rules with large language models for knowledge graph reasoning,” arXiv preprint arXiv:2309.01538, 2023.\n\n\n\n\n[117]\n\nJ. Wang, Q. Sun, N. Chen, X. Li, and M. Gao, “Boosting language models reasoning with chain-of-knowledge prompting,” arXiv preprint arXiv:2306.06427, 2023.\n\n\n\n\n[118]\n\nZ. Jiang, F. F. Xu, J. Araki, and G. Neubig, “How can we know what language models know?” Transactions of the Association for Computational Linguistics, vol. 8, pp. 423–438, 2020.\n\n\n\n\n[119]\n\nT. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh, “Autoprompt: Eliciting knowledge from language models with automatically generated prompts,” arXiv preprint arXiv:2010.15980, 2020.\n\n\n\n\n[120]\n\nZ. Meng, F. Liu, E. Shareghi, Y. Su, C. Collins, and N. Collier, “Rewire-then-probe: A contrastive recipe for probing biomedical knowledge of pre-trained language models,” arXiv preprint arXiv:2110.08173, 2021.\n\n\n\n\n[121]\n\nL. Luo, T.-T. Vu, D. Phung, and G. Haffari, “Systematic assessment of factual knowledge in large language models,” in EMNLP, 2023.\n\n\n\n\n[122]\n\nV. Swamy, A. Romanou, and M. Jaggi, “Interpreting language models through knowledge graph extraction,” arXiv preprint arXiv:2111.08546, 2021.\n\n\n\n\n[123]\n\nS. Li, X. Li, L. Shang, Z. Dong, C. Sun, B. Liu, Z. Ji, X. Jiang, and Q. Liu, “How pre-trained language models capture factual knowledge? a causal-inspired analysis,” arXiv preprint arXiv:2203.16747, 2022.\n\n\n\n\n[124]\n\nH. Tian, C. Gao, X. Xiao, H. Liu, B. He, H. Wu, H. Wang, and F. Wu, “SKEP: Sentiment knowledge enhanced pre-training for sentiment analysis,” in ACL, 2020, pp. 4067–4076.\n\n\n\n\n[125]\n\nW. Yu, C. Zhu, Y. Fang, D. Yu, S. Wang, Y. Xu, M. Zeng, and M. Jiang, “Dict-BERT: Enhancing language model pre-training with dictionary,” in ACL, 2022, pp. 1907–1918.\n\n\n\n\n[126]\n\nT. McCoy, E. Pavlick, and T. Linzen, “Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference,” in ACL, 2019, pp. 3428–3448.\n\n\n\n\n[127]\n\nD. Wilmot and F. Keller, “Memory and knowledge augmented language models for inferring salience in long-form stories,” in EMNLP, 2021, pp. 851–865.\n\n\n\n\n[128]\n\nL. Adolphs, S. Dhuliawala, and T. Hofmann, “How to query language models?” arXiv preprint arXiv:2108.01928, 2021.\n\n\n\n\n[129]\n\nM. Sung, J. Lee, S. Yi, M. Jeon, S. Kim, and J. Kang, “Can language models be biomedical knowledge bases?” in EMNLP, 2021, pp. 4723–4734.\n\n\n\n\n[130]\n\nA. Mallen, A. Asai, V. Zhong, R. Das, H. Hajishirzi, and D. Khashabi, “When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories,” arXiv preprint arXiv:2212.10511, 2022.\n\n\n\n\n[131]\n\nM. Yasunaga, H. Ren, A. Bosselut, P. Liang, and J. Leskovec, “QA-GNN: Reasoning with language models and knowledge graphs for question answering,” in NAACL, 2021, pp. 535–546.\n\n\n\n\n[132]\n\nM. Nayyeri, Z. Wang, M. Akter, M. M. Alam, M. R. A. H. Rony, J. Lehmann, S. Staab et al., “Integrating knowledge graph embedding and pretrained language models in hypercomplex spaces,” arXiv preprint arXiv:2208.02743, 2022.\n\n\n\n\n[133]\n\nN. Huang, Y. R. Deshpande, Y. Liu, H. Alberts, K. Cho, C. Vania, and I. Calixto, “Endowing language models with multimodal knowledge graph representations,” arXiv preprint arXiv:2206.13163, 2022.\n\n\n\n\n[134]\n\nM. M. Alam, M. R. A. H. Rony, M. Nayyeri, K. Mohiuddin, M. M. Akter, S. Vahdati, and J. Lehmann, “Language model guided knowledge graph embeddings,” IEEE Access, vol. 10, pp. 76 008–76 020, 2022.\n\n\n\n\n[135]\n\nX. Wang, Q. He, J. Liang, and Y. Xiao, “Language models as knowledge embeddings,” arXiv preprint arXiv:2206.12617, 2022.\n\n\n\n\n[136]\n\nN. Zhang, X. Xie, X. Chen, S. Deng, C. Tan, F. Huang, X. Cheng, and H. Chen, “Reasoning through memorization: Nearest neighbor knowledge graph embeddings,” arXiv preprint arXiv:2201.05575, 2022.\n\n\n\n\n[137]\n\nX. Xie, Z. Li, X. Wang, Y. Zhu, N. Zhang, J. Zhang, S. Cheng, B. Tian, S. Deng, F. Xiong, and H. Chen, “Lambdakg: A library for pre-trained language model-based knowledge graph embeddings,” 2022.\n\n\n\n\n[138]\n\nB. Kim, T. Hong, Y. Ko, and J. Seo, “Multi-task learning for knowledge graph completion with pre-trained language models,” in COLING, 2020, pp. 1737–1743.\n\n\n\n\n[139]\n\nX. Lv, Y. Lin, Y. Cao, L. Hou, J. Li, Z. Liu, P. Li, and J. Zhou, “Do pre-trained models benefit knowledge graph completion? A reliable evaluation and a reasonable approach,” in ACL, 2022, pp. 3570–3581.\n\n\n\n\n[140]\n\nJ. Shen, C. Wang, L. Gong, and D. Song, “Joint language semantic and structure embedding for knowledge graph completion,” in COLING, 2022, pp. 1965–1978.\n\n\n\n\n[141]\n\nB. Choi, D. Jang, and Y. Ko, “MEM-KGC: masked entity model for knowledge graph completion with pre-trained language model,” IEEE Access, vol. 9, pp. 132 025–132 032, 2021.\n\n\n\n\n[142]\n\nB. Choi and Y. Ko, “Knowledge graph extension with a pre-trained language model via unified learning method,” Knowl. Based Syst., vol. 262, p. 110245, 2023.\n\n\n\n\n[143]\n\nB. Wang, T. Shen, G. Long, T. Zhou, Y. Wang, and Y. Chang, “Structure-augmented text representation learning for efficient knowledge graph completion,” in WWW, 2021, pp. 1737–1748.\n\n\n\n\n[144]\n\nL. Wang, W. Zhao, Z. Wei, and J. Liu, “Simkgc: Simple contrastive knowledge graph completion with pre-trained language models,” in ACL, 2022, pp. 4281–4294.\n\n\n\n\n[145]\n\nD. Li, M. Yi, and Y. He, “Lp-bert: Multi-task pre-training knowledge graph bert for link prediction,” arXiv preprint arXiv:2201.04843, 2022.\n\n\n\n\n[146]\n\nA. Saxena, A. Kochsiek, and R. Gemulla, “Sequence-to-sequence knowledge graph completion and question answering,” in ACL, 2022, pp. 2814–2828.\n\n\n\n\n[147]\n\nC. Chen, Y. Wang, B. Li, and K. Lam, “Knowledge is flat: A seq2seq generative framework for various knowledge graph completion,” in COLING, 2022, pp. 4005–4017.\n\n\n\n\n[148]\n\nM. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, “Deep contextualized word representations,” in NAACL, 2018, pp. 2227–2237.\n\n\n\n\n[149]\n\nH. Yan, T. Gui, J. Dai, Q. Guo, Z. Zhang, and X. Qiu, “A unified generative framework for various NER subtasks,” in ACL, 2021, pp. 5808–5822.\n\n\n\n\n[150]\n\nY. Onoe and G. Durrett, “Learning to denoise distantly-labeled data for entity typing,” in NAACL, 2019, pp. 2407–2417.\n\n\n\n\n[151]\n\nY. Onoe, M. Boratko, A. McCallum, and G. Durrett, “Modeling fine-grained entity types with box embeddings,” in ACL, 2021, pp. 2051–2064.\n\n\n\n\n[152]\n\nB. Z. Li, S. Min, S. Iyer, Y. Mehdad, and W. Yih, “Efficient one-pass end-to-end entity linking for questions,” in EMNLP, 2020, pp. 6433–6441.\n\n\n\n\n[153]\n\nT. Ayoola, S. Tyagi, J. Fisher, C. Christodoulopoulos, and A. Pierleoni, “Refined: An efficient zero-shot-capable approach to end-to-end entity linking,” in NAACL, 2022, pp. 209–220.\n\n\n\n\n[154]\n\nM. Joshi, O. Levy, L. Zettlemoyer, and D. S. Weld, “BERT for coreference resolution: Baselines and analysis,” in EMNLP, 2019, pp. 5802–5807.\n\n\n\n\n[155]\n\nM. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy, “Spanbert: Improving pre-training by representing and predicting spans,” Trans. Assoc. Comput. Linguistics, vol. 8, pp. 64–77, 2020.\n\n\n\n\n[156]\n\nA. Caciularu, A. Cohan, I. Beltagy, M. E. Peters, A. Cattan, and I. Dagan, “CDLM: cross-document language modeling,” in EMNLP, 2021, pp. 2648–2662.\n\n\n\n\n[157]\n\nA. Cattan, A. Eirew, G. Stanovsky, M. Joshi, and I. Dagan, “Cross-document coreference resolution over predicted mentions,” in ACL, 2021, pp. 5100–5107.\n\n\n\n\n[158]\n\nY. Wang, Y. Shen, and H. Jin, “An end-to-end actor-critic-based neural coreference resolution system,” in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021, 2021, pp. 7848–7852.\n\n\n\n\n[159]\n\nP. Shi and J. Lin, “Simple BERT models for relation extraction and semantic role labeling,” CoRR, vol. abs/1904.05255, 2019.\n\n\n\n\n[160]\n\nS. Park and H. Kim, “Improving sentence-level relation extraction through curriculum learning,” CoRR, vol. abs/2107.09332, 2021.\n\n\n\n\n[161]\n\nY. Ma, A. Wang, and N. Okazaki, “DREEAM: guiding attention with evidence for improving document-level relation extraction,” in EACL, 2023, pp. 1963–1975.\n\n\n\n\n[162]\n\nQ. Guo, Y. Sun, G. Liu, Z. Wang, Z. Ji, Y. Shen, and X. Wang, “Constructing chinese historical literature knowledge graph based on bert,” in Web Information Systems and Applications: 18th International Conference, WISA 2021, Kaifeng, China, September 24–26, 2021, Proceedings 18.   Springer, 2021, pp. 323–334.\n\n\n\n\n[163]\n\nJ. Han, N. Collier, W. Buntine, and E. Shareghi, “Pive: Prompting with iterative verification improving graph-based generative capability of llms,” arXiv preprint arXiv:2305.12392, 2023.\n\n\n\n\n[164]\n\nA. Bosselut, H. Rashkin, M. Sap, C. Malaviya, A. Celikyilmaz, and Y. Choi, “Comet: Commonsense transformers for knowledge graph construction,” in ACL, 2019.\n\n\n\n\n[165]\n\nS. Hao, B. Tan, K. Tang, H. Zhang, E. P. Xing, and Z. Hu, “Bertnet: Harvesting knowledge graphs from pretrained language models,” arXiv preprint arXiv:2206.14268, 2022.\n\n\n\n\n[166]\n\nP. West, C. Bhagavatula, J. Hessel, J. Hwang, L. Jiang, R. Le Bras, X. Lu, S. Welleck, and Y. Choi, “Symbolic knowledge distillation: from general language models to commonsense models,” in NAACL, 2022, pp. 4602–4625.\n\n\n\n\n[167]\n\nL. F. R. Ribeiro, M. Schmitt, H. Schütze, and I. Gurevych, “Investigating pretrained language models for graph-to-text generation,” in Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI, 2021, pp. 211–227.\n\n\n\n\n[168]\n\nJ. Li, T. Tang, W. X. Zhao, Z. Wei, N. J. Yuan, and J.-R. Wen, “Few-shot knowledge graph-to-text generation with pretrained language models,” in ACL, 2021, pp. 1558–1568.\n\n\n\n\n[169]\n\nA. Colas, M. Alvandipour, and D. Z. Wang, “GAP: A graph-aware language model framework for knowledge graph-to-text generation,” in Proceedings of the 29th International Conference on Computational Linguistics, 2022, pp. 5755–5769.\n\n\n\n\n[170]\n\nZ. Jin, Q. Guo, X. Qiu, and Z. Zhang, “GenWiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation,” in Proceedings of the 28th International Conference on Computational Linguistics, 2020, pp. 2398–2409.\n\n\n\n\n[171]\n\nW. Chen, Y. Su, X. Yan, and W. Y. Wang, “KGPT: Knowledge-grounded pre-training for data-to-text generation,” in EMNLP, 2020, pp. 8635–8648.\n\n\n\n\n[172]\n\nD. Lukovnikov, A. Fischer, and J. Lehmann, “Pretrained transformers for simple question answering over knowledge graphs,” in The Semantic Web–ISWC 2019: 18th International Semantic Web Conference, Auckland, New Zealand, October 26–30, 2019, Proceedings, Part I 18.   Springer, 2019, pp. 470–486.\n\n\n\n\n[173]\n\nD. Luo, J. Su, and S. Yu, “A bert-based approach with relation-aware attention for knowledge base question answering,” in IJCNN.   IEEE, 2020, pp. 1–8.\n\n\n\n\n[174]\n\nN. Hu, Y. Wu, G. Qi, D. Min, J. Chen, J. Z. Pan, and Z. Ali, “An empirical study of pre-trained language models in simple knowledge graph question answering,” arXiv preprint arXiv:2303.10368, 2023.\n\n\n\n\n[175]\n\nY. Xu, C. Zhu, R. Xu, Y. Liu, M. Zeng, and X. Huang, “Fusing context into knowledge graph for commonsense question answering,” in ACL, 2021, pp. 1201–1207.\n\n\n\n\n[176]\n\nM. Zhang, R. Dai, M. Dong, and T. He, “Drlk: Dynamic hierarchical reasoning with language model and knowledge graph for question answering,” in EMNLP, 2022, pp. 5123–5133.\n\n\n\n\n[177]\n\nZ. Hu, Y. Xu, W. Yu, S. Wang, Z. Yang, C. Zhu, K.-W. Chang, and Y. Sun, “Empowering language models with knowledge graph reasoning for open-domain question answering,” in EMNLP, 2022, pp. 9562–9581.\n\n\n\n\n[178]\n\nX. Zhang, A. Bosselut, M. Yasunaga, H. Ren, P. Liang, C. D. Manning, and J. Leskovec, “Greaselm: Graph reasoning enhanced language models,” in ICLR, 2022.\n\n\n\n\n[179]\n\nX. Cao and Y. Liu, “Relmkg: reasoning with pre-trained language models and knowledge graphs for complex question answering,” Applied Intelligence, pp. 1–15, 2022.\n\n\n\n\n[180]\n\nX. Huang, J. Zhang, D. Li, and P. Li, “Knowledge graph embedding based question answering,” in WSDM, 2019, pp. 105–113.\n\n\n\n\n[181]\n\nH. Wang, F. Zhang, X. Xie, and M. Guo, “Dkn: Deep knowledge-aware network for news recommendation,” in WWW, 2018, pp. 1835–1844.\n\n\n\n\n[182]\n\nB. Yang, S. W.-t. Yih, X. He, J. Gao, and L. Deng, “Embedding entities and relations for learning and inference in knowledge bases,” in ICLR, 2015.\n\n\n\n\n[183]\n\nW. Xiong, M. Yu, S. Chang, X. Guo, and W. Y. Wang, “One-shot relational learning for knowledge graphs,” in EMNLP, 2018, pp. 1980–1990.\n\n\n\n\n[184]\n\nP. Wang, J. Han, C. Li, and R. Pan, “Logic attention based neighborhood aggregation for inductive knowledge graph embedding,” in AAAI, vol. 33, no. 01, 2019, pp. 7152–7159.\n\n\n\n\n[185]\n\nY. Lin, Z. Liu, M. Sun, Y. Liu, and X. Zhu, “Learning entity and relation embeddings for knowledge graph completion,” in Proceedings of the AAAI conference on artificial intelligence, vol. 29, no. 1, 2015.\n\n\n\n\n[186]\n\nC. Chen, Y. Wang, A. Sun, B. Li, and L. Kwok-Yan, “Dipping plms sauce: Bridging structure and text for effective knowledge graph completion via conditional soft prompting,” in ACL, 2023.\n\n\n\n\n[187]\n\nJ. Lovelace and C. P. Rosé, “A framework for adapting pre-trained language models to knowledge graph completion,” in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022, pp. 5937–5955.\n\n\n\n\n[188]\n\nJ. Fu, L. Feng, Q. Zhang, X. Huang, and P. Liu, “Larger-context tagging: When and why does it work?” in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, 2021, pp. 1463–1475.\n\n\n\n\n[189]\n\nX. Liu, K. Ji, Y. Fu, Z. Du, Z. Yang, and J. Tang, “P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks,” CoRR, vol. abs/2110.07602, 2021.\n\n\n\n\n[190]\n\nJ. Yu, B. Bohnet, and M. Poesio, “Named entity recognition as dependency parsing,” in ACL, 2020, pp. 6470–6476.\n\n\n\n\n[191]\n\nF. Li, Z. Lin, M. Zhang, and D. Ji, “A span-based model for joint overlapped and discontinuous named entity recognition,” in ACL, 2021, pp. 4814–4828.\n\n\n\n\n[192]\n\nC. Tan, W. Qiu, M. Chen, R. Wang, and F. Huang, “Boundary enhanced neural span classification for nested named entity recognition,” in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, 2020, pp. 9016–9023.\n\n\n\n\n[193]\n\nY. Xu, H. Huang, C. Feng, and Y. Hu, “A supervised multi-head self-attention network for nested named entity recognition,” in Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, 2021, pp. 14 185–14 193.\n\n\n\n\n[194]\n\nJ. Yu, B. Ji, S. Li, J. Ma, H. Liu, and H. Xu, “S-NER: A concise and efficient span-based model for named entity recognition,” Sensors, vol. 22, no. 8, p. 2852, 2022.\n\n\n\n\n[195]\n\nY. Fu, C. Tan, M. Chen, S. Huang, and F. Huang, “Nested named entity recognition with partially-observed treecrfs,” in AAAI, 2021, pp. 12 839–12 847.\n\n\n\n\n[196]\n\nC. Lou, S. Yang, and K. Tu, “Nested named entity recognition as latent lexicalized constituency parsing,” in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022, pp. 6183–6198.\n\n\n\n\n[197]\n\nS. Yang and K. Tu, “Bottom-up constituency parsing and nested named entity recognition with pointer networks,” in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022, pp. 2403–2416.\n\n\n\n\n[198]\n\nF. Li, Z. Lin, M. Zhang, and D. Ji, “A span-based model for joint overlapped and discontinuous named entity recognition,” in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, 2021, pp. 4814–4828.\n\n\n\n\n[199]\n\nQ. Liu, H. Lin, X. Xiao, X. Han, L. Sun, and H. Wu, “Fine-grained entity typing via label reasoning,” in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, 2021, pp. 4611–4622.\n\n\n\n\n[200]\n\nH. Dai, Y. Song, and H. Wang, “Ultra-fine entity typing with weak supervision from a masked language model,” in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, 2021, pp. 1790–1799.\n\n\n\n\n[201]\n\nN. Ding, Y. Chen, X. Han, G. Xu, X. Wang, P. Xie, H. Zheng, Z. Liu, J. Li, and H. Kim, “Prompt-learning for fine-grained entity typing,” in Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022, pp. 6888–6901.\n\n\n\n\n[202]\n\nW. Pan, W. Wei, and F. Zhu, “Automatic noisy label correction for fine-grained entity typing,” in Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022, 2022, pp. 4317–4323.\n\n\n\n\n[203]\n\nB. Li, W. Yin, and M. Chen, “Ultra-fine entity typing with indirect supervision from natural language inference,” Trans. Assoc. Comput. Linguistics, vol. 10, pp. 607–622, 2022.\n\n\n\n\n[204]\n\nS. Broscheit, “Investigating entity knowledge in BERT with simple neural end-to-end entity linking,” CoRR, vol. abs/2003.05473, 2020.\n\n\n\n\n[205]\n\nN. D. Cao, G. Izacard, S. Riedel, and F. Petroni, “Autoregressive entity retrieval,” in 9th ICLR, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.\n\n\n\n\n[206]\n\nN. D. Cao, L. Wu, K. Popat, M. Artetxe, N. Goyal, M. Plekhanov, L. Zettlemoyer, N. Cancedda, S. Riedel, and F. Petroni, “Multilingual autoregressive entity linking,” Trans. Assoc. Comput. Linguistics, vol. 10, pp. 274–290, 2022.\n\n\n\n\n[207]\n\nN. D. Cao, W. Aziz, and I. Titov, “Highly parallel autoregressive entity linking with discriminative correction,” in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, 2021, pp. 7662–7669.\n\n\n\n\n[208]\n\nK. Lee, L. He, and L. Zettlemoyer, “Higher-order coreference resolution with coarse-to-fine inference,” in NAACL, 2018, pp. 687–692.\n\n\n\n\n[209]\n\nT. M. Lai, T. Bui, and D. S. Kim, “End-to-end neural coreference resolution revisited: A simple yet effective baseline,” in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022, 2022, pp. 8147–8151.\n\n\n\n\n[210]\n\nW. Wu, F. Wang, A. Yuan, F. Wu, and J. Li, “Corefqa: Coreference resolution as query-based span prediction,” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, 2020, pp. 6953–6963.\n\n\n\n\n[211]\n\nT. M. Lai, H. Ji, T. Bui, Q. H. Tran, F. Dernoncourt, and W. Chang, “A context-dependent gated module for incorporating symbolic semantics into event coreference resolution,” in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, 2021, pp. 3491–3499.\n\n\n\n\n[212]\n\nY. Kirstain, O. Ram, and O. Levy, “Coreference resolution without span representations,” in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual Event, August 1-6, 2021, 2021, pp. 14–19.\n\n\n\n\n[213]\n\nR. Thirukovalluru, N. Monath, K. Shridhar, M. Zaheer, M. Sachan, and A. McCallum, “Scaling within document coreference to long texts,” in Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, ser. Findings of ACL, vol. ACL/IJCNLP 2021, 2021, pp. 3921–3931.\n\n\n\n\n[214]\n\nI. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-document transformer,” CoRR, vol. abs/2004.05150, 2020.\n\n\n\n\n[215]\n\nC. Alt, M. Hübner, and L. Hennig, “Improving relation extraction by pre-trained language representations,” in 1st Conference on Automated Knowledge Base Construction, AKBC 2019, Amherst, MA, USA, May 20-22, 2019, 2019.\n\n\n\n\n[216]\n\nL. B. Soares, N. FitzGerald, J. Ling, and T. Kwiatkowski, “Matching the blanks: Distributional similarity for relation learning,” in ACL, 2019, pp. 2895–2905.\n\n\n\n\n[217]\n\nS. Lyu and H. Chen, “Relation classification with entity type restriction,” in Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, ser. Findings of ACL, vol. ACL/IJCNLP 2021, 2021, pp. 390–395.\n\n\n\n\n[218]\n\nJ. Zheng and Z. Chen, “Sentence-level relation extraction via contrastive learning with descriptive relation prompts,” CoRR, vol. abs/2304.04935, 2023.\n\n\n\n\n[219]\n\nH. Wang, C. Focke, R. Sylvester, N. Mishra, and W. Y. Wang, “Fine-tune bert for docred with two-step process,” CoRR, vol. abs/1909.11898, 2019.\n\n\n\n\n[220]\n\nH. Tang, Y. Cao, Z. Zhang, J. Cao, F. Fang, S. Wang, and P. Yin, “HIN: hierarchical inference network for document-level relation extraction,” in PAKDD, ser. Lecture Notes in Computer Science, vol. 12084, 2020, pp. 197–209.\n\n\n\n\n[221]\n\nD. Wang, W. Hu, E. Cao, and W. Sun, “Global-to-local neural networks for document-level relation extraction,” in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, 2020, pp. 3711–3721.\n\n\n\n\n[222]\n\nS. Zeng, Y. Wu, and B. Chang, “SIRE: separate intra- and inter-sentential reasoning for document-level relation extraction,” in Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, ser. Findings of ACL, vol. ACL/IJCNLP 2021, 2021, pp. 524–534.\n\n\n\n\n[223]\n\nG. Nan, Z. Guo, I. Sekulic, and W. Lu, “Reasoning with latent structure refinement for document-level relation extraction,” in ACL, 2020, pp. 1546–1557.\n\n\n\n\n[224]\n\nS. Zeng, R. Xu, B. Chang, and L. Li, “Double graph based reasoning for document-level relation extraction,” in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, 2020, pp. 1630–1640.\n\n\n\n\n[225]\n\nN. Zhang, X. Chen, X. Xie, S. Deng, C. Tan, M. Chen, F. Huang, L. Si, and H. Chen, “Document-level relation extraction as semantic segmentation,” in IJCAI, 2021, pp. 3999–4006.\n\n\n\n\n[226]\n\nO. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015 - 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III, ser. Lecture Notes in Computer Science, vol. 9351, 2015, pp. 234–241.\n\n\n\n\n[227]\n\nW. Zhou, K. Huang, T. Ma, and J. Huang, “Document-level relation extraction with adaptive thresholding and localized context pooling,” in AAAI, 2021, pp. 14 612–14 620.\n\n\n\n\n[228]\n\nC. Gardent, A. Shimorina, S. Narayan, and L. Perez-Beltrachini, “The WebNLG challenge: Generating text from RDF data,” in Proceedings of the 10th International Conference on Natural Language Generation, 2017, pp. 124–133.\n\n\n\n\n[229]\n\nJ. Guan, Y. Wang, and M. Huang, “Story ending generation with incremental encoding and commonsense knowledge,” in AAAI, 2019, pp. 6473–6480.\n\n\n\n\n[230]\n\nH. Zhou, T. Young, M. Huang, H. Zhao, J. Xu, and X. Zhu, “Commonsense knowledge aware conversation generation with graph attention,” in IJCAI, 2018, pp. 4623–4629.\n\n\n\n\n[231]\n\nM. Kale and A. Rastogi, “Text-to-text pre-training for data-to-text tasks,” in Proceedings of the 13th International Conference on Natural Language Generation, 2020, pp. 97–102.\n\n\n\n\n[232]\n\nM. Mintz, S. Bills, R. Snow, and D. Jurafsky, “Distant supervision for relation extraction without labeled data,” in ACL, 2009, pp. 1003–1011.\n\n\n\n\n[233]\n\nA. Saxena, A. Tripathi, and P. Talukdar, “Improving multi-hop question answering over knowledge graphs using knowledge base embeddings,” in ACL, 2020, pp. 4498–4507.\n\n\n\n\n[234]\n\nY. Feng, X. Chen, B. Y. Lin, P. Wang, J. Yan, and X. Ren, “Scalable multi-hop relational reasoning for knowledge-aware question answering,” in EMNLP, 2020, pp. 1295–1309.\n\n\n\n\n[235]\n\nY. Yan, R. Li, S. Wang, H. Zhang, Z. Daoguang, F. Zhang, W. Wu, and W. Xu, “Large-scale relation learning for question answering over knowledge bases with pre-trained language models,” in EMNLP, 2021, pp. 3653–3660.\n\n\n\n\n[236]\n\nJ. Zhang, X. Zhang, J. Yu, J. Tang, J. Tang, C. Li, and H. Chen, “Subgraph retrieval enhanced model for multi-hop knowledge base question answering,” in ACL (Volume 1: Long Papers), 2022, pp. 5773–5784.\n\n\n\n\n[237]\n\nJ. Jiang, K. Zhou, Z. Dong, K. Ye, W. X. Zhao, and J.-R. Wen, “Structgpt: A general framework for large language model to reason over structured data,” arXiv preprint arXiv:2305.09645, 2023.\n\n\n\n\n[238]\n\nH. Zhu, H. Peng, Z. Lyu, L. Hou, J. Li, and J. Xiao, “Pre-training language model incorporating domain-specific heterogeneous knowledge into a unified representation,” Expert Systems with Applications, vol. 215, p. 119369, 2023.\n\n\n\n\n[239]\n\nC. Feng, X. Zhang, and Z. Fei, “Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs,” arXiv preprint arXiv:2309.03118, 2023.\n\n\n\n\n[240]\n\nJ. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y. Gong, H.-Y. Shum, and J. Guo, “Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph,” arXiv preprint arXiv:2307.07697, 2023.\n\n\n\n\n[241]\n\nB. He, D. Zhou, J. Xiao, X. Jiang, Q. Liu, N. J. Yuan, and T. Xu, “BERT-MK: Integrating graph contextualized knowledge into pre-trained language models,” in EMNLP, 2020, pp. 2281–2290.\n\n\n\n\n[242]\n\nY. Su, X. Han, Z. Zhang, Y. Lin, P. Li, Z. Liu, J. Zhou, and M. Sun, “Cokebert: Contextual knowledge selection and embedding towards enhanced pre-trained language models,” AI Open, vol. 2, pp. 127–134, 2021.\n\n\n\n\n[243]\n\nD. Yu, C. Zhu, Y. Yang, and M. Zeng, “JAKET: joint pre-training of knowledge graph and language understanding,” in AAAI, 2022, pp. 11 630–11 638.\n\n\n\n\n[244]\n\nX. Wang, P. Kapanipathi, R. Musa, M. Yu, K. Talamadupula, I. Abdelaziz, M. Chang, A. Fokoue, B. Makni, N. Mattei, and M. Witbrock, “Improving natural language inference using external knowledge in the science questions domain,” in AAAI, 2019, pp. 7208–7215.\n\n\n\n\n[245]\n\nY. Sun, Q. Shi, L. Qi, and Y. Zhang, “JointLK: Joint reasoning with language models and knowledge graphs for commonsense question answering,” in NAACL, 2022, pp. 5049–5060.\n\n\n\n\n[246]\n\nX. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang et al., “Agentbench: Evaluating llms as agents,” arXiv preprint arXiv:2308.03688, 2023.\n\n\n\n\n[247]\n\nY. Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr, “Knowledge graph prompting for multi-document question answering,” arXiv preprint arXiv:2308.11730, 2023.\n\n\n\n\n[248]\n\nA. Zeng, M. Liu, R. Lu, B. Wang, X. Liu, Y. Dong, and J. Tang, “Agenttuning: Enabling generalized agent abilities for llms,” 2023.\n\n\n\n\n[249]\n\nW. Kryściński, B. McCann, C. Xiong, and R. Socher, “Evaluating the factual consistency of abstractive text summarization,” arXiv preprint arXiv:1910.12840, 2019.\n\n\n\n\n[250]\n\nZ. Ji, Z. Liu, N. Lee, T. Yu, B. Wilie, M. Zeng, and P. Fung, “Rho (\\ρ\\absent𝜌\\backslash\\rho\\ italic_ρ): Reducing hallucination in open-domain dialogues with knowledge grounding,” arXiv preprint arXiv:2212.01588, 2022.\n\n\n\n\n[251]\n\nS. Feng, V. Balachandran, Y. Bai, and Y. Tsvetkov, “Factkb: Generalizable factuality evaluation using language models enhanced with factual knowledge,” arXiv preprint arXiv:2305.08281, 2023.\n\n\n\n\n[252]\n\nY. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang, “Editing large language models: Problems, methods, and opportunities,” arXiv preprint arXiv:2305.13172, 2023.\n\n\n\n\n[253]\n\nZ. Li, N. Zhang, Y. Yao, M. Wang, X. Chen, and H. Chen, “Unveiling the pitfalls of knowledge editing for large language models,” arXiv preprint arXiv:2310.02129, 2023.\n\n\n\n\n[254]\n\nR. Cohen, E. Biran, O. Yoran, A. Globerson, and M. Geva, “Evaluating the ripple effects of knowledge editing in language models,” arXiv preprint arXiv:2307.12976, 2023.\n\n\n\n\n[255]\n\nS. Diao, Z. Huang, R. Xu, X. Li, Y. Lin, X. Zhou, and T. Zhang, “Black-box prompt learning for pre-trained language models,” arXiv preprint arXiv:2201.08531, 2022.\n\n\n\n\n[256]\n\nT. Sun, Y. Shao, H. Qian, X. Huang, and X. Qiu, “Black-box tuning for language-model-as-a-service,” in International Conference on Machine Learning.   PMLR, 2022, pp. 20 841–20 855.\n\n\n\n\n[257]\n\nX. Chen, A. Shrivastava, and A. Gupta, “NEIL: extracting visual knowledge from web data,” in IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013, 2013, pp. 1409–1416.\n\n\n\n\n[258]\n\nM. Warren and P. J. Hayes, “Bounding ambiguity: Experiences with an image annotation system,” in Proceedings of the 1st Workshop on Subjectivity, Ambiguity and Disagreement in Crowdsourcing, ser. CEUR Workshop Proceedings, vol. 2276, 2018, pp. 41–54.\n\n\n\n\n[259]\n\nZ. Chen, Y. Huang, J. Chen, Y. Geng, Y. Fang, J. Z. Pan, N. Zhang, and W. Zhang, “Lako: Knowledge-driven visual estion answering via late knowledge-to-text injection,” 2022.\n\n\n\n\n[260]\n\nR. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra, “Imagebind: One embedding space to bind them all,” in ICCV, 2023, pp. 15 180–15 190.\n\n\n\n\n[261]\n\nJ. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review,” Information Fusion, vol. 59, pp. 103–126, 2020.\n\n\n\n\n[262]\n\nH. Zhang, B. Wu, X. Yuan, S. Pan, H. Tong, and J. Pei, “Trustworthy graph neural networks: Aspects, methods and trends,” arXiv:2205.07424, 2022.\n\n\n\n\n[263]\n\nT. Wu, M. Caccia, Z. Li, Y.-F. Li, G. Qi, and G. Haffari, “Pretrained language model in continual learning: A comparative study,” in ICLR, 2022.\n\n\n\n\n[264]\n\nX. L. Li, A. Kuncoro, J. Hoffmann, C. de Masson d’Autume, P. Blunsom, and A. Nematzadeh, “A systematic investigation of commonsense knowledge in large language models,” in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022, pp. 11 838–11 855.\n\n\n\n\n[265]\n\nY. Zheng, H. Y. Koh, J. Ju, A. T. Nguyen, L. T. May, G. I. Webb, and S. Pan, “Large language models for scientific synthesis, inference and explanation,” arXiv preprint arXiv:2310.07984, 2023.\n\n\n\n\n[266]\n\nB. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heintz, and D. Roth, “Recent advances in natural language processing via large pre-trained language models: A survey,” ACM Computing Surveys, vol. 56, no. 2, pp. 1–40, 2023.\n\n\n\n\n[267]\n\nJ. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, “Finetuned language models are zero-shot learners,” in International Conference on Learning Representations, 2021.\n\n\n\n\n[268]\n\nY. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen, L. Wang, A. T. Luu, W. Bi, F. Shi, and S. Shi, “Siren’s song in the ai ocean: A survey on hallucination in large language models,” arXiv preprint arXiv:2309.01219, 2023.\n\n\n\n\n\n\n\nAppendix A Pros and Cons for LLMs and KGs\nIn this section, we introduce the pros and cons of LLMs and KGs in detail. We summarize the pros and cons of LLMs and KGs in Fig. 1, respectively.\n\nLLM pros.\n\n\n•\nGeneral Knowledge [11]: LLMs pre-trained on large-scale corpora, which contain a large amount of general knowledge, such as commonsense knowledge [264] and factual knowledge [14]. Such knowledge can be distilled from LLMs and used for downstream tasks [265].\n\n\n•\nLanguage Processing [12]: LLMs have shown great performance in understanding natural language [266]. Therefore, LLMs can be used in many natural language processing tasks, such as question answering [4], machine translation [5], and text generation [6].\n\n\n•\nGeneralizability [13]: LLMs enable great generalizability, which can be applied to various downstream tasks [267]. By providing few-shot examples [59] or finetuning on multi-task data [3], LLMs achieve great performance on many tasks.\n\n\n\n\nLLM cons.\n\n\n•\nImplicit Knowledge [14]: LLMs represent knowledge implicitly in their parameters. It is difficult to interpret or validate the knowledge obtained by LLMs.\n\n\n•\nHallucination [15]: LLMs often experience hallucinations by generating content that while seemingly plausible but are factually incorrect [268]. This problem greatly reduces the trustworthiness of LLMs in real-world scenarios.\n\n\n•\nIndecisiveness [16]: LLMs perform reasoning by generating from a probability model, which is an indecisive process. The generated results are sampled from the probability distribution, which is difficult to control.\n\n\n•\nBlack-box [17]: LLMs are criticized for their lack of interpretability. It is unclear to know the specific patterns and functions LLMs use to arrive at predictions or decisions.\n\n\n•\nLacking Domain-specific/New Knowledge [18]: LLMs trained on general corpus might not be able to generalize well to specific domains or new knowledge due to the lack of domain-specific knowledge or new training data.\n\n\n\n\nKG pros.\n\n\n•\nStructural Knowledge [19]: KGs store facts in a structural format (i.e., triples), which can be understandable by both humans and machines.\n\n\n•\nAccuracy [20]: Facts in KGs are usually manually curated or validated by experts, which are more accurate and dependable than those in LLMs.\n\n\n•\nDecisiveness [21]: The factual knowledge in KGs is stored in a decisive manner. The reasoning algorithm in KGs is also deterministic, which can provide decisive results.\n\n\n•\nInterpretability [22]: KGs are renowned for their symbolic reasoning ability, which provides an interpretable reasoning process that can be understood by humans.\n\n\n•\nDomain-specific Knowledge [23]: Many domains can construct their KGs by experts to provide precise and dependable domain-specific knowledge.\n\n\n•\nEvolving Knowledge [24]: The facts in KGs are continuously evolving. The KGs can be updated with new facts by inserting new triples and deleting outdated ones.\n\n\nKG cons.\n\n\n•\nIncompleteness [25]: KGs are hard to construct and often incomplete, which limits the ability of KGs to provide comprehensive knowledge.\n\n\n•\nLacking Language Understanding [33]: Most studies on KGs model the structure of knowledge, but ignore the textual information in KGs. The textual information in KGs is often ignored in KG-related tasks, such as KG completion [26] and KGQA [43].\n\n\n•\nUnseen Facts [27]: KGs are dynamically changing, which makes it difficult to model unseen entities and represent new facts.\n\n\n\n\n","length":169495,"excerpt":"Shirui Pan, Senior Member, IEEE, Linhao Luo,\n Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu, Fellow, IEEE\n\nShirui Pan is with the School of Information and Communication Technology and Institute for Integrated and Intelligent Systems (IIIS), Griffith University, Queensland, Australia.\nEmail: s.pan@griffith.edu.au;\nLinhao Luo and Yufei Wang are with the Department of Data Science and AI, Monash University, Melbourne, Australia. E-mail: linhao.luo@monash.edu, garyyufei@gmail.com.\nChen Chen is with the Nanyang Technological University, Singapore. E-mail: s190009@ntu.edu.sg.\nJiapu Wang is with the Faculty of Information Technology, Beijing University of Technology, Beijing, China. E-mail: jpwang@emails.bjut.edu.cn.\nXindong Wu is with the Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Technology, Hefei, China, and also with the Research Center for Knowledge Engineering, Zhejiang Lab, Hangzhou, China.\nEmail: xwu@hfut.edu.cn.\nShirui Pan and Linhao Luo contributed equally to this work. Corresponding Author: Xindong Wu.","byline":null,"dir":null,"siteName":null,"lang":"en"},"finalizedMeta":{"title":"Unifying Large Language Models and Knowledge Graphs: A Roadmap","description":"Shirui Pan, Senior Member, IEEE, Linhao Luo,\n Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu, Fellow, IEEE\n\nShirui Pan is with the School of Information and Communication Technology and Institute for Integrated and Intelligent Systems (IIIS), Griffith University, Queensland, Australia.\nEmail: s.pan@griffith.edu.au;\nLinhao Luo and Yufei Wang are with the Department of Data Science and AI, Monash University, Melbourne, Australia. E-mail: linhao.luo@monash.edu, garyyufei@gmail.com.\nChen Chen is with the Nanyang Technological University, Singapore. E-mail: s190009@ntu.edu.sg.\nJiapu Wang is with the Faculty of Information Technology, Beijing University of Technology, Beijing, China. E-mail: jpwang@emails.bjut.edu.cn.\nXindong Wu is with the Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Technology, Hefei, China, and also with the Research Center for Knowledge Engineering, Zhejiang Lab, Hangzhou, China.\nEmail: xwu@hfut.edu.cn.\nShirui Pan and Linhao Luo contributed equally to this work. Corresponding Author: Xindong Wu.","author":false,"creator":"","publisher":false,"date":"2024-11-04T17:06:16.283Z","subject":"Natural Language Processing","topics":["Natural Language Processing","Large Language Models","Generative Pre-Training","Knowledge Graphs","Roadmap","Bidirectional Reasoning."]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"Unifying Large Language Models and Knowledge Graphs: A Roadmap","description":false,"canonical":"https://arxiv.org/html/2306.08302v3","keywords":["Natural Language Processing","Large Language Models","Generative Pre-Training","Knowledge Graphs","Roadmap","Bidirectional Reasoning."],"image":"extracted/5367551/figs/LLM_vs_KG.png","firstParagraph":"Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages.\nIn this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.\n"},"dublinCore":{},"opengraph":{"title":false,"description":false,"url":false,"site_name":false,"locale":false,"type":false,"typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":false},"twitter":{"site":false,"description":false,"card":false,"creator":false,"title":false,"image":false},"archivedData":{"link":false,"wayback":false}}}