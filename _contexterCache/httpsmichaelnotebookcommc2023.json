{"initialLink":"https://michaelnotebook.com/mc2023/","sanitizedLink":"https://michaelnotebook.com/mc2023/","finalLink":"https://michaelnotebook.com/mc2023/","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://michaelnotebook.com/mc2023/\" itemprop=\"url\">How is AI impacting science?</a></contexter-box-head></contexter-box-head><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2023-07-11T16:37:39.158Z\">7/11/2023</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>  The biggestsuccess of AI in science so far is the AlphaFold 2 system. This is adeep learning system which has made large strides on a fundamentalscientific problem: how to predict the 3-dimensional structure of aprotein from the sequence of amino acids making up that protein.  Thisbreakthrough has helped set off an ongoing deep learning revolution inmolecular biology.  While obviously of interest to molecularbiologists, I believe this is of much broader interest for science asa whole, as a concrete prototype for how artificial intelligence mayimpact discovery. In this short survey talk I briefly discussquestions including: how can such systems be validated?  Can they beused to identify general principles that human scientists can learnfrom? And what should we expect a good theory or explanation toprovide anyway?  The focus of the talk is extant results and the nearterm, not the longer-term future.  However, I hope it may help ingrounding an understanding of that longer-term future, and of thebenefits and risks of AI systems.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a href=\"https://web.archive.org/web/20230711163753/https://michaelnotebook.com/mc2023/\" is=\"contexter-link\" target=\"_blank\" rel=\"timemap\" class=\"read-link archive-link\" itemprop=\"archivedAt\" slot=\"archive-link\">Archived</a><a is=\"contexter-link\" href=\"https://michaelnotebook.com/mc2023/\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"fc48fcc645bee38742db58b564b23692124232b3","data":{"originalLink":"https://michaelnotebook.com/mc2023/","sanitizedLink":"https://michaelnotebook.com/mc2023/","canonical":"https://michaelnotebook.com/mc2023/","htmlText":"<!DOCTYPE html>\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"\" xml:lang=\"\">\n<head>\n  <meta charset=\"utf-8\" />\n  <meta name=\"generator\" content=\"pandoc\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\" />\n  <title>How is AI impacting science?</title>\n  <style>\n    html {\n      line-height: 1.5;\n      font-family: Georgia, serif;\n      font-size: 20px;\n      color: #1a1a1a;\n      background-color: #fdfdfd;\n    }\n    body {\n      margin: 0 auto;\n      max-width: 36em;\n      padding-left: 50px;\n      padding-right: 50px;\n      padding-top: 50px;\n      padding-bottom: 50px;\n      hyphens: auto;\n      overflow-wrap: break-word;\n      text-rendering: optimizeLegibility;\n      font-kerning: normal;\n    }\n    @media (max-width: 600px) {\n      body {\n        font-size: 0.9em;\n        padding: 1em;\n      }\n    }\n    @media print {\n      body {\n        background-color: transparent;\n        color: black;\n        font-size: 12pt;\n      }\n      p, h2, h3 {\n        orphans: 3;\n        widows: 3;\n      }\n      h2, h3, h4 {\n        page-break-after: avoid;\n      }\n    }\n    p {\n      margin: 1em 0;\n    }\n    a {\n      color: #1a1a1a;\n    }\n    a:visited {\n      color: #1a1a1a;\n    }\n    img {\n      max-width: 100%;\n    }\n    h1, h2, h3, h4, h5, h6 {\n      margin-top: 1.4em;\n    }\n    h5, h6 {\n      font-size: 1em;\n      font-style: italic;\n    }\n    h6 {\n      font-weight: normal;\n    }\n    ol, ul {\n      padding-left: 1.7em;\n      margin-top: 1em;\n    }\n    li > ol, li > ul {\n      margin-top: 0;\n    }\n    blockquote {\n      margin: 1em 0 1em 1.7em;\n      padding-left: 1em;\n      border-left: 2px solid #e6e6e6;\n      color: #606060;\n    }\n    code {\n      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;\n      font-size: 85%;\n      margin: 0;\n    }\n    pre {\n      margin: 1em 0;\n      overflow: auto;\n    }\n    pre code {\n      padding: 0;\n      overflow: visible;\n      overflow-wrap: normal;\n    }\n    .sourceCode {\n     background-color: transparent;\n     overflow: visible;\n    }\n    hr {\n      background-color: #1a1a1a;\n      border: none;\n      height: 1px;\n      margin: 1em 0;\n    }\n    table {\n      margin: 1em 0;\n      border-collapse: collapse;\n      width: 100%;\n      overflow-x: auto;\n      display: block;\n      font-variant-numeric: lining-nums tabular-nums;\n    }\n    table caption {\n      margin-bottom: 0.75em;\n    }\n    tbody {\n      margin-top: 0.5em;\n      border-top: 1px solid #1a1a1a;\n      border-bottom: 1px solid #1a1a1a;\n    }\n    th {\n      border-top: 1px solid #1a1a1a;\n      padding: 0.25em 0.5em 0.25em 0.5em;\n    }\n    td {\n      padding: 0.125em 0.5em 0.25em 0.5em;\n    }\n    header {\n      margin-bottom: 4em;\n      text-align: center;\n    }\n    #TOC li {\n      list-style: none;\n    }\n    #TOC a:not(:hover) {\n      text-decoration: none;\n    }\n    code{white-space: pre-wrap;}\n    span.smallcaps{font-variant: small-caps;}\n    span.underline{text-decoration: underline;}\n    div.column{display: inline-block; vertical-align: top; width: 50%;}\n    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}\n    ul.task-list{list-style: none;}\n  </style>\n  <link rel=\"stylesheet\" href=\"/pandoc.css\" />\n  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js\"></script>\n  <script>document.addEventListener(\"DOMContentLoaded\", function () {\n   var mathElements = document.getElementsByClassName(\"math\");\n   var macros = [];\n   for (var i = 0; i < mathElements.length; i++) {\n    var texText = mathElements[i].firstChild;\n    if (mathElements[i].tagName == \"SPAN\") {\n     katex.render(texText.data, mathElements[i], {\n      displayMode: mathElements[i].classList.contains('display'),\n      throwOnError: false,\n      macros: macros,\n      fleqn: false\n     });\n  }}});\n  </script>\n  <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css\" />\n  <!--[if lt IE 9]>\n    <script src=\"//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js\"></script>\n    <![endif]-->\n     <!-- Global site tag (gtag.js) - Google Analytics -->\n<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-0LY12HRJWG\"></script>\n<script>\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'G-0LY12HRJWG');\n</script>\n</head>\n<body>\n\n  \n  <div id=\"linkback\">\n  <a href=\"/index.html\"><img src=\"/assets/home.png\" height=\"24px\"/></a>\n  </div>\n\n    <div id=\"tagbox\">\n        <a href=\"https://michaelnotebook.com/tag/ai.html\">ai</a><br>\n        <a href=\"https://michaelnotebook.com/tag/metascience.html\">metascience</a><br>\n        <a href=\"https://michaelnotebook.com/tag/fom.html\">fom</a><br>\n        <a href=\"https://michaelnotebook.com/tag/talk.html\">talk</a><br>\n      </div>\n    \n    <header id=\"title-block-header\">\n    <h1 class=\"title\">How is AI impacting science?</h1>\n              </header>\n      <center>Michael Nielsen</center>\n\n      <center>Astera Institute</center>\n\n      <center>May 14, 2023</center>\n\n      <p style=\"margin-left:10%; margin-right:10%;\"> <em> The biggest\n      success of AI in science so far is the AlphaFold 2 system. This is a\n      deep learning system which has made large strides on a fundamental\n      scientific problem: how to predict the 3-dimensional structure of a\n      protein from the sequence of amino acids making up that protein.  This\n      breakthrough has helped set off an ongoing deep learning revolution in\n      molecular biology.  While obviously of interest to molecular\n      biologists, I believe this is of much broader interest for science as\n      a whole, as a concrete prototype for how artificial intelligence may\n      impact discovery. In this short survey talk I briefly discuss\n      questions including: how can such systems be validated?  Can they be\n      used to identify general principles that human scientists can learn\n      from? And what should we expect a good theory or explanation to\n      provide anyway?  The focus of the talk is extant results and the near\n      term, not the longer-term future.  However, I hope it may help in\n      grounding an understanding of that longer-term future, and of the\n      benefits and risks of AI systems.</em></p>\n\n      <p><em>The text for a talk given at the <a href=\"https://metascience.info/\">Metascience 2023 Conference</a> in Washington, D.C., May 2023.</em></p>\n      <p>In 2020 the deep learning system AlphaFold 2<a href=\"#fn1\" class=\"footnote-ref\" id=\"fnref1\" role=\"doc-noteref\"><sup>1</sup></a> surprised biologists when it was shown to routinely make correct near atomic-precision predictions for protein structure. That is, using just the linear sequence of amino acids making up a protein, AlphaFold was able to predict the positions of the atoms in the protein. These results were not cherrypicked, but rather emerged from an adversarial, blind competition with over a hundred other modeling groups. In a few cases, AlphaFold<a href=\"#fn2\" class=\"footnote-ref\" id=\"fnref2\" role=\"doc-noteref\"><sup>2</sup></a> has even exceeded experimental accuracy, causing existing experimental results to be re-evaluated and improved.</p>\n      <p>While AlphaFold is impressive, it's also far from complete: it's really a bridge to a new era, opening up many scientific and metascientific questions. These include: what we expect a good theory or explanation to provide; what it means to validate that understanding; and what we humans can learn from these systems. Most of all: whether and how AI systems may impact the progress of science as a whole, as a systemic intervention. In my talk, I'll treat AlphaFold as a concrete prototype for how AI may be used across science. For these reasons, it's valuable for metascientists to engage with AlphaFold and successor systems, even if you have no prior interest in proteins or even in biology.</p>\n      <p>The talk is a survey. I am not a molecular biologist, so my apologies for any errors on that front. That said, it's been enjoyable and often inspiring to learn about proteins. Let me take a few minutes to remind you of some background for those of you (like me) who aren't biologists. Molecular biology feels a bit like wandering into an immense workshop full of wonderful and varied machines. Large databases like UniProt contain the amino acid sequences for hundreds of millions of proteins. You have, for example, kinesin proteins, which transport large molecules around the interior of the cell. You have haemoglobin, which carries oxygen in your blood, and helps power your metabolism. You have green fluorescent protein, which emits green light when exposed to ultraviolet light, and which can be used to tag and track other biomolecules. All these and many more molecular machines, created and sorted by the demanding sieve of evolution by natural selection. Every individual machine could be the subject of a lifetime's study. There are, for instance, thousands of papers about the kinesin superfamily, and yet we're really just beginning to understand it. But while this wealth of biological machines is astonishing, we don't <em>a priori</em> know what those machines do, or how they do it. We have no instruction manual, and we're trying to figure it out.</p>\n      <p>In fact, for the vast majority of the hundreds of millions of proteins known<a href=\"#fn3\" class=\"footnote-ref\" id=\"fnref3\" role=\"doc-noteref\"><sup>3</sup></a>, all we can easily directly determine is the basic blueprint: using genome sequencing we can find the linear sequence of amino acids that form the protein, at a cost of no more than cents. But proteins are tiny 3-dimensional structures, typically nanometers across, making it extremely difficult to directly image them. For a single protein it will routinely take <em>months</em> of work to experimentally determine the corresponding 3-dimensional structure, usually using x-ray crystallography or cryo-electron microscopy or NMR.</p>\n      <p>This discrepancy really matters. It matters because understanding the shape is crucial for understanding questions like:</p>\n      <ul>\n      <li>What antigens can an antibody protein bind to, as part of an immune response?</li>\n      <li>What can the protein carry around? [E.g., the way haemoglobin carries oxygen]</li>\n      <li>How do proteins form larger complexes? [E.g., the ribosome]</li>\n      </ul>\n      <p>Understanding the shape of a protein doesn't tell you everything about its function. But it is fundamental to understanding what a protein can do, and how it does it.</p>\n      <p>Ideally, we'd be able to determine the shape from the amino acid sequence alone. There are reasons chemists and biologists expect this to often be possible<a href=\"#fn4\" class=\"footnote-ref\" id=\"fnref4\" role=\"doc-noteref\"><sup>4</sup></a>, and in the 1970s, scientists began doing physics simulations to attempt to determine the shape a protein will fold into. Many techniques have been adopted since (kinetics, thermodynamics, evolutionary, ….) For a long time these made only very slow progress. And even for that progress, one must wonder if modelers are cherrypicking, even with the best will in the world. There were, for instance, claims in the 1990s from Johns Hopkins to have largely \"solved\" the protein structure prediction problem<a href=\"#fn5\" class=\"footnote-ref\" id=\"fnref5\" role=\"doc-noteref\"><sup>5</sup></a>.</p>\n      <p>To assess progress in a fair but demanding way, in 1994 a competition named <a href=\"https://predictioncenter.org/\">CASP</a> (Critical Assessment of protein Structure Prediction) was begun. Running every two years, CASP asks modelers to do <em>blind predictions</em> of protein structure. That is, they're asked to predict structures for proteins whose amino acid sequence is known, but where biologists are still working on experimentally determining the three-dimensional structure, and expect to know it shortly after the competition is over, so it can be used to score predictions. The score for any given model is (very roughly) what percentage of amino acids positions are predicted correctly, according to some demanding threshold<a href=\"#fn6\" class=\"footnote-ref\" id=\"fnref6\" role=\"doc-noteref\"><sup>6</sup></a>. As you can see, through the 2010s the winner would typically score in the range 30 to 50<a href=\"#fn7\" class=\"footnote-ref\" id=\"fnref7\" role=\"doc-noteref\"><sup>7</sup></a>:</p>\n      <center><img src=\"/mc2023/assets/scores_GDT_partial.jpg\" width=\"446px\"></center>\n      <p>That is, the winner would (roughly) place somewhat than half of amino acids near perfectly. Then, in 2018 and 2020, DeepMind entered with AlphaFold and AlphaFold 2:</p>\n      <center><img src=\"/mc2023/assets/scores_GDT.jpg\" width=\"640px\"></center>\n      <p>I'll focus on CASP 14 in 2020, when AlphaFold 2 achieved of score of 87, so roughly 87 percent of amino acids within a demanding threshold. This was AlphaFold's score in the most demanding category, the free modeling category, where contestants are asked to predict the structure of proteins for which few similar proteins were previously known. Across all categories, AlphaFold predicted the position of the alpha-carbon atoms in the backbone with a root mean square distance of 0.96 Angstroms<a href=\"#fn8\" class=\"footnote-ref\" id=\"fnref8\" role=\"doc-noteref\"><sup>8</sup></a>. The next-best technique was roughly one third as accurate, with a root mean square distance of 2.8 Angstroms<a href=\"#fn9\" class=\"footnote-ref\" id=\"fnref9\" role=\"doc-noteref\"><sup>9</sup></a>:</p>\n      <center><img src=\"/mc2023/assets/rmsd.jpg\" width=\"400px\"></center>\n      <p>For comparison, the van der Waals diameter of a carbon atom is roughly 1.4 Angstroms. AlphaFold's accuracy is often comparable to experimental accuracy. The co-founder of CASP, John Moult of the University of Maryland, said<a href=\"#fn10\" class=\"footnote-ref\" id=\"fnref10\" role=\"doc-noteref\"><sup>10</sup></a>:</p>\n      <blockquote>\n      <p>We have been stuck on this one problem – how do proteins fold up – for nearly 50 years. To see DeepMind produce a solution for this, having worked personally on this problem for so long and after so many stops and starts, wondering if we’d ever get there, is a very special moment.</p>\n      </blockquote>\n      <p>That is a very strong statement, and it's worth digging into in what sense AlphaFold solves the protein structure prediction problem, and what remains to be understood. However, even AlphaFold's competitors were laudatory. Here's Mohammed AlQuraishi of Columbia University<a href=\"#fn11\" class=\"footnote-ref\" id=\"fnref11\" role=\"doc-noteref\"><sup>11</sup></a>:</p>\n      <blockquote>\n      <p>Does this constitute a solution of the static protein structure prediction problem? I think so but there are all these wrinkles. Honest, thoughtful people can disagree here and it comes down to one’s definition of what the word “solution” really means… the bulk of the scientific problem is solved; what’s left now is execution.</p>\n      </blockquote>\n      <p>That was two-and-a-half years ago. I think a reasonable broad view today is: AlphaFold is a huge leap, but much remains to be done even on the basic problem, and many enormous new problems can now be attacked.</p>\n      <p>AlphaFold's high-level architecture would take several hours to describe. I want to emphasize just a few points today:</p>\n      <img src=\"/mc2023/assets/architecture.png\" width=\"100%\"/>\n      <p>It's a deep neural network, meaning a hierarchical model, with 93 million parameters learned through training. An amino acid sequence is input, and a 3-dimensional structure is output, together with some error estimates quantifying how confident AlphaFold is in the placement of each amino acid. The basic training data is the protein data bank (PDB), humanity's repository of the protein structures experimentally determined since the 1970s. At training time, that was 170,000 protein structures (though a small fraction were omitted for technical reasons). The parameters in the AlphaFold network are adjusted using gradient descent to ensure the network outputs the correct structure, given the input.</p>\n      <p>That's a (very!) broad picture of how the network learned to predict structures. Many more ideas were important. One clever and important (albeit existing) idea was a way to learn from the hundreds of millions of known sequences for proteins. The idea is to find many other proteins whose amino acid sequences are similar to the input protein, meaning they're likely to be evolutionarily related. Maybe, for instance, it's essentially the same protein, but in some other species. AlphaFold tries to find many such similar proteins, and to learn from them. To get an intuition for how it might do that, suppose by looking at many similar proteins AlphaFold finds that there is some particular pair of amino acids which are a long way apart in the linear chain, but where changes in one amino acid seem to be correlated to changes in the other. If you saw that you might suspect these amino acids are likely to be close together in space, and are co-evolving together to preserve the shape of the protein. In fact, this often does happen, and it provides a way AlphaFold can learn both from <em>known structural information</em> (in the PDB), and from <em>known evolutionary information</em> (in protein sequence databases). A nice way of putting it, in <a href=\"https://www.youtube.com/watch?v=EjZMxq4oEt4&amp;t=443s&amp;ab_channel=RCSBProteinDataBank\">a talk from John Jumper</a>, lead author on the AlphaFold paper, is that the physics informs AlphaFold's understanding of the evolutionary history, and the evolutionary history informs AlphaFold's understanding of the physics.</p>\n      <h1 id=\"generalization-and-reliability\">Generalization and reliability</h1>\n      <p>You might wonder: is AlphaFold just memorizing its training data, or can it generalize? CASP provides a basic validation: the competition structures were not in the PDB at the time AlphaFold predicted them. Furthermore, CASP is in some sense a \"natural\" sample: the structural biology community prefers to solve structure which are biologically important<a href=\"#fn12\" class=\"footnote-ref\" id=\"fnref12\" role=\"doc-noteref\"><sup>12</sup></a>. So it suggests some ability to generalize to proteins of interest to biologists at large.</p>\n      <p>What if we use deep learning to study proteins which don't occur in nature, maybe as the result of mutations, or as part of protein design? There's a huge amount of work going on, and frankly it's an exciting mess right now, all over the place – any reasonable overview would cover dozens if not hundreds of papers. There are papers saying AlphaFold works <em>badly</em> for such-and-such a type of mutation, it works <em>well</em> for such-and-such a kind of mutation, all sorts of deep learning fixes for protein design, and so on. My takeaway: it's going to keep biologists busy for years figuring out the shortcomings, and improving the systems to address those shortcomings.</p>\n      <p>Some interesting tests of deep learning's ability to generalize were done by OpenFold<a href=\"#fn13\" class=\"footnote-ref\" id=\"fnref13\" role=\"doc-noteref\"><sup>13</sup></a>, an open source near-clone of AlphaFold. For instance, they retrained OpenFold with most broad classes of topology simply removed from the training set. To do this, they used a standard classification scheme, the CATH classification of protein topology. They removed 95% (!!!) of the topologies from the training set, and then retrained OpenFold from scratch. Even with the great majority of topologies removed, performance was similar to AlphaFold 1<a href=\"#fn14\" class=\"footnote-ref\" id=\"fnref14\" role=\"doc-noteref\"><sup>14</sup></a>: just a couple of years prior to AlphaFold 2 it would have been state-of-the-art.</p>\n      <p>In another variation on this idea, they retrained OpenFold starting from much smaller subsamples of the protein data bank. Instead of using 170,000 structures, they retrained with training sets as small as 1,000 structures, chosen at random. Even with such a tiny training set, they achieved a performance comparable to (in fact, slightly better than) AlphaFold 1. And with just 10,000 structures as training data they achieved a performance comparable to the full OpenFold.</p>\n      <p>As a practical matter, the question of AlphaFold generalization matters in part because DeepMind has released AlphaFold DB, a database of 215 million protein structures, including the (almost-complete) proteomes of 48 species, including humans, mice, and fruit flies. These were obtained by taking genetic sequences in UniProt, and then using AlphaFold to predict the structure. You can view the whole process as:</p>\n      <center><img src=\"/mc2023/assets/AFDB.png\" width=\"80%\"/></center>\n      <p>It's an astonishing act of generalization. If we had a perfectly reliable model, it would extend our understanding of protein structures by roughly three orders of magnitude. It's worth emphasizing that no additional experiments were done by AlphaFold; no additional data were taken. And yet by \"just thinking\" it was possible to obtain a very large number of predictions that people expect to be very good. I've heard from several biologists variations on the sentiment: \"No-one would take an AlphaFold prediction as true on its own; but it’s an extremely helpful starting point, and can save you months of work\".</p>\n      <p>As the models get better still, I expect the line between model and experiment to become blurry. That may sound strange, but in fact traditional \"experimentally-determined structures\" actually require immensely-complicated theories to go from data to structure. If you believe AlphaFold (or a successor) offered a stronger theory, you might end up believing the predictions from the deep learning system more than you believe (today's) \"experimental results\". There are early hints of this beginning to happen. In the CASP assessment, AlphaFold performed poorly on several structures which had been determined using NMR spectroscopy. A 2022 paper<a href=\"#fn15\" class=\"footnote-ref\" id=\"fnref15\" role=\"doc-noteref\"><sup>15</sup></a> examined \"904 human proteins with both Alpha-Fold and NMR structures\" and concluded that \"Alpha-Fold predictions are usually more accurate than NMR structures\". One of the authors, Mike Williamson, actually helped pioneer the use of NMR for structural biology.</p>\n      <p>To make the same point in a simpler setting: how we interpret the images from a telescope depends upon our theory of optics; if we were to change or improve our theory of optics, our understanding of the so-called \"raw data\" from the telescope would change. Indeed, something related has really happened in our understanding of the bending of light by gravitation. In that case, we've changed our understanding of the way light travels through space, and that has affected the way we interpret experimental data, particularly in understanding phenomena like gravitational lensing of distant galaxies.</p>\n      <p>In a similar way, \"experimental\" protein structure determination depends strongly on theory. You can get a gist for this in x-ray crystallography, which requires many difficult steps: purification of protein sample; crystallization (!) of the proteins; x-ray diffraction to obtain two-dimensional images; procedures to invert and solve for the three-dimensional structure; criteria for when the inversion is good enough. A lot of theory is involved! Indeed, the inversion procedure typically involves starting with a good \"guess\", a candidate search structure. Often people use related proteins, but sometimes they can't find a good search structure, and this can prevent a solution being found. AlphaFold has been used to find good search structures to help invert data for particularly challenging structures. So there is already a blurry line between theory and experiment. I expect figuring out how to validate AI \"solutions\" to problems will be a major topic of scientific and metascientific interest in the years to come.</p>\n      <h1 id=\"is-a-simple-set-of-principles-for-protein-structure-possible-might-ai-help-us-discover-it\">Is a simple set of principles for protein structure possible? Might AI help us discover it?</h1>\n      <p>Any model with 93 million learned parameters is complicated. It's not a \"theory\" or \"explanation\" in the conventional sense. You might wonder: can AlphaFold (or a successor) be used to help discover such a theory, even if only partial? Might, for instance, a simple set of principles for protein structure prediction be possible? And what, exactly, is AlphaFold 2 learning? To avoid disappointment, let me say: we don't yet know the answers to such questions. But pursuing them is a useful intuition pump for thinking about the role of AI in science.</p>\n      <p>One approach to finding such principles is \"behaviorist artificial psychology\": inferring high-level principles by observing the behavior of the system. Of course, AlphaFold's detailed predictions have already been used widely by biologists, for things like discovering new binding sites on proteins. But while this is very useful, it isn't the same as inferring novel high-level principles about protein structure. However, there are other important deep learning systems in which interesting new high-level principles have been found by observing behavior.</p>\n      <p>For instance, by observing the AlphaZero chess system<a href=\"#fn16\" class=\"footnote-ref\" id=\"fnref16\" role=\"doc-noteref\"><sup>16</sup></a> behaviors have been inferred which violated conventional chess grandmaster wisdom. These behaviors were announced in December 2018<a href=\"#fn17\" class=\"footnote-ref\" id=\"fnref17\" role=\"doc-noteref\"><sup>17</sup></a> and January 2019<a href=\"#fn18\" class=\"footnote-ref\" id=\"fnref18\" role=\"doc-noteref\"><sup>18</sup></a>. A recent paper<a href=\"#fn19\" class=\"footnote-ref\" id=\"fnref19\" role=\"doc-noteref\"><sup>19</sup></a> examined these behaviors, and tried to determine how (and whether) human players had changed in response to the system. They found few changes in the top ten players in 2019, with one exception: the world's top player then and now, Magnus Carlsen. They identify multiple ways in which Carlsen changed his play significantly in 2019, plausibly influenced by AlphaZero.</p>\n      <p>Among the changes: Carlsen advanced his h pawn early in the game<a href=\"#fn20\" class=\"footnote-ref\" id=\"fnref20\" role=\"doc-noteref\"><sup>20</sup></a> much more frequently (a 333% increase with white, a 175% increase with black). He changed his opening strategies for both white and black. Indeed, his two most common 2019 opening strategies with white (variations of the Queen's Gambit declined and of the Grunfeld Defense) were openings he didn't play at all in 2018. And with black he played the Sicilian less than 10% of the time in 2018, but 45% of the time in 2019. They also observed an increase in Carlsen's willingness to sacrifice material. All these changes were consistent with learning from AlphaZero. Carlsen lost no classical games in 2019, and was the only top grandmaster to considerably improve his Elo rating, despite already being the top-rated player: he increased by 37 rating points. By contrast, 6 of the top 10 players actually lost points, and none of the other 3 gained more than 6 points. Of course, we do not know how much of this was due to Carlsen learning from AlphaZero, but the paper makes a plausible case that Carlsen learned much from AlphaZero.</p>\n      <p>Such behavioral observation is interesting but frustrating: it doesn't tell us <em>why</em> these behaviors occur. We may know that AlphaZero likes to advance the h pawn early, but what we'd really like is to understand why. Can we instead look inside the neural networks, and understand how they do what they do? As far as I know this kind of investigation has only been done casually for AlphaFold. But for simpler neural nets people are discovering interesting structure. Last year, for example, Neel Nanda and Tom Lieberum<a href=\"#fn21\" class=\"footnote-ref\" id=\"fnref21\" role=\"doc-noteref\"><sup>21</sup></a> trained a single-layer transformer neural network to add two integers modulo 113. At first, the network simply memorized all the examples in the training set. It could add those well, but (unsurprisingly) gave terrible performance elsewhere. But as they trained the network for much longer, it transitioned to performing vastly better on held-out examples. Somehow, with no additional training data, it was learning to add.</p>\n      <p>Nanda and Lieberum spent weeks looking inside the network to understand what had changed. By reverse engineering the network they discovered it had learned to add in a remarkable way. The rough gist is: given numbers <span class=\"math inline\">x</span> and <span class=\"math inline\">y</span> the network computed a wave <span class=\"math inline\">e^{2\\pi i\n      kx/113}</span>, then did a phase shift to <span class=\"math inline\">e^{2\\pi i k x/ 113} \\times\n      e^{2\\pi i ky/113}</span>, and finally tried to find a canceling phase shift <span class=\"math inline\">z</span> which would result in the wave <span class=\"math inline\">e^{2 \\pi i k(x+y-z)/113}</span> having the flattest possible \"tone\". It's a radio frequency engineer or group representation theorist's way of doing addition. This came as quite a surprise to Nanda and Lieberum. As Nanda said:</p>\n      <blockquote>\n      <p>To emphasize, this algorithm was purely learned by gradient descent! I did not predict or understand this algorithm in advance and did nothing to encourage the model to learn this way of doing modular addition. I only discovered it by reverse engineering the weights.</p>\n      </blockquote>\n      <p>This sequence of events greatly puzzled me when first I heard it: why did the network switch to this more general algorithm? After all, it initially memorized the training data, and provided excellent performance: why change? The answer is that during training the neural network pays a cost for more complex models: the loss function is chosen so gradient descent prefers lower-weight models. And the wave algorithm is actually lower-weight. It's a kind of mechanical implementation of Occam's razor, preferring an approach which is simpler and, in this case, more general. Indeed, by varying the loss function you can potentially impose Occam's razor in many different ways. It's intriguing to wonder: will one day we be able to do similar things for systems like AlphaFold, perhaps discovering new basic principles of protein structure?</p>\n      <h1 id=\"concluding-thought\">Concluding thought</h1>\n      <p>I hope you've enjoyed this brief look at AlphaFold and reflections on the use of AI in science. Of course, there's far more to say. But I believe it's clear there are many fundamental scientific and metascientific questions posed by AI systems. Most of all: as they get better at different types of cognitive operation, and as they become more able to sense and actuate in the environment, how will they impact science as a whole? Will AI systems eventually systematically change the entire practice of science? Will they perhaps greatly speed up scientific discovery? And if so, what risks and benefits does that carry?</p>\n      <h1 id=\"afterword-what-about-messier-problems-with-less-good-data-to-learn-from\">Afterword: what about messier problems, with less good data to learn from?</h1>\n      <p>At the end of my talk many interesting questions were asked by audience members. One striking question came from Evan Miyazono. It was, roughly: \"As a target for AI, protein structure prediction benefits from having a lot of very clean prior data which the system can learn from. How useful do you think AI will be for problems that don't have lots of clean prior data to learn from?\"</p>\n      <p>It's a great question. A year or so ago I would have said that yes, it seems challenging to apply AI when you're working with messier problems with less clear metrics of success. But I've changed my mind over the year, as I've better understood foundation models, transfer learning, and zero-shot learning.</p>\n      <p>To explain why I've changed my mind, let me give two pieces of context. First, some informal gossip: many people have observed that ChatGPT is much better at coding than GitHub Copilot. I don't know the reason for sure. But a common speculation I've heard from informed people is that training on lots of text (as well as code) substantially improves the model's ability to code. Somehow, it seems the regularities in the text are improving ChatGPT's ability to understand code as well.</p>\n      <p>The second piece of context: a similar idea is used with AlphaFold. To explain this I need to explain a piece of AlphaFold's training that I didn't discuss in the body of the talk: it actually implicitly includes a large language model, treating the sequence of amino acids as a kind of \"text\" to be filled in. To understand this, recall that given a sequence of amino acids, AlphaFold looks up similar sequences in existing genetic databases. And during training, just like a language model, AlphaFold masks out (or mutates) some of the individual amino acids in those sequences, and attempts to predict the missing values. Doing this forces the network to learn more about the structure of the evolutionary information; it then takes advantage of that forced understanding to do better at protein structure prediction. In this way, AlphaFold benefits from transfer learning from a related domain (genetics) where far more information is available.</p>\n      <p>Returning to the broader context of science, I expect large multi-modal foundation models will: (a) gradually begin to outperform special-purpose systems, just as large language models often outperform more specialized natural language algorithms; and (b) they will exhibit zero-shot or few-shot learning. These large multi-modal models will be trained not just on text, but also on images and actions and code and genetic data and geographic data and all sorts of sensor and actuator data. And, just as in the language models, they will use what they understand in one domain to help inform their work in adjacent, often messier domains, which the model may perhaps have limited information about. Indeed, there are already hints of protein language models moving in this direction.</p>\n      <p>Of course, all this is merely a story and an intuition. It hasn't happened yet, except in very nascent ways, and I may be quite wrong! I certainly expect new ideas will be needed to make this work well. But hopefully that sums up how my intuition has changed over the last year: I think AI models will, in the next few years, be surprisingly good at addressing messy problems with only a little prior data. And they'll do it using multi-model foundation models, transfer learning, and zero- and few-shot learning.</p>\n      <h1 id=\"acknowledgments\">Acknowledgments</h1>\n      <p>This talk was supported by the Astera Institute. Thanks to Alexander Berger, David Chapman, Evan Miyazono, Neel Nanda, and Pranav Shah for comments that helped improve this talk. And thanks to my many correspondents on Twitter, who have helped me deepen my understanding of AI and of molecular biology.</p>\n      <h1 id=\"citation-information\">Citation information</h1>\n      <p>For attribution in academic contexts, please cite this work as:</p>\n      <p>Michael Nielsen, \"How AI is impacting science\", <a href=\"https://michaelnotebook.com/mc2023/index.html\">https://michaelnotebook.com/mc2023/index.html</a>, San Francisco (2023).</p>\n      <h1 id=\"footnotes\">Footnotes</h1>\n      <section class=\"footnotes footnotes-end-of-document\" role=\"doc-endnotes\">\n      <hr />\n      <ol>\n      <li id=\"fn1\" role=\"doc-endnote\"><p>John Jumper, Richard Evans, Alexander Pritzel <em>et al</em>, <a href=\"https://www.nature.com/articles/s41586-021-03819-2\">Highly accurate protein structure prediction with AlphaFold</a>, <em>Nature</em> (2021). See also <a href=\"https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb\">this colab</a>, which provides an executable version of AlphaFold from Google DeepMind, the company behind AlphaFold. Note that other sources provide faster implementations, with comparable performance. See, for instance this <a href=\"https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb\">ColabFold colab</a>. And, as we shall see later, other groups have developed open source implementations of the code training the system, not just for making structure predictions.<a href=\"#fnref1\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn2\" role=\"doc-endnote\"><p>I'll use \"AlphaFold\" to refer to AlphaFold 2. Of course, it is a followup to an earlier AlphaFold system for protein structure prediction. However, we won't discuss that system in any depth, and so I will usually omit the number. Note that AlphaFold 1 and AlphaFold 2 had very different architectures, apart from both being deep learning systems for protein structure prediction.<a href=\"#fnref2\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn3\" role=\"doc-endnote\"><p>Indeed, billions may be found (though of less clear provenance!) in metagenomics databases.<a href=\"#fnref3\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn4\" role=\"doc-endnote\"><p>This is a complicated subject, sometimes referred to as Anfinsen's hypothesis or Anfinsen's dogma. It is not true all the time – there are certainly circumstances under which the same amino acid sequence can fold in different ways, or has no stable structure. And yet it has served as an extremely useful basis for beginning to understand proteins and their function. In this talk I'll mostly assume it's true.<a href=\"#fnref4\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn5\" role=\"doc-endnote\"><p>See, for instance, this <a href=\"https://pages.jh.edu/jhumag/695web/profold.html\">remarkable press release</a> from Johns Hopkins, about the work of George Rose and Rajgopal Srinivasan \"solving\" the problem of predicting the backbone structure for globular proteins.<a href=\"#fnref5\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn6\" role=\"doc-endnote\"><p>More precisely, the global distance test (GDT) used in CASP is an average of four different percentages, measuring the fraction of predictions for the backbone alpha carbon atoms which fall within 1, 2, 4, and 8 Angstroms of the experimental structures. For comparison, I am told that good experimental structures are often thought to be accurate to about 1-2 Angstroms; better has been achieved, but worse is not uncommon, depending on the technique.<a href=\"#fnref6\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn7\" role=\"doc-endnote\"><p>This graph and the one that follow are adapted from <a href=\"https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology\">AlphaFold: a solution to a 50-year-old grand challenge in biology</a> (2021).<a href=\"#fnref7\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn8\" role=\"doc-endnote\"><p>This was for 95% coverage of the alpha carbons. For 100% coverage, it was 1.5 Angstroms.<a href=\"#fnref8\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn9\" role=\"doc-endnote\"><p>Figure from: John Jumper, Richard Evans, Alexander Pritzel <em>et al</em>, <a href=\"https://www.nature.com/articles/s41586-021-03819-2\">Highly accurate protein structure prediction with AlphaFold</a>, <em>Nature</em> (2021).<a href=\"#fnref9\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn10\" role=\"doc-endnote\"><p>Quoted in: <a href=\"https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology\">AlphaFold: a solution to a 50-year-old grand challenge in biology</a> (2021).<a href=\"#fnref10\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn11\" role=\"doc-endnote\"><p>Mohammed AlQuraishi, <a href=\"https://moalquraishi.wordpress.com/2020/12/08/alphafold2-casp14-it-feels-like-ones-child-has-left-home/\">AlphaFold2 @ CASP14: “It feels like one’s child has left home.”</a> (2020).<a href=\"#fnref11\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn12\" role=\"doc-endnote\"><p>Of course, other concerns like the experimental tractability of a structure also play a role.<a href=\"#fnref12\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn13\" role=\"doc-endnote\"><p>Gustaf Ahdritz, Nazim Bouatta, Sachin Kadyan <em>et al</em>, <a href=\"https://www.biorxiv.org/content/10.1101/2022.11.20.517210v2.full\">OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization</a> (2022).<a href=\"#fnref13\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn14\" role=\"doc-endnote\"><p>They use the local distance difference test (lDDT-C<span class=\"math inline\">\\alpha</span>) as their metric, rather than the global distance test emphasized in AlphaFold's performance on CASP. I doubt this makes much difference, but haven't checked; I must admit, I wonder if there's some important point I'm missing here.<a href=\"#fnref14\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn15\" role=\"doc-endnote\"><p>Nicholas J. Fowler and Mike P. Williamson, <a href=\"https://www.sciencedirect.com/science/article/pii/S0969212622001320\">The accuracy of protein structures in solution determined by AlphaFold and NMR</a>, <em>Structure</em> (2022).<a href=\"#fnref15\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn16\" role=\"doc-endnote\"><p>David Silver, Thomas Hubert, Julian Schrittwieser <em>et al</em>, <a href=\"https://arxiv.org/abs/1712.01815\">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</a> (2017).<a href=\"#fnref16\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn17\" role=\"doc-endnote\"><p>David Silver, Thomas Hubert, Julian Schrittwieser <em>et al</em>, \"A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and Go Through Self-Play\", <em>Science</em> (2018).<a href=\"#fnref17\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn18\" role=\"doc-endnote\"><p>Natasha Regan and Matthew Sadler, \"Game Changer: AlphaZero’s Groundbreaking Chess Strategies and the Promise of AI\" (2019).<a href=\"#fnref18\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn19\" role=\"doc-endnote\"><p>Julio González-Díaz and Ignacio Palacios-Huerta, <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4140916\">AlphaZero Ideas</a> (2022).<a href=\"#fnref19\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn20\" role=\"doc-endnote\"><p>All of this is for classical games.<a href=\"#fnref20\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn21\" role=\"doc-endnote\"><p>The original work was done by Neel Nanda and Tom Lieberum, and presented in <a href=\"https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking\">A Mechanistic Interpretability Analysis of Grokking</a> (2022). A fuller writeup may be found in: Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt, <a href=\"https://arxiv.org/abs/2301.05217\">Progress measures for grokking via mechanistic interpretability</a> (2023).<a href=\"#fnref21\" class=\"footnote-back\" role=\"doc-backlink\">↩︎</a></p></li>\n      </ol>\n      </section>\n    </body>\n</html>\n","oembed":false,"readabilityObject":{"title":"How is AI impacting science?","content":"<div id=\"readability-page-1\" class=\"page\">\n\n  \n  <p><a href=\"/index.html\"><img src=\"/assets/home.png\" height=\"24px\"></a>\n  </p>\n\n    \n    \n    \n      <center>Michael Nielsen</center>\n\n      <center>Astera Institute</center>\n\n      <center>May 14, 2023</center>\n\n      <p> <em> The biggest\n      success of AI in science so far is the AlphaFold 2 system. This is a\n      deep learning system which has made large strides on a fundamental\n      scientific problem: how to predict the 3-dimensional structure of a\n      protein from the sequence of amino acids making up that protein.  This\n      breakthrough has helped set off an ongoing deep learning revolution in\n      molecular biology.  While obviously of interest to molecular\n      biologists, I believe this is of much broader interest for science as\n      a whole, as a concrete prototype for how artificial intelligence may\n      impact discovery. In this short survey talk I briefly discuss\n      questions including: how can such systems be validated?  Can they be\n      used to identify general principles that human scientists can learn\n      from? And what should we expect a good theory or explanation to\n      provide anyway?  The focus of the talk is extant results and the near\n      term, not the longer-term future.  However, I hope it may help in\n      grounding an understanding of that longer-term future, and of the\n      benefits and risks of AI systems.</em></p>\n\n      <p><em>The text for a talk given at the <a href=\"https://metascience.info/\">Metascience 2023 Conference</a> in Washington, D.C., May 2023.</em></p>\n      <p>In 2020 the deep learning system AlphaFold 2<a href=\"#fn1\" id=\"fnref1\" role=\"doc-noteref\"><sup>1</sup></a> surprised biologists when it was shown to routinely make correct near atomic-precision predictions for protein structure. That is, using just the linear sequence of amino acids making up a protein, AlphaFold was able to predict the positions of the atoms in the protein. These results were not cherrypicked, but rather emerged from an adversarial, blind competition with over a hundred other modeling groups. In a few cases, AlphaFold<a href=\"#fn2\" id=\"fnref2\" role=\"doc-noteref\"><sup>2</sup></a> has even exceeded experimental accuracy, causing existing experimental results to be re-evaluated and improved.</p>\n      <p>While AlphaFold is impressive, it's also far from complete: it's really a bridge to a new era, opening up many scientific and metascientific questions. These include: what we expect a good theory or explanation to provide; what it means to validate that understanding; and what we humans can learn from these systems. Most of all: whether and how AI systems may impact the progress of science as a whole, as a systemic intervention. In my talk, I'll treat AlphaFold as a concrete prototype for how AI may be used across science. For these reasons, it's valuable for metascientists to engage with AlphaFold and successor systems, even if you have no prior interest in proteins or even in biology.</p>\n      <p>The talk is a survey. I am not a molecular biologist, so my apologies for any errors on that front. That said, it's been enjoyable and often inspiring to learn about proteins. Let me take a few minutes to remind you of some background for those of you (like me) who aren't biologists. Molecular biology feels a bit like wandering into an immense workshop full of wonderful and varied machines. Large databases like UniProt contain the amino acid sequences for hundreds of millions of proteins. You have, for example, kinesin proteins, which transport large molecules around the interior of the cell. You have haemoglobin, which carries oxygen in your blood, and helps power your metabolism. You have green fluorescent protein, which emits green light when exposed to ultraviolet light, and which can be used to tag and track other biomolecules. All these and many more molecular machines, created and sorted by the demanding sieve of evolution by natural selection. Every individual machine could be the subject of a lifetime's study. There are, for instance, thousands of papers about the kinesin superfamily, and yet we're really just beginning to understand it. But while this wealth of biological machines is astonishing, we don't <em>a priori</em> know what those machines do, or how they do it. We have no instruction manual, and we're trying to figure it out.</p>\n      <p>In fact, for the vast majority of the hundreds of millions of proteins known<a href=\"#fn3\" id=\"fnref3\" role=\"doc-noteref\"><sup>3</sup></a>, all we can easily directly determine is the basic blueprint: using genome sequencing we can find the linear sequence of amino acids that form the protein, at a cost of no more than cents. But proteins are tiny 3-dimensional structures, typically nanometers across, making it extremely difficult to directly image them. For a single protein it will routinely take <em>months</em> of work to experimentally determine the corresponding 3-dimensional structure, usually using x-ray crystallography or cryo-electron microscopy or NMR.</p>\n      <p>This discrepancy really matters. It matters because understanding the shape is crucial for understanding questions like:</p>\n      <ul>\n      <li>What antigens can an antibody protein bind to, as part of an immune response?</li>\n      <li>What can the protein carry around? [E.g., the way haemoglobin carries oxygen]</li>\n      <li>How do proteins form larger complexes? [E.g., the ribosome]</li>\n      </ul>\n      <p>Understanding the shape of a protein doesn't tell you everything about its function. But it is fundamental to understanding what a protein can do, and how it does it.</p>\n      <p>Ideally, we'd be able to determine the shape from the amino acid sequence alone. There are reasons chemists and biologists expect this to often be possible<a href=\"#fn4\" id=\"fnref4\" role=\"doc-noteref\"><sup>4</sup></a>, and in the 1970s, scientists began doing physics simulations to attempt to determine the shape a protein will fold into. Many techniques have been adopted since (kinetics, thermodynamics, evolutionary, ….) For a long time these made only very slow progress. And even for that progress, one must wonder if modelers are cherrypicking, even with the best will in the world. There were, for instance, claims in the 1990s from Johns Hopkins to have largely \"solved\" the protein structure prediction problem<a href=\"#fn5\" id=\"fnref5\" role=\"doc-noteref\"><sup>5</sup></a>.</p>\n      <p>To assess progress in a fair but demanding way, in 1994 a competition named <a href=\"https://predictioncenter.org/\">CASP</a> (Critical Assessment of protein Structure Prediction) was begun. Running every two years, CASP asks modelers to do <em>blind predictions</em> of protein structure. That is, they're asked to predict structures for proteins whose amino acid sequence is known, but where biologists are still working on experimentally determining the three-dimensional structure, and expect to know it shortly after the competition is over, so it can be used to score predictions. The score for any given model is (very roughly) what percentage of amino acids positions are predicted correctly, according to some demanding threshold<a href=\"#fn6\" id=\"fnref6\" role=\"doc-noteref\"><sup>6</sup></a>. As you can see, through the 2010s the winner would typically score in the range 30 to 50<a href=\"#fn7\" id=\"fnref7\" role=\"doc-noteref\"><sup>7</sup></a>:</p>\n      <center><img src=\"/mc2023/assets/scores_GDT_partial.jpg\" width=\"446px\"></center>\n      <p>That is, the winner would (roughly) place somewhat than half of amino acids near perfectly. Then, in 2018 and 2020, DeepMind entered with AlphaFold and AlphaFold 2:</p>\n      <center><img src=\"/mc2023/assets/scores_GDT.jpg\" width=\"640px\"></center>\n      <p>I'll focus on CASP 14 in 2020, when AlphaFold 2 achieved of score of 87, so roughly 87 percent of amino acids within a demanding threshold. This was AlphaFold's score in the most demanding category, the free modeling category, where contestants are asked to predict the structure of proteins for which few similar proteins were previously known. Across all categories, AlphaFold predicted the position of the alpha-carbon atoms in the backbone with a root mean square distance of 0.96 Angstroms<a href=\"#fn8\" id=\"fnref8\" role=\"doc-noteref\"><sup>8</sup></a>. The next-best technique was roughly one third as accurate, with a root mean square distance of 2.8 Angstroms<a href=\"#fn9\" id=\"fnref9\" role=\"doc-noteref\"><sup>9</sup></a>:</p>\n      <center><img src=\"/mc2023/assets/rmsd.jpg\" width=\"400px\"></center>\n      <p>For comparison, the van der Waals diameter of a carbon atom is roughly 1.4 Angstroms. AlphaFold's accuracy is often comparable to experimental accuracy. The co-founder of CASP, John Moult of the University of Maryland, said<a href=\"#fn10\" id=\"fnref10\" role=\"doc-noteref\"><sup>10</sup></a>:</p>\n      <blockquote>\n      <p>We have been stuck on this one problem – how do proteins fold up – for nearly 50 years. To see DeepMind produce a solution for this, having worked personally on this problem for so long and after so many stops and starts, wondering if we’d ever get there, is a very special moment.</p>\n      </blockquote>\n      <p>That is a very strong statement, and it's worth digging into in what sense AlphaFold solves the protein structure prediction problem, and what remains to be understood. However, even AlphaFold's competitors were laudatory. Here's Mohammed AlQuraishi of Columbia University<a href=\"#fn11\" id=\"fnref11\" role=\"doc-noteref\"><sup>11</sup></a>:</p>\n      <blockquote>\n      <p>Does this constitute a solution of the static protein structure prediction problem? I think so but there are all these wrinkles. Honest, thoughtful people can disagree here and it comes down to one’s definition of what the word “solution” really means… the bulk of the scientific problem is solved; what’s left now is execution.</p>\n      </blockquote>\n      <p>That was two-and-a-half years ago. I think a reasonable broad view today is: AlphaFold is a huge leap, but much remains to be done even on the basic problem, and many enormous new problems can now be attacked.</p>\n      <p>AlphaFold's high-level architecture would take several hours to describe. I want to emphasize just a few points today:</p>\n      <img src=\"/mc2023/assets/architecture.png\" width=\"100%\">\n      <p>It's a deep neural network, meaning a hierarchical model, with 93 million parameters learned through training. An amino acid sequence is input, and a 3-dimensional structure is output, together with some error estimates quantifying how confident AlphaFold is in the placement of each amino acid. The basic training data is the protein data bank (PDB), humanity's repository of the protein structures experimentally determined since the 1970s. At training time, that was 170,000 protein structures (though a small fraction were omitted for technical reasons). The parameters in the AlphaFold network are adjusted using gradient descent to ensure the network outputs the correct structure, given the input.</p>\n      <p>That's a (very!) broad picture of how the network learned to predict structures. Many more ideas were important. One clever and important (albeit existing) idea was a way to learn from the hundreds of millions of known sequences for proteins. The idea is to find many other proteins whose amino acid sequences are similar to the input protein, meaning they're likely to be evolutionarily related. Maybe, for instance, it's essentially the same protein, but in some other species. AlphaFold tries to find many such similar proteins, and to learn from them. To get an intuition for how it might do that, suppose by looking at many similar proteins AlphaFold finds that there is some particular pair of amino acids which are a long way apart in the linear chain, but where changes in one amino acid seem to be correlated to changes in the other. If you saw that you might suspect these amino acids are likely to be close together in space, and are co-evolving together to preserve the shape of the protein. In fact, this often does happen, and it provides a way AlphaFold can learn both from <em>known structural information</em> (in the PDB), and from <em>known evolutionary information</em> (in protein sequence databases). A nice way of putting it, in <a href=\"https://www.youtube.com/watch?v=EjZMxq4oEt4&amp;t=443s&amp;ab_channel=RCSBProteinDataBank\">a talk from John Jumper</a>, lead author on the AlphaFold paper, is that the physics informs AlphaFold's understanding of the evolutionary history, and the evolutionary history informs AlphaFold's understanding of the physics.</p>\n      <h2 id=\"generalization-and-reliability\">Generalization and reliability</h2>\n      <p>You might wonder: is AlphaFold just memorizing its training data, or can it generalize? CASP provides a basic validation: the competition structures were not in the PDB at the time AlphaFold predicted them. Furthermore, CASP is in some sense a \"natural\" sample: the structural biology community prefers to solve structure which are biologically important<a href=\"#fn12\" id=\"fnref12\" role=\"doc-noteref\"><sup>12</sup></a>. So it suggests some ability to generalize to proteins of interest to biologists at large.</p>\n      <p>What if we use deep learning to study proteins which don't occur in nature, maybe as the result of mutations, or as part of protein design? There's a huge amount of work going on, and frankly it's an exciting mess right now, all over the place – any reasonable overview would cover dozens if not hundreds of papers. There are papers saying AlphaFold works <em>badly</em> for such-and-such a type of mutation, it works <em>well</em> for such-and-such a kind of mutation, all sorts of deep learning fixes for protein design, and so on. My takeaway: it's going to keep biologists busy for years figuring out the shortcomings, and improving the systems to address those shortcomings.</p>\n      <p>Some interesting tests of deep learning's ability to generalize were done by OpenFold<a href=\"#fn13\" id=\"fnref13\" role=\"doc-noteref\"><sup>13</sup></a>, an open source near-clone of AlphaFold. For instance, they retrained OpenFold with most broad classes of topology simply removed from the training set. To do this, they used a standard classification scheme, the CATH classification of protein topology. They removed 95% (!!!) of the topologies from the training set, and then retrained OpenFold from scratch. Even with the great majority of topologies removed, performance was similar to AlphaFold 1<a href=\"#fn14\" id=\"fnref14\" role=\"doc-noteref\"><sup>14</sup></a>: just a couple of years prior to AlphaFold 2 it would have been state-of-the-art.</p>\n      <p>In another variation on this idea, they retrained OpenFold starting from much smaller subsamples of the protein data bank. Instead of using 170,000 structures, they retrained with training sets as small as 1,000 structures, chosen at random. Even with such a tiny training set, they achieved a performance comparable to (in fact, slightly better than) AlphaFold 1. And with just 10,000 structures as training data they achieved a performance comparable to the full OpenFold.</p>\n      <p>As a practical matter, the question of AlphaFold generalization matters in part because DeepMind has released AlphaFold DB, a database of 215 million protein structures, including the (almost-complete) proteomes of 48 species, including humans, mice, and fruit flies. These were obtained by taking genetic sequences in UniProt, and then using AlphaFold to predict the structure. You can view the whole process as:</p>\n      <center><img src=\"/mc2023/assets/AFDB.png\" width=\"80%\"></center>\n      <p>It's an astonishing act of generalization. If we had a perfectly reliable model, it would extend our understanding of protein structures by roughly three orders of magnitude. It's worth emphasizing that no additional experiments were done by AlphaFold; no additional data were taken. And yet by \"just thinking\" it was possible to obtain a very large number of predictions that people expect to be very good. I've heard from several biologists variations on the sentiment: \"No-one would take an AlphaFold prediction as true on its own; but it’s an extremely helpful starting point, and can save you months of work\".</p>\n      <p>As the models get better still, I expect the line between model and experiment to become blurry. That may sound strange, but in fact traditional \"experimentally-determined structures\" actually require immensely-complicated theories to go from data to structure. If you believe AlphaFold (or a successor) offered a stronger theory, you might end up believing the predictions from the deep learning system more than you believe (today's) \"experimental results\". There are early hints of this beginning to happen. In the CASP assessment, AlphaFold performed poorly on several structures which had been determined using NMR spectroscopy. A 2022 paper<a href=\"#fn15\" id=\"fnref15\" role=\"doc-noteref\"><sup>15</sup></a> examined \"904 human proteins with both Alpha-Fold and NMR structures\" and concluded that \"Alpha-Fold predictions are usually more accurate than NMR structures\". One of the authors, Mike Williamson, actually helped pioneer the use of NMR for structural biology.</p>\n      <p>To make the same point in a simpler setting: how we interpret the images from a telescope depends upon our theory of optics; if we were to change or improve our theory of optics, our understanding of the so-called \"raw data\" from the telescope would change. Indeed, something related has really happened in our understanding of the bending of light by gravitation. In that case, we've changed our understanding of the way light travels through space, and that has affected the way we interpret experimental data, particularly in understanding phenomena like gravitational lensing of distant galaxies.</p>\n      <p>In a similar way, \"experimental\" protein structure determination depends strongly on theory. You can get a gist for this in x-ray crystallography, which requires many difficult steps: purification of protein sample; crystallization (!) of the proteins; x-ray diffraction to obtain two-dimensional images; procedures to invert and solve for the three-dimensional structure; criteria for when the inversion is good enough. A lot of theory is involved! Indeed, the inversion procedure typically involves starting with a good \"guess\", a candidate search structure. Often people use related proteins, but sometimes they can't find a good search structure, and this can prevent a solution being found. AlphaFold has been used to find good search structures to help invert data for particularly challenging structures. So there is already a blurry line between theory and experiment. I expect figuring out how to validate AI \"solutions\" to problems will be a major topic of scientific and metascientific interest in the years to come.</p>\n      <h2 id=\"is-a-simple-set-of-principles-for-protein-structure-possible-might-ai-help-us-discover-it\">Is a simple set of principles for protein structure possible? Might AI help us discover it?</h2>\n      <p>Any model with 93 million learned parameters is complicated. It's not a \"theory\" or \"explanation\" in the conventional sense. You might wonder: can AlphaFold (or a successor) be used to help discover such a theory, even if only partial? Might, for instance, a simple set of principles for protein structure prediction be possible? And what, exactly, is AlphaFold 2 learning? To avoid disappointment, let me say: we don't yet know the answers to such questions. But pursuing them is a useful intuition pump for thinking about the role of AI in science.</p>\n      <p>One approach to finding such principles is \"behaviorist artificial psychology\": inferring high-level principles by observing the behavior of the system. Of course, AlphaFold's detailed predictions have already been used widely by biologists, for things like discovering new binding sites on proteins. But while this is very useful, it isn't the same as inferring novel high-level principles about protein structure. However, there are other important deep learning systems in which interesting new high-level principles have been found by observing behavior.</p>\n      <p>For instance, by observing the AlphaZero chess system<a href=\"#fn16\" id=\"fnref16\" role=\"doc-noteref\"><sup>16</sup></a> behaviors have been inferred which violated conventional chess grandmaster wisdom. These behaviors were announced in December 2018<a href=\"#fn17\" id=\"fnref17\" role=\"doc-noteref\"><sup>17</sup></a> and January 2019<a href=\"#fn18\" id=\"fnref18\" role=\"doc-noteref\"><sup>18</sup></a>. A recent paper<a href=\"#fn19\" id=\"fnref19\" role=\"doc-noteref\"><sup>19</sup></a> examined these behaviors, and tried to determine how (and whether) human players had changed in response to the system. They found few changes in the top ten players in 2019, with one exception: the world's top player then and now, Magnus Carlsen. They identify multiple ways in which Carlsen changed his play significantly in 2019, plausibly influenced by AlphaZero.</p>\n      <p>Among the changes: Carlsen advanced his h pawn early in the game<a href=\"#fn20\" id=\"fnref20\" role=\"doc-noteref\"><sup>20</sup></a> much more frequently (a 333% increase with white, a 175% increase with black). He changed his opening strategies for both white and black. Indeed, his two most common 2019 opening strategies with white (variations of the Queen's Gambit declined and of the Grunfeld Defense) were openings he didn't play at all in 2018. And with black he played the Sicilian less than 10% of the time in 2018, but 45% of the time in 2019. They also observed an increase in Carlsen's willingness to sacrifice material. All these changes were consistent with learning from AlphaZero. Carlsen lost no classical games in 2019, and was the only top grandmaster to considerably improve his Elo rating, despite already being the top-rated player: he increased by 37 rating points. By contrast, 6 of the top 10 players actually lost points, and none of the other 3 gained more than 6 points. Of course, we do not know how much of this was due to Carlsen learning from AlphaZero, but the paper makes a plausible case that Carlsen learned much from AlphaZero.</p>\n      <p>Such behavioral observation is interesting but frustrating: it doesn't tell us <em>why</em> these behaviors occur. We may know that AlphaZero likes to advance the h pawn early, but what we'd really like is to understand why. Can we instead look inside the neural networks, and understand how they do what they do? As far as I know this kind of investigation has only been done casually for AlphaFold. But for simpler neural nets people are discovering interesting structure. Last year, for example, Neel Nanda and Tom Lieberum<a href=\"#fn21\" id=\"fnref21\" role=\"doc-noteref\"><sup>21</sup></a> trained a single-layer transformer neural network to add two integers modulo 113. At first, the network simply memorized all the examples in the training set. It could add those well, but (unsurprisingly) gave terrible performance elsewhere. But as they trained the network for much longer, it transitioned to performing vastly better on held-out examples. Somehow, with no additional training data, it was learning to add.</p>\n      <p>Nanda and Lieberum spent weeks looking inside the network to understand what had changed. By reverse engineering the network they discovered it had learned to add in a remarkable way. The rough gist is: given numbers <span>x</span> and <span>y</span> the network computed a wave <span>e^{2\\pi i\n      kx/113}</span>, then did a phase shift to <span>e^{2\\pi i k x/ 113} \\times\n      e^{2\\pi i ky/113}</span>, and finally tried to find a canceling phase shift <span>z</span> which would result in the wave <span>e^{2 \\pi i k(x+y-z)/113}</span> having the flattest possible \"tone\". It's a radio frequency engineer or group representation theorist's way of doing addition. This came as quite a surprise to Nanda and Lieberum. As Nanda said:</p>\n      <blockquote>\n      <p>To emphasize, this algorithm was purely learned by gradient descent! I did not predict or understand this algorithm in advance and did nothing to encourage the model to learn this way of doing modular addition. I only discovered it by reverse engineering the weights.</p>\n      </blockquote>\n      <p>This sequence of events greatly puzzled me when first I heard it: why did the network switch to this more general algorithm? After all, it initially memorized the training data, and provided excellent performance: why change? The answer is that during training the neural network pays a cost for more complex models: the loss function is chosen so gradient descent prefers lower-weight models. And the wave algorithm is actually lower-weight. It's a kind of mechanical implementation of Occam's razor, preferring an approach which is simpler and, in this case, more general. Indeed, by varying the loss function you can potentially impose Occam's razor in many different ways. It's intriguing to wonder: will one day we be able to do similar things for systems like AlphaFold, perhaps discovering new basic principles of protein structure?</p>\n      <h2 id=\"concluding-thought\">Concluding thought</h2>\n      <p>I hope you've enjoyed this brief look at AlphaFold and reflections on the use of AI in science. Of course, there's far more to say. But I believe it's clear there are many fundamental scientific and metascientific questions posed by AI systems. Most of all: as they get better at different types of cognitive operation, and as they become more able to sense and actuate in the environment, how will they impact science as a whole? Will AI systems eventually systematically change the entire practice of science? Will they perhaps greatly speed up scientific discovery? And if so, what risks and benefits does that carry?</p>\n      <h2 id=\"afterword-what-about-messier-problems-with-less-good-data-to-learn-from\">Afterword: what about messier problems, with less good data to learn from?</h2>\n      <p>At the end of my talk many interesting questions were asked by audience members. One striking question came from Evan Miyazono. It was, roughly: \"As a target for AI, protein structure prediction benefits from having a lot of very clean prior data which the system can learn from. How useful do you think AI will be for problems that don't have lots of clean prior data to learn from?\"</p>\n      <p>It's a great question. A year or so ago I would have said that yes, it seems challenging to apply AI when you're working with messier problems with less clear metrics of success. But I've changed my mind over the year, as I've better understood foundation models, transfer learning, and zero-shot learning.</p>\n      <p>To explain why I've changed my mind, let me give two pieces of context. First, some informal gossip: many people have observed that ChatGPT is much better at coding than GitHub Copilot. I don't know the reason for sure. But a common speculation I've heard from informed people is that training on lots of text (as well as code) substantially improves the model's ability to code. Somehow, it seems the regularities in the text are improving ChatGPT's ability to understand code as well.</p>\n      <p>The second piece of context: a similar idea is used with AlphaFold. To explain this I need to explain a piece of AlphaFold's training that I didn't discuss in the body of the talk: it actually implicitly includes a large language model, treating the sequence of amino acids as a kind of \"text\" to be filled in. To understand this, recall that given a sequence of amino acids, AlphaFold looks up similar sequences in existing genetic databases. And during training, just like a language model, AlphaFold masks out (or mutates) some of the individual amino acids in those sequences, and attempts to predict the missing values. Doing this forces the network to learn more about the structure of the evolutionary information; it then takes advantage of that forced understanding to do better at protein structure prediction. In this way, AlphaFold benefits from transfer learning from a related domain (genetics) where far more information is available.</p>\n      <p>Returning to the broader context of science, I expect large multi-modal foundation models will: (a) gradually begin to outperform special-purpose systems, just as large language models often outperform more specialized natural language algorithms; and (b) they will exhibit zero-shot or few-shot learning. These large multi-modal models will be trained not just on text, but also on images and actions and code and genetic data and geographic data and all sorts of sensor and actuator data. And, just as in the language models, they will use what they understand in one domain to help inform their work in adjacent, often messier domains, which the model may perhaps have limited information about. Indeed, there are already hints of protein language models moving in this direction.</p>\n      <p>Of course, all this is merely a story and an intuition. It hasn't happened yet, except in very nascent ways, and I may be quite wrong! I certainly expect new ideas will be needed to make this work well. But hopefully that sums up how my intuition has changed over the last year: I think AI models will, in the next few years, be surprisingly good at addressing messy problems with only a little prior data. And they'll do it using multi-model foundation models, transfer learning, and zero- and few-shot learning.</p>\n      <h2 id=\"acknowledgments\">Acknowledgments</h2>\n      <p>This talk was supported by the Astera Institute. Thanks to Alexander Berger, David Chapman, Evan Miyazono, Neel Nanda, and Pranav Shah for comments that helped improve this talk. And thanks to my many correspondents on Twitter, who have helped me deepen my understanding of AI and of molecular biology.</p>\n      <h2 id=\"citation-information\">Citation information</h2>\n      <p>For attribution in academic contexts, please cite this work as:</p>\n      <p>Michael Nielsen, \"How AI is impacting science\", <a href=\"https://michaelnotebook.com/mc2023/index.html\">https://michaelnotebook.com/mc2023/index.html</a>, San Francisco (2023).</p>\n      \n      <section role=\"doc-endnotes\">\n      <hr>\n      <ol>\n      <li id=\"fn1\" role=\"doc-endnote\"><p>John Jumper, Richard Evans, Alexander Pritzel <em>et al</em>, <a href=\"https://www.nature.com/articles/s41586-021-03819-2\">Highly accurate protein structure prediction with AlphaFold</a>, <em>Nature</em> (2021). See also <a href=\"https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb\">this colab</a>, which provides an executable version of AlphaFold from Google DeepMind, the company behind AlphaFold. Note that other sources provide faster implementations, with comparable performance. See, for instance this <a href=\"https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb\">ColabFold colab</a>. And, as we shall see later, other groups have developed open source implementations of the code training the system, not just for making structure predictions.<a href=\"#fnref1\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn2\" role=\"doc-endnote\"><p>I'll use \"AlphaFold\" to refer to AlphaFold 2. Of course, it is a followup to an earlier AlphaFold system for protein structure prediction. However, we won't discuss that system in any depth, and so I will usually omit the number. Note that AlphaFold 1 and AlphaFold 2 had very different architectures, apart from both being deep learning systems for protein structure prediction.<a href=\"#fnref2\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn3\" role=\"doc-endnote\"><p>Indeed, billions may be found (though of less clear provenance!) in metagenomics databases.<a href=\"#fnref3\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn4\" role=\"doc-endnote\"><p>This is a complicated subject, sometimes referred to as Anfinsen's hypothesis or Anfinsen's dogma. It is not true all the time – there are certainly circumstances under which the same amino acid sequence can fold in different ways, or has no stable structure. And yet it has served as an extremely useful basis for beginning to understand proteins and their function. In this talk I'll mostly assume it's true.<a href=\"#fnref4\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn5\" role=\"doc-endnote\"><p>See, for instance, this <a href=\"https://pages.jh.edu/jhumag/695web/profold.html\">remarkable press release</a> from Johns Hopkins, about the work of George Rose and Rajgopal Srinivasan \"solving\" the problem of predicting the backbone structure for globular proteins.<a href=\"#fnref5\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn6\" role=\"doc-endnote\"><p>More precisely, the global distance test (GDT) used in CASP is an average of four different percentages, measuring the fraction of predictions for the backbone alpha carbon atoms which fall within 1, 2, 4, and 8 Angstroms of the experimental structures. For comparison, I am told that good experimental structures are often thought to be accurate to about 1-2 Angstroms; better has been achieved, but worse is not uncommon, depending on the technique.<a href=\"#fnref6\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn7\" role=\"doc-endnote\"><p>This graph and the one that follow are adapted from <a href=\"https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology\">AlphaFold: a solution to a 50-year-old grand challenge in biology</a> (2021).<a href=\"#fnref7\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn8\" role=\"doc-endnote\"><p>This was for 95% coverage of the alpha carbons. For 100% coverage, it was 1.5 Angstroms.<a href=\"#fnref8\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn9\" role=\"doc-endnote\"><p>Figure from: John Jumper, Richard Evans, Alexander Pritzel <em>et al</em>, <a href=\"https://www.nature.com/articles/s41586-021-03819-2\">Highly accurate protein structure prediction with AlphaFold</a>, <em>Nature</em> (2021).<a href=\"#fnref9\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn10\" role=\"doc-endnote\"><p>Quoted in: <a href=\"https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology\">AlphaFold: a solution to a 50-year-old grand challenge in biology</a> (2021).<a href=\"#fnref10\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn11\" role=\"doc-endnote\"><p>Mohammed AlQuraishi, <a href=\"https://moalquraishi.wordpress.com/2020/12/08/alphafold2-casp14-it-feels-like-ones-child-has-left-home/\">AlphaFold2 @ CASP14: “It feels like one’s child has left home.”</a> (2020).<a href=\"#fnref11\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn12\" role=\"doc-endnote\"><p>Of course, other concerns like the experimental tractability of a structure also play a role.<a href=\"#fnref12\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn13\" role=\"doc-endnote\"><p>Gustaf Ahdritz, Nazim Bouatta, Sachin Kadyan <em>et al</em>, <a href=\"https://www.biorxiv.org/content/10.1101/2022.11.20.517210v2.full\">OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization</a> (2022).<a href=\"#fnref13\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn14\" role=\"doc-endnote\"><p>They use the local distance difference test (lDDT-C<span>\\alpha</span>) as their metric, rather than the global distance test emphasized in AlphaFold's performance on CASP. I doubt this makes much difference, but haven't checked; I must admit, I wonder if there's some important point I'm missing here.<a href=\"#fnref14\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn15\" role=\"doc-endnote\"><p>Nicholas J. Fowler and Mike P. Williamson, <a href=\"https://www.sciencedirect.com/science/article/pii/S0969212622001320\">The accuracy of protein structures in solution determined by AlphaFold and NMR</a>, <em>Structure</em> (2022).<a href=\"#fnref15\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn16\" role=\"doc-endnote\"><p>David Silver, Thomas Hubert, Julian Schrittwieser <em>et al</em>, <a href=\"https://arxiv.org/abs/1712.01815\">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</a> (2017).<a href=\"#fnref16\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn17\" role=\"doc-endnote\"><p>David Silver, Thomas Hubert, Julian Schrittwieser <em>et al</em>, \"A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and Go Through Self-Play\", <em>Science</em> (2018).<a href=\"#fnref17\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn18\" role=\"doc-endnote\"><p>Natasha Regan and Matthew Sadler, \"Game Changer: AlphaZero’s Groundbreaking Chess Strategies and the Promise of AI\" (2019).<a href=\"#fnref18\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn19\" role=\"doc-endnote\"><p>Julio González-Díaz and Ignacio Palacios-Huerta, <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4140916\">AlphaZero Ideas</a> (2022).<a href=\"#fnref19\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn20\" role=\"doc-endnote\"><p>All of this is for classical games.<a href=\"#fnref20\" role=\"doc-backlink\">↩︎</a></p></li>\n      <li id=\"fn21\" role=\"doc-endnote\"><p>The original work was done by Neel Nanda and Tom Lieberum, and presented in <a href=\"https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking\">A Mechanistic Interpretability Analysis of Grokking</a> (2022). A fuller writeup may be found in: Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt, <a href=\"https://arxiv.org/abs/2301.05217\">Progress measures for grokking via mechanistic interpretability</a> (2023).<a href=\"#fnref21\" role=\"doc-backlink\">↩︎</a></p></li>\n      </ol>\n      </section>\n    \n\n</div>","textContent":"\n\n  \n  \n  \n\n    \n    \n    \n      Michael Nielsen\n\n      Astera Institute\n\n      May 14, 2023\n\n        The biggest\n      success of AI in science so far is the AlphaFold 2 system. This is a\n      deep learning system which has made large strides on a fundamental\n      scientific problem: how to predict the 3-dimensional structure of a\n      protein from the sequence of amino acids making up that protein.  This\n      breakthrough has helped set off an ongoing deep learning revolution in\n      molecular biology.  While obviously of interest to molecular\n      biologists, I believe this is of much broader interest for science as\n      a whole, as a concrete prototype for how artificial intelligence may\n      impact discovery. In this short survey talk I briefly discuss\n      questions including: how can such systems be validated?  Can they be\n      used to identify general principles that human scientists can learn\n      from? And what should we expect a good theory or explanation to\n      provide anyway?  The focus of the talk is extant results and the near\n      term, not the longer-term future.  However, I hope it may help in\n      grounding an understanding of that longer-term future, and of the\n      benefits and risks of AI systems.\n\n      The text for a talk given at the Metascience 2023 Conference in Washington, D.C., May 2023.\n      In 2020 the deep learning system AlphaFold 21 surprised biologists when it was shown to routinely make correct near atomic-precision predictions for protein structure. That is, using just the linear sequence of amino acids making up a protein, AlphaFold was able to predict the positions of the atoms in the protein. These results were not cherrypicked, but rather emerged from an adversarial, blind competition with over a hundred other modeling groups. In a few cases, AlphaFold2 has even exceeded experimental accuracy, causing existing experimental results to be re-evaluated and improved.\n      While AlphaFold is impressive, it's also far from complete: it's really a bridge to a new era, opening up many scientific and metascientific questions. These include: what we expect a good theory or explanation to provide; what it means to validate that understanding; and what we humans can learn from these systems. Most of all: whether and how AI systems may impact the progress of science as a whole, as a systemic intervention. In my talk, I'll treat AlphaFold as a concrete prototype for how AI may be used across science. For these reasons, it's valuable for metascientists to engage with AlphaFold and successor systems, even if you have no prior interest in proteins or even in biology.\n      The talk is a survey. I am not a molecular biologist, so my apologies for any errors on that front. That said, it's been enjoyable and often inspiring to learn about proteins. Let me take a few minutes to remind you of some background for those of you (like me) who aren't biologists. Molecular biology feels a bit like wandering into an immense workshop full of wonderful and varied machines. Large databases like UniProt contain the amino acid sequences for hundreds of millions of proteins. You have, for example, kinesin proteins, which transport large molecules around the interior of the cell. You have haemoglobin, which carries oxygen in your blood, and helps power your metabolism. You have green fluorescent protein, which emits green light when exposed to ultraviolet light, and which can be used to tag and track other biomolecules. All these and many more molecular machines, created and sorted by the demanding sieve of evolution by natural selection. Every individual machine could be the subject of a lifetime's study. There are, for instance, thousands of papers about the kinesin superfamily, and yet we're really just beginning to understand it. But while this wealth of biological machines is astonishing, we don't a priori know what those machines do, or how they do it. We have no instruction manual, and we're trying to figure it out.\n      In fact, for the vast majority of the hundreds of millions of proteins known3, all we can easily directly determine is the basic blueprint: using genome sequencing we can find the linear sequence of amino acids that form the protein, at a cost of no more than cents. But proteins are tiny 3-dimensional structures, typically nanometers across, making it extremely difficult to directly image them. For a single protein it will routinely take months of work to experimentally determine the corresponding 3-dimensional structure, usually using x-ray crystallography or cryo-electron microscopy or NMR.\n      This discrepancy really matters. It matters because understanding the shape is crucial for understanding questions like:\n      \n      What antigens can an antibody protein bind to, as part of an immune response?\n      What can the protein carry around? [E.g., the way haemoglobin carries oxygen]\n      How do proteins form larger complexes? [E.g., the ribosome]\n      \n      Understanding the shape of a protein doesn't tell you everything about its function. But it is fundamental to understanding what a protein can do, and how it does it.\n      Ideally, we'd be able to determine the shape from the amino acid sequence alone. There are reasons chemists and biologists expect this to often be possible4, and in the 1970s, scientists began doing physics simulations to attempt to determine the shape a protein will fold into. Many techniques have been adopted since (kinetics, thermodynamics, evolutionary, ….) For a long time these made only very slow progress. And even for that progress, one must wonder if modelers are cherrypicking, even with the best will in the world. There were, for instance, claims in the 1990s from Johns Hopkins to have largely \"solved\" the protein structure prediction problem5.\n      To assess progress in a fair but demanding way, in 1994 a competition named CASP (Critical Assessment of protein Structure Prediction) was begun. Running every two years, CASP asks modelers to do blind predictions of protein structure. That is, they're asked to predict structures for proteins whose amino acid sequence is known, but where biologists are still working on experimentally determining the three-dimensional structure, and expect to know it shortly after the competition is over, so it can be used to score predictions. The score for any given model is (very roughly) what percentage of amino acids positions are predicted correctly, according to some demanding threshold6. As you can see, through the 2010s the winner would typically score in the range 30 to 507:\n      \n      That is, the winner would (roughly) place somewhat than half of amino acids near perfectly. Then, in 2018 and 2020, DeepMind entered with AlphaFold and AlphaFold 2:\n      \n      I'll focus on CASP 14 in 2020, when AlphaFold 2 achieved of score of 87, so roughly 87 percent of amino acids within a demanding threshold. This was AlphaFold's score in the most demanding category, the free modeling category, where contestants are asked to predict the structure of proteins for which few similar proteins were previously known. Across all categories, AlphaFold predicted the position of the alpha-carbon atoms in the backbone with a root mean square distance of 0.96 Angstroms8. The next-best technique was roughly one third as accurate, with a root mean square distance of 2.8 Angstroms9:\n      \n      For comparison, the van der Waals diameter of a carbon atom is roughly 1.4 Angstroms. AlphaFold's accuracy is often comparable to experimental accuracy. The co-founder of CASP, John Moult of the University of Maryland, said10:\n      \n      We have been stuck on this one problem – how do proteins fold up – for nearly 50 years. To see DeepMind produce a solution for this, having worked personally on this problem for so long and after so many stops and starts, wondering if we’d ever get there, is a very special moment.\n      \n      That is a very strong statement, and it's worth digging into in what sense AlphaFold solves the protein structure prediction problem, and what remains to be understood. However, even AlphaFold's competitors were laudatory. Here's Mohammed AlQuraishi of Columbia University11:\n      \n      Does this constitute a solution of the static protein structure prediction problem? I think so but there are all these wrinkles. Honest, thoughtful people can disagree here and it comes down to one’s definition of what the word “solution” really means… the bulk of the scientific problem is solved; what’s left now is execution.\n      \n      That was two-and-a-half years ago. I think a reasonable broad view today is: AlphaFold is a huge leap, but much remains to be done even on the basic problem, and many enormous new problems can now be attacked.\n      AlphaFold's high-level architecture would take several hours to describe. I want to emphasize just a few points today:\n      \n      It's a deep neural network, meaning a hierarchical model, with 93 million parameters learned through training. An amino acid sequence is input, and a 3-dimensional structure is output, together with some error estimates quantifying how confident AlphaFold is in the placement of each amino acid. The basic training data is the protein data bank (PDB), humanity's repository of the protein structures experimentally determined since the 1970s. At training time, that was 170,000 protein structures (though a small fraction were omitted for technical reasons). The parameters in the AlphaFold network are adjusted using gradient descent to ensure the network outputs the correct structure, given the input.\n      That's a (very!) broad picture of how the network learned to predict structures. Many more ideas were important. One clever and important (albeit existing) idea was a way to learn from the hundreds of millions of known sequences for proteins. The idea is to find many other proteins whose amino acid sequences are similar to the input protein, meaning they're likely to be evolutionarily related. Maybe, for instance, it's essentially the same protein, but in some other species. AlphaFold tries to find many such similar proteins, and to learn from them. To get an intuition for how it might do that, suppose by looking at many similar proteins AlphaFold finds that there is some particular pair of amino acids which are a long way apart in the linear chain, but where changes in one amino acid seem to be correlated to changes in the other. If you saw that you might suspect these amino acids are likely to be close together in space, and are co-evolving together to preserve the shape of the protein. In fact, this often does happen, and it provides a way AlphaFold can learn both from known structural information (in the PDB), and from known evolutionary information (in protein sequence databases). A nice way of putting it, in a talk from John Jumper, lead author on the AlphaFold paper, is that the physics informs AlphaFold's understanding of the evolutionary history, and the evolutionary history informs AlphaFold's understanding of the physics.\n      Generalization and reliability\n      You might wonder: is AlphaFold just memorizing its training data, or can it generalize? CASP provides a basic validation: the competition structures were not in the PDB at the time AlphaFold predicted them. Furthermore, CASP is in some sense a \"natural\" sample: the structural biology community prefers to solve structure which are biologically important12. So it suggests some ability to generalize to proteins of interest to biologists at large.\n      What if we use deep learning to study proteins which don't occur in nature, maybe as the result of mutations, or as part of protein design? There's a huge amount of work going on, and frankly it's an exciting mess right now, all over the place – any reasonable overview would cover dozens if not hundreds of papers. There are papers saying AlphaFold works badly for such-and-such a type of mutation, it works well for such-and-such a kind of mutation, all sorts of deep learning fixes for protein design, and so on. My takeaway: it's going to keep biologists busy for years figuring out the shortcomings, and improving the systems to address those shortcomings.\n      Some interesting tests of deep learning's ability to generalize were done by OpenFold13, an open source near-clone of AlphaFold. For instance, they retrained OpenFold with most broad classes of topology simply removed from the training set. To do this, they used a standard classification scheme, the CATH classification of protein topology. They removed 95% (!!!) of the topologies from the training set, and then retrained OpenFold from scratch. Even with the great majority of topologies removed, performance was similar to AlphaFold 114: just a couple of years prior to AlphaFold 2 it would have been state-of-the-art.\n      In another variation on this idea, they retrained OpenFold starting from much smaller subsamples of the protein data bank. Instead of using 170,000 structures, they retrained with training sets as small as 1,000 structures, chosen at random. Even with such a tiny training set, they achieved a performance comparable to (in fact, slightly better than) AlphaFold 1. And with just 10,000 structures as training data they achieved a performance comparable to the full OpenFold.\n      As a practical matter, the question of AlphaFold generalization matters in part because DeepMind has released AlphaFold DB, a database of 215 million protein structures, including the (almost-complete) proteomes of 48 species, including humans, mice, and fruit flies. These were obtained by taking genetic sequences in UniProt, and then using AlphaFold to predict the structure. You can view the whole process as:\n      \n      It's an astonishing act of generalization. If we had a perfectly reliable model, it would extend our understanding of protein structures by roughly three orders of magnitude. It's worth emphasizing that no additional experiments were done by AlphaFold; no additional data were taken. And yet by \"just thinking\" it was possible to obtain a very large number of predictions that people expect to be very good. I've heard from several biologists variations on the sentiment: \"No-one would take an AlphaFold prediction as true on its own; but it’s an extremely helpful starting point, and can save you months of work\".\n      As the models get better still, I expect the line between model and experiment to become blurry. That may sound strange, but in fact traditional \"experimentally-determined structures\" actually require immensely-complicated theories to go from data to structure. If you believe AlphaFold (or a successor) offered a stronger theory, you might end up believing the predictions from the deep learning system more than you believe (today's) \"experimental results\". There are early hints of this beginning to happen. In the CASP assessment, AlphaFold performed poorly on several structures which had been determined using NMR spectroscopy. A 2022 paper15 examined \"904 human proteins with both Alpha-Fold and NMR structures\" and concluded that \"Alpha-Fold predictions are usually more accurate than NMR structures\". One of the authors, Mike Williamson, actually helped pioneer the use of NMR for structural biology.\n      To make the same point in a simpler setting: how we interpret the images from a telescope depends upon our theory of optics; if we were to change or improve our theory of optics, our understanding of the so-called \"raw data\" from the telescope would change. Indeed, something related has really happened in our understanding of the bending of light by gravitation. In that case, we've changed our understanding of the way light travels through space, and that has affected the way we interpret experimental data, particularly in understanding phenomena like gravitational lensing of distant galaxies.\n      In a similar way, \"experimental\" protein structure determination depends strongly on theory. You can get a gist for this in x-ray crystallography, which requires many difficult steps: purification of protein sample; crystallization (!) of the proteins; x-ray diffraction to obtain two-dimensional images; procedures to invert and solve for the three-dimensional structure; criteria for when the inversion is good enough. A lot of theory is involved! Indeed, the inversion procedure typically involves starting with a good \"guess\", a candidate search structure. Often people use related proteins, but sometimes they can't find a good search structure, and this can prevent a solution being found. AlphaFold has been used to find good search structures to help invert data for particularly challenging structures. So there is already a blurry line between theory and experiment. I expect figuring out how to validate AI \"solutions\" to problems will be a major topic of scientific and metascientific interest in the years to come.\n      Is a simple set of principles for protein structure possible? Might AI help us discover it?\n      Any model with 93 million learned parameters is complicated. It's not a \"theory\" or \"explanation\" in the conventional sense. You might wonder: can AlphaFold (or a successor) be used to help discover such a theory, even if only partial? Might, for instance, a simple set of principles for protein structure prediction be possible? And what, exactly, is AlphaFold 2 learning? To avoid disappointment, let me say: we don't yet know the answers to such questions. But pursuing them is a useful intuition pump for thinking about the role of AI in science.\n      One approach to finding such principles is \"behaviorist artificial psychology\": inferring high-level principles by observing the behavior of the system. Of course, AlphaFold's detailed predictions have already been used widely by biologists, for things like discovering new binding sites on proteins. But while this is very useful, it isn't the same as inferring novel high-level principles about protein structure. However, there are other important deep learning systems in which interesting new high-level principles have been found by observing behavior.\n      For instance, by observing the AlphaZero chess system16 behaviors have been inferred which violated conventional chess grandmaster wisdom. These behaviors were announced in December 201817 and January 201918. A recent paper19 examined these behaviors, and tried to determine how (and whether) human players had changed in response to the system. They found few changes in the top ten players in 2019, with one exception: the world's top player then and now, Magnus Carlsen. They identify multiple ways in which Carlsen changed his play significantly in 2019, plausibly influenced by AlphaZero.\n      Among the changes: Carlsen advanced his h pawn early in the game20 much more frequently (a 333% increase with white, a 175% increase with black). He changed his opening strategies for both white and black. Indeed, his two most common 2019 opening strategies with white (variations of the Queen's Gambit declined and of the Grunfeld Defense) were openings he didn't play at all in 2018. And with black he played the Sicilian less than 10% of the time in 2018, but 45% of the time in 2019. They also observed an increase in Carlsen's willingness to sacrifice material. All these changes were consistent with learning from AlphaZero. Carlsen lost no classical games in 2019, and was the only top grandmaster to considerably improve his Elo rating, despite already being the top-rated player: he increased by 37 rating points. By contrast, 6 of the top 10 players actually lost points, and none of the other 3 gained more than 6 points. Of course, we do not know how much of this was due to Carlsen learning from AlphaZero, but the paper makes a plausible case that Carlsen learned much from AlphaZero.\n      Such behavioral observation is interesting but frustrating: it doesn't tell us why these behaviors occur. We may know that AlphaZero likes to advance the h pawn early, but what we'd really like is to understand why. Can we instead look inside the neural networks, and understand how they do what they do? As far as I know this kind of investigation has only been done casually for AlphaFold. But for simpler neural nets people are discovering interesting structure. Last year, for example, Neel Nanda and Tom Lieberum21 trained a single-layer transformer neural network to add two integers modulo 113. At first, the network simply memorized all the examples in the training set. It could add those well, but (unsurprisingly) gave terrible performance elsewhere. But as they trained the network for much longer, it transitioned to performing vastly better on held-out examples. Somehow, with no additional training data, it was learning to add.\n      Nanda and Lieberum spent weeks looking inside the network to understand what had changed. By reverse engineering the network they discovered it had learned to add in a remarkable way. The rough gist is: given numbers x and y the network computed a wave e^{2\\pi i\n      kx/113}, then did a phase shift to e^{2\\pi i k x/ 113} \\times\n      e^{2\\pi i ky/113}, and finally tried to find a canceling phase shift z which would result in the wave e^{2 \\pi i k(x+y-z)/113} having the flattest possible \"tone\". It's a radio frequency engineer or group representation theorist's way of doing addition. This came as quite a surprise to Nanda and Lieberum. As Nanda said:\n      \n      To emphasize, this algorithm was purely learned by gradient descent! I did not predict or understand this algorithm in advance and did nothing to encourage the model to learn this way of doing modular addition. I only discovered it by reverse engineering the weights.\n      \n      This sequence of events greatly puzzled me when first I heard it: why did the network switch to this more general algorithm? After all, it initially memorized the training data, and provided excellent performance: why change? The answer is that during training the neural network pays a cost for more complex models: the loss function is chosen so gradient descent prefers lower-weight models. And the wave algorithm is actually lower-weight. It's a kind of mechanical implementation of Occam's razor, preferring an approach which is simpler and, in this case, more general. Indeed, by varying the loss function you can potentially impose Occam's razor in many different ways. It's intriguing to wonder: will one day we be able to do similar things for systems like AlphaFold, perhaps discovering new basic principles of protein structure?\n      Concluding thought\n      I hope you've enjoyed this brief look at AlphaFold and reflections on the use of AI in science. Of course, there's far more to say. But I believe it's clear there are many fundamental scientific and metascientific questions posed by AI systems. Most of all: as they get better at different types of cognitive operation, and as they become more able to sense and actuate in the environment, how will they impact science as a whole? Will AI systems eventually systematically change the entire practice of science? Will they perhaps greatly speed up scientific discovery? And if so, what risks and benefits does that carry?\n      Afterword: what about messier problems, with less good data to learn from?\n      At the end of my talk many interesting questions were asked by audience members. One striking question came from Evan Miyazono. It was, roughly: \"As a target for AI, protein structure prediction benefits from having a lot of very clean prior data which the system can learn from. How useful do you think AI will be for problems that don't have lots of clean prior data to learn from?\"\n      It's a great question. A year or so ago I would have said that yes, it seems challenging to apply AI when you're working with messier problems with less clear metrics of success. But I've changed my mind over the year, as I've better understood foundation models, transfer learning, and zero-shot learning.\n      To explain why I've changed my mind, let me give two pieces of context. First, some informal gossip: many people have observed that ChatGPT is much better at coding than GitHub Copilot. I don't know the reason for sure. But a common speculation I've heard from informed people is that training on lots of text (as well as code) substantially improves the model's ability to code. Somehow, it seems the regularities in the text are improving ChatGPT's ability to understand code as well.\n      The second piece of context: a similar idea is used with AlphaFold. To explain this I need to explain a piece of AlphaFold's training that I didn't discuss in the body of the talk: it actually implicitly includes a large language model, treating the sequence of amino acids as a kind of \"text\" to be filled in. To understand this, recall that given a sequence of amino acids, AlphaFold looks up similar sequences in existing genetic databases. And during training, just like a language model, AlphaFold masks out (or mutates) some of the individual amino acids in those sequences, and attempts to predict the missing values. Doing this forces the network to learn more about the structure of the evolutionary information; it then takes advantage of that forced understanding to do better at protein structure prediction. In this way, AlphaFold benefits from transfer learning from a related domain (genetics) where far more information is available.\n      Returning to the broader context of science, I expect large multi-modal foundation models will: (a) gradually begin to outperform special-purpose systems, just as large language models often outperform more specialized natural language algorithms; and (b) they will exhibit zero-shot or few-shot learning. These large multi-modal models will be trained not just on text, but also on images and actions and code and genetic data and geographic data and all sorts of sensor and actuator data. And, just as in the language models, they will use what they understand in one domain to help inform their work in adjacent, often messier domains, which the model may perhaps have limited information about. Indeed, there are already hints of protein language models moving in this direction.\n      Of course, all this is merely a story and an intuition. It hasn't happened yet, except in very nascent ways, and I may be quite wrong! I certainly expect new ideas will be needed to make this work well. But hopefully that sums up how my intuition has changed over the last year: I think AI models will, in the next few years, be surprisingly good at addressing messy problems with only a little prior data. And they'll do it using multi-model foundation models, transfer learning, and zero- and few-shot learning.\n      Acknowledgments\n      This talk was supported by the Astera Institute. Thanks to Alexander Berger, David Chapman, Evan Miyazono, Neel Nanda, and Pranav Shah for comments that helped improve this talk. And thanks to my many correspondents on Twitter, who have helped me deepen my understanding of AI and of molecular biology.\n      Citation information\n      For attribution in academic contexts, please cite this work as:\n      Michael Nielsen, \"How AI is impacting science\", https://michaelnotebook.com/mc2023/index.html, San Francisco (2023).\n      \n      \n      \n      \n      John Jumper, Richard Evans, Alexander Pritzel et al, Highly accurate protein structure prediction with AlphaFold, Nature (2021). See also this colab, which provides an executable version of AlphaFold from Google DeepMind, the company behind AlphaFold. Note that other sources provide faster implementations, with comparable performance. See, for instance this ColabFold colab. And, as we shall see later, other groups have developed open source implementations of the code training the system, not just for making structure predictions.↩︎\n      I'll use \"AlphaFold\" to refer to AlphaFold 2. Of course, it is a followup to an earlier AlphaFold system for protein structure prediction. However, we won't discuss that system in any depth, and so I will usually omit the number. Note that AlphaFold 1 and AlphaFold 2 had very different architectures, apart from both being deep learning systems for protein structure prediction.↩︎\n      Indeed, billions may be found (though of less clear provenance!) in metagenomics databases.↩︎\n      This is a complicated subject, sometimes referred to as Anfinsen's hypothesis or Anfinsen's dogma. It is not true all the time – there are certainly circumstances under which the same amino acid sequence can fold in different ways, or has no stable structure. And yet it has served as an extremely useful basis for beginning to understand proteins and their function. In this talk I'll mostly assume it's true.↩︎\n      See, for instance, this remarkable press release from Johns Hopkins, about the work of George Rose and Rajgopal Srinivasan \"solving\" the problem of predicting the backbone structure for globular proteins.↩︎\n      More precisely, the global distance test (GDT) used in CASP is an average of four different percentages, measuring the fraction of predictions for the backbone alpha carbon atoms which fall within 1, 2, 4, and 8 Angstroms of the experimental structures. For comparison, I am told that good experimental structures are often thought to be accurate to about 1-2 Angstroms; better has been achieved, but worse is not uncommon, depending on the technique.↩︎\n      This graph and the one that follow are adapted from AlphaFold: a solution to a 50-year-old grand challenge in biology (2021).↩︎\n      This was for 95% coverage of the alpha carbons. For 100% coverage, it was 1.5 Angstroms.↩︎\n      Figure from: John Jumper, Richard Evans, Alexander Pritzel et al, Highly accurate protein structure prediction with AlphaFold, Nature (2021).↩︎\n      Quoted in: AlphaFold: a solution to a 50-year-old grand challenge in biology (2021).↩︎\n      Mohammed AlQuraishi, AlphaFold2 @ CASP14: “It feels like one’s child has left home.” (2020).↩︎\n      Of course, other concerns like the experimental tractability of a structure also play a role.↩︎\n      Gustaf Ahdritz, Nazim Bouatta, Sachin Kadyan et al, OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization (2022).↩︎\n      They use the local distance difference test (lDDT-C\\alpha) as their metric, rather than the global distance test emphasized in AlphaFold's performance on CASP. I doubt this makes much difference, but haven't checked; I must admit, I wonder if there's some important point I'm missing here.↩︎\n      Nicholas J. Fowler and Mike P. Williamson, The accuracy of protein structures in solution determined by AlphaFold and NMR, Structure (2022).↩︎\n      David Silver, Thomas Hubert, Julian Schrittwieser et al, Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm (2017).↩︎\n      David Silver, Thomas Hubert, Julian Schrittwieser et al, \"A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and Go Through Self-Play\", Science (2018).↩︎\n      Natasha Regan and Matthew Sadler, \"Game Changer: AlphaZero’s Groundbreaking Chess Strategies and the Promise of AI\" (2019).↩︎\n      Julio González-Díaz and Ignacio Palacios-Huerta, AlphaZero Ideas (2022).↩︎\n      All of this is for classical games.↩︎\n      The original work was done by Neel Nanda and Tom Lieberum, and presented in A Mechanistic Interpretability Analysis of Grokking (2022). A fuller writeup may be found in: Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt, Progress measures for grokking via mechanistic interpretability (2023).↩︎\n      \n      \n    \n\n","length":31814,"excerpt":"","byline":null,"dir":null,"siteName":null,"lang":""},"finalizedMeta":{"title":"How is AI impacting science?","description":"","author":false,"creator":"","publisher":false,"date":"2023-07-11T16:37:39.158Z","topics":[]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"How is AI impacting science?","description":false,"canonical":"https://michaelnotebook.com/mc2023/","keywords":[],"image":"/assets/home.png","firstParagraph":"  The biggest\n      success of AI in science so far is the AlphaFold 2 system. This is a\n      deep learning system which has made large strides on a fundamental\n      scientific problem: how to predict the 3-dimensional structure of a\n      protein from the sequence of amino acids making up that protein.  This\n      breakthrough has helped set off an ongoing deep learning revolution in\n      molecular biology.  While obviously of interest to molecular\n      biologists, I believe this is of much broader interest for science as\n      a whole, as a concrete prototype for how artificial intelligence may\n      impact discovery. In this short survey talk I briefly discuss\n      questions including: how can such systems be validated?  Can they be\n      used to identify general principles that human scientists can learn\n      from? And what should we expect a good theory or explanation to\n      provide anyway?  The focus of the talk is extant results and the near\n      term, not the longer-term future.  However, I hope it may help in\n      grounding an understanding of that longer-term future, and of the\n      benefits and risks of AI systems."},"dublinCore":{},"opengraph":{"title":false,"description":false,"url":false,"site_name":false,"locale":false,"type":false,"typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":false},"twitter":{"site":false,"description":false,"card":false,"creator":false,"title":false,"image":false},"archivedData":{"link":"https://web.archive.org/web/20230711163753/https://michaelnotebook.com/mc2023/","wayback":"https://web.archive.org/web/20230711163753/https://michaelnotebook.com/mc2023/"}}}