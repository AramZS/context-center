{"initialLink":"https://interconnected.org/home/2023/02/07/braggoscope","sanitizedLink":"https://interconnected.org/home/2023/02/07/braggoscope","finalLink":"https://interconnected.org/home/2023/02/07/braggoscope","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://interconnected.org/home/2023/02/07/braggoscope\" itemprop=\"url\">New thing! Browse the BBC In Our Time archive by Dewey decimal code</a></contexter-box-head></contexter-box-head><contexter-byline class=\"p-author author\" slot=\"author\"><span class=\"p-name byline\" rel=\"author\" itemprop=\"author\">@genmon</span></contexter-byline><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2023-04-24T14:13:31.561Z\">4/24/2023</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>Posted on Tuesday 7 Feb 2023. 1,208 words, 14 links. By Matt Webb.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a is=\"contexter-link\" href=\"https://interconnected.org/home/2023/02/07/braggoscope\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"1a3096e9f7cfea3cdfa6e3ab8e6369aa985198f6","data":{"originalLink":"https://interconnected.org/home/2023/02/07/braggoscope","sanitizedLink":"https://interconnected.org/home/2023/02/07/braggoscope","canonical":"https://interconnected.org/home/2023/02/07/braggoscope","htmlText":"\n<!DOCTYPE html>\n<html lang=\"en\"><!--\n\n       .....           .....\n   ,ad8PPPP88b,     ,d88PPPP8ba,\n  d8P\"      \"Y8b, ,d8P\"      \"Y8b\n dP'           \"8a8\"           `Yd\n 8(              \"              )8\n I8                             8I\n  Yb,          T h e          ,dP\n   \"8a,      W o r l d      ,a8\"\n     \"8a,     W i d e     ,a8\"\n       \"Yba    W e b    adP\"\n         `Y8a         a8P'\n           `88,     ,88'\n             \"8b   d8\"\n              \"8b d8\"\n               `888'\n                 \"\n\n-->\n  <head>\n    \n\n\n\n<!-- Google tag (gtag.js) -->\n<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-BSB5MGLFKF\"></script>\n<script>\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'G-BSB5MGLFKF');\n</script>\n\n\n\n    <title>New thing! Browse the BBC In Our Time archive by Dewey decimal code (Interconnected)</title>\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n    <link rel=\"stylesheet\" href=\"/home/static/styles/tachyons-4.12.min.css\">\n    <link rel=\"stylesheet\" href=\"/home/static/styles/interconnected-2020.css?v=8\">\n    \n<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\"> \n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin> \n<link href=\"https://fonts.googleapis.com/css2?family=Archivo:wght@700&display=swap\" rel=\"stylesheet\">\n\n<link\n  rel=\"alternate\"\n  type=\"application/rss+xml\"\n  title=\"Interconnected RSS feed\"\n  href=\"https://interconnected.org/home/feed\">\n\n<meta name=\"twitter:card\" content=\"summary\">\n<meta name=\"twitter:site\" content=\"@intrcnnctd\">\n<meta name=\"twitter:creator\" content=\"@genmon\">\n\n<meta property=\"og:url\" content=\"https://interconnected.org/home/2023/02/07/braggoscope\">\n\n<meta property=\"og:title\" content=\"New thing! Browse the BBC In Our Time archive by Dewey decimal code\">\n\n<meta property=\"og:description\" content=\"Posted on Tuesday 7 Feb 2023. 1,208 words, 14 links. By Matt Webb.\">\n<meta property=\"og:image\" content=\"https://interconnected.org/home/2023/02/07/braggoscope.png?v=1\">\n<meta property=\"og:image:alt\" content=\"An abstract, procedurally generated image which looks busier when there are more words.\">\n<meta property=\"og:type\" content=\"article\">\n<meta property=\"og:site_name\" content=\"Interconnected, a blog by Matt Webb\">\n<meta property=\"og:updated_time\" content=\"2023-02-07T18:17:00\">\n\n<script src=\"/home/static/js/social-select.js?v=1\" defer></script>\n<script>\n  window.addEventListener(\"DOMContentLoaded\", () => {\n  new social_select.Controller({\n    pusher: {\n      channelName: \"presence-interconnected.org-social-select\",\n      key: \"142948a0d11c4b8eba37\",\n      cluster: \"eu\",\n      authEndpoint: \"/home/pusher/auth\",\n    },\n    targets: {\n      permalinkPath: document.getElementById(\"permalink\"),\n      root: document.getElementById(\"social-select-root\"),\n      consent: document.getElementById(\"social-select-consent\"),\n      hud: document.getElementById(\"social-select-hud\"),\n    },\n  }).start();\n});\n</script>\n\n  </head>\n  <body class=\"container blog\">\n  \n  <main class=\"content\">\n  \n\n\n<div class=\"mw9-l center ph3-ns pt4\">\n<div class=\"cf ph2-ns\">\n  <div class=\"fl w-100 w5-l ph2\">\n    &nbsp;\n  </div>\n  <div class=\"fl w-100 w-70-l ph3-l ph2\">\n    <div>\n      <h1 class=\"f-subheadline-l f1-m f2 mt0 mb0 pb1 pb0-ns\"><a href=\"/home/\" class=\"link white hover-orange\">Interconnected</a></h1>\n    </div>\n  </div>\n</div>\n</div>\n\n<div class=\"mw9-l center ph3-ns\">\n<div class=\"cf ph2-ns\">\n  <div class=\"fl w-100 w5-l ph2 sticky-l\">\n    <aside>\n      <h5 class=\"f6-l f7 ttu tracked black-50 mt1 mb0 dib\">\n        A blog by Matt Webb\n      </h5>\n      <ul class=\"list pl0 mt1 mb0 dib mb4-l\">\n        <li class=\"f5-l f6 lh-title dib\"><a href=\"/\" class=\"link underline black-50 hover-bg-white\">About</a></li>\n        <li class=\"f5-l f6 lh-title dib\"><a href=\"#archive\" class=\"link underline black-50 hover-bg-white\">Archive</a></li>\n        <!-- <li class=\"f5-l f6 lh-title dib\"><a href=\"https://mwie.com\" class=\"link underline black-50 hover-bg-white\">Work</a></li> -->\n      </ul>\n    </aside>\n    <aside>\n      <h5 class=\"f6-l f7 ttu tracked black-50 normal mt0 mb0 dib\">\n        Subscribe for $0\n      </h5>\n      <ul class=\"list pl0 mt0 mt1-l mb0 dib mb4-l\">\n        <li class=\"f5-l f6 lh-title dib\"><a href=\"https://eepurl.com/befEuL\" class=\"link underline black-50 hover-bg-white\">Email</a></li>\n        <li class=\"f5-l f6 lh-title dib\"><a href=\"https://interconnected.org/home/feed\" class=\"link underline black-50 hover-bg-white\">Feed</a></li>\n        <li class=\"f5-l f6 lh-title dib\"><a href=\"https://aboutfeeds.com\" class=\"link underline black-50 hover-bg-white\">(What is a feed?)</a></li>\n      </ul>\n    </aside>\n    <aside>\n      <h5 class=\"f6-l f7 ttu tracked black-50 normal mt0 mb0 dib\">\n        Unoffice Hours\n      </h5>\n      <ul class=\"list pl0 mt0 mt1-l mb0 dib mb4-l\">\n        <li class=\"f5-l f6 lh-title dib\"><a href=\"https://calendly.com/mwie/30min\" class=\"link underline black-50 hover-bg-white\">Book a call</a></li>\n        <li class=\"f5-l f6 lh-title dib\"><a href=\"/home/2020/09/24/unoffice_hours\" class=\"link underline black-50 hover-bg-white\">(What is this?)</a></li>\n      </ul>\n    </aside>\n    <aside>\n      <h5 class=\"f6-l f7 ttu tracked black-50 normal mt0 mb0 dib\">\n        Follow me on Twitter\n      </h5>\n      <ul class=\"list pl0 mt0 mt1-l mb4 dib\">\n        <li class=\"f5-l f6 lh-title dib\"><a href=\"https://twitter.com/genmon\" class=\"link underline black-50 hover-bg-white\">@genmon</a></li>\n        <li class=\"f5-l f6 lh-title dib\"><a href=\"https://twitter.com/intrcnnctd\" class=\"link underline black-50 hover-bg-white\">@intrcnnctd</a></li>\n      </ul>\n    </aside>\n  </div>\n  <div class=\"fl w-100 w-70-l ph3-l ph2\">\n    \n\n<article id=\"post\">\n  \n    <h2 class=\"f-subheadline-l f1-m f2 lh-solid mt0 pb1 pb4-ns black-40 mw6-m mb3 mb5-ns\">New thing! Browse the BBC In Our Time archive by Dewey decimal code</h2>\n  \n\n  <h5 class=\"f5-l f6 black-50 normal mb2 lh-title mt0 mt3-ns\">\n    18.17, Tuesday 7 Feb 2023\n    <span class=\"nowrap\">\n      <a\n        href=\"/home/2023/02/07/braggoscope\"\n        class=\"link underline black-50 hover-bg-white\"\n        id=\"permalink\">Link to this post</a>\n    </span>\n  </h5>\n\n  \n  <div class=\"f5 f4-l measure-wide\" id=\"social-select-root\">\n  <p class=\"measure-wide f6 f5-l lh-copy black-80\">I love listening to <em>In Our Time</em> with Melvyn Bragg and guests (<a href=\"https://www.bbc.co.uk/programmes/b006qykl\">official site here</a>). It‚Äôs the best radio.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">There are almost 1,000 episodes (it has been broadcasting on Radio 4 since 1998) and when I want to learn about, like, Ancient Greek tragedies, or the evolution of teeth, this show is where I turn first. All the audio is online, which is amazing! Thank you BBC!</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">But actually trawling through the back catalogue is hard.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">So I made a <em>very unofficial</em> website to find old episodes to listen to.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\"><strong><a href=\"https://genmon.github.io/braggoscope/\">Braggoscope</a></strong> lets you explore the <em>In Our Time</em> archive. Check it out!</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">There are multiple ways to explore:</p>\n<ul class=\"list ph0 ph0-ns bulleted-list\">\n<li class=\"measure-wide f6 f5-l lh-copy black-80\"><a href=\"https://genmon.github.io/braggoscope/directory\">Directory</a> - browse episodes by topic, organised using standard Dewey decimal library codes</li>\n<li class=\"measure-wide f6 f5-l lh-copy black-80\">From an episode page, e.g. <a href=\"https://genmon.github.io/braggoscope/2013/01/10/le-morte-darthur.html\">Le Morte d‚ÄôArthur</a> (Jan 10, 2013) you can pivot on <strong>guest</strong> (each discussion has three academics) and <strong>similar episodes</strong> which are surprisingly good.</li>\n</ul>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">Each episode links through to the BBC website so you can listen. The full show description and reading list are included too.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">I guess I would call this <em>pre-pre-alpha‚Ä¶</em> there‚Äôs no real design yet, and there are surely some bugs with the data.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">HOWEVER: I‚Äôm using it to discover new episodes already.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">For example. I loved learning about the late Devonian extinction recently. <a href=\"https://genmon.github.io/braggoscope/2021/03/11/the-late-devonian-extinction.html\">Here‚Äôs the episode page.</a> Now I can go down the reading list to find books, and find my way to similar episodes about the Permian-Triassic Boundary, the Cambrian Period, the fish-tetrapod transition and so on. Like I said, surprisingly good.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">And browsing the Directory is super fun.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">(Oh and hi to anyone from the Beeb who is reading this! I‚Äôll take this project private if you need me to, or share the approach.)</p>\n<hr class=\"h1 xh2-ns w1 xw2-ns ml4 mv4 bb bw1 b--white\">\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">HEY: you can stop reading here unless you want all the stuff about how it works and my opinions about AI.</p>\n<hr class=\"h1 xh2-ns w1 xw2-ns ml4 mv4 bb bw1 b--white\">\n<p class=\"measure-wide f6 f5-l lh-copy black-80\"><em>For posterity‚Ä¶ in the event that Braggoscope changes URL or disappears, I want to remember what it looked like in years to come. So here are a couple of screenshots: <a href=\"/more/2023/02/braggoscope-directory.png\">the Directory</a>; <a href=\"/more/2023/02/braggoscope-episode.png\">an episode page</a>.</em></p>\n<hr class=\"h1 xh2-ns w1 xw2-ns ml4 mv4 bb bw1 b--white\">\n<h3 class=\"measure f4 f3-l lh-copy black-80 pt3 pt4-l pb0 mb0\">I used GPT-3 for the heavy lifting and now I have oh so many opinions</h3>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">I wrote up the tools I use in Braggoscope on the <a href=\"https://genmon.github.io/braggoscope/about\">About</a> page, but as a quick overview.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">The process:</p>\n<ol class=\"list ph0 ph0-ns numbered-list\">\n<li class=\"measure-wide f6 f5-l lh-copy black-80\">Spider the official website and <strong>fetch</strong> all the HTML (a 1,000 pages or so, not too many). The show notes are what we‚Äôre interested in, but they‚Äôre not super well structured.</li>\n<li class=\"measure-wide f6 f5-l lh-copy black-80\">We <strong>extract</strong> data like the episode synopsis, guests (names and affiliations), reading list (title, author, etc) in a machine-readable format. Imagine starting with prose, and ending up with tidy columns in an Excel spreadsheet: that‚Äôs what I mean by structured data.</li>\n<li class=\"measure-wide f6 f5-l lh-copy black-80\">The data can then be <strong>enriched</strong> by classifying it and processing it to figure out similarities.</li>\n<li class=\"measure-wide f6 f5-l lh-copy black-80\">Finally there is <strong>site build</strong> where the data is written out into HTML.</li>\n</ol>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">I‚Äôm a casual coder but the above is pretty straightforward.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">Except the <strong>extract</strong> step. This is tedious. It‚Äôs a few days of writing fiddly code to catch all the different ways that guests might be listed, or how show notes might be written.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">OR:</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">It occurred to me‚Ä¶ why not just give this to OpenAI‚Äôs GPT-3?</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">So that‚Äôs what I did. It took 20 minutes to write the integration, then I left the code running overnight. It costs me pennies per inference so I‚Äôve replaced a few days of boring graft with $30 on my credit card.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">And this is <em>interesting</em> right?</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">I‚Äôve been used to thinking about generative AI and LLMs (language models) as smart autocomplete.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">But this is more like a universal coupling.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">I set temperature=0 ‚Äì this is a parameter that governs creativity, so by doing this I was asking GPT-3 to be pretty deterministic.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">In the prompt, I specified that GPT-3 should return structured data as JSON (a data interchange format based on Javascript objects) and provided a type definition.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">It doesn‚Äôt <em>always</em> return valid JSON. I have some wrapper code that fixes it up.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">It was while I was getting structured data back for the synopsis that I thought: I wonder if I could get GPT-3 to classify this? How about using Dewey decimal classification‚Ä¶?</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">And sure enough, it works! It‚Äôs not perfect but it‚Äôs preeeeetty good.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\"><em>(Now I read down the <a href=\"https://en.wikipedia.org/wiki/List_of_Dewey_Decimal_classes\">list of Dewey Decimals classes</a> with some considerable side-eye. It has, uh, a particular perspective. And it turns out that Melvil Dewey was <a href=\"https://en.wikipedia.org/wiki/Melvil_Dewey\">a seriously bigoted and unpleasant guy</a>. But it‚Äôs a well-known hierarchy that is small enough to wrap your arms around, and it makes topics findable. So‚Ä¶ I would love an alternative but that‚Äôs for another day.)</em></p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">The <em>‚ÄúSimilar episodes‚Äù</em> list also uses OpenAI ‚Äì each show synopsis is translated into an ‚Äúembedding,‚Äù a ten thousand parameter vector representing its position in the ‚Äúmeaning space‚Äù of the language model. Then similar episodes are simply <em>nearest neighbours</em> (calculated with cosine similarity).</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">Again - this is surprisingly good! While I was developing Braggoscope I tried using tags too but honestly, for finding related shows, this embedding approach is way better.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">This is pretty technical but you can explore the whole space yourself: here are <a href=\"/more/2023/02/in_our_time-PCA-plot.html\">all the episode embeddings in a single chart</a> (hover over each dot for the title). This uses PCA (principal component analysis) on the embeddings, then the top two components (being the most significant vectors of variability) are the x and y axes. It‚Äôs code that OpenAI provides but will be pretty easy to customise - PCA is a ton easier than when I used it back in undergrad! - so I‚Äôm thinking about what to use this for.</p>\n<hr class=\"h1 xh2-ns w1 xw2-ns ml4 mv4 bb bw1 b--white\">\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">I feel like this programmatic use of LLMs is where AI gets really interesting.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">There‚Äôs the experience of it‚Ä¶</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">Using GitHub Copilot to write code (<a href=\"/home/2023/01/27/copilot\">as previously discussed</a>) and calling out to GPT-3 programmatically to dodge days of graft actually brought tears to my eyes. I‚Äôve coded, mostly as a hobby, my whole life ‚Äì it‚Äôs a big creative outlet alongside writing ‚Äì it‚Äôs so rarely felt like this. It feels like flying.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">But the actual literal engineering of it too‚Ä¶</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">Sure <a href=\"https://blog.google/technology/ai/bard-google-ai-search-updates/\">Google is all-in on AI in products</a>, announcing chatbots to compete with ChatGPT, and synthesised text in the search engine. BUT.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">Using GPT-3 as a function call.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">Using GPT-3 as a universal coupling.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">It brings a lot within reach.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">I think the magnitude of this shift‚Ä¶ I would say it‚Äôs on the order of the web from the mid 90s? There was a radical simplification and democratisation of software (architecture, development, deployment, use) that took decades to really unfold.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">There is so much tooling to build around temperature=0 language model calls. There‚Äôs a startup or nine just in that.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">I would like to see frameworks and programming languages that have first class support for this as a pattern.</p>\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">Anyway!</p>\n<hr class=\"h1 xh2-ns w1 xw2-ns ml4 mv4 bb bw1 b--white\">\n<p class=\"measure-wide f6 f5-l lh-copy black-80\">Hey, some trivia: <a href=\"/home/2017/12/21/filtered\">I was involved</a> in setting up the <em>In Our Time</em> podcast, way back in 2004. It was the first podcast by the BBC, and the BBC was the first national broadcaster to do any podcasting at all. I hand wrote the first XML files that were uploaded to the servers! Still a fan.</p>\n  </div>\n\n  </article>\n\n  <hr class=\"invisible\">\n\n  \n    <h5 class=\"f6-l f7 ttu tracked black-50 normal mb2 lh-title mt5\">\n      More posts tagged:\n    </h5>\n      <ul class=\"list ph0 ph0-ns bulleted-list\">\n        \n        <li class=\"measure f6 f5-l lh-copy black-50\">\n          <a class=\"link underline black-50 hover-orange\" href=\"/home/tagged/gpt-3\">gpt-3</a>\n          (15 posts)\n        </li>\n        \n        <li class=\"measure f6 f5-l lh-copy black-50\">\n          <a class=\"link underline black-50 hover-orange\" href=\"/home/tagged/in-our-time-episode-mentions\">in-our-time-episode-mentions</a>\n          (8 posts)\n        </li>\n        \n      </ul>\n  \n\n  \n    <h5 class=\"f6-l f7 ttu tracked black-50 normal mb2 lh-title mt5\">\n      Follow-up posts:\n    </h5>\n\n    <ul class=\"list ph0 ph0-ns bulleted-list\">\n      \n      <li class=\"measure f6 f5-l lh-copy black-40\">\n        <a class=\"link underline black-50 hover-orange\" href=\"/home/2023/02/10/progress\">A notification center for progress bars that sounds like birdsong</a>\n        (10 Feb 2023)\n      </li>\n      \n      <li class=\"measure f6 f5-l lh-copy black-40\">\n        <a class=\"link underline black-50 hover-orange\" href=\"/home/2023/03/16/singularity\">The surprising ease and effectiveness of AI in a loop</a>\n        (16 Mar 2023)\n      </li>\n      \n    </ul>\n  \n\n  \n  \n  \n  \n  <p class=\"measure f6 f5-l lh-copy black-50 mt6 mb3 i\">If you enjoyed this post, please consider sharing it by email or on social media. <a class=\"link underline black-50 hover-bg-white\" href=\"/home/2023/02/07/braggoscope\">Here‚Äôs the link.</a> Thanks, <span class=\"nowrap\">‚ÄîMatt.</span></p>\n  \n\n\n  </div>\n</div>\n</div>\n\n\n\n<div id=\"social-select-hud\">üò¥</div>\n\n\n  </main>\n  <footer class=\"bg-black-90 white-40 pv5 mt3\" id=\"archive\">\n  <div class=\"mw9-l center ph3-ns\">\n    <div class=\"cf ph2-ns\">\n      <div class=\"fl w-100 w5-l ph3-l ph2\">\n        &nbsp;\n      </div>\n      <div class=\"fl w-100 w-70-l ph3-l ph2\">\n        <div>\n          \n            \n            <h2 class=\"lh-title f1-m f2 mt0 mb0\">Most recent posts</h2>\n            <ul class=\"list ph0 lh-body\">\n              \n              <li class=\"b f5\">\n                \n                  <a href=\"/home/2023/04/20/oat\" class=\"link underline white-70 hover-orange\">On the shift to oat and the milk hysteresis curve</a>\n                  <span class=\"nowrap\">20 Apr 2023</span>\n                \n              </li>\n              \n              <li class=\"b f5\">\n                \n                  <a href=\"/home/2023/04/13/phone\" class=\"link underline white-70 hover-orange\">Time to rethink the phone call</a>\n                  <span class=\"nowrap\">13 Apr 2023</span>\n                \n              </li>\n              \n              <li class=\"b f5\">\n                \n                  <a href=\"/home/2023/04/06/microwaves\" class=\"link underline white-70 hover-orange\">Music for microwaves</a>\n                  <span class=\"nowrap\">6 Apr 2023</span>\n                \n              </li>\n              \n              <li class=\"b f5\">\n                \n                  <a href=\"/home/2023/03/31/filtered\" class=\"link underline white-70 hover-orange\">Filtered for clocks</a>\n                  <span class=\"nowrap\">31 Mar 2023</span>\n                \n              </li>\n              \n              <li class=\"b f5\">\n                \n                  <a href=\"/home/2023/03/22/tuning\" class=\"link underline white-70 hover-orange\">My new job is AI sommelier and I detect the bouquet of progress</a>\n                  <span class=\"nowrap\">22 Mar 2023</span>\n                \n              </li>\n              \n              <li class=\"b f5\">\n                \n                  <a href=\"/home/2023/03/16/singularity\" class=\"link underline white-70 hover-orange\">The surprising ease and effectiveness of AI in a loop</a>\n                  <span class=\"nowrap\">16 Mar 2023</span>\n                \n              </li>\n              \n              <li class=\"b f5\">\n                \n                  <a href=\"/home/2023/03/09/turing\" class=\"link underline white-70 hover-orange\">Marking the moment of my first Reverse Turing Test</a>\n                  <span class=\"nowrap\">9 Mar 2023</span>\n                \n              </li>\n              \n              <li class=\"b f5\">\n                \n                  <a href=\"/home/2023/03/08/monkeys\" class=\"link underline white-70 hover-orange\">An infinite number of monkeys eventually wrote this blog post</a>\n                  <span class=\"nowrap\">8 Mar 2023</span>\n                \n              </li>\n              \n              <li class=\"b f5\">\n                \n                  <a href=\"/home/2023/03/03/filtered\" class=\"link underline white-70 hover-orange\">Filtered for causes and kissing</a>\n                  <span class=\"nowrap\">3 Mar 2023</span>\n                \n              </li>\n              \n              <li class=\"b f5\">\n                \n                  <a href=\"/home/2023/02/23/peeping\" class=\"link underline white-70 hover-orange\">Tinkering with hyperlinks</a>\n                  <span class=\"nowrap\">23 Feb 2023</span>\n                \n              </li>\n              \n              <li class=\"b f5\">\n                \n                  <a href=\"/home/2023/02/15/maintenance\" class=\"link underline white-70 hover-orange\">Don‚Äôt bother me now I‚Äôm waxing my phone</a>\n                  <span class=\"nowrap\">15 Feb 2023</span>\n                \n              </li>\n              \n              <li class=\"b f5\">\n                \n                  <a href=\"/home/2023/02/10/progress\" class=\"link underline white-70 hover-orange\">A notification center for progress bars that sounds like birdsong</a>\n                  <span class=\"nowrap\">10 Feb 2023</span>\n                \n              </li>\n              \n            </ul>\n\n            \n            <p class=\"b f5\">\n              Continue reading:\n              <a href=\"/home/2023\" class=\"link underline white-70 hover-orange\">All in 2023</a>\n            </p>\n\n            \n            \n            <p class=\"lh-solid b f5\">\n              <span class=\"f6 ttu tracked orange\">streak</span>\n              <span class=\"nowrap\">\n              New posts for <span class=\"streak-badge bg-orange black nb2 nb0-ns\">160</span> consecutive weeks\n              </span>\n              <span class=\"nowrap\">\n              (see: <a href=\"/home/2020/09/10/streak\" class=\"link underline white-70 hover-orange\">blogging tips</a>)\n              </span>\n            </p>\n            \n\n            <p class=\"b f5 lh-title\">\n              New? Start here:\n              <a href=\"/home/2022/12/21/top_posts\" class=\"link underline white-70 hover-orange\">Best of 2022</a>\n              (<a href=\"/home/2021/12/23/top_posts\" class=\"link underline white-70 hover-orange\">also 2021</a>, <a href=\"/home/2020/12/17/top_posts\" class=\"link underline white-70 hover-orange\">also 2020</a>)\n              <br>\n              Or explore the archives:\n              <a href=\"/home/on-this-day\" class=\"link underline white-70 hover-orange\">On this day</a>\n            </p>\n\n            \n          \n\n          <hr class=\"invisible\">\n\n          \n            \n              <h2 class=\"lh-title f1-m f2 mt4 mb0\">Archive</h2>\n  \n              <ul class=\"list pl0\">\n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2023\" class=\"link underline white-70 hover-orange\">2023</a>\n                    23 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2022\" class=\"link underline white-70 hover-orange\">2022</a>\n                    96 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2021\" class=\"link underline white-70 hover-orange\">2021</a>\n                    128 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2020\" class=\"link underline white-70 hover-orange\">2020</a>\n                    116 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2019\" class=\"link underline white-70 hover-orange\">2019</a>\n                    8 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2018\" class=\"link underline white-70 hover-orange\">2018</a>\n                    16 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2017\" class=\"link underline white-70 hover-orange\">2017</a>\n                    22 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2016\" class=\"link underline white-70 hover-orange\">2016</a>\n                    48 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2015\" class=\"link underline white-70 hover-orange\">2015</a>\n                    88 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2014\" class=\"link underline white-70 hover-orange\">2014</a>\n                    30 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2013\" class=\"link underline white-70 hover-orange\">2013</a>\n                    6 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2012\" class=\"link underline white-70 hover-orange\">2012</a>\n                    27 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2011\" class=\"link underline white-70 hover-orange\">2011</a>\n                    76 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2010\" class=\"link underline white-70 hover-orange\">2010</a>\n                    2 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2009\" class=\"link underline white-70 hover-orange\">2009</a>\n                    2 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2008\" class=\"link underline white-70 hover-orange\">2008</a>\n                    59 posts\n                  </li>\n                  \n                \n                  \n                  <li class=\"b f3\">\n                    <a href=\"/home/2007\" class=\"link underline white-70 hover-orange\">2007</a>\n                    20 posts\n                  </li>\n                  \n                \n                  \n                \n                  \n                \n                  \n                \n                  \n                \n                  \n                \n                  \n                \n                  \n                \n              </ul>\n            \n          \n        </div>\n        \n        <hr class=\"invisible\">\n\n\t    \t\n        <form action=\"https://www.google.com/search\" method=\"get\" class=\"mt4 mb0\">\n\t\t\t    <input class=\"input-reset f4 pa2\" name=\"q\" type=\"text\" placeholder=\"e.g. music\" aria-label=\"Search\" size=\"12\">\n\t\t    \t<input type=\"hidden\" name=\"as_sitesearch\" value=\"interconnected.org/home\">\n\t\t\t    <button class=\"button-reset f4 pa2\" type=\"submit\">Search</button>\n        </form>\n\n        <hr class=\"invisible\">\n\n        <div class=\"flex items-center justify-left mt3 pr3\">\n          <div class=\"dib mr2\">\n            Enable secret experimental features\n          </div>\n          <label class=\"dib switch\">\n            <input type=\"checkbox\" id=\"social-select-consent\">\n            <span class=\"slider round\"></span>\n          </label>\n        </div>\n\n        <p class=\"mt5\">\n          Since February 2000.  Copyright ¬© 2023 Matt Webb. p.s. <a class=\"white-70 hover-orange link underline\" href=\"/home/blogroll\">my blogroll</a>.\n        </p>\n\n      </div>\n    </div>\n  </div>  \n\n</footer>\n  </body>\n</html>","oembed":false,"readabilityObject":{"title":"New thing! Browse the BBC In Our Time archive by Dewey decimal code","content":"<div id=\"readability-page-1\" class=\"page\"><div id=\"social-select-root\">\n  <p>I love listening to <em>In Our Time</em> with Melvyn Bragg and guests (<a href=\"https://www.bbc.co.uk/programmes/b006qykl\">official site here</a>). It‚Äôs the best radio.</p>\n<p>There are almost 1,000 episodes (it has been broadcasting on Radio 4 since 1998) and when I want to learn about, like, Ancient Greek tragedies, or the evolution of teeth, this show is where I turn first. All the audio is online, which is amazing! Thank you BBC!</p>\n<p>But actually trawling through the back catalogue is hard.</p>\n<p>So I made a <em>very unofficial</em> website to find old episodes to listen to.</p>\n<p><strong><a href=\"https://genmon.github.io/braggoscope/\">Braggoscope</a></strong> lets you explore the <em>In Our Time</em> archive. Check it out!</p>\n<p>There are multiple ways to explore:</p>\n<ul>\n<li><a href=\"https://genmon.github.io/braggoscope/directory\">Directory</a> - browse episodes by topic, organised using standard Dewey decimal library codes</li>\n<li>From an episode page, e.g. <a href=\"https://genmon.github.io/braggoscope/2013/01/10/le-morte-darthur.html\">Le Morte d‚ÄôArthur</a> (Jan 10, 2013) you can pivot on <strong>guest</strong> (each discussion has three academics) and <strong>similar episodes</strong> which are surprisingly good.</li>\n</ul>\n<p>Each episode links through to the BBC website so you can listen. The full show description and reading list are included too.</p>\n<p>I guess I would call this <em>pre-pre-alpha‚Ä¶</em> there‚Äôs no real design yet, and there are surely some bugs with the data.</p>\n<p>HOWEVER: I‚Äôm using it to discover new episodes already.</p>\n<p>For example. I loved learning about the late Devonian extinction recently. <a href=\"https://genmon.github.io/braggoscope/2021/03/11/the-late-devonian-extinction.html\">Here‚Äôs the episode page.</a> Now I can go down the reading list to find books, and find my way to similar episodes about the Permian-Triassic Boundary, the Cambrian Period, the fish-tetrapod transition and so on. Like I said, surprisingly good.</p>\n<p>And browsing the Directory is super fun.</p>\n<p>(Oh and hi to anyone from the Beeb who is reading this! I‚Äôll take this project private if you need me to, or share the approach.)</p>\n<hr>\n<p>HEY: you can stop reading here unless you want all the stuff about how it works and my opinions about AI.</p>\n<hr>\n<p><em>For posterity‚Ä¶ in the event that Braggoscope changes URL or disappears, I want to remember what it looked like in years to come. So here are a couple of screenshots: <a href=\"/more/2023/02/braggoscope-directory.png\">the Directory</a>; <a href=\"/more/2023/02/braggoscope-episode.png\">an episode page</a>.</em></p>\n<hr>\n<h3>I used GPT-3 for the heavy lifting and now I have oh so many opinions</h3>\n<p>I wrote up the tools I use in Braggoscope on the <a href=\"https://genmon.github.io/braggoscope/about\">About</a> page, but as a quick overview.</p>\n<p>The process:</p>\n<ol>\n<li>Spider the official website and <strong>fetch</strong> all the HTML (a 1,000 pages or so, not too many). The show notes are what we‚Äôre interested in, but they‚Äôre not super well structured.</li>\n<li>We <strong>extract</strong> data like the episode synopsis, guests (names and affiliations), reading list (title, author, etc) in a machine-readable format. Imagine starting with prose, and ending up with tidy columns in an Excel spreadsheet: that‚Äôs what I mean by structured data.</li>\n<li>The data can then be <strong>enriched</strong> by classifying it and processing it to figure out similarities.</li>\n<li>Finally there is <strong>site build</strong> where the data is written out into HTML.</li>\n</ol>\n<p>I‚Äôm a casual coder but the above is pretty straightforward.</p>\n<p>Except the <strong>extract</strong> step. This is tedious. It‚Äôs a few days of writing fiddly code to catch all the different ways that guests might be listed, or how show notes might be written.</p>\n<p>OR:</p>\n<p>It occurred to me‚Ä¶ why not just give this to OpenAI‚Äôs GPT-3?</p>\n<p>So that‚Äôs what I did. It took 20 minutes to write the integration, then I left the code running overnight. It costs me pennies per inference so I‚Äôve replaced a few days of boring graft with $30 on my credit card.</p>\n<p>And this is <em>interesting</em> right?</p>\n<p>I‚Äôve been used to thinking about generative AI and LLMs (language models) as smart autocomplete.</p>\n<p>But this is more like a universal coupling.</p>\n<p>I set temperature=0 ‚Äì this is a parameter that governs creativity, so by doing this I was asking GPT-3 to be pretty deterministic.</p>\n<p>In the prompt, I specified that GPT-3 should return structured data as JSON (a data interchange format based on Javascript objects) and provided a type definition.</p>\n<p>It doesn‚Äôt <em>always</em> return valid JSON. I have some wrapper code that fixes it up.</p>\n<p>It was while I was getting structured data back for the synopsis that I thought: I wonder if I could get GPT-3 to classify this? How about using Dewey decimal classification‚Ä¶?</p>\n<p>And sure enough, it works! It‚Äôs not perfect but it‚Äôs preeeeetty good.</p>\n<p><em>(Now I read down the <a href=\"https://en.wikipedia.org/wiki/List_of_Dewey_Decimal_classes\">list of Dewey Decimals classes</a> with some considerable side-eye. It has, uh, a particular perspective. And it turns out that Melvil Dewey was <a href=\"https://en.wikipedia.org/wiki/Melvil_Dewey\">a seriously bigoted and unpleasant guy</a>. But it‚Äôs a well-known hierarchy that is small enough to wrap your arms around, and it makes topics findable. So‚Ä¶ I would love an alternative but that‚Äôs for another day.)</em></p>\n<p>The <em>‚ÄúSimilar episodes‚Äù</em> list also uses OpenAI ‚Äì each show synopsis is translated into an ‚Äúembedding,‚Äù a ten thousand parameter vector representing its position in the ‚Äúmeaning space‚Äù of the language model. Then similar episodes are simply <em>nearest neighbours</em> (calculated with cosine similarity).</p>\n<p>Again - this is surprisingly good! While I was developing Braggoscope I tried using tags too but honestly, for finding related shows, this embedding approach is way better.</p>\n<p>This is pretty technical but you can explore the whole space yourself: here are <a href=\"/more/2023/02/in_our_time-PCA-plot.html\">all the episode embeddings in a single chart</a> (hover over each dot for the title). This uses PCA (principal component analysis) on the embeddings, then the top two components (being the most significant vectors of variability) are the x and y axes. It‚Äôs code that OpenAI provides but will be pretty easy to customise - PCA is a ton easier than when I used it back in undergrad! - so I‚Äôm thinking about what to use this for.</p>\n<hr>\n<p>I feel like this programmatic use of LLMs is where AI gets really interesting.</p>\n<p>There‚Äôs the experience of it‚Ä¶</p>\n<p>Using GitHub Copilot to write code (<a href=\"/home/2023/01/27/copilot\">as previously discussed</a>) and calling out to GPT-3 programmatically to dodge days of graft actually brought tears to my eyes. I‚Äôve coded, mostly as a hobby, my whole life ‚Äì it‚Äôs a big creative outlet alongside writing ‚Äì it‚Äôs so rarely felt like this. It feels like flying.</p>\n<p>But the actual literal engineering of it too‚Ä¶</p>\n<p>Sure <a href=\"https://blog.google/technology/ai/bard-google-ai-search-updates/\">Google is all-in on AI in products</a>, announcing chatbots to compete with ChatGPT, and synthesised text in the search engine. BUT.</p>\n<p>Using GPT-3 as a function call.</p>\n<p>Using GPT-3 as a universal coupling.</p>\n<p>It brings a lot within reach.</p>\n<p>I think the magnitude of this shift‚Ä¶ I would say it‚Äôs on the order of the web from the mid 90s? There was a radical simplification and democratisation of software (architecture, development, deployment, use) that took decades to really unfold.</p>\n<p>There is so much tooling to build around temperature=0 language model calls. There‚Äôs a startup or nine just in that.</p>\n<p>I would like to see frameworks and programming languages that have first class support for this as a pattern.</p>\n<p>Anyway!</p>\n<hr>\n<p>Hey, some trivia: <a href=\"/home/2017/12/21/filtered\">I was involved</a> in setting up the <em>In Our Time</em> podcast, way back in 2004. It was the first podcast by the BBC, and the BBC was the first national broadcaster to do any podcasting at all. I hand wrote the first XML files that were uploaded to the servers! Still a fan.</p>\n  </div></div>","textContent":"\n  I love listening to In Our Time with Melvyn Bragg and guests (official site here). It‚Äôs the best radio.\nThere are almost 1,000 episodes (it has been broadcasting on Radio 4 since 1998) and when I want to learn about, like, Ancient Greek tragedies, or the evolution of teeth, this show is where I turn first. All the audio is online, which is amazing! Thank you BBC!\nBut actually trawling through the back catalogue is hard.\nSo I made a very unofficial website to find old episodes to listen to.\nBraggoscope lets you explore the In Our Time archive. Check it out!\nThere are multiple ways to explore:\n\nDirectory - browse episodes by topic, organised using standard Dewey decimal library codes\nFrom an episode page, e.g. Le Morte d‚ÄôArthur (Jan 10, 2013) you can pivot on guest (each discussion has three academics) and similar episodes which are surprisingly good.\n\nEach episode links through to the BBC website so you can listen. The full show description and reading list are included too.\nI guess I would call this pre-pre-alpha‚Ä¶ there‚Äôs no real design yet, and there are surely some bugs with the data.\nHOWEVER: I‚Äôm using it to discover new episodes already.\nFor example. I loved learning about the late Devonian extinction recently. Here‚Äôs the episode page. Now I can go down the reading list to find books, and find my way to similar episodes about the Permian-Triassic Boundary, the Cambrian Period, the fish-tetrapod transition and so on. Like I said, surprisingly good.\nAnd browsing the Directory is super fun.\n(Oh and hi to anyone from the Beeb who is reading this! I‚Äôll take this project private if you need me to, or share the approach.)\n\nHEY: you can stop reading here unless you want all the stuff about how it works and my opinions about AI.\n\nFor posterity‚Ä¶ in the event that Braggoscope changes URL or disappears, I want to remember what it looked like in years to come. So here are a couple of screenshots: the Directory; an episode page.\n\nI used GPT-3 for the heavy lifting and now I have oh so many opinions\nI wrote up the tools I use in Braggoscope on the About page, but as a quick overview.\nThe process:\n\nSpider the official website and fetch all the HTML (a 1,000 pages or so, not too many). The show notes are what we‚Äôre interested in, but they‚Äôre not super well structured.\nWe extract data like the episode synopsis, guests (names and affiliations), reading list (title, author, etc) in a machine-readable format. Imagine starting with prose, and ending up with tidy columns in an Excel spreadsheet: that‚Äôs what I mean by structured data.\nThe data can then be enriched by classifying it and processing it to figure out similarities.\nFinally there is site build where the data is written out into HTML.\n\nI‚Äôm a casual coder but the above is pretty straightforward.\nExcept the extract step. This is tedious. It‚Äôs a few days of writing fiddly code to catch all the different ways that guests might be listed, or how show notes might be written.\nOR:\nIt occurred to me‚Ä¶ why not just give this to OpenAI‚Äôs GPT-3?\nSo that‚Äôs what I did. It took 20 minutes to write the integration, then I left the code running overnight. It costs me pennies per inference so I‚Äôve replaced a few days of boring graft with $30 on my credit card.\nAnd this is interesting right?\nI‚Äôve been used to thinking about generative AI and LLMs (language models) as smart autocomplete.\nBut this is more like a universal coupling.\nI set temperature=0 ‚Äì this is a parameter that governs creativity, so by doing this I was asking GPT-3 to be pretty deterministic.\nIn the prompt, I specified that GPT-3 should return structured data as JSON (a data interchange format based on Javascript objects) and provided a type definition.\nIt doesn‚Äôt always return valid JSON. I have some wrapper code that fixes it up.\nIt was while I was getting structured data back for the synopsis that I thought: I wonder if I could get GPT-3 to classify this? How about using Dewey decimal classification‚Ä¶?\nAnd sure enough, it works! It‚Äôs not perfect but it‚Äôs preeeeetty good.\n(Now I read down the list of Dewey Decimals classes with some considerable side-eye. It has, uh, a particular perspective. And it turns out that Melvil Dewey was a seriously bigoted and unpleasant guy. But it‚Äôs a well-known hierarchy that is small enough to wrap your arms around, and it makes topics findable. So‚Ä¶ I would love an alternative but that‚Äôs for another day.)\nThe ‚ÄúSimilar episodes‚Äù list also uses OpenAI ‚Äì each show synopsis is translated into an ‚Äúembedding,‚Äù a ten thousand parameter vector representing its position in the ‚Äúmeaning space‚Äù of the language model. Then similar episodes are simply nearest neighbours (calculated with cosine similarity).\nAgain - this is surprisingly good! While I was developing Braggoscope I tried using tags too but honestly, for finding related shows, this embedding approach is way better.\nThis is pretty technical but you can explore the whole space yourself: here are all the episode embeddings in a single chart (hover over each dot for the title). This uses PCA (principal component analysis) on the embeddings, then the top two components (being the most significant vectors of variability) are the x and y axes. It‚Äôs code that OpenAI provides but will be pretty easy to customise - PCA is a ton easier than when I used it back in undergrad! - so I‚Äôm thinking about what to use this for.\n\nI feel like this programmatic use of LLMs is where AI gets really interesting.\nThere‚Äôs the experience of it‚Ä¶\nUsing GitHub Copilot to write code (as previously discussed) and calling out to GPT-3 programmatically to dodge days of graft actually brought tears to my eyes. I‚Äôve coded, mostly as a hobby, my whole life ‚Äì it‚Äôs a big creative outlet alongside writing ‚Äì it‚Äôs so rarely felt like this. It feels like flying.\nBut the actual literal engineering of it too‚Ä¶\nSure Google is all-in on AI in products, announcing chatbots to compete with ChatGPT, and synthesised text in the search engine. BUT.\nUsing GPT-3 as a function call.\nUsing GPT-3 as a universal coupling.\nIt brings a lot within reach.\nI think the magnitude of this shift‚Ä¶ I would say it‚Äôs on the order of the web from the mid 90s? There was a radical simplification and democratisation of software (architecture, development, deployment, use) that took decades to really unfold.\nThere is so much tooling to build around temperature=0 language model calls. There‚Äôs a startup or nine just in that.\nI would like to see frameworks and programming languages that have first class support for this as a pattern.\nAnyway!\n\nHey, some trivia: I was involved in setting up the In Our Time podcast, way back in 2004. It was the first podcast by the BBC, and the BBC was the first national broadcaster to do any podcasting at all. I hand wrote the first XML files that were uploaded to the servers! Still a fan.\n  ","length":6842,"excerpt":"Posted on Tuesday 7 Feb 2023. 1,208 words, 14 links. By Matt Webb.","byline":null,"dir":null,"siteName":"Interconnected, a blog by Matt Webb","lang":"en"},"finalizedMeta":{"title":"New thing! Browse the BBC In Our Time archive by Dewey decimal code","description":"Posted on Tuesday 7 Feb 2023. 1,208 words, 14 links. By Matt Webb.","author":false,"creator":"@genmon","publisher":false,"date":"2023-04-24T14:13:31.561Z","topics":[]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":false,"title":"New thing! Browse the BBC In Our Time archive by Dewey decimal code (Interconnected)","description":false,"canonical":"https://interconnected.org/home/2023/02/07/braggoscope","keywords":[],"image":false,"firstParagraph":"I love listening to In Our Time with Melvyn Bragg and guests (official site here). It‚Äôs the best radio."},"dublinCore":{},"opengraph":{"title":"New thing! Browse the BBC In Our Time archive by Dewey decimal code","description":"Posted on Tuesday 7 Feb 2023. 1,208 words, 14 links. By Matt Webb.","url":"https://interconnected.org/home/2023/02/07/braggoscope","site_name":"Interconnected, a blog by Matt Webb","locale":false,"type":"article","typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":"https://interconnected.org/home/2023/02/07/braggoscope.png?v=1","image:alt":"An abstract, procedurally generated image which looks busier when there are more words.","updated_time":"2023-02-07T18:17:00"},"twitter":{"site":"@intrcnnctd","description":false,"card":"summary","creator":"@genmon","title":false,"image":false},"archivedData":{"link":false,"wayback":false}}}