{"initialLink":"https://www.anildash.com/2023/06/08/ai-is-unreasonable/","sanitizedLink":"https://www.anildash.com/2023/06/08/ai-is-unreasonable/","finalLink":"https://anildash.com/2023/06/08/ai-is-unreasonable/","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://anildash.com/2023/06/08/ai-is-unreasonable/\" itemprop=\"url\">Today's AI is unreasonable - Anil Dash</a></contexter-box-head></contexter-box-head><contexter-byline class=\"p-author author\" slot=\"author\"><span class=\"p-name byline\" rel=\"author\" itemprop=\"author\">undefined</span></contexter-byline><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2023-07-11T16:37:38.899Z\">7/11/2023</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>A blog about making culture. Since 1999.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"></contexter-keywordset><a is=\"contexter-link\" href=\"https://anildash.com/2023/06/08/ai-is-unreasonable/\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"0fa827649687ea3b2ad848d65dd61087258ba7f4","data":{"originalLink":"https://www.anildash.com/2023/06/08/ai-is-unreasonable/","sanitizedLink":"https://www.anildash.com/2023/06/08/ai-is-unreasonable/","canonical":"https://anildash.com/2023/06/08/ai-is-unreasonable/","htmlText":"<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"/public/styles.css\" />\n    <link rel=\"alternate\" type=\"application/atom+xml\" href=\"/feed.xml\" title=\"Atom 1.0\">\n    <title>Today&apos;s AI is unreasonable - Anil Dash</title>\n\n<meta name=\"description\" content=\"A blog about making culture. Since 1999.\">\n<meta name=\"robots\" content=\"index,follow\">\n<meta name=\"author\" content=\"undefined\">\n\n<link rel=\"canonical\" href=\"https://anildash.com/2023/06/08/ai-is-unreasonable/\">\n\n<meta property=\"og:title\" content=\"Today&apos;s AI is unreasonable - Anil Dash\">\n<meta property=\"og:type\" content=\"article\">\n\n<meta property=\"og:url\" content=\"https://anildash.com/2023/06/08/ai-is-unreasonable/\">\n<meta property=\"og:description\" content=\"A blog about making culture. Since 1999.\">\n<meta property=\"og:image\" content=\"https://cdn.glitch.global/c4e475b2-a54e-47e0-973c-ed0bd1b46262/anildash-2022.png?v=1669512851303\">\n\n<meta name=\"twitter:card\" content=\"summary\">\n\n<meta name=\"twitter:url\" content=\"https://anildash.com/2023/06/08/ai-is-unreasonable/\">\n<meta name=\"twitter:title\" content=\"Today&apos;s AI is unreasonable - Anil Dash\">\n<meta name=\"twitter:description\" content=\"A blog about making culture. Since 1999.\">\n<meta name=\"twitter:image\" content=\"https://cdn.glitch.global/c4e475b2-a54e-47e0-973c-ed0bd1b46262/anildash-2022.png?v=1669512851303\">\n\n\n\n    <link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"https://cdn.glitch.global/c4e475b2-a54e-47e0-973c-ed0bd1b46262/apple-touch-icon.png?v=1670219223610\">\n    <link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://cdn.glitch.global/c4e475b2-a54e-47e0-973c-ed0bd1b46262/favicon-32x32.png?v=1670219224099\">\n    <link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://cdn.glitch.global/c4e475b2-a54e-47e0-973c-ed0bd1b46262/favicon-16x16.png?v=1670219223859\">\n    <link rel=\"icon\" type= \"image/x-icon\" href=\"https://cdn.glitch.global/c4e475b2-a54e-47e0-973c-ed0bd1b46262/apple-touch-icon.png?v=1670219223610\">\n    \n    <meta name=\"generator\" content=\"Eleventy v2.0.1\">\n\n    \n      <meta name=\"twitter:creator\" content=\"@anildash\"/>\n    \n\n  </head>\n  <body>\n    <div class=\"layout-wrapper\">\n\n      <header class=\"header\">\n        <div class=\"header__content\">\n          <h1 class=\"site-title\">\n            <a href=/>\n              Anil Dash\n            </a>\n          </h1>\n          <nav class=\"nav\">\n            <ul class=\"nav__list\">\n                <li class=\"nav-item\">\n                  <a href=\"https://anildash.com/about\">About</a>\n                </li>\n                <li class=\"nav-item\">\n                    <a rel=\"me\" href=\"https://me.dm/@anildash\">anildash@me.dm</a>\n                </li>\n            </ul>\n          </nav>\n        </div>\n      </header>\n\n      <main class=\"main\">\n        \n<article class=\"post\">\n  <header class=\"post__header\">\n    <h1>Today&#39;s AI is unreasonable</h1>\n    <div class=\"post__details\">\n      <time datetime=\"2023-06-08\">\n        08 Jun 2023\n      </time>\n      <div class=\"post__tags\">\n      \n        \n        <a href=\"/tags/tech/\">tech</a>\n      \n      </div>\n      <button id=\"share-link\" aria-label=\"share\"></button>\n    </div>\n\n    \n    <figure class=\"heroimage\" style=\"background-image: url(https://cdn.glitch.global/d45aff89-36ba-46db-8c7c-3da7c8a93931/victor-3YW2jxSblE8-unsplash.jpg?v=1686235653044);\">\n      \n    </figure>\n    \n  </header>\n\n  <main class=\"post__content\">\n    <p>There's an extraordinary amount of hype around &quot;AI&quot; right now, perhaps even greater than in past cycles, where we've seen an AI bubble about once per decade. This time, the focus is on generative systems, particularly LLMs and other tools designed to generate plausible outputs that either make people feel like the response is correct, or where the response is sufficient to fill in for domains where correctness doesn't matter.</p>\n<p>But we can tell the traditional tech industry (the handful of giant tech companies, along with startups backed by the handful of most powerful venture capital firms) is in the midst of building another &quot;Web3&quot;-style froth bubble because they've again abandoned one of the core values of actual technology-based advancement: reason.</p>\n<p>I don't say this lightly, I say this with purpose. Amongst engineers, coders, technical architects, and product designers, one of the most important traits that a system can have is that one can <em>reason</em> about that system in a consistent and predictable way. Even &quot;garbage in, garbage out&quot; is an articulation of this principle¬†‚Äî a system should be predictable enough in its operation that we can then rely on it when building other systems upon it.</p>\n<p>This core concept of a system being reason-able is pervasive in the intellectual architecture of true technologies. Postel's Law (&quot;Be liberal in what you accept, and conservative in what you send.&quot;) depends on reasonable-ness. The famous <a href=\"https://datatracker.ietf.org/doc/html/rfc2119\">IETF keywords list</a>, which offers a specific technical definition for terms like &quot;MUST&quot;, &quot;MUST NOT&quot;, &quot;SHOULD&quot;, and &quot;SHOULD NOT&quot;, <em>assumes</em> that a system will behave in a reasonable and predictable way, and the entire internet runs on specifications that sit on top of that assumption.</p>\n<p>The very act of debugging assumes that a system is meant to work in a particular way, with repeatable outputs, and that deviations from those expectations are the manifestation of that bug, which is why being able to reproduce a bug is the very first step to debugging.</p>\n<p>Into that world, let's introduce bullshit. Today's highly-hyped generative AI systems (most famously OpenAI) are designed to generate bullshit by design. To be clear, bullshit can sometimes be useful, and even accidentally correct, but that doesn't keep it from being bullshit. Worse, these systems are not meant to generate <em>consistent</em> bullshit ‚Äî you can get different bullshit answers from the same prompts. You can put garbage in and get... bullshit out, but the same quality bullshit that you get from non-garbage inputs! And enthusiasts are current mistaking the fact that the bullshit is consistently wrapped in the same envelope as meaning that the bullshit inside is consistent, laundering the unreasonable-ness into <em>appearing</em> reasonable.</p>\n<p>Now we have billions of dollars being invested into technologies where it is impossible to make falsifiable assertions. A system that you cannot debug through a logical, socratic process is a vulnerability that exploitative tech tycoons will use to do what they always do, undermine the vulnerable.</p>\n<p>So, what can we do? A simple thing for technologists, or those who work with them, to do is to make a simple demand: we need systems we can reason about. A system where we can provide the same input multiple times, and the response will change in minor or major ways, for unknown and unknowable reasons, and yet we're expected to rebuild entire other industries or ecosystems around it, is merely a tool for manipulation.</p>\n<p>Narcissists and abusers use the inconsistent and capricious changing of responses as a way of controlling and manipulating their victims. They are <em>unreasonable</em> because it is an effective way to keep the vulnerable in a place where they constantly have to respond, or where they have to live in a constant state of fear and anticipation about how they will be expected to react. Technologies are created by people, and systems reflect the values of their creators.</p>\n<p>We should react to unreasonableness in purported technologies in the same way we react to intentional unreasonableness in people in positions of power: set firm boundaries, be ready to walk away, don't debate, demand consistent and reasonable behavior.</p>\n\n  </main>\n\n  <aside class=\"post__aside\">\n\n    <nav class=\"post__pagination\">\n\n        <a href=\"/2023/03/14/what-was-selling-out/\">\n          <span>‚Üê</span>\n          <span>What was selling out?</span>\n        </a>\n\n      \n\n        <a href=\"/2023/07/07/vc-qanon/\">\n          <span>&quot;VC qanon&quot; and the radicalization of the tech tycoons</span>\n          <span>‚Üí</span>\n        </a>\n\n      \n      \n    </nav>\n  </aside>\n\n</article>\n\n  <script>\n    const shareData = {\n      title: 'Today&#39;s AI is unreasonable',\n      url: 'https://anildash.com/2023/06/08/ai-is-unreasonable/'\n    }\n\n    const btn = document.querySelector('#share-link');\n\n    // Share must be triggered by \"user activation\"\n    btn.addEventListener('click', async () => {\n      try {\n        await navigator.share(shareData);\n      } catch (err) {\n      }\n    });  \n  </script>\n      </main>\n<footer class=\"footer\">\n        <div class=\"footer__content\">\n          <ul class=\"hero__social-links\">\n            \n\n\n                <li>\n                  <a href=\"https://glitch.com/edit/#!/remix/glitch-hello-eleventy\" target=\"_blank\" rel=\"noopener noreferrer\">Remix on üéèGlitch</a>\n                </li>\n\n                <li>\n                  <a href=\"https://twitter.com/anildash\" target=\"_blank\" rel=\"noopener noreferrer\">@anildash</a>\n                \n                </li>\n\n                <li>\n                  <a href=\"/feed.xml\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"rss-icon\"><img src=\"https://cdn.glitch.global/c4e475b2-a54e-47e0-973c-ed0bd1b46262/rss.svg?v=1669925974414\" alt=\"RSS\" /></a>\n                </li>\n                <li>\n                  <a href=\"https://anilytics-live.glitch.me\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://anilytics-live.glitch.me/counter.png?fallback=anildash.com&color=black\" alt=\"site analytics\" style=\"margin-top: 0;\" aria-hidden=\"true\" /></a>\n                </li>\n          </ul>\n        </div>\n      </footer>\n  </body>\n</html>","oembed":false,"readabilityObject":{"title":"Today's AI is unreasonable - Anil Dash","content":"<div id=\"readability-page-1\" class=\"page\"><div>\n    <p>There's an extraordinary amount of hype around \"AI\" right now, perhaps even greater than in past cycles, where we've seen an AI bubble about once per decade. This time, the focus is on generative systems, particularly LLMs and other tools designed to generate plausible outputs that either make people feel like the response is correct, or where the response is sufficient to fill in for domains where correctness doesn't matter.</p>\n<p>But we can tell the traditional tech industry (the handful of giant tech companies, along with startups backed by the handful of most powerful venture capital firms) is in the midst of building another \"Web3\"-style froth bubble because they've again abandoned one of the core values of actual technology-based advancement: reason.</p>\n<p>I don't say this lightly, I say this with purpose. Amongst engineers, coders, technical architects, and product designers, one of the most important traits that a system can have is that one can <em>reason</em> about that system in a consistent and predictable way. Even \"garbage in, garbage out\" is an articulation of this principle&nbsp;‚Äî a system should be predictable enough in its operation that we can then rely on it when building other systems upon it.</p>\n<p>This core concept of a system being reason-able is pervasive in the intellectual architecture of true technologies. Postel's Law (\"Be liberal in what you accept, and conservative in what you send.\") depends on reasonable-ness. The famous <a href=\"https://datatracker.ietf.org/doc/html/rfc2119\">IETF keywords list</a>, which offers a specific technical definition for terms like \"MUST\", \"MUST NOT\", \"SHOULD\", and \"SHOULD NOT\", <em>assumes</em> that a system will behave in a reasonable and predictable way, and the entire internet runs on specifications that sit on top of that assumption.</p>\n<p>The very act of debugging assumes that a system is meant to work in a particular way, with repeatable outputs, and that deviations from those expectations are the manifestation of that bug, which is why being able to reproduce a bug is the very first step to debugging.</p>\n<p>Into that world, let's introduce bullshit. Today's highly-hyped generative AI systems (most famously OpenAI) are designed to generate bullshit by design. To be clear, bullshit can sometimes be useful, and even accidentally correct, but that doesn't keep it from being bullshit. Worse, these systems are not meant to generate <em>consistent</em> bullshit ‚Äî you can get different bullshit answers from the same prompts. You can put garbage in and get... bullshit out, but the same quality bullshit that you get from non-garbage inputs! And enthusiasts are current mistaking the fact that the bullshit is consistently wrapped in the same envelope as meaning that the bullshit inside is consistent, laundering the unreasonable-ness into <em>appearing</em> reasonable.</p>\n<p>Now we have billions of dollars being invested into technologies where it is impossible to make falsifiable assertions. A system that you cannot debug through a logical, socratic process is a vulnerability that exploitative tech tycoons will use to do what they always do, undermine the vulnerable.</p>\n<p>So, what can we do? A simple thing for technologists, or those who work with them, to do is to make a simple demand: we need systems we can reason about. A system where we can provide the same input multiple times, and the response will change in minor or major ways, for unknown and unknowable reasons, and yet we're expected to rebuild entire other industries or ecosystems around it, is merely a tool for manipulation.</p>\n<p>Narcissists and abusers use the inconsistent and capricious changing of responses as a way of controlling and manipulating their victims. They are <em>unreasonable</em> because it is an effective way to keep the vulnerable in a place where they constantly have to respond, or where they have to live in a constant state of fear and anticipation about how they will be expected to react. Technologies are created by people, and systems reflect the values of their creators.</p>\n<p>We should react to unreasonableness in purported technologies in the same way we react to intentional unreasonableness in people in positions of power: set firm boundaries, be ready to walk away, don't debate, demand consistent and reasonable behavior.</p>\n\n  </div></div>","textContent":"\n    There's an extraordinary amount of hype around \"AI\" right now, perhaps even greater than in past cycles, where we've seen an AI bubble about once per decade. This time, the focus is on generative systems, particularly LLMs and other tools designed to generate plausible outputs that either make people feel like the response is correct, or where the response is sufficient to fill in for domains where correctness doesn't matter.\nBut we can tell the traditional tech industry (the handful of giant tech companies, along with startups backed by the handful of most powerful venture capital firms) is in the midst of building another \"Web3\"-style froth bubble because they've again abandoned one of the core values of actual technology-based advancement: reason.\nI don't say this lightly, I say this with purpose. Amongst engineers, coders, technical architects, and product designers, one of the most important traits that a system can have is that one can reason about that system in a consistent and predictable way. Even \"garbage in, garbage out\" is an articulation of this principle¬†‚Äî a system should be predictable enough in its operation that we can then rely on it when building other systems upon it.\nThis core concept of a system being reason-able is pervasive in the intellectual architecture of true technologies. Postel's Law (\"Be liberal in what you accept, and conservative in what you send.\") depends on reasonable-ness. The famous IETF keywords list, which offers a specific technical definition for terms like \"MUST\", \"MUST NOT\", \"SHOULD\", and \"SHOULD NOT\", assumes that a system will behave in a reasonable and predictable way, and the entire internet runs on specifications that sit on top of that assumption.\nThe very act of debugging assumes that a system is meant to work in a particular way, with repeatable outputs, and that deviations from those expectations are the manifestation of that bug, which is why being able to reproduce a bug is the very first step to debugging.\nInto that world, let's introduce bullshit. Today's highly-hyped generative AI systems (most famously OpenAI) are designed to generate bullshit by design. To be clear, bullshit can sometimes be useful, and even accidentally correct, but that doesn't keep it from being bullshit. Worse, these systems are not meant to generate consistent bullshit ‚Äî you can get different bullshit answers from the same prompts. You can put garbage in and get... bullshit out, but the same quality bullshit that you get from non-garbage inputs! And enthusiasts are current mistaking the fact that the bullshit is consistently wrapped in the same envelope as meaning that the bullshit inside is consistent, laundering the unreasonable-ness into appearing reasonable.\nNow we have billions of dollars being invested into technologies where it is impossible to make falsifiable assertions. A system that you cannot debug through a logical, socratic process is a vulnerability that exploitative tech tycoons will use to do what they always do, undermine the vulnerable.\nSo, what can we do? A simple thing for technologists, or those who work with them, to do is to make a simple demand: we need systems we can reason about. A system where we can provide the same input multiple times, and the response will change in minor or major ways, for unknown and unknowable reasons, and yet we're expected to rebuild entire other industries or ecosystems around it, is merely a tool for manipulation.\nNarcissists and abusers use the inconsistent and capricious changing of responses as a way of controlling and manipulating their victims. They are unreasonable because it is an effective way to keep the vulnerable in a place where they constantly have to respond, or where they have to live in a constant state of fear and anticipation about how they will be expected to react. Technologies are created by people, and systems reflect the values of their creators.\nWe should react to unreasonableness in purported technologies in the same way we react to intentional unreasonableness in people in positions of power: set firm boundaries, be ready to walk away, don't debate, demand consistent and reasonable behavior.\n\n  ","length":4190,"excerpt":"A blog about making culture. Since 1999.","byline":"undefined","dir":null,"siteName":null,"lang":"en"},"finalizedMeta":{"title":"Today's AI is unreasonable - Anil Dash","description":"A blog about making culture. Since 1999.","author":false,"creator":"undefined","publisher":false,"date":"2023-07-11T16:37:38.899Z","topics":[]},"jsonLd":{"@type":false,"headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false}},"twitterObj":false,"status":200,"metadata":{"author":"undefined","title":"Today's AI is unreasonable - Anil Dash","description":"A blog about making culture. Since 1999.","canonical":"https://anildash.com/2023/06/08/ai-is-unreasonable/","keywords":[],"image":"https://cdn.glitch.global/c4e475b2-a54e-47e0-973c-ed0bd1b46262/rss.svg?v=1669925974414","firstParagraph":"There's an extraordinary amount of hype around \"AI\" right now, perhaps even greater than in past cycles, where we've seen an AI bubble about once per decade. This time, the focus is on generative systems, particularly LLMs and other tools designed to generate plausible outputs that either make people feel like the response is correct, or where the response is sufficient to fill in for domains where correctness doesn't matter."},"dublinCore":{},"opengraph":{"title":"Today's AI is unreasonable - Anil Dash","description":"A blog about making culture. Since 1999.","url":"https://anildash.com/2023/06/08/ai-is-unreasonable/","site_name":false,"locale":false,"type":"article","typeObject":{"published_time":false,"modified_time":false,"author":false,"publisher":false,"section":false,"tag":[]},"image":"https://cdn.glitch.global/c4e475b2-a54e-47e0-973c-ed0bd1b46262/anildash-2022.png?v=1669512851303"},"twitter":{"site":false,"description":"A blog about making culture. Since 1999.","card":"summary","creator":"@anildash","title":"Today's AI is unreasonable - Anil Dash","image":"https://cdn.glitch.global/c4e475b2-a54e-47e0-973c-ed0bd1b46262/anildash-2022.png?v=1669512851303","url":"https://anildash.com/2023/06/08/ai-is-unreasonable/"},"archivedData":{"link":false,"wayback":false}}}