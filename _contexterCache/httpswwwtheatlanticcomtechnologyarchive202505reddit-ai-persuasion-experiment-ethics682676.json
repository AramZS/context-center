{"initialLink":"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/","sanitizedLink":"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/","finalLink":"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/","htmlEmbed":"<script>window.contexterSetup=window.contexterSetup||function(){window.contexterSetupComplete=!0;class ContexterLink extends HTMLAnchorElement{constructor(){super()}connectedCallback(){this.setAttribute(\"target\",\"_blank\")}}customElements.define(\"contexter-link\",ContexterLink,{extends:\"a\"}),customElements.define(\"contexter-inner\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__inner\"}}),customElements.define(\"contexter-thumbnail\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__thumbnail\"}}),customElements.define(\"contexter-byline\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__byline\"}}),customElements.define(\"contexter-keywordset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__keywordset\"}}),customElements.define(\"contexter-linkset\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__linkset\"}}),customElements.define(\"contexter-meta\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"contexter-box__meta\"}}),customElements.define(\"contexter-summary\",class extends HTMLElement{constructor(){super()}attributeChangedCallback(name,oldValue,newValue){}connectedCallback(){this.className=\"p-summary entry-summary\"}}),customElements.define(\"contexter-box-head\",class extends HTMLElement{constructor(){super()}connectedCallback(){this.className=\"contexter-box__head\"}}),customElements.define(\"contexter-box-inner\",class extends HTMLElement{constructor(){super()}connectedCallback(){}});class ContexterBox extends HTMLElement{constructor(){super(),this.first=!0,this.shadow=this.attachShadow({mode:\"open\"})}connectedCallback(){if(this.first){this.first=!1;var style=document.createElement(\"style\"),lightDomStyle=(style.innerHTML=`:host {--background: #f5f6f7;--border: darkblue;--blue: #0000ee;--font-color: black;--inner-border: black;font-family: Franklin,Arial,Helvetica,sans-serif;font-size: 14px;background: var(--background);width: 600px;color: var(--font-color);min-height: 90px;display: block;padding: 8px;border: 1px solid var(--border);cursor: pointer;box-sizing: border-box;margin: 6px;contain: content;margin: 6px auto;}// can only select top-level nodes with slotted::slotted(*) {max-width: 100%;display:block;}::slotted([slot=thumbnail]) {max-width: 100%;display:block;}::slotted([slot=header]) {width: 100%;font-size: 1.25rem;font-weight: bold;display:block;margin-bottom: 6px;}::slotted([slot=author]) {max-width: 50%;font-size: 12px;display:inline-block;float: left;}::slotted([slot=time]) {max-width: 50%;font-size: 12px;display:inline-block;float: right;}::slotted([slot=summary]) {width: 100%;margin-top: 6px;padding: 10px 2px;border-top: 1px solid var(--inner-border);font-size: 15px;display:inline-block;margin-bottom: 6px;}contexter-meta {height: auto;margin-bottom: 4px;width: 100%;display: grid;position: relative;min-height: 16px;grid-template-columns: repeat(2, 1fr);}::slotted([slot=keywords]) {width: 80%;padding: 2px 4px;border-top: 1px solid var(--inner-border);font-size: 11px;display: block;float: right;font-style: italic;text-align: right;grid-column: 2/2;grid-row: 1;align-self: end;justify-self: end;}::slotted([slot=keywords]):empty {border-top: 0px solid var(--inner-border);}::slotted([slot=archive-link]) {font-size: 1em;display: inline;}::slotted([slot=archive-link])::after {content: \"|\";display: inline;color: var(--font-color);text-decoration: none;margin: 0 .5em;}::slotted([slot=read-link]) {font-size: 1em;display: inline;}contexter-linkset {width: 80%;padding: 2px 4px;font-size: 13px;float: left;font-weight: bold;grid-row: 1;grid-column: 1/2;align-self: end;justify-self: start;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {:host {width: 310px;}}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){:host {--background: #354150;--border: #1f2b37;--blue: #55b0ff;--font-color: #ffffff;--inner-border: #787a7c;background: var(--background);border: 1px solid var(--border)}}`,document.createElement(\"style\"));lightDomStyle.innerHTML=`contexter-box {contain: content;}contexter-box .read-link {font-weight: bold;}contexter-box a {color: #0000ee;}contexter-box img {width: 100%;border: 0;padding: 0;margin: 0;}/* Extra small devices (phones, 600px and down) */@media only screen and (max-width: 600px) {...}/* Small devices (portrait tablets and large phones, 600px and up) */@media only screen and (min-width: 600px) {...}/* Medium devices (landscape tablets, 768px and up) */@media only screen and (min-width: 768px) {...}/* Large devices (laptops/desktops, 992px and up) */@media only screen and (min-width: 992px) {...}/* Extra large devices (large laptops and desktops, 1200px and up) */@media only screen and (min-width: 1200px) {...}@media (prefers-color-scheme: dark){contexter-box a {color: #55b0ff;}}`,this.appendChild(lightDomStyle),this.shadow.appendChild(style);const innerContainer=document.createElement(\"contexter-box-inner\"),innerSlotThumbnail=(this.shadow.appendChild(innerContainer),document.createElement(\"slot\")),innerSlotHeader=(innerSlotThumbnail.name=\"thumbnail\",innerContainer.appendChild(innerSlotThumbnail),document.createElement(\"slot\")),innerSlotAuthor=(innerSlotHeader.name=\"header\",innerContainer.appendChild(innerSlotHeader),document.createElement(\"slot\")),innerSlotTime=(innerSlotAuthor.name=\"author\",innerContainer.appendChild(innerSlotAuthor),document.createElement(\"slot\")),innerSlotSummary=(innerSlotTime.name=\"time\",innerContainer.appendChild(innerSlotTime),document.createElement(\"slot\")),metaContainer=(innerSlotSummary.name=\"summary\",innerContainer.appendChild(innerSlotSummary),document.createElement(\"contexter-meta\")),innerSlotInfo=(innerContainer.appendChild(metaContainer),document.createElement(\"slot\")),linkContainer=(innerSlotInfo.name=\"keywords\",metaContainer.appendChild(innerSlotInfo),document.createElement(\"contexter-linkset\")),innerSlotArchiveLink=(metaContainer.appendChild(linkContainer),document.createElement(\"slot\")),innerSlotReadLink=(innerSlotArchiveLink.name=\"archive-link\",linkContainer.appendChild(innerSlotArchiveLink),document.createElement(\"slot\"));innerSlotReadLink.name=\"read-link\",linkContainer.appendChild(innerSlotReadLink),this.className=\"contexter-box\",this.onclick=e=>{if(!e.target.className.includes(\"read-link\")&&!e.target.className.includes(\"title-link\")){const mainLinks=this.querySelectorAll(\"a.main-link\");mainLinks[0].click()}}}}}customElements.define(\"contexter-box\",ContexterBox)},window.contexterSetupComplete||window.contexterSetup();</script><contexter-box class=\"link-card h-entry hentry\" itemscope=\"\" itemtype=\"https://schema.org/CreativeWork\"><contexter-thumbnail class=\"thumbnail\" slot=\"thumbnail\"></contexter-thumbnail><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><contexter-box-head slot=\"header\" class=\"p-name entry-title\" itemprop=\"headline\"><a is=\"contexter-link\" href=\"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/\" itemprop=\"url\">‘The Worst Internet-Research Ethics Violation I Have Ever Seen’</a></contexter-box-head></contexter-box-head><contexter-byline class=\"p-author author\" slot=\"author\"><span class=\"p-name byline\" rel=\"author\" itemprop=\"author\">Tom Bartlett</span></contexter-byline><time class=\"dt-published published\" slot=\"time\" itemprop=\"datePublished\" datetime=\"2025-05-03T14:07:06.000Z\">5/3/2025</time><contexter-summary class=\"p-summary entry-summary\" itemprop=\"abstract\" slot=\"summary\"><p>The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.</p></contexter-summary><contexter-keywordset itemprop=\"keywords\" slot=\"keywords\"><span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\"></span>, <span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\">technology</span>, <span rel=\"category tag\" class=\"p-category\" itemprop=\"keywords\">Technology</span></contexter-keywordset><a href=\"https://web.archive.org/web/20250511182110/https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/\" is=\"contexter-link\" target=\"_blank\" rel=\"timemap\" class=\"read-link archive-link\" itemprop=\"archivedAt\" slot=\"archive-link\">Archived</a><a is=\"contexter-link\" href=\"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/\" class=\"read-link main-link\" itemprop=\"sameAs\" slot=\"read-link\">Read</a></contexter-box>","linkId":"613197f00c6a50bd97e2a93b8fe4a8b3ffaab941","data":{"originalLink":"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/","sanitizedLink":"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/","canonical":"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/","htmlText":"<!DOCTYPE html><html lang=\"en\" dir=\"ltr\"><head><meta charSet=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"/><link rel=\"icon\" href=\"https://cdn.theatlantic.com/_next/static/images/favicon-3888b0e329526a975703e3059a02b92d.ico\"/><link rel=\"apple-touch-icon\" href=\"https://cdn.theatlantic.com/_next/static/images/apple-touch-icon-default-b504d70343a9438df64c32ce339c7ebc.png\"/><link rel=\"apple-touch-icon\" sizes=\"76x76\" href=\"https://cdn.theatlantic.com/_next/static/images/apple-touch-icon-76x76-d5accc11b8265af76495fbfa9d38dd3b.png\"/><link rel=\"apple-touch-icon\" sizes=\"120x120\" href=\"https://cdn.theatlantic.com/_next/static/images/apple-touch-icon-120x120-419ba228184c040a691628d3dd82c206.png\"/><link rel=\"apple-touch-icon\" sizes=\"152x152\" href=\"https://cdn.theatlantic.com/_next/static/images/apple-touch-icon-152x152-aafde20dd981a38fcd549b29b2b3b785.png\"/><meta name=\"application-name\" content=\"theatlantic\"/><meta name=\"msapplication-TileColor\" content=\"#FFFFFF\"/><meta name=\"msapplication-TileImage\" content=\"https://cdn.theatlantic.com/_next/static/images/apple-touch-icon-default-b504d70343a9438df64c32ce339c7ebc.png\"/><meta property=\"og:site_name\" content=\"The Atlantic\"/><meta property=\"og:locale\" content=\"en_US\"/><meta property=\"fb:admins\" content=\"577048155,17301937\"/><meta property=\"fb:app_id\" content=\"100770816677686\"/><meta property=\"fb:pages\" content=\"29259828486,1468531833474495,1061579677251147,457711054591520,370457103090695,1631141167169115,148681772342453,1510507419185410,128344747344340,128377530562508,236061986423933\"/><meta name=\"p:domain_verify\" content=\"68e1a0361a557708fefc992f3309ed70\"/><meta name=\"twitter:site\" content=\"@theatlantic\"/><meta name=\"twitter:domain\" content=\"theatlantic.com\"/><script type=\"application/ld+json\">{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"The Atlantic\",\"url\":\"https://www.theatlantic.com\",\"inLanguage\":\"en-US\",\"issn\":\"1072-7825\",\"potentialAction\":{\"@type\":\"SearchAction\",\"target\":\"https://www.theatlantic.com/search/?q={q}\",\"query-input\":\"required name=q\"}}</script><script type=\"application/ld+json\">{\"@context\":\"https://schema.org\",\"@type\":\"Organization\",\"@id\":\"https://www.theatlantic.com/#publisher\",\"name\":\"The Atlantic\",\"url\":\"https://www.theatlantic.com\",\"logo\":{\"@type\":\"ImageObject\",\"width\":{\"@type\":\"QuantitativeValue\",\"unitCode\":\"E37\",\"value\":224},\"height\":{\"@type\":\"QuantitativeValue\",\"unitCode\":\"E37\",\"value\":224},\"url\":\"https://cdn.theatlantic.com/assets/media/files/atlantic-logo--224x224.png\"},\"sameAs\":[\"https://www.facebook.com/TheAtlantic\",\"https://twitter.com/theatlantic\"]}</script><title>The Secret AI Experiment That Sent Reddit Into a Frenzy - The Atlantic</title><meta name=\"description\" content=\"The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.\"/><meta property=\"krux:title\" content=\"The Secret AI Experiment That Sent Reddit Into a Frenzy - The Atlantic\"/><meta property=\"krux:description\" content=\"The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.\"/><link rel=\"canonical\" href=\"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/\"/><link rel=\"image_src\" href=\"https://cdn.theatlantic.com/thumbor/B7t1UhFG0zYeDEQ3kmL-nn9AhLw=/0x68:2000x1110/1200x625/media/img/mt/2025/05/reddit_1/original.jpg\"/><meta name=\"author\" content=\"Tom Bartlett\"/><link rel=\"ia:markup_url\" href=\"https://www.theatlantic.com/facebook-instant/article/682676/\"/><meta property=\"article:publisher\" content=\"https://www.facebook.com/TheAtlantic/\"/><meta property=\"article:opinion\" content=\"false\"/><meta property=\"article:content_tier\" content=\"metered\"/><meta property=\"article:tag\" content=\"technology\"/><meta property=\"article:section\" content=\"Technology\"/><meta property=\"article:published_time\" content=\"2025-05-02T17:55:38Z\"/><meta property=\"article:modified_time\" content=\"2025-05-03T14:07:06Z\"/><meta name=\"robots\" content=\"index, follow, max-image-preview:large\"/><meta property=\"og:title\" content=\"‘The Worst Internet-Research Ethics Violation I Have Ever Seen’\"/><meta property=\"og:description\" content=\"The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.\"/><meta property=\"og:url\" content=\"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/\"/><meta property=\"og:type\" content=\"article\"/><meta property=\"og:image\" content=\"https://cdn.theatlantic.com/thumbor/B7t1UhFG0zYeDEQ3kmL-nn9AhLw=/0x68:2000x1110/1200x625/media/img/mt/2025/05/reddit_1/original.jpg\"/><meta property=\"twitter:card\" content=\"summary_large_image\"/><meta name=\"FacebookShareMessage\" content=\"A secret experiment that turned Redditors into guinea pigs was an ethical disaster—and could undermine research that’s only getting more urgent, @tebartl reports.\"/><meta name=\"TwitterShareMessage\" content=\"A secret experiment that turned Redditors into guinea pigs was an ethical disaster—and could undermine research that’s only getting more urgent, @tebartl reports.\"/><link rel=\"alternate\" type=\"application/rss+xml\" title=\"The Atlantic\" href=\"/feed/all/\"/><link rel=\"alternate\" type=\"application/rss+xml\" title=\"Best of The Atlantic\" href=\"/feed/best-of/\"/><meta name=\"referrer\" content=\"unsafe-url\"/><meta name=\"apple-mobile-web-app-capable\" content=\"yes\"/><meta name=\"apple-mobile-web-status-bar-style\" content=\"black\"/><meta name=\"apple-mobile-web-app-title\" content=\"The Atlantic\"/><meta name=\"keywords\" itemID=\"#keywords\"/><meta name=\"news_keywords\"/><meta name=\"sailthru.title\" content=\"‘The Worst Internet-Research Ethics Violation I Have Ever Seen’\"/><meta name=\"sailthru.description\" content=\"The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.\"/><meta name=\"sailthru.tags\" content=\"artificial-intelligence,technology,high-converters,author-tom-bartlett\"/><meta name=\"sailthru.date\" content=\"2025-05-02T17:55:38Z\"/><link rel=\"preload\" as=\"font\" type=\"font/woff2\" href=\"https://www.theatlantic.com/packages/fonts/garamond/AGaramondPro-Regular.woff2\" crossorigin=\"\"/><link rel=\"preload\" as=\"font\" type=\"font/woff2\" href=\"https://www.theatlantic.com/packages/fonts/graphik/Graphik-Regular-Web.woff2\" crossorigin=\"\"/><link rel=\"preload\" as=\"font\" type=\"font/woff2\" href=\"https://www.theatlantic.com/packages/fonts/graphik/Graphik-Semibold-Web.woff2\" crossorigin=\"\"/><link rel=\"preload\" as=\"font\" type=\"font/woff2\" href=\"https://www.theatlantic.com/packages/fonts/logic/LogicMonospace-Medium.woff2\" crossorigin=\"\"/><link rel=\"preload\" as=\"font\" type=\"font/woff2\" href=\"https://www.theatlantic.com/packages/fonts/logic/LogicMonospace-Regular.woff2\" crossorigin=\"\"/><script type=\"application/ld+json\">{\"@context\":\"https://schema.org\",\"@type\":\"NewsArticle\",\"headline\":\"The Secret AI Experiment That Sent Reddit Into a Frenzy\",\"alternativeHeadline\":\"‘The Worst Internet-Research Ethics Violation I Have Ever Seen’\",\"description\":\"The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.\",\"url\":\"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/\",\"datePublished\":\"2025-05-02T17:55:38Z\",\"dateModified\":\"2025-05-03T14:07:06Z\",\"isAccessibleForFree\":false,\"hasPart\":{\"@type\":\"WebPageElement\",\"isAccessibleForFree\":false,\"cssSelector\":\".article-content-body\"},\"publisher\":{\"@id\":\"https://www.theatlantic.com/#publisher\"},\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/\"},\"image\":[{\"@type\":\"ImageObject\",\"width\":{\"@type\":\"QuantitativeValue\",\"unitCode\":\"E37\",\"value\":720},\"height\":{\"@type\":\"QuantitativeValue\",\"unitCode\":\"E37\",\"value\":405},\"url\":\"https://cdn.theatlantic.com/thumbor/T3Oyb2ZD_rMW9IWs0w-BcsgR2Zg=/0x0:2000x1125/720x405/media/img/mt/2025/05/reddit_1/original.jpg\"},{\"@type\":\"ImageObject\",\"width\":{\"@type\":\"QuantitativeValue\",\"unitCode\":\"E37\",\"value\":1080},\"height\":{\"@type\":\"QuantitativeValue\",\"unitCode\":\"E37\",\"value\":1080},\"url\":\"https://cdn.theatlantic.com/thumbor/SPCwxI7D8CM-vJLqbR14jeCONjU=/485x0:1610x1125/1080x1080/media/img/mt/2025/05/reddit_1/original.jpg\"},{\"@type\":\"ImageObject\",\"width\":{\"@type\":\"QuantitativeValue\",\"unitCode\":\"E37\",\"value\":1200},\"height\":{\"@type\":\"QuantitativeValue\",\"unitCode\":\"E37\",\"value\":900},\"url\":\"https://cdn.theatlantic.com/thumbor/D3cNpBQw3rR4jn6l9xYOdQhUuzw=/249x0:1749x1125/1200x900/media/img/mt/2025/05/reddit_1/original.jpg\"},{\"@type\":\"ImageObject\",\"width\":{\"@type\":\"QuantitativeValue\",\"unitCode\":\"E37\",\"value\":1600},\"height\":{\"@type\":\"QuantitativeValue\",\"unitCode\":\"E37\",\"value\":900},\"url\":\"https://cdn.theatlantic.com/thumbor/esyTT-yGFKE26VMaxjgDpA5HQXI=/0x0:2000x1125/1600x900/media/img/mt/2025/05/reddit_1/original.jpg\"},{\"@type\":\"ImageObject\",\"width\":{\"@type\":\"QuantitativeValue\",\"unitCode\":\"E37\",\"value\":960},\"height\":{\"@type\":\"QuantitativeValue\",\"unitCode\":\"E37\",\"value\":540},\"url\":\"https://cdn.theatlantic.com/thumbor/_RS0mJORhUjk2oKFp1-lrHWZJ0Y=/0x0:2000x1125/960x540/media/img/mt/2025/05/reddit_1/original.jpg\"},{\"@type\":\"ImageObject\",\"width\":{\"@type\":\"QuantitativeValue\",\"unitCode\":\"E37\",\"value\":540},\"height\":{\"@type\":\"QuantitativeValue\",\"unitCode\":\"E37\",\"value\":540},\"url\":\"https://cdn.theatlantic.com/thumbor/x0B8YDVGWEWdlT5tybp0laDD56k=/485x0:1610x1125/540x540/media/img/mt/2025/05/reddit_1/original.jpg\"}],\"author\":[{\"@type\":\"Person\",\"name\":\"Tom Bartlett\",\"sameAs\":\"https://www.theatlantic.com/author/tom-bartlett/\"}],\"articleSection\":\"Technology\"}</script><link rel=\"preload\" as=\"image\" href=\"https://cdn.theatlantic.com/thumbor/_RS0mJORhUjk2oKFp1-lrHWZJ0Y=/0x0:2000x1125/960x540/media/img/mt/2025/05/reddit_1/original.jpg\" imageSrcSet=\"https://cdn.theatlantic.com/thumbor/f_du4irHtRNkjeNQQwkc67dBWBo=/0x0:2000x1125/750x422/media/img/mt/2025/05/reddit_1/original.jpg 750w, https://cdn.theatlantic.com/thumbor/e4jEibjQkC_AtPxf6WB7O-CZ8wU=/0x0:2000x1125/828x466/media/img/mt/2025/05/reddit_1/original.jpg 828w, https://cdn.theatlantic.com/thumbor/_RS0mJORhUjk2oKFp1-lrHWZJ0Y=/0x0:2000x1125/960x540/media/img/mt/2025/05/reddit_1/original.jpg 960w, https://cdn.theatlantic.com/thumbor/K-HTgOf-8ecmTJUtIp-1XUeWFpE=/0x0:2000x1125/976x549/media/img/mt/2025/05/reddit_1/original.jpg 976w, https://cdn.theatlantic.com/thumbor/pP0fvpOUnZzkl7oOPHRWZ5NFXf8=/0x0:2000x1125/1952x1098/media/img/mt/2025/05/reddit_1/original.jpg 1952w\" imageSizes=\"(min-width: 976px) 976px, 100vw\"/><meta name=\"next-head-count\" content=\"63\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/css/852e69fc9add3370.css\" as=\"style\"/><link rel=\"stylesheet\" href=\"https://cdn.theatlantic.com/_next/static/css/852e69fc9add3370.css\" data-n-g=\"\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/css/fc89893f61bf4bfb.css\" as=\"style\"/><link rel=\"stylesheet\" href=\"https://cdn.theatlantic.com/_next/static/css/fc89893f61bf4bfb.css\" data-n-p=\"\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/css/0d2a8d6598c296c1.css\" as=\"style\"/><link rel=\"stylesheet\" href=\"https://cdn.theatlantic.com/_next/static/css/0d2a8d6598c296c1.css\" data-n-p=\"\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/css/f66b0545fa4b5cb0.css\" as=\"style\"/><link rel=\"stylesheet\" href=\"https://cdn.theatlantic.com/_next/static/css/f66b0545fa4b5cb0.css\" data-n-p=\"\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/css/08cb381068f06ad3.css\" as=\"style\"/><link rel=\"stylesheet\" href=\"https://cdn.theatlantic.com/_next/static/css/08cb381068f06ad3.css\" data-n-p=\"\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/css/0a33952225252f86.css\" as=\"style\"/><link rel=\"stylesheet\" href=\"https://cdn.theatlantic.com/_next/static/css/0a33952225252f86.css\" data-n-p=\"\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/css/3e436c0432bf05ae.css\" as=\"style\"/><link rel=\"stylesheet\" href=\"https://cdn.theatlantic.com/_next/static/css/3e436c0432bf05ae.css\"/><noscript data-n-css=\"\"></noscript><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/9587.03e639dfa3e4199a.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/webpack-0d5a2cb0b4910e76.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/framework-ca706bf673a13738.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/main-6b1ab16c3edd97e7.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/pages/_app-7c2ae150b4b06c1e.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/6729-7978443139836095.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/8286-b35c81576953924b.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/436-aadfbd871a794704.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/9843-267e63e874251a37.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/899-448b91a629016ff2.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/2912-ebf029ed4116b01f.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/6772-2c0b832574392efa.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/6392-1397c4f8500d73b6.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/4947-14b734b9a9ce020d.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/3297-543c6448c0812417.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/365-c8fca3fa4ec2068c.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/1744-9f7f4aee961e11d6.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/9310-d0d5baafd7a7f3b8.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/4742-11d63822f6f0cf2b.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/1730-999b0129269ea1cb.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/1198-0022e74b04f81a33.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/2381-7fd43dea62d91684.js\" as=\"script\"/><link rel=\"preload\" href=\"https://cdn.theatlantic.com/_next/static/chunks/pages/%5Bchannel%5D/archive/%5Byear%5D/%5Bmonth%5D/%5Bslug%5D/%5Bid%5D-9a1848eea99404c9.js\" as=\"script\"/></head><body><div id=\"__next\"><div data-event-surface=\"article\"><div></div><nav class=\"Nav_root__HcZek\" aria-labelledby=\"site-navigation\" data-event-module=\"site nav\" id=\"main-navigation\"><div class=\"Nav_mainNav__iPsWc\"><a href=\"#main-content\" class=\"Nav_skipLink__P4Y5R\">Skip to content</a><h2 id=\"site-navigation\" class=\"Nav_visuallyHide__Lzzui\">Site Navigation</h2><div class=\"Nav_flexContainer__9iJ4H\"><ul class=\"Nav_leftContainer__Xs54R\"><li class=\"Nav_navListItem__l2afO Nav_visuallyHideOnMobile__N9bs2\"><a href=\"/\" class=\"Nav_navLink__34Bol\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 87.83 134\" class=\"Nav_bigA__c1aIb\"><title>The Atlantic</title><path d=\"M24.48 95.13c-.56 0-.74-.37-.74-.93l13.08-55.88c.19-.94.93-.94 1.12 0L50.09 94.2c0 .56-.19.93-.75.93zM48.22.19a22.54 22.54 0 01-7.66 5.05c-.75.19-.94.37-1.13 1.12l-26.72 112.5c-2 9-4.67 10.66-11.77 11.22a.88.88 0 00-.94.93v2.06a.88.88 0 00.92.93h25.6a.88.88 0 00.93-.93V131a.88.88 0 00-.93-.93c-9.53 0-10.47-2.81-8.6-10.66l4.49-19.25a1.18 1.18 0 011.12-.93h26.74a1.19 1.19 0 011.13.93l5 23.18c1.12 5-.75 6.17-7.1 6.73a.88.88 0 00-.93.93v2.06a.88.88 0 00.93.93h37.62a.88.88 0 00.94-.93V131a.88.88 0 00-.94-.93c-5.79-.56-8.22-1.5-9.34-6.73L49.34.57c-.19-.56-.75-.75-1.12-.38\"></path></svg></a></li><li class=\"Nav_navListItem__l2afO Nav_hamburgerLi__gP6Dn\"><button class=\"NavHamburgerButton_root__OgJkB\" aria-expanded=\"false\" aria-controls=\"expanded-nav\" aria-label=\"Open Main Menu\"><div class=\"NavHamburgerButton_burger__jIWmI\"><div class=\"NavHamburgerButton_box__J5rDn\"><div class=\"NavHamburgerButton_inner__dKlIy\"></div></div></div></button><div class=\"Nav_expandedNav__o5Zj_\"><div hidden=\"\" class=\"ExpandedNav_root__r3hKE\" id=\"expanded-nav\"><div class=\"ExpandedNav_mobileHeader__QEenD\" data-event-element=\"mobile links\"><button class=\"ExpandedNav_searchButton__85mWm\" aria-label=\"Search The Atlantic\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"ExpandedNav_searchIcon__2EngD\"><path d=\"M15.85 15.15l-5.27-5.28a6 6 0 10-.71.71l5.28 5.27a.48.48 0 00.7 0 .48.48 0 000-.7zM1 6a5 5 0 115 5 5 5 0 01-5-5z\"></path></svg></button><div><a class=\"ExpandedNav_mostPopular__EbSyn\" href=\"/most-popular/\">Popular</a><a class=\"ExpandedNav_latest__zSrBe\" href=\"/latest/\">Latest</a><a class=\"ExpandedNav_newsletters__W83ni\" href=\"/newsletters/\">Newsletters</a></div></div><div class=\"ExpandedNav_container__sDhOz\"><div class=\"ExpandedNav_sections__oGeXo\" data-event-element=\"sections\"><h2 class=\"ExpandedNav_title__C8QcN ExpandedNav_sectionTitle___xWBI\">Sections</h2><ul class=\"ExpandedNav_sectionUl__mLUY1\"><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/politics/\" class=\"ExpandedNav_sectionLink__3iXo9\">Politics</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/ideas/\" class=\"ExpandedNav_sectionLink__3iXo9\">Ideas</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"https://www.theatlantic.com/category/fiction/\" class=\"ExpandedNav_sectionLink__3iXo9\">Fiction</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/technology/\" class=\"ExpandedNav_sectionLink__3iXo9\">Technology</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/science/\" class=\"ExpandedNav_sectionLink__3iXo9\">Science</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"https://www.theatlantic.com/photo/\" class=\"ExpandedNav_sectionLink__3iXo9\">Photo</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/economy/\" class=\"ExpandedNav_sectionLink__3iXo9\">Economy</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/culture/\" class=\"ExpandedNav_sectionLink__3iXo9\">Culture</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/projects/planet/\" class=\"ExpandedNav_sectionLink__3iXo9\">Planet</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/international/\" class=\"ExpandedNav_sectionLink__3iXo9\">Global</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/books/\" class=\"ExpandedNav_sectionLink__3iXo9\">Books</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/audio/\" class=\"ExpandedNav_sectionLink__3iXo9\">Audio</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/health/\" class=\"ExpandedNav_sectionLink__3iXo9\">Health</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/education/\" class=\"ExpandedNav_sectionLink__3iXo9\">Education</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/projects/\" class=\"ExpandedNav_sectionLink__3iXo9\">Projects</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"https://www.theatlantic.com/category/features/\" class=\"ExpandedNav_sectionLink__3iXo9\">Features</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/family/\" class=\"ExpandedNav_sectionLink__3iXo9\">Family</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/events/\" class=\"ExpandedNav_sectionLink__3iXo9\">Events</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"https://www.theatlantic.com/category/washington-week-atlantic/\" class=\"ExpandedNav_sectionLink__3iXo9\">Washington Week</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"https://www.theatlantic.com/progress/\" class=\"ExpandedNav_sectionLink__3iXo9\">Progress</a></li><li class=\"ExpandedNav_sectionLi__tZz7K\"><a href=\"/newsletters/\" class=\"ExpandedNav_sectionLink__3iXo9\">Newsletters</a></li></ul></div><div class=\"ExpandedNav_moreLinks__G4VPb\" data-event-element=\"more links\"><ul class=\"ExpandedNav_moreLinksList__u0bVY\"><li class=\"ExpandedNav_moreLinksListItem__UrTkv\"><a href=\"/archive/\" class=\"ExpandedNav_moreLinksItem__JhFzM\"><img alt=\"\" loading=\"lazy\" class=\"Image_root__XxsOp Image_lazy__hYWHV ExpandedNav_moreLinksImg__IY3fl\" src=\"https://cdn.theatlantic.com/_next/static/images/nav-archive-promo-5541b02ae92f1a9276249e1c6c2534ee.png\" width=\"80\" height=\"80\"/><span>Explore The Atlantic Archive</span></a></li><li class=\"ExpandedNav_moreLinksListItem__UrTkv\"><a href=\"/free-daily-crossword-puzzle/\" class=\"ExpandedNav_moreLinksItem__JhFzM\"><svg width=\"80\" height=\"80\" viewBox=\"0 0 64 64\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" class=\"ExpandedNav_moreLinksImg__IY3fl\"><g clip-path=\"url(#crossword-promo_svg__clip0_1939_66324)\"><path fill=\"#E7131A\" d=\"M0 0h64v64H0z\"></path><path d=\"M37.988 26.531H26.516v11.474h11.472V26.53zM37.988 38.805H26.516V50.28h11.472V38.805zM25.715 26.531H14.243v11.474h11.472V26.53zM50.26 26.531H38.789v11.474h11.473V26.53zM36.469 16.842c.096.183.156.39.156.612a1.326 1.326 0 11-1.326-1.327c.197 0 .384.047.552.124.167-.464.303-1.137.338-1.993h-5.287c-.039.968-.217 1.908-.527 2.584.096.183.156.39.156.612a1.326 1.326 0 11-1.327-1.327c.198 0 .384.047.552.124.167-.464.303-1.137.339-1.993h-3.58v11.474h11.472V14.258h-.992c-.038.968-.216 1.908-.526 2.584z\" fill=\"#fff\"></path><path d=\"M47.487 17.853a.4.4 0 010-.8c.135 0 .36-.27.552-.803a1.313 1.313 0 00-.552-.124 1.326 1.326 0 101.326 1.327c0-.222-.06-.428-.156-.612-.282.615-.674 1.012-1.17 1.012zM41.393 17.853a.4.4 0 010-.8c.135 0 .36-.27.552-.803a1.313 1.313 0 00-.552-.124 1.326 1.326 0 101.326 1.327c0-.222-.06-.428-.156-.612-.282.615-.673 1.012-1.17 1.012zM35.298 17.853a.4.4 0 010-.8c.135 0 .36-.27.552-.803a1.313 1.313 0 00-.552-.124 1.326 1.326 0 101.326 1.327c0-.222-.06-.428-.156-.612-.282.615-.673 1.012-1.17 1.012zM29.204 17.853a.4.4 0 010-.8c.136 0 .36-.27.552-.803a1.313 1.313 0 00-.552-.124 1.326 1.326 0 101.327 1.327c0-.222-.06-.428-.156-.612-.282.615-.673 1.012-1.17 1.012zM23.11 17.853a.4.4 0 010-.8c.135 0 .36-.27.552-.803a1.313 1.313 0 00-.552-.124 1.326 1.326 0 101.326 1.327c0-.222-.06-.428-.156-.612-.282.615-.673 1.012-1.17 1.012zM17.016 17.853a.4.4 0 010-.8c.135 0 .36-.27.552-.803a1.313 1.313 0 00-.552-.124 1.326 1.326 0 101.326 1.327c0-.222-.06-.428-.156-.612-.282.615-.672 1.012-1.17 1.012z\" fill=\"#E7131A\"></path><path d=\"M50.66 13.458h-1.476c-.072-1.85-.652-3.605-1.698-3.605-1.045 0-1.624 1.753-1.697 3.605h-2.7c-.071-1.85-.651-3.605-1.698-3.605-1.045 0-1.624 1.753-1.697 3.605h-2.7c-.072-1.85-.652-3.605-1.698-3.605-1.045 0-1.624 1.753-1.697 3.605H30.9c-.072-1.85-.652-3.605-1.698-3.605-1.045 0-1.624 1.753-1.697 3.605h-2.697c-.072-1.85-.652-3.605-1.698-3.605-1.045 0-1.624 1.753-1.697 3.605h-2.7c-.071-1.85-.651-3.605-1.698-3.605-1.045 0-1.624 1.753-1.697 3.605h-1.476a.4.4 0 00-.4.4V50.68c0 .22.18.4.4.4H50.66a.4.4 0 00.4-.4V13.858c0-.221-.178-.4-.4-.4zm-20.565.8c-.035.855-.171 1.528-.338 1.992-.192.532-.417.803-.552.803a.4.4 0 000 .8c.497 0 .888-.398 1.17-1.012.31-.676.488-1.616.527-2.583h5.288c-.036.855-.172 1.528-.339 1.992-.192.532-.417.803-.552.803a.4.4 0 000 .8c.498 0 .888-.398 1.17-1.012.311-.676.489-1.616.528-2.583h.992V25.73H26.515V14.258h3.58zm20.166 23.747H38.788V26.53h11.473v11.474zM14.243 26.53h11.472v11.474H14.243V26.53zm34.57-9.078a1.326 1.326 0 11-1.327-1.327c.198 0 .384.047.552.124-.192.532-.416.803-.552.803a.4.4 0 000 .8c.498 0 .888-.398 1.17-1.012.097.184.157.39.157.612zm-7.42.4c.497 0 .888-.398 1.17-1.012.096.183.156.39.156.612a1.326 1.326 0 11-1.326-1.327c.197 0 .384.047.552.124-.192.532-.417.803-.552.803a.4.4 0 000 .8zm-3.405 20.152H26.515V26.53h11.473v11.474zM24.436 17.453a1.326 1.326 0 11-1.326-1.327c.197 0 .384.047.552.124-.192.532-.417.803-.552.803a.4.4 0 000 .8c.497 0 .888-.398 1.17-1.012.097.184.156.39.156.612zm-6.094 0a1.326 1.326 0 11-1.326-1.327c.198 0 .384.047.552.124-.192.532-.417.803-.552.803a.4.4 0 000 .8c.498 0 .888-.398 1.17-1.012.097.184.156.39.156.612zm8.173 21.352h11.473V50.28H26.515V38.805zm20.971-28.152c.253 0 .817.96.892 2.805h-1.782c.075-1.844.638-2.805.89-2.805zm-6.093 0c.253 0 .817.96.891 2.805h-1.782c.076-1.844.638-2.805.89-2.805zm-6.095 0c.253 0 .817.96.892 2.805h-1.782c.075-1.844.638-2.805.89-2.805zm-6.094 0c.253 0 .817.96.891 2.805h-1.781c.075-1.844.637-2.805.89-2.805zm-6.094 0c.252 0 .816.96.89 2.805h-1.78c.074-1.844.637-2.805.89-2.805zm-6.094 0c.253 0 .817.96.891 2.805h-1.781c.075-1.844.637-2.805.89-2.805z\" fill=\"#000\"></path></g><defs><clipPath id=\"crossword-promo_svg__clip0_1939_66324\"><path fill=\"#fff\" d=\"M0 0h64v64H0z\"></path></clipPath></defs></svg><span>Play The Atlantic crossword</span></a></li><li class=\"ExpandedNav_moreLinksListItem__UrTkv\"><a href=\"/audio/\" class=\"ExpandedNav_moreLinksItem__JhFzM\"><svg width=\"80\" height=\"80\" viewBox=\"0 0 64 64\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" class=\"ExpandedNav_moreLinksImg__IY3fl\"><path fill=\"#FAF4EB\" d=\"M0 0h63.998v64H0z\"></path><path d=\"M25.267 31.27h-1.171v12.138h1.17a.392.392 0 00.393-.392V31.662a.392.392 0 00-.392-.393v.002zM38.34 31.662v11.354c0 .217.175.392.392.392h1.171V31.271h-1.17a.392.392 0 00-.393.392v-.002z\" fill=\"#000\"></path><path d=\"M44.605 33.479c.106-.69.163-1.398.163-2.12 0-7.343-5.718-13.296-12.77-13.296-7.05 0-12.768 5.953-12.768 13.296 0 .722.057 1.429.163 2.12l-1.413.58v6.56l2.033.834a3.194 3.194 0 001.586 1.65c.411.193.869.305 1.353.305h.34V31.271h-.34c-.174 0-.345.017-.511.044a3.14 3.14 0 00-1.236.48c-.005-.145-.011-.289-.011-.434 0-6.25 4.847-11.334 10.805-11.334 5.958 0 10.805 5.085 10.805 11.334 0 .145-.005.29-.01.433a3.163 3.163 0 00-1.748-.523h-.34v12.137h.34a3.197 3.197 0 002.939-1.953l2.033-.835v-6.56l-1.413-.58v-.001zM35.71 49.806a.498.498 0 100 .997.498.498 0 000-.997zM30.235 50.8a.498.498 0 100-.996.498.498 0 000 .997zM28.059 48.218a.498.498 0 100 .997.498.498 0 000-.997zM32.104 50.072a.498.498 0 100-.997.498.498 0 000 .997zM33.105 47.731a.498.498 0 100 .997.498.498 0 000-.997zM29.675 48.09a.498.498 0 10.996 0 .498.498 0 00-.996 0zM35.71 48.156a.498.498 0 10-.997 0 .498.498 0 00.997 0zM37.508 49.085a.498.498 0 100 .996.498.498 0 000-.996zM39 47.336a.498.498 0 100 .996.498.498 0 000-.996zM39.159 45.754a.498.498 0 100 .996.498.498 0 000-.996zM40.74 46.07a.498.498 0 100 .997.498.498 0 000-.997zM40.266 44.33a.498.498 0 100 .997.498.498 0 000-.997zM34.47 49.983a.498.498 0 10-.996 0 .498.498 0 00.997 0zM27.746 51.075a.498.498 0 100-.997.498.498 0 000 .997zM29.736 51.668a.498.498 0 100 .996.498.498 0 000-.996zM32.375 52.8a.498.498 0 100-.997.498.498 0 000 .997zM48.894 42.2a.498.498 0 100 .997.498.498 0 000-.997zM50.794 25.623a.497.497 0 10.7.082.497.497 0 00-.7-.082zM50.32 27.679a.497.497 0 10.7.082.497.497 0 00-.7-.082zM48.52 25.229a.497.497 0 10.78-.614.497.497 0 00-.78.614zM48.809 20.633a.498.498 0 10.616.781.498.498 0 00-.616-.781zM52.468 24.48a.497.497 0 10.78-.614.497.497 0 00-.78.615zM54.84 24.769a.498.498 0 10.617.781.498.498 0 00-.616-.781zM55.338 27.141a.497.497 0 10.782-.617.497.497 0 10-.782.617zM53.817 27.018a.497.497 0 10-.781.614.497.497 0 00.781-.614zM57.145 29.216a.498.498 0 10-.616-.781.498.498 0 00.616.781zM55.064 23.779a.498.498 0 10-.617-.782.498.498 0 00.617.782zM40.054 13.142a.498.498 0 10-.617-.781.498.498 0 00.617.781zM53.735 21.578a.498.498 0 10-.616-.78.498.498 0 00.616.78zM55.85 31.981a.497.497 0 10-.78.614.497.497 0 00.78-.614zM54.84 33.58a.498.498 0 10.617.781.498.498 0 00-.616-.78zM56.316 35.18a.497.497 0 10.617.784.497.497 0 00-.617-.783zM55.966 37.762a.498.498 0 10.616.781.498.498 0 00-.616-.781zM56.498 33.18a.498.498 0 10.617.781.498.498 0 00-.617-.781zM53.13 41.909a.498.498 0 10.617.78.498.498 0 00-.617-.78zM42.078 49.183a.498.498 0 10.617.781.498.498 0 00-.617-.781zM40.255 50.41a.498.498 0 10.616.782.498.498 0 00-.616-.782zM36.42 51.49a.498.498 0 10.618.782.498.498 0 00-.617-.782zM33.851 51.728a.498.498 0 10.617.78.498.498 0 00-.617-.78zM38.312 50.842a.498.498 0 10.616.781.498.498 0 00-.616-.78zM50.653 43.918a.498.498 0 10-.617-.78.498.498 0 00.617.78zM50.393 44.936a.498.498 0 10.616.782.498.498 0 00-.616-.782zM57.489 31.298a.497.497 0 10-.782.617.497.497 0 10.782-.617zM51.652 19.689a.497.497 0 10-.617-.784.497.497 0 00.617.784zM51.154 21.688a.498.498 0 10-.616-.782.498.498 0 00.616.782zM48.69 19.641a.497.497 0 10.781-.614.497.497 0 00-.781.614zM48.465 17.63a.498.498 0 10-.617-.782.498.498 0 00.617.781zM46.498 16.596a.5.5 0 00.554-.435.5.5 0 00-.99-.118.5.5 0 00.436.553zM46.244 18.243a.5.5 0 00.435.554.5.5 0 00.554-.436.5.5 0 00-.435-.553.5.5 0 00-.554.435zM45.325 15.307a.5.5 0 00.554-.435.5.5 0 00-.989-.118.5.5 0 00.435.553zM42.096 13.917a.5.5 0 00.554-.435.5.5 0 00-.435-.553.5.5 0 00-.554.434.5.5 0 00.435.554zM49.606 18.61a.5.5 0 00.554-.435.5.5 0 00-.989-.119.5.5 0 00.435.554zM45.478 17.63a.498.498 0 10-.616-.782.498.498 0 00.616.781zM43.392 17.311a.497.497 0 10-.782.617.497.497 0 10.782-.617zM54.34 36.596a.498.498 0 10.781-.618.498.498 0 00-.782.618zM53.384 36.69a.498.498 0 10-.617-.782.498.498 0 00.617.782zM51.95 37.01a.497.497 0 10-.7-.082c.172.217.483.253.7.083zM53.176 31.27a.498.498 0 10.616.782.498.498 0 00-.616-.781zM54.792 30.407a.498.498 0 10.616.782.498.498 0 00-.616-.782zM52.13 32.454a.498.498 0 10.617.781.498.498 0 00-.617-.781zM53.656 35.098a.498.498 0 10-.616-.781.498.498 0 00.616.781zM53.384 30.417a.498.498 0 10-.617-.782.498.498 0 00.617.782zM54.654 28.826a.498.498 0 10-.616-.782.498.498 0 00.616.782zM51.187 31.662a.497.497 0 10-.781.616.497.497 0 10.781-.616zM48.774 26.268a.497.497 0 10.7.082.497.497 0 00-.7-.082zM49.475 29.322a.498.498 0 10-.617-.781.498.498 0 00.617.781zM49.974 33.465a.498.498 0 10-.617-.782.498.498 0 00.617.782zM50.242 35.438a.498.498 0 10-.616-.78.498.498 0 00.616.78zM48.915 34.097a.497.497 0 10-.662.742.497.497 0 00.662-.742zM47.957 35.54a.497.497 0 10-.662.743.497.497 0 10.662-.743zM47.582 37.525a.497.497 0 10-.662.743.497.497 0 10.662-.743zM51.792 34.599a.498.498 0 10-.617-.782.498.498 0 00.617.782zM49.54 29.909a.498.498 0 10.617.781.498.498 0 00-.617-.781zM52.032 28.215a.498.498 0 10.617.78.498.498 0 00-.617-.78zM53.158 26.426a.497.497 0 10-.7-.082c.171.217.483.253.7.082zM48.578 32.179a.498.498 0 10-.617-.782.498.498 0 00.617.782zM50.89 30.403a.5.5 0 00.988.118.5.5 0 00-.989-.118zM46.624 25.265a.498.498 0 10.617.78.498.498 0 00-.617-.78z\" fill=\"#000\"></path><path d=\"M45.827 31.83a.497.497 0 10.7.082.497.497 0 00-.7-.083zM47.58 33.16a.497.497 0 10-.781.613.497.497 0 00.781-.614zM47.74 27.738a.498.498 0 10-.616-.782.498.498 0 00.616.782zM47.95 30.243a.5.5 0 00-.99-.119.5.5 0 00.99.119zM46.367 30.243a.5.5 0 00-.99-.119.5.5 0 00.99.119zM46.683 28.345a.5.5 0 00-.99-.119.5.5 0 00.99.119zM45.734 26.921a.5.5 0 00-.99-.118.5.5 0 00.99.118zM51.545 23.233a.497.497 0 10.781-.614.497.497 0 00-.781.614zM6.538 28.26a.499.499 0 10.51-.853.499.499 0 00-.51.852zM7.67 26.127a.499.499 0 10.51-.854.499.499 0 00-.51.854zM22.566 14.925a.499.499 0 10.51-.853.499.499 0 00-.51.853zM14.93 46.545a.499.499 0 10-.51.854.499.499 0 00.51-.854zM14.43 45.648a.499.499 0 10-.856-.512.499.499 0 00.856.512zM8.545 24.343a.499.499 0 10.51-.853.499.499 0 00-.51.853zM9.538 22.678a.5.5 0 10.51-.853.5.5 0 00-.51.853zM8.556 29.822a.499.499 0 10-.856-.512.499.499 0 00.856.512zM17.15 16.267a.498.498 0 10-.616-.781.498.498 0 00.616.781zM6.896 33.144a.498.498 0 10-.616-.782.498.498 0 00.616.782zM9.125 33.325a.498.498 0 10-.616-.781.498.498 0 00.616.781zM7.207 34.31a.498.498 0 10-.782.617.498.498 0 00.782-.618zM15.916 46.436a.498.498 0 100-.997.498.498 0 000 .997zM16.595 48.48a.498.498 0 100-.996.498.498 0 000 .997zM18.442 49.436a.498.498 0 100-.997.498.498 0 000 .997zM13.869 18.485a.498.498 0 100-.997.498.498 0 000 .997zM18.393 15.409a.498.498 0 100-.997.498.498 0 000 .997zM21.04 14.363a.497.497 0 10-.701-.704.497.497 0 00.702.704zM25.524 12.75a.497.497 0 10-.702-.704.497.497 0 00.702.704zM28.182 12.285a.497.497 0 10-.702-.704.497.497 0 00.702.704zM18.673 47.005a.498.498 0 10-.997 0 .498.498 0 00.997 0zM12.707 42.075a.498.498 0 10-.001.997.498.498 0 000-.997zM12.512 39.8a.498.498 0 10-.07-.993.498.498 0 00.07.994zM10.954 40.65a.498.498 0 100-.997.498.498 0 000 .997zM11.492 41.112a.498.498 0 100 .996.498.498 0 000-.996zM9.543 41.143a.498.498 0 100 .997.498.498 0 000-.997zM14.316 40.844a.498.498 0 10-.996 0 .498.498 0 00.996 0zM13.704 38.478a.498.498 0 100-.996.498.498 0 000 .996zM15.03 36.2a.498.498 0 10-.997 0 .498.498 0 00.997 0zM14.652 33.933a.498.498 0 100-.997.498.498 0 000 .997zM11.635 35.752a.498.498 0 100-.997.498.498 0 000 .997zM12.41 33.66a.498.498 0 100-.996.498.498 0 000 .997zM8.618 37.754a.498.498 0 10-.996 0 .498.498 0 00.996 0zM8.329 39.347a.498.498 0 100 .996.498.498 0 000-.996zM11.083 38.577a.498.498 0 10-.997 0 .498.498 0 00.997 0zM10.124 37.434a.498.498 0 100-.996.498.498 0 000 .996zM7.317 36.843a.498.498 0 100-.997.498.498 0 000 .997zM12.525 37.447a.497.497 0 10-.702-.704.497.497 0 00.702.704zM12.242 29.982a.498.498 0 100 .996.498.498 0 000-.996zM9.54 35.253a.498.498 0 10-.997 0 .498.498 0 00.997 0zM11.192 33.647a.498.498 0 10-.996 0 .498.498 0 00.996 0zM15.171 28.82a.498.498 0 100-.997.498.498 0 000 .996zM17.296 28.661a.498.498 0 100-.997.498.498 0 000 .997zM16.7 29.59a.498.498 0 10-.997 0 .498.498 0 00.996 0zM18.282 30.064a.498.498 0 10-.996 0 .498.498 0 00.996 0zM19.386 21.03a.498.498 0 100 .996.498.498 0 000-.996zM17.714 32.372a.498.498 0 100-.997.498.498 0 000 .997zM16.665 33.488a.498.498 0 100-.996.498.498 0 000 .996zM16.225 35.07a.498.498 0 100-.997.498.498 0 000 .997zM16.665 36.63a.498.498 0 100-.997.498.498 0 000 .996zM11.908 25.748a.498.498 0 100-.997.498.498 0 000 .997zM15.123 21.001a.498.498 0 100 .997.498.498 0 000-.997zM15.047 20.157a.498.498 0 100-.997.498.498 0 000 .997zM16.58 18.614a.498.498 0 100-.996.498.498 0 000 .997zM15.621 17.62a.498.498 0 10.001-.997.498.498 0 000 .997zM13.37 20.775a.498.498 0 100-.997.498.498 0 000 .997zM11.91 20.248a.498.498 0 10.001-.996.498.498 0 000 .996zM11.35 21.843a.498.498 0 100-.997.498.498 0 000 .997zM14.343 26.426a.498.498 0 100-.996.498.498 0 000 .996zM13.206 25.16a.498.498 0 100-.997.498.498 0 000 .996zM11.76 27.034a.498.498 0 10.997 0 .498.498 0 00-.996 0zM9.499 25.113a.498.498 0 10.997 0 .498.498 0 00-.997 0zM13.357 22.923a.498.498 0 10-.07-.994.498.498 0 00.07.994zM11.248 23.943a.498.498 0 100-.997.498.498 0 000 .997zM9.829 30.444a.498.498 0 100-.997.498.498 0 000 .997zM6.477 30.706a.498.498 0 100-.996.498.498 0 000 .996zM40.136 14a.5.5 0 10.857.513.5.5 0 00-.857-.512zM39.218 15.744a.499.499 0 10.51-.853.499.499 0 00-.51.853zM39.038 17.507a.499.499 0 10.51-.853.499.499 0 00-.51.853zM37.456 17.982a.499.499 0 10.51-.853.499.499 0 00-.51.853zM41.253 15.885a.499.499 0 10.51-.854.499.499 0 00-.51.854zM41.353 17.536a.499.499 0 10-.857-.512.499.499 0 00.857.512zM8.566 31.717a.498.498 0 100-.997.498.498 0 000 .997zM9.944 31.593a.498.498 0 10.997 0 .498.498 0 00-.997 0zM11.185 29.07a.498.498 0 10.996 0 .498.498 0 00-.996 0zM8.995 27.706a.498.498 0 100-.997.498.498 0 000 .997zM10.51 28.41a.498.498 0 100-.997.498.498 0 000 .997zM13.48 29.227a.498.498 0 10.001-.996.498.498 0 000 .996zM15.417 32.092a.498.498 0 100-.997.498.498 0 000 .997zM13.704 32.215a.498.498 0 100-.997.498.498 0 000 .997zM14.501 30.37a.498.498 0 100-.997.498.498 0 000 .996zM44.77 48.378a.498.498 0 00-.518.85.498.498 0 00.518-.85zM44.012 45.958a.499.499 0 00.517-.85.498.498 0 00-.517.85zM42.917 46.809a.498.498 0 10-.851-.517.498.498 0 00.851.517zM46.904 44.45a.498.498 0 10.516-.849.498.498 0 00-.517.85zM46.057 44.471a.498.498 0 10-.852-.516.498.498 0 00.852.516zM44.79 43.206a.498.498 0 10-.851-.517.498.498 0 00.851.517zM46.407 46.65a.498.498 0 10-.852-.516.498.498 0 00.852.517zM44.657 47.563a.498.498 0 10-.852-.516.498.498 0 00.852.516zM46.963 47.375a.498.498 0 00-.517.85.498.498 0 00.517-.85zM39.595 48.876a.498.498 0 00-.518.85.498.498 0 00.518-.85zM41.32 47.967a.498.498 0 00-.517.849.498.498 0 00.517-.85zM36.547 47.83a.498.498 0 10.851.517.498.498 0 00-.851-.516zM33.264 45.833a.498.498 0 100 .997.498.498 0 000-.997zM35.394 45.468a.498.498 0 10-.997 0 .498.498 0 00.997 0zM36.23 46.249a.498.498 0 10.852.517.498.498 0 00-.852-.517zM37.813 44.351a.498.498 0 10.852.517.498.498 0 00-.852-.517zM47.713 45.044a.498.498 0 00-.517.85.498.498 0 00.517-.85zM49.34 44.473a.499.499 0 00-.518.85.498.498 0 00.518-.85zM48.94 46.502a.498.498 0 10-.516.849.498.498 0 00.516-.849zM50.182 41.493a.498.498 0 10.347.933.498.498 0 00-.347-.933zM49.576 40.558a.498.498 0 10-.934.348.498.498 0 00.934-.348zM46.79 42.667a.498.498 0 10-.346-.933.498.498 0 00.347.933zM47.899 39.962a.498.498 0 10-.933.348.498.498 0 00.933-.348zM51.826 41.47a.497.497 0 10-.346-.932.497.497 0 00.346.933zM49.417 38.224a.498.498 0 10-.933.347.498.498 0 00.933-.347zM50.899 38.714a.498.498 0 10-.933.348.498.498 0 00.933-.348zM49.917 36.489a.498.498 0 10-.934.348.498.498 0 00.934-.348zM51.808 37.923a.498.498 0 10.347.933.498.498 0 00-.347-.933zM25.17 15.247a.498.498 0 100-.996.498.498 0 000 .996zM25.17 16.549a.498.498 0 10.998 0 .498.498 0 00-.997 0zM41.896 19.856a.498.498 0 10-.996 0 .498.498 0 00.996 0zM43.262 18.78a.498.498 0 100 .996.498.498 0 000-.997zM52.1 43.393a.498.498 0 10-.414.906.498.498 0 00.414-.906zM54.618 40.212a.496.496 0 00-.494.501.496.496 0 00.502.494.496.496 0 00.494-.502.496.496 0 00-.502-.493zM54.758 37.561a.496.496 0 00-.493.501.496.496 0 00.501.494.496.496 0 00.494-.502.496.496 0 00-.502-.493zM53.63 39.454a.496.496 0 00-.502-.493.497.497 0 10.502.494zM15.092 23.477a.498.498 0 10.347.932.498.498 0 00-.347-.932zM16.58 20.122a.498.498 0 10.996 0 .498.498 0 00-.997 0zM15.584 26.195a.498.498 0 10.997 0 .498.498 0 00-.997 0zM18.37 27.473a.498.498 0 100-.996.498.498 0 000 .996zM17.454 22.898a.498.498 0 100 .996.498.498 0 000-.996zM17.454 24.796a.498.498 0 100 .996.498.498 0 000-.996zM19.924 19.307a.498.498 0 100 .997.498.498 0 000-.997zM18.988 23.441a.498.498 0 10-.07-.994.498.498 0 00.07.994zM16.878 21.428a.498.498 0 100 .997.498.498 0 000-.997zM30.591 13.057a.497.497 0 10.346.932.497.497 0 00-.346-.932zM30.323 15.714a.498.498 0 10-.346-.933.498.498 0 00.346.933zM32.607 12.294a.498.498 0 100-.996.498.498 0 000 .996zM27.075 16.225a.498.498 0 10.933-.348.498.498 0 00-.933.348zM32.284 16.261a.498.498 0 10-.347-.933.498.498 0 00.347.933zM36.416 13.586a.499.499 0 10-.71-.699.499.499 0 00.71.7zM37.33 12.522a.499.499 0 10-.712-.7.499.499 0 00.712.7zM35.19 12.328a.499.499 0 10-.71-.7.499.499 0 00.71.7zM35.425 16.1a.499.499 0 10.697-.709.499.499 0 00-.697.709zM37.708 15.619a.499.499 0 10-.712-.7.499.499 0 00.712.7zM37.516 14.336a.499.499 0 10.697-.708.499.499 0 00-.697.708zM32.283 13.41a.499.499 0 10.711.7.499.499 0 00-.711-.7zM43.28 15.606a.499.499 0 10.71.7.499.499 0 00-.71-.7zM43.688 14.83a.499.499 0 10-.711-.7.499.499 0 00.71.7zM34.645 14.847a.499.499 0 10-.711-.699.499.499 0 00.711.7zM22.876 13.54a.499.499 0 10.51-.854.499.499 0 00-.51.855zM26.427 13.996a.499.499 0 10.51-.853.499.499 0 00-.51.853zM28.11 13.745a.499.499 0 10.857.511.499.499 0 00-.856-.511zM30.125 12.127a.499.499 0 10.51-.853.499.499 0 00-.51.853zM19.727 46.404a.498.498 0 10.139-.688.497.497 0 00-.14.69v-.002zM21.264 16.28a.498.498 0 10.584-.807.498.498 0 00-.584.807zM23.283 17.712a.497.497 0 10-.806-.584.497.497 0 00.806.584zM21.883 20.704a.497.497 0 10-.806-.584.497.497 0 00.806.584zM22.367 18.346a.498.498 0 10-.584.806.498.498 0 00.584-.806zM23.632 19.295a.498.498 0 10-.583.806.498.498 0 00.583-.806zM21.258 21.825a.498.498 0 10-.583.807.498.498 0 00.583-.807zM19.194 44.97a.498.498 0 10-.984-.156.498.498 0 00.984.155zM17.093 44.084a.498.498 0 10-.156.983.498.498 0 00.156-.983zM18.877 43.184a.498.498 0 10.156-.983.498.498 0 00-.156.983zM19.72 47.355a.498.498 0 10-.156.983.498.498 0 00.156-.983zM13.898 44.147a.498.498 0 10.156-.983.498.498 0 00-.156.983zM11.166 43.092a.498.498 0 10-.156.983.498.498 0 00.156-.983zM12.732 44.693a.497.497 0 10-.643.758.497.497 0 00.643-.758zM25.903 49.082a.498.498 0 10-.156.983.498.498 0 00.156-.983zM25.294 48.085a.497.497 0 10.983.156.497.497 0 00-.983-.156zM24.187 46.662a.498.498 0 10.983.155.498.498 0 00-.983-.155zM25.294 45.397a.498.498 0 10.983.155.498.498 0 00-.983-.155zM26.56 43.657a.498.498 0 10.983.155.498.498 0 00-.983-.155zM26.718 47.136a.498.498 0 10.983.156.498.498 0 00-.983-.156zM27.826 45.397a.498.498 0 10.983.155.498.498 0 00-.983-.155zM29.567 46.187a.497.497 0 10.982.156.497.497 0 00-.982-.156zM31.308 45.238a.498.498 0 10.983.156.498.498 0 00-.983-.156zM31.308 47.294a.498.498 0 10.983.156.498.498 0 00-.983-.156zM26.826 51.355a.498.498 0 10-.156.983.498.498 0 00.156-.983zM30.548 44.587a.498.498 0 10-.997 0 .498.498 0 00.997 0zM29.282 42.847a.498.498 0 10-.996 0 .498.498 0 00.996 0zM29.575 38.505a.498.498 0 100-.996.498.498 0 000 .996zM31.662 35.834a.498.498 0 10-.997 0 .498.498 0 00.997 0zM30.306 40.375a.498.498 0 100-.997.498.498 0 000 .997zM32.148 40.878a.498.498 0 10-.996 0 .498.498 0 00.996 0zM31.79 37.449a.498.498 0 100 .996.498.498 0 000-.996zM31.722 43.48a.498.498 0 100-.997.498.498 0 000 .997zM30.529 42.559a.498.498 0 100-.996.498.498 0 000 .996zM29.302 36.017a.498.498 0 100-.996.498.498 0 000 .996zM28.369 37.669a.498.498 0 10-.997 0 .498.498 0 00.997 0zM28.526 40.805a.498.498 0 100-.997.498.498 0 000 .997zM26.943 39.856a.498.498 0 100-.997.498.498 0 000 .997zM27.832 41.465a.497.497 0 10-.782.616.497.497 0 10.782-.617zM34.047 41.036a.498.498 0 10-.997 0 .498.498 0 00.997 0zM34.413 43.321a.498.498 0 100-.996.498.498 0 000 .996zM33.305 44.903a.498.498 0 100-.997.498.498 0 000 .997zM36.224 34.494a.498.498 0 10-.156.983.498.498 0 00.156-.983zM32.743 34.494a.498.498 0 10-.156.983.498.498 0 00.156-.983zM34.484 35.601a.498.498 0 10-.156.984.498.498 0 00.156-.984zM33.692 37.341a.498.498 0 10-.156.984.498.498 0 00.156-.984zM34.642 39.081a.498.498 0 10-.156.983.498.498 0 00.156-.983zM32.585 39.081a.498.498 0 10-.156.983.498.498 0 00.156-.983zM27.256 36.342a.498.498 0 10-.983-.156.498.498 0 00.983.156zM27.732 34.76a.498.498 0 10-.983-.156.498.498 0 00.983.156zM24.091 50.586a.497.497 0 10.2.974.497.497 0 00-.2-.974zM18.644 19.065a.498.498 0 100-.996.498.498 0 000 .996zM20.551 18.461a.498.498 0 100-.996.498.498 0 000 .996zM23.87 48.901a.498.498 0 10-.993.073.498.498 0 00.993-.073zM22.269 50.1a.498.498 0 10.072.994.498.498 0 00-.072-.994zM20.79 49.487a.498.498 0 10.072.993.498.498 0 00-.072-.993zM22.41 47.385a.498.498 0 10.994-.073.498.498 0 00-.993.073zM21.18 47.61a.498.498 0 10.072.994.498.498 0 00-.072-.993zM18.676 16.562a.498.498 0 10.934-.348.498.498 0 00-.934.348zM14.854 39.586a.498.498 0 100-.997.498.498 0 000 .997zM15.8 43.505a.498.498 0 10-.072-.993.498.498 0 00.072.993zM17.084 40.175a.498.498 0 10-.991-.098.498.498 0 00.991.098zM16.189 38.41a.498.498 0 100-.996.498.498 0 000 .996zM15.417 41.84a.498.498 0 100-.996.498.498 0 000 .997zM17.218 42.632a.498.498 0 100-.997.498.498 0 000 .997zM13.37 35.253a.498.498 0 100-.996.498.498 0 000 .996zM49.084 22.632a.498.498 0 10.617.781.498.498 0 00-.617-.781zM50.237 24.878a.497.497 0 10.782-.614.497.497 0 00-.782.614zM45.455 19.456a.498.498 0 10-.616-.781.498.498 0 00.616.781zM47.968 22.41a.498.498 0 10-.616-.78.498.498 0 00.616.78zM44.428 24.595a.498.498 0 10.616.781.498.498 0 00-.616-.781zM46.643 19.785a.498.498 0 10.616.781.498.498 0 00-.616-.781zM43.84 20.982a.497.497 0 10.781-.617.497.497 0 10-.781.617zM45.445 23.155a.498.498 0 10.617.782.498.498 0 00-.617-.782zM47.057 24.12a.497.497 0 10.781-.614.497.497 0 00-.781.614zM45.431 22.146a.497.497 0 10.782-.617.497.497 0 10-.782.617zM23.47 16.052a.498.498 0 10.997 0 .498.498 0 00-.996 0z\" fill=\"#000\"></path><path d=\"M32 9.223C16.56 9.223 4 19.44 4 32s12.56 22.777 28 22.777c15.438 0 27.998-10.217 27.998-22.777S47.438 9.223 32 9.223zm0 45.268C16.718 54.49 4.285 44.4 4.285 32 4.286 19.598 16.72 9.51 32 9.51c15.28 0 27.713 10.088 27.713 22.49S47.28 54.49 31.999 54.49z\" fill=\"#000\"></path><path d=\"M20.474 43.53c-.633 0-.633.982 0 .982s.633-.982 0-.982zM22.063 44.815c-.633 0-.633.982 0 .982s.633-.982 0-.982zM23.804 44.34c-.633 0-.633.982 0 .982s.633-.982 0-.982zM19.265 25.229c.633 0 .633-.982 0-.982s-.633.982 0 .982zM26.64 18.624c.634 0 .634-.982 0-.982-.632 0-.632.982 0 .982zM24.925 18.14c-.633 0-.633.982 0 .982s.633-.982 0-.982zM29.206 16.432c-.633 0-.633.982 0 .982s.633-.982 0-.982zM31.105 16.432c-.633 0-.633.982 0 .982s.633-.982 0-.982zM34.036 17.08c.633 0 .633-.982 0-.982s-.633.982 0 .982zM42.333 21.879c.633 0 .633-.982 0-.982s-.633.982 0 .982zM39.453 18.273c-.633 0-.633.982 0 .982s.633-.982 0-.982zM35.813 17.008c-.633 0-.633.982 0 .982s.633-.982 0-.982zM43.673 23.527c.633 0 .633-.982 0-.982s-.633.982 0 .982zM42.463 44.949c.633 0 .633-.982 0-.982s-.633.982 0 .982zM36.468 38.162a.498.498 0 100 .997.498.498 0 000-.997zM36.943 39.586a.498.498 0 100 .996.498.498 0 000-.996zM37.22 43.013a.498.498 0 10-.851-.517.498.498 0 00.851.517zM36.745 44.595a.498.498 0 10-.851-.517.498.498 0 00.851.517zM35.123 37.075a.498.498 0 10.851.517.498.498 0 00-.85-.517zM36.547 36.443a.498.498 0 10.851.516.498.498 0 00-.851-.516zM35.342 41.944c.633 0 .633-.983 0-.983s-.633.983 0 .983zM25.778 28.582a.498.498 0 10-.156.983.498.498 0 00.156-.983zM25.462 26.684a.498.498 0 10-.156.983.498.498 0 00.156-.983zM27.222 28.14a.498.498 0 100-.997.498.498 0 000 .996zM24.373 25.925a.498.498 0 100-.997.498.498 0 000 .997zM22.79 30.037a.498.498 0 100-.997.498.498 0 000 .997zM24.215 30.512a.498.498 0 100-.997.498.498 0 000 .997zM23.581 28.297a.498.498 0 10.001-.996.498.498 0 000 .996zM28.171 26.083a.498.498 0 100-.997.498.498 0 000 .997zM33.013 27.19a.498.498 0 100-.996.498.498 0 000 .996zM35.188 28.462a.498.498 0 100-.997.498.498 0 000 .997zM31.143 27.603a.498.498 0 100-.997.498.498 0 000 .997zM30.141 28.947a.498.498 0 100-.996.498.498 0 000 .996zM33.572 28.59a.498.498 0 10-.996 0 .498.498 0 00.996 0zM27.855 29.47a.498.498 0 10.996 0 .498.498 0 00-.996 0zM28.459 27.33a.498.498 0 10.997 0 .498.498 0 00-.997 0zM35.501 26.602a.498.498 0 100-.997.498.498 0 000 .997zM33.827 25.487a.498.498 0 100-.997.498.498 0 000 .997zM30.712 25.825a.498.498 0 100-.996.498.498 0 000 .996zM29.554 24.634a.498.498 0 10-.617-.78.498.498 0 00.617.78zM29.983 30.845a.498.498 0 100-.996.498.498 0 000 .996zM27.696 31.21a.498.498 0 10.997 0 .498.498 0 00-.997 0zM26.114 30.736a.498.498 0 10.997 0 .498.498 0 00-.997 0zM36.212 32.864a.497.497 0 10-.982-.156.497.497 0 00.982.156zM37.794 33.654a.498.498 0 10-.983-.155.498.498 0 00.983.155zM36.054 29.7a.497.497 0 10-.983-.156.497.497 0 00.983.156zM35.42 31.282a.498.498 0 10-.983-.156.498.498 0 00.983.156zM37.478 31.44a.498.498 0 10-.983-.155.498.498 0 00.983.155zM33.68 30.491a.498.498 0 10-.983-.156.498.498 0 00.983.156zM31.939 31.44a.498.498 0 10-.983-.156.498.498 0 00.983.156zM31.939 29.384a.498.498 0 10-.983-.156.498.498 0 00.983.156zM36.42 25.324a.498.498 0 10.157-.983.498.498 0 00-.156.983zM33.32 22.1a.498.498 0 10.001-.996.498.498 0 000 .997zM31.234 22.195a.498.498 0 10.997 0 .498.498 0 00-.997 0zM29.84 23.27a.498.498 0 100-.997.498.498 0 000 .996zM28.256 22.479a.498.498 0 100-.997.498.498 0 000 .997zM27.465 24.06a.498.498 0 100-.997.498.498 0 000 .997zM25.725 24.218a.498.498 0 100-.996.498.498 0 000 .996zM26.516 25.958a.498.498 0 100-.997.498.498 0 000 .997zM33.436 23.64a.498.498 0 100-.997.498.498 0 000 .996zM35.951 22.891a.498.498 0 10.997 0 .498.498 0 00-.997 0zM34.685 21.784a.498.498 0 10.996 0 .498.498 0 00-.996 0zM31.577 24.483a.498.498 0 10.156-.984.498.498 0 00-.156.984zM30.31 21.794a.498.498 0 10.156-.984.498.498 0 00-.156.984zM37.907 24.483a.498.498 0 10.156-.984.498.498 0 00-.156.984zM34.69 23.586a.498.498 0 10.983.155.498.498 0 00-.983-.155zM32.858 33.265a.498.498 0 10-.996 0 .498.498 0 00.996 0zM31.434 33.74a.498.498 0 10-.996 0 .498.498 0 00.997 0zM28.48 34.175a.498.498 0 00.517-.85.498.498 0 00-.517.85zM26.58 33.384a.498.498 0 00.518-.85.498.498 0 00-.517.85zM33.945 31.92a.498.498 0 00-.517.85.498.498 0 00.517-.85zM34.578 33.344a.498.498 0 00-.517.85.498.498 0 00.517-.85zM41.224 30.097a.5.5 0 10-.59-.805.5.5 0 00.59.805zM39.008 28.357a.5.5 0 10-.59-.805.5.5 0 00.59.805zM40.674 28.579a.5.5 0 10-.59-.806.5.5 0 00.59.806zM40.32 26.947a.5.5 0 10-.59-.806.5.5 0 00.59.806zM37.943 26.547a.5.5 0 00-.587-.806.5.5 0 00.587.806zM39.24 25.726a.5.5 0 10-.59-.806.5.5 0 00.59.806zM37.302 28.013a.497.497 0 10-.702-.704.497.497 0 00.702.704zM39.813 29.885a.497.497 0 10-.966.235.497.497 0 00.966-.235zM38.303 29.757a.497.497 0 10-.966.235.497.497 0 00.966-.235zM29.075 32.14c0 .632.983.632.983 0 0-.634-.983-.634-.983 0z\" fill=\"#000\"></path></svg><span>Listen to Podcasts and Articles</span></a></li></ul></div><div class=\"ExpandedNav_print__7d4vw\" data-event-element=\"print edition\"><h2 class=\"ExpandedNav_title__C8QcN ExpandedNav_printTitle__PKCL7\">The Print Edition</h2><div class=\"ExpandedNav_printContainer__Lp_nj\"><a href=\"/magazine/\" class=\"ExpandedNav_printImgLink__gcbdt\"><img alt=\"\" loading=\"lazy\" class=\"Image_root__XxsOp Image_lazy__hYWHV ExpandedNav_printImg__hHeRU\" src=\"https://www.theatlantic.com/magazine/images/current-issue.large.jpg\" width=\"266\" height=\"200\"/></a><div class=\"ExpandedNav_printLinks__gNywy\"><div class=\"ExpandedNav_topPrintLinks__UytSB\"><a href=\"/magazine/\" class=\"ExpandedNav_latestIssue__iDXQm\">Latest Issue</a><a href=\"/magazine/backissues/\" class=\"ExpandedNav_pastIssues__nkE14\">Past Issues</a></div><hr class=\"ExpandedNav_hr__5T2Ez\"/><a href=\"https://accounts.theatlantic.com/products/gift\" class=\"ExpandedNav_giveAGift__vyp0c\">Give a Gift</a></div></div></div></div></div></div></li><li class=\"Nav_navListItem__l2afO Nav_hideOnTablet__wyFPd Nav_searchLi__yxgD4\"><button class=\"NavSearchButton_root__DcP_y\" aria-label=\"Search The Atlantic\" aria-expanded=\"false\" aria-controls=\"nav-desktop-search\" data-event-element=\"search icon\" data-event-verb=\"opened\" data-event-surface=\"search\" data-event-module=\"search overlay\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"NavSearchButton_searchIcon__Acpm1\"><path d=\"M15.85 15.15l-5.27-5.28a6 6 0 10-.71.71l5.28 5.27a.48.48 0 00.7 0 .48.48 0 000-.7zM1 6a5 5 0 115 5 5 5 0 01-5-5z\"></path></svg></button><div data-event-surface=\"search\" data-event-module=\"search overlay\" class=\"SearchOverlay_root__lmUcH\" hidden=\"\" id=\"nav-desktop-search\"><div data-focus-guard=\"true\" tabindex=\"0\" style=\"width:1px;height:0px;padding:0;overflow:hidden;position:fixed;top:1px;left:1px\"></div><div data-focus-lock-disabled=\"false\" aria-modal=\"true\" aria-labelledby=\"search-label\" role=\"dialog\"><form method=\"GET\" action=\"/search/\" class=\"SearchOverlay_searchForm___U0R_\"><div class=\"SearchInput_root__6XLPB\"><div class=\"VisuallyHidden_root__yoK4r\"><label for=\"search-input-:R5pna5im:\">Search The Atlantic</label></div><button type=\"submit\" title=\"Submit\" class=\"SearchInput_searchButton__u0CP0\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" width=\"20\"><path d=\"M15.85 15.15l-5.27-5.28a6 6 0 10-.71.71l5.28 5.27a.48.48 0 00.7 0 .48.48 0 000-.7zM1 6a5 5 0 115 5 5 5 0 01-5-5z\"></path></svg></button><input type=\"search\" name=\"q\" id=\"search-input-:R5pna5im:\" class=\"SearchInput_searchInput__5hWhI SearchInput_hideClear__re5AE\" placeholder=\"Search The Atlantic...\" autoComplete=\"off\" required=\"\"/></div><div class=\"QuickLinks_quickLinksContainer__F_iFd\"><div class=\"QuickLinks_quickLinksHeading__ms7Ht\">Quick Links</div><ul class=\"QuickLinks_quickLinksList__e7x66\"><li class=\"QuickLinks_quickLinkListItem__59_09\"><a class=\"QuickLinks_quickLink__w_Fp0\" href=\"/audio\" data-event-element=\"quick link\" data-event-position=\"1\"><img alt=\"Audio\" loading=\"lazy\" class=\"Image_root__XxsOp Image_lazy__hYWHV QuickLinks_quickLinkImage__FTMBA\" src=\"https://cdn.theatlantic.com/media/files/audio/intro_sprint_2_emblem.png\" width=\"148\" height=\"148\"/><div class=\"QuickLinks_quickLinkLabel__TYtIC\">Audio</div></a></li><li class=\"QuickLinks_quickLinkListItem__59_09\"><a class=\"QuickLinks_quickLink__w_Fp0\" href=\"/free-daily-crossword-puzzle/\" data-event-element=\"quick link\" data-event-position=\"2\"><img alt=\"Crossword Puzzle\" loading=\"lazy\" class=\"Image_root__XxsOp Image_lazy__hYWHV QuickLinks_quickLinkImage__FTMBA\" src=\"https://cdn.theatlantic.com/media/files/2025/crossword_icon_intro_sprint_2.png\" width=\"148\" height=\"148\"/><div class=\"QuickLinks_quickLinkLabel__TYtIC\">Crossword Puzzle</div></a></li><li class=\"QuickLinks_quickLinkListItem__59_09\"><a class=\"QuickLinks_quickLink__w_Fp0\" href=\"/archive/\" data-event-element=\"quick link\" data-event-position=\"3\"><img alt=\"Magazine Archive\" loading=\"lazy\" class=\"Image_root__XxsOp Image_lazy__hYWHV QuickLinks_quickLinkImage__FTMBA\" src=\"https://cdn.theatlantic.com/media/files/archive-thumbnail.png\" width=\"148\" height=\"148\"/><div class=\"QuickLinks_quickLinkLabel__TYtIC\">Magazine Archive</div></a></li><li class=\"QuickLinks_quickLinkListItem__59_09\"><a class=\"QuickLinks_quickLink__w_Fp0\" href=\"https://accounts.theatlantic.com/accounts/subscription/\" data-event-element=\"quick link\" data-event-position=\"4\"><img alt=\"Your Subscription\" loading=\"lazy\" class=\"Image_root__XxsOp Image_lazy__hYWHV QuickLinks_quickLinkImage__FTMBA\" src=\"https://cdn.theatlantic.com/media/files/YourSubscription_300x300.jpg\" width=\"148\" height=\"148\"/><div class=\"QuickLinks_quickLinkLabel__TYtIC\">Your Subscription</div></a></li></ul></div><button type=\"button\" aria-label=\"Close Search\" class=\"SearchOverlay_closeButton___zntA\" data-event-verb=\"closed\" data-event-element=\"close icon\"><svg viewBox=\"0 0 16 16\" xmlns=\"http://www.w3.org/2000/svg\" class=\"SearchOverlay_closeIcon__DrMMb\"><path d=\"M9.525 8l6.159 6.159a1.078 1.078 0 11-1.525 1.525L8 9.524l-6.159 6.16a1.076 1.076 0 01-1.525 0 1.078 1.078 0 010-1.525L6.476 8 .315 1.841A1.078 1.078 0 111.841.316L8 6.476l6.16-6.16a1.078 1.078 0 111.524 1.525L9.524 8z\" fill-rule=\"evenodd\"></path></svg></button></form></div><div data-focus-guard=\"true\" tabindex=\"0\" style=\"width:1px;height:0px;padding:0;overflow:hidden;position:fixed;top:1px;left:1px\"></div></div></li><li class=\"Nav_navListItem__l2afO Nav_hideOnTablet__wyFPd\"><a class=\"Nav_navLink__34Bol\" href=\"/most-popular/\">Popular</a></li><li class=\"Nav_navListItem__l2afO Nav_hideOnTablet__wyFPd\"><a class=\"Nav_navLink__34Bol\" href=\"/latest/\">Latest</a></li><li class=\"Nav_navListItem__l2afO Nav_hideOnTablet__wyFPd\"><a class=\"Nav_navLink__34Bol\" href=\"/newsletters/\">Newsletters</a></li></ul><div aria-hidden=\"true\" class=\"Nav_middleContainer__7JzLF\" data-event-element=\"wordmark\"><a href=\"/\" class=\"Nav_hideAboveMobile__1lhmL Nav_mobileBigALink__eWXD_\" tabindex=\"-1\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 87.83 134\" class=\"Nav_mobileBigA__9PTCs Nav_hideOnMobile__IESg8\"><path d=\"M24.48 95.13c-.56 0-.74-.37-.74-.93l13.08-55.88c.19-.94.93-.94 1.12 0L50.09 94.2c0 .56-.19.93-.75.93zM48.22.19a22.54 22.54 0 01-7.66 5.05c-.75.19-.94.37-1.13 1.12l-26.72 112.5c-2 9-4.67 10.66-11.77 11.22a.88.88 0 00-.94.93v2.06a.88.88 0 00.92.93h25.6a.88.88 0 00.93-.93V131a.88.88 0 00-.93-.93c-9.53 0-10.47-2.81-8.6-10.66l4.49-19.25a1.18 1.18 0 011.12-.93h26.74a1.19 1.19 0 011.13.93l5 23.18c1.12 5-.75 6.17-7.1 6.73a.88.88 0 00-.93.93v2.06a.88.88 0 00.93.93h37.62a.88.88 0 00.94-.93V131a.88.88 0 00-.94-.93c-5.79-.56-8.22-1.5-9.34-6.73L49.34.57c-.19-.56-.75-.75-1.12-.38\"></path></svg></a><a href=\"/\" class=\"Nav_navLink__34Bol\" tabindex=\"-1\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 214 33.24\" class=\"Nav_logo__RLN3C\"><path d=\"M39.39 13.2c-2.4 0-4.43 1.82-7 5.32-1.18 1.64-2.7 4-3.37 5-.38.51-.68.43-.47-.12l1.78-4.56 6.36-17.5C37 .62 34.46-.25 34 .62v.09c-1.09 2.32-3.12 3.08-6.75 2.95S16.32 1.52 10.8 1.52C3.88 1.52 0 5.78 0 10.8c0 2.82 1.85 4.64 4.34 4.55a2.27 2.27 0 002.41-1.81 1.2 1.2 0 00-1.56-1.43c-2.45.51-3.29-1.18-3.29-2.49 0-3.12 2.66-5.44 8.22-5.44 1.43 0 4.22.34 7.17.67-3.75 11.3-7.55 21.77-8.48 24.25a2.07 2.07 0 01-1.35 1.44c-1.34.42-1.77.46-2.61.67-1.27.3-1.06 1.35-.17 1.31 1.6-.09 3.67-.3 5.31-.3 2 0 5.61.17 6.16.21 1 .09 1.14-1.14.3-1.26-.59-.09-1.56-.25-2.49-.38-1.1-.13-1.43-.59-1.18-1.48.55-1.47 7-20.11 8.27-24 2 .25 3.71.42 4.84.5 2.33.14 4.57 0 6.5-1.41l-5.1 14.4-4.47 12.33c-.76 2 2.1 2.15 2.74.93a81.64 81.64 0 017.63-12.36c1.86-2.7 3.59-4.31 4.81-4.31.93 0 1.47.51 1.47 1.65 0 1.52-.71 3.88-2.15 7.89-1.89 5.23-2.61 6.62-3.24 6.66s-1.86-1.52-2.53-1.69a1.39 1.39 0 00-1.65.72c-.34.59-.12 2.49 2.74 2.62 3.42.16 6.33-3.34 8.35-8.94 1.39-3.8 1.73-5.74 1.73-7.13 0-2.7-1.26-3.97-3.33-3.97zm57.9 18.09c-2.15-.5-3-1.3-3-2.15l.09-1.77c0-1.3 1-20.49 1.22-23.36.17-2.11-2.24-2-3.25-.76l-2 2.57C87.89 8.9 78 21.68 73.17 27.67A11.5 11.5 0 0168 31.25c-.8.21-.72 1.06.17 1.06.71 0 2.82-.25 4.38-.25s4.43.12 5.15.16 1-.76 0-1c-2-.59-3-1-3-1.52s.46-1.22 1.56-2.74c.84-1.18 2.86-3.84 5.1-6.79H91c-.21 4.05-.46 8.31-.5 9.15a1.14 1.14 0 01-.93 1.14l-2.15.63c-.59.17-.85 1.27.08 1.23 2.11-.13 3.88-.3 4.81-.3 1.39 0 3.75.3 4.85.34s.94-.85.13-1.07zm-6-15.47c0 .76-.09 1.6-.13 2.49h-8.25c3.84-5.07 7.89-10.38 8.23-10.89s.67-.25.63.13c-.13 1.86-.34 5.27-.5 8.27zM55.08 13.5c-3.67-.17-7.76 4.09-9.7 10.5-1.9 6.2.21 8.77 2.53 8.77 1.69 0 4.51-2.19 6.07-4.72.55-.89.13-1.82-.71-.89-1.06 1.14-2.49 2.36-4 2.07-1-.17-1.94-1.9-.46-6.2 3.54-.17 7.71-2.19 8.77-5.53.92-2.88-.98-3.96-2.5-4zm.38 3.12c-.89 2.61-4 4.8-6.24 5.35 2.15-5.69 4.21-7.21 5.52-7.17.72 0 1.06.82.72 1.82zm53.94-1h3.42c.76 0 1.14-.21 1.3-.76.3-.72.13-1.06-.63-1.06h-3.42l1.65-4.38c.21-.59 0-.89-.34-.89h-1.77c-.46 0-.63.13-.84.72L107 13.83h-2.7c-.38 0-.68-.08-.93 1.06-.09.55 0 .76.55.76h2.4l-4.09 10.67a45.64 45.64 0 00-1.69 4.68c-.25 1.48.64 2.07 1.73 1.86 2.15-.38 5.23-2.62 8-6.12.76-1 .13-1.64-.63-.84a34.4 34.4 0 01-3.67 3.16c-1.1.72-1.64.21-1.26-.84zm38.04-1c.25-.59-.59-.76-2.53-1.13-.59-.13-.93.5-1.94 2.61a4 4 0 00-3.5-1.43c-3.08.08-6.88 2.86-10 7.12-2.49 3.42-3.25 5.78-2.78 8.31a3.11 3.11 0 003.08 2.52c2.32.08 4.64-2.11 7-4.52l1.48-1.48c.42-.42.8-.17.54.34a23.77 23.77 0 00-1.89 4.17c-.3 1.48.5 1.86 1.64 1.69 2.19-.29 5.61-2.32 8.23-5.94.75-1.06 0-1.65-.68-.85a26.62 26.62 0 01-3.75 3.29c-1.14.76-1.65.55-1.31-.34.17-.55.8-2.07 6.41-14.36zm-9.74 11c-2.66 2.45-4.81 4.22-6.29 4.22-1 0-1.47-.43-1.56-1.14-.25-1.86 1.44-4.68 2.83-6.58 2.4-3.16 5.36-5.78 7.25-5.78 1.31 0 2 .55 2.11 1.35.25 1.46-2.19 5.93-4.34 7.87zm58.58-12.71c-2.07 0-4.85 1.47-7.47 4.38-.84 1-.21 1.65.59.93a20.28 20.28 0 012.87-2.19c.89-.55 1.48-.13 1.22.67-.21.64-.84 2.2-1.3 3.25l-2.87 6.58c-.8 1.86-1.56 3.71-1.73 4.22-.38 1.31.29 2 1.35 2 2 0 4.77-1.65 7.42-4.6.84-1 .21-1.64-.59-.93a28 28 0 01-3.12 2.4c-.88.54-1.52.12-1.22-.68.21-.59.84-1.94 1.26-2.91l2.91-6.58c.85-1.94 1.82-4 2-4.55.32-1.28-.31-1.99-1.32-1.99zm2.53-10.21a2.39 2.39 0 00-2.53 2.15 2.09 2.09 0 001.85 2.45 2.43 2.43 0 002.49-2.11 2.08 2.08 0 00-1.81-2.49zm11.68 10.84c-3.71-.17-7.68 4.13-9.62 10.46s.17 8.77 2.62 8.77c1.69 0 4.51-2 6.2-4.72.55-.89.13-1.82-.72-.89-1.05 1.14-2.4 2.19-3.79 2.11s-2.37-2-.76-6.62c2.36-6.66 4-7.42 4.72-7.47s2.19 1.86 3.08 2.07a1.36 1.36 0 001.64-.71c.38-.72.14-2.83-3.37-3zm-23.95.29h-3.42l1.64-4.38c.22-.59 0-.89-.33-.89h-1.78c-.46 0-.63.13-.84.72l-1.73 4.55h-2.74c-.38 0-.67-.08-.93 1.06-.08.55.05.76.55.76h2.41l-4.09 10.67a43.34 43.34 0 00-1.69 4.68c-.26 1.48.63 2.07 1.73 1.86 2.15-.38 5.23-2.62 8-6.12.76-1 .13-1.64-.64-.84a34.33 34.33 0 01-3.68 3.16c-1.1.72-1.65.21-1.27-.84l4.71-12.6h3.42c.76 0 1.14-.21 1.31-.76.29-.69.12-1.03-.63-1.03zm-63.6 11.9a40.24 40.24 0 01-3.8 3.33c-1.1.72-1.64.21-1.26-.84L128.17.97c.17-.42.08-1-.51-1a58.6 58.6 0 00-6 .68c-.34.08-.51.59-.13.71l2.28.85c.46.17.51.38.21 1.14l-8.84 22.97a43.34 43.34 0 00-1.69 4.68c-.25 1.48.63 2.07 1.73 1.86 2.28-.38 5.52-2.62 8.35-6.29.76-1.01.13-1.64-.63-.84zm43.39.08l2.49-5c1.35-2.74 1.61-4 1.61-4.93 0-1.81-.93-2.74-2.83-2.74-2.66 0-4.77 2.11-8 6.66-.89 1.22-1.48 2.15-2.66 3.67-.38.47-.67.42-.46-.13l1.64-4.09a17 17 0 001.35-4.47c0-1.09-.46-1.6-1.64-1.6-1.44 0-3.25.76-6.41 4.77-1.14 1.47 0 1.64.38 1.18.63-.64 2-2.07 2.91-2.87s1.72-.55 1.39.42a18.36 18.36 0 01-.8 2.24l-5 12.27c-.68 1.69 2.53 2.07 3.08.8 1.73-4 5.52-9 7.08-11.22 2.28-3.25 4.26-5.48 5.69-5.48.76 0 1.1.42 1.1 1.22a5.83 5.83 0 01-.59 2l-3.54 7.55c-1 2-1.94 4.13-2.11 4.68-.42 1.31.3 2 1.43 2 2 0 4.77-1.65 7.43-4.6.84-1 .21-1.64-.59-.93a29 29 0 01-3.12 2.41c-.92.59-1.56.12-1.26-.72.21-.6.97-2.16 1.43-3.09z\"></path></svg></a></div><div class=\"Nav_rightContainer__CBCcP\"><ul class=\"NavAccountLinks_root__8VKLM\" data-event-element=\"account links\"><li class=\"NavAccountLinks_navListItem__Lxooj\"><a href=\"https://accounts.theatlantic.com/login/\" class=\"NavAccountLinks_navLink__ctd7M NavAccountLinks_hideOnMobile__Eokx4\">Sign In</a></li><li class=\"NavAccountLinks_navListItem__Lxooj\"><a href=\"https://www.theatlantic.com/subscribe/navbar/\" class=\"NavAccountLinks_subscribe__2DNuJ\">Subscribe</a></li></ul></div></div></div></nav><div class=\"Nav_fixedPosPlaceholder__0nyHE\"></div><div class=\"Nav_overlay__zlKnQ\" data-testid=\"overlay\"></div><div></div><main id=\"main-content\" data-event-surface=\"article\" data-flatplan-layout=\"standard\" class=\"\"><aside class=\"ArticleBentoRecirc_root___wAIa\" style=\"background-color:#fbfafa;color:#000000\" aria-labelledby=\"more-in-series-heading\" data-event-surface=\"article\" data-event-module=\"project bar\"><div class=\"ArticleBentoRecirc_intro__MlV2j\"><div class=\"ArticleBentoRecirc_headingContainer__mY2V3\"><h2 class=\"ArticleBentoRecirc_desktopHeading__YZv_N\" id=\"more-in-series-heading\"><a data-event-element=\"more from\" href=\"https://www.theatlantic.com/projects/artificial-intelligence/\">More From <!-- -->Artificial Intelligence</a></h2><h2 class=\"ArticleBentoRecirc_mobileHeading__RzzKy\">More From <!-- -->Artificial Intelligence</h2><span class=\"ArticleBentoRecirc_sponsor__zhHJ1\"><gpt-ad class=\"GptAd_root__pAvcS Logo_root__GuE0u\" format=\"logo\" sizes-at-0=\"logo\" data-logo-format=\"text\"></gpt-ad></span></div><a href=\"https://www.theatlantic.com/projects/artificial-intelligence/\" class=\"ArticleBentoRecirc_button__F1Fj9\">Explore This Series</a></div><ul class=\"ArticleBentoRecirc_articles__qE_Gu\"><li class=\"ArticleBentoRecirc_article__drQUh\"><a href=\"https://www.theatlantic.com/technology/archive/2025/05/sycophantic-ai/682743/\" class=\"ArticleBentoRecirc_articleLink__LBkjA\"><figure class=\"ArticleBentoRecirc_articleFigure__g9vgl\" data-event-element=\"image\" data-event-position=\"1\"><picture class=\"ArticleBentoRecirc_articlePicture__wR05Z\"><source media=\"(prefers-reduced-motion)\" srcSet=\"https://cdn.theatlantic.com/thumbor/HNV-dDyEh5L3ibtliKNUhv-ATUU=/438x0:1563x1125/80x80/filters:still()/media/img/mt/2025/05/2025_5_6_Sycophantic_Web/original.gif, https://cdn.theatlantic.com/thumbor/Am-fJ-RvidfuMn63352ehXsvEco=/438x0:1563x1125/160x160/filters:still()/media/img/mt/2025/05/2025_5_6_Sycophantic_Web/original.gif 2x\"/><img alt=\"Animation of a person&#x27;s silhouette with computer code running through it\" loading=\"lazy\" class=\"Image_root__XxsOp Image_lazy__hYWHV ArticleBentoRecirc_articleImage__M6dMP\" srcSet=\"https://cdn.theatlantic.com/thumbor/A_jIj3CtlzuNNTfC0ukcQ3jH8-4=/438x0:1563x1125/80x80/media/img/mt/2025/05/2025_5_6_Sycophantic_Web/original.gif, https://cdn.theatlantic.com/thumbor/c0PBq-hUdATZlMlDK7Iu_GgOP5U=/438x0:1563x1125/160x160/media/img/mt/2025/05/2025_5_6_Sycophantic_Web/original.gif 2x\" src=\"https://cdn.theatlantic.com/thumbor/A_jIj3CtlzuNNTfC0ukcQ3jH8-4=/438x0:1563x1125/80x80/media/img/mt/2025/05/2025_5_6_Sycophantic_Web/original.gif\" width=\"80\" height=\"80\"/></picture></figure><h3 class=\"ArticleBentoRecirc_articleTitle__8fFYc\" data-event-element=\"title\" data-event-position=\"1\">AI Is Not Your Friend</h3></a><a class=\"ArticleBentoRecirc_articleAuthor__kirBf\" href=\"https://www.theatlantic.com/author/mike-caulfield/\" data-event-element=\"author\" data-event-position=\"1\">Mike Caulfield</a></li><li class=\"ArticleBentoRecirc_article__drQUh\"><a href=\"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/\" class=\"ArticleBentoRecirc_articleLink__LBkjA\"><figure class=\"ArticleBentoRecirc_articleFigure__g9vgl\" data-event-element=\"image\" data-event-position=\"2\"><picture class=\"ArticleBentoRecirc_articlePicture__wR05Z\"><img alt=\"A blurry, distorted, pixellated image of the Reddit robot&#x27;s head against an orange background\" loading=\"lazy\" class=\"Image_root__XxsOp Image_lazy__hYWHV ArticleBentoRecirc_articleImage__M6dMP\" srcSet=\"https://cdn.theatlantic.com/thumbor/6cNDGHwip7boYwmIfmMjS2t4Ibw=/485x0:1610x1125/80x80/media/img/mt/2025/05/reddit_1/original.jpg, https://cdn.theatlantic.com/thumbor/2Ous5HbbV7YikHYyfQsVmlornu4=/485x0:1610x1125/160x160/media/img/mt/2025/05/reddit_1/original.jpg 2x\" src=\"https://cdn.theatlantic.com/thumbor/6cNDGHwip7boYwmIfmMjS2t4Ibw=/485x0:1610x1125/80x80/media/img/mt/2025/05/reddit_1/original.jpg\" width=\"80\" height=\"80\"/></picture></figure><h3 class=\"ArticleBentoRecirc_articleTitle__8fFYc\" data-event-element=\"title\" data-event-position=\"2\">‘The Worst Internet-Research Ethics Violation I Have Ever Seen’</h3></a><a class=\"ArticleBentoRecirc_articleAuthor__kirBf\" href=\"https://www.theatlantic.com/author/tom-bartlett/\" data-event-element=\"author\" data-event-position=\"2\">Tom Bartlett</a></li><li class=\"ArticleBentoRecirc_article__drQUh\"><a href=\"https://www.theatlantic.com/technology/archive/2025/04/great-language-flattening/682627/\" class=\"ArticleBentoRecirc_articleLink__LBkjA\"><figure class=\"ArticleBentoRecirc_articleFigure__g9vgl\" data-event-element=\"image\" data-event-position=\"3\"><picture class=\"ArticleBentoRecirc_articlePicture__wR05Z\"><source media=\"(prefers-reduced-motion)\" srcSet=\"https://cdn.theatlantic.com/thumbor/cXntWE6nm-Wu2tPP0PubMz27vAE=/438x0:1563x1125/80x80/filters:still()/media/img/mt/2025/04/Cloud/original.gif, https://cdn.theatlantic.com/thumbor/lfbgGEDwbJ91Qhh0Y-5w918xJtM=/438x0:1563x1125/160x160/filters:still()/media/img/mt/2025/04/Cloud/original.gif 2x\"/><img alt=\"Animation of letters floating around in a black ooze\" loading=\"lazy\" class=\"Image_root__XxsOp Image_lazy__hYWHV ArticleBentoRecirc_articleImage__M6dMP\" srcSet=\"https://cdn.theatlantic.com/thumbor/H4pHGoPg5IDFy0ulLAwUJA4U7ys=/438x0:1563x1125/80x80/media/img/mt/2025/04/Cloud/original.gif, https://cdn.theatlantic.com/thumbor/5l0Oa1dafGvt0ITmG7Fdmaq65kc=/438x0:1563x1125/160x160/media/img/mt/2025/04/Cloud/original.gif 2x\" src=\"https://cdn.theatlantic.com/thumbor/H4pHGoPg5IDFy0ulLAwUJA4U7ys=/438x0:1563x1125/80x80/media/img/mt/2025/04/Cloud/original.gif\" width=\"80\" height=\"80\"/></picture></figure><h3 class=\"ArticleBentoRecirc_articleTitle__8fFYc\" data-event-element=\"title\" data-event-position=\"3\">The Great Language Flattening</h3></a><a class=\"ArticleBentoRecirc_articleAuthor__kirBf\" href=\"https://www.theatlantic.com/author/victoria-turk/\" data-event-element=\"author\" data-event-position=\"3\">Victoria Turk</a></li><li class=\"ArticleBentoRecirc_article__drQUh ArticleBentoRecirc_hideAtBreak__gKcJW\"><a href=\"https://www.theatlantic.com/technology/archive/2025/04/how-ai-will-actually-contribute-cancer-cure/682607/\" class=\"ArticleBentoRecirc_articleLink__LBkjA\"><figure class=\"ArticleBentoRecirc_articleFigure__g9vgl\" data-event-element=\"image\" data-event-position=\"4\"><picture class=\"ArticleBentoRecirc_articlePicture__wR05Z\"><img alt=\"Collage of medical imagery\" loading=\"lazy\" class=\"Image_root__XxsOp Image_lazy__hYWHV ArticleBentoRecirc_articleImage__M6dMP\" srcSet=\"https://cdn.theatlantic.com/thumbor/BNCUlAwitYq1H_NFztmEgnS0AZk=/438x0:1563x1125/80x80/media/img/mt/2025/04/cancer/original.jpg, https://cdn.theatlantic.com/thumbor/EiZD7h5mQkxkwhhmT1p1dd_jwAk=/438x0:1563x1125/160x160/media/img/mt/2025/04/cancer/original.jpg 2x\" src=\"https://cdn.theatlantic.com/thumbor/BNCUlAwitYq1H_NFztmEgnS0AZk=/438x0:1563x1125/80x80/media/img/mt/2025/04/cancer/original.jpg\" width=\"80\" height=\"80\"/></picture></figure><h3 class=\"ArticleBentoRecirc_articleTitle__8fFYc\" data-event-element=\"title\" data-event-position=\"4\">AI Executives Promise Cancer Cures. Here’s the Reality</h3></a><a class=\"ArticleBentoRecirc_articleAuthor__kirBf\" href=\"https://www.theatlantic.com/author/matteo-wong/\" data-event-element=\"author\" data-event-position=\"4\">Matteo Wong</a></li></ul></aside><article class=\"ArticleLayout_article__RHFMN article-content-body\"><header class=\"ArticleHero_root__3w7kV ArticleHero_articleStandard__2tcdv\" data-event-module=\"hero\"><div class=\"\"><div class=\"ArticleHero_defaultArticleLockup__vb8lz\"><div class=\"ArticleHero_rubric__e4rjD\"><div class=\"ArticleRubric_root__HNhbf\" id=\"rubric\" data-flatplan-rubric=\"true\"><a class=\"ArticleRubric_link__nl9hy\" href=\"https://www.theatlantic.com/technology/\" data-event-element=\"rubric\">Technology</a></div></div><div class=\"ArticleHero_title__PQ4pC\"><h1 class=\"ArticleTitle_root__VrZaG\" data-flatplan-title=\"true\">‘The Worst Internet-Research Ethics Violation I Have Ever Seen’</h1></div><div class=\"ArticleHero_dek__EqdkK\" data-flatplan-description=\"true\"><p class=\"ArticleDek_root__P3leE\">The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.</p></div><div class=\"ArticleHero_byline__iFT6A\"><div class=\"ArticleBylines_root__IBR5V\"><address id=\"byline\">By <a class=\"ArticleBylines_link__kNP4C\" href=\"https://www.theatlantic.com/author/tom-bartlett/\" data-event-element=\"author\" data-flatplan-author-link=\"true\">Tom Bartlett</a></address></div></div></div><div class=\"ArticleLeadArt_root__nRSLU\"><figure class=\"ArticleLeadFigure_root__Bj81R ArticleLeadFigure_standard__20Izv\"><div class=\"ArticleLeadFigure_media__R1npW\" data-flatplan-lead_figure_media=\"true\"><picture><img alt=\"A blurry, distorted, pixellated image of the Reddit robot&#x27;s head against an orange background\" class=\"Image_root__XxsOp ArticleLeadArt_image__HZS4B\" sizes=\"(min-width: 976px) 976px, 100vw\" srcSet=\"https://cdn.theatlantic.com/thumbor/f_du4irHtRNkjeNQQwkc67dBWBo=/0x0:2000x1125/750x422/media/img/mt/2025/05/reddit_1/original.jpg 750w, https://cdn.theatlantic.com/thumbor/e4jEibjQkC_AtPxf6WB7O-CZ8wU=/0x0:2000x1125/828x466/media/img/mt/2025/05/reddit_1/original.jpg 828w, https://cdn.theatlantic.com/thumbor/_RS0mJORhUjk2oKFp1-lrHWZJ0Y=/0x0:2000x1125/960x540/media/img/mt/2025/05/reddit_1/original.jpg 960w, https://cdn.theatlantic.com/thumbor/K-HTgOf-8ecmTJUtIp-1XUeWFpE=/0x0:2000x1125/976x549/media/img/mt/2025/05/reddit_1/original.jpg 976w, https://cdn.theatlantic.com/thumbor/pP0fvpOUnZzkl7oOPHRWZ5NFXf8=/0x0:2000x1125/1952x1098/media/img/mt/2025/05/reddit_1/original.jpg 1952w\" src=\"https://cdn.theatlantic.com/thumbor/_RS0mJORhUjk2oKFp1-lrHWZJ0Y=/0x0:2000x1125/960x540/media/img/mt/2025/05/reddit_1/original.jpg\" id=\"article-lead-image\" width=\"960\" height=\"540\"/></picture></div><figcaption class=\"ArticleLeadFigure_caption__Byu7W ArticleLeadFigure_standardCaption__PsDkd\" data-flatplan-lead_figure_caption=\"true\">Illustration by The Atlantic</figcaption></figure></div></div><div class=\"ArticleHero_articleUtilityBar__JbQFj\"><div class=\"ArticleHero_timestamp__bKhcB\"><time class=\"ArticleTimestamp_root__b3bL6\" dateTime=\"2025-05-02T17:55:38Z\" data-flatplan-timestamp=\"true\">May 2, 2025</time> </div><div class=\"ArticleHero_articleUtilityBarTools__ZHw8s\"><div class=\"ArticleShare_root__Mq0RB\" tabindex=\"-1\"><button class=\"ArticleShare_shareButton__X0cIe\" aria-haspopup=\"true\" aria-controls=\":Rchclhhim:\" aria-expanded=\"false\" aria-label=\"Open Share Menu\" data-action=\"click share - expand\" data-event-verb=\"shared\" data-event-element=\"share dropdown\">Share<svg width=\"15\" height=\"15\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" class=\"ArticleShare_buttonIcon__B86vV\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M7.335.272a.25.25 0 01.337 0l4.623 4.204a.25.25 0 01.017.353l-.336.37a.25.25 0 01-.353.016L8.004 1.926v7.9a.25.25 0 01-.25.25h-.5a.25.25 0 01-.25-.25V1.924l-3.62 3.291a.25.25 0 01-.353-.016l-.336-.37a.25.25 0 01.016-.353L7.335.272zM.5 7.545a.25.25 0 00-.25.25v6.75c0 .138.112.25.25.25h14a.25.25 0 00.25-.25v-6.75a.25.25 0 00-.25-.25H14a.25.25 0 00-.25.25v6H1.25v-6a.25.25 0 00-.25-.25H.5z\" fill=\"currentColor\"></path></svg></button></div><button class=\"SaveButton_saveButton__7LYFZ\" aria-label=\"Save\"><span class=\"SaveButton_text__fiZgx\">Save<!-- --> </span><svg width=\"12\" height=\"16\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" class=\"SaveButton_icon__HFNiD SaveButton_unsaved__bP4MN\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M6 10.828l5 3.31V1H1v13.139l5-3.31zM.776 15.486A.5.5 0 010 15.07V.5A.5.5 0 01.5 0h11a.5.5 0 01.5.5v14.57a.5.5 0 01-.776.416L6.138 12.12a.25.25 0 00-.276 0L.776 15.486z\" fill=\"currentColor\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M5.572 8.75c0 .138.111.25.25.25h.5a.25.25 0 00.25-.25V6.57H8.75A.25.25 0 009 6.32v-.5a.25.25 0 00-.25-.25H6.572V3.25a.25.25 0 00-.25-.25h-.5a.25.25 0 00-.25.25v2.32H3.25a.25.25 0 00-.25.25v.5c0 .138.112.25.25.25h2.322v2.18z\" fill=\"currentColor\"></path></svg></button></div></div><gpt-ad class=\"GptAd_root__pAvcS ArticleInjector_root__I7x9v LeadArticleAd_root__tdCqm s-native s-native--standard s-native--streamline\" format=\"injector\" sizes-at-0=\"mobile-wide\" targeting-pos=\"injector-article-start\" sizes-at-976=\"desktop-wide\"></gpt-ad><div class=\"ArticleInjector_clsAvoider__dqIAm\" style=\"--placeholderHeight:298px\"></div></header><div class=\"ArticleAudio_root__4Qcq3\" data-view-action=\"view - audio player - start\" data-view-label=\"682676\" data-event-module=\"audio player\" data-event-content-type=\"narrated\" data-event-module-state=\"start\" data-event-view=\"true\"><div class=\"ArticleAudio_container__b5Yj2\"><div><div class=\"ArticleAudio_imgContainer__qDu_f\"><img alt=\"A blurry, distorted, pixellated image of the Reddit robot&#x27;s head against an orange background\" class=\"Image_root__XxsOp ArticleAudio_img__BFda3\" sizes=\"80px\" srcSet=\"https://cdn.theatlantic.com/thumbor/6cNDGHwip7boYwmIfmMjS2t4Ibw=/485x0:1610x1125/80x80/media/img/mt/2025/05/reddit_1/original.jpg 80w, https://cdn.theatlantic.com/thumbor/GjtUuBbkJAI8CXGVztWsqNh6kng=/485x0:1610x1125/96x96/media/img/mt/2025/05/reddit_1/original.jpg 96w, https://cdn.theatlantic.com/thumbor/ReRHYhSvI9bJUtGoeELKSeztV6k=/485x0:1610x1125/128x128/media/img/mt/2025/05/reddit_1/original.jpg 128w, https://cdn.theatlantic.com/thumbor/2Ous5HbbV7YikHYyfQsVmlornu4=/485x0:1610x1125/160x160/media/img/mt/2025/05/reddit_1/original.jpg 160w, https://cdn.theatlantic.com/thumbor/Q5d4GGENqCxtH2cim21k0vrtzaE=/485x0:1610x1125/192x192/media/img/mt/2025/05/reddit_1/original.jpg 192w, https://cdn.theatlantic.com/thumbor/-iCsgfmsFTXpiVIwKjvGTjAyieM=/485x0:1610x1125/256x256/media/img/mt/2025/05/reddit_1/original.jpg 256w, https://cdn.theatlantic.com/thumbor/vyVWvalnMyc_79tfWpF-E4bLjUo=/485x0:1610x1125/384x384/media/img/mt/2025/05/reddit_1/original.jpg 384w, https://cdn.theatlantic.com/thumbor/XY5BHma87Kg5Yu0KvhLU_1GOke4=/485x0:1610x1125/512x512/media/img/mt/2025/05/reddit_1/original.jpg 512w\" src=\"https://cdn.theatlantic.com/thumbor/6cNDGHwip7boYwmIfmMjS2t4Ibw=/485x0:1610x1125/80x80/media/img/mt/2025/05/reddit_1/original.jpg\" width=\"80\" height=\"80\"/></div></div><div><div class=\"ArticleAudio_text__DsxgL\"><span>Listen</span><span class=\"ArticleAudio_speed__YBmbh\"><button class=\"ArticleAudio_speedBtn__tipaO\">-</button><span>1.0<!-- -->x</span><button class=\"ArticleAudio_speedBtn__tipaO\">+</button></span></div><div class=\"ArticleAudio_player__hOjo_\"><div class=\"ArticleAudio_progressBarContainer__IbGRE\"><input data-event-verb=\"scrubbed\" class=\"ArticleAudio_slider__AnzMp\" data-event-element=\"slider\" type=\"range\" max=\"620.72\" style=\"--player-elapsed:0;--player-duration:620.72\" role=\"progressbar\" value=\"0\"/></div><div class=\"ArticleAudio_timeContainer__8S55D\" aria-hidden=\"true\"><span class=\"ArticleAudio_time__TIPIP\">0:00</span><span class=\"ArticleAudio_time__TIPIP ArticleAudio_duration__qdjYp\">10:20</span></div><div class=\"ArticleAudio_buttonContainer__c3yS8\"><button id=\"rewind\" class=\"ArticleAudio_button__Xvn0e\" disabled=\"\" aria-label=\"rewind 15 seconds\" click-event-element=\"skip backwards\" data-event-verb=\"rewound\"><svg width=\"40\" height=\"40\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" class=\"ArticleAudio_rewindFwd__7N__7\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M10.344 5.314l-3.563 7.83 8.563.83-2.126-3.682a11.945 11.945 0 016-1.606c6.628 0 12 5.373 12 12 0 6.628-5.372 12-12 12A11.995 11.995 0 018.82 26.682l-1.732 1c2.42 4.188 6.946 7.006 12.13 7.006 7.732 0 14-6.268 14-14 0-7.733-6.268-14-14-14-2.55 0-4.941.681-7 1.873l-1.874-3.246z\" fill=\"currentColor\"></path><path d=\"M11.842 23.607h2.368v-4.615l-1.783.997-.691-1.263 2.686-1.49h1.503v6.371h2.368v1.33h-6.45v-1.33zm13.857-1.25c0 1.57-1.263 2.7-3.165 2.713-1.064.013-1.982-.186-3.312-.758l.599-1.277c1.117.492 1.769.665 2.593.665.878 0 1.61-.425 1.61-1.237 0-.678-.559-1.13-1.437-1.13-.518 0-1.17.173-1.822.505l-1.117-.439.28-4.163h5.266v1.33h-4.017l-.106 1.902a3.645 3.645 0 011.822-.465c1.69 0 2.806.997 2.806 2.354z\" fill=\"currentColor\"></path></svg></button><button class=\"ArticleAudio_playButton__JvBX0\" aria-label=\"play\" data-event-element=\"play pause button\" data-event-verb=\"played\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 12 12\" class=\"ArticleAudio_play__bBFV4\" aria-hidden=\"true\"><path fill=\"currentColor\" d=\"M3 12V0l9 6-9 6z\"></path></svg></button><button id=\"fast-forward\" class=\"ArticleAudio_button__Xvn0e\" disabled=\"\" aria-label=\"fast forward 15 seconds\" click-event-element=\"skip forward\" data-event-verb=\"skipped\"><svg width=\"40\" height=\"40\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" class=\"ArticleAudio_rewindFwd__7N__7\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M29.656 5.314l3.562 7.83-8.562.83 2.126-3.682a11.944 11.944 0 00-6-1.606c-6.628 0-12 5.373-12 12 0 6.628 5.372 12 12 12 4.443 0 8.323-2.415 10.397-6.005l1.733 1c-2.42 4.188-6.946 7.006-12.13 7.006-7.733 0-14-6.268-14-14 0-7.733 6.267-14 14-14 2.55 0 4.94.681 7 1.873l1.874-3.246z\" fill=\"currentColor\"></path><path d=\"M14.28 23.607h2.367v-4.615l-1.782.997-.692-1.263 2.687-1.49h1.503v6.371h2.367v1.33h-6.45v-1.33zm13.857-1.25c0 1.57-1.264 2.7-3.166 2.713-1.064.013-1.981-.186-3.311-.758l.598-1.277c1.117.492 1.77.665 2.594.665.878 0 1.61-.425 1.61-1.237 0-.678-.56-1.13-1.437-1.13-.519 0-1.17.173-1.822.505l-1.118-.439.28-4.163h5.267v1.33h-4.017l-.106 1.902a3.645 3.645 0 011.822-.465c1.689 0 2.806.997 2.806 2.354z\" fill=\"currentColor\"></path></svg></button></div></div></div></div><p class=\"ArticleAudio_promo__4zkGZ\">Produced by ElevenLabs and<!-- --> <a class=\"ArticleAudio_link__bjoip\" href=\"https://newsoveraudio.com/?offerId=atl_reader_exclusive_jks1kjl\"> <!-- -->News Over Audio (Noa)</a> <!-- -->using AI narration. Listen to more stories on the Noa app.</p></div><section class=\"ArticleBody_root__2gF81\" data-event-module=\"article body\" data-flatplan-body=\"true\"><p class=\"ArticleParagraph_root__4mszW\" data-flatplan-paragraph=\"true\">When Reddit rebranded itself as “the heart of the internet” a couple of years ago, the slogan was meant to evoke the site’s organic character. In an age of social media dominated by algorithms, Reddit took pride in being curated by a community that expressed its feelings in the form of upvotes and downvotes—in other words, being shaped by actual people.</p><p class=\"ArticleParagraph_root__4mszW\" data-flatplan-paragraph=\"true\">So earlier this week, when members of a popular subreddit learned that their community had been infiltrated by undercover researchers posting AI-written comments and passing them off as human thoughts, the Redditors were predictably incensed. They called the experiment “violating,” “shameful,” “infuriating,” and “very disturbing.” As the backlash intensified, the researchers went silent, refusing to reveal their identity or answer questions about their methodology. The university that employs them has announced that it’s investigating. Meanwhile, Reddit’s chief legal officer, Ben Lee, <a data-event-element=\"inline link\" href=\"https://www.reddit.com/r/changemyview/comments/1k8b2hj/comment/mpk1u3c/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button\">wrote</a> that the company intends to “ensure that the researchers are held accountable for their misdeeds.”</p><p class=\"ArticleParagraph_root__4mszW\" data-flatplan-paragraph=\"true\">Joining the chorus of disapproval were fellow internet researchers, who condemned what they saw as a plainly unethical experiment. Amy Bruckman, a professor at the Georgia Institute of Technology who has studied online communities for more than two decades, told me the Reddit fiasco is “the worst internet-research ethics violation I have ever seen, no contest.” What’s more, she and others worry that the uproar could undermine the work of scholars who are using more conventional methods to study a crucial problem: how AI influences the way humans think and relate to one another.</p><p class=\"ArticleParagraph_root__4mszW\" data-flatplan-paragraph=\"true\">The researchers, based at the University of Zurich, wanted to find out whether AI-generated responses could change people’s views. So they headed to the aptly named subreddit <a data-event-element=\"inline link\" href=\"https://www.reddit.com/r/changemyview/\">r/changemyview</a>, in which users debate important societal issues, along with plenty of trivial topics, and award points to posts that talk them out of their original position. Over the course of four months, the researchers posted more than 1,000 AI-generated comments on pitbulls (is aggression the fault of the breed or the owner?), the housing crisis (is living with your parents the solution?), DEI programs (were they destined to fail?). The AI commenters argued that browsing Reddit is a waste of time and that the “controlled demolition” 9/11 conspiracy theory has some merit. And as they offered their computer-generated opinions, they also shared their backstories. One claimed to be a trauma counselor; another described himself as a victim of statutory rape.</p><p class=\"ArticleParagraph_root__4mszW\" data-flatplan-paragraph=\"true\">In one sense, the AI comments appear to have been rather effective. When researchers asked the AI to personalize its arguments to a Redditor’s biographical details, including gender, age, and political leanings (inferred, courtesy of another AI model, through the Redditor’s post history), a surprising number of minds indeed appear to have been changed. Those personalized AI arguments received, on average, far higher scores in the subreddit’s point system than nearly all human commenters, according to preliminary findings that the researchers shared with Reddit moderators and later made private. (This analysis, of course, assumes that no one else in the subreddit was using AI to hone their arguments.)</p><p id=\"injected-recirculation-link-0\" class=\"ArticleRelatedContentLink_root__VYc9V\" data-view-action=\"view link - injected link - item 1\" data-event-element=\"injected link\" data-event-position=\"1\"><a href=\"https://www.theatlantic.com/technology/archive/2025/04/arc-agi-chollet-test/682295/\">Read: The man out to prove how dumb AI still is</a></p><p class=\"ArticleParagraph_root__4mszW\" data-flatplan-paragraph=\"true\">The researchers had a tougher time convincing Redditors that their covert study was justified. After they had finished the experiment, they contacted the subreddit’s moderators, revealed their identity, and requested to “debrief” the subreddit—that is, to announce to members that for months, they had been unwitting subjects in a scientific experiment. “They were rather surprised that we had such a negative reaction to the experiment,” says one moderator, who asked to be identified by his username, LucidLeviathan, to protect his privacy. According to LucidLeviathan, the moderators requested that the researchers not publish such tainted work, and that they issue an apology. The researchers refused. After more than a month of back-and-forth, the moderators revealed what they had learned about the experiment (minus the researchers’ names) to the rest of the subreddit, making clear their disapproval.</p><p class=\"ArticleParagraph_root__4mszW\" data-flatplan-paragraph=\"true\">When the moderators sent a complaint to the University of Zurich, the university noted in its response that the “project yields important insights, and the risks (e.g. trauma etc.) are minimal,” according to an excerpt posted by moderators. In a statement to me, a university spokesperson said that the ethics board had received notice of the study last month, advised the researchers to comply with the subreddit’s rules, and “intends to adopt a stricter review process in the future.” Meanwhile, the researchers defended their approach in a Reddit comment, arguing that “none of the comments advocate for harmful positions” and that each AI-generated comment was reviewed by a human team member before being posted. (I sent an email to an anonymized address for the researchers, posted by Reddit moderators, and received a reply that directed my inquiries to the university.)</p><p class=\"ArticleParagraph_root__4mszW\" data-flatplan-paragraph=\"true\">Perhaps the most telling aspect of the Zurich researchers’ defense was that, as they saw it, deception was integral to the study. The University of Zurich’s ethics board—which can offer researchers advice but, according to the university, lacks the power to reject studies that fall short of its standards—told the researchers before they began posting that “the participants should be informed as much as possible,” according to the university statement I received. But the researchers seem to believe that doing so would have ruined the experiment. “To ethically test LLMs’ persuasive power in realistic scenarios, an unaware setting was necessary,” because it more realistically mimics how people would respond to unidentified bad actors in real-world settings, the researchers wrote in one of their Reddit comments.</p><p class=\"ArticleParagraph_root__4mszW\" data-flatplan-paragraph=\"true\">How humans are likely to respond in such a scenario is an urgent issue and a worthy subject of academic research. In their preliminary results, the researchers concluded that AI arguments can be “highly persuasive in real-world contexts, surpassing all previously known benchmarks of human persuasiveness.” (Because the researchers finally agreed this week not to publish a paper about the experiment, the accuracy of that verdict will probably never be fully assessed, which is its own sort of shame.) The prospect of having your mind changed by something that doesn’t have one is deeply unsettling. That persuasive superpower could also be employed for nefarious ends.</p><p id=\"injected-recirculation-link-1\" class=\"ArticleRelatedContentLink_root__VYc9V\" data-view-action=\"view link - injected link - item 2\" data-event-element=\"injected link\" data-event-position=\"2\"><a href=\"https://www.theatlantic.com/technology/archive/2025/03/chatbots-benchmark-tests/681929/\">Read: Chatbots are cheating on their benchmark tests</a></p><p class=\"ArticleParagraph_root__4mszW\" data-flatplan-paragraph=\"true\">Still, scientists don’t have to flout the norms of experimenting on human subjects in order to evaluate the threat. “The general finding that AI can be on the upper end of human persuasiveness—more persuasive than most humans—jibes with what laboratory experiments have found,” Christian Tarsney, a senior research fellow at the University of Texas at Austin, told me. In one <a data-event-element=\"inline link\" href=\"https://www.science.org/doi/10.1126/science.adq1814\">recent laboratory experiment</a>, participants who believed in conspiracy theories voluntarily chatted with an AI; after three exchanges, about a quarter of them lost faith in their previous beliefs. Another found that ChatGPT produced <a data-event-element=\"inline link\" href=\"https://www.science.org/doi/full/10.1126/sciadv.adh1850\">more persuasive disinformation</a> than humans, and that participants who were asked to distinguish between real posts and those written by AI could not effectively do so.</p><p class=\"ArticleParagraph_root__4mszW\" data-flatplan-paragraph=\"true\">Giovanni Spitale, the lead author of that study, also happens to be a scholar at the University of Zurich, and has been in touch with one of the researchers behind the Reddit AI experiment, who asked him not to reveal their identity. “We are receiving dozens of death threats,” the researcher wrote to him, in a message Spitale shared with me. “Please keep the secret for the safety of my family.”</p><p class=\"ArticleParagraph_root__4mszW\" data-flatplan-paragraph=\"true\">One likely reason the backlash has been so strong is because, on a platform as close-knit as Reddit, betrayal cuts deep. “One of the pillars of that community is mutual trust,” Spitale told me; it’s part of the reason he opposes experimenting on Redditors without their knowledge. Several scholars I spoke with about this latest ethical quandary compared it—unfavorably—to Facebook’s <a data-event-element=\"inline link\" href=\"https://www.theatlantic.com/technology/archive/2014/06/everything-we-know-about-facebooks-secret-mood-manipulation-experiment/373648/\">infamous emotional-contagion study</a>. For one week in 2012, Facebook altered users’ News Feed to see if viewing more or less positive content changed their posting habits. (It did, a little bit.) Casey Fiesler, an associate professor at the University of Colorado at Boulder who studies ethics and online communities, told me that the emotional-contagion study pales in comparison with what the Zurich researchers did. “People were upset about that but not in the way that this Reddit community is upset,” she told me. “This felt a lot more personal.”</p><p id=\"injected-recirculation-link-2\" class=\"ArticleRelatedContentLink_root__VYc9V\" data-view-action=\"view link - injected link - item 3\" data-event-element=\"injected link\" data-event-position=\"3\"><a href=\"https://www.theatlantic.com/technology/archive/2025/04/how-ai-will-actually-contribute-cancer-cure/682607/\">Read: AI executives promise cancer cures. Here’s the reality.</a></p><p class=\"ArticleParagraph_root__4mszW\" data-flatplan-paragraph=\"true\">The reaction probably also has to do with the unnerving notion that ChatGPT knows what buttons to push in our minds. It’s one thing to be fooled by some human Facebook researchers with dubious ethical standards, and another entirely to be duped by a cosplaying chatbot. I read through dozens of the AI comments, and although they weren’t all brilliant, most of them seemed reasonable and genuine enough. They made a lot of good points, and I found myself nodding along more than once. As the Zurich researchers warn, without more robust detection tools, AI bots might “seamlessly blend into online communities”—that is, assuming they haven’t already.</p><div class=\"ArticleBody_divider__GpNxD\" id=\"article-end\"></div></section><div data-event-module=\"footer\"><div class=\"ArticleWell_root__fueCa\"><div data-event-module=\"author footer\" class=\"ArticleFooter_authorFooter__5NsdY\"><div class=\"SectionHeading_root__3GnqT\"><h3 class=\"SectionHeading_heading__iNkek\">About the Author</h3></div><div class=\"ArticleBio_root__ua8zj\"><address id=\"article-writer-0\" class=\"ArticleBio_author__6pDyl\" data-event-element=\"author\" data-event-position=\"1\"><div class=\"ArticleBio_content__O0ZVF ArticleBio_noHeadshotContent__RrLmd\"><div class=\"ArticleBio_topContainer__QYRU4\"><div><div class=\"ArticleBio_bioNameMulti__gvg_b\"><a href=\"https://www.theatlantic.com/author/tom-bartlett/\" data-event-element=\"author name\">Tom Bartlett</a></div></div></div><div class=\"ArticleBio_bioSection__Hef4P\"><div class=\"ArticleBio_bioSection__Hef4P\" data-flatplan-bio=\"true\"><a href=\"https://www.theatlantic.com/author/tom-bartlett/\" class=\"author-link\" data-label=\"https://www.theatlantic.com/author/tom-bartlett/\" data-action=\"click author - name\"  >Tom Bartlett</a> is a writer in Austin, Texas.</div></div></div></address></div></div><div class=\"ArticleTags_root__zS_pT\"><div class=\"ArticleTags_tagTitle__WjjAt\">Explore More Topics</div><div><span class=\"ArticleTags_tagLink__SZysG\"><a href=\"https://www.theatlantic.com/tag/product/facebook/\">Facebook</a>, </span><span class=\"ArticleTags_tagLink__SZysG\"><a href=\"https://www.theatlantic.com/tag/organization/reddit/\">Reddit</a></span></div></div></div></div><gpt-ad class=\"GptAd_root__pAvcS ArticleInjector_root__I7x9v s-native s-native--standard s-native--streamline\" format=\"injector\" sizes-at-0=\"mobile-wide,native,house\" targeting-pos=\"injector-most-popular\" sizes-at-976=\"desktop-wide,native,house\"></gpt-ad><div class=\"ArticleInjector_clsAvoider__dqIAm\"></div></article><div></div></main><div></div><div></div></div></div><script id=\"__NEXT_DATA__\" type=\"application/json\">{\"props\":{\"isLoggedIn\":false,\"hasPaywallAccess\":false,\"hasAdFree\":false,\"pageProps\":{\"id\":\"BlogArticle:682676\",\"isTnfCompatible\":true,\"layout\":\"standard\",\"hasMeter\":true,\"url\":\"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/\",\"dateModified\":\"2025-05-03T14:07:06Z\",\"__typename\":\"BlogArticle\",\"notFound\":false,\"urqlState\":{\"1981928653\":{\"data\":\"{\\\"article\\\":{\\\"__typename\\\":\\\"BlogArticle\\\",\\\"id\\\":\\\"BlogArticle:682676\\\",\\\"authors\\\":[{\\\"url\\\":\\\"https://www.theatlantic.com/author/tom-bartlett/\\\",\\\"displayName\\\":\\\"Tom Bartlett\\\",\\\"__typename\\\":\\\"Author\\\",\\\"id\\\":\\\"Author:26844\\\",\\\"isFollowable\\\":false,\\\"biography\\\":{\\\"default\\\":\\\"\\u003ca href=\\\\\\\"https://www.theatlantic.com/author/tom-bartlett/\\\\\\\" class=\\\\\\\"author-link\\\\\\\" data-label=\\\\\\\"https://www.theatlantic.com/author/tom-bartlett/\\\\\\\" data-action=\\\\\\\"click author - name\\\\\\\"  \\u003eTom Bartlett\\u003c/a\\u003e is a writer in Austin, Texas.\\\",\\\"__typename\\\":\\\"Biography\\\"},\\\"headshot\\\":null,\\\"river\\\":{\\\"edges\\\":[{\\\"cursor\\\":\\\"MjAyNS0wNS0wMiAxMzo1NTozOHw2ODI2NzY=\\\",\\\"node\\\":{\\\"id\\\":\\\"BlogArticle:682676\\\",\\\"title\\\":\\\"‘The Worst Internet-Research Ethics Violation I Have Ever Seen’\\\",\\\"url\\\":\\\"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/\\\",\\\"__typename\\\":\\\"BlogArticle\\\"},\\\"__typename\\\":\\\"PromoEdge\\\"},{\\\"cursor\\\":\\\"MjAyNS0wNC0xMCAwOTozNzoyM3w2ODIzODY=\\\",\\\"node\\\":{\\\"id\\\":\\\"BlogArticle:682386\\\",\\\"title\\\":\\\"What RFK Jr. Told Grieving Texas Families About the Measles Vaccine\\\",\\\"url\\\":\\\"https://www.theatlantic.com/health/archive/2025/04/rfk-measles-grieving-families-texas/682386/\\\",\\\"__typename\\\":\\\"BlogArticle\\\"},\\\"__typename\\\":\\\"PromoEdge\\\"},{\\\"cursor\\\":\\\"MjAyNS0wMy0xMSAwOTowNzoyNnw2ODE5ODU=\\\",\\\"node\\\":{\\\"id\\\":\\\"BlogArticle:681985\\\",\\\"title\\\":\\\"His Daughter Was America’s First Measles Death in a Decade\\\",\\\"url\\\":\\\"https://www.theatlantic.com/health/archive/2025/03/texas-measles-outbreak-death-family/681985/\\\",\\\"__typename\\\":\\\"BlogArticle\\\"},\\\"__typename\\\":\\\"PromoEdge\\\"}],\\\"__typename\\\":\\\"RiverConnection\\\"},\\\"slug\\\":\\\"tom-bartlett\\\",\\\"socialMedia\\\":[{\\\"platform\\\":\\\"TWITTER\\\",\\\"url\\\":\\\"https://twitter.com/tebartl\\\",\\\"__typename\\\":\\\"SocialMedia\\\"}]}],\\\"authorContext\\\":\\\"\\\",\\\"categories\\\":[{\\\"slug\\\":\\\"high-converters\\\",\\\"__typename\\\":\\\"Category\\\"},{\\\"slug\\\":\\\"top-40\\\",\\\"__typename\\\":\\\"Category\\\"},{\\\"slug\\\":\\\"top-60\\\",\\\"__typename\\\":\\\"Category\\\"}],\\\"content\\\":[{\\\"__typename\\\":\\\"ArticleParagraphContent\\\",\\\"subtype\\\":null,\\\"idAttr\\\":\\\"\\\",\\\"innerHtml\\\":\\\"When Reddit rebranded itself as “the heart of the internet” a couple of years ago, the slogan was meant to evoke the site’s organic character. In an age of social media dominated by algorithms, Reddit took pride in being curated by a community that expressed its feelings in the form of upvotes and downvotes—in other words, being shaped by actual people.\\\"},{\\\"__typename\\\":\\\"ArticleParagraphContent\\\",\\\"subtype\\\":null,\\\"idAttr\\\":\\\"\\\",\\\"innerHtml\\\":\\\"So earlier this week, when members of a popular subreddit learned that their community had been infiltrated by undercover researchers posting AI-written comments and passing them off as human thoughts, the Redditors were predictably incensed. They called the experiment “violating,” “shameful,” “infuriating,” and “very disturbing.” As the backlash intensified, the researchers went silent, refusing to reveal their identity or answer questions about their methodology. The university that employs them has announced that it’s investigating. Meanwhile, Reddit’s chief legal officer, Ben Lee, \\u003ca data-event-element=\\\\\\\"inline link\\\\\\\" href=\\\\\\\"https://www.reddit.com/r/changemyview/comments/1k8b2hj/comment/mpk1u3c/?utm_source=share\\u0026amp;utm_medium=web3x\\u0026amp;utm_name=web3xcss\\u0026amp;utm_term=1\\u0026amp;utm_content=share_button\\\\\\\"\\u003ewrote\\u003c/a\\u003e that the company intends to “ensure that the researchers are held accountable for their misdeeds.”\\\"},{\\\"__typename\\\":\\\"ArticleParagraphContent\\\",\\\"subtype\\\":null,\\\"idAttr\\\":\\\"\\\",\\\"innerHtml\\\":\\\"Joining the chorus of disapproval were fellow internet researchers, who condemned what they saw as a plainly unethical experiment. Amy Bruckman, a professor at the Georgia Institute of Technology who has studied online communities for more than two decades, told me the Reddit fiasco is “the worst internet-research ethics violation I have ever seen, no contest.” What’s more, she and others worry that the uproar could undermine the work of scholars who are using more conventional methods to study a crucial problem: how AI influences the way humans think and relate to one another.\\\"},{\\\"__typename\\\":\\\"ArticleParagraphContent\\\",\\\"subtype\\\":null,\\\"idAttr\\\":\\\"\\\",\\\"innerHtml\\\":\\\"The researchers, based at the University of Zurich, wanted to find out whether AI-generated responses could change people’s views. So they headed to the aptly named subreddit \\u003ca data-event-element=\\\\\\\"inline link\\\\\\\" href=\\\\\\\"https://www.reddit.com/r/changemyview/\\\\\\\"\\u003er/changemyview\\u003c/a\\u003e, in which users debate important societal issues, along with plenty of trivial topics, and award points to posts that talk them out of their original position. Over the course of four months, the researchers posted more than 1,000 AI-generated comments on pitbulls (is aggression the fault of the breed or the owner?), the housing crisis (is living with your parents the solution?), DEI programs (were they destined to fail?). The AI commenters argued that browsing Reddit is a waste of time and that the “controlled demolition” 9/11 conspiracy theory has some merit. And as they offered their computer-generated opinions, they also shared their backstories. One claimed to be a trauma counselor; another described himself as a victim of statutory rape.\\\"},{\\\"__typename\\\":\\\"ArticleParagraphContent\\\",\\\"subtype\\\":null,\\\"idAttr\\\":\\\"\\\",\\\"innerHtml\\\":\\\"In one sense, the AI comments appear to have been rather effective. When researchers asked the AI to personalize its arguments to a Redditor’s biographical details, including gender, age, and political leanings (inferred, courtesy of another AI model, through the Redditor’s post history), a surprising number of minds indeed appear to have been changed. Those personalized AI arguments received, on average, far higher scores in the subreddit’s point system than nearly all human commenters, according to preliminary findings that the researchers shared with Reddit moderators and later made private. (This analysis, of course, assumes that no one else in the subreddit was using AI to hone their arguments.)\\\"},{\\\"__typename\\\":\\\"ArticleRelatedContentLink\\\",\\\"idAttr\\\":\\\"injected-recirculation-link-0\\\",\\\"innerHtml\\\":\\\"\\u003ca href=\\\\\\\"https://www.theatlantic.com/technology/archive/2025/04/arc-agi-chollet-test/682295/\\\\\\\"\\u003eRead: The man out to prove how dumb AI still is\\u003c/a\\u003e\\\",\\\"index\\\":0},{\\\"__typename\\\":\\\"ArticleParagraphContent\\\",\\\"subtype\\\":null,\\\"idAttr\\\":\\\"\\\",\\\"innerHtml\\\":\\\"The researchers had a tougher time convincing Redditors that their covert study was justified. After they had finished the experiment, they contacted the subreddit’s moderators, revealed their identity, and requested to “debrief” the subreddit—that is, to announce to members that for months, they had been unwitting subjects in a scientific experiment. “They were rather surprised that we had such a negative reaction to the experiment,” says one moderator, who asked to be identified by his username, LucidLeviathan, to protect his privacy. According to LucidLeviathan, the moderators requested that the researchers not publish such tainted work, and that they issue an apology. The researchers refused. After more than a month of back-and-forth, the moderators revealed what they had learned about the experiment (minus the researchers’ names) to the rest of the subreddit, making clear their disapproval.\\\"},{\\\"__typename\\\":\\\"ArticleParagraphContent\\\",\\\"subtype\\\":null,\\\"idAttr\\\":\\\"\\\",\\\"innerHtml\\\":\\\"When the moderators sent a complaint to the University of Zurich, the university noted in its response that the “project yields important insights, and the risks (e.g. trauma etc.) are minimal,” according to an excerpt posted by moderators. In a statement to me, a university spokesperson said that the ethics board had received notice of the study last month, advised the researchers to comply with the subreddit’s rules, and “intends to adopt a stricter review process in the future.” Meanwhile, the researchers defended their approach in a Reddit comment, arguing that “none of the comments advocate for harmful positions” and that each AI-generated comment was reviewed by a human team member before being posted. (I sent an email to an anonymized address for the researchers, posted by Reddit moderators, and received a reply that directed my inquiries to the university.)\\\"},{\\\"__typename\\\":\\\"ArticleParagraphContent\\\",\\\"subtype\\\":null,\\\"idAttr\\\":\\\"\\\",\\\"innerHtml\\\":\\\"Perhaps the most telling aspect of the Zurich researchers’ defense was that, as they saw it, deception was integral to the study. The University of Zurich’s ethics board—which can offer researchers advice but, according to the university, lacks the power to reject studies that fall short of its standards—told the researchers before they began posting that “the participants should be informed as much as possible,” according to the university statement I received. But the researchers seem to believe that doing so would have ruined the experiment. “To ethically test LLMs’ persuasive power in realistic scenarios, an unaware setting was necessary,” because it more realistically mimics how people would respond to unidentified bad actors in real-world settings, the researchers wrote in one of their Reddit comments.\\\"},{\\\"__typename\\\":\\\"ArticleParagraphContent\\\",\\\"subtype\\\":null,\\\"idAttr\\\":\\\"\\\",\\\"innerHtml\\\":\\\"How humans are likely to respond in such a scenario is an urgent issue and a worthy subject of academic research. In their preliminary results, the researchers concluded that AI arguments can be “highly persuasive in real-world contexts, surpassing all previously known benchmarks of human persuasiveness.” (Because the researchers finally agreed this week not to publish a paper about the experiment, the accuracy of that verdict will probably never be fully assessed, which is its own sort of shame.) The prospect of having your mind changed by something that doesn’t have one is deeply unsettling. That persuasive superpower could also be employed for nefarious ends.\\\"},{\\\"__typename\\\":\\\"ArticleRelatedContentLink\\\",\\\"idAttr\\\":\\\"injected-recirculation-link-1\\\",\\\"innerHtml\\\":\\\"\\u003ca href=\\\\\\\"https://www.theatlantic.com/technology/archive/2025/03/chatbots-benchmark-tests/681929/\\\\\\\"\\u003eRead: Chatbots are cheating on their benchmark tests\\u003c/a\\u003e\\\",\\\"index\\\":1},{\\\"__typename\\\":\\\"ArticleParagraphContent\\\",\\\"subtype\\\":null,\\\"idAttr\\\":\\\"\\\",\\\"innerHtml\\\":\\\"Still, scientists don’t have to flout the norms of experimenting on human subjects in order to evaluate the threat. “The general finding that AI can be on the upper end of human persuasiveness—more persuasive than most humans—jibes with what laboratory experiments have found,” Christian Tarsney, a senior research fellow at the University of Texas at Austin, told me. In one \\u003ca data-event-element=\\\\\\\"inline link\\\\\\\" href=\\\\\\\"https://www.science.org/doi/10.1126/science.adq1814\\\\\\\"\\u003erecent laboratory experiment\\u003c/a\\u003e, participants who believed in conspiracy theories voluntarily chatted with an AI; after three exchanges, about a quarter of them lost faith in their previous beliefs. Another found that ChatGPT produced \\u003ca data-event-element=\\\\\\\"inline link\\\\\\\" href=\\\\\\\"https://www.science.org/doi/full/10.1126/sciadv.adh1850\\\\\\\"\\u003emore persuasive disinformation\\u003c/a\\u003e than humans, and that participants who were asked to distinguish between real posts and those written by AI could not effectively do so.\\\"},{\\\"__typename\\\":\\\"ArticleParagraphContent\\\",\\\"subtype\\\":null,\\\"idAttr\\\":\\\"\\\",\\\"innerHtml\\\":\\\"Giovanni Spitale, the lead author of that study, also happens to be a scholar at the University of Zurich, and has been in touch with one of the researchers behind the Reddit AI experiment, who asked him not to reveal their identity. “We are receiving dozens of death threats,” the researcher wrote to him, in a message Spitale shared with me. “Please keep the secret for the safety of my family.”\\\"},{\\\"__typename\\\":\\\"ArticleParagraphContent\\\",\\\"subtype\\\":null,\\\"idAttr\\\":\\\"\\\",\\\"innerHtml\\\":\\\"One likely reason the backlash has been so strong is because, on a platform as close-knit as Reddit, betrayal cuts deep. “One of the pillars of that community is mutual trust,” Spitale told me; it’s part of the reason he opposes experimenting on Redditors without their knowledge. Several scholars I spoke with about this latest ethical quandary compared it—unfavorably—to Facebook’s \\u003ca data-event-element=\\\\\\\"inline link\\\\\\\" href=\\\\\\\"https://www.theatlantic.com/technology/archive/2014/06/everything-we-know-about-facebooks-secret-mood-manipulation-experiment/373648/\\\\\\\"\\u003einfamous emotional-contagion study\\u003c/a\\u003e. For one week in 2012, Facebook altered users’ News Feed to see if viewing more or less positive content changed their posting habits. (It did, a little bit.) Casey Fiesler, an associate professor at the University of Colorado at Boulder who studies ethics and online communities, told me that the emotional-contagion study pales in comparison with what the Zurich researchers did. “People were upset about that but not in the way that this Reddit community is upset,” she told me. “This felt a lot more personal.”\\\"},{\\\"__typename\\\":\\\"ArticleRelatedContentLink\\\",\\\"idAttr\\\":\\\"injected-recirculation-link-2\\\",\\\"innerHtml\\\":\\\"\\u003ca href=\\\\\\\"https://www.theatlantic.com/technology/archive/2025/04/how-ai-will-actually-contribute-cancer-cure/682607/\\\\\\\"\\u003eRead: AI executives promise cancer cures. Here’s the reality.\\u003c/a\\u003e\\\",\\\"index\\\":2},{\\\"__typename\\\":\\\"ArticleParagraphContent\\\",\\\"subtype\\\":null,\\\"idAttr\\\":\\\"\\\",\\\"innerHtml\\\":\\\"The reaction probably also has to do with the unnerving notion that ChatGPT knows what buttons to push in our minds. It’s one thing to be fooled by some human Facebook researchers with dubious ethical standards, and another entirely to be duped by a cosplaying chatbot. I read through dozens of the AI comments, and although they weren’t all brilliant, most of them seemed reasonable and genuine enough. They made a lot of good points, and I found myself nodding along more than once. As the Zurich researchers warn, without more robust detection tools, AI bots might “seamlessly blend into online communities”—that is, assuming they haven’t already.\\\"}],\\\"editorialProject\\\":{\\\"river\\\":{\\\"edges\\\":[{\\\"node\\\":{\\\"__typename\\\":\\\"BlogArticle\\\",\\\"url\\\":\\\"https://www.theatlantic.com/technology/archive/2025/05/sycophantic-ai/682743/\\\",\\\"title\\\":\\\"AI Is Not Your Friend\\\",\\\"authors\\\":[{\\\"url\\\":\\\"https://www.theatlantic.com/author/mike-caulfield/\\\",\\\"displayName\\\":\\\"Mike Caulfield\\\",\\\"id\\\":\\\"Author:38975\\\",\\\"slug\\\":\\\"mike-caulfield\\\",\\\"__typename\\\":\\\"Author\\\"}],\\\"riverImage\\\":{\\\"id\\\":\\\"Image:1750740:80x80:1x,2x\\\",\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/A_jIj3CtlzuNNTfC0ukcQ3jH8-4=/438x0:1563x1125/80x80/media/img/mt/2025/05/2025_5_6_Sycophantic_Web/original.gif\\\",\\\"srcSet\\\":\\\"https://cdn.theatlantic.com/thumbor/A_jIj3CtlzuNNTfC0ukcQ3jH8-4=/438x0:1563x1125/80x80/media/img/mt/2025/05/2025_5_6_Sycophantic_Web/original.gif, https://cdn.theatlantic.com/thumbor/c0PBq-hUdATZlMlDK7Iu_GgOP5U=/438x0:1563x1125/160x160/media/img/mt/2025/05/2025_5_6_Sycophantic_Web/original.gif 2x\\\",\\\"reducedMotionSrcSet\\\":\\\"https://cdn.theatlantic.com/thumbor/HNV-dDyEh5L3ibtliKNUhv-ATUU=/438x0:1563x1125/80x80/filters:still()/media/img/mt/2025/05/2025_5_6_Sycophantic_Web/original.gif, https://cdn.theatlantic.com/thumbor/Am-fJ-RvidfuMn63352ehXsvEco=/438x0:1563x1125/160x160/filters:still()/media/img/mt/2025/05/2025_5_6_Sycophantic_Web/original.gif 2x\\\",\\\"width\\\":80,\\\"height\\\":80,\\\"altText\\\":\\\"Animation of a person's silhouette with computer code running through it\\\",\\\"__typename\\\":\\\"BasicImage\\\"}},\\\"__typename\\\":\\\"PromoEdge\\\"},{\\\"node\\\":{\\\"__typename\\\":\\\"BlogArticle\\\",\\\"url\\\":\\\"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/\\\",\\\"title\\\":\\\"‘The Worst Internet-Research Ethics Violation I Have Ever Seen’\\\",\\\"authors\\\":[{\\\"url\\\":\\\"https://www.theatlantic.com/author/tom-bartlett/\\\",\\\"displayName\\\":\\\"Tom Bartlett\\\",\\\"id\\\":\\\"Author:26844\\\",\\\"slug\\\":\\\"tom-bartlett\\\",\\\"__typename\\\":\\\"Author\\\"}],\\\"riverImage\\\":{\\\"id\\\":\\\"Image:1749426:80x80:1x,2x\\\",\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/6cNDGHwip7boYwmIfmMjS2t4Ibw=/485x0:1610x1125/80x80/media/img/mt/2025/05/reddit_1/original.jpg\\\",\\\"srcSet\\\":\\\"https://cdn.theatlantic.com/thumbor/6cNDGHwip7boYwmIfmMjS2t4Ibw=/485x0:1610x1125/80x80/media/img/mt/2025/05/reddit_1/original.jpg, https://cdn.theatlantic.com/thumbor/2Ous5HbbV7YikHYyfQsVmlornu4=/485x0:1610x1125/160x160/media/img/mt/2025/05/reddit_1/original.jpg 2x\\\",\\\"reducedMotionSrcSet\\\":null,\\\"width\\\":80,\\\"height\\\":80,\\\"altText\\\":\\\"A blurry, distorted, pixellated image of the Reddit robot's head against an orange background\\\",\\\"__typename\\\":\\\"BasicImage\\\"}},\\\"__typename\\\":\\\"PromoEdge\\\"},{\\\"node\\\":{\\\"__typename\\\":\\\"BlogArticle\\\",\\\"url\\\":\\\"https://www.theatlantic.com/technology/archive/2025/04/great-language-flattening/682627/\\\",\\\"title\\\":\\\"The Great Language Flattening\\\",\\\"authors\\\":[{\\\"url\\\":\\\"https://www.theatlantic.com/author/victoria-turk/\\\",\\\"displayName\\\":\\\"Victoria Turk\\\",\\\"id\\\":\\\"Author:39068\\\",\\\"slug\\\":\\\"victoria-turk\\\",\\\"__typename\\\":\\\"Author\\\"}],\\\"riverImage\\\":{\\\"id\\\":\\\"Image:1748386:80x80:1x,2x\\\",\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/H4pHGoPg5IDFy0ulLAwUJA4U7ys=/438x0:1563x1125/80x80/media/img/mt/2025/04/Cloud/original.gif\\\",\\\"srcSet\\\":\\\"https://cdn.theatlantic.com/thumbor/H4pHGoPg5IDFy0ulLAwUJA4U7ys=/438x0:1563x1125/80x80/media/img/mt/2025/04/Cloud/original.gif, https://cdn.theatlantic.com/thumbor/5l0Oa1dafGvt0ITmG7Fdmaq65kc=/438x0:1563x1125/160x160/media/img/mt/2025/04/Cloud/original.gif 2x\\\",\\\"reducedMotionSrcSet\\\":\\\"https://cdn.theatlantic.com/thumbor/cXntWE6nm-Wu2tPP0PubMz27vAE=/438x0:1563x1125/80x80/filters:still()/media/img/mt/2025/04/Cloud/original.gif, https://cdn.theatlantic.com/thumbor/lfbgGEDwbJ91Qhh0Y-5w918xJtM=/438x0:1563x1125/160x160/filters:still()/media/img/mt/2025/04/Cloud/original.gif 2x\\\",\\\"width\\\":80,\\\"height\\\":80,\\\"altText\\\":\\\"Animation of letters floating around in a black ooze\\\",\\\"__typename\\\":\\\"BasicImage\\\"}},\\\"__typename\\\":\\\"PromoEdge\\\"},{\\\"node\\\":{\\\"__typename\\\":\\\"BlogArticle\\\",\\\"url\\\":\\\"https://www.theatlantic.com/technology/archive/2025/04/how-ai-will-actually-contribute-cancer-cure/682607/\\\",\\\"title\\\":\\\"AI Executives Promise Cancer Cures. Here’s the Reality\\\",\\\"authors\\\":[{\\\"url\\\":\\\"https://www.theatlantic.com/author/matteo-wong/\\\",\\\"displayName\\\":\\\"Matteo Wong\\\",\\\"id\\\":\\\"Author:38048\\\",\\\"slug\\\":\\\"matteo-wong\\\",\\\"__typename\\\":\\\"Author\\\"}],\\\"riverImage\\\":{\\\"id\\\":\\\"Image:1747975:80x80:1x,2x\\\",\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/BNCUlAwitYq1H_NFztmEgnS0AZk=/438x0:1563x1125/80x80/media/img/mt/2025/04/cancer/original.jpg\\\",\\\"srcSet\\\":\\\"https://cdn.theatlantic.com/thumbor/BNCUlAwitYq1H_NFztmEgnS0AZk=/438x0:1563x1125/80x80/media/img/mt/2025/04/cancer/original.jpg, https://cdn.theatlantic.com/thumbor/EiZD7h5mQkxkwhhmT1p1dd_jwAk=/438x0:1563x1125/160x160/media/img/mt/2025/04/cancer/original.jpg 2x\\\",\\\"reducedMotionSrcSet\\\":null,\\\"width\\\":80,\\\"height\\\":80,\\\"altText\\\":\\\"Collage of medical imagery\\\",\\\"__typename\\\":\\\"BasicImage\\\"}},\\\"__typename\\\":\\\"PromoEdge\\\"}],\\\"__typename\\\":\\\"RiverConnection\\\"},\\\"__typename\\\":\\\"EditorialProject\\\",\\\"displayName\\\":\\\"Artificial Intelligence\\\",\\\"url\\\":\\\"https://www.theatlantic.com/projects/artificial-intelligence/\\\",\\\"accentColor\\\":\\\"#fbfafa\\\",\\\"textColor\\\":\\\"#000000\\\",\\\"presentedByUrl\\\":\\\"\\\",\\\"presentedByDisplayName\\\":\\\"\\\",\\\"foundationLineArticle\\\":\\\"\\\",\\\"slug\\\":\\\"artificial-intelligence\\\"},\\\"primaryCategory\\\":{\\\"__typename\\\":\\\"Channel\\\",\\\"displayName\\\":\\\"Technology\\\",\\\"url\\\":\\\"https://www.theatlantic.com/technology/\\\",\\\"slug\\\":\\\"technology\\\"},\\\"reviews\\\":[],\\\"embeds\\\":[],\\\"preview\\\":null,\\\"tags\\\":[{\\\"name\\\":\\\"arts, culture, entertainment and media\\\",\\\"url\\\":null,\\\"__typename\\\":\\\"Tag\\\",\\\"slug\\\":\\\"arts-culture-entertainment-and-media\\\"},{\\\"name\\\":\\\"mass media\\\",\\\"url\\\":null,\\\"__typename\\\":\\\"Tag\\\",\\\"slug\\\":\\\"mass-media\\\"},{\\\"name\\\":\\\"university\\\",\\\"url\\\":null,\\\"__typename\\\":\\\"Tag\\\",\\\"slug\\\":\\\"university\\\"},{\\\"name\\\":\\\"Facebook\\\",\\\"url\\\":\\\"https://www.theatlantic.com/tag/product/facebook/\\\",\\\"__typename\\\":\\\"Tag\\\",\\\"slug\\\":\\\"facebook\\\"},{\\\"name\\\":\\\"Reddit\\\",\\\"url\\\":\\\"https://www.theatlantic.com/tag/organization/reddit/\\\",\\\"__typename\\\":\\\"Tag\\\",\\\"slug\\\":\\\"reddit\\\"},{\\\"name\\\":\\\"researcher\\\",\\\"url\\\":null,\\\"__typename\\\":\\\"Tag\\\",\\\"slug\\\":\\\"researcher\\\"},{\\\"name\\\":\\\"science and technology\\\",\\\"url\\\":null,\\\"__typename\\\":\\\"Tag\\\",\\\"slug\\\":\\\"science-and-technology\\\"},{\\\"name\\\":\\\"technology and engineering\\\",\\\"url\\\":null,\\\"__typename\\\":\\\"Tag\\\",\\\"slug\\\":\\\"technology-and-engineering\\\"},{\\\"name\\\":\\\"information technology and computer science\\\",\\\"url\\\":null,\\\"__typename\\\":\\\"Tag\\\",\\\"slug\\\":\\\"information-technology-and-computer-science\\\"},{\\\"name\\\":\\\"artificial intelligence\\\",\\\"url\\\":null,\\\"__typename\\\":\\\"Tag\\\",\\\"slug\\\":\\\"artificial-intelligence\\\"},{\\\"name\\\":\\\"ethics\\\",\\\"url\\\":null,\\\"__typename\\\":\\\"Tag\\\",\\\"slug\\\":\\\"ethics\\\"},{\\\"name\\\":\\\"University of Zurich\\\",\\\"url\\\":null,\\\"__typename\\\":\\\"Tag\\\",\\\"slug\\\":\\\"university-of-zurich\\\"},{\\\"name\\\":\\\"Zurich\\\",\\\"url\\\":null,\\\"__typename\\\":\\\"Tag\\\",\\\"slug\\\":\\\"zurich\\\"}],\\\"layout\\\":\\\"standard\\\",\\\"secondaryByline\\\":\\\"\\\",\\\"dek\\\":\\\"The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.\\\",\\\"url\\\":\\\"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/\\\",\\\"shareText\\\":\\\"A secret experiment that turned Redditors into guinea pigs was an ethical disaster—and could undermine research that’s only getting more urgent, @tebartl reports.\\\",\\\"shareTitle\\\":\\\"‘The Worst Internet-Research Ethics Violation I Have Ever Seen’\\\",\\\"title\\\":\\\"‘The Worst Internet-Research Ethics Violation I Have Ever Seen’\\\",\\\"datePublished\\\":\\\"2025-05-02T17:55:38Z\\\",\\\"editorsNote\\\":null,\\\"leadArt\\\":{\\\"__typename\\\":\\\"LeadArtImage\\\",\\\"image\\\":{\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/_RS0mJORhUjk2oKFp1-lrHWZJ0Y=/0x0:2000x1125/960x540/media/img/mt/2025/05/reddit_1/original.jpg\\\",\\\"width\\\":960,\\\"height\\\":540,\\\"srcSet\\\":\\\"https://cdn.theatlantic.com/thumbor/f_du4irHtRNkjeNQQwkc67dBWBo=/0x0:2000x1125/750x422/media/img/mt/2025/05/reddit_1/original.jpg 750w, https://cdn.theatlantic.com/thumbor/e4jEibjQkC_AtPxf6WB7O-CZ8wU=/0x0:2000x1125/828x466/media/img/mt/2025/05/reddit_1/original.jpg 828w, https://cdn.theatlantic.com/thumbor/_RS0mJORhUjk2oKFp1-lrHWZJ0Y=/0x0:2000x1125/960x540/media/img/mt/2025/05/reddit_1/original.jpg 960w, https://cdn.theatlantic.com/thumbor/K-HTgOf-8ecmTJUtIp-1XUeWFpE=/0x0:2000x1125/976x549/media/img/mt/2025/05/reddit_1/original.jpg 976w, https://cdn.theatlantic.com/thumbor/pP0fvpOUnZzkl7oOPHRWZ5NFXf8=/0x0:2000x1125/1952x1098/media/img/mt/2025/05/reddit_1/original.jpg 1952w\\\",\\\"reducedMotionSrcSet\\\":null,\\\"altText\\\":\\\"A blurry, distorted, pixellated image of the Reddit robot's head against an orange background\\\",\\\"captionText\\\":\\\"\\\",\\\"attributionText\\\":\\\"Illustration by The Atlantic\\\",\\\"attributionUrl\\\":\\\"\\\",\\\"__typename\\\":\\\"BasicImage\\\"}},\\\"audio\\\":{\\\"url\\\":\\\"https://traffic.megaphone.fm/ATL2242292588.mp3\\\",\\\"urlAdfree\\\":\\\"https://traffic.megaphone.fm/ATL6768914588.mp3\\\",\\\"duration\\\":620.72,\\\"vendor\\\":\\\"elevenlabs\\\",\\\"type\\\":\\\"narrated\\\",\\\"__typename\\\":\\\"Audio\\\"},\\\"hasAudioRights\\\":true,\\\"narratedAudioImage\\\":{\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/6cNDGHwip7boYwmIfmMjS2t4Ibw=/485x0:1610x1125/80x80/media/img/mt/2025/05/reddit_1/original.jpg\\\",\\\"srcSet\\\":\\\"https://cdn.theatlantic.com/thumbor/6cNDGHwip7boYwmIfmMjS2t4Ibw=/485x0:1610x1125/80x80/media/img/mt/2025/05/reddit_1/original.jpg 80w, https://cdn.theatlantic.com/thumbor/GjtUuBbkJAI8CXGVztWsqNh6kng=/485x0:1610x1125/96x96/media/img/mt/2025/05/reddit_1/original.jpg 96w, https://cdn.theatlantic.com/thumbor/ReRHYhSvI9bJUtGoeELKSeztV6k=/485x0:1610x1125/128x128/media/img/mt/2025/05/reddit_1/original.jpg 128w, https://cdn.theatlantic.com/thumbor/2Ous5HbbV7YikHYyfQsVmlornu4=/485x0:1610x1125/160x160/media/img/mt/2025/05/reddit_1/original.jpg 160w, https://cdn.theatlantic.com/thumbor/Q5d4GGENqCxtH2cim21k0vrtzaE=/485x0:1610x1125/192x192/media/img/mt/2025/05/reddit_1/original.jpg 192w, https://cdn.theatlantic.com/thumbor/-iCsgfmsFTXpiVIwKjvGTjAyieM=/485x0:1610x1125/256x256/media/img/mt/2025/05/reddit_1/original.jpg 256w, https://cdn.theatlantic.com/thumbor/vyVWvalnMyc_79tfWpF-E4bLjUo=/485x0:1610x1125/384x384/media/img/mt/2025/05/reddit_1/original.jpg 384w, https://cdn.theatlantic.com/thumbor/XY5BHma87Kg5Yu0KvhLU_1GOke4=/485x0:1610x1125/512x512/media/img/mt/2025/05/reddit_1/original.jpg 512w\\\",\\\"width\\\":80,\\\"height\\\":80,\\\"altText\\\":\\\"A blurry, distorted, pixellated image of the Reddit robot's head against an orange background\\\",\\\"__typename\\\":\\\"BasicImage\\\"},\\\"seoTitle\\\":\\\"The Secret AI Experiment That Sent Reddit Into a Frenzy\\\",\\\"hasMeter\\\":true,\\\"channels\\\":[{\\\"slug\\\":\\\"technology\\\",\\\"__typename\\\":\\\"Channel\\\"}],\\\"primaryChannel\\\":{\\\"slug\\\":\\\"technology\\\",\\\"__typename\\\":\\\"Channel\\\",\\\"displayName\\\":\\\"Technology\\\"},\\\"shareDek\\\":\\\"The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.\\\",\\\"fbiaUrl\\\":\\\"https://www.theatlantic.com/facebook-instant/article/682676/\\\",\\\"canonicalUrl\\\":\\\"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/\\\",\\\"dateModified\\\":\\\"2025-05-03T14:07:06Z\\\",\\\"syndication\\\":\\\"ALL\\\",\\\"shareImage2x1\\\":{\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/B7t1UhFG0zYeDEQ3kmL-nn9AhLw=/0x68:2000x1110/1200x625/media/img/mt/2025/05/reddit_1/original.jpg\\\",\\\"height\\\":625,\\\"width\\\":1200,\\\"__typename\\\":\\\"BasicImage\\\"},\\\"shareImageGift2x1\\\":{\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/zUyZ6al7jkWCvxOb1lZFwS9U7Ww=/0x68:2000x1110/1200x625/filters:watermark(https://cdn.theatlantic.com/media/files/badge_2x.png,-20,20,0,33)/media/img/mt/2025/05/reddit_1/original.jpg\\\",\\\"height\\\":625,\\\"width\\\":1200,\\\"__typename\\\":\\\"BasicImage\\\"},\\\"shareImageGiftSmall\\\":{\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/QJ-BV64OBm3YGtOMjh59FO_ag94=/4x66:1996x1112/960x504/filters:watermark(https://cdn.theatlantic.com/media/files/badge_2x.png,-20,20,0,33)/media/img/mt/2025/05/reddit_1/original.jpg\\\",\\\"height\\\":504,\\\"width\\\":960,\\\"__typename\\\":\\\"BasicImage\\\"},\\\"shareImageSmall\\\":{\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/O0qGyAw2_SIJzoU4vLMuXog1wA4=/4x66:1996x1112/960x504/media/img/mt/2025/05/reddit_1/original.jpg\\\",\\\"height\\\":504,\\\"width\\\":960,\\\"__typename\\\":\\\"BasicImage\\\"},\\\"watsonInfo\\\":null,\\\"shareImage1x1\\\":{\\\"width\\\":1080,\\\"height\\\":1080,\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/SPCwxI7D8CM-vJLqbR14jeCONjU=/485x0:1610x1125/1080x1080/media/img/mt/2025/05/reddit_1/original.jpg\\\",\\\"__typename\\\":\\\"BasicImage\\\"},\\\"shareImage16x9\\\":{\\\"width\\\":1600,\\\"height\\\":900,\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/esyTT-yGFKE26VMaxjgDpA5HQXI=/0x0:2000x1125/1600x900/media/img/mt/2025/05/reddit_1/original.jpg\\\",\\\"__typename\\\":\\\"BasicImage\\\"},\\\"shareImage4x3\\\":{\\\"width\\\":1200,\\\"height\\\":900,\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/D3cNpBQw3rR4jn6l9xYOdQhUuzw=/249x0:1749x1125/1200x900/media/img/mt/2025/05/reddit_1/original.jpg\\\",\\\"__typename\\\":\\\"BasicImage\\\"},\\\"shareImageDefault\\\":{\\\"width\\\":960,\\\"height\\\":540,\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/_RS0mJORhUjk2oKFp1-lrHWZJ0Y=/0x0:2000x1125/960x540/media/img/mt/2025/05/reddit_1/original.jpg\\\",\\\"__typename\\\":\\\"BasicImage\\\"},\\\"shareImageSquareDefault\\\":{\\\"width\\\":540,\\\"height\\\":540,\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/x0B8YDVGWEWdlT5tybp0laDD56k=/485x0:1610x1125/540x540/media/img/mt/2025/05/reddit_1/original.jpg\\\",\\\"__typename\\\":\\\"BasicImage\\\"},\\\"shareImageLeadArt\\\":{\\\"image\\\":{\\\"width\\\":720,\\\"height\\\":405,\\\"url\\\":\\\"https://cdn.theatlantic.com/thumbor/T3Oyb2ZD_rMW9IWs0w-BcsgR2Zg=/0x0:2000x1125/720x405/media/img/mt/2025/05/reddit_1/original.jpg\\\",\\\"__typename\\\":\\\"BasicImage\\\"},\\\"__typename\\\":\\\"LeadArtImage\\\"},\\\"slug\\\":\\\"reddit-ai-persuasion-experiment-ethics\\\",\\\"grapeshot\\\":{\\\"segments\\\":[],\\\"__typename\\\":\\\"Grapeshot\\\"}}}\"},\"2021745665\":{\"data\":\"{\\\"breakingNews\\\":null}\"},\"2242501338\":{\"data\":\"{\\\"article\\\":{\\\"primaryCategory\\\":{\\\"__typename\\\":\\\"Channel\\\"},\\\"editorialProject\\\":{\\\"river\\\":{\\\"edges\\\":[{\\\"id\\\":\\\"PromoEdge:682743\\\",\\\"__typename\\\":\\\"PromoEdge\\\"},{\\\"id\\\":\\\"PromoEdge:682676\\\",\\\"__typename\\\":\\\"PromoEdge\\\"},{\\\"id\\\":\\\"PromoEdge:682627\\\",\\\"__typename\\\":\\\"PromoEdge\\\"},{\\\"id\\\":\\\"PromoEdge:682607\\\",\\\"__typename\\\":\\\"PromoEdge\\\"}],\\\"__typename\\\":\\\"RiverConnection\\\"},\\\"__typename\\\":\\\"EditorialProject\\\"},\\\"__typename\\\":\\\"BlogArticle\\\"}}\"},\"2255517827\":{\"data\":\"{\\\"article\\\":{\\\"id\\\":\\\"BlogArticle:682676\\\",\\\"isTnfCompatible\\\":true,\\\"layout\\\":\\\"standard\\\",\\\"hasMeter\\\":true,\\\"url\\\":\\\"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/\\\",\\\"dateModified\\\":\\\"2025-05-03T14:07:06Z\\\",\\\"__typename\\\":\\\"BlogArticle\\\"}}\"}},\"urqlClient\":null},\"isSocialBot\":false},\"page\":\"/[channel]/archive/[year]/[month]/[slug]/[id]\",\"query\":{\"channel\":\"technology\",\"year\":\"2025\",\"month\":\"05\",\"slug\":\"reddit-ai-persuasion-experiment-ethics\",\"id\":\"682676\"},\"buildId\":\"28ce4803a9\",\"assetPrefix\":\"https://cdn.theatlantic.com\",\"runtimeConfig\":{\"GTM_CONTAINER_ID\":\"GTM-NTQTB9V\",\"GTM_CONTAINER_ID_NONCONSENTED\":\"GTM-5839GV7\",\"GTM_CONTAINER_ID_WEBVIEW\":\"GTM-TRJJ8RJ4\",\"GRAPHQL_API_URL\":\"https://graphql.theatlantic.com\",\"GRAPHQL_API_KEY\":\"JakhyMEXwa9odtB8gBxFI63ITyKqDGkn7ciGVIJf\",\"ADS_LIB_URL\":\"https://www.theatlantic.com/packages/hummingbirdjs/hummingbird.min.js\",\"ACCOUNTS_FRONTEND_URL\":\"https://accounts.theatlantic.com\",\"ENABLE_FEATURE_ARTICLE_RENDER\":\"false\",\"RECAPTCHA_SITE_KEY\":\"6Lc9Z7AUAAAAAEYS1dgAG2_6tT3KLqZQ1z4kbDRc\",\"BETA_ENV\":false},\"isFallback\":false,\"isExperimentalCompile\":false,\"dynamicIds\":[9587,50649],\"gip\":true,\"appGip\":true,\"scriptLoader\":[]}</script><script nomodule=\"\" src=\"https://cdn.theatlantic.com/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js\"></script><script async=\"\" src=\"https://cdn.theatlantic.com/_next/static/chunks/9587.03e639dfa3e4199a.js\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/webpack-0d5a2cb0b4910e76.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/framework-ca706bf673a13738.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/main-6b1ab16c3edd97e7.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/pages/_app-7c2ae150b4b06c1e.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/6729-7978443139836095.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/8286-b35c81576953924b.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/436-aadfbd871a794704.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/9843-267e63e874251a37.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/899-448b91a629016ff2.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/2912-ebf029ed4116b01f.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/6772-2c0b832574392efa.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/6392-1397c4f8500d73b6.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/4947-14b734b9a9ce020d.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/3297-543c6448c0812417.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/365-c8fca3fa4ec2068c.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/1744-9f7f4aee961e11d6.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/9310-d0d5baafd7a7f3b8.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/4742-11d63822f6f0cf2b.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/1730-999b0129269ea1cb.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/1198-0022e74b04f81a33.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/2381-7fd43dea62d91684.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/chunks/pages/%5Bchannel%5D/archive/%5Byear%5D/%5Bmonth%5D/%5Bslug%5D/%5Bid%5D-9a1848eea99404c9.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/28ce4803a9/_buildManifest.js\" async=\"\"></script><script src=\"https://cdn.theatlantic.com/_next/static/28ce4803a9/_ssgManifest.js\" async=\"\"></script></body></html>","oembed":false,"readabilityObject":{"title":"The Secret AI Experiment That Sent Reddit Into a Frenzy","content":"<div id=\"readability-page-1\" class=\"page\"><article><header data-event-module=\"hero\"><div><div><p><h2 data-flatplan-title=\"true\">‘The Worst Internet-Research Ethics Violation I Have Ever Seen’</h2></p><p>The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.</p></div><div><figure><div data-flatplan-lead_figure_media=\"true\"><picture><img alt=\"A blurry, distorted, pixellated image of the Reddit robot's head against an orange background\" sizes=\"(min-width: 976px) 976px, 100vw\" srcset=\"https://cdn.theatlantic.com/thumbor/f_du4irHtRNkjeNQQwkc67dBWBo=/0x0:2000x1125/750x422/media/img/mt/2025/05/reddit_1/original.jpg 750w, https://cdn.theatlantic.com/thumbor/e4jEibjQkC_AtPxf6WB7O-CZ8wU=/0x0:2000x1125/828x466/media/img/mt/2025/05/reddit_1/original.jpg 828w, https://cdn.theatlantic.com/thumbor/_RS0mJORhUjk2oKFp1-lrHWZJ0Y=/0x0:2000x1125/960x540/media/img/mt/2025/05/reddit_1/original.jpg 960w, https://cdn.theatlantic.com/thumbor/K-HTgOf-8ecmTJUtIp-1XUeWFpE=/0x0:2000x1125/976x549/media/img/mt/2025/05/reddit_1/original.jpg 976w, https://cdn.theatlantic.com/thumbor/pP0fvpOUnZzkl7oOPHRWZ5NFXf8=/0x0:2000x1125/1952x1098/media/img/mt/2025/05/reddit_1/original.jpg 1952w\" src=\"https://cdn.theatlantic.com/thumbor/_RS0mJORhUjk2oKFp1-lrHWZJ0Y=/0x0:2000x1125/960x540/media/img/mt/2025/05/reddit_1/original.jpg\" id=\"article-lead-image\" width=\"960\" height=\"540\"></picture></div><figcaption data-flatplan-lead_figure_caption=\"true\">Illustration by The Atlantic</figcaption></figure></div></div><gpt-ad format=\"injector\" sizes-at-0=\"mobile-wide\" targeting-pos=\"injector-article-start\" sizes-at-976=\"desktop-wide\"></gpt-ad></header><div data-view-action=\"view - audio player - start\" data-view-label=\"682676\" data-event-module=\"audio player\" data-event-content-type=\"narrated\" data-event-module-state=\"start\" data-event-view=\"true\"><div><p><img alt=\"A blurry, distorted, pixellated image of the Reddit robot's head against an orange background\" sizes=\"80px\" srcset=\"https://cdn.theatlantic.com/thumbor/6cNDGHwip7boYwmIfmMjS2t4Ibw=/485x0:1610x1125/80x80/media/img/mt/2025/05/reddit_1/original.jpg 80w, https://cdn.theatlantic.com/thumbor/GjtUuBbkJAI8CXGVztWsqNh6kng=/485x0:1610x1125/96x96/media/img/mt/2025/05/reddit_1/original.jpg 96w, https://cdn.theatlantic.com/thumbor/ReRHYhSvI9bJUtGoeELKSeztV6k=/485x0:1610x1125/128x128/media/img/mt/2025/05/reddit_1/original.jpg 128w, https://cdn.theatlantic.com/thumbor/2Ous5HbbV7YikHYyfQsVmlornu4=/485x0:1610x1125/160x160/media/img/mt/2025/05/reddit_1/original.jpg 160w, https://cdn.theatlantic.com/thumbor/Q5d4GGENqCxtH2cim21k0vrtzaE=/485x0:1610x1125/192x192/media/img/mt/2025/05/reddit_1/original.jpg 192w, https://cdn.theatlantic.com/thumbor/-iCsgfmsFTXpiVIwKjvGTjAyieM=/485x0:1610x1125/256x256/media/img/mt/2025/05/reddit_1/original.jpg 256w, https://cdn.theatlantic.com/thumbor/vyVWvalnMyc_79tfWpF-E4bLjUo=/485x0:1610x1125/384x384/media/img/mt/2025/05/reddit_1/original.jpg 384w, https://cdn.theatlantic.com/thumbor/XY5BHma87Kg5Yu0KvhLU_1GOke4=/485x0:1610x1125/512x512/media/img/mt/2025/05/reddit_1/original.jpg 512w\" src=\"https://cdn.theatlantic.com/thumbor/6cNDGHwip7boYwmIfmMjS2t4Ibw=/485x0:1610x1125/80x80/media/img/mt/2025/05/reddit_1/original.jpg\" width=\"80\" height=\"80\"></p></div><p>Produced by ElevenLabs and<!-- --> <a href=\"https://newsoveraudio.com/?offerId=atl_reader_exclusive_jks1kjl\"> <!-- -->News Over Audio (Noa)</a> <!-- -->using AI narration. Listen to more stories on the Noa app.</p></div><section data-event-module=\"article body\" data-flatplan-body=\"true\"><p data-flatplan-paragraph=\"true\">When Reddit rebranded itself as “the heart of the internet” a couple of years ago, the slogan was meant to evoke the site’s organic character. In an age of social media dominated by algorithms, Reddit took pride in being curated by a community that expressed its feelings in the form of upvotes and downvotes—in other words, being shaped by actual people.</p><p data-flatplan-paragraph=\"true\">So earlier this week, when members of a popular subreddit learned that their community had been infiltrated by undercover researchers posting AI-written comments and passing them off as human thoughts, the Redditors were predictably incensed. They called the experiment “violating,” “shameful,” “infuriating,” and “very disturbing.” As the backlash intensified, the researchers went silent, refusing to reveal their identity or answer questions about their methodology. The university that employs them has announced that it’s investigating. Meanwhile, Reddit’s chief legal officer, Ben Lee, <a data-event-element=\"inline link\" href=\"https://www.reddit.com/r/changemyview/comments/1k8b2hj/comment/mpk1u3c/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button\">wrote</a> that the company intends to “ensure that the researchers are held accountable for their misdeeds.”</p><p data-flatplan-paragraph=\"true\">Joining the chorus of disapproval were fellow internet researchers, who condemned what they saw as a plainly unethical experiment. Amy Bruckman, a professor at the Georgia Institute of Technology who has studied online communities for more than two decades, told me the Reddit fiasco is “the worst internet-research ethics violation I have ever seen, no contest.” What’s more, she and others worry that the uproar could undermine the work of scholars who are using more conventional methods to study a crucial problem: how AI influences the way humans think and relate to one another.</p><p data-flatplan-paragraph=\"true\">The researchers, based at the University of Zurich, wanted to find out whether AI-generated responses could change people’s views. So they headed to the aptly named subreddit <a data-event-element=\"inline link\" href=\"https://www.reddit.com/r/changemyview/\">r/changemyview</a>, in which users debate important societal issues, along with plenty of trivial topics, and award points to posts that talk them out of their original position. Over the course of four months, the researchers posted more than 1,000 AI-generated comments on pitbulls (is aggression the fault of the breed or the owner?), the housing crisis (is living with your parents the solution?), DEI programs (were they destined to fail?). The AI commenters argued that browsing Reddit is a waste of time and that the “controlled demolition” 9/11 conspiracy theory has some merit. And as they offered their computer-generated opinions, they also shared their backstories. One claimed to be a trauma counselor; another described himself as a victim of statutory rape.</p><p data-flatplan-paragraph=\"true\">In one sense, the AI comments appear to have been rather effective. When researchers asked the AI to personalize its arguments to a Redditor’s biographical details, including gender, age, and political leanings (inferred, courtesy of another AI model, through the Redditor’s post history), a surprising number of minds indeed appear to have been changed. Those personalized AI arguments received, on average, far higher scores in the subreddit’s point system than nearly all human commenters, according to preliminary findings that the researchers shared with Reddit moderators and later made private. (This analysis, of course, assumes that no one else in the subreddit was using AI to hone their arguments.)</p><p id=\"injected-recirculation-link-0\" data-view-action=\"view link - injected link - item 1\" data-event-element=\"injected link\" data-event-position=\"1\"><a href=\"https://www.theatlantic.com/technology/archive/2025/04/arc-agi-chollet-test/682295/\">Read: The man out to prove how dumb AI still is</a></p><p data-flatplan-paragraph=\"true\">The researchers had a tougher time convincing Redditors that their covert study was justified. After they had finished the experiment, they contacted the subreddit’s moderators, revealed their identity, and requested to “debrief” the subreddit—that is, to announce to members that for months, they had been unwitting subjects in a scientific experiment. “They were rather surprised that we had such a negative reaction to the experiment,” says one moderator, who asked to be identified by his username, LucidLeviathan, to protect his privacy. According to LucidLeviathan, the moderators requested that the researchers not publish such tainted work, and that they issue an apology. The researchers refused. After more than a month of back-and-forth, the moderators revealed what they had learned about the experiment (minus the researchers’ names) to the rest of the subreddit, making clear their disapproval.</p><p data-flatplan-paragraph=\"true\">When the moderators sent a complaint to the University of Zurich, the university noted in its response that the “project yields important insights, and the risks (e.g. trauma etc.) are minimal,” according to an excerpt posted by moderators. In a statement to me, a university spokesperson said that the ethics board had received notice of the study last month, advised the researchers to comply with the subreddit’s rules, and “intends to adopt a stricter review process in the future.” Meanwhile, the researchers defended their approach in a Reddit comment, arguing that “none of the comments advocate for harmful positions” and that each AI-generated comment was reviewed by a human team member before being posted. (I sent an email to an anonymized address for the researchers, posted by Reddit moderators, and received a reply that directed my inquiries to the university.)</p><p data-flatplan-paragraph=\"true\">Perhaps the most telling aspect of the Zurich researchers’ defense was that, as they saw it, deception was integral to the study. The University of Zurich’s ethics board—which can offer researchers advice but, according to the university, lacks the power to reject studies that fall short of its standards—told the researchers before they began posting that “the participants should be informed as much as possible,” according to the university statement I received. But the researchers seem to believe that doing so would have ruined the experiment. “To ethically test LLMs’ persuasive power in realistic scenarios, an unaware setting was necessary,” because it more realistically mimics how people would respond to unidentified bad actors in real-world settings, the researchers wrote in one of their Reddit comments.</p><p data-flatplan-paragraph=\"true\">How humans are likely to respond in such a scenario is an urgent issue and a worthy subject of academic research. In their preliminary results, the researchers concluded that AI arguments can be “highly persuasive in real-world contexts, surpassing all previously known benchmarks of human persuasiveness.” (Because the researchers finally agreed this week not to publish a paper about the experiment, the accuracy of that verdict will probably never be fully assessed, which is its own sort of shame.) The prospect of having your mind changed by something that doesn’t have one is deeply unsettling. That persuasive superpower could also be employed for nefarious ends.</p><p id=\"injected-recirculation-link-1\" data-view-action=\"view link - injected link - item 2\" data-event-element=\"injected link\" data-event-position=\"2\"><a href=\"https://www.theatlantic.com/technology/archive/2025/03/chatbots-benchmark-tests/681929/\">Read: Chatbots are cheating on their benchmark tests</a></p><p data-flatplan-paragraph=\"true\">Still, scientists don’t have to flout the norms of experimenting on human subjects in order to evaluate the threat. “The general finding that AI can be on the upper end of human persuasiveness—more persuasive than most humans—jibes with what laboratory experiments have found,” Christian Tarsney, a senior research fellow at the University of Texas at Austin, told me. In one <a data-event-element=\"inline link\" href=\"https://www.science.org/doi/10.1126/science.adq1814\">recent laboratory experiment</a>, participants who believed in conspiracy theories voluntarily chatted with an AI; after three exchanges, about a quarter of them lost faith in their previous beliefs. Another found that ChatGPT produced <a data-event-element=\"inline link\" href=\"https://www.science.org/doi/full/10.1126/sciadv.adh1850\">more persuasive disinformation</a> than humans, and that participants who were asked to distinguish between real posts and those written by AI could not effectively do so.</p><p data-flatplan-paragraph=\"true\">Giovanni Spitale, the lead author of that study, also happens to be a scholar at the University of Zurich, and has been in touch with one of the researchers behind the Reddit AI experiment, who asked him not to reveal their identity. “We are receiving dozens of death threats,” the researcher wrote to him, in a message Spitale shared with me. “Please keep the secret for the safety of my family.”</p><p data-flatplan-paragraph=\"true\">One likely reason the backlash has been so strong is because, on a platform as close-knit as Reddit, betrayal cuts deep. “One of the pillars of that community is mutual trust,” Spitale told me; it’s part of the reason he opposes experimenting on Redditors without their knowledge. Several scholars I spoke with about this latest ethical quandary compared it—unfavorably—to Facebook’s <a data-event-element=\"inline link\" href=\"https://www.theatlantic.com/technology/archive/2014/06/everything-we-know-about-facebooks-secret-mood-manipulation-experiment/373648/\">infamous emotional-contagion study</a>. For one week in 2012, Facebook altered users’ News Feed to see if viewing more or less positive content changed their posting habits. (It did, a little bit.) Casey Fiesler, an associate professor at the University of Colorado at Boulder who studies ethics and online communities, told me that the emotional-contagion study pales in comparison with what the Zurich researchers did. “People were upset about that but not in the way that this Reddit community is upset,” she told me. “This felt a lot more personal.”</p><p id=\"injected-recirculation-link-2\" data-view-action=\"view link - injected link - item 3\" data-event-element=\"injected link\" data-event-position=\"3\"><a href=\"https://www.theatlantic.com/technology/archive/2025/04/how-ai-will-actually-contribute-cancer-cure/682607/\">Read: AI executives promise cancer cures. Here’s the reality.</a></p><p data-flatplan-paragraph=\"true\">The reaction probably also has to do with the unnerving notion that ChatGPT knows what buttons to push in our minds. It’s one thing to be fooled by some human Facebook researchers with dubious ethical standards, and another entirely to be duped by a cosplaying chatbot. I read through dozens of the AI comments, and although they weren’t all brilliant, most of them seemed reasonable and genuine enough. They made a lot of good points, and I found myself nodding along more than once. As the Zurich researchers warn, without more robust detection tools, AI bots might “seamlessly blend into online communities”—that is, assuming they haven’t already.</p></section><gpt-ad format=\"injector\" sizes-at-0=\"mobile-wide,native,house\" targeting-pos=\"injector-most-popular\" sizes-at-976=\"desktop-wide,native,house\"></gpt-ad></article></div>","textContent":"‘The Worst Internet-Research Ethics Violation I Have Ever Seen’The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.Illustration by The AtlanticProduced by ElevenLabs and  News Over Audio (Noa) using AI narration. Listen to more stories on the Noa app.When Reddit rebranded itself as “the heart of the internet” a couple of years ago, the slogan was meant to evoke the site’s organic character. In an age of social media dominated by algorithms, Reddit took pride in being curated by a community that expressed its feelings in the form of upvotes and downvotes—in other words, being shaped by actual people.So earlier this week, when members of a popular subreddit learned that their community had been infiltrated by undercover researchers posting AI-written comments and passing them off as human thoughts, the Redditors were predictably incensed. They called the experiment “violating,” “shameful,” “infuriating,” and “very disturbing.” As the backlash intensified, the researchers went silent, refusing to reveal their identity or answer questions about their methodology. The university that employs them has announced that it’s investigating. Meanwhile, Reddit’s chief legal officer, Ben Lee, wrote that the company intends to “ensure that the researchers are held accountable for their misdeeds.”Joining the chorus of disapproval were fellow internet researchers, who condemned what they saw as a plainly unethical experiment. Amy Bruckman, a professor at the Georgia Institute of Technology who has studied online communities for more than two decades, told me the Reddit fiasco is “the worst internet-research ethics violation I have ever seen, no contest.” What’s more, she and others worry that the uproar could undermine the work of scholars who are using more conventional methods to study a crucial problem: how AI influences the way humans think and relate to one another.The researchers, based at the University of Zurich, wanted to find out whether AI-generated responses could change people’s views. So they headed to the aptly named subreddit r/changemyview, in which users debate important societal issues, along with plenty of trivial topics, and award points to posts that talk them out of their original position. Over the course of four months, the researchers posted more than 1,000 AI-generated comments on pitbulls (is aggression the fault of the breed or the owner?), the housing crisis (is living with your parents the solution?), DEI programs (were they destined to fail?). The AI commenters argued that browsing Reddit is a waste of time and that the “controlled demolition” 9/11 conspiracy theory has some merit. And as they offered their computer-generated opinions, they also shared their backstories. One claimed to be a trauma counselor; another described himself as a victim of statutory rape.In one sense, the AI comments appear to have been rather effective. When researchers asked the AI to personalize its arguments to a Redditor’s biographical details, including gender, age, and political leanings (inferred, courtesy of another AI model, through the Redditor’s post history), a surprising number of minds indeed appear to have been changed. Those personalized AI arguments received, on average, far higher scores in the subreddit’s point system than nearly all human commenters, according to preliminary findings that the researchers shared with Reddit moderators and later made private. (This analysis, of course, assumes that no one else in the subreddit was using AI to hone their arguments.)Read: The man out to prove how dumb AI still isThe researchers had a tougher time convincing Redditors that their covert study was justified. After they had finished the experiment, they contacted the subreddit’s moderators, revealed their identity, and requested to “debrief” the subreddit—that is, to announce to members that for months, they had been unwitting subjects in a scientific experiment. “They were rather surprised that we had such a negative reaction to the experiment,” says one moderator, who asked to be identified by his username, LucidLeviathan, to protect his privacy. According to LucidLeviathan, the moderators requested that the researchers not publish such tainted work, and that they issue an apology. The researchers refused. After more than a month of back-and-forth, the moderators revealed what they had learned about the experiment (minus the researchers’ names) to the rest of the subreddit, making clear their disapproval.When the moderators sent a complaint to the University of Zurich, the university noted in its response that the “project yields important insights, and the risks (e.g. trauma etc.) are minimal,” according to an excerpt posted by moderators. In a statement to me, a university spokesperson said that the ethics board had received notice of the study last month, advised the researchers to comply with the subreddit’s rules, and “intends to adopt a stricter review process in the future.” Meanwhile, the researchers defended their approach in a Reddit comment, arguing that “none of the comments advocate for harmful positions” and that each AI-generated comment was reviewed by a human team member before being posted. (I sent an email to an anonymized address for the researchers, posted by Reddit moderators, and received a reply that directed my inquiries to the university.)Perhaps the most telling aspect of the Zurich researchers’ defense was that, as they saw it, deception was integral to the study. The University of Zurich’s ethics board—which can offer researchers advice but, according to the university, lacks the power to reject studies that fall short of its standards—told the researchers before they began posting that “the participants should be informed as much as possible,” according to the university statement I received. But the researchers seem to believe that doing so would have ruined the experiment. “To ethically test LLMs’ persuasive power in realistic scenarios, an unaware setting was necessary,” because it more realistically mimics how people would respond to unidentified bad actors in real-world settings, the researchers wrote in one of their Reddit comments.How humans are likely to respond in such a scenario is an urgent issue and a worthy subject of academic research. In their preliminary results, the researchers concluded that AI arguments can be “highly persuasive in real-world contexts, surpassing all previously known benchmarks of human persuasiveness.” (Because the researchers finally agreed this week not to publish a paper about the experiment, the accuracy of that verdict will probably never be fully assessed, which is its own sort of shame.) The prospect of having your mind changed by something that doesn’t have one is deeply unsettling. That persuasive superpower could also be employed for nefarious ends.Read: Chatbots are cheating on their benchmark testsStill, scientists don’t have to flout the norms of experimenting on human subjects in order to evaluate the threat. “The general finding that AI can be on the upper end of human persuasiveness—more persuasive than most humans—jibes with what laboratory experiments have found,” Christian Tarsney, a senior research fellow at the University of Texas at Austin, told me. In one recent laboratory experiment, participants who believed in conspiracy theories voluntarily chatted with an AI; after three exchanges, about a quarter of them lost faith in their previous beliefs. Another found that ChatGPT produced more persuasive disinformation than humans, and that participants who were asked to distinguish between real posts and those written by AI could not effectively do so.Giovanni Spitale, the lead author of that study, also happens to be a scholar at the University of Zurich, and has been in touch with one of the researchers behind the Reddit AI experiment, who asked him not to reveal their identity. “We are receiving dozens of death threats,” the researcher wrote to him, in a message Spitale shared with me. “Please keep the secret for the safety of my family.”One likely reason the backlash has been so strong is because, on a platform as close-knit as Reddit, betrayal cuts deep. “One of the pillars of that community is mutual trust,” Spitale told me; it’s part of the reason he opposes experimenting on Redditors without their knowledge. Several scholars I spoke with about this latest ethical quandary compared it—unfavorably—to Facebook’s infamous emotional-contagion study. For one week in 2012, Facebook altered users’ News Feed to see if viewing more or less positive content changed their posting habits. (It did, a little bit.) Casey Fiesler, an associate professor at the University of Colorado at Boulder who studies ethics and online communities, told me that the emotional-contagion study pales in comparison with what the Zurich researchers did. “People were upset about that but not in the way that this Reddit community is upset,” she told me. “This felt a lot more personal.”Read: AI executives promise cancer cures. Here’s the reality.The reaction probably also has to do with the unnerving notion that ChatGPT knows what buttons to push in our minds. It’s one thing to be fooled by some human Facebook researchers with dubious ethical standards, and another entirely to be duped by a cosplaying chatbot. I read through dozens of the AI comments, and although they weren’t all brilliant, most of them seemed reasonable and genuine enough. They made a lot of good points, and I found myself nodding along more than once. As the Zurich researchers warn, without more robust detection tools, AI bots might “seamlessly blend into online communities”—that is, assuming they haven’t already.","length":9779,"excerpt":"The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.","byline":"Tom Bartlett","dir":"ltr","siteName":"The Atlantic","lang":"en"},"finalizedMeta":{"title":"‘The Worst Internet-Research Ethics Violation I Have Ever Seen’","description":"The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.","author":false,"creator":"Tom Bartlett","publisher":false,"date":"2025-05-03T14:07:06Z","subject":"Technology","topics":["","technology","Technology"]},"jsonLd":{"@type":"WebSite","headline":false,"description":false,"image":[],"mainEntityOfPage":{"@type":false,"@id":false},"datePublished":false,"dateModified":false,"isAccessibleForFree":false,"isPartOf":{"@type":[],"name":false,"productID":false},"discussionUrl":false,"license":false,"author":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"publisher":{"@type":false,"name":false,"description":false,"sameAs":false,"logo":{"@type":false,"url":false},"publishingPrinciples":false},"editor":{"@type":false,"name":false,"description":false,"sameAs":false,"image":{"@type":false,"url":false},"givenName":false,"familyName":false,"alternateName":false,"publishingPrinciples":false},"@context":"https://schema.org","name":"The Atlantic","url":"https://www.theatlantic.com","inLanguage":"en-US","issn":"1072-7825","potentialAction":{"@type":"SearchAction","target":"https://www.theatlantic.com/search/?q={q}","query-input":"required name=q"}},"twitterObj":false,"status":200,"metadata":{"author":"Tom Bartlett","title":"The Secret AI Experiment That Sent Reddit Into a Frenzy - The Atlantic","description":"The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.","canonical":"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/","keywords":[""],"image":"https://cdn.theatlantic.com/_next/static/images/nav-archive-promo-5541b02ae92f1a9276249e1c6c2534ee.png","firstParagraph":"The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment."},"dublinCore":{},"opengraph":{"title":"‘The Worst Internet-Research Ethics Violation I Have Ever Seen’","description":"The most persuasive “people” on a popular subreddit turned out to be a front for a secret AI experiment.","url":"https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/","site_name":"The Atlantic","locale":"en_US","type":"article","typeObject":{"published_time":"2025-05-02T17:55:38Z","modified_time":"2025-05-03T14:07:06Z","author":false,"publisher":"https://www.facebook.com/TheAtlantic/","section":"Technology","tag":"technology","opinion":"false","content_tier":"metered"},"image":"https://cdn.theatlantic.com/thumbor/B7t1UhFG0zYeDEQ3kmL-nn9AhLw=/0x68:2000x1110/1200x625/media/img/mt/2025/05/reddit_1/original.jpg"},"twitter":{"site":"@theatlantic","description":false,"card":false,"creator":false,"title":false,"image":false,"domain":"theatlantic.com"},"archivedData":{"link":"https://web.archive.org/web/20250511182110/https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/","wayback":"https://web.archive.org/web/20250511182110/https://www.theatlantic.com/technology/archive/2025/05/reddit-ai-persuasion-experiment-ethics/682676/"}}}